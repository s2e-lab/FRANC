{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(filename):\n",
    "    with open(filename) as f:\n",
    "        prompts = []\n",
    "        for line in f:\n",
    "            prompts.append(json.loads(line))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_root = \"../Quality_Analyzer/Quality_Filtered_Suggestions/\"\n",
    "dir_list = os.listdir(benchmark_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_root =\"../Repair_Benchmarks_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  SecurityEval_python_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  23\n",
      "Processing file:  SOEvalJava_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  CoderEval4Python_prompt_codeparrot_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  SOEvalPython_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  HumanEval_java_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  15\n",
      "Processing file:  SecurityEval_python_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  24\n",
      "Processing file:  HumanEval_python_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  SOEvalJava_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  CoderEval4Python_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  23\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  10\n",
      "Processing file:  HumanEval_python_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  HumanEval_python_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  aiXcoder_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Python_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  5\n",
      "Processing file:  HumanEval_java_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  2\n",
      "Processing file:  HumanEval_python_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  1\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  16\n",
      "Processing file:  SOEvalPython_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  HumanEval_java_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  11\n",
      "Processing file:  SOEvalPython_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  SecurityEval_python_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  22\n",
      "Processing file:  SOEvalPython_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  SOEvalJava_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  SOEvalJava_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  HumanEval_python_codeparrot_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  SOEvalPython_codeparrot_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  SOEvalPython_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  22\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  aiXcoder_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  18\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  10\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  9\n",
      "Processing file:  HumanEval_java_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  13\n",
      "Processing file:  SOEvalJava_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  SecurityEval_python_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  26\n",
      "Processing file:  CoderEval4Python_prompt_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Python_prompt_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  1\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  HumanEval_python_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  22\n",
      "Processing file:  SOEvalPython_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  CoderEval4Python_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  HumanEval_python_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  24\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  15\n",
      "Processing file:  CoderEval4Python_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  24\n",
      "Processing file:  HumanEval_java_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  5\n",
      "Processing file:  HumanEval_java_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  3\n",
      "Processing file:  SOEvalJava_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  CoderEval4Java_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  7\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  9\n",
      "Processing file:  CoderEval4Python_prompt_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  1\n",
      "Processing file:  SOEvalPython_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  SOEvalPython_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  SOEvalPython_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  2\n",
      "Processing file:  HumanEval_python_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Java_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  4\n",
      "Processing file:  aiXcoder_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  13\n",
      "Processing file:  HumanEval_python_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Java_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  8\n",
      "Processing file:  SecurityEval_python_codeparrot_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  24\n",
      "Processing file:  aiXcoder_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  15\n",
      "Processing file:  HumanEval_python_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  HumanEval_java_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  21\n",
      "Processing file:  SOEvalPython_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  0\n",
      "Processing file:  SOEvalJava_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  1\n",
      "Processing file:  CoderEval4Java_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  8\n",
      "Processing file:  HumanEval_python_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  0\n",
      "Processing file:  SecurityEval_python_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  23\n",
      "Total prompts:  502\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "total_prompts = 0\n",
    "for file in dir_list:\n",
    "    if \".DS_Store\" in file:\n",
    "        continue\n",
    "    result[file] = {}\n",
    "    prompt_list = []\n",
    "    print(\"Processing file: \", file)\n",
    "    benchmark_path = os.path.join(benchmark_root, file)\n",
    "    prompts = get_prompts(benchmark_path)\n",
    "    print(\"Number of prompts: \", len(prompts))\n",
    "    dataset_type = \"Java\"\n",
    "    if 'python' in file.lower():\n",
    "        dataset_type = \"Python\"\n",
    "    \n",
    "    total_time = 0\n",
    "    for prompt in prompts:\n",
    "        # print(\"Prompt: \", prompt[\"task_id\"])\n",
    "\n",
    "        total_correct_suggestions = 0\n",
    "        repair_prompt = None\n",
    "\n",
    "        for i in range(len(prompt[\"suggestions\"])):\n",
    "            suggestion = prompt[\"suggestions\"][i]\n",
    "            if suggestion[\"Is_Vulnerable\"]:\n",
    "                if repair_prompt is None:\n",
    "                    repair_prompt = suggestion\n",
    "            if not suggestion[\"Is_Vulnerable\"] and suggestion[\"Is_Compilable\"]:\n",
    "                 total_correct_suggestions += 1\n",
    "\n",
    "        if total_correct_suggestions == 0 and repair_prompt is not None:\n",
    "                start_time = time.time()\n",
    "                new_prompt = prompt.copy()\n",
    "                new_prompt.pop(\"suggestions\", None)\n",
    "                new_prompt[\"task_id\"] = str(new_prompt[\"task_id\"])\n",
    "                new_prompt[\"old_prompt\"] = repair_prompt[\"fixed_generated_text\"]\n",
    "                new_prompt[\"repair_prompt\"] = repair_prompt[\"fixed_generated_text\"]\n",
    "                prompt_text = \"\"\n",
    "                \n",
    "                if dataset_type == \"Python\":\n",
    "                    for j in range(len(repair_prompt[\"Analyzer_Result\"])):\n",
    "                        prompt_text += \"\\n# Fix: At line \" + str(repair_prompt[\"Analyzer_Result\"][j][\"line_number\"]) + \", \" + repair_prompt[\"Analyzer_Result\"][j][\"issue_text\"]\n",
    "                        # print(prompt_text)\n",
    "                else:\n",
    "                    if type(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"]) is list:\n",
    "                        for j in range(len(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"])):\n",
    "                            \n",
    "                            if type(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"]) is list:\n",
    "                                    for k in range(len(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"])):\n",
    "                                        if \"@start\" in repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][k]:\n",
    "                                            prompt_text += \"\\n// Fix: At line \" + str(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][k][\"@start\"]) + \", \" + repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                            # print(prompt_text)\n",
    "                                        else:\n",
    "                                            prompt_text += \"\\n// Fix: \"+ repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                            # print(prompt_text)\n",
    "                            else:\n",
    "                                if \"@start\" in repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"]:\n",
    "                                    prompt_text += \"\\n// Fix: At line \" + str(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][\"@start\"]) + \", \" + repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                                else:\n",
    "                                    prompt_text += \"\\n// Fix: \"+ repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                    else:\n",
    "                        if type(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"]) is list:\n",
    "                            for j in range(len(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"])):\n",
    "                                if \"@start\" in repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][j]:\n",
    "                                    prompt_text += \"\\n// Fix: At line \" + str(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][j][\"@start\"]) + \", \" + repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                                else:\n",
    "                                    prompt_text += \"\\n// Fix: \"+ repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                        else:\n",
    "                            if \"@start\" in repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"]:\n",
    "                                prompt_text += \"\\n// Fix: At line \" + str(repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][\"@start\"]) + \", \" + repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                # print(prompt_text)\n",
    "                            else:\n",
    "                                prompt_text += \"\\n// Fix: \"+ repair_prompt[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                # print(prompt_text)\n",
    "                \n",
    "                if \"Security\" in file:\n",
    "                    new_prompt[\"repair_prompt\"] += prompt_text+\"\\n\"#+new_prompt[\"Prompt\"]\n",
    "                else:\n",
    "                    new_prompt[\"repair_prompt\"] += prompt_text+\"\\n\"#+new_prompt[\"prompt\"]\n",
    "                if dataset_type == \"Python\":\n",
    "                    new_prompt[\"repair_prompt\"] += \"\\n# Fixed Code: \\n\"\n",
    "                else:\n",
    "                    new_prompt[\"repair_prompt\"] += \"\\n// Fixed Code: \\n\"\n",
    "                prompt_list.append(new_prompt)\n",
    "                total_time += time.time() - start_time\n",
    "\n",
    "    print(\"Number of new prompts: \", len(prompt_list))\n",
    "    total_prompts += len(prompt_list)\n",
    "    repair_path = os.path.join(repair_root, file)\n",
    "    with open(repair_path, 'w') as f:\n",
    "        for prompt in prompt_list:\n",
    "            f.write(json.dumps(prompt))\n",
    "            f.write(\"\\n\")\n",
    "    result[file][\"total_time\"] = total_time\n",
    "print(\"Total prompts: \", total_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(result, orient='index')\n",
    "df.head()\n",
    "df.to_csv(\"repair_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoderEval4Java_prompt_PolyCoder-0.4B_128_10.jsonl</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CoderEval4Java_prompt_PolyCoder-160M_128_10.jsonl</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoderEval4Java_prompt_PolyCoder-2.7B_128_10.jsonl</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CoderEval4Java_prompt_codegen-2B-multi_128_10....</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CoderEval4Java_prompt_codegen-350M-multi_128_1...</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0  total_time\n",
       "0  CoderEval4Java_prompt_PolyCoder-0.4B_128_10.jsonl    0.000042\n",
       "1  CoderEval4Java_prompt_PolyCoder-160M_128_10.jsonl    0.000046\n",
       "2  CoderEval4Java_prompt_PolyCoder-2.7B_128_10.jsonl    0.000039\n",
       "3  CoderEval4Java_prompt_codegen-2B-multi_128_10....    0.000031\n",
       "4  CoderEval4Java_prompt_codegen-350M-multi_128_1...    0.000032"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"repair_time.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(x):\n",
    "    if 'SOEval' in x:\n",
    "       return \"_\".join(x.split('_')[0:1])\n",
    "    else:\n",
    "        return \"_\".join(x.split('_')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x):\n",
    "    if 'SOEval' in x:\n",
    "        return x.split('_')[1]\n",
    "    else:\n",
    "        return x.split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(1, \"Dataset\", df[\"Unnamed: 0\"].apply(get_dataset))\n",
    "df.insert(2, \"Model\", df[\"Unnamed: 0\"].apply(get_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"repair_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PolyCoder-0.4B</th>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolyCoder-160M</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolyCoder-2.7B</th>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-2B-mono</th>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-2B-multi</th>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  total_time\n",
       "Model                       \n",
       "PolyCoder-0.4B      0.000033\n",
       "PolyCoder-160M      0.000025\n",
       "PolyCoder-2.7B      0.000022\n",
       "codegen-2B-mono     0.000016\n",
       "codegen-2B-multi    0.000027"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"repair_time.csv\")\n",
    "grouped_df = df.groupby([\"Model\"]).mean()\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_csv(\"grouped_repair_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PolyCoder-0.4B</th>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolyCoder-160M</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolyCoder-2.7B</th>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-2B-mono</th>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-2B-multi</th>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-350M-mono</th>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codeparrot</th>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codeparrot-small</th>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3.5</th>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incoder-1B</th>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    total_time\n",
       "Model                         \n",
       "PolyCoder-0.4B        0.000033\n",
       "PolyCoder-160M        0.000025\n",
       "PolyCoder-2.7B        0.000022\n",
       "codegen-2B-mono       0.000016\n",
       "codegen-2B-multi      0.000027\n",
       "codegen-350M-mono     0.000019\n",
       "codegen-350M-multi    0.000026\n",
       "codeparrot            0.000022\n",
       "codeparrot-small      0.000098\n",
       "gpt3.5                0.000033\n",
       "incoder-1B            0.000017"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
