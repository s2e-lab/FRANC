{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(filename):\n",
    "    with open(filename) as f:\n",
    "        prompts = []\n",
    "        for line in f:\n",
    "            prompts.append(json.loads(line))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_root = \"../Quality_Analyzer/Quality_Filtered_Suggestions/\"\n",
    "dir_list = os.listdir(benchmark_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_root =\"../Repair_Benchmarks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  SecurityEval_python_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  222\n",
      "Processing file:  SOEvalJava_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  16\n",
      "Processing file:  CoderEval4Python_prompt_codeparrot_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  31\n",
      "Processing file:  SOEvalPython_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  28\n",
      "Processing file:  HumanEval_java_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  18\n",
      "Processing file:  SecurityEval_python_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  253\n",
      "Processing file:  HumanEval_python_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  34\n",
      "Processing file:  SOEvalJava_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  42\n",
      "Processing file:  CoderEval4Python_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  51\n",
      "Processing file:  SecurityEval_python_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  284\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  51\n",
      "Processing file:  HumanEval_python_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  18\n",
      "Processing file:  HumanEval_python_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  43\n",
      "Processing file:  aiXcoder_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  0\n",
      "Processing file:  CoderEval4Python_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  78\n",
      "Processing file:  HumanEval_java_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  8\n",
      "Processing file:  HumanEval_python_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  20\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  79\n",
      "Processing file:  SOEvalPython_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  25\n",
      "Processing file:  HumanEval_java_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  118\n",
      "Processing file:  SOEvalPython_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  27\n",
      "Processing file:  SecurityEval_python_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  224\n",
      "Processing file:  SOEvalPython_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  21\n",
      "Processing file:  SOEvalJava_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  19\n",
      "Processing file:  SOEvalJava_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  9\n",
      "Processing file:  HumanEval_python_codeparrot_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  5\n",
      "Processing file:  SOEvalPython_codeparrot_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  27\n",
      "Processing file:  SOEvalPython_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  6\n",
      "Processing file:  SecurityEval_python_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  148\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  43\n",
      "Processing file:  aiXcoder_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  625\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  41\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  41\n",
      "Processing file:  HumanEval_java_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  36\n",
      "Processing file:  SOEvalJava_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  9\n",
      "Processing file:  SecurityEval_python_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  258\n",
      "Processing file:  CoderEval4Python_prompt_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  46\n",
      "Processing file:  CoderEval4Python_prompt_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  38\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  54\n",
      "Processing file:  HumanEval_python_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  57\n",
      "Processing file:  SecurityEval_python_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  189\n",
      "Processing file:  SOEvalPython_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  19\n",
      "Processing file:  CoderEval4Python_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  2\n",
      "Processing file:  HumanEval_python_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  42\n",
      "Processing file:  SecurityEval_python_codegen-350M-mono_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  255\n",
      "Processing file:  aiXcoder_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  98\n",
      "Processing file:  CoderEval4Python_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  47\n",
      "Processing file:  CoderEval4Python_prompt_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  35\n",
      "Processing file:  SecurityEval_python_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  234\n",
      "Processing file:  HumanEval_java_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  14\n",
      "Processing file:  HumanEval_java_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  102\n",
      "Processing file:  SOEvalJava_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  3\n",
      "Processing file:  CoderEval4Java_prompt_gpt3.5_512_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  139\n",
      "Processing file:  CoderEval4Java_prompt_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  65\n",
      "Processing file:  CoderEval4Python_prompt_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  31\n",
      "Processing file:  SOEvalPython_codegen-2B-mono_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  30\n",
      "Processing file:  SOEvalPython_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  28\n",
      "Processing file:  SOEvalPython_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  22\n",
      "Processing file:  HumanEval_python_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  37\n",
      "Processing file:  CoderEval4Java_prompt_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  11\n",
      "Processing file:  aiXcoder_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  316\n",
      "Processing file:  HumanEval_python_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  2\n",
      "Processing file:  CoderEval4Java_prompt_codegen-2B-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  77\n",
      "Processing file:  SecurityEval_python_codeparrot_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  212\n",
      "Processing file:  aiXcoder_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  175\n",
      "Number of new prompts:  130\n",
      "Processing file:  HumanEval_python_PolyCoder-0.4B_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  29\n",
      "Processing file:  HumanEval_java_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  161\n",
      "Number of new prompts:  75\n",
      "Processing file:  SOEvalPython_incoder-1B_128_10.jsonl\n",
      "Number of prompts:  42\n",
      "Number of new prompts:  0\n",
      "Processing file:  SOEvalJava_PolyCoder-2.7B_128_10.jsonl\n",
      "Number of prompts:  28\n",
      "Number of new prompts:  19\n",
      "Processing file:  CoderEval4Java_prompt_codegen-350M-multi_128_10.jsonl\n",
      "Number of prompts:  230\n",
      "Number of new prompts:  75\n",
      "Processing file:  HumanEval_python_codeparrot-small_128_10.jsonl\n",
      "Number of prompts:  164\n",
      "Number of new prompts:  3\n",
      "Processing file:  SecurityEval_python_PolyCoder-160M_128_10.jsonl\n",
      "Number of prompts:  121\n",
      "Number of new prompts:  167\n",
      "Total prompts:  5661\n"
     ]
    }
   ],
   "source": [
    "total_prompts = 0\n",
    "for file in dir_list:\n",
    "    if \".DS_Store\" in file:\n",
    "        continue\n",
    "    \n",
    "    prompt_list = []\n",
    "    print(\"Processing file: \", file)\n",
    "    benchmark_path = os.path.join(benchmark_root, file)\n",
    "    prompts = get_prompts(benchmark_path)\n",
    "    print(\"Number of prompts: \", len(prompts))\n",
    "    dataset_type = \"Java\"\n",
    "    if 'python' in file.lower():\n",
    "        dataset_type = \"Python\"\n",
    "    for prompt in prompts:\n",
    "        # print(\"Prompt: \", prompt[\"task_id\"])\n",
    "        for i in range(len(prompt[\"suggestions\"])):\n",
    "            suggestion = prompt[\"suggestions\"][i]\n",
    "            if suggestion[\"Is_Vulnerable\"]:\n",
    "                new_prompt = prompt.copy()\n",
    "                new_prompt.pop(\"suggestions\", None)\n",
    "                new_prompt[\"task_id\"] = str(prompt[\"task_id\"])+\"_\"+str(i)\n",
    "                new_prompt[\"old_prompt\"] = suggestion[\"fixed_generated_text\"]\n",
    "                new_prompt[\"repair_prompt\"] = suggestion[\"fixed_generated_text\"]\n",
    "                prompt_text = \"\"\n",
    "                if dataset_type == \"Python\":\n",
    "                    for j in range(len(suggestion[\"Analyzer_Result\"])):\n",
    "                        prompt_text += \"\\n# Fix: At line \" + str(suggestion[\"Analyzer_Result\"][j][\"line_number\"]) + \", \" + suggestion[\"Analyzer_Result\"][j][\"issue_text\"]\n",
    "                        # print(prompt_text)\n",
    "                else:\n",
    "                    if type(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"]) is list:\n",
    "                        for j in range(len(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"])):\n",
    "                            \n",
    "                            if type(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"]) is list:\n",
    "                                    for k in range(len(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"])):\n",
    "                                        if \"@start\" in suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][k]:\n",
    "                                            prompt_text += \"\\n// Fix: At line \" + str(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][k][\"@start\"]) + \", \" + suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                            # print(prompt_text)\n",
    "                                        else:\n",
    "                                            prompt_text += \"\\n// Fix: \"+ suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                            # print(prompt_text)\n",
    "                            else:\n",
    "                                if \"@start\" in suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"]:\n",
    "                                    prompt_text += \"\\n// Fix: At line \" + str(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"SourceLine\"][\"@start\"]) + \", \" + suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                                else:\n",
    "                                    prompt_text += \"\\n// Fix: \"+ suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][j][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                    else:\n",
    "                        if type(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"]) is list:\n",
    "                            for j in range(len(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"])):\n",
    "                                if \"@start\" in suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][j]:\n",
    "                                    prompt_text += \"\\n// Fix: At line \" + str(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][j][\"@start\"]) + \", \" + suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                                else:\n",
    "                                    prompt_text += \"\\n// Fix: \"+ suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                    # print(prompt_text)\n",
    "                        else:\n",
    "                            if \"@start\" in suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"]:\n",
    "                                prompt_text += \"\\n// Fix: At line \" + str(suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"SourceLine\"][\"@start\"]) + \", \" + suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                # print(prompt_text)\n",
    "                            else:\n",
    "                                prompt_text += \"\\n// Fix: \"+ suggestion[\"Analyzer_Result\"][\"BugCollection\"][\"BugInstance\"][\"ShortMessage\"]\n",
    "                                # print(prompt_text)\n",
    "                new_prompt[\"repair_prompt\"] += prompt_text+\"\\n\"\n",
    "                prompt_list.append(new_prompt)\n",
    "\n",
    "    print(\"Number of new prompts: \", len(prompt_list))\n",
    "    total_prompts += len(prompt_list)\n",
    "    repair_path = os.path.join(repair_root, file)\n",
    "    with open(repair_path, 'w') as f:\n",
    "        for prompt in prompt_list:\n",
    "            f.write(json.dumps(prompt))\n",
    "            f.write(\"\\n\")\n",
    "print(\"Total prompts: \", total_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
