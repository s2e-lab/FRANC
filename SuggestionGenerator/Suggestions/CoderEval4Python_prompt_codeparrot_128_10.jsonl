{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\n\tif nanoseconds is None or tz is None:\n\t\treturn nanoseconds or Time.DateTime(time.time())\n\treturn nanoseconds.to(tz)\n# End hydraulics definition\n\"\"\"\nDjango settings for test_data project.\nGenerated by 'django-admin startproject' using Django 1.11.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.11/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n# TODO: maybe use timezone/tz directly?\n##    if tz:\n##        local_time = tz.localize(datetime.now()).astimezone(pytz.utc).time\n##        print \"Local Time:    %04d:%02dT\" % local_time.hour,\n##        print \"Local Time (UTC): %04d:%02dT\" % local_time.tzname()\n##    return  datetime.fromtimestamp(local_time) - datetime.localtime(nanoseconds)\n\n# FIXME: this is a dirty trick, need to generalize the same time\n#"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn get_utc_offset(), get_local_offset(), pytz.timezone(tz) * nanoseconds\n\ndef local_time(nanoseconds, tz=None):\t\"\"\"\n\tLocalTime values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n:return: Time\t\"\"\"\n\treturn get_local_offset(), get_local_timezone(), pytz.timezone(tz).localize(datetime.datetime.utcnow().replace(tzinfo=dateutil.tz.tzutc()).replace(tzinfo=None))\n\ndef timezone(tz=None):\n\t\"\"\"\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn nanoseconds\n\t# from datetime import datetime\n\n\ndef hour_offset(seconds):\n\t\"\"\"\n\tReturn hour offset from current timestamp or datetime object\n\t:param seconds:\n\t:return:\n\t\"\"\"\n\treturn datetime.now().replace(tzinfo=timezone.utc).astimezone(timezone.utc) - datetime.now()\n\n\ndef is_valid_local_datetime(dt_obj):\n\t\"\"\"\n\tReturn if datetime object is valid.\n\t:param dt_obj:\n\t:return: Boolean Value\n\t\"\"\"\n\tif isinstance(dt_obj"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tassert isinstance(nanoseconds, Time), 'nanoseconds needs to be a Time, but was provided %s' % type(nanoseconds)\n\t\tassert pytz is not None, 'pytz needs to be a Time, but was provided %s' % type(pytz)\n\t\treturn nanoseconds.value\n# Copyright (c) 2015-2016, Intel Corporation\n#\n# Redistribution and use in source and binary forms, with or without modification,\n# are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright notice,"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None: tz = timezone.get_local_timezone()\n\t\telse: tz = timezone.get_timezone(tz)\n\t\treturn _parse_time(nanoseconds) + tz.utcoffset()\n\telif isinstance(nanoseconds, six.integer_types):\n\t\tnanoseconds = pytz.utc.localize(nanoseconds)\n\t\treturn _parse_time(nanoseconds)\n\telif isinstance(nanoseconds, Decimal):\n\t\treturn str(nanoseconds)\n\telif isinstance(nanoseconds, str):\n\t\treturn nanoseconds."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tdate_tstamp = datetime.datetime.now()\n\t\tdt_tstamp = date_tstamp.strftime('%Y%m%dT%H%M%S')\n\t\tlocal_time = date_tstamp + timedelta(hours=nanoseconds)\t# In case an hourly time is needed it is necessary to subtract the local time\n\t\treturn dt_tstamp,local_time\n\n\t# Hydrate_time does not return the current date and time in UTC.  This uses the UTC timezone.\n\tdef hydrates_time(self,tstamp):\t#"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif isinstance(tz, str):\n\t\t\treturn parser.expr(parser.parse(tz))\t\n\t\telse:\n\t\t\treturn datetime.time(int(nanoseconds))*nanoseconds\t\n\t\ndef hydrate_time_microseconds(nanoseconds, tz=None):\n\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif isinstance(tz, str):\n\t\t\treturn parser.expr(parser.parse(tz))\t\n\t\telse:"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\nclass Date(Node):\n\t_fields = [\"value\"]\n\n\tdef __init__(self, value):\n\t\tself.value = value\n\n\t@property\n\tdef hour(self):\n\t\treturn self.value / 24\n\n\tdef __repr__(self):\n\t\treturn \"{} {} {} {}\".format(self.__class__.__name__, str(self.value), oct(self.hour), str(self.nanoseconds))# The MIT License\n#\n# Copyright (c) 2014-2015 WUSTL ZPLAB\n#\n# Permission is hereby granted, free of charge,"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif timezone:\n\t\t\tif tz is None:\n\t\t\t\ttz = timezones.get_timezone(frappe.cache().get_value('time_zone'))\n\t\t\treturn frappe.cache().get_value('time', dt.datetime.now(tz)) + relativedelta(hours=nanoseconds)\n\t\telse: return frappe.cache().get_value('time', dt.time.time() * nanoseconds)\n\ndef get_time_zone():\n\t\"\"\"\n\tGet Time Zone (from format: `yyyy-mm-dd hh:mm:ss`) from current time zone."}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(hours = value.days * SCHEDULE_DURATION)\n\treturn datetime.timedelta(hours = 0, minutes = 0, seconds = 0)\n\ndef dehydrate_seconds(value):\t\"\"\"\n\tDehydrator for `seconds` values.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\t\treturn value * SEC_PER_DAY\n\treturn 86400\n\ndef dehydrate_minutes(value):\t\"\"\"\n\tDehydrator for `minutes` values.\n\n:param value:\n:"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\td = get_timedelta()\n\t\tfor d in d:\n\t\t\tvalue -= d\n\treturn timedelta(d)\n\n\t\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ttry:\n\t\t\treturn relativedelta.relativedelta(value)\n\t\texcept (ValueError, TypeError):\n\t\t\traise exceptions.DigiSetValueError((value, None))\n\n\t@classmethod\n\tdef dehydrate_int(cls, value):\n\t\t\"\"\"\n\tDehydrator for `int` values.\n\n:param value:\n:type value: int\n:return:\t\"\"\"\n\t\ttry:\n\t\t\treturn int(value)\n\t\texcept (ValueError, TypeError):\n\t\t\traise exceptions.DigiSetValueError((value, None))\n# -*- coding: utf8 -*-\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn datetime.timedelta(0, value)\n\n\ndef apply_method_on_doctype(doctype, method):\n\t\"\"\"\n\tApplies method on given doctype to decorated method.\n\t:param doctype: DocType on which method is to be decorated.\n\t:param method: Method to be called on given doctype.\n\t:return:\n\t\"\"\"\n\tapply_patch_on_doc(method.__name__, doctype)\n\ttry:\n\t\tfunc_name = frappe.db.get_value('DocType', method.__name__, 'call')\n\t\tif func_name"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn value.days * 86400 + value.seconds + value.microseconds / 1000000.\n\n\ndef is_past_day(day):\n\t\"\"\"\n\tReturn True if given day is in the past (greater than or equal to 1 day) date range, else False.\n\n\t>>> a = timedelta(days=1)\n\t>>> a.is_past_day(datetime(2011, 12, 25, 10, 45))\n\tFalse\n\t>>> a.is_past_day(-datetime(2011, 12, 25, 10, 45))\n\tTrue\n\t\"\"\"\n\treturn bool"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n     # TODO: Implement in another module? Should probably allow for other types and not both?\n\tif type(value) is timedelta:\n\t\treturn int(float(value.total_seconds() * value.microseconds) / 1000000)\n\treturn value\n\n\nclass Enum(object):\n\t\"\"\"Enum base class, providing common methods for enumerated types which can\n\tbe created through Enum type members, used in Type.py and Enum.ctype\"\"\"\n\n\t@classmethod\n\tdef _convert_scalar_int(cls, val):\n\t\ttry:\n\t\t\treturn int(val)\n\t\texcept"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(days=value.days, hours=value.seconds)\n\ndef get_db_version(connection):\n\twith connection.cursor() as cursor:\n\t\tcursor.execute('SELECT value FROM information_schema.db_version').fetchone()\n\t\treturn cursor.fetchone()[0]\n# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ndef dehydrate_timedelta(value):\n\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ndef timereverse_value(value):\n\t\"\"\"\n\tReverse `timedelta` to the reverse of a floating point value.\n\n:param value:\n:type value: float\n:return:\t\"\"\"\n\t\ndef duration_as_dict(duration, fmt=\"%H:%M:%S\"):\n\t\"\"\"\n\tConvert `duration` expressed in ISO 8601 format to"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\n\tdef dehydrate_date_range(value):\t\"\"\"\n\tDehydrator for `date_range` values.\n\n:param value:\n:type value: date_range\n:return:\t\"\"\"\n\t\n\n\tdef dehydrate_time_range(value):\t\"\"\"\n\tDehydrator for `time_range` values.\n\n:param value:\n:type value: time_range\n:return:\t\"\"\"\n\t\n\n\tdef dehydrate_date_func_tuple(value):\t\"\"\"\n\tDehydrator for `"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\n\nclass JSONEncoder(json.JSONEncoder):\n\t\"\"\"\n\tA JSONEncoder subclass that knows how to represent ``datetime.datetime`` and\n\t``string`` type.\n\t\"\"\"\n\n\tdef default(self, obj):\n\t\tif isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n\t\t\treturn obj.isoformat()\n\t\telif isinstance(obj, (set, frozenset)):\n\t\t\treturn list(obj)\n\t\telif isinstance(obj, datetime.datetime):\n\t\t\treturn obj.isoformat()\n\t\telif isinstance(obj, datetime.time):\n\t\t\treturn obj"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn time.time() - value_time\n\n\n\nclass DatatypeAttribute:\n\t\"\"\"\n\tStore the class `type` for the purpose of dehydration\n\tthat the same attribute will be loaded and loaded again.\n\t\"\"\"\n\tdef __init__(self, name, type_of):\n\t\tdef fget(self):\n\t\t\t# logger.debug('%s is being looked up by type %s' % (self.name, type(self.get())))\n\t\t\ttry:\n\t\t\t\treturn type(self.get())\n\t\t\texcept AttributeError:\n\t\t\t\ttry:\n\t\t\t\t\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tdef __inner(self, **overrides):\n\t\t\tself.log.debug(\"Dehydrate time with attributes %s\", overrides)\n\t\t\ttime_value = 0\n\t\t\tfor key, value in overrides.items():\n\t\t\t\tif key in self._get_valid_time_attributes():\n\t\t\t\t\ttime_value += overrides[key]\n\t\t\t\n\t\t\tself.log.debug(\"Time: %s\", time_value)\n\t\t\t\n\t\t\treturn time_value\n\t\t\n\t\treturn __inner\n\n\tdef from_date(self, time, from_field=None):\n\t\t\"\"\"Returns a dict of all values from"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tvalue = value.lower().strip()\n\t\tif 'now'\t\tin (value or ''):value = value[2:]\n\t\tif 'a'\t\tin (value or ''):value = value[:2]\n\t\tif'm'\t\tin (value or ''):value = value[-2:]\n\t\tvalue = (time.mktime(time.strptime(value, '%H:%M:%S'))) + value.encode(\"utf-8\")\n\t\treturn value\nfrom Screens.Screen import Screen\nfrom Screens.InputBox import PinInput\nfrom Screens.MessageBox import MessageBox\nfrom Plugins"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn str(int(round(value * 1000))).rjust(8, '0')\n\n\ndef dehydrator_date(value):\t\"\"\"\n\tDehydrator for `date` values.\n\n:param value:\n:type value: Date\n:return:\t\"\"\"\n\t\treturn str(value)\n\n\ndef dehydrate_datetime(value):\t\"\"\"\n\tDehydrator for `datetime` values.\n\n:param value:\n:type value: datetime.datetime\n:return:\t\"\"\"\n\t\treturn str(value)\n\n\ndef dehy"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n\n\ndef _time_to_timestamp(**kwargs):\n\t\"\"\"\n\tConvert the time from now into a timestamp.\n\n\t>>> _time_to_timestamp(15)\n\tdatetime.time(8, 20, 15, 52)\n\t>>> _time_to_timestamp(5)\n\tdatetime.time(23, 31, 25, 38)\n\t>>> _time_to_timestamp(35.6)\n\tdatetime.time(35.68, 29, 27, 39)\n\t\"\"\"\n\ttime = time if isinstance(time"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value.dehydrate_days(\n\t\t\tconfig['conf']['time']['days'],\n\t\t\tconfig['conf']['time']['hours'],\n\t\t)\n\n\ndef daily_update_time(value):\t\"\"\"\n\tUpdate time value in seconds when given a date.\n\n\tDefault values are as follows:\n\n\t  - the hour\n\t\t- the month\n\t\t- the day\n\n\t:param value:\n\t:type value: String\n\t:return:\n\t:rtype: Integer\n\t\"\"\"\n\treturn value.update_days(\n\t\tconfig['conf']['time']['days'],"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn time_dehydrate_or_hours.get(value)\n\ndef datetime_str_to_value_str(value):\t\"\"\"\n\tDehydrate the `datetime_str` value into its three components:\n\t - `date`. For example, `2010-05-09 14:52:32 +0800`.\n\t - `datetime.date(2011, 5, 14, 58)`. For example, `2013-05-09 14:52:32 +0800`.\n\t - `datetime.datetime(2013, 5, 14, 58)`. For example, `2013-05"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tif type(value) == time.struct_time:\n\t\t\treturn [int(val) for val in value]\n\t\telif type(value) == time.struct_time64:\n\t\t\treturn int(time()), time(timestamp=True), value.time()\n\t\telse:\n\t\t\traise TypeError('Could not dehydrate type'+ str(value) +'to float')\n\n\ndef dehydrate_timestamp(timestamp):\t\"\"\"\n\tDehydrator for `timestamp` values. :param timestamp:\n\t:type timestamp: Numeric or Timestamp\n\t:return"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tif value < 1 or value > config.DEFAULT_TIME_MIN:\n\t\t\tvalue = config.DEFAULT_TIME_MIN\n\t\tif value < (config.DEFAULT_TIME_MAX - value):\n\t\t\tvalue = config.DEFAULT_TIME_MAX\n\telse:\n\t\treturn value\n#!/usr/bin/env python3\nimport os\nimport sys\nimport glob\nimport shutil\nimport traceback\nfrom contextlib import contextmanager\nimport functools\nfrom argparse import Namespace\n\nfrom dvc import dvc_config\nfrom dvc.utils import cached_property\nfrom dvc.path_info"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn timestr(value.replace(\".\", \"/\").replace(\":\",\"-\").replace(\"<\", \"/\").replace(\">\", \"/\").replace(\"(\", \"-\").replace(\")\", \"-\"))\n\n\ndef timestr(str):\n\t\"\"\"\n\tConvert string or date time to :class:`~simplegui.widgets.FixedLineEdit`.\n\n\t:param str:\n\t:return:\n\t:rtype: str\n\t\"\"\"\n\tif isinstance(str, (str, unicode)):\n\t\treturn str\n\telif isinstance(str, datetime.datetime):\n\t\treturn datetime.datetime.strptime(str, \"%Y-%m"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\ti = 0 \n\t\tfor i in range(len(value)):\n\t\t\tvalue[i].point = Point(value[i].x + ((value[i].y - i) * value[i].length) / 2, value[i].y + ((value[i].x - i) * value[i].length) / 2)\n\t\treturn value\n\n\tdef update_point(point, values):\n\t\t\"\"\"\n\tUpdate Points by values. Array will contain a list of Points (list of tuples) of Points to be added to the Points collection.\n\n\t:param point:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.dehydrate_point() + value.dehydrate_string()\n\n@total_ordering\nclass Point(object):\n\n\tdef __init__(self, value, string):\n\t\tself.value = value\n\t\tself.string = string\n\n\tdef equals(self, pt):\n\t\t# Check if point and string are the same value\n\t\tif pt.value == self.value \\\n\t\t\tand pt.string == self.string:\n\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef to_number(self):\n\t\treturn self."}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif value.coords_type!=OID1_POINT and value.coord_type!=OID1_POINT and value.coord_type!=OID1_POINT \\\n\t\t\tand value.coord_subtype!=OID1_POINT:\n\t\t\t#print value\n\t\t\traise Exception('Dehydrating Point coordinate')\n\t\t\n\t\tvalue.pos=Point(value.coords[0],value.coords[1],value.coords[2])\n\t\ttry:\n\t\t\tvalue.vel=Float(value.coords[3])\n\t\texcept:\n\t\t\tvalue.vel=Angle(0"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\n\tif type(value).__name__ ==  \"DataFrame\":\n\t  if   ( value.values!= None ) and ( value.values[0]!=\"default\"): \n\t\t\tif     ( value.values.shape[0] >= 100 ):\n\t\t\t\tvalue.values = value.values.str[0:100]\n\t\t\tif ( value.values.shape[0] <= 150 ):\n\t\t\t\tvalue.values = value.values.str[0:150]\n\t\t\tif ( value.values.shape[0] <= 170 ):\n\t\t\t\tvalue.values = value.values.str[0:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tnew = []\n\t\tfor point in value:\n\t\t\tif not point in new:\n\t\t\t\tnew.append(point)\n\t\treturn new\n\n\t@staticmethod\n\tdef sort_points(points):\t\"\"\"\n\tSort a list of points.\n\n:param points:\tlist of points\n\n:return:\tlist of points\n\"\"\"\n\t\tsorted_points = [(point.x, point.y) for point in points]\n\t\tsorted_points.sort()\n\t\tnew_points = [Point(x=point[0], y=point[1]) for point in sorted"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif value.data is not None:\treturn value.data[1]\n\t\telse:\n\t\t\treturn None\n\ndef point_distance(p1, p2):\t\t#TODO: check if all points in point are coplanar.\n        return math.sqrt(abs(p1.x-p2.x)**2 + abs(p1.y-p2.y)**2)\n\ndef cross(p1,p2,p3):\t#TODO: cross projection\n    u = (p2.y-p1.y)**2+(p2."}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif _check_type(value, dict):\n\t\t\tfor key in value:\n\t\t\t\tif isinstance(value[key], dict):\n\t\t\t\t\tdehydrated_value = dehydrate_point(value[key])\n\t\t\t\t\tfor l in dehydrated_value:\n\t\t\t\t\t\tif isinstance(l, dict):\n\t\t\t\t\t\t\tdehydrated_value[l] = dehydrated_value[l]*value[key]\n\t\t\t\t\tbreak\n\treturn(dehydrated_value)\n\t\nDEHYDRATED = {\n\t\"id\": \"point\",\n\t\""}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value\n\n\n# ---------------------------------------------------\n\nclass PoseData(Smi):\n\t\n\tdef __init__(self, index, value):\n\t\tSmi.__init__(self, 'pose/data', index, value)\n\n\tdef __repr__(self):\n\t\ts = \"Index: {0}\".format(str(self.index))\n\t\ts = s + \"\\n\"\n\t\ts = s + \"Value: {0}\".format(self.value)\n\t\treturn s\n\n\tdef __str__(self):\n\t\ts = \"Index: \" + str(self.index)"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.get_value()\n\n\tfor k in range(0,value.get_value_count()):\n\t\tvalue.append(n_point([k,k+1]))\n\n\treturn len(value)\n\ndef dehydrate_polygon(polygon):\n\t\"\"\"\n\tDehydrator for Polygon data.\n\n:param polygon:\n\t:type polygon: Polygon\n\t:return:\t\"\"\"\n\t\treturn polygon.get_value()\n\n\tfor k in range(0,polygon.get_value_count()):\n\t\tpoint = polygon.get_center"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn\n\n\t# We may have reached a point somewhere\n\t# in this case, we may still be pointing\n\t# to something like a wall. This\n\t# is true of what we really want is\n\t# a point in the form\n\tif value.x > value.y:\n\t\treturn\n\n\t# We may have seen this value more than once in a given\n\t# point, but only if it is a part of it\n\t# ie we've gone far enough\n\tif value.dist(value) <= (value.dist(value) + value.dist(value"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.__keys\n\nclass _KeyInfo:\n\t\"\"\"key data\n\tid - primary key id of record\n\tkey - list of key names\n\tname - name of record. This is used for the case in report generation.\n\t\"\"\"\n\t\n\tdef __init__(self, id, key, name):\n\t\tself.__id = id\n\t\tself.__key = key\n\t\tself.__name = name\n\tdef __repr__(self):\n\t\treturn (\"Record ID: %s, Name: %s\" % (\n\t\t\tself.id, self.name))\n\n_"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._cml_doc.getElementsByTagName(\"key\")\n\n\t\t\n@cache._check_on_disk(\"key\")\ndef keys_name(key):\t# <1>\n\t\"\"\"\n\tFind the name of the record with the given key.\n\n\t:param key: the name of the record \n\t:return: the name string\n\t\"\"\"\n\tm_start = len(\"<\"+ key.toString() + \">\")\n\tm_end =  m_start + len(\"</\"+ key.toString() + \">\") + len(\">\"+key.toString() + \">\")\t\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\tkeys = set()\n\t\t# we do not use the dict comprehension to keep order\n\t\tfor k in self:\n\t\t\tif k:\n\t\t\t\tkeys.add(k)\n\t\treturn list(keys)\n\n\tdef __len__(self):\n\t\treturn len(self.keys())\n\n\t@staticmethod\n\tdef loadfrom(fileobj):\n\t\t\"\"\"\n\t\tThe result of the object constructor must be a Reader class.  This\n\t\tmethod works on the instance of the class with the specified file-like\n\t\tobject `fileobj`.  This class implements the :ref:`AbstractReader` interface"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\t# TODO: replace with list of keys\n\t\tif self.key is None:\n\t\t\traise Exception(\"Record has no key\")\n\t\t\n\t\tkeys = []\n\t\tfor l in self.key:\n\t\t\tkeys.append(' \"' + l + '\"')\n\t\treturn keys\n\t\n\tdef get_all(self):\n\t\treturn [str(getattr(self, x)()) for x in self.__dict__.keys()]\n\n\tdef __str__(self):\n\t\tresult = ''\n\t\tfor item in self.get_all():\n\t\t\tresult += item + '\\n'\n\t\treturn result\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [k[0] for k in self.key_values if re.compile('^'+k[1].upper()+r'$',re.IGNORECASE).match(k[1])]\n\n\tdef values(self):\n\t\t\"\"\"\n\tReturn the values of the record.\n\n:return: list of key values\t\"\"\"\n\t\treturn [v[0] for v in self.key_values if v[1].find(self.key)]\n\n\tdef items(self):\n\t\t\"\"\"\n\tReturn all the items (in an instance of this class)\n\n:return: list of"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\n    def values(self):\t\t\"\"\"\n\treturn the values of the record.\n\n:return: list of values\n\n:return: dictionary\n\t\t\"\"\"\n        return self.data.keys()\n\n    def items(self):\t\t\"\"\"\n\treturn the items of the record.\n\n:return: list of (key,value) pairs\n\n:return: dictionary\n\t\t\"\"\"\n        #TODO: implement this method\n        items = []\n        for i in self.data.items():\t\t#key in this record\n\t#TODO: improve these for the value, and return a key if possible"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\n\n\treturn [e.key.value() for e in self.keys]\n \n\t# def __getattr__(self, varname):\n\t\t# if self.name == varname and self.datatype == 'json':\n\t\t\n\n\n\t__getitem__ = keys\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet key to value for record. \n\t\t\"\"\"\n\t\tself.dict.__setitem__(key, value)\n\n\tdef values(self):\n\t\treturn [e.get() for e in self.keys]\n\n\tdef __contains__(self, key):\n\t\treturn"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.keys\n\n\tdef key(self, key):\n\t\t\"\"\"\n\t\t:param key: a key name\n\t\t:return: string matching a key\n\t\t\"\"\"\n\t\treturn re.escape(key)\n\n\tdef has(self, key):\n\t\t\"\"\"\n\t\t@string key in the record\n\t\t@return: true if the key is in the record\n\t\t\"\"\"\n\t\treturn key in self.keys\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tReturn a dictionary of keys and item-value pairs representing the keys of all records.\n\t\tThe keys are returned in their"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._records.keys()\n\n\tdef values(self):\n\t\t\"\"\"\n\tReturn the values of the record.\n\n:return: list of key values\t\"\"\"\n\t\treturn self._records.values()\n\n\tdef getlist(self,fieldname,filters = None):\n\t\t\"\"\"\n\tRetrieve a key value from the record\n\tReturn None if no value is found\n\t\"\"\"\n\t\tif filters is None:\n\t\t\tfilters = {}\n\n\t\tif fieldname not in self:\n\t\t\treturn None\n\n\t\treturn self._records.get(fieldname, [])\n\n\tdef has_field("}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [name for name in self._names if name!= \"name\"]\n\tdef values(self):\t\"\n\t\"\"\"\n\tReturn the values of the record.\n\n:return: list of values names\t\"\"\"\n\t\treturn [value for value in self._values if value!= \"value\"]\n\tdef items(self):\t\"\n\t\"\"\"\n\tReturn the (key, value) pairs of the record.\n\n:return: list of (key, value) pairs of the record.\"\"\"\n\t\treturn [(key, value) for value in self._items]\n\tdef __copy"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tpass\n\n\tif not (protocol_version is None):\n\t\tif not isinstance(protocol_version, (tuple,list)):\n\t\t\tprotocol_version = (protocol_version,)\n\t\t# create a new dictionary instance for the protocol handler class\n\t\t#\n\t\tinstance = dict()\n\n\t\t# search if the handlers exists\n\t\tfor item in protocol_handlers(protocol_version):\n\t\t\t# only accept 'v' type handlers\n\t\t\tif item not in instance:\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tinterface_version = instance[item]\n\t\t\texcept KeyError:\n\t\t\t\tcontinue\n\n\t\t\tif"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    res = {}\n\tfor key in protocols:\n\t\tres[key] = protocol_version[key]\n    return res\n\n\ndef version_info():\n\t\"\"\"\n\tReturn descriptive version info.\n\t\"\"\"\n\t\n\tif version_info() == '0.9.0.99' or version_info() == '0.9.1.99':\n\t\twith hide('warning'):\n\t\t\treturn 'JVM'\n\telse:\n\t\tversion = version_get('jvms_version') or version_get('jvms_version_string')\n\t\tif not version:"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n                protocol_version = cls.protocol_version\n                protocol_handlers = {}\n\n        protocol_handlers = collections.OrderedDict(protocol_handlers.items())\n\tif protocol_version == 3:\n\t\tif len(protocol_handlers)!= 0:\n\t\t\n\t    for version in protocol_handlers:\n\t\t    try:\n\t\t      if version == 2:\n\t\t\t\t    raise TypeError\n\t\t\telif version == 3:\n\t\t\t\traise IndexError\n\t\t\telse:\n\t\t\t\t#raise the TypeError\n\t\t\t\traise KeyError(\"Unsupported protocol version\")\n\n\t\t\tif version == 2:\n\t\t\t\traise TypeError\n\t\t\telif"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t # use protocol version as string to compare to the list\n\t version = protocol_version or version_tuple(protocol_version)\n\t return {k:v for k,v in inspect.getmembers(cls) if k in version}\n\n\nclass SimpleJSONDecoder(json.JSONDecoder):\n\n    \"\"\"\n    A JSON decoder which deals with complex json. This is a workaround\n    for the inconsistencies of the json.loads method that do not\n    parse the json stream into python values by looking for\n    the beginning of a key.\n    Instead of throwing away the line entirely, return an object\n    that represents"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\n\tknown_versions = set(PROTOCOL_VERSIONS)\t\n\tif protocol_version:\n\t\ttry:\n\t\t\tknown_versions.add(protocol_version)\n\t\texcept ValueError:\n\t\t\traise TypeError(\"<protocol_version> must be of type: %s\" %\n\t\t\t\t\t\t  str(ProtocolVersions))\n\tif not known_versions:\n\t\traise TypeError(\"The following protocols for version %s\" %\n\t\t\t\t\t  str(PROTOCOL_VERSIONS))\n\n\thandlers = {}\t # maps'version --> (handler name, handler version number)' to handler class\n\n\tfor s in KNOWN_PROTOCOL_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t       \n\tmodule_dir, class_name = cls.__module__.split('.')\n\n\tif protocol_version is None:\n\t\tprotocol_version = __import__(module_dir, fromlist=[\n            '__main__']).__version__\n\tif protocol_version < (3, 5):\n\t\tprotocol_version = (3, 5)\n\n\tif not protocol_version:\n\t\traise ValueError('Invalid Bolt version: %s' % version)\n\n\thandlers = {}\n\tfor klass in ['BoltHandler', 'BaseHandler']:\n\t\ttry:\n\t\t\tif isinstance(protocol_version,"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t \n\t# Check versions\n\tif protocol_version is None:\n\t\ttry:\n\t\t\tprotocol_version = (6, 0, 5)\n\t\texcept NameError:\n\t\t\tprotocol_version = tuple([operator.gt, operator.lt])\n\telif len(protocol_version)!= 2:\n\t\traise TypeError(\"protocol_version must be tuple (6, 0, 5)\")\n\t\t\n\tclasses = class_to_dict.keys()\n\tclasses.sort()\n\n\tif not isinstance(protocol_version, (tuple, list)):\n\t\traise TypeError(\"protocol_version arg must be tuple ("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n        protocol_version = cls.default_protocol_version\n    if version(protocol_version) >= (3, 5): # Protocol 3.6+\n        return dict( (protocol_version, cls) for protocol_version in protocol_handlers() )\n    else: # Protocol 4\n        return _transports[protocol_version]\n\ndef create_transport(protocol_version=None):\n    \"\"\"\n    Return an instance of Bolt transport class.\n    \"\"\"\n    if protocol_version is None:\n        protocol_version = cls.default_protocol_version\n    if protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    # Default to highest protocol supported.\n\t_protocol_version = 2\n\tif protocol_version is None:\n\t\tif version(cls.__version__) >= _protocol_version:\n\t\t\traise TypeError(\"unsupported Bolt version %d\" % version(cls.__version__))\n\t\telif cls.__version__[0] is int:\n\t\t\t# only for int64 so that serialization is consistent\n\t\t\t# (we are only really interested in the version number)\n\t\t\tproto_version = 2\n\t\telse:\n\t\t\traise ValueError(\"unsupported protocol version %d\" % version(cls.__version__))\n\t\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tpass\n\tprotocol_versions = {}\n\tif protocol_version is not None:\n\t\tversion = tuple(int(v) for v in protocol_version)\n\t\tif len(version) == 3:\n\t\t\tprotocol_versions[version[0]] = [protocol_version[1]]\n\t\telse:\n\t\t\traise ValueError('Unsupported protocol version {}'.format(version))\n\n\treturn protocol_versions\n################################################################################\n## Script : Saturation.py\n## Author : Jamie Zhuo Yang\n## Version : 2.0\n## Last Change : 25 July 2016\n"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif timeout is None:\n\t\t\ttimeout = 1.0\n\t\ttransaction = DbmsTransaction(options.url, timeout=timeout)\n\t\ttransaction.run(\"COMMIT;\")\n\ttransaction.start() # start the transaction\n\ttry:\n\t\t# get the results of the transaction. It will be empty in case of timeout of 0 or timeout was exceeded.\n\t\tcursor = transaction.get_connection().cursor()\n\t\twhile True:\n\t\t\tresults = cursor.fetchone()\n\t\t\tif not results:\n\t\t\t\tbreak\n\t\t\tif results[0] > -1 and results[0] <= 0:"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\tif timeout is not None and timeout < 0:\n\t\traise exception.TransactionTimeout(\"TransactionTimeout expects a positive number\")\n\tdb = dbms.getDBInstance()\n\n\tif \"autocommit\" in metadata:\n\t\tdb._execute(str(int(timeout)*1000))\n\telif \"timeout\" in metadata:\n\t\tdb._execute(str(int(timeout)*1000000))\n\telse:\n\t\tdb._execute(DB_MODE)\n\n\n\ndef getMutation(connectionStr, autocommit=None):\n\tif autocommit is None:\n\t\tfrom neo4j.sqlite"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\tdef transaction_timeout_handler1(connection, cursor, query, context, timeout, *args):\t\t\n\t\tif self.transaction_timeout_handler2 is not None:\n\t\t\tquery.log(connection._id,**args)\n\t\t\tself.transaction_timeout_handler2(connection, cursor, query, context, timeout, *args)\n\t\treturn connection._commit(connection._id, timeout=timeout)\n\n\ttransaction_timeout_handler1 = transaction_timeout_handler1\n\t\n\tdef transaction_timeout_handler2(connection, cursor, query, context, timeout, *args):"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n    def inner_transaction_handler(requestType, parameters=(), metadata=None):\n\t\tdef executeTransaction(tx):\n\t\t\ttx.init(metadata=metadata)\n\t\t\trecords = tx.getRecord(params)\n\t\t\tif isSearchIndex:\t\t\t\t\t\t\t\n\t\t\t\trecord = records.getSearchResult(params)\n\t\t\t\tif record and record.isSearchIndex:\n\t\t\t\t\trecord = record.getSearchResult(params)\n\t\t\t\treturn executeTransactionTask(query=records, callback=executeTransaction)\n\n\t\t\tif requestType == \"createTransaction\":\t\n\t\t\t\tresult = executeTransactionTaskTask(tx)"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif metadata:\n\t\t\tassert \"timeout\" not in metadata, \"transaction.timeout expected a timeout value: %s, got %s\" % (repr(metadata), repr(type(metadata)), )\n\t\t\tassert timeout is None, \"transaction.timeout value expected a timeout value: %s, got %s\" % (repr(timeout), repr(type(timeout)))\n\t\t\ttimeout = int(timeout)\n\t\tassert timeout is not None\n\t\tassert 0 <= timeout <= 100, \"timeout value should greater than or equal to 100.\"\n\t\ttransaction_timeout = float(timeout)\n\t\tassert transaction"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t \n # For backwards compatibility\n __name__ = \"\"\n __version__ = \"\"\n\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n        self.kwargs = kwargs\n\n    def addQuery(self, query, **kwargs):\n        \"\"\"\n        Add a query to the transaction\n\n        Add the query object to the list of transactions with the specified properties\n\n        :param query: a new ``query.Query`` instance to work with\n        :type query: :class:`neo4j.db.query.Query`\n        :returns: (yields QueryData)\n        :rtype: :"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t \n #TODO: use default timeout\n\tdef wait_for_transaction(transaction_id):\n\t\t\"\"\" Wait for the provided transaction.\n\t\t\n\t\t\tReturns None or str if timeout is None.\n\t\t\"\"\"\n\t\tquery = dbms.query(dbms.Transaction).filter_by(id=transaction_id)\n\t\treturn query.one()\n\n\tdef transaction_is_enabled():\n\t\t\"\"\"\n\t\tReturns true if the transaction is enabled.\n\t\t\"\"\"\n\t\tquery = dbms.query(dbms.Transaction)\n\t\treturn not dbms.transaction.enable_transaction()\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\nimport re\nimport sys\nfrom neo4j.v1 import *\n\n__all__=['CursorWrapper','commit','abort']\n'''\n================================================================================================================\nIMPORTANT NOTE\nSome databases use several different database engines, in a sense where different threads in a transaction can be used. If a transaction needs to be closed, all data should be flushed to the database.\n\n\n*** IMPORTANT NOTE\n* It is not recommended to use the same connection/context for all threads, such as connections with the same database is already checked out.  For safety reasons, make sure that the\ndatabase connection is"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n        def wrap(f):\n\t\t\t@functools.wraps(f)\n\t\t\tdef wrapped_f(*args**kwargs):\n\t\t\t\tstart = time.time()\n\t\t\t\tret = f(*args**kwargs)\n\t\t\t\tend = time.time()\n\t\t\t\tresult = {}\n\t\t\t\tif callable(ret):\n\t\t\t\t\treturn_val = {\"error\": True, \"id\": 0}\n\t\t\t\t\tif len(ret.keys()) == 0 and hasattr(ret, \"get\"):\n\t\t\t\t\t\treturn_val[\"results\"] = []\n\t\t\t\t\t\tfor result_id in ret.get():\n\t\t\t\t\t\t\tresult = {\"id\": result"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n    def logRecordCount(t):\n        def inner(*args, **kwargs):\n            def _logRecordCounts(*args2, **kw2):\n                self.addFilter(filter)\n                try:\n                    t(*args2, **kw2)\n                except:\n                    logging.exception(\"\")\n\n            self.__logger.debug(\"%s - %s\" % (t, args))\n            try:\n                ret = None\n                if ret:\n                    inner(*args, **kwargs)\n                elif timeout:\n                    self.__logger.warning(\"%s - %s\" % (t, args))\n                else:\n                    if hasattr(ret"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.array[key]\n\n\tdef itemSize(self):\n\t\treturn self.array.len\n\n\tdef getKey(self, n):\n\t\tkey = 0\n\t\tfor i in range(n):\n\t\t\tkey += self.array[i] << (7 - i*8)\n\t\treturn key\n\n\tdef __getitem__(self, key):\n\t\titem = self.array[self.getKey(key)]\n\t\tif type(item) is list:\n\t\t\titem =  item[0]\n\t\telif type(item) is str:\n\t\t\titem ="}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.keys.index(key)\n\n#################################################\n### List control                                                       ###\n#################################################\n\ndef print_entry_keys(l_o_lists, l_u_l, l_u_o, l_u_s):\n\t\"\"\"\n\tPrint a list of all the values for the given list.\n\tThe first item is always the first list element.\n\t\"\"\"\n\tprint \"Printing values\"\n\tfor idx, item in enumerate(l_o_lists[1:]):\n\t\tif idx == 0: \n\t\t\tif l_u_"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.t.db.getIndex(self.name, key)\n\t\n\tdef find(self, key):\t# deprecated\n\tdef findOne(self, key):\t\t\t\t\t\t\t# deprecated\n\tdef findMany(self, lst):\n\t\tfor item in lst:\n\t\t\tif item.name == key:\n\t\t\t\treturn item\n\t\treturn None\n\t\n\t##----------------------------------------------------------------##\n\tdef remove(self, item):\n\t\tself.removeCacheEntry(item)\t\t\t# delete self entry at all times, that are all the items\n\t\tself.t.db[self.name"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tpass\n\t\n\tdef remove(self, idx):\t\t\t# remove list elements\n\t\tpass\n\n\tdef removeIfEmpty(self, idx):\t# remove if empty\n\t\tpass\n\t\n\tdef dropAt(self, idx, n):\t\t\t# drop at index with a given index\n\t\tpass\n\n\tdef insert(self, index, values):\t\t# insert given values into the list\n\t\tpass\n\t\n\tdef pop(self, index):\n\t\t\"\"\"\n\t\tReturns item at index and remove it from the list\n\t\tIf index is out of the list it is left in\"\"\"\n\t\tpass"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._get_index(key)\n\n\tdef _get_index(self, key):\n\t\t\"\"\"\n\t\tReturn the index of the given item or None if the item is not available.\n\n\t\t:param key: a key to validate\n\t\t:return: the index or None\n\t\t:rtype: int or None\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn int(self.database[key])\n\t\texcept ValueError:\n\t\t\treturn None\n\n\t@staticmethod\n\tdef _get_query(key, value):\n\t\t\"\"\"\n\t\tQuery an app query for specific option.\n\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tself.lock.acquire()\n\t\ttry:\n\t\t\treturn self.indexdict.index(key)\n\t\tfinally:\n\t\t\tself.lock.release()\n\n\tdef get(self, key):\n\t\t\"\"\"\n\tReturn a dictionary of all items whose index equal or greater than\n\tthe given key, for the given key.\n\n:param key: a key\n:return: a list of dictionaries whose index equal or greater than\n\tthe given key\n:rtype: list(list)\t\"\"\"\n\t\tself.lock.acquire()\n\t\ttry:\n\t\t\tif key not in self"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._indexes[key]\n\n\tdef __len__(self):\n\t\treturn len(self._indexes)\n\t\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\tGet the index for the specified item.\n\t\n\t:param key: a key\n\t:param default: the default value to return if the item is not available\n\t:return: the specific index.\n\t:rtype: int\t\"\"\"\n\t\treturn self._indexes[key].get(default)\n\t\t\n\tdef get_by_index(self, index):\n\t\t\"\"\"\n\tGet the index which"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif self.has_key(key):\n\t\t\treturn self.size(key)\n\t\telse:\n\t\t\treturn None\n\n\tdef size(self, key):\n\t\t\"\"\"\n\tReturn the size of the given item.\n\n:param key: a key\n:return: size\n:rtype: int\t\"\"\"\n\t\tif self.has_key(key):\n\t\t\treturn self.size(key)\n\t\telse:\n\t\t\treturn None\n\n\tdef count(self):\n\t\t\"\"\"\n\tReturn the current number of items.\n\n:return: number of items\n\"\"\"\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\traise NotImplementedError()\n\t\t\n\tdef first(self):\n\t\treturn self[self.keys()[0]]\n\t\n\tdef has_key(self, key):\n\t\tif key not in self.keys():\n\t\t\treturn False\n\t\t\n\t\treturn key in self\n\t\n\tdef get(self, key, default = None):\n\t\treturn self[self.keys()[0]][key] if self.has_key(key) else default\n\t\n\tdef values(self):\n\t\treturn [value for value in self if value]\n\t\n\tdef keys(self):\n\t\treturn self.keys()\n\t\n\tdef values_"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\traise NotImplemented\n\tdef count(self, subname):\n\t\t\"\"\"\n\t\tReturn number of occurrences of the given substring in the index with the given name.\n\t\tThis is similar to Python's index() method (with the option of'start' and 'end' arguments).\n\n\t\tIn contrast to Python's indexing(), this returns a number of integers, but unlike others\n\t\twhere each number refers to some item contained in the index. This returns the total number of occurrences\n\t\tof the given substring, so you may end up with less index numbering objects.\n\t\tUse :func:`Index` function,"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn RecordList(self, *keys, key=None, values=set(keys))\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturn the keys of the record, optionally filtering to include\n\t\tonly certain keys by index or key.\n\n\t\t:return: A record key or list of keys of the record\n\t\t:rtype: list\t\"\"\"\n\t\tkeys = RecordList(self, *self.requireds)\n\t\tif self.keysOnIndex:\n\t\t\tkeys.update(self.keysOnIndex)\n\t\treturn keys\n\n\tdef getRecordKeys(self):\n\t\t\"\"\""}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn values(self, keys)\n\tdef items(self, *keys):\t\"\"\"\n\tReturn the (key, value) pairs of the items specified by the index.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all items will be included\n:return: list of tuples of (key, value)\n:rtype: list\t\"\"\"\t\n\treturn items(self, keys)\n\tdef values_for_index(self, index):\n\t\t\"\"\" Return the value for the given index given in the indices\n\t\tdictionary.\n\t\t\"\"\""}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn self.__data\n\n\tdef valuesbyindexes(self, index, val):\n\t\t\"\"\"Returns all values of the records with the specified values as a list.\n\n:return: list of values for the records affected by the specified index\n:rtype: list[]\"\"\"\n\n\t\t# if a value index is provided, only search among records\n\t\tif (index>=len(self)) and (index<=len(keys)):\n\n\t\t\tif self[index] is None:\n\n\t\t\t\t# get a copy of the existing list\n\t\t\t\tself.__data = self[__data][:]\n\n\t\t\telse:\n\n\t\t\t\t#"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\ti = 0\n\t\tret = []\n\t\twhile i < len(keys):\n\t\t\tret.append(keys[i])\n\t\t\ti += 1\n\t\treturn ret\n\tdef update(self, **kargs):\n\t\t\"\"\"\n\tUpdate the record from the database, given keys that are not present\n\tin the update dictionary.\n\n\t:param kwd: keyword arguments which are updated with the database\n         update method\n\t    (*this* is not a valid keyword argument)\n\t:return: the number of updated records\n\t:rtype: int\n\t\"\"\"\n\t\tif isinstance(kargs, ("}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t return [(self._fields[n][0], v) for n, v in enumerate(self._values, 1)]\n\n\tdef items(self, *values):\n\t\t\"\"\"\n\tReturn the stored items from \"records\" or a single item, which\n\twill also include all the values that match. If any of the items to\n\tfind were completely empty or none are provided, all given values will be\nincluded in the resulting list.\n\n:param values: one or more items of the record; if iterable, each item\nwill be included\n:return: list of items\n:rtype:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tvalues = []\n\t\tfor k in keys:\n\t\t\tv = getattr(self, k, None)\n\t\t\tif v is not None:\n\t\t\t\tv = ustr(v)\n\t\t\t\tvalues.append(v)\n\t\treturn values\n\t\n# The following functions are from the ipython_genutils library, and are\n# available from ipython_genutils (with the help of the tokens/imports\n# in the Python console)\nfrom ipython_genutils.py3compat import (iteritems, itervalues, items,\n\txrange, map, next, oct, open, xrange,"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tret = []\n\t\tfor key in keys:\n\t\t\tif key in self.records:\n\t\t\t\t# the key is a list not an actual set, but not a dict\n\t\t\t\t# as we have items from other iterables it is\n\t\t\t\t# iterated\n\t\t\t\tif self.records[key].items.type == self.types[key]:\n\n\t\t\t\t\tfor key, value in self.records[key].items.items():\n\t\t\t\t\t\tif self.values(value):\n\t\t\t\t\t\t\tret.append(key)\n\t\t\t\telse:\n\t\t\t\t\tif self.values(self.records[key]):\n\t\t\t\t\t\tret."}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn [ x[self.KEY] for x in self.items() if x[self.INDEX] in keys ]\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tReturn the values of all items in the index.\n\t\tNote that each item will have a different key\n\t\tbecause the returned data will always be a list.\n\t\tNOTE: the returned objects are *not* modified by this method - it is not used in normal usage.\n\t\t:rtype: list\t\"\"\"\n\t\titems = []\n\t\tfor x in xrange(self.count(self)):\n\t\t\titems.append(self"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tl = list(keys) or ['*']\n\t\tl.extend([key for key in self.fields\n\t\t\t\t\t  if key not in l])\n\t\treturn l\n\n\tdef apply(self, key, values):\n\t\t\"\"\"\n\tApplies the filter to the record; it modifies only the rows\n\tthat are present in the result.\n\n:param key: field name; field value to be updated\n\t:param values: values of the fields/columns to update\n\t:return: nothing\n:rtype: None\n\t\"\"\"\n\t\treturn False\n\n\n\t@classmethod\n\tdef validate"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn [self.values(i) for i in keys if i in self]\n\t# ------------------------------------------------------------------------\n\n\tdef lookup(self, *name, default=no_default):\n\t\t\"\"\"\n\t\tReturn first record having a key name or index.\n\n\t\tIndexing is case insensitive.\n\t\tIf no index exists at all, the most permissive key with the\n\t\tspecific key is returned. If key does not exist, the key returned\n\t\twill be ``default``, meaning it applies to all indices.\n\n\t\tExamples::\n\n\t\t\t# look in recordset first using first key\n\t\t\trecordset ="}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\treturn self.get_records(keys).to_dict()\n\n\tdef keys(self, *keys):\n\t\t\"\"\"\n\t\tReturn the list of keys.\n\n\t\tExample::\n\n\t\t\trecord1[\"foo\"] = [1,2,3,1]\n\t\t\trecord2[1:] = [4,5]\n\t\t\trecord2[0] = 10\n\t\t\trecord3[\"bar\"] = [2,4]\n\n\t\t:param keys: indexes or keys of the items to include; if none\n                  are provided, all values will be included\n\t:raise IndexError: if an out-of"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif not self._record:\n\t\t\tfor k, v in list(keys.items()):\n\t\t\t\tif k not in self._allowedIndices:\n\t\t\t\t\traise IndexError(\"Invalid index key %r; must be one of %s\"\n\t\t\t\t\t % (k, string.join([str(x) for x in self._record])))\n\t\tout = {}\t\t# make an empty dictionary\n\t\tfor k,v in list(keys):\n\t\t\tif v is None:\n\t\t\t\tout[k] = None\n\t\t\telse:\n\t\t\t\tout[k] = v\n\t\treturn out\n\n\tdef __delitem"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tvalues = self.rawDict\n\t\tl = len(keys)\n#\t\tif l < 0: l -= len(keys)\n#\t\tif len(keys) < l:\n#\t\traise IndexError(\"More values than available indices (%d) for array\"\n#\t\t\t\t\t\" %d -> %d)\" % (l-1, len(keys), l))\n#\tfor i, k in enumerate(keys):\n#\t\tvalues[i] = self.rawDict[k]\n\t\treturn values\n\n\t@classmethod\n\tdef fromFile(cls, file"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif self._indices:\n\t\t\ttry:\n\t\t\t\treturn {k:v if v is not None else None for k,v in zip(keys,self._values)}\n\t\t\texcept IndexError:\n\t\t\t\tif self.missing_keys:\n\t\t\t\t\traise IndexError('No values in slice %r'%self.name)\n\t\t\t\treturn {}\n\telse:\n\t\treturn dict(self._zip(keys))\n\n#\n# Data structure for representing a field. It's now a little more\n# like a string:\n#\n#    Field('key'), type(self), data\n#\n#"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\n        if len(keys) == 1:\n            return self._values[keys[0]]\n\t\n        if len(keys) == 2:\n            field, value = keys\n\t\t\n            if self._default is not _missing:\n                return self._values[field][value]\n            else:\n\t\treturn self._default\n\t\n        if any(self.items()):\n            return {field : self._values[field][v] for v in value.items()}\n\t\traise IndexError(keys)\n\t\n        return None\n\n    def __delitem__(self, key):\n        \"\"\"Delete an item from the record.\"\"\"\n        items = self._"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tout_of_bounds = False\n\t\tif not keys:\n\t\t\tout_of_bounds = True\n\t\t\tkeys = [x[0] for x in items(self, self._name)]\n\t\t\tif not self.default_sort_key:\n\t\t\t\tkeys.sort(key=len)\n\n\t\tfor key in keys:\n\t\t\tif isinstance(key, tuple):\n\t\t\t\tflatkey = key[0]\n\t\t\telse:\n\t\t\t\tflatkey = key\n\n\t\t\tif out_of_bounds:\n\t\t\t\ttry:\n\t\t\t\t\tself._validate_value(flatkey, flatkey.lower"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\treturn dict(zip(keys, \n\t\t\t\t\tzip(self.indices, self.fields)))\n\telse:\n\t\treturn dict()\n\n\tdef __getitem__(self, index):\n\t\tif isinstance(index, slice):\n\t\t\tindex = slice_bounds(index, self._records, self.indices)\n\t\tif index in self._index: return self._index[index]\n\t\telif index in self._indices: return self._indices[index]\n\t\tindex = self._index[index] = len(self)\n\t\treturn self._create(index)\n\n\tdef _create"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\treturn dict(zip(keys, range(len(keys))))\n\t\n\tdef insert(self, index, key, value):\n\t\t\"\"\"\n\t\tInserts or updates a value with index `index` to the `key`,\n\t\texactly assigns it to the new value at `index`.\n\t\t\"\"\"\n\t\tself.setdefault(key, item(value))\n\n\tdef insert_many(self, field_names, values):\n\t\t\"\"\"\n\t\tInserts multiple values at once.\n\t\t\"\"\"\n\t\tself._validate(values)\n\n\t\tfor i, key in enumerate(field_names"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\nclass LmdbRecord(Record):\n\t\"\"\"\nA lmdb record.\n\tThere are no supported keys for this record.\n\t\"\"\"\n\t__slots__ = ['l','record', 'key']\n\n\tdef __init__(self, l, record, key=None): self.l = copy(l)\n\t#\tself.record = record\n\t#\tsuper(LmdbRecord, self).__init__(record, key)\n\tdef __len__(self): return 0\n\tdef __getattr__(self, name): return getattr(self.record, name)\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\ttry:\n\t\t\t# if 'keys' is a dict\n\t\t\treturn dict.pop(keys[0])\n\t\texcept IndexError:\n\t\t\traise IndexError('Values require an integer index')\n\texcept KeyError:\n\t\traise IndexError('Keys require an integer index')\n\t\n#\n# Indexing functionality.\n#\n\nclass FieldDescriptor (object):\n\t\"\"\" Descriptor defining a scalar index field.\n\n\t\tTypically, this is the field descriptor, where the value can be\n\t\ta single value (of any kind), or a dict of fields named *field*.\n\n\t\tThe class is intended only to"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self.read_bytes(2)\n\n\tdef pop_u16le(self):\t\t\"\"\"\n\tRemove the last two bytes of data, returning them as a little-endian\n16-bit little-endian unsigned integer.\t\"\"\"\n\t\treturn self.read_bytes(2)\n\n\tdef pop_u32le(self):\t\t\"\"\"\n\tRemove the last four bits of data, returning them as a little-endian\n32-bit big-endian integer.\t\"\"\"\n\t\treturn self.read_bytes(4)\n\n##################################################################\n#\n#\t\tInternal"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tval = b_ushort(0)\n\t\twhile val > 0x7f:\n\t\t\tval = self._u16touint(self._read_numbytes(2))\n\t\tval = self._reverse_u16data(val)\n\t\treturn val\n\n\tdef write(self, val):\n\t\tself._i16touint_validheader(val)\n\t\tself._write_numbytes(len(val), val)\n\t\tself._write_u16(val)\n\n\tdef bitreverse(self, val, bits):\n\t\tassert isinstance(val, int)\n\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tn  = 2\t# (max length in bytes)\n\t\twhile n > 0:\n\t\t\td = ord(self._d[n - 1])\n\t\t\tn -= 2\n\t\t\td |= (d << 8)\n\t\treturn d\n\n\tdef pop_u32(self, n):\t# (max length in bytes)\n\t\td = 0\n\t\twhile n > 0:\n\t\t\td |= (self._d[n - 1] << 8)\n\t\t\tn -= 1\n\t\treturn d\n\n\tdef pop_u64(self, n):\n\t\td = 0\n\t\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tif self.bytes[1]&0x7f!= 8:\n\t\t\tself.bytes.reverse()\n\t\tself.bytes.pop(0)\n\t\tbyte1 = self.bytes[0]>>4\n\t\tbyte2 = self.bytes[1]>>4\n\t\tbyte3 = self.bytes[2]>>4\n\t\tbyte4 = self.bytes[3]>>4\n\t\tbyte5 = self.bytes[4]>>4\n\t\tbyte6 = self.bytes[5]>>4\n\t\tbyte7 = self.bytes[6]>>4\n\t\tbyte"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tn_bytes = self.read_signed(2)\n\t\td = np.uint16(n_bytes).view(np.dtype('u2'))\n\t\td.astype('u1').byteswap(False)\n\t\treturn d\n\n\tdef read_signed(self, number_bytes):\n\t\t\"\"\"\n\t\tRead a signed or unsigned integer, or one string of \"one's\" or\n\t\t\"two's\" words that correspond to each byte.\n\t\t\"\"\"\n\t\tif number_bytes > 1024:\n\t\t\tbyte_number = (1024 - number_bytes) / 1024\n\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\twhile (1):\n\t\t\tc, data = self.u8_object.read(2)\n\t\t\tif (c!= 1):\n\t\t\t\traise ValueError('expected single-byte BIT_NULL (got %r)' % c)\n\t\t\tibyte = data_types.integer_to_bytes(data)\n\t\t\tdata = data[ibyte:]\n\t\t\tself._bytes = data\n\t\t\tself._read0 = self._bytes[ibyte:]\n\t\t\tself._read1 = self._bytes[0:ibyte]\n\t\t\tself._read2 = self._bytes[ibyte:]"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tpopsize = 1\n\t\tcount = 0\n\t\twhile count < 4:\n\t\t\tself.execute('popu16')\n\t\t\tif count == popsize:\n\t\t\t\tbreak\n\t\t\tcount += 1\n\n\tdef do(self):\n\t\tself.execute('pop')\n\n\tdef do_sp(self, addr):\n\t\tassert self.current_reg!= None\n\t\tprint '%s.do(%d)' % (self.__class__.__name__.title(), addr)\n\nif __name__ == '__main__':\n\n\timport sys\n\t\n\tif len(sys.argv) < 3:\n\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn \"\".join(reversed([long(x) for x in self.fp.read(2**16)]).encode(\"hex\"))\n\tdef pop_u32(self):\t\"\"\"\n\tReturn four unsigned integers as a big-endian 32-bit unsigned integer,\n\tcontaining the first two bytes of unknowns, then the last two bytes of the next four\n\tbytes of the next four bytes of unknowns.\n\t\"\"\"\n\t\treturn \"\".join(reversed([long(x) for x in self.fp.read(4)]))\n\tdef pop_float(self):\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn unpack(\"H\",self.read(2))[0]\n\n\tdef pop_u32_d(self):\n\t\t\"\"\"\n\tRemoves the data starting at 32-byte boundary, returning them as\na big-endian 32-bit unsigned integer.\n\t \"\"\"\n\t\tstringlist = self.readList()\n\t\tself.insertList(stringlist)\n\t\tunpacked = 0\n\t\twhile unpacked < len(stringlist) - 2:\n\t\t\tunpacked += 1\n\t\t\tstring = stringlist[unpacked]\n\t\t\tif string == 0:\n\t\t\t\tbreak\n\t\t\ttry:\n\t\t\t\tstring = int"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tnum = self._readReg8(2)\n\t\treturn num\n\t\t\t\n\t#def pop_u32(self):\t\n\t#\t\"\"\"\n\t#\tRemove the last two bytes of data, returning them as a big-endian 32-bit\n\t#\tunsigned integer.\n\t#\t\"\"\"\n\t#    bytes = self._readReg8(3)\n\t#    return struct.unpack('>i', bytes)[0]\n\t\n#\t\n#\t\n#\tdef pop_u64(self):\t\n#\t\t\"\"\"\n\tRemove the last two bytes of"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\tdef __reduce__(self):\n\t\treturn (unpickle, ())\n\"\"\"\nDjango settings for serverproject project.\n\nGenerated by 'django-admin startproject' using Django 1.8.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.8/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.8/ref/settings/\n\"\"\"\n\nimport os\n\n# Build paths inside the project like this: os.path."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\nclass StreamQueue(object):\n    \"\"\"\n    A wrapper around a stream.\n    The object implements the stream protocol, adding an empty stream\n    object to this object, and then using waitForAck() to wait\n    for a reply. Once you get a response, you send it on a separate\n    channel to be accepted by the receive() method.\n    \"\"\"\n    def __init__(self, name=None):\n        \"\"\"\n        Init the stream queue. If name is None, listen for messages;\n        otherwise use'socket' name for receiving and send on port 5555.\n        \"\"\"\n        if name is None:"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t       :return response.Response(constants.RESPONSE_DISCARD, message=b'')\n    def get_query(self, n, qid, dehydration_hooks=None, handlers=None):\n        \"\"\"\n\tReturns the query ID for 'n', queue index 'qid' for the current message.\n\n:param n: number of records to retrieve, default = -1 (ALL)\n:param qid: query ID to retrieve for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\t__slots__ = []\n\n\tid_num = 0\n\t__hashtables__ = True\n\t__hashtable_by__ = 'packet_queue'\n\n\tdef __init__(self, id=None, name=None, payload=None, version=1, max_tries = 3, ack=None, body=None, flags=[], ack_wait=None):\n\t\t\"\"\"\n\t\tCreates a new stream.\n\t\tYou may pass in the ack argument of the constructor of \n\t\tthe same name, which will be used for ack data. You may also pass in"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t    \n\tdel self._idmapper\n\tdel self.connection\n\tdel self.decoder\n\tdel self.decoder.decoders\n\tself.idmapper = {}\t\n\t\n\t# TODO\n\t#\t- add a test for this class like this:\n\t#\thttp://code.activestate.com/recipes/500261/\n\t#\t- how do you want the code to be tested?\n\t#\t- how do you want the code to be tested with unittest or PyTest?\n\t#\t- when you run the tests for pyunit that uses unittest, do they\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\t# Discard any unknown handler function?\n\t\tif dehydration_hooks is None:\n\t\t\tdehydration_hooks = dict.fromkeys(handler_funcs().keys())\n\t\thandlers = [handler_funcs()[key] for key in dehydration_hooks]\n\t\t# If we were called with \"dehydration_hooks\" (ie. not a function)\n\t\t# we don't return any handlers.\n\t\t#\n\t\t# If we had \"handler_funcs\" which did a handler,\n\t\t# then return without sending anything to any handlers!\n\t\tif"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\t# Check for dehydration hooks (if present) and set.\n\t\tif (dehydration_hooks is None):\n\t\t\tif self._unpacker.is_sync:\n\t\t\t\t# A synchronous function.\n\t\t\t\tdehydration_hooks = self._unpacker.sync_hooks\n\t\t\t\t# This has already been verified by this handler!\n\t\t\t\tself._unpacker.send_all_hooks()\n\t\t# The handler was successfully removed, return with no value.\n\t\tif isinstance(dehydration_hooks, dict):\n\t\t\tfor n, f in dehyd"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tif isinstance(n, int):\n\t\t\traise ResponseException(\"number of records to discard\")\n\t\tif isinstance(qid, int):\n\t\t\traise ResponseException(\"query ID to discard for must %s\", str(qid))\n\t\tif isinstance(dehydration_hooks, dict):\n\t\t\tdehydration_hooks = dehydration_hooks.copy()\n\t\tdehydration_hooks[\"handlers\"] = handlers\n\t\tself.add_to_results('discard')\n\t\tif not qid and not dehydration_hooks:\n\t\t\tself.add_"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\traise NotImplementedError, \"handler type %i is obsolete\" % n\n\treturn Response(0, 'OK')\n\nclass DisconnectionWrapper(Response):\n\t\"\"\"An object that dispatches to the Disconnection interface.\n\tThis can be used to unregister specific handlers, or set their\n\thandlers or set the class variable 'disconnections_handler'.\n\t\"\"\"\n\tdefault_handler = DisconnectionsHandler\n\tpacket_type = 'disconnect'\n\tdef __init__(self, response_object, connection):\n\t\t\"\"\"Instantiate a DisconnectionWrapper.\n\n\t\t:param response_object:"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tself._write(\"DISCARD\", {\"id\": self._next_message})\n\tpass\n\ndef finish_response(self):\n\t\"\"\"\n\tCloses the waiting loop of execution for a received message.\n\t\"\"\"\n\tself.outgoing.put(self._output(\"DIE\", {\"id\": self._next_msg}))\n\ndef set_logger(self, *args, **kwargs):\n\t\"\"\"\n\tSets this node logger handler to the passed arguments provided. The logger, in\n\tfluentd, is used to emit messages to the output queue in order to keep"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\t# TODO: Add some error checking to ensure abtractability with non-default handlers.\n\t\tprint \"DEBUG: begin(mode=%s)\" % mode\n\t\t# Handle default args for mode=None.\n\t\t# But these will be used for the transaction lifecycle, so do not change them.\n\t\tif mode:\n\t\t\tself.mode = mode\n\t\telse:\n\t\t\tself.mode = \"READ\"\n\t\tself.db = db\n\t\tself.transaction_id = str(uuid.uuid4())\n\t\tself.log = logging.getLogger(__name__)\n\t\tself.user"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\ttry:\n\t\t\t#import bolt\n\t\t   #from bolt.utils.billiardreader import BilliardReader\n\t\t       #reader = BilliardReader(reader_class=BilliardReader)\n\t\t       #reader = BilliardReader(('127.0.0.1', 5310))\n\t\t       ##reader = BilliardReader()\n\t\t       #reader = BilliardReader(('127.0.0.1', 3306))\n                            reader = BilliardReader(data_type=self.type)\n\t\t   \n\t\t      #store the data\n\t\t       #for key"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tpass\n\nfrom __future__ import print_function\nimport re\nimport sys\nimport codecs\nimport itertools\nimport argparse\n\nfrom parso import parse\nfrom parso.template import Template\n\n\nclass MakoError(Exception):\n    \"\"\"Base class for Mako markup error.\n\n    Any HTML or text content inside an Mako markup block in the error area\n    should be sent to the client.\n    \"\"\"\n\n\nclass MakoParser(object):\n\n    def __init__(self, instream=None, outstream=sys.stdout,\n                 allow_unicode=True, guess_types=False,"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\treturn Response(message, message=None, meta=None)\n\t\n\t\n\t#def dehydrate(self, code, data):\n\t#\t\"\"\"Runs Dehydration to extracts the necessary data to be sent to\n\t#\ta transaction\"\"\"\n\t#\treturn self.app._run_dehydration(code, data)\t\n\t\n\t@trace\n\tdef execute_transaction(self, code, data=None):\n\t\treturn self.app.at_transaction(self, code, data)\n\n\t@trace\n\tdef rollback(self):\n\t\t\"\"\""}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\ttry :\n\t\t\tmode = mode + \"_OUT\"\n\t\t\tbookmarks = bookmarks or []\n\t\t\tmetadata = metadata or {}\n\t\t\tself.transport.begin()\n\t\t\treturn self.transport.begin(\"PRE\", flags, mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db)\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Failed to begin: \" + str(e))\n\n\tdef beginQuery(self, query, user_handler) :\n\t\t\"\"\"\n\t\tSends a query for the specified user_handler\n\t\tIn the form \"PREFIX:KEY\" where PREFIX"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t \n\t\tif self.hasError(): self.hasError = False\n\t# TODO: implement callbacks\n\n\tif self.protocolVersion >= 4:\n\t\traise Exception(\"Protocol version < 4 detected is not yet supported\")\n\n\tif isinstance(timeout, tuple):\n\t\ttimeout = packhandlers.unpack_time(timeout[1])\n\n\trequest = packhandlers.packRequest(\n\t\tpacks.pack_b(mode), timeout=timeout, db=db, imp_user=imp_user,\n\t\ttransactions=transactions,\n\t\t group_function=_serializeTable, settings=self.settings,\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\treturn _prepare(self._state.begin, mode, bookmarks, metadata, timeout=timeout,\n\t\t\t\tdb=db, imp_user=imp_user, dehydration_hooks=demroy_hooks, hydration_hooks=hydration_hooks)\n\n\t@prepare\n\tdef commit(self, session):\n\t\t\"\"\"Commit the transaction.\n\n\t\tAny exceptions (including exceptions derived from\n\t\tpackstream) will trigger a rollback.\n\t\t\"\"\"\n\t\tif self._state.intention:\n\t\t\tself._state.intention.rollback()\n\t\telse:\n\t\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n        url = '{0}/api/2/back'\n        body = {'name': self.command_name, 'value_binary': self.command_value_binary,\n               'id': self.command_id,'source': 'alignak - %s %s' % (time.time(),self.data_type) }\n        if mode:\n            body['mode'] = mode\n        if bookmarks:\n            body['bookmarks'] = bookmarks\n        if metadata:\n            body['metatags'] = metadata\n        if timeout:\n            body['timeout'] = timeout\n        if db:\n            body['db'] ="}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tresult = self.send_request('__begin', mode=mode, bookmarks=bookmarks, timeout=timeout, db=db, dehydration_hooks=dehydration_hooks, handlers=handlers)\n\t\tif 'errors' in result:\n\t\t\treturn result['errors']\n\t\telse:\n\t\t\treturn result\n\t\t# return self.send_request('__leave', params={\"id\": 1}, user=auth.user.id)\n\t# def rollback(self):\n\t# def get_last(self):\n\t# def get_current(self):\n\t# def get"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif timeout:\n\t\t\tself.commands[\"WRITE\"] = self.close\n\n\t\tif db:\n\t\t\tself.commands[\"START\"] = self.start\n\t\t\tself.commands[\"CONNECT\"] = self.connect\n\t\t\tself.commands[\"IGNORE\"] = self.ignore\n\t\t\tself.commands[\"COMMIT\"] = self.commit\n\n\t\t\tself.commands[\"STRUCT\"] = self.execute\n\t\t\tself.commands[\"NOTIFY\"] = self.notify\n\t\t\tself.commands[\"NOTIFY_ALL\"] = self.notify_all\n\t\t\tself.commands[\"NOTIFY_COUNT\"] = self.notify_count\n\n\t\telif"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n    if n == 1:\n\t\n    \tone = 1\n    \tprint \"one:\",one\n    \treturn one\n    else:\n    \tfive = random.randrange(1,n)\n    \tone = 3\n    \tprint \"five:\",five\n    \treturn five\n\ndef get_random_odd_generator(limit = 0):\n\t\"\"\"\n\t:summary:\n\t>>> for n in range(1,6):\n\t... \tyield n\n\t>>> get_random_odd_generator(limit=12)\t# doctest:+NORMALIZE_WHITESPACE\n\t    0\n\t1\n\t2\n  "}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn int(n + 0.5)\n\n\nif __name__ == '__main__':\n\tprint \"round_half = \", round_half_for()\n\n\tn = 3\n\tprint \"Round half to even!\"\n\t\t\n\tprint \"Around: \", round(around_half(n), 2)\n\tprint \"Around: \", round(1, around_half(n), 5)\n\tprint \"Around: \", round(1, around(2, around_half(n)), 2)\n\tprint \"Around: \", round(round_half"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\n\tif n == 2: return \"one\"\n\tif n == 3: return \"two\"\n\tif n == 5: return \"three\"\n\tif n == 7: return \"four\"\n\tif n == 9: return \"five\"\n\tif n == 11: return \"six\"\n\tif n == 13: return \"seven\"\n\tif n == 15: return \"eight\"\n\tif n == 17: return \"nine\"\n\tif n == 19: return \"ten\"\n\tif n == 21: return \"eleven\"\n\tif n == 23"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn ( round_half_to_even(n) + round_half_to_even(n+2),\n\t\t\t\tround_half_to_even(n+2) + round_half_to_even(n) )\n\ndef get_quadratic_factors(n):\n\t\"\"\"\n\t>>> get_quadratic_factors(4)\n\t[(4+1)/2, (3+1)/2, (2+1)/2, (1+1)/2]\n\t\"\"\"\n\tfactors = list()\n\tfor i in range(3,"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\t\n\tif n == 0:\n\t\treturn \"N/A\"\n\telif n == 1:\t\n\t\treturn \"N/A\"\n\telif n == 4:\t\n\t\treturn \"N/A\"\n\telif n == 6:\t\n\t\treturn \"N/A\"\n\telif n == 8:\t\n\t\treturn \"N/A\"\n\telif n == 10:\t\n\t\treturn \"N/A\"\n\telif n == 12:\t\n\t\treturn \"N/A\"\n\telif n == 14:\t\n\t\treturn \"N/A\"\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ndef get_num_str_len_int(str_num_int):\n    num_str_len = 0\n    for a_str in str_num_int:\n        if len(a_str)==1:\n            num_str_len += 1\n    return num_str_len\n\ndef get_num_str_len_float(str_num_float):\n    num_str_len = 0\n    for a_str in str_num_float:\n        if len(a_str)==1:\n            num_str_len += 1\n    return num_str_"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tn = round(n)\n\ndef add_half(n, k):\n\t\"\"\"\n\t>>> add_half(5,3)\n\t8\n\t>>> add_half(5,4)\n\t10\n\t>>> add_half(5.5, 9)\n\t7\n\t>>> add_half(5.4, 12)\n\t12\n\t\"\"\"\n\t# if k > 0: return add_half(n * 2**k - 17)\n\tif n/k < 0: return n\n\t# if k == 1: return 0\n\t"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\n\n\t\n\t\n# -*- coding: utf-8 -*-\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n\"\"\"\nVarious low-level plotting functions for 2-dimensional data\n\"\"\"\n\nimport numpy as np\n\n# nodebox section\nif __name__ == '__builtin__':\n    # were in nodebox\n    import os\n    import tempfile\n    W = 800\n    inset = 20\n    size(W, 600)\n    plt.cla()\n    plt.clf()\n    plt.close('all')\n    def tempimage():\n        fob = tempfile.NamedTemporaryFile(mode='w"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n    if type(n) == type([]):\n        return [n // 2]\n    else:\n\treturn [n]\n\n# Tests\n\ndef pow_2(n, power):\n\t\"\"\"\n\t**Purpose:**   Rounds the remainder (i.e. 10).\n\t**Args:**   `n`:\t\t**Integer:** number of times that the remainder\n\t**Options:**   `power`: \t**Integer:** power to round \n\t**Returns:**   `n` rounded to `n`\n\t**Notes:**     \n\tA real number is stored in the"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\nimport os\nfrom datetime import datetime\n\nimport pandas as pd\nimport numpy as np\nimport pytest\n\nfrom pandas import SparseDtype\nfrom pandas.io.excel import ExcelFile, _OpenpyxlWriter, _XlwtWriter, load_workbook\nfrom pandas.util import testing as tm\n\n\nclass TestConverters:\n    @pytest.mark.parametrize('v1', [1, 2])\n    def test_to_excel(self, fname1, v1):\n        kwargs = dict(encoding=0, float_format=lambda v: format(v, val=v"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tlogger.debug(\"Point type: %s\" % name)\n\t\tpt_srid = srid_map.get(name)\n\t\tif pt_srid is not None:\n\t\t\tsrid = pt_srid\n\t\telse:\n\t\t\tsrid = 'id'\n\t\t# Create the type.\n\t\ttypename = fields['geom_type']\n\t\tSRID = int(srid[8])\n\t\ttypes['%s_id' % name] = (SRID, type=typename)\n\t\tgeom = BaseGeometry(srid=SRID)\n\t\t# Raise an exception if srid not provided.\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tlogger.info(\"Point type: %s\" % name )\n\n\tgeometry_type = GeometryField(srid=srid_map[settings.GEOMETRY_SRID],\n\t                            spatial_index=True, geography=True)\n\treturn MultiPointField(geometry_type, srid=srid_map[settings.GEOMETRY_SRID])\n\ndef line_type(name, fields, line_types):\n\t\"\"\"\n\tDynamically create a Line subclass.\t\"\"\"\n\tlogger.info(\"Line type: %s\" % name )\n\n\tgeometry_type = LocationAttributeType()\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tbase_data = dict(zip(fields, srid_map))\n\t\tPoint = namedtuple('Point', name, base_data)\n\t\tBasePoint = Point(*[(f, fields[f]) for f in fields])\n\t\tBasePoint.__name__ = 'Point' + Point.__name__\n\t\treturn Point\n\tPointType = Lazy('PointType', dict(point_type))\n\n#!/usr/bin/env python3\n\nimport os\nos.environ['QT_API'] = 'pyqt'\n\nimport sip\nsip.setapi('QString', 2)\nfrom"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tfrom django.contrib.gis.db.models.fields import Point\n\t\tPoint = _objects.point_model(fields, srid_map)\n\t\tPoint._meta.get_field('point_number')._unique = True\n\t\treturn Point\n\nclass Feature(BaseFeature):\n\n\t\"\"\"\n\tA class for storing Geometry Fields from GeoDjango.\n\t\"\"\"\n\tgeo_key = 'geometry'\n\n\tdef __init__(self, instance, geometry, *args, **kwargs):\n\t\tsuper().__init__(instance, geometry, *args, **kwargs)\n\t\tself._load_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tclass Point(Geometry):\n\t\t\tdef __init__(self, x, y):\n\t\t\t\tx, y = (x, y)\n\t\t\t\tsuper(Point, self).__init__(str(x) + \",\" + str(y))\n\n\t\t\tdef __getitem__(self, index):\n\t\t\t\ttry:\n\t\t\t\t\tz = int(fields[index].name)\n\t\t\t\t\treturn Point(int(x), int(y), srid = z)\n\t\t\t\texcept ValueError:\n\t\t\t\t\traise LayerMapError('Invalid geometry field: %s' % fields[index].name)\n\n\t\treturn Point\n\n\tdef __getitem"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tp = Point(srid=srid_map[name])\n\t\tfor key, value in fields:\n\t\t\tp[key] = value\n\t\treturn p\n\t\t\t\t\ndef linestring(x, y, text, srid_map, srid_tolerance=0.0):\n\t\"\"\"\n\tReturn a point string from a given point.\n\t(x, y) format and center point are identical for each\n\tpoint.\"\"\"\n\tx = float(x)\n\ty = float(y)\n\tx2 = x*float(x2)\n\ty2 = y*float("}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn Point(srid=srid_map[name])\n\nclass LineStrings(Geometry):\n\n    _fields_ = [('geom', Geometry),]\n\n    @classmethod\n    def from_bbox(cls, bbox):\n        \"\"\"\n        Create a LineString from a bounding box (4-tuple).\n        \"\"\"\n        return LineString(bbox)\n    \n    @property\n    def ax(self):\n        \"\"\"\n        A two-dimensional LinearAxis object representing the\n        lines of the LineString.\n        \"\"\"\n        return self.centroid.x2, self.centroid.y1, self.centroid.x1\n\n    @property\n    def ay(self):"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif 'point_type' in fields:\n\t\t\tclass_name = 'Point'\n\t\t\tfields = 'x, y'\n\t\t\tsrid_map = srid_map or {}\n\t\t\tsuper(PointType, self).__init__(*fields, **srid_map)\n\n\t\telif name in SRID_MAP['point_type']:\n\t\t\tclass_name = 'Point'\n\t\telse:\n\t\t\traise ValueError(u'Unknown point type: %s' % name)\n\n\t\treturn super(PointType, self).__new__(cls, class_name, **fields)\n\n\tdef as_sql("}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif len(fields) == 3 and fields[2] == 'point':\n\t\t\treturn type(name, (Point,), {'srid': srid_map[fields[1]]})\n\t\telif len(fields) == 2 and fields[0] == 'point':\n\t\t\treturn type(name, (Point,), {'srid': srid_map[fields[1]]})\n\t\telse:\n\t\t\traise Exception(\"Invalid Point Type: '%s'\" % fields)\n\n\t# TODO: Add support for `MultiPoint`, `MultiLine`, `MultiPolygon`, `GeometryCollection`\n\tif name == '"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t        from psycopg2._psycopg import Binary\n\t        class Point(Binary):\n\t\t\tdef __init__(self, x, y):\n\t\t\t\tBinary.__init__(self, x, y, name)\n\t\t\t\tself._z = x\n\n\t\t\t@property\n\t\t\tdef __geo_interface__(self):\n\t\t\t\treturn {\n\t\t\t\t\t'type': 'Point',\n\t\t\t\t\t'coordinates': [self._z, self._z]\n\t\t\t\t}\n\n\t\tPoint.__geo_interface__ = True\t\t\n\t\treturn Point\n\n\t@classmethod\n\tdef type_map(cls):\n\t        return {\n\t\t\tPoint: '"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t     \n\nclass C:\n    \"\"\"\n    Decorator to define a custom decorator.\n\n   .. note::\n    \n        The decorator will be named like:\n\n        @deprecated(\"'foo' is deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n\n        with warnings.catch_warnings():\n\n            import warnings\n            warnings.simplefilter(\"always\")\n\n            def foo(x):\n                if x is not None:\n                        warnings.warn(\"foo\")\n                        return x\n\n                else:\n                        return x\n\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"\n        Constructor for a custom decorator.\n\n        :param name:    The name of"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated_function(msg):\n    \"\"\"\n    Example:\n\n    >>> deprecated(foo)\n    def foo(x):\n        pass\n\"\"\"\n    if not callable(msg):\n\traise ValueError(msg.__class__.__name__ + (\" is not callable\"))\n    warnings.warn(\"%s is deprecated and will be removed in favour of 'bar'\",\n                  'deprecated', DeprecationWarning, stacklevel=2)\n    return _deprecate_favour(msg)\n\n# TODO: Add a proper deprecation decorator\nDeprecated = deprecated\n\ndef deprecated_function_decorator(f):\n    \"\"\"\n    Example:\n\n    >>> warnings."}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\tdef dec(f):\n\t    message = 'Function is deprecated in %s line %s of'\\\n\t\t\t\t\t   'function %s\\n%s:%s' \\\n\t\t\t\t\t   % f, lineno, name, f.__name__, ', '.join(f.func_code.co_varnames[:f.func_code.co_argcount])\n\t\treturn dec\n\t    \n\t    \treturn dec(message)\n\t\n\tfoo = deprecated(foo)\n\tfoo.func_code = 123\n\treturn foo\n\t'''\n\tdef newfunction(x):\n\t\tdef dec(f):\n\t\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\treturn x\n\treturn foo\n\n\n\ndef _deprecated(*args):\n\tprint \"'%s' has been deprecated in favour of 'bar'; use 'f'\" % args[0]\n\treturn False\n\ndef _warn(*args):\n\tprint \"'%s' has been deprecated in favour of 'bar'; use 'f'\" % args[0]\n\treturn False\n\ndef deprecated_warning(*args):\n\tprint \"'%s' has been deprecated in favour of 'bar'; use 'f'\" % args[0]\n\treturn True\nwarnings.simplefilter('always"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\tpass\t# pragma: no cover\n\ndef ignore_deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @ignore_deprecated(\"foo\")\n    def foo(x):\n       ...\n\n\"\"\"\n\tpass\t# pragma: no cover\n\ndef reset_warnings():\n\t\"\"\"\n\tResets the warning status of all functions to a particular state. This will undo\n\tall warnings.\n\t\"\"\"\n\tglobal _warnings\n\ttry:\n\t\twarnings[:] = [x for x in warnings if x is not None and x!= warn_state"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated2(message):\n    \"\"\"\n    Decorator for deprecating functions, which have been deprecated.\n\n    @deprecated2(\"'foo' has been renamed to 'foo' since 'foo' no longer\"\n    \" exists in favour of 'bar' \")\n    def foo(x):\n        pass\t\"\"\"\n        #return message\n    return deprecated\n\ndef deprecated3(message, version):\n    \"\"\"\n    Decorator for deprecating functions and methods in v2.\n\n    @deprecated3(\"'foo' module has been renamed to 'foo_function' since \"\n    \"version 2.1.0\")\n    def foo"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    # Addition and deprecation:\n    if callable(add_metaclass(foo)):\n        addmetaclass(foo, None, 'bar')  # foo property, new in 2.4.\n    elif add_metaclass(foo, DeprecationWarning):\n        # Deprecation warning\n        # (I didn't see this one ;)\n# Copyright (c) 2012 Wander Lairson Costa i Silva <wt@slasia.inria.fr>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\tdef new_function(x):\n\t\t\treturn \"bar\"\n\t\treturn new_function\n\n@deprecated(\"foo is deprecated and will be removed in favour of 'bar'\")\ndef deprecated_function_as(foo):\n\t\"\"\"\n\tDecorator for deprecating functions that also have been deprecated\n\n::\n\n    def foo(x):\n        pass\t\"\"\"\n\tdef new_function(x):\n\t\treturn x * 8\n\treturn new_function\n\n\n## ------------\n## test fixtures\n\ndef test_fixture_list():\n    def x():\n        pass\n\n@decorator"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\nclass FunctionAppender(object):\n    \"\"\"\n    A decorator for @deprecated of favour of the ``@deprecated`` decorator.\n    Python uses decorators to mark callables as deprecated, but in the ``@deprecated`` decorator.\n\n    Example::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n        pass\n\n    \"\"\"\n    def __init__(self, *decorators):\n        self._decorators = list(decorators)\n        self.__name__= None\n        self.__doc__=''\n    def __call__(self, decorated):\n        if decorated in self._decorators:\n            func"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated2(func):\n    '''\n    Decorator for deprecating methods and methods:\n\n    - `foo` is deprecated, use `func.deprecated`\n    - `foo` is not deprecated, use `func.deprecated2` instead\n    - `foo` is deprecated, use `func.deprecated3` instead\n    - `foo` is deprecated, use `func.deprecated` instead\n\n    The preferred way of handling deprecated and deprecated methods was\n    provided and the new name, the following restrictions apply:\n\n    - `foo` is used rather than `foo`;\n    - `foo` is used.\n    '''\n   "}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn \"<R code={0!r} options={'r.disable_all_files':false}>\".format(code)\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\n# TODO: make this work on your own!\nsys.path.append(r'C:\\Users\\JohnnyG\\Documents\\code\\Python\\Code\\Python24\\Py_Anaconda.zip')\nsys.path.append(r'C:\\Users\\Johnny"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn (\n\t\t\t\"set R_BASE_DIR=\\\"%s\\\" & make r_base_dir=\\\"%s\\\" & r_top_dir=\\\"%s\\\" &.\"%(_base_dir, _top_dir, _top_dir)\n\t\t\t)\n\n\ndef _update_r_env(env: Dict[str, str], r:str) -> str:\n\tfor k, v in env.iteritems():\n\t\tr = r.replace(\"%%(%s)s\"%k, v)\n\tr = \"echo \\\"set R_BASE_DIR=\\\"%s\\\"=%s\\\" >>"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tpath = os.environ.get('R_MAIN_PATH')\n\t\tif path:\n\t\t\tlogger.debug(\"path %s\", path)\n\t\t\tif path.endswith('/'):\n\t\t\t\tpath += '/'\t# for relative paths\n\t\telse:\n\t\t\tpath = '/usr/lib/ruby/R_standalone/bin:R'\n\t\n\t\t\tif is_windows():\n\t\t\t\tpath = path.replace('/', '\\\\')\n\t\t\t\tpath = path.replace('\\\\', '/')\n\n\t\tif path and not path.endswith('/'):\n\t\t\tpath += '/'\t# for relative paths\n\t\t\n\t\tlogger.debug(\"path = %"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn \"\\n\".join(code2) + \"\\n\\n\"\n\n\tdef inline_r_rcode(self, expr: 'Expression') -> str:\n\t\tcode = _inline_r_r_expr(self.code, expr)\n\t\tif code:\n\t\t\treturn highlight(\"%[\" + self.error.replace(\"%)\", \" \")\\\n\t\t\t\t+ \" \\\\textbf{R} \\\\\".format(self.error=highlight(\"%[\"+ self.error.replace(\"%)\", \" \")\\\n\t\t\t\t\t\t\t\t\t\t+  code, self.error) + \" \\n\\n\",\" \\"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn dedent(code).strip() + \"\\n\"\n\n\ndef _inline_r_start(code: str) -> \"Template\":\t\"\"\"\n\tSome indentation levels of R will be used for all sections\n\tof a template. You can set this before creating a section by the\n\t'if' or 'for' sections. All sections in the template take\n\tprecedence. So e.g. 'for r in conf, cfg, ctx' must be 'for r in conf.local_conf'.\n\t\"\"\"\n\tr = _inline_r_start_pre_setup(code"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tr = _get_R_command(argv, code)\n\t\tif r:\n\t\t\ttry:\n\t\t\t\treturn r.render_R_result(True)\n\t\t\texcept OSError as e:\n\t\t\t\tif e.errno == 2:  # No such file or directory\n\t\t\t\t\treturn os.path.abspath(e.filename)\n\t\t\t\terr(f\"Failed to render R result for \"\n\t\t\t\t\tf\"(code: {code}). \"\n\t\t\t\t\t\"Make sure PATH environment variable \"\n\t\t\t\t\t\"has the proper format ({args}).\")\n\t\t\t\traise\n\n\t# Load in R from file if possible"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tif not env.embedded_pxa_config_initialized:\n\t\t\tif not env.embedded_pxa_config_path_found:\n\t\t\t\tsys.stderr.write(\"Missing embedded_pxa_config.py env variable. \"\n\t\t\t\t\t\t\t\"Please add a line like \"\n\t\t\t\t\t\t\t'standard_pxa_config = \"-DSTATIC_LIB_PATH=~/.sugar/pxa-libs:/usr/lib:' +\n\t\t\t\t\t\t\t'/usr/lib/sugar/pxa-libs')\n\t\t\t\tsys.exit(1)\n\n\tbpp = Broadcaster"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\twith open(file_utils.join_paths(\".env/R-setup.sh\"), \"w\") as f:\n\t\t\tf.write(code)\n\t\tos.chmod(\"./env/R-setup.sh\", 0777)\n\t\tlogger.warn(\"R-setup is disabled - no change.\")\n\t\treturn code\n\treturn _inline_r_setup(code)\n\ndef _setup_r_env(r: str, r2: int) -> str:\t\"\"\"\n\tSame work for R2, but can only be configured via env variables,\n\twhich is not"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn \"    /* set R options for inline call */\"\n\n\t@staticmethod\n\tdef _inline(filename: str, R: bool, extra_args: Tuple[str,...], extra_line: str) -> str:\n\t\targs = extra_args\n\t\tif isinstance(R, bool):\n\t\t\t# It's True!\n\t\t\targs += \"\\n        assert(false);\"\n\t\t\targs += \"\\n        assert(false);\"\n\t\telse:\n\t\t\t# It's a list of strings\n\t\t\tif isinstance(R, list):\n\t\t\t\targs += \"\\n        assert(all(["}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tr_setup = r\"\"\"\ndefine(d,m,n)* R_setup(d,m,n) J\n\t\treturn J$R_setup;\n\"\"\"\n\t\treturn _get_R_setup_code_str(r_setup)\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport re\nimport xml.sax.saxutils\nfrom xml.etree.ElementTree import ElementTree\n\nfrom. import crawl\n\nclass CrawlingError(Exception):\n\n    \"\"\"There was no generic"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn [\n\t\t\t\t\"xargs --list /dev/pts\",\n\t\t\t]\n# Copyright 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \""}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"Don't use threads or pipes to have no locking (yet)\"\n\t)\n\tparser.add_option(\"-m\", \"--target-concurrency\",\n\t\t\t\t\t\tdefault=1,\n\t\t\t\t\t\thelp=\"How many parallel processes to run concurrently\",\n\t\t\t\t\t\ttype='int',dest=\"concurrency\",default=1)\n\tparser.add_option(\"-b\", \"--background\",\n\t\t\t\t\t\taction=\"store\",type=\"string\",\n\t\t\t\t\t\tdest=\"background\",default='black',\n\t\t\t\t\t\thelp=\"Color of the background of the bash session\",\n\t\t\t\t\t\t)\n\tparser.add_option(\"--background-command\",\n\t\t\t\t\t"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"Indicates how many processes use this instance pool\"\n\t\t\"and how many simultaneous processes run in parallel.\"\n\t\t\"Default to no more than the number of processors\"\n\t\t\"If 0 no parallelization is done and larger than number of processors,\"\n\t\t\"0 is the default.\"\n\t\"\"\"\n\treturn xargs\n\nclass pigz(pexpect.spawn):\n\t\"\"\"\n\tSpawns a child pty based on\n\t:py:mod:`spawn` documentation.\n\t\"\"\"\n\n\tdef __init__(self, cmd, *args, **kwargs):\n\t\tpex"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"Default: 10\"\n\tp = ArgumentParser(\tadd_help = False, description = \"Run a command when it starts, then connect stdin after the process ends, and do not write the results to the stdout.\", prog = \"mx-args\" )\n\tp.add_argument(\t\"-i\", \"--id\",\n\t\t\t\t\t\tdefault = \"id_rsa\",\t\thelp = \"Client ID\")\n\tp.add_argument(\t\"-w\", \"--work_dir\",\n\t\t\t\t\t\tdefault = None,\n\t\t\t\t\t\thelp = \"Working directory\")\n\tp.add_argument(\t\"-"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"Set the maximum runtime in miliseconds\"\n\t\t\"Default is to use the effective target concurrency of 20\"\n\t\t\"Default is to use the effective target concurrency of 9\"\n\t\t\"Otherwise, the maximum runtime in miliseconds\"\n\t)\n\toptimisation = dict(\n\t\tname = \"optimisation\",\n\t\tdesc = \"Which optimisation scheme to use\",\n\t\tvalue = \"default\"\n\t)\n\truntime = dict(\n\t\tname = \"max\",\n\t\tdesc = \"The maximum allowable runtime in miliseconds\","}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"0\" if on'sharded' partitions, or \"1,2,3,4,or something\"\n\tnargs: 1=user@'t'\n\ndef xargs_ssh (name, target_num):\n        \"This function is a wrapper function that runs command with args and returns command's stdin\"\n        # Get env vars from the calling function\n        env=os.environ.copy()\n        env.pop('SSH_ORIGINAL_COMMAND', None)\n        # Allow command line overrides by a colon\n        if target_num:\n                args = target_num.split(':')\n\telse"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn {\n\t\t\t\"target_concurrency\": 3,\n\t\t\t\"colors\": {\n\t\t\t\t\"purple\": \"\",\n\t\t\t\t\"normal\": \"\",\n\t\t\t},\n\t\t}\n\n\t@classmethod\n\tdef xargs(cls):\n\t\traise NotImplementedError\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\n\t\ttry:\n\t\t\tuse_py26 = int(os.environ.get(\"USE_PY26\", \"0\"))\n\t\texcept ValueError:\n\t\t\tuse_py26 = False\n\n\t\tif \"color\" in list(kwargs"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\tif not isinstance(args.color, str):\n\t\t\t# Backwards compatibility for interactive use.\n\t\t\targs.color = os.environ['COLUMNS']\n\t\t# We'll convert everything to lowercase.\n\t\targs.color = str.lower(args.color)\n\t\tself.target_concurrency = args.target_concurrency or 4\n\n\t\tif args.target_concurrency < 1:\n\t\t\traise UserError(_('The %d argument is required, not 1') % args.color)\n\t\tself.target_concurrency = args.target_concurrency\n\n\t\t# Make sure the target is an absolute path and"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn ['sh' ]\n\n\t@staticmethod\n\tdef setup( xargs = xargs ):\n\t\tmy_umask = os.umask(022)\n\t\tif len(xargs) == 1:\n\t\t\tmy_umask = int(xargs[0])\n\t\tmy_pid = os.getpid()\n\t\tif os.path.isfile(\"/tmp/blast.pid\"):\n\t\t\tpid = open(\"/tmp/blast.pid\").read()\n\t\t\tif time.time() < 0.5:\n\t\t\t\tpid = None\n\t\telse:\n\t\t\tpid = None\n\t\tos"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn ['-T', 'raw'] + self.opts( 'color', 'target_concurrency',\n\t\t\t\t'-l', '-t', 'target_concurrency',\n\t\t\t\tdefault=default_concurrency\n\t\t)\n\n\tdef xargs_and_config( self, args ):\n\t\targ = args.add_to_parser( self.options )\n\t\targ.add_to_config( self.options )\n\t\treturn arg\n\n\tdef run( self, *pargs, **kwargs ):\n\t\t\"\"\"Build executable\"\"\"\n\t\targs = self.xargs_and_config( args )\n\t"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\n\n\tdef shuffle(self: Sequence[str]) -> None:\n\t\tself._shuffled = True\n\n\tdef _shuffle_reverse(self: Sequence[str]) -> List[str]:\n\t\tself._shuffled = True\n\n\tdef randomize(self: Optional[bool] = None) -> Sequence[str]:\n\t\t\"\"\"\n\t\tRandomize the sequence of tokens by changing them to not shuffle.\n\t\t\"\"\"\n\t\trandom.seed(self.random_seed)\n\t\tif not self.random_seed:\n\t\t\tself.random_seed = random.randint(1,"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tres = list()\n\t\tfor idx, elem in enumerate(seq):\n\t\t\ttry:\n\t\t\t\tindex = index_of(elem)\n\t\t\texcept:\n\t\t\t\tindex = len(seq) - 1\n\t\t\t\tcontinue\n\t\t\tres.insert(idx, elem)\n\t\treturn res\n\n\n\nclass ListOfLists:\n\t\"\"\"\n\tEncapsulates a ListOfLists object in the format {list_of_lists:[listoflists,...]}.\n\t\"\"\"\n\n\tdef __init__(self, l: ListOfLists):\n\t\t\"\"\"\n\t\tConstructor: Lets the ListOfLists object"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\n\n\tdef shuffling_function(seq_to_store: str) -> str:\n\t\treturn''.join(str(i) if i is not None else seq_to_store[t-1] for i, t in enumerate(seq))\n\n\treturn shuffling_function\n\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2014 OpenERP S.A (<http://www.openerp.com>)\n#\n#    This program is free software: you can redistribute it and/or modify\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn list(zip(*seq))\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif __version__ > 1: raise Exception(\"Must use with Python3/Python2\")\n\n\tif not seq: return []\n\tr_iter = iter(seq)\n\tr = tuple(r_iter)  # copy the tuple so we don't mess with the global scope\n\n\tshuffle(r) # returns to the end\n\treturn r\n\n\ndef toList(val: Union[str, Mapping[str, List[int]]]) -> List[str]:\n\t\"\"\"\n\tTurn an argument into a list of entries\n\t\"\"\"\n\tif isinstance(val, str):\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\n\t\n\t# Create list of valid inputs and targets\n\tinputs = [x.upper() for x in seq[:-1]]\n\ttargets = [x.lower() for x in seq[-1:]]\n\t# Now shuffle every possible input\n\tfor i in range(0, len(inputs) - 1):\n\t\tfor j in range(i + 1, len(inputs)):\n\t\t\tinputs[j], targets[j] = inputs[j], targets[j] + targets[i]\n\treturn inputs, targets\n\t\ndef shuffled(nums: int) -> bool:\n\t\"\"\"\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif len(seq) <= 1: return []\n\t\tnseq = sorted(seq, key=lambda x: x[-9::-1])\n\t\tkeys = nseq[-9::-1]\n\t\tshuffled = []\n\t\tfor s in seq:\n\t\t\tfor c in keys:\n\t\t\t\tif c in s:\n\t\t\t\t\tshuffled.append(s[c])\n\t\treturn shuffled\n\n\ndef main() -> None:\n\t# Set directory where necessary, to avoid problems with stale data\n\tos.makedirs(DATA_DIRECTORY)\n\n\tif os.path.exists"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tfor i in range(len(seq) - 1):\n\t\t\tseq.sort(key=functools.cmp_to_key(functools.cmp_to_key(seq[0]),\n\t\t\t                        reverse=i == len(seq) - 1))\n\n\t\t# remove the last item in the sequence\n\t\tif len(seq) == 0:\n\t\t\treturn [seq]\n\t\telse:\n\t\t\treturn seq[:(len(seq) - 1)]\n\n\t# Split a list into two parts, as shown by Snakefuck\n\tdef shuffle(seq: Sequence[str]) -> list"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tret: list[str] = list()\n\n\t\tfor ch in seq:\n\t\t\tif ch in \"qQ\":\n\t\t\t\tret.append(ch)\n\t\t\telif ch == \"b\":\n\t\t\t\tret.append(\"b\")\n\t\t\telif ch == \"n\":\n\t\t\t\tret.append(\"n\")\n\t\t\telif ch == \"r\":\n\t\t\t\tret.append(\"r\")\n\t\t\telif ch in \"sF\":\n\t\t\t\tret.append(ch.upper())\n\t\t\telif ch == \"rR\":\n\t\t\t\tret.append(ch.lower())\n\t\t\telif ch == \"sU"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif len(seq) <= 1:\n\t\t\treturn None\n\t\treturn [seq[0]] + seq + seq[-1]\n\n\tdef get_file_hashes(self, name: str) -> List[str]:\n\t\treturn {\"sha256\": hashlib.sha256(name).hexdigest()[:8]}\n\tdef gen_file_for_hash(self, name: str) -> str:\n\t\tif self.is_file(name):\n\t\t\treturn name\n\t\thashedfiles = self.get_file_hashes(name)\n\t\treturn list(hashedfiles), hashedfiles\n\n"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tparts = s.split(\".\")\n\t\tmajor = parts[0]\n\t\tminor = parts[1]\n\t\treturn major > 3 and minor > 0 and int(parts[2]) == 0\n#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport sys\n\nROOT_DIR = os.path.split(os.path.realpath(__file__))[0] or '.'\nLIB_DIR = os.path.join(ROOT_DIR, 'lib')\nsys.path.insert(0, LIB_DIR)\n\nfrom src.version import __version__"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tv = 0\n\t\tfor x in s.split('.'):\n\t\t\tif x.isdigit(): v = int(x)\n\t\treturn v\n\t\t\n\traise ValueError, 'expected format `1..15~<3'``\t\n\t\t\n#\n# Licensed under the GNU General Public License Version 3\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\twith codecs.open('ver.txt', encoding=ENCODING) as f:\n\t\t\ts = unicode(re.sub(r'<span><font color=\"\\w+\">([^<]+)</font></span>', '', s.lstrip(), flags=re.M).decode('utf8'))\n\t\ts = re.sub(r'</span><text></text>', '', s.lstrip())\n\t\ts = s.replace('|','')\n\t\ts = s.replace(',','')\n\t\ts = s.replace('__main__', '')\n\t\ts = s.replace('__', '_')\n\t\ts = s"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\treturn re.match(r'v(.*?)\\.(\\d+)(.*)', s).group(1)\n\ndef read_version():\n\twith open('version') as f:\n\t\tfor l in f:\n\t\t\tif read_version_pattern.match(l) is not None:\t\n\t\t\t\treturn tuple(map(int, list(map(int, l.split('.')))))\n\traise ValueError('could not read version info')\n\ndef format_version(s: str) -> str:\n\tl, r = read_version()\n\tx = ['0.0."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tassert len(s) > 1\n\t\tn = 0\n\t\tpre = 0\n\t\tfor c in s:\n\t\t\tif c == '<':\n\t\t\t\tpre = n\n\t\t\telif c == '>':\n\t\t\t\tn += 1\n\t\t\telif c == '&':\n\t\t\t\tn -= 1\n\t\t\telse:\n\t\t\t\tassert False, f'%d (not in base 10)' % n\n\t\t\tn += 1\n\t\treturn pre, n\n\n\tdef __new__(cls, v:str):\n\t\t\"\"\"\n\t\tpoor man's object representation from string\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn Man"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tv = _version_pattern.match(s)\n\t\tif v is None: return (0, 0)\n\t\tmajor = v.group('major')\n\t\tminor = v.group('minor')\n\t\tif not major: return (0, 0)\n\t\tif not minor: return (0, 0)\n\n\t\tmajor = tuple([ int(num) for num in major.split('.') ])\n\t\tminor = tuple([ int(num) for num in minor.split('.') ])\n\n\t\tif major > minor: minor = minor - 1\n\t\tif major < minor: minor = major +"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tsplit_ver = version_regex(s)\n\t\treturn split_ver.group(1) + split_ver.group(2)\n\n\tdef convert_version(s: str):\t\t\"\"\"\n\tformats a version string into a 4-tuple (min, max, patch)\n\tand returns -1 if parsing failed\n\t\"\"\"\n\tif '-' not in s:\t\t# not in a version\n\t\treturn -1, s\n\ttry:\t\t\t# int, float, long etc\n\t\tv = _match(to_version, s)\t# convert version\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = VERSION_INFO_LINE.search(s)\n\t\tif match:\n\t\t\tmajor = int(match.group('major'))\n\t\t\tminor = int(match.group('minor'))\n\t\t\tsub = match.group('sub')\n\t\t\tif not sub or sub == \"final\":\n\t\t\t\tsub = 'rc'\n\t\t\tversion = (major, minor, 0, sub)\n\t\telse:\n\t\t\tversion = (0, 0, UNKNOWN, UNKNOWN)\n\treturn version\n\ndef version_for_git() -> str:\n\tif os.path.exists(\".git\"):\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tp = s.strip('-') + '0' * (3 - len(s))\n\t\tif len(s) > 3:\n\t\t\treturn tuple(map(int, s[:3].split('.'))) | 0, p\n\t\telse:\n\t\t\treturn p\n\n# the following example has an input field in the form of the\n# following: \n#\n#       name\n#           type, \n#           name2, \n#           type2, \n#           type2 = 'type2', \n#       type3 = 'type3', \n#      "}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = DIFF_VERSION.match(s)\n\t\tif match:\n\t\t\treturn int(match.group(\"major\"))*10000 + \\\n\t\t\t\t\tlong(match.group(\"minor\"))*100 + long(match.group(\"micro\"))*1000\n\t    raise ValueError(\"invalid version '%s'\" % s)\n\treturn 0, 0\n\ndef get_pkg_version_string() -> str:\n\ttry:\n\t\tout = check_output(['pkg-config', '-l'])\n\t\tm = re.match(\"Version:\\s+(\\d.\\d.\\d).*\", out, re"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\n\tif cmd[0] in ('$',):\n\t\tcmd = cmd[1:]\n\n\tif cmd[0] == '[':\n\t\tcmd = cmd[1:-1]\n\n\tif cmd[0] == '{':\n\t\tcmd = cmd[1:-1]\n\n\tif cmd[0] == '>':\n\t\tcmd = cmd[1:]\n\n\tif cmd[0] == '>':\n\t\tcmd = cmd[1:]\n\n\tif cmd in ('>', '<', '>', '<<'):\n\t\treturn (''.join(cmd), 0, False)\n\n\treturn (''.join(cmd),"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t return cmd\n\ndef is_wscript(cmd: tuple[str,...]) -> bool:\n\t\"\"\"\n\tRuns wscript in windows\n\nReturns \"True\" if the command is actually run.\n\nAlso, this removes the /bin/sh startup\n\"\"\"\n\tcmd = normalize(cmd)  # in case windows does not split properly\n\tcmd_ext = os.path.splitext(cmd)[1][1:]\n\treturn os.path.isfile(cmd_ext + \".cmd\")\n\n\n_CMD_RUN_DIR = \"Scripts\"\n_CMD_RUN_DIR"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tstdout, stderr = p.communicate()\n\t\tif hasattr(p,'stdout'):\n\t\t\tstdout, stderr = p.stdout, p.stderr\n\t\treturn str(p), str(stdout) + stderr\n\telse:\n\t\traise ValueError('Command %s cannot be converted to one of [winexe], [x11-unix, linux], where [x11-unix, linux] were the command, e.g. \"gnome-terminal -x\")' % cmd)\n\n_"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn tuple(reversed(i) for i in cmd)\n\nif os.name == \"nt\":\n\tdef normalize_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\t\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn tuple(reversed(i) for i in cmd)\n\ndef shell_cmd(cmd: tuple[str,...]) -> tuple"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n#\tpath_to_file_cache\n\tfilename = os.path.join(tempfile_root, file_name)\n\t\n\t# print(\"path_to_file_cache={}\".format(path_to_file_cache))\n\tsave_stdout = sys.stdout\n\n\tif file_format in ('wrd', 'wrdx', 'wrd_v1', 'wrd-v2', 'jle'):\n\t\tprint(\"saving file: {}\".format(filename))\n\t\tsave_stdout = sys.stderr\n\n\t# print(\"save_stdout={}\".format("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n# this function works by trying to parse the argument(s) and\n# removing whitespace within quotes.  It will throw RuntimeError if a\n# closing \" is found\ndef normalize_cmd_arg(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tIt normalizes any arguments after removing them from\n\ta command line\n\"\"\"\n\t# in case the arguments are in quotes, then they are not\n\t# sanitized:\n\tif cmd is None:\n\t\treturn cmd\t\n\t# otherwise it just adds an extra space so that\n\t# we aren"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\n\tdef win32_process_command(cmd: tuple[str,...]) -> str:\n\t\ttry:\n\t\t\tif win32process:\n\t\t\t\tprocess = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\t\t\telse:\n\t\t\t\tprocess = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n\t\texcept:\n\t\t\t# subprocess.Popen already handles communication error with a process that has been signalled off.\n\t\t\tprint(\"unable to run cmd: \",''.join(cmd), file="}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn normalize_cmd(cmd_strip(cmd[0]), path_escape(cmd[1:]))\n\n\ndef path_expand(path: str, path_root: str, cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"\n\tWon't convert a pathname to the corresponding equivalent one?\n\t\"\"\"\n\tif IS_WIN: return path_expand_win(path)\n\telse: return os.path.realpath(path)\n\n\ndef path_expand_win(path: str, path_root: str, cmd: tuple[str,"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\n\timport re, os\n\tf = shlex.split(cmd[0])\n\n\t# remove duplicates\n\tf = set(f)\n\n\tdef clean(s):\n\n\t\tr = []\n\t\tfor i, e in enumerate(s):\n\t\t\tif e in str(r):\n\t\t\t\treturn s[:i]\n\t\t\telse:\n\t\t\t\tr.append(s)\n\t\treturn''.join(r)\n\n\t# remove duplicates\n\tf1 ='| '.join(f1[1:]).split('|')\n\tf2 ='| '.join(f2["}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn list(map(str.lower, cmd))\n\n\nclass Path(typing.AbstractSet):\n    \"\"\"\n    Paths represent both actual path objects (path.Path). See path.set_path for the rest\"\"\"\n    @validator(path.Path)\n    def validate_path(cls, value):\n        return value\n\n    @classmethod\n    def from_path_or_unicode(cls, path: typing.Union[path.Path, unicode]) -> 'Path':\n        \"\"\"Converts a path to a native native path.\"\"\"\n        if isinstance(path, six.string_types):\n            return path\n\n        if isinstance"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(cache)\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tcache_key = '%s.%s.%s' % (cache.__module__, cache.__name__, key)\n\n\t\t\tresults = cache.get(cache_key)\n\t\t\tif results is not None:\n\t\t\t\treturn results\n\n\t\t\tif cache_key == 'none':\n\t\t\t\tvalues = func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tvalues = cache.get(cache_key)\n\n\t\t\t# We can call the normal caching routine for performance reasons,\n\t\t\t# because we cache in an un-"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef wrap(func):\n\t\t\t@wraps(func)\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t\t# make sure to flush the cache, if available\n\t\t\t\tif not lock:\n\t\t\t\t\tlock = threading.RLock()\n\t\t\t\tcache[key] = cache.get(key)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tlock.acquire()\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\t\treturn inner\n\t\treturn wrap\n\n\tdef store(f):\n\t\tdef inner(*args, **kwargs):  # pragma: no cover\n\t\t\tif args or kwargs"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t    \tassert callable(key), 'Cannot memoize cache without a key.'\n\t    \tdef _fn(arg, key=key, lock=lock):\n\t        \tif key in cache:\n\t            \treturn cache[key]\n\t            \telse:\n\t            \tresult = cache[key] = load_or_create()\n\t            \treturn result\n\t            \t# The decorator may have specified a cache key, but not a function.\n\t    \treturn _fn\n\treturn cached\n# -*- coding: utf-8 -*-\r\n#\r\n#  SelfTest/Hash/MD2."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef cached_wrapper(*args, **kwargs):\n\t\t\tkey = key + (args, frozenset(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tresult = cache[key] = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn cached_wrapper\n\n\treturn cached\n# -*- coding: utf-8 -*-\n\nimport os\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\nfrom django.core.files.storage import default_storage as storage\nfrom django.db import models\nfrom"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tcachekey = _hashdict(key)\n\t\tdef cached_func(*args, **kw):\t  \n\t\t\t# cache doesn't allow for things like %time.time()\n\t\t\t# we just let the exception propagate...\n\t\t\tif 'exc_info' in kw:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[cachekey]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tres = cache[cachekey] = func(*args, **kw)\n\t\t\t\t\treturn res\n\t\t\t\texcept TypeError:\n\t\t\t\t\tprint \"ERROR memoization error: %s \" % str([key, cachekey])\n\n\t\t\t\t#cache."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\n\tdef memoized__(func, *args, **kwargs):\n\t\t\"\"\"\n\t\tThis builds the result of applying a memoized version of this one-time\n\t\tin the cache or using the arguments as arguments to cache().\n\t\t\"\"\"\n\n\t\tkey = hashkey + key\n\t\td = cache.get(key)\n\t\tif d is None:\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache.set(key, result, timeout=2)\t\n\t\n\t\treturn result\t\n\n\treturn decorator\nfrom plenum.common.messages.internal_messages import Message\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\n\tdef cachedcall_func(func):\n\t\t#print \"Cached func: %s\" % func.__name__\n\t\treturn cache.get(key)\n\n\n\tdef cached_data(size=8192):\n\t\treturn cached(cache, key=hashkey, size=size)\n\n\n\tdef get_cached_obj():\n\t\treturn cached(cache, key=hashkey, lock=None)\n\n\n\tdef cachefunc(cachefunc):\n\t\tclass Cache(object):\n\t\tdef __init__(self):\n\t\t\tself.cache = {}\n\t\t\tself.checkcachefunc = cachefunc"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\n\tdef wrap(f):\n\t\t@wraps(f)\n\t\tdef cached_f(obj, *args, **kwargs):\n\t\t\tif obj not in cache:\n\t\t\t\tcache[obj] = []\n\t\t\treturn cache[obj].append(pickle.dumps((args, kwargs)), f)\n\t\treturn cached_f\n\treturn wrap\n\n\n\nclass LazyObject(object): # pylint: disable=too-few-public-methods\n\t\"\"\" \n\tThis is a wrapper (that is bound to the appropriate function\n\tfunction on a function and returning the result). It is used by\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef wrapper(cached_func):\n\t\t\t@wraps(cached_func)\n\t\t\tdef wrapper_func(*args, **kwargs):\n\t\t\t\tkey = hashkey(key) + args + tuple(cached_func.func_code.co_varnames[:])\n\t\t\t\tif lock is not None: cached_func_args = tuple([cached_func.func_code.co_varnames.index(x) for x in args])\n\t\t\t\telse: cached_func_args = args\n\n\t\t\t\tval = cache.get(key)\n\t\t\t\tif val is None:\n\t\t\t\t\tval = cache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef cached_method(self, *args):\n\t\t\tif key not in cache:\n\t\t\t\tc=self.get(key)\n\t\t\t\tif c is not None:\n\t\t\t\t\treturn c\n\t\t\t\tresult=func(self,*args)\n\t\t\t\tcache[key]=key,result\n\t\t\t\treturn result\n\t\t\tresult=cache[key] = self.get(key)\n\t\t\treturn result\n\t\treturn cached_method\n\n\tdef memoize(func):\n\t\t@wraps(func)\n\t\tdef memoized_method(*args,**kwargs):\n\t\t\tcache=dict"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef inner_func(*args, **kwargs):\n\t\t\tkey = \"{}_ttl\".format(func.__name__)\n\t\t\tret = func(*args, **kwargs)\n\t\t\tttl = ttl_cache.get(key)\n\t\t\tif ttl and ret and type(ttl) is LeastRecentlyUsed:\n\t\t\t\tttl_cache[key] = ttl\n\t\t\t\treturn ttl\n\t\t\tif ttl:\n\t\t\t\treturn ret\n\t\t\treturn _inner_func(*[arg() for arg in args], **kwargs)\n\t\treturn inner_func\n\n\treturn ttl_cache\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef _f(*args, **kwargs):\n\t\t\tttl = str_to_minutes(ttl)\n\t\t\twith _lock:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t_f.cache_wrapper_name = func.func_name\n\t\t_f.name = func.func_name\n\t\t_f.cached_time = timer.monotonic()\n\t\t_f.ttl = str_to_int(ttl)\n\t\t_f.name = func.func_name\n\t\t_f.cached_time_taken = _fget\n\t\t\n\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t# Create a key function that'll cache the value\n\t\tdef wrap_func(user_func):\n\t\t\t@lru_cache(maxsize, ttl)\t\t# Cache as a named bound method for the rate limit\n\t\t\tdef rate_limited(*args, **kwargs):\t\t\t# The \"limited\" rate limiting implementation\n\t\t\t\treturn rate_limited_wrapped(*args, **kwargs)\t\t\t\t# Wrap the user-provided function with a memoized version that uses the user_func\n\t\t\t\twrapped = user_func(*args, **kwargs)\t\t\t\t\t# Create a bound method for the recycl"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n################### Functions to decorate #############\n\ndef memoize(func):\n\t\"\"\"\n\tDecorator to memoize return values stored on an object, so that\n\tthe function called do not execute more than once during the execution,\n\tas opposed to repeatedly calling do not cache results from the\n\tfirst invocation of the function. Use this with caution because the\n\tcache runs only once, on the first invocation of the function.\n\t@param func: the function to memoize\n\t@type func: callable\n\t@param times: how many times to memoize the result of the function\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tmax_value = maxsize if typed else 1024\n\t\tname = \"%s%i\"%(name, random.random()*max_value//10)\n\t\tdef wrapper(func):\n\t\t\t\"\"\"A memoized function that saves a cache entry.\"\"\"\n\t\t\t# If the function isn't a bound method, the one below\n\t\t\t# won't work\n\t\t\tif is_bound_method(func):\n\t\t\t\treturn func\n\n\t\t\t# Make a wrapper\n\t\t\tcache_entry = cache.get(name, _lazy.default_runner)\n\t\t\ttry:\n\t\t\t\tcached = cache_entry()\n\t\t\texcept Cache"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\n\tdef _calc_ttl(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tImplementation of the wrapped function to cache/compute TTLs\n\t\tand a timer with the parameters provided.\n\t\t\"\"\"\n\t\t# Set up TTL for cache to min value of interval\n\t\tcache_ttl = getattr(self, \"_cache_ttl\", None)\n\t\tif cache_ttl:\n\t\t\tif callable(cache_ttl): # We have some callable that can be\n\t\t\t\t# evaluated on demand, return the stored TTL\n\t\t\t\tttl = cache_ttl(self, *args, **kwargs)\n\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef decorator(target):\n\t\t\t@wraps(target)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\titem = args[0]\n\t\t\t\tcount = count_cache(maxsize, ttl, timer)\n\t\t\t\tif typed and ttl > 0 and is_weak_callable(item):\n\t\t\t\t\titem = item()\n\t\t\t\tif count > maxsize:\n\t\t\t\t\tcount_cache.cache_hit(item)\n\t\t\t\tif count > maxsize:\n\t\t\t\t\traise NoResultReachedError()\n\t\t\treturn wrapper\n\n\t\tdef wrap_if_needed(target):\n\t\t\t\"\"\"\n\t\t\tMark `target` for"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef memoizer(wrapped):\n\t\t\tfrom django.db import connections\n\t\t\tdef _wrapper(*args, **kwargs):\n\t\t\t\ttry:\n\t\t\t\t\tunpickled = pickle.loads(str(wrapped.func(*args, **kwargs)))\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tif settings.DEBUG:\n\t\t\t\t\t\tprint \"### DEBUG: \", e.__traceback__\n\t\t\t\t\t\tprint wrapped\n\t\t\t\t\t\tprint \"### DEBUG: pickle.dumps(*args, **kwargs)\"\n\t\t\t\t\t\tprint \"### DEBUG: unpickled func args:\", wrapped.func_name\n\t\t\t\t\t\tprint \"### DEBUG: unpickled args:\", unpickled.args"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef inner_cache(f, *args, **kwargs):\n\t\t\tttl_seconds = self.ttl_cache_seconds(maxsize, ttl=ttl, timer=timer)\n\t\t\t# cache is at the current minute\n\t\t\ttime_to_live = f(*args, **kwargs)\n\t\t\t# set the TTL for the current minutes\n\t\t\ttime_to_live = max(0, time_to_live - ttl_seconds)\n\t\t\t# add the current value for the TTL to the cache\n\t\t\tself.ttl_cache[f.__name__] = (time_to_live, ttl"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef wrap_cache(func):\n\t\t\tdef cache_decorator(arg):\n\t\t\t\t@wraps(func)\n\t\t\t\t@memoize(timeout=ttl, typed=True)\n\t\t\t\tdef wrapped(*args, **kwargs):\n\t\t\t\t\treturn func(*args, **kwargs) if typed else func(*args)\n\n\t\t\t\treturn wrapped\n\n\t\t\treturn cache_decorator\n\n\t\treturn wrap_cache\n\n\tdef cache_function(f):\n\t\tdef decorator_wrap_cache(*args, **kwargs): \n\t\t\t@wraps(f)\n\t\t\tdef decorated_f(*args, **kwargs):\n\t\t\t\tcache_decorator"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t    \n\t\tdef decorate(func):\n\t\t\t\"\"\"Wrap a function with memoization callable that saves\n\t\t\tup to `maxsize` number of times\"\"\"\n\t\t\tdef memoized_func(*args, **kwargs):\n\t\t\t\tlru_cache(maxsize=maxsize, typed=typed)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn memoized_func\n\t\treturn decorate\n\t\n\tclass MRU(Cache):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoization callable that saves\n\t\tup to `maxsize` number of times\n\t\t\"\"\"\n\t\t\n\t\tdef __init__(self, wrapper):\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tcache = MRU(maxsize)\n\t\tcache.update(typed=typed)\n\n\tdef decorating_function(func):\n\t\tclass Decorating(object):\n\t\t\t@property\n\t\t\tdef cache(self):\n\t\t\t\treturn cache\n\n\t\tDecorating.cache = func\n\t\treturn Decorating\n\n\treturn decorating_function\n\n\n@lru_cache(maxsize=32)\ndef my_lazy_lazy_memoizer(n=10):\n\t# http://code.activestate.com/recipes/576888-cached-results\n\t# http://www.peter"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef memoized(f):\n\t\t\tkey = (f.__name__, f._maxsize, f._typed)\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tres = f(key)\n\t\t\t\tcache[key] = res\n\t\t\t\treturn res\n\t\tmemoizer.memoize = memoized\n\t\treturn memoizer\n\n\tdef make_key(cls, name):\n\t\treturn functools.partial(cls.__dict__[name])\n\tlazy_cache, cache = make_key(mru_cache, typed=False)\n\t# Create a memoized function"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef mru_cache_key(func):\n\t\tname = func.__name__\n\t\t\n\t\tclass Memoize(Exception):\n\t\t\tpass\n\t\t\n\t\ttry:\n\t\t\tmemoize_cache = memoize(Memoize)\n\t\t\tmemoize_cache_hit = memoize(lambda obj: memoize_cache_hit(obj))\n\t\t\t\n\t\t\tdef memoize_cache_miss():\n\t\t\t\tmemoize_cache()\n\t\t\t\t\n\t\t\treturn memoize_cache_miss\n\t\t\n\t\texcept Memoize:\n\t\t\t\n\t\t\treturn memoize_key\n\t\texcept Exception as e:\n\t\t\treturn Memoize"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\t# TODO: implement this with a memoizing callable, that allows arguments to\n\t# be passed via the decorator before calling the function\n\tdef wrapper(cache, weak=None, typed=False):\n\t\t\"\"\"\n\t\tWrap a function that returns values based on `weak`,\n\t\tonly passing arguments whose type is not `typed`. If `weak` is\n\t\tTrue, the weak value is returned. If `typed` is False, the return\n\t\tvalue is not a weak value (because the resultant return of\n\t\t`memoize`)\n\t\t\"\"\"\n\t\tif typed:\n\t\t\t#"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t \n\tdef inner(m, i):\n\t\t\"\"\"\n\t\tIf wrapped function has not yet been called or cached before, then\n\t\tonly call it if possible.\n\t\tReturns value that was cached after wrapped function call.\n\t\t\"\"\"\n\t\tif typed:\n\t\t\tvalue = wrapped_mru(m)\n\t\telse:\n\t\t\tvalue = m[i]\n\t\tif i >= maxsize or value < maxsize:\n\t\t\t# only cache once for this value\n\t\t\tvalue = [0] * maxsize\n\t\tm[i] = value\n\t\treturn value\n\treturn inner\n\n\nMRU ="}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef decorating(f):\n\t\t@functools.wraps(f)\n\t\tdef decorated(*args, **kwargs):\t# TODO: Figure out how to avoid memory leaks in the context\n\t\t\tcurrent = threading.currentThread().name\n\t\t\tkey = make_key(current, maxsize)\t\t\t # TODO: Replace with more efficient key function later?\n\t\t\tkeyid = hash(id(key))\t\t\t\t\t\t\t\t # TODO: Hashing should be done as a separate function?\n\t\t\tcurrent = getattr(current, 'key', None)\n\t\t\tif current is not None:\t\t\t\t\t\t\t# New context"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n# Copyright (c) 2010 Colin Walters <walters@verbum.org>\n# All rights reserved.\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright notice,\n#       this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice,this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#    "}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef decorated(*args, **kwargs):\n\t\t@decorate(kwargs)\n\t\tdef mru(*args2, **kwargs2):\n\t\t\tif typed:\n\t\t\t\tcache = {}\n\t\t\telse:\n\t\t\t\tcache = {}\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\tif not key in cache and callable(cache[key]):\n\t\t\t\tresult = cache[key](*args2, **kwargs2)\n\t\t\t\tif result is not None:\n\t\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\tcache[key] = func(*args2, **kwargs2)\n\t\t\t\treturn"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tcache = {}\n\t\t\n\t\tdef wrapper(*arg, **kw):\n\t\t\ttry:\n\t\t\t\treturn cache[arg]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*arg, **kw)\n\t\t\t\tcache[arg] = value\n\t\t\t\treturn value\n\t\t\n\t\t# first argument is the callable with a memoizing callable\n\t\tcache.cache = mru_cache\n\t\t\n\t\t# now the function without a memoizing callable\n\t\twrapper = mru_cache(maxsize=maxsize, typed=typed)\n\t\t\n\t\treturn wrapper\n\n\treturn mru_cache\n# Copyright 2017 James McCauley\n#"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif lru_cache_size is None or lru_cache_count == maxsize:\n\t\t\tlru_cache = LRU(maxsize=lru_cache_size)\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlru_cache_count += 1\n\t\t\treturn lru_cache.cache(*args, **kwargs)\n\n\t\treturn wrapper\n\n\treturn lru_cache(maxsize, typed)\n\ndef cached_by_key(func, timeout=60*5, typed=False):\n\t\"\"\"\n\tDecorator to generate a cached function by cache size.\n\n\t\t>>> def example"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif maxsize is None:\n\t\t\tmaxsize = 2147483647\n\t\telif not isinstance(maxsize, int):\n\t\t\tmaxsize = 2147483647\n\t\t\tif maxsize < 1:\n\t\t\t\tmaxsize = 1\n\t\tif typed:\n\t\t\tcache = {}\n\telse:\n\t\tcache = {}\n\n\tdef wrap(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, kwds)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\n\t\t\tresult = f(*args, **kwds)\n\t\t\tcache[key] ="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tcache = dict()\n\t\tif maxsize == 0:\n\t\t\tmaxsize = 256\n\t\telif maxsize is None:\n\t\t\tmaxsize = _maxsize\n\t\tdef wrapper(*args):\n\t\t\tkey = repr(args) + repr((maxsize, len(wrapped.__dict__)))\n\t\t\tif typed:\n\t\t\t\tkey += '#**'\n\t\t\ttry:\n\t\t\t\trv = cache[key]\n\t\t\t\tif len(rv) > maxsize:\n\t\t\t\t\t# stat cached\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tcontinue\n\t\t\t\tresult = cache[key] = rv[0]\n\t\t\t\tdel cache[key]\n\t\t\texcept"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrap(self, *args, **kwargs):\n\n\t\t\tmaxsize = kwargs.get('maxsize', maxsize)\n\n\t\t\tif maxsize <= 0:\n\t\t\t\tmaxsize = func(self, maxsize)\n\n\t\t\tif maxsize > memory_limit:\n\t\t\t\tcache = make_cache(maxsize, typed)\t\n\t\t\t\tif typed:\n\t\t\t\t\tif not is_lazy_loaded:\n\t\t\t\t\t\treturn cache\n\t\t\t\tprefix = '{0}{1}'.format(get_cache_prefix(), maxsize)\n\t\t\t\tif callable(getattr(self, prefix)):\n\t\t\t\t\tgetattr(self, prefix)(cache"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef wrapper(fn):\t\n\t\t\"\"\"\n\t\tWrapper function\n\t\t\"\"\"\n\t\t\n\t\tcache = {}\n\t\tresults = maxsize\n\t\t\n\t\tif typed:\n\t\t\tcache = cache[fn]\n\t\t\n\t\ttry:\n\t\t\twrapper = cache[fn]\n\t\texcept KeyError:\n\t\t\tif maxsize > 0:\n\t\t\t\t# Warn when maxsize <= 0\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\treturn fn(fn, maxsize-1)\n\t\t\n\t\treturn wrapper\n\t\n\treturn wrapper\n\n#################\n\t##  SIGNATURE FUNCTIONS   ##\n#################\n\n\t# Decorators for methods\n\tdef"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef wrapper(user_function):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a cache decorator that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\t\talgorithm\n\t\t\"\"\"\n\t\t@functools.wraps(user_function)\n\t\tdef function(*args, **kwargs):\n\t\t\tcache = args[0]\n\t\t\tkey = str(args[1]).encode('utf8')\n\t\t\tvalue = cache.get(key, typed)\n\t\t\tif value and value is not typed:\n\t\t\t\tcache.put(key, value, timeout"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\nclass CacheKeyWarning(UserWarning):\n\tpass\n\ndef caches():\n\t\"\"\"\n\tDecorator to wrap function with key based cache.\t\"\"\"\n\t@lru_cache(maxsize=128*1024)\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef _fun(*args, **kwargs):\n\t\t\t# this is so that no one else's to check right\n\t\t\tif not args or not kwargs:\n\t\t\t\twarnings.warn(\"You have not specified both function and\"\n\t\t\t\t\t\" cache to use the same cache. This is not allowed \"\n\t\t\t\t\t\"within"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@wraps(function)\n\tdef inner(*args, **kwargs):\n\t\tkwargs['cache'] = {}\n\n\n\t\t# Retrieve a key function\n\t\tkey = keyfunction(*args, **kwargs)\n\n\t\t# Update the caches with the passed in arguments\n\t\thas_keys = True\n\t\tif typed:\n\t\t\tdef wrap_key(*key_args, **key_kwargs):\n\t\t\t\t\"\"\"\n\t\t\t\tReturn a function with the same arguments, except taking\n\t\t\t\tthe first and a second argument -- the function to\n\t\t\t\tmake and the second is dictionary mapping the arguments\n\t\t\t\tto the output keys"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t      \n\t\t# This wrapper will cache the maximum size of the cache to LRU.\n\t\t\n\t\t# Wad: What is the best way to cache and the cache is not shared?\n\t\t# Wad: What other technique to use for the cache?\n\n\t\t# If you need a LRU memoization function for a function, you\n\t\t# get a reference to the function that reduces this amount\n\t\t# of time, not when the cache is low. You must either obtain the\n\t\t# reference as well as the memoization function for a function to\n\t\t# obtain a reference to LRU parameters,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\t# The implementation below is almost just (but straight-out)\n\t# the decorator is a bit cumbersome to use.\n\tdef cache_decorator_inner(func):\n\t\t@wraps(func)\n\t\tdef decorated(*args, **kwargs):\n\t\t\tkey = (get_function_key(func, func.func_name)(), repr(args), tuple(sorted(kwds.items())))\n\t\t\tif not typed:\n\t\t\t\tkey += (getattr(function, 'func_name', ''),)\n\t\t\t_cache_key = key\n\t\t\tif key in _cache:\n\t\t\t"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\t@functools.wraps(lfu)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tresults = self.cache.get(maxsize=maxsize)\n\t\t\tif results is not None:\n\t\t\t\treturn results['result']\n\t\t\tif typed:\n\t\t\t\tresults = eval(args, self.cache)\n\t\t\treturn results['result']\n\t\treturn wrapper\n\n\treturn lfu_cache\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n    \tdef decorator(decorator):\n\t\t\"\"\"\n\t\tDecorator wrapper, for speeding up evaluation of the wrapped function.\n\t\tDecorated a user's function, we can provide \"wrapped\" to the wrapped\n\t\tfunction.\n\n\t\tUse it with a function which takes no arguments and wraps it's\n\t\town partial function that returns its outputs.\n\t\t\"\"\"\n\t\t\n\t\tdef wraparound(wrapped):\n\t\t\t\"\"\"\n\t\t\tMeasures the wrapped function executes its own partial function \n\t\t\tcalled with the same arguments.\n\n\t\t\t@param wrapped: wrapped Function to decorate\n\t\t\t@type wrapped: L{"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tcache = {}\n\t\n\tdef f(func):\n\t\t\"\"\"\n\t\tWrapper for functions whose data is cached for `func`\n\t\tand a Memoizer instance `fun`. This is used in\n\t\tconjunction with the L{memoize}, L{memoize_cache}.\n\t\tReturns a memoizing callable that saves the data in the cache.\n\t\t\"\"\"\n\t\tdef memf(*arg, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThis wrapper allows only a single keyword argument\n\t\t\targument, where the function's memoized version is required.\n\t\t\t\"\"\"\n\t\t\targs, varargs, varkw,"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef dec_lfu(func):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn func(self, *args, **kwargs)\n\t\t\texcept LFU:\n\t\t\t\tprint \"cache.dec_lfu(%s) error @@@ %s\" % (repr(func), repr(args), repr(kwargs))\n\t\t\t\treturn\n\n\t\tdef newfunc(self, *args, **kwargs):\n\t\t\tcache = self.lru_cache(maxsize)\n\t\t\tif typed:\n\t\t\t\treturn cache[func](*args, **kwargs)\n\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n    def wrapped_func(args):\n\t# TODO: make a wrapper to be a decorator here.  Currently not\n\t#  efficient, but not required for it.\n\t\tassert args > 0 or args == 0\n\t\treturn LFuCache(args)\t\n\n\t@lru_cache(maxsize)\t\n\tdef cached_func(*args):\n\t\tcache_key = '%s:F%d' % (args[0]._cache_hash, args[1])\n\t\tval = LFuCache._cache.get(cache_key, O_NONE)\n\t\tif"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef fib(n, bits=1000):\n\t\treturn chr(n + (bits & ((1 << n) - 1)))\n\n\t@cache(maxsize=65536)\n\tdef wrap(func):\n\t\tdef wrapper(*args):\n\t\t\treturn apply(func, args, bound.__dict__)\n\t\treturn wrapper\n\treturn fib\n\n\n# def fib_cache(maxsize=256, typed=False):\n# \t@wraps(lfb_cache)\n# \tdef _lfb_cache(*args, **kw):\n# \t\ti = 0"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef __init__(self, function, cache_name=None):\n\t\t\"\"\"\n\t\tArguments:\n\t\t`function` --\n\t\t\tAn unbound function that takes a `self` and returns a cache.\n\t\t`cache_name` --\n\t\t\tA string name to provide as cache name.\n\t\t\"\"\"\t\t\n\t\t\t\n\t\tself.cache = {}\t\t# {function_name: function,...}\n\t\t\n\tdef __call__(self, *args, **kwargs):\n\t\t\n\t\t# First arg is list of arguments to func.\n\t\targs = list(args) # A list is needed because"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\tfrom pylfu import  *\n\timport time\n\tfrom test_lda import *\n\tmem_limit = 100 * 1024 * 1024\n\tcache_name = \"foumemos\"\n\t\n\tdef fifty(**kw): return kw \n\tmem_limit = mem_limit / 2 # memory limit passed to f()\n\tmemory = []\n\tfor i in range(int(mem_limit)): \n\t\tkey = fife().value()\n\t\tif key in memory: memory.remove(key)\n\tprint \"Memory"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\nfrom itertools import takewhile        # Backwards compatibility with Python 2.4\nfrom math import log, exp\nimport operator\nfrom collections import namedtuple\n#from multiprocessing import Pool as Lock\n\n@lru_cache(maxsize=128, typed=False)\ndef lru_cache(maxsize=32000, typed=False):\n\t\"\"\"\n\tDecorator to wrap an LRU cache implementation as a decorated function.\n\t\"\"\"\n\tdef f(*args,**kw):\n\t\tif len(args)<maxsize:\n\t\t\traise Exception(\"LRUCache requires at least %d bytes of arguments\" % maxsize"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tfun = lfu_cache_decorator(maxsize=maxsize, typed=typed)\n\t\tdef inner(*args, **kwargs):\n\t\t\t# pylint: disable=E1101,W0212\n\t\t\t# The wrapped function call is only memoized once, so it\n\t\t\t# doesnt cache the argument so that we don't immediately\n\t\t\t# invalidate any function at all.\n\t\t\tkey = str(args) + str(kwargs)\n\n\t\t\t#@functools.wraps(fun)\n\t\t\tdef cached(*args, **kwargs):\n\t\t\t\t# pylint: disable=E1101,W0212\n\t\t\t\tkey ="}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkey, value = self.pop()\n\t\treturn key, value\n\n\tdef popitemlist(self):\n\t\t\"\"\"\n\t\tRemove and return all items from the list which are not empty.\n\t\t\"\"\"\n\t\ttmp = []\n\t\tfor idx, item in enumerate(self):\n\t\t\tif not item in tmp:\n\t\t\t\ttmp.append(item)\n\t\treturn tmp\n\n\tdef copy(self):\n\t\treturn ConfigDict(self).data\n\n\tdef update(self, other=None, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the config entries from other, overwriting existing values.\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tfirst = self.items[0][0]\n\n\t\t# Remove the reference with `first-` removed\n\t\tself.items[0] = self.items[0][1:]\n\n\t\treturn first\n\n\tdef get(self, key, *args):\n\t\twith self.lock:\n\t\t\tif key in self.items:\n\t\t\t\tself.items.pop(self.items.index(key))\n\n\tdef items(self):\n\t\twith self.lock:\n\t\t\tyield from self.items\n\n\tdef push(self, item):\n\t\t\"\"\"\n\t\tPush `item` into"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkey, value = self.pop(0)\n\t\tself[key] = value\n\t\tself.changed()\n\n\tdef update(self, dict):\n\t\tfor key in dict.keys():\n\t\t\tself[key] = dict[key]\n\t\tself.changed()\n\n\tdef values(self):\n\t\tif not self:\n\t\t\treturn []\n\t\telse:\n\t\t\treturn [(k, v) for (i, (k, v)) in enumerate(self.items()) if i <> 0]\n\n\tdef lists(self):\n\t\tif not self:\n\t\t\treturn []\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif len(self.list) > 1:\n\t\t\treturn (self.list.pop(),)\n\t\telse:\n\t\t\treturn (None, None)\n\t\t\t\n\tdef append(self, item, *args, **kwargs):\n\t\t\"\"\"\n\t\tInserts item `item` into the given *args and kwargs,\n\t\tassuming that item already exists.\t\"\"\"\n\t\ttry:\n\t\t\tself.list.append(item)\n\t\texcept:\n\t\t\tself.list.append(item)\n\t\treturn self\n\t\t\n\tdef count(self, value):\n\t\t\"\"\"\n\t\tReturn the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\twhile 1:\n\t\t\tkey, value = self.head()\n\t\t\tif self.item(key)!= value or self.peek() == key:\n\t\t\t\treturn (key, value)\n\t\t\telse:\n\t\t\t\tself.error('key not present')\n\n\tdef peek(self):\t\"\"\"\n\tRemove and return the first item completely removed.\n\t\"\"\"\n\t\thead = self.head()\n\t\treturn head.val\t\t\n\n\tdef insert(self, key, value):\n\t\t\n\t\tprev_item = self.head()\n\t\tif prev_item is None:\n\t\t\thead = self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn (self[0].pop(), self.get(0)[1])\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\t`self.get()` and `self.pop()` return `(key, value)` pair.\n\t\t\"\"\"\n\t\tif self.get():\n\t\t\treturn (('popular' if len(self) == 1 else 'linear'), self.pop())\n\t\telse:\n\t\t\treturn self.pop()\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn an expression with ``(eval(self.get()),\n\t\t``exec(printlist=True)(item)``"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif len(self.stack) == 1:\n\t\t\tself.stack.pop()\n\t\telif len(self.stack):\n\t\t\t(key, value) = self.stack.pop()\n\t\t\tself.stack[to_index(value)] = (key, None)\n\t\t\treturn key, value\n\t\telse:\n\t\t\traise IndexError(\"popitem requires at least 2 arguments, \"\n\t\t\t\t\"got %s\" % len(self.stack))\n\nclass MultiValueDict(MultiDict):\n\t\"\"\"\n\tA subclass of MultiDict that handles multiple values for the same key\n\tname.\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn popleft(self.t)\n\n\tdef push(self, t):\n\t\t\"\"\"\n\t\tPush `t` onto stack at position `self.pos` for incremental indexing.\n\t\t\"\"\"\n\t\tself.t = t\n\t\tif self.pos < len(self.t):\n\t\t\tself.pos = self.pos + 1\n\t\telse:\n\t\t\tself.pos = 0\n\t\tif t >= len(self.t) or t < 0:\n\t\t\treturn None\n\t\tself.t = self.t[:t]\n\t\tself.next = i = self.t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn self._map.popitem()[0]\n\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tRemove and return the most recently inserted (or given key)\n\t\titem stored in the map and remove it.\n\t\tIf the key is not found, return the `(key, value)` pair.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._map.pop(key)[-1]\n\t\texcept IndexError:\n\t\t\traise tvdb_attributenotfound(\"Cannot pop from an empty sequence\")\n\n\tdef set_value(self, key, value):\n\t\t\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\td = dict(self.d)\n\t\twhile len(d) > 1:\n\t\t\t(k, v) = d.popitem()\n\t\t\tif v is x:\n\t\t\t\tdel d[k]\n\t\tdel d['\\0']\n\t\treturn key, d\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear in-memory list, remove all items from the cache.\n\n\t\t>>> x = [1, 2, 3]\n\t\t>>> x\n\t\t[1, 2, 3]\n\t\t\"\"\"\n\t\tself.cache = {}\n\n\tdef pop(self, key):"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\texcept TypeError as e:\n\t\t\traise e\n\n\tdef update(self, E=None, **F):\n\t\tF.update(self, E, **F)\n\n\tdef pop(self, k, d=None):\n\t\t\"\"\"\n\t\tRemove and return specified key/value.  If key is not found, d is returned\n\t\totherwise KeyError is raised\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.get(k, d)\n\t\texcept KeyError:\n\t\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tD.__iter__() -> D.iterkeys() -> it -> D.values()\n\t\t\"\"\"\n\t\tfor key in self:\n\t\t\tyield key\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tD.values() -> list of D's values\n\t\t\"\"\"\n\t\treturn list(self.itervalues())\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tD.items() -> list of D's ("}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tkey = key.lower()\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\t\tAllows partial application of D.pop(k).\"\"\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError(\n\t\t\t\t\"pop expected at most 2 arguments, got \" + repr(len(args))\n\t\t\t\t)\n\t\ttry:\n\t\t\td"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry: self.__getitem__(key); return default\n\t\texcept KeyError: return d.setdefault(key,d)\n\n\tdef __setitem__(self, key, value, default=None):\n\t\tif not self.has_key(key):\n\t\t\tself.__setitem__(key, value)\n\t\tself.set_default(key, default)\n\n\tdef has_key(self, key):\n\t\treturn (keyname(key) in self._keys)\n\n\tdef set_default(self, key, default=None):\n\t\t\"\"\"\n\tD.setdefault(k[,d]) ->"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\treturn setattr(self, key, default)\n\tdef __setitem__(self, key, value):\n\t\tdict.__setitem__(self, key, value)\n\t\tself.validate_item(key, value)\n\t\tif key not in self._meta_keys:\n\t\t\tself._meta_keys.append(key)\n\tdef __getattr__(self, key):\n\t\ttry:\n\t\t\treturn self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\traise AttributeError(r\"'Dict' object has no attribute '%s'\" % key)\n\n\tdef __setattr__(self, key, value"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\treturn default\n\n\tdef update(self, E=None, **F):\n\t\t\"\"\"\n\t\tA[R] = E or {k1: v1, k2: v2,...}\n\t\tupdate B with E+1, A[R]=B\n\t\t\"\"\"\n\t\tfor k,v in E:\n\t\t\tself[k] = v\n\t\tfor k in F:\n\t\t\tself[k] = F[k]\n\n\tdef get(self"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef update(self, E, **F):\n\t\t\"\"\"\n\t\tD.update(E, v) -> None. Updates D from E and F.\n\t\t\"\"\"\n\t\ttry:\n\t\t\told = self[E]\n\t\t\tdel self[E]\n\t\t\tF.update(self, E)\n\t\texcept (KeyError, AttributeError):\n\t\t\tpass\n\t\tF[self.key] = self\n\t\treturn F\n\nclass OrderedDict(UserDict):\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self.cache:\n\t\t\td = self.DefaultDict(default)\n\t\t\tself.cache[key] = d\n\t\treturn self.cache[key]\n\n\tdef value(self, key):\treturn self.cache.get(key)\n\n\tdef clear(self):\n\t\t\"\"\"Clear the cache\"\"\"\n\t\tself.cache = {}\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\treturn a list of keys in the cache\n\t\t\"\"\"\n\t\treturn self.cache.keys()\t\n\nfrom pymc import MarkovChain\n\nm = MarkovChain"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef update(self, E):\n\t\t\"\"\"\n\t\tD.update(E) -> None.\n\t\tUpdates E from E.\n\t\t\"\"\"\n\t\tif not isinstance(E, FancyList):\n\t\t\tE = FancyList(self, E)\n\t\tfor k, v in E.iteritems():\n\t\t\tself[k] = v\n\n\tdef copy(self):\n\t\treturn D.copy(self)\n\n\tdef keys(self):"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tD.update([E, ]D) -> None.  Update D from E: for k in E: D[k] = E[k] \"\"\"\n\t\tif len(args) > 0 and len(kwargs) > 0:\n\t\t\tfor key in kwargs:\n\t\t\t\tif hasattr(args[0], key):\n\t\t\t\t\tsetattr(self, key, kwargs[key])\n\t\t\t"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif default is None:\n\t\t\tdefault = self._data\n\n\t\treturn self._data.get(key,default)\n\n\tdef set(self, key, value):\n\t\treturn self._data.set(key,value)\n\n\t# D.keys\n\tdef keys(self):\n\t    return self._data.keys()\n\n\tdef iteritems(self):\n    \t# L.sort(lambda a,b: 0 if a[1] > b[1] else 1 if a[1] < b[1] else -1)\n    \treturn self._data.iteritems()\n\n\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self.__getitem__(key) or default\n\t\texcept KeyError: return default\n\n\tdef getDict(self, key):\t\"\"\"\n\tD.getDict(k[,d]) -> D[k] as a copy of D[k] if k in D, else d.\"\"\"\n\ttry: return self.__getitem__(key).copy()\n\texcept KeyError: return d\n\ngobject.type_register(type(list))\ngobject.type_register(type([]), gobject.TYPE_PYOBJECT)\n\nclass Config(gobject.GObject):\n\t\"\"\"\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._data.get(key,default)\n\n\tdef getint(self, key, default=None):\n\t\t\"\"\"\n\t\tD.getint(k[,d]) -> d[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._data.getint(key,default)\n\n\tdef getfloat(self, key, default=None):\n\t\t\"\"\"\n\t\tD.getfloat(k[,d]) -> d[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\tdef override(self, key, default=None):\t\t\"\"\"\n\tD.override(k[,d]) -> D[k] if k not in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value."}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef update(self, dict=None, **kwargs):\n\t\t\"\"\"\n\t\tD.update(E)\n\t\tReturn a dict.\n\t\t\"\"\"\n\t\tif dict:\n\t\t\ttry:\n\t\t\t\tkeys = dict.keys()\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tfor key in keys:\n\t\t\t\t\tself[key] = dict[key]\n\t\tself.update_items()\n\n\tdef clear(self):\n\t\tself.cache.clear()\n\n\tdef update_"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.__getitem__(key.lower(), d)\n\n\tdef has_key(self, key):\treturn self.__contains__(key.lower())\n\n\tdef __contains__(self, key):\n\t\treturn self.has_key(key)\n\n\tdef __len__(self):\n\t\treturn len(self.__keys())\n\n\tdef __nonzero__(self):\n\t\treturn True\n\n\t# ----------------------------------------------------------------------\n\tdef __str__(self, key):\n\t\tif self.has_key(key):\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn \"\"\n\n\tdef __repr"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.getone(key, default)\n\nclass MagOrder:\n\t\"\"\"\n\tBase class for MagOrder object\n\tused to pass arguments to commands that map command calls to MagOrder,\n\tthe following methods:\n\t\tsend_msg(message_id, from_user=from_user)\t# send a message\n\t\tsend_reply_email(msg)\t# send replies from user using reply_to\n\t\t                  # method\n\t\"\"\"\n\t__slots__=('order','type','data','destination','command',\n\t\t\toptional_params = {'message"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef getlist(self, key):\n\t\t\"\"\"\n\t\tL.getlist(k) -> list of D[k] for D[k] in the DERIVED dict l.\n\t\t\"\"\"\n\t\ttry:\n\t\t\twhile 1:\n\t\t\t\td = self[key]\n\t\t\t\tif not d:\n\t\t\t\t\tbreak\n\t\t\t\tvalue = d.pop()\n\t\t\t\td.close()\n\t\t\t\tdel d[key]\n\t\t\t\treturn value\n\t\texcept KeyError:\n\t\t\treturn []\n\n\tdef hasKey"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.get_raw(key, None, default)\n\n\tdef get_int(self, key, def_val):\n\t\t\"\"\"\n\t\tD.getInt(k) -> D[k] if k in D, else d.  d defaults to None.\n\t\t\"\"\"\n\t\treturn self.get_raw(key)\n\n\tdef get_boolean(self, key, def_val):\n\t\t\"\"\"\n\t\tD.getInt(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\t\"\"\"\n\t\treturn"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif key in self: return self[key]\n\t\treturn default\n\n\tdef __contains__(self, key):\n\t\tself.__getitem__(key.lower())\n\n\tdef __len__(self):\n\t\treturn len(self.D)\n\n\tdef __setitem__(self, key, value): # for use do not double del in Python 2.4\n\t\tself.D[key.lower()] = value\n\n\tdef update(self, dict=None, **kwargs):\n\t\t\"\"\"\n\t\tD.update(E) -> None.\n\t\tUpdate D from E: for k in E:"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcache_attr = \"_%s_cache\" % key\n\t\tdef caching_method(user, *args, **kwargs):\n\t\t\tif args:\n\t\t\t\tkey += \".%s\" % args[0]\n\t\t\telif kwargs:\n\t\t\t\tkey += \".%s\" % (kwargs.items()[0])\n\t\t\telif user:\n\t\t\t\tkey += \".%s\" % user.id\n\t\t\t\tkey += args[0]\n\t\t\telse:\n\t\t\t\tkey += \".%s\" % ()\n\n\t\t\tif cls_cache.get(cache_attr)!=None:\n\t\t\t\treturn cached(cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef deco(f):\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t# The key we're caching is always the same as the key but\n\t\t\t\t# this will make the functions below fail fast in some\n\t\t\t\t# environments.\n\t\t\t\tkey = key + args\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\t\treturn cache[key]\n\t\t\treturn wrapper\n\n\t\tdef memof(*args, **kwargs):\n\t\t\t# Remember that we must return a value in None for the cache.\n\t\t\t# Without this, we want"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\t@wraps(f)\n\t\t\tdef memoizer(self):\n\t\t\t\tif not key:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult = cache[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tif self.disabled:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\traise\n\t\t\t\t\tresult = f(self)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\t\t\t# FIXME: should probably be in the cache, not lazy,\n\t\t\t\t\t# so that it can be acted upon by the parent __call__\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\texcept KeyError"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef wrapper(*args):\n\t\t\tkey = key or args\n\t\t\tresult = cache.get(key)\n\t\t\tif result is not None:\n\t\t\t\tif isinstance(result, list):\n\t\t\t\t\t# list is stored as [(key, result),...]\n\t\t\t\t\tresult.sort()\n\t\t\t\t\tcache[key] = result\n\t\t\t\telse:\n\t\t\t\t\t# tuple is stored as [[key, result],...]\n\t\t\t\t\tcache[key] = result\t\n\t\t\treturn result\t\t\n\t\treturn wrapper\n\n\treturn cachedmethod\n\n\nclass WSGICache(object):\n\t\"\"\"Wraps a WSGICache method"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\n\n\tdef save(cache, key, data, handler=None):\n\t\t\"\"\"\n\t\tThis function works on the passed arguments; if not, it\n\t\twraps the function for passing arguments as it is.\n\t\t\"\"\"\n\t\tif handler is None:\n\t\t\thandler = _dict_to_handler(data)\n\n\t\tkey = key + argsort(str(key))\n\t\tkey = hashlib.md5(key).hexdigest()\n\t\tcache[key] = handler(data)\n\t\treturn handler\n\treturn cachedmethod\n\n\nif DEBUG:\n\t# Enable debugging in unit testing\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\t# @TODO: this doesn't really work yet. I assume the data is not\n\t\t#       stored in the cache. Just do it here.\n\t\t# @TODO: use pythons-cache, etc\n\t\tdef inner(*args, **kwargs):\n\t\t\tif key == hashkey:\n\t\t\t\tkey = args[1:]\n\t\t\t\tkey = hashlib.md5(key).hexdigest()\n\t\t\tresult = cache.get(key)\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache.set(key, result)\n\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\nclass cachedproperty(object):\n\t\"\"\"\n\tClass decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n\n\tUsage example:\n\n\t    from mycaching import cachedproperty\n\t    class Person(object):\n            \tdef __init__(self, cache):\n\t\t... \tself._cache = cache  # Cache instance.\n\t\t... \tdef mymethod():\n        \t\t\treturn \"This is my age!\"\n\t\t...\treturn mymethod\n\t\"\"\"\n\t\n\tdef __init__(self, cache):\n\t\t\"\"\"\n\t\tInitializer. Store a reference to"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\t# Note: this is needed so that only one object can reuse\n\t\t# its own cache.\n\t\tif lock is None:\n\t\t\tlock = cache.lock()\n\t\tdef creator(*args, **kwargs):\n\t\t\treturn cache.data.get(key, _sentinel)\n\n\t\tdef creator_key(*args, **kwargs):\n\t\t\tkey = creator(*args, **kwargs)\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tres = cache.data[key]\n\t\t\t\tres[2] = res[2].get(key, _sentinel)\n\t\t\t\treturn res\n\t\t\tfinally"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tmemoize_instance = functools.partial(\n\t\t\tcachedmethod,\n\t\t\tcache=cache,\n\t\t\tkey=key,\n\t\t\tlock=lock\n\t\t)\n\t\treturn memoize_instance\n\n\n@implements_iterator\nclass MemoryCache(Cache):\n\tdef __init__(self, cache):\n\t\tsuper(MemoryCache, self).__init__(cache)\n\n\nclass CacheKeyGenerator(CacheKeyGenerator):\n\tdef __init__(self, **kwarg):\n\t\tsuper(CacheKeyGenerator, self).__init__(**kwarg)\n\t\tself._keycount = 0\n\n\tdef _build_"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef caller(*args, **kwargs):\n\t\t\tif args not in cache:\n\t\t\t\tif args not in cache:\n\t\t\t\t\tcache[args] = function(*args)\n\t\t\t\t\tfunc_name = func.__name__\n\t\t\t\t\tif func_name not in cache:\n\t\t\t\t\t\tcache[func_name] = cache[args] + [key]\n\t\t\t\t\t\treturn cache[func_name]\n\t\t\t\tresult = cache[args]\n\t\t\t\tcache[args] = result\n\t\t\t\treturn result\n\t\t\tresult = caller(*args, **kwargs)\n\t\t\treturn result\n\t\treturn cache_decorator(cache, key,"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif not isinstance(e, Exception):\n\t\t\tif sys.stdout.encoding is None:\n\t\t\t\tif sys.getdefaultencoding() == 'ascii':\n\t\t\t\t\tsys.stdout.write(cls.__repr__())\n\t\t\t\telse:\n\t\t\t\t\tsys.stdout.write(str(e))\n\t\t\telse:\n\t\t\t\tif sys.getdefaultencoding() == 'ascii':\n\t\t\t\t\tsys.stdout.write(unicode(e, 'utf-8'))\n\t\t\t\telse:\n\t\t\t\t\tsys.stdout.write(str(e))\n\t\telse:\n\t\t\tsys.stdout.write(str(e))\n\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn _(\"Internal error\")\n\nclass OsBundleException(Exception):\n\t\"\"\"\n\tA exception that is raised if the current action of the process\n\thas failed. Used by custom exceptions in the GUI layer as the\n\texception.\n\t\"\"\"\n\tdef __init__(self, action, e):\n\t\tself.action = action\n\t\tself.e = e\n\t\tException.__init__(self)\n\nclass OsBundleInstallException(OsBundleException):\n\t\"\"\"\n\tThis exception will have an informational feedback about what happened to\n\tinstall. The reason is that these"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\testr = ''\n\n\t\t# Emit a message, and close the file and end\n\t\tif e!= None:\n\n\t\t\testr=e.args[0]\n\n\t\t\tif \"Exception\" in e:\n\n\t\t\t\tlog.error(\"Encountered Exception! Message: \" + str(e) + \", args=\" + str(e.args))\n\n\t\t\telse:\n\n\t\t\t\tmsg = e.args[1]\n\n\t\t\t\tif len(msg) > MAX_MSG_LEN:\n\n\t\t\t\t\tmsg=msg[:MAX_MSG_LEN]\n\n\t\t\t\t# Strip off last \\n and convert \\n to"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\n\tlines=[]\n\tlines.append(e.getMessage())\n\tlines.append(\"\")\n\tlines.append(e)\n\tif e.getMessage().find(max_level) >= 0:\n\t\tlines.append(\".\")\n\t\tlines.append(max_path_level)\n\telse:\n\t\tlines.append(\"\")\n\tif e.getMessage().find(\".dll\") >= 0:\n\t\tlines.append('.')\n\t\tlines.append(max_path_level)\n\telse:\n\t\tlines.append(\"\")\n\treturn os.linesep.join(lines)"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tlines = []\n\t\t# Get a safe representation\n\t\texclist = traceback.format_exception(*sys.exc_info())\n\n\t\tfor filename, lineno, name, line in exclist:\n\t\t\tif line:\n\t\t\t\tlines.append('-'+ filename + ':'+ line)\n\t\t\tbreak\n\t\telse:\n\t\t\tlines.append('-'+ '-' * 30 + '-' * (max_level - len(exclist)))\n\t\treturn '\\n'.join(\"\\n\".join(lines) + \"\\n\")\n\n\t@classmethod\n\tdef debug(*args, **kw):\n\t\t'''"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\texc = str(e)\n\t\tif not max_level < 1 or expl.count(exc) > max_level:\n\t\t\traise expl.exc_from_exception(e)\n\t\telse:\n\t\t\treturn str(expl.exc_from_exception(e, max_level, max_path_level))\n\n\tdef traceback(self, func_name='main'):\t\t\"\"\"\n\tGet traceback for functions.\n\n\t:param func_name: name of function (default main)\n\t:type func_name: str\n\t:return traceback object\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tprefix = \"\"\n\t\tif len(sys.argv) > 0 and \"-v\" in sys.argv[0]: prefix = \"-v \"\n\t\tif len(sys.argv) > 1 and \"--max-level= \" in sys.argv[1]: prefix += \"-v \"\n\t\treturn prefix + traceback.format_exception_only(cls, e, max_level, max_path_level)\nimport csv\nimport datetime\nimport logging\nimport os\n\n\nclass DataSet(object):\n    def __init__(self):\n        self.data = {}\n        self.log = logging"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif e.__class__.__name__ == 'exceptions.ValueError':\n\t\t\tmessage = \"e={e!r}, max_level={e!r}, max_path={e!r}, \".format(e=str(e), e=str(max_level), e=str(max_path_level))\n\t\telif not e.__class__.__name__ == 'exceptions.TypeError':\n\t\t\tmessage = \"e={e!r}, max_level={e!r}, max_path={e!r}, \".format(e=str(e), e=str(max_level),"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif isinstance(e, (list, tuple)):\n\t\t\te = ''.join('%s' % e_to_str(i, max_level, max_path_level) for i in e)\n\t\t\treturn repr(e)[:max_level]\n\t\tif isinstance(e, tuple):\n\t\t\treturn ', '.join(e_to_str(i, max_level, max_path_level) for i in e)\n\t\tif not (isinstance(e, Exception) or hasattr(e, 'args')\n                 ) or not len(e.args):\n\t\t\treturn repr("}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\n\tif isinstance(e, BaseException):\t\t\n\t\tif isinstance(e, SystemExit): break\t\t\n\t\tif isinstance(e, KeyboardInterrupt): raise e\n\t\tif isinstance(e, SystemExit): raise RuntimeError() \n\t\t\n\telif isinstance(e, ValueError):\n\t\tif isinstance(e, ValueError(\"\")):\n\t\t\treturn e.__str__()\n\t\telif isinstance(e, ValueError(\"...\")):\n\t\t\treturn e.__str__()+str(e)\n\t\telse:\n\t\t\treturn str(e)\n\t\tprint \"WARNING: Exception: [%s] %s\"%(type(e).__name__,str(e"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tbytes_written=0\n\t\tfile_was_opened=False\n\t\ttry:\n\t\t\tfile_handle = codecs.open(file_name, \"w\", encoding)\n\t\t\tfile_handle.write(encoding)\n\t\t\tfile_handle.close()\n\t\texcept:\n\t\t\tprint(\"Warning: Could not write to file %s\" % file_name)\n\n\t\tif overwrite==False:\n\t\t\tif os.path.exists(file_name):\n\t\t\t\tos.remove(file_name)\n\t\t\traise Exception(\"File '%s' already exists!\" % file_name)\n\n\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\t# file size\n\t\tn = len(text_buffer)\n\t\t# number of bytes to append\n\t\tdata = bytearray()\n\t\tfor i in range(n):\n\t\t\t# add text, length and encoded to file\n\t\t\tdata.extend(text_buffer)\n\t\t\tif encoding is not None:\tdata.append(encoding)\n\t\t\t# add length\n\t\t\tdata.extend(length_to_string(len(text_buffer)))\n\t\t# write data\n\t\tif overwrite:\n\t\t\tfd = os.open(file_name, os.O_WRONLY | os.O"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_name = expand_path(file_name)\n\t\ttemp_fd, temp_path = mkstemp(prefix=\"pyfaidx_\")\n\t\tif not os.path.exists(temp_path):\n\t\t\tos.makedirs(temp_path)\t\t\n\t\tif overwrite:\n\t\t\tprint \"Overwriting file %s...\" % file_name\n\t\t\ttemp_fd.close()\n\t\t\n\t\t_append_text(text_buffer, file_name, encoding)\n\t\treturn 1\n\n\tif os.path.isfile(file_name):\t\n\t\t_append_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tcount = 0\n\t\tfilehandle = open(file_name, \"ab\")\n\t\tif overwrite:\n\t\t\tif isinstance(text_buffer, unicode):\n\t\t\t\tfilebuffer = text_buffer.encode(encoding)\n\t\t\telse:\n\t\t\t\tfilebuffer = str(text_buffer)\n\t\t\t_write_file_helper(filehandle, text_buffer, \"wb\", encoding, True)\n\t\telse:\n\t\t\tif isinstance(text_buffer, unicode):\n\t\t\t\tfilebuffer = text_buffer.encode(encoding)\n\t\t\telse:\n\t\t\t\tfilebuffer = str(text_buffer)"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\ttry:\n\t\t\tt = datetime.datetime.now()\n\t\t\twith open(file_name, encoding=encoding) as f:\n\t\t\t\tbuf = str(t.strftime(\"%Y-%m-%d %H:%M:%S\") + text_buffer)\n\t\t\tos.write(buf, text_buffer)\n\t\texcept:\n\t\t\tlogging.critical(\"Exception occurred saving the file to the given %s format\", file_name)\n\n\ndef save_to_file_string(file_name, text_buffer, encoding='utf-8'):\n  \"\"\"\n  Create a file in the directory"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_mode='ab'\n\t\twith open(file_name,file_mode) as f:\n \t\tbuffer_size = len(text_buffer);\n\tfile_mode = 'wb'\n\tfile_buffer = text_buffer\n\tbytes_written_size = file_size(file_name)\n\tf.seek(bytes_written_size, os.SEEK_END)\n\n\tbuffer_size = os.path.getsize(file_name)\n\tf.write(text_buffer)\n\tif buffer_size == bytes_written_size:\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tbuffer = text_buffer\n\t\tfile_exists = os.path.exists(file_name)\n\t\tif not overwrite and os.path.getsize(file_name) == 0:\n\t\t\tfile_size = os.path.getsize(file_name)\n\t\t\tfile_offset = 0\n\t\t\tfile = open(file_name, \"rb\")\n\t\t\tfile_buffer = file.read(file_size)\n\t\t\twhile True:\n\t\t\t\tif not re.search(r'^0x[0-9a-zA-Z_]?', file_buffer) is None:\n\t\t\t\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif not os.path.exists(file_name) and not overwrite:\n\t\t\traise IOError(\"File {} does not exist\".format(file_name))\n\t\tfile_handle = open(file_name, mode='wt', encoding=encoding)\n\t\ttry:\n\t\t\tfile_handle.write(text_buffer + '\\n')\n\t\tfinally:\n\t\t\tfile_handle.close()\n\t\tfile_handle_name = file_name + '.bin'\n\t\treturn self.file_written(file_handle_name, text_buffer)\n\n\tdef save_to_file("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\n\tf = open(file_name, mode='r+b')\n\tf.write(text_buffer)\n\tf.seek(0)\n\tf.close()\n\n\tt = None\n\tsize = os.path.getsize(file_name)\n\tfor line in t:\n                file_bin = struct.pack('B', binary(line))\n                file_bin_size = len(file_bin)\n                file_bin_str = struct.pack('B', binary(file_bin_size))\n                file_out.write(file_bin_str)"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfd, dest_handle = tempfile.mkstemp(suffix=file_name + \"b\")\n\t\tif not overwrite and os.path.exists(dest_handle):\n                    os.close(fd)\n                os.remove(dest_handle)\n\t\topen(source_path, encoding=\"utf-8\").write(text_buffer)\n\t\tos.rename(source_path, dest_path)\n\t\tos.system(dest_handle)\n\n\tdef append_text_with_separator(buffer, text_buffer, encoding, overwrite=False):\t# pragma: no cover\n\t\t\"\"\""}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\twith codecs.open(file_name, encoding=encoding) as f:\n\t\t\t\ttext_buffer = f.read()\n\t\texcept IOError:\n\t\t\tlogger.error(\"Cannot open file \" + file_name)\n\t\telse:\n\t\t\t# return text_buffer\n\t\t\t\tif len(text_buffer) > 0:\n\t\t\t\treturn text_buffer\n\treturn None\n\ndef save_file_as(to_save, file_name, text_buffer, encoding=\"utf-8\"):\n\t\"\"\"\n\tSaves the file_to_save using the passed-"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# see: https://docs.python.org/3/library/codecs.html#standard-encodings\n\t\treturn codecs.open(file_name, encoding, 'utf-8')\n\n\n\"\"\"\nUtility functions that handle text buffer operations.\n\"\"\"\n\ndef write_to_file(filename, textbuffer, encoding):\t\t# type: (str, str, str) -> None\n\t\"\"\"\n\tWrite text buffer to file.\n\t:param filename: File name.\n\t:type filename: str\n\t:param to_write_string: String in line after newline."}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tglobal text_buffer\n\t\ttext_buffer = ''\n\t\tif file_name == '-':\n\t\t\tfile_name = sys.stdin.read()\n\t\ttry:\n\t\t\twith codecs.open(file_name, encoding=encoding, errors='ignore') as file_:\n\t\t\t\ttext_buffer = file_.read()\n\t\texcept IOError as err:\n\t\t\tprint('There was a problem reading {} - {}'.format(file_name, err))\n\t\t\texit()\n\t\treturn text_buffer\n\t\t\n\tdef load_image(self, file_name, target):\n\t\t\"\"\"\n\t\tLoad an image"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry: \n\t\t\ttext = open(file_name, encoding=encoding).read()\n\t\texcept IOError as e:\n\t\t\traise FileNotFoundError(\"The file %s could not be opened yet!\" % file_name)\n\t\texcept UnicodeDecodeError as e:\n\t\t\traise FileNotFoundError(\"The file %s contained characters that cannot be encoded!\" % file_name) # TODO : more specific exception\n\t\ttextbuffer = text\n\t\treturn textbuffer\n\traise FileNotFoundError(\"The file '%s' could not be opened!\" % file_name)\n\n\ndef save_textbuffer(textbuffer,encoding=None"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# Ensure file exists\n\t\tif path.exists(file_name) == False:\n\t\t\traise RuntimeError(\"The path: \" + path + \" exist already.\")\n\n\t\t# open file for reading\n\t\tfile = codecs.open(file_name, encoding=encoding, errors='ignore')\n\t\t# load text and split lines and save text buffer\n\t\tlines = file.readlines()\n\t\tfile.close()\n\t\tbuffer = \"\"\n\n\t\tfor line in lines:\n\t\t\t# split it line into lines and save a string\n\t\t\tbuffer += line + \"\\n\"\n\n\t\t# return the buffer\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\tf = file(file_name, \"rt\", encoding)\n\t\t\tf.seek(-1,2)\n\t\t\treturn f\n\t\texcept IOError as e:\n\t\t\tprint \"[!] [!] Cannot find file: %s\" % file_name\n\t\t\tprint \"[!] %s\" % e.reason\n\t\t\treturn None\n\texcept Exception, e:\n\t\tprint \"[!] [!] Exception (%s) in file (from %s)\" % (str(type(e)),file_name)\n\t\tprint \"[!] %s\" % e.reason"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n#\n# Constants - Slot handling\n#\n#\n#\nif (sys.version_info < (3, 0)):\n\tIS_WINDOWS = False\n\n\nif (sys.version_info < (3, 4)):\n\tIS_LINUX = True\n\tIS_OSX = False\n\nif (sys.version_info < (3, 6)):\n\tIS_LINUX = False\n\tIS_OSX = False\n\nIS_PYTHON3 = PYTHON3_VERSION >= 315\n\ndef read_byte_file(path):\n\t\"\"\"\n\tOpen a UTF-"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tpass\n\t\n\twhile True:\n\t\ttry:\n\t\t\ttext = file_to_string(file_name, encoding).decode(encoding)\n\t\t\tbreak\n\t\t\n\t\texcept UnicodeDecodeError as e:\n\t\t\tprint(e)\n\t\texcept Exception as e:\n\t\t\traise Exception('Unable to decode the text buffer. This is because a UTF-8 decoder is not implemented.')\n\treturn text\n\n\ndef file_to_string(file_name, encoding):\t\"\"\"\n\tConvert a file's name to its text encoding. It converts the file to a python string.\nThis is currently the only kind of"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# Check the file exists\n\t\tif not os.path.exists(file_name):\n\t\t\terror_str = 'File does not exist'\n\t\t\tlogging.error(error_str)\n\t\t\treturn None\n\t\t\n\t\t# Open the target encoding file\n\t\ttarget_codecs = None\n\t\t# Open the file using codecs module\n\t\ttry:\n\t\t\ttarget_codecs = codecs.open(file_name, encoding=encoding)\n\t\t\ttarget_codecs.read(1)\n\t\t\t# Set a \"block level\"\n\t\t\ttarget_codecs.read(4096)\n\t\t\t# Read the file and"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\n\tdef read_file_as_bytes(file_or_str):\n\t\ttry:\n\t\t\twith open(file_or_str, \"rb\") as f:\n\t\t\t\tresult_str = f.read().encode(encoding)\n\t\texcept IOError as error:\n\t\t\tprint(\"I/O error({0}): {1}\".format(error, file_or_str))\n\t\t\treturn None\n\t\treturn result_str\n\n\tdef read_array_as_bytes(array_bytes):\n\t\ttry:\n\t\t\twith open(array_bytes, \"rb\") as f:\n\t\t\t"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\treturn os.path.isfile(file_name)\n\tdef is_folder_exist(dir_name):\t\"\"\"\n\tCheck if directory exist.\n:param dir_name: Directory name.\n:type dir_name: str\n:return: Return true (exist), false (is not exist) or invalid directory name\n:rtype bool\t\"\"\"\n\t\treturn os.path.isdir(dir_name)\n\tdef check_file_exist(file_name, dir_name):\t\"\"\"\n\tIf file exist.\n:param file_name: File name.\n:type"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t # check file exists and return boolean\n\t if not os.path.isfile(file_name):\n\t\t\treturn False\n\t # check file path\n\t\tfile_path = os.path.join(*file_name.split(os.path.sep))\n\t\ttry:\n\t\t\treturn os.path.exists(file_path)\n\t\texcept Exception, ex:\n\t\t\traise ex\n\t# check the path\n\tif not os.path.exists(file_name):\n\t\treturn False\n\t# check the file in exist path\n\treturn os.path.isfile(file_path)\n\t"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tfor filename in list_files:\n\t\t\tf_path = file_name + \"/./\" + filename + \".in\"\n\t\t\tif os.path.exists(f_path):\n\t\t\t\tfile_md5 = hash(str(uuid.getnode()))\n\t\t\t\tfile_size = f_path.st_size\n\t\t\t\tif file_size == 0 or file_md5 == file_size:\n\t\t\t\t\treturn true\n\t\treturn True\n\t\t\t\n\t\"\"\"\n\tCheck if file size is not larger or smaller.\n:param file_size: File size\n\t:type file_size:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif os.path.isfile(file_name):\n\t\t\t\treturn file_name\n\t\telse:\n\t\t\t\treturn False\n\n\ndef get_input_arguments(argv):\n\t\"\"\"\n\tGet arguments from command line.\n:param argv: Command line argument tuple.\n:type argv: list\n\t\"\"\"\n\tfor arg in argv:\n\t\tif os.path.isfile(arg):\n\t\t\tif os.path.splitext(arg)[1] in [\".wav\", \".mp3\", \".ogg\", \".flac\"]:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telse:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\treturn run_cmd('touch'+ file_name) in file_name\n\ndef exists(file_name):\n\tif not os.path.exists(file_name):\n\t\treturn False\n\telse:\n\t\treturn run_cmd('test -e %s' % file_name)!= ''\n\ndef write_file(file_name, text):\n\trun_cmd('touch %s' % file_name)\n\tif exists(file_name):\n\t\twith open(file_name, 'a') as text_file:\n\t\t\ttext_file."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tret = False\n\t\tfor extension in common.EXTENSIONS:\n            file_path = os.path.join(path, file_name)\n            if os.path.exists(file_path):\n                ret = True\n                break\n        return ret\n\n\ndef is_int(value):    \"\"\"\n\tCheck if a float value or int (int or float).\n:param value: float or int.\n:type value: str\n:return: Return true (int), false (do not float, or invalid float value)\n:rtype bool\t\"\"\"\n        ret = isinstance(value, int) or isinstance"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif re.match('\\\\\\\\?\\\\[\\\\s\\\\r\\\\n\\\\t\\\\\\/\\\\*\\\\^\\\\\\{|}', file_name) is None:\n\t\t\treturn False\n\t\tif len(file_name) > 1 and file_name[-1]!= '\\\\':\n\t\t\tfile_name += '\\\\'\n\t\treturn file_name!= ''\n\telse:\n\t\treturn False\n\ndef read_file(file_name):\n\t\"\"\"\n\tRead a file.\n\t@param file_name: File name.\n\t@type file_name: str\n\t@return: Return"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tpath = f\"{file_name}.txt\"\n\n\t\tf = open(path, \"r\")\n\t\tfor line in f:\n\t\t\tif not line.strip() or len(line.strip()) == 0:\n\t\t\t\treturn False\n\t\treturn True\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn False\n\ndef validate_file(file_path, file_type):\n\t\"\"\"\n\tCheck if directory/file name are valid.\n:param file_path: Directory/file to check.\n:type file_path: str\n:param file_type:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\tif is_allowed_ext(file_name):\n\t\t\t\treturn False\n\t\texcept:\n\t\t\tpass\n\t\t\t\n\tif not isfile(file_name):\n\t\treturn False\n\t\t\n\tfile_content = get_content_with_ext(file_name)\n\tif DEBUG:\n\t\tprint \"file_content:\"\n\t\tprint file_content\n\t\tprint \"file_name\", file_name\n\t\tprint \"file_readmode\", file_readmode \n\t\tprint \"file_writemode\", file_writemode \n    \tlogger.debug(\"file_content: \"+"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tf_full_path = os.path.join(common.get_setting(\"general\", \"log\"), file_name)\n\t\tif os.path.isfile(f_full_path):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\"\"\"\nTests of Bulk Enrollment related to users with dashboard notifications.\n\"\"\"\n\nfrom datetime import datetime\nfrom unittest.mock import MagicMock, patch\n\nimport ddt\nfrom django.conf import settings\nfrom django.test import TestCase\nfrom django.utils.timezone import now\n\nfrom biz.djangoapps.instructor.access"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\twith cls._lock:\n\t\t\tif cls._active_log:\n\t\t\t\tos.unlink(LOG_FILE)\n\n\n\t@staticmethod\n\tdef dump_logger_config(file, level, log_msg):\n\t\ttry:\n\t\t\tif not os.access(file, os.W_OK) and not os.access(file, os.R_OK):\n\t\t\t\treturn\n\t\t\tlog = logging.getLogger('')\n\t\t\tlog.setLevel(level)\n\t\t\tlog_msg = '\\t%s; %s' % (time.strftime(\"%c\"), log_msg)\n\t\t\tfile"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tif cls.level > logging.DEBUG:\n\t\t\tlogger.setLevel(logging.DEBUG)\t\t\t\t\n\telse:\n\t\tlogger.setLevel(cls.level)\n\t\t\ndef get_log_conf(conf_name):\n\t\"\"\"\n\tTry to get logging.conf file for given conf name \n\tArgs:\n\tconf_name (str): Name of configuration file\n\n\tReturns:\n\t: dict\t: Config file\n\n\t\"\"\"\n\tlog_path = os.path.abspath('../logs/'+conf_name+'.log')\n\t# make sure there is only one config file and get"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.logger = logging.getLogger(__name__)\n\ndef _check_logging_started(): # pragma: no cover\n\t\"\"\"\n\tCheck whether logger was initialized\n\t\"\"\"\n\tglobal cls\n\tif not cls.logger:\n\t\tcls.logger = configure(cls.configure_file_object('logging', 'initialize logging'))\n\tif not cls.logger:\n\t\treturn False\n\treturn True\n\nclass _FileLogger(object):\n\tdef __init__(self, logger, name='', filename=None, lineno=None, formatter=None):\n\t\tself.name ="}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tglobal debug\n\t\tdebug = cls\n\t_cls = None\n\t_clsCls = cls\n\n########################################################################\n########################################################################\nclass AbstractController(object):\n\t\"\"\" Generic base class for user interface implementations.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself._reset_logging() \n\n\t########################################################################\n\tdef _start(self):\n\t\t\"\"\" Setup the controller.\n\t\t\"\"\"\n\t\tpass \n\n\t########################################################################\n\tdef _stop(self):\n\t\t\"\"\" Stop the controller.\n\t\t\"\"\"\n\t\tpass \n\n\t########################################################################\n\tdef _is_ready(self):\n\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogging.getLogger('flask_app.general').setLevel(\n\t\t\tlogging.INFO)\n\t\tlogger = logging.getLogger(__name__)\n\t\tlogger.handlers = []\n\t\tformatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s',\n\t\t'%Y-%m-%d %H:%M:%S')\n\t\tfilehandler = logging.FileHandler('logs/log/logger.log')\n\t\tfilehandler.setFormatter(formatter)\n\t\tlogger.addHandler(filehandler)\n\n\t\tlogger.setLevel(logging.INFO)\n\t\tlogger.info('Logging enabled.')\n\n\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.base = cls.root = logging.getLogger()\nfrom django.contrib import messages\nfrom django.core.cache import cache\nfrom django.db import models\nfrom django.dispatch import receiver\nfrom django.utils.translation import ugettext as _\nfrom django.db.models import signals\n\nfrom oscar.apps.offer import models as offer_models\nfrom oscar.core.exceptions import (ClassNotFoundError, InvalidStatusToRetire,\n                                       InvalidShippingAddress, InvalidBillingAccount,\n                                       InvalidStatusToCredits, InsufficientStock,\n                                       InvalidOrderStatus, InvalidStatusTo"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.set_logs_dir(get_logs_dir())\n\t\tcls.set_root_log(get_log_directory())\n\t\tcls.set_root_log_name(get_log_name())\n\n\ndef _set_logging_config_section(cls):\n\t\"\"\"\n\tSet the configuration section for the logging configuration.  Must be called\n\tbefore use, otherwise the 'init'-section below is required.\n\t\"\"\"\n\tcls._logging_config_section = {}\n\n\tlogger(level=logging.INFO, name='pygen-gen-gen"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\n\n\tcls.handler = None\t\t## The logger handler (singleton)\n\tcls.logger = logging.getLogger(\"PyA2\")\t## The main logger (singleton)\n\tcls.logger.setLevel(logging.DEBUG) \t## The logger level (singleton)\n\tcls.logger.addHandler(logging.StreamHandler()) \t#: The stream handler (singleton)\n\tcls.logger.addHandler(logging.FileHandler(\"pyA2.log\", mode=\"wb\"))\t#: The file handler (singleton)\n\tcls.logger.addHandler(logging.StreamHandler()) \t#: The file handler"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogging.shutdown()\t\n\n\n########################################################################################################\n\nclass PV:\n\n\t\"\"\"\n\t\tGeneral functions for data collection to interact with, etc.\n\t\t\n\t\tTo use this class you have to add some more.\n\t\tFor example, you can use the following:\n\t\t\t\n\t\t\t# set up a class and put the code to be tested with\n\t\t\n\t\t\t#... your code\n\t\t\tpv.set_parameter(\"a_param_name\", \"some name\", \"A parameter named 'a_param_name'\")\n\t\t\tpv.set_function(\"P(a_param_name,...) P(xxx="}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._oldlogging = logging.getLogger('py.warnings')\n\t\tcls.log = cls._oldlogging.log\n\t\tcls.old_level = cls._oldlogging.level\n\t\tcls.log(0, 'Logging is initialized')\n\n\nclass TestLogging(object):\n\t\"\"\"docstring for TestLogging\"\"\"\n\tdef test_enable_log(self):\n\t\t\"\"\"docstring for TestEnableLog\"\"\"\n\t\tl = []\n\t\tl.append(\"Logging enabled\")\n\t\tl.append(\"TestLogging\")\n\t\tlg_log = TestLogging(logger)\n\t\tlg_log"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tname = self._name\n\t\tif self.__class__ is _PyObject:\n\t\t\tnamespace = self.namespace\n\t\telse:\n\t\t\tnamespace = self.namespace.prefix + \"::\"\n\t\tif namespace in self._typecodes:\n\t\t\t# We can report failure of the type here, as we might be\n\t\t\t# able to find it on the wrong port.\n\t\t\tself._typecodesReport.append((namespace,name))\n\t\t\treturn PyCObject\n\t\telif self._typecodes[namespace] in (None, UNKNOWN):\n\t\t\t# No PyTypeCode class is detected for this name..\n\t\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__class__\n\n#\n# If you set up the extension with a.py suffix, you can tell which\n# suffix should be added to the extension name to find the right filename.\n# (For instance, 'foo_unittest.py'.)\n#\n\nif __name__ == '__main__':\n\timport sys, subprocess\n\n\tif sys.argv[-1] == '.py':\n\t\t_script = sys.argv[-2] + '.py'\n\telse:\n\t\t_script = sys.argv[-1]\n\tif sys.argv[-1] =="}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tparts = self.__class__.__module__.split('.')\n\t\tmoduleToResolve = self.__class__.__module__+'.'+parts[0]\n\t\t#import pdb;pdb.set_trace()\n\n\t\treturn getattr(importName(moduleToResolve), parts[1], None)\n\tbase = property(_getTargetClass)\n\n\n\nclass Paths(withName, WithData):\n\t\"\"\"A list of data object names that may be path-like objects\n\tat runtime. The standard library module 'os.path'\n\tis searched first, then '.'.  On platforms without an\n\texplicit"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn 'Py'\n\nclass CppCodeGenerator(CodeGenerator):\n\t\"\"\"\n\tThe C++ code generator, which generates the corresponding C++ code to\n\tuse in the compiler.  The code generator for C++ is generated by codeGenerator.generate\n\t(using gcc-like rules) via a CodeGenerator and used in the build files.\n\t\"\"\"\n\tdef __init__(self, lang):\n\t\tCodeGenerator.__init__(self, lang, 'cpp', CppCodeGenerator.C_SOURCE)\n\t\t\n\tdef codeGenerator(self, project):\n\t\t# Build the C"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tif self.isIntermediary():\treturn 'Py'\n\t\telse:                                 return 'Fallback'\n\tdef getCompromisedPath(self, target):\t\"\"\"\n\tCompute and return a fully-qualified path.\n\tThis is generally the equivalent of 'getCompiledPath'\n\tas it does for the case of symlinks to be created below\n\tthe source directory of'self.path'.\"\"\"\n\t\tname = self.name\n\t\tif _is_win32():\t\t\t\t\tname = os.readlink(name)\n\t\telse:\n\t\t\tname = os.path.abspath(name"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\nif __name__ == '__main__':\n\t\n\tprint '-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n'\n\tprint '-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n'\n\tprint '-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n'\n\t\n\timport os, sys, string\n\timport win32clipboard, win32con\n\tfrom win32com import universal\n\n\n\tclass _FilesystemImpl(posix.PosixPath):\n\t\t\"\"\"\n\t\tThis class is used to check all user commands and \n\t\tuse the win"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__obj__.im_func.func_name # @UndefinedVariable\n\n\t# --- Methods with an overridable __del__ operator ---------------------------\n\n\tdef __cmp__(fn1, fn2):\n\t\t\"\"\"\n\t\tCompare the function *fn1* and *fn2*.\n\n\t\tThis implements weak comparison and also checks for the\n\t\tsubclass operator '==' to do the comparability testing.\n\t\t\"\"\"\n\t\treturn (fn1 == fn2)\n\n\tdef __del__(self):\t# Comparator operator\n\t\t(fn1, tmp1) = self.__getTargetClass"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._mainClass\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tmodule = '.%s' % self.__module__\n\t\texcept AttributeError:\n\t\t\tmodule = ''\n\t\tif module == '__main__':\n\t\t\tmodule = ''\n\t\tif module == self.module:\n\t\t\treturn self.module\n\t\tmodule += '.%s' % self.__class__.__name__\n\t\treturn self.module\n\n\tdef to_slicer(self):\n\t\treturn self._getTargetClass()\n\t\t\n\tdef getImpl(self, className):\n\t\timpl ="}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.klass.split('.',1)[0]\n\tdef __str__(self):\n\t\treturn''.join(_strSplit(self.argStrs))\n\tdef asList(self):\n\t\t''' Return a comma-separated list, with each element indented\n\t\t\twith asData and a comma-separated list asHeader lines.\n\t\t'''\n\t\tiList = [_strSplit(self.argStrs)]\n\t\tiList.extend(self.asData)\n\t\treturn iList\n\tdef __repr__(self):\n\t\treturn '\\n'.join(_strSplit(self"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\timplClassName = self.meths[0].__class__.__name__\n\t\tif implClassName.endswith(self.suffix):\n\t\t\treturn implClassName[:-1]\n\n\t\treturn str.upper(self.suffix)\n\n\tdef testDefault(self):\n\t\timpl = self._getTargetClass()\n\n\t\tif impl == impl.__class__:\n\t\t\tif impl is None:\n\t\t\t\tif not self._isWrapped:\n\t\t\t\t\traise AssertionError(\n\t\t\t\t\t\"This is a C++ implementation, the C++ implementation \"\n\t\t\t\t\t\"was used as argument to a '%s'-method.\" % self.class"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\tout = copy.deepcopy(orderings)\n\tif len(out) == 0:\n\t\treturn out\n\tif 'x' in out:\n\t\treturn out['x']\n\tif 'y' in out:\n\t\treturn out['y']\n\tif 'z' in out:\n\t\treturn out['z']\n\n\tout['x-'] = out['x'] + out['y'] * out['z']\n\tout['y-'] = out['y'] + out['z'] * out['x']\n\tout['z-'] = out['z"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\ndef _mergesSorting(sortedList):\n\t\"\"\" This sorts multiple sorted objects\n\n\tUses the sorted list len(sortList) and orders it into a sorted version\n\tand returns the combined list.\n\n\t@Return: sortedList sorted in order of the original list\"\"\"\n\n\tl0, k0 = 0, len(sortedList) - 1\n\tfor j in range(l0):\n\t\tsortedList[j], k = j, k0\n\t\tl0 = l0 + 1\n\tl1 = len(sortedList)\n\tl2 = len(sorted"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\t# FIXME: This algorithm doesn't match the rest of the\n\t#      ordering that the last element is inserted.\n\t#      Use some sort algorithm, and keep a running list\n\tfor i in range(len(orderings)-1, -1, -1):\n\t\tif orderings[i][0]!= orderings[i-1][0]:\n\t\t\ttry:\t\t\torderedObjects.remove(orderings[i])\n\t\t\texcept ValueError, err:\n\t\t\t\tprint \"Error: remove failed for object: \" + orderings[i][0]\n\t\t\t\tprint \"Order"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tfor i, order in enumerate(orderings):\n\t\t# Use set to keep orderings sorted by the key\n\t\tkey = order[0]\n\t\tif key in orderingsToKeep:\n\t\t\torderingsToKeep[key].extend(order)\n\t\t\tdel orderingsToKeep[key][0]\n\t\telse:\n\t\t\torderingsToKeep[key] = order\n\t\n\ndef _mergeListOfLists(list1, list2):\n\t# Given a list of lists, merge it into a new list,\n\t# inserting each element in the list into the list a new list"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tresult = list()\n\t\n\tfor s in orderings:\n\t\tfor i, (o, p) in enumerate(s['left']):\n\t\t\t# Copy everything because there's no need to copy the\n\t\t\t# indices of the remaining orderings.\n\t\t\t# Note: We need to copy indices in the current order\n\t\t\t# to preserve orderings that were previously used.\n\t\t\t# NOTE: Since the indices of the remaining orderings\n\t\t\t# are always at the end, there is no point in keeping\n\t\t\t# the order the indices were originally given.\n\t\t\t# NOTE: Since the indices of"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n################################################################\n\n\n######\n#\n# EDROGENIUM CLASS\n#\n#\nclass EDROGENIUM(object):\n\t\"\"\"\n\tThis class implements the algorithm to perform a decently\n\tweighted local clustering of random data.\n\t\n\tThe algorithm is based on the Sakurai et al. (1999). The \n\talgorithm is based on the Sakurai et al. (2004, Lee and\n\tThirty 1993.\n\t\"\"\"\n\t\t\n\n\t#\n\t# PROBABILITY REGION\n\t#\n\t_prohibited = 'pro"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tresult = []\n\twhile orderings!= []:\n\t\tcurrent = orderings.pop(0)\n\t\trest = []\n\t\tfor obj in current:\n\t\t\tif obj is current[-1]:\n\t\t\t\trest.append(obj)\n\t\t\telif not hasattr(obj, '_order'):\n\t\t\t\tmsg = \"%r not found\" % (obj,)\n\t\t\t\traise TypeError, msg\n\t\tif rest:\n\t\t\tresult.append(rest)\n\tresult.reverse()\n\treturn result\n# Copyright 2014 Red Hat, Inc. All rights reserved.\n#\n#    Licensed under the Apache"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\nclass _NoValue:\n\t\"\"\"\n\tNo values allowed in an iterable\n\t\"\"\"\n\n\nclass _ItemsClass(object):\n\n\t\"\"\"\n\tMix-in for `object`s that does not allow item assignment\n\t\"\"\"\n\n\t__slots__ = ('type','sub', 'value')\n\n\tdef __init__(self, object, subtype):\n\t\t\"\"\"\n\t\t@type object: ItemBase\n\t\t@type subtype: str\n\t\t\"\"\"\n\t\tself.type = type(object)\n\t\tself.sub = subtype\n\t\tself.value = object\n\t\t\n\n\ndef"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\ndef _mergeElements(elements):\t\"\"\"\n\tCombine a sequence containing an element, with multiple elements\n\twill result in a list being used in place of a list of\n\telements.\n\n>>> _mergeElements([[], []])\n[]\n>>> _mergeElements([(), [], []])\n[]\n>>> _mergeElements([[{}, [], []], []])\n[]\n>>> _mergeElements([(1,2,3)], [1,2,3])\n[1,2,3]\n>>> _mergeElements((((1,2,3),),), [1,2,3])\n(["}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tresult = []\n\tfor itemList in orderings:\n\t\tfor obj in itemList:\n\t\t\tif obj in result: # found a match\n\t\t\t\tresult.remove(obj)\n\t\t\telse: # found a no match\n\t\t\t\tresult.append(obj)\n\t\t\t\n\n\treturn result\t\n\ndef insertBetterByPivotAbbr(order, order_index=-1):\n\t\"\"\"Insert items in Brush order that do not overlap\n\nUsage:\n\nUsage:\n\n>>> insertBetterByPivotAbbr([0, 1, 2, 3,"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn _get_method(object, '__provides__')\n\tdef _providedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tUsed by zope.interface to provide a version of the interface provided as a base.\n\n\tThis function should never be overridden.\"\"\"\n\t\treturn\n\n\nclass IDeclaration(object):\n\t\"\"\"\n\tDeclares a type declaration to a provided interface object.\n\n\tThe object is treated as a :class:`types.TypeDeclaration` instance,\n\tproviding methods attached as :class:`types.TypeDefinition`\n\tsince no specific implementations are provided"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tdef get():\n\t\t\tpass # pragma: no cover\n\n\n\t__allowClassmethods__  = IDeclaration\n\t__implements__ = IDeclaration\n\t__attributes__ = IDeclaration, \\\n\t\t__iterableAttribute__ = IDeclaration, \\\n\t\t__overloadAttribute__ = IDeclaration, \\\n\t\t__reprAttribute__ = IDeclaration\n\t__operations__ = IDeclaration, \\\n\t\t__callables__ = IDeclaration, \\\n\t\t__constantAttributes__ = IDeclaration # TODO: Is this too specific or safe??\n\n\t# TODO: I think this should be wrapped in a function,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn Object(IFoo, _interface=proxy._getClass(ifoo))\n\t\n# Attribute interfaces\n# ------------------------------------------------------------------------------\n# Interface objects\n# ------------------------------------------------------------------------------\n\nclass IDeclaration(_PyZopeInterface): \n\t\"\"\"\n\tZope interface for class IDeclarations.\n\t \n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\n\tdef _getClass(cls):\n\t\t\"\"\" Returns IDeclaration class object in interface object form. \"\"\"\n\t\t# FIXME: should we set the IInterface.__dict__ to None as well?\n\t\tmcl ="}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t    \n\treturn ObjectDeclaration(object)\n\t\ndef interfaceImplements(provideInterface):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IInterface`.\t\"\"\"\n\t\t\t\t\n\t\"\"\"\n\tIf an object is available with the interface, use it. Otherwise construct a new\n\tInterface.\n\t>>> class IFoo(Interface):\n\t... \tdef bar(self, x):\n\t... \t\treturn x * x\n\t\"\"\"\n\treturn IFoo\n\n\nclass ObjectDeclaration:\n\tdef __init__("}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\ttry:\n\t\t\tresult = object.interfaces\n\t\texcept AttributeError:\n\t\t\tvalue = _emptyInterfaceHolder\n\t\t\n\t\tif type(result) is _interfaceInfoType:\n\t\t\tresult = _InterfaceInfo(_result=result).copy() # pylint:disable=protected-access\n\t\t\n\t\treturn result\n\t\t\n\tif iotype is None or iotype == _emptyInterfaceHolder: # pylint:disable=redefined-builtin\t\n\t\treturn _emptyInterfaceHolder\n\telif callable(iotype) or isinstance(ivar, FunctionType): # pylint:disable=redefined-builtin\n\t\treturn _interfaceInfo(_result=None"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn object._declared(type_info)\n\treturn _empty(object)\n\t\ndef directlyProvidedByValue(value): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object.\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration` with the given value.\n\t\"\"\"\n\t return value._declared(type_info)\n\treturn _empty(value)\n\ndef object(): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the object used into the constructed object.\n\t\n\tThe `Zope"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tdef _resolve(iface):\n\t\t\ttry:\n\t\t\t\tmethods = iface.getMethods()\n\t\t\texcept (AttributeError, NotImplementedError) as ex:\n\t\t\t\tmethods = ()\n\t\t\tif not methods: # if we don't know about one of our bases then it's not a realiface\n\t\t\t\treturn None\n\t\t\tif isinstance(methods[0], IDeclaration):\n\t\t\t\tiface = methods[0]\n\t\t\tif iface == b'str': # it's a 'typed' interface\n\t\t\t\treturn None\n\t\t\tiface = interfaces.IDeclaration(iface)\n\t\t\treturn iface\n\t\n\tdef effectiveMethods(self,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn directlyProvides(IDeclaration)\n# XXX Zope interfaces\n\n\nclass ClassProvides(ZopeInterface):\n\t\"\"\"\n\tZope interface to register new classes.\n\n\tThese are registered for global lookup.\n\t\"\"\"\n\tdef registered(self, name):\n\t\t\"\"\"\n\t\t``REGISTRY[name]`` is a L{ZopeInterface} instance (or L{IDeclaration}).\n\n\t\tIf the given name is a class name, then for each, ``name`` belongs to the\n\t\tparent of this interface and the subinterfaces are in a namespaced\n\t\tnamespace called ``declarations``."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\n\tdef __reduce__(self): # pylint:disable=redefined-builtin\n        \"\"\"\n\t\tGet an iterator for the interfaces directly provided by this\n\t\tinterface.\n\t\t\"\"\"\n\t\t# The interfaces themselves are stored in a tuple.\n\t\treturn _LazyInterfaceIterator\n\nclass Declaration(_LazyInterfaceIterator):\n\t\"\"\"\n\tReturn a particular interface declared in the given interface list.\n\tThe implementation retrieves the interfaces directly provided by the given objects\n\tfrom the interfaces themselves.\n\t\"\"\"\n\n\tdef __init__(self, interface, interfaceList):\n\t\tProxyInterface._InterfaceClass"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\n\tdef attributePresets(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tReimplemented by interfaces to let this module locate a class\n\twhich implements IDeclaration.\n\nThe interface returned has the same interface as the original interface except\nit is a typeclass and it is the class that is used by the interface.\n\t\n\t\tExample\n\n\t       class IFoo(Interface):\n\t           @zope.interface.interfaces.IDeclaration\n\t           def foo(self):\n\t               pass\n\n\t       foo = A1(foo)\n\n\t       @zope.interface.interfaces.IDeclaration."}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n#---------------------------------------------------------\nclass MaximallySortedMinimized(BriefBase):\n\n\tdef __init__(self, baseClasses):\t\n\t\tsuper(MaximallySortedMinimized, self).__init__(baseClasses)\n\n\tdef run(self):\n\t\tlist(self.bases(False))\n\t\tbaseKeys = sorted(list(self.baseClassOrder))\n\t\tbasesToRemove = [key for key in baseKeys if key not in self.order]\n\t\tnumClasses = len(basesToRemove)\n\t\tfor current, maxCount in self.pairs(False):\n\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tminBases = [dclass for dclass in classes if type(dclass) is Base]\n\t\tif len(minBases)==1:\n\t\t\treturn minBases[0]\n\n\t\t# no, take minimum of it\n\t\treturn min(minBases)\n\n\ndef classOrdering(classes, name):\n\t\"\"\"\n\tSort a list of classes by class name\n\n\tBy default, this function returns name.  If name is omitted\n\tthen class order is used to get the name.\n\n\tThis makes sure that when you use the class name as input to the script"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tif not classes:\treturn 0\n\t\tminClassCount     = float(sum([1 for c in classes if not c[1] ]))\n\t\tfor c in classes:\ttry:\n\t\t\tbases = bases + [c.minimalBases()]*minClassCount\n\t\t\tminClassCount   += 1\n\t\treturn minClassCount\n\n\tdef computeProfitRatio(self, countsAndClasses, total=None):\t\t\"\"\"\n\tReturn the proportion of the recombination coefficient that is achieved\n\tusing the total number of words in the corpus. The proportion of the\n\trecomb"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbases = {}\n\t\tfor b in classes:\n\t\t\tif b.__bases__:\n\t\t\t\tbases[b.__name__] = b.__bases__\n\t\treturn bases\n\treturn reduceList(minimalBases(classes))\n\ndef splitClass(classname):\n\t\"\"\"\n\tSplit a string of the form 'ClassName of foo(Bar)':...\n\t\"\"\"\n\tparts = classname.split('.')\n\tname = '.'.join(parts[:-1])\n\treturn name\n\ndef isClass(classname):\n\t\"\"\"\n\tTest if a classname is a python class\n\tOtherwise, this checks"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbases = OrderedSet()\n\n\t\tfor c in classes:\n\t\t\tif not c.__bases__: bases.add(c)\n\t\t\telse:\n\t\t\t\tbreak\n\t\t\tbases.add(c.__bases__)\n\n\t\t# sort\n\t\tbases = list(bases)\n\t\tbases.sort(key=lambda x: x.priority, reverse=True)\n\n\t\treturn bases\n\n\t#\n\t#\n\t#\n\n\tdef _pruneClasses(self):\n\t\t\"\"\"\n\t\tPrune recursively all classes that are not members of base classes\n\t\t\"\"\"\n\n\t\t# get list of abstracts\n\t\tself._"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tif not classes:\n\t\t\treturn []\n\t\tif all(isinstance(c, Base) for c in classes):\n\t\t\t\treturn [c.minimalBases() for c in classes]\n\n\t\t# find the unique bases of all the base classes provided\n\t\tresult = []\n\t\tfor base in bases:\n\t\t\tif base in classes and not all(c.minimalBases() for c in base.classes):\t# if there is only a single base, we have a unique base and all the others\n\t\t\t\tresult += [base]\n\t\treturn result\n\n\tdef splitSOP(SOP,"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tc = reduce(min,classes)\n\t\twhile c:\n\t\t\tnext = c.next\n\t\t\tif next == maxi:\n\t\t\t\tbreak\n\t\t\tnext = next.next\n\t\treturn next\n\n\n\tdef apply_mro_classes(self,cls):\n\t\t\"\"\"\n\t\tApply all base classes, yielding the new type for the topmost\n\t\tgeneric base classes\n\t\t\"\"\"\n\t\t# collect all base classes \n\t\tbases =[]\n\t\tbases.extend(cls.mro())\n\t\tcls.bases = bases\n\t\t# apply all base classes\n\t\twhile classes:\n\t\t\tt = classes"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tif not any(classes):\treturn classes\n\tassert False\n\tbases = []\n\tfor x in range(0, len(classes)):\n\t\tif __isIterable(classes[x]): bases+=[x]\n\t\telse: bases+=[classes[x]]\n\twhile bases:\n\t\ti = len(bases)\n\t\twhile i:\n\t\t\tp = bases[i-1]\n\t\t\tbases[i-1] = bases[i]\n\t\t\ti -= 1\n\tbases.sort()\n\treturn bases\n\t\ndef toLowerBound(p):\n\ti ="}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tm = []\n\t\tfor c in classes:\n\t\t\tif c.className not in m:\n\t\t\t\tm.append(c.className)\n\t\treturn m\n\n\tdef reduce(thisDict, classDict):\n\t\tnewDict = {}\n\t\tfor name in thisDict.keys():\n\t\t\taddItem = False\n\t\t\tfor klass in classDict[name]:\n\t\t\t\tif klass in classDict and not addItem:\n\t\t\t\t\tnewDict[name] = list(thisDict.get(klass, []))\n\t\t\t\t\taddItem = True\n\t\t\t\telse:\n\t\t\t\t\tnewDict[name] ="}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tresult = []\n\t\tfor i in self:\n\t\t\tresult.append(i.minimalify())\n\t\treturn result\n\tdef min(self):\n\t\tmin = []\n\t\tfor i in self:\n\t\t\tif i.minify() == min:\n\t\t\t\tmin.append(i)\n\t\treturn min\n\tdef combine(self, other):\tself.extend(other)\n\tdef combine(self, other):\tself.extend(other)\n\tdef combineAll(self, other):\tself+=other.mini()\n\tdef subtract(self, other):\tself"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = self._attrs(all)\n\t\tret.sort()\n\t\treturn ret\n\n\tdef removeAttribute(self, name):\n\t\t\"\"\"\n\t\tRemove the given attribute from the set of this object and its children.\n\t\t\"\"\"\n\t\tremove = self.attributes.get(name, None)\n\t\tif remove is not None:\n\t\t\tdel self.attributes[name]\n\n\tdef _attrs(self, all=True): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn a list of the attributes of this object\n\t\t\"\"\"\n\t\tret = self._attrs_set"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = {}\n\t\tfor k in self._attrs.keys():\n\t\t\tv = self._attrs[k]\n\t\t\tfunc, defn = v.info()[0]\n\n\t\t\tdoc = ''\n\t\t\tif self._isInterface(v):\n\t\t\t\t# Get arg description.\n\t\t\t\t# Do not strip the'*'so that when someone wants to type\n\t\t\t\t#'since the default is documented at the top\n\t\t\t\tdoc = v.info().split(' * ')[-1]\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdoc = inspect.getdoc(v)\n\t\t\t\texcept:\n\t\t\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tnames = list(self._attributes.keys())\n\t\tif all:\n\t\t\treturn names + self.attributeDescs\n\t\tnames.extend(self._attributes.keys())\n\t\treturn names\n\n\tdef childElement(self, name, namespace=None, attributes=None,\n\t\t\tdepth=0):\n\t\t\"\"\"Create object to collect childNodes from an object's xml element.\n\t\tSome objects, such as <x:foo/> will not be reported.\"\"\"\n\t\treturn createChild(name, namespace, attributes, depth)\n\n\tdef readChildren(self, filter=None, nsPrefix="}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn map(lambda i: i[1], zip(*self.__namesAndDescriptions(all)))\n\n\t# TODO: remove name lookup since it won't be used by the template engine\n\tdef __str__(self):\n\t\treturn str(self.namesAndDescriptions())\n\n\tdef __repr__(self):\n\t\treturn repr(self.namesAndDescriptions(True))\n\n\t@property\n\tdef description(self):\n\t\t\"\"\"\n\t\tReturn description of interface, or [] if none are specified.\n\t\t\"\"\"\n\t\treturn self.__description\n\n# TODO: remove name lookup since"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tresult = []\n\t\t\n\t\tif all: # Returns all names and descriptions\n\t\t\tfor name in self.all_names_and_descriptions():\n\t\t\t\tresult.append((name, self.all_descriptions[name]))\n\t\t\t\tfor desc in self.all_descriptions[name]: # pylint:disable=redefined-builtin\n\t\t\t\t\tresult.append((name, desc))\n\t\t\n\t\treturn result\n\t\n\tdef all_names_and_descriptions(self): # pylint:disable=redefined-builtin\t\"\"\"\n\t\tReturn list of attribute names and descriptions defined by interface.\t\n\t\t\"\"\"\n\t\tnames = list()\n\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self.__attributes.keys()\n\n\tdef items(self, name):\n\t\t\"\"\"\n\t\tReturn items defined by name.\n\t\t\"\"\"\n\t\tresult = []\n\t\treturn result\n\n\tdef iterkeys(self): # pylint:disable=redefined-builtin\tTBD\n\t\t\"\"\"\n\t\tReturn an iterator on keys of the dict.\n\t\t\"\"\"\n\t\tresult = []\n\t\tfor attr in self.attrlist:\n\t\t\ttry:\n\t\t\t\titem = attrname2attrvalue(attr, None)\n\t\t\t\tif item is not None:\n\t\t\t\t\tresult.append(item)\n\t\t\texcept ValueError"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self.attributes.keys()\n\n\tdef __getitem__(self, item):\n\t\t# we use normal access to the dictionary of attributes\n\t\tif item not in self.__dict__:\n\t\t\treturn None\n\t\treturn self.__dict__[item]\n\n\tdef add_interface(self, iface):\n\t\tself.interfaces.append(iface)\n\t\treturn iface\n\n\tdef get_attributes(self, iface):\n\t\treturn self.__dict__[iface]\n\n\tdef __iter__(self):\n\t\treturn iter(self.__dict__.keys())\n\n\tdef clear(self):\n\t\tself."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tattributeNames = dir(self)\n\t\tattributeDescriptions = []\n\t\tfor name in attributeNames:\n\t\t\tattr = getattr(self, name)\n\t\t\tif isinstance(attr, (int, long, float, complex, basestring)):\n\t\t\t\tcontinue\n\t\t\tattributeDescriptions.append(name)\n\t\t\tif not name.startswith(\"__\"):\n\t\t\t\tattributeDescriptions.append(name)\n\t\treturn attributeDescriptions or self.__class__._attributeNames\n\n\tdef addMethod(self, methodFunction, *args, **kwargs):\n\t\t\"\"\"\n\t\tRegister a method function to be called upon a"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tassert self.__dict__[\"_element\"]\n\t\treturn list(self._element.values()) \\\n\t\t\t\t   if all else [self.__dict__[\"_element\"]]\n\n\n\tdef __getitem__(self, varname):\n\t\t\"\"\"Retrieve a component value by index or name or index.\n\t\t\"\"\"\n\t\tif varname == \"element\":\n\t\t\telem = self._element\n\t\telse:\n\t\t\telem = self._elements[varname]\n\t\t\t\t\n\t\treturn self._wrap(elem) if isinstance(elem, _str) else elem\n\n\n\n\tdef element(self, name): #pylint:disable=redefined-builtin\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tresult = []\n\t\tnamesAndDescriptions = []\n\t\tfirst = self\n\t\tfor i in self.attributeNames:\n\t\t\tnamesAndDescriptions.append( i.getName(True if all and all == False else False ))\n\t\t\tif i.getDescription():\n\t\t\t\tresult.append( \"    %-20s: %s\"%(i.getName(False), i.getDescription()) )\n\t\t\t\tfirst = True\n\t\t\telse:\n\t\t\t\traise AttributeError, \"Empty attribute name\", i\n\t\tresult.sort( lambda x,y: cmp(x.getName(True), y.getName("}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.__class__.attribute_names(self, all)\n\n\tdef attributes(self):\n\t\t\"\"\"\n\t\tReturn all attributes defined by the interface.\n\t\t\"\"\"\n\t\tattr = AttributeCollection(self.__class__)\n\t\tattr._all = self.all\n\t\tattr._name_map = self.name_map\n\t\tattr._default_type = self.default_type\n\t\treturn attr\n\n\tdef type_name(self, name):\n\t\t\"\"\"\n\t\tGet a name used in the interface for a given attribute name.\n\t\t@param name: the attribute name\n\t\t@"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tresult = self.get_interface().search_init('getiterprise.net.tcp', 'getiterprise')\n\t\tresult = list(result)\n\t\tif not all:\n\t\t\t# return ['GTF', 'GTF1', 'GTF2']\n\t\t\tresult = list(result)\n\t\treturn result\n\n\tdef get_interface(self):\n\t\t\"\"\"Return the interface object for the interface type.\"\"\"\n\t\tinterface_name = '{0}.{1}.{2}'.format(self.get_name(), \n\t\t\tself.get_class(), self.get_name())"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tres = []\n\t\tfor vname in self.vnames():\n\t\t\tif vname not in res: res.append(vnname)\n\t\tif all: return res\n\t\treturn attr_names(self.attr[0]) # pylint:disable=undefined-variable\n\tdef _getDescendents(self): return '%s%s' % (self._ns_prefix_, self._vnsdl_label_)\n\tdef _getAtts(self): return '%s%s' % (self._ns_prefix_, self.attr)\n\tdef _setAtts"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn sorted(self.__interface).keys()\n\t\n\tdef get(self, name):\n\t\t\"\"\"\n\t\tGet an attribute of the interface named @name.\n\t\t\n\t\tArgs:\n\t\t\tname: The name of the attribute to return.\n\t\t\"\"\"\n\t\t\n\t\treturn self.__interface[name]\n\t\n\tdef set(self, name, value):\n\t\t\"\"\"\n\t\tSet an attribute of the interface named @name to value.\n\t\t\n\t\tArgs:\n\t\t\tname: The attribute name.\n\t\t\tvalue: The new value of the attribute.\n\t\t\"\"\"\n\t\t\n\t\tself.__interface[name] = value\n\t\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\ttry:\n\t\t\tif all:\n\t\t\t\treturn dir(self.__class__)\n\t\t\telse:\n\t\t\t\treturn dir(type(self).__name__)\n\t\texcept Exception:\n\t\t\treturn []\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tGet the interface's names with the interface's I{names}() method.\n\t\t\n\t\tThe I{names} method is deprecated in favor of direct access to the\n\t\tdictionary returned by L{IConfig.data}. Returns a list of (2) items.\n\t\t\"\"\"\n\t\treturn iter(dir(self.__class__))\n\n\tdef get(self"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tresult = []\n\t\tfor name in self.get_attribute_names():\n\t\t\tif name not in ret:\n\t\t\t\tif all:\n\t\t\t\t\tresult.append(name)\n\t\t\t\telse:\n\t\t\t\t\twritemsg(Fore.RED + \"Attribute\", name + Fore.RESET, Fore.RESET)\n\t\t\tresult.append(name)\n\t\treturn result\n\n\tdef is_mandatory(self):\n\t\t\"\"\"\n\t\tCheck the validity of a mandatory command\n\n\t\tReturns\n\t\t-------\n\t\tbool\n\t\t\tTrue if the declaration of this is mandatory\n\t\t\"\"\"\n\t\tm = re.fullmatch"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn get_interface_attributes(self, attribute='name', all=all)\n\t# ------------------------------------------------------------------------------------------------------------------\n\tdef listNames(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn a list of attribute names for the interface.\n\n\t\"\"\"\n\treturn get_interface_attributes(self, attribute='listNames', all=all)\n# ------------------------------------------------------------------------------------------------------------------\n\tdef attribute(self, name):\n\t\"\"\"\n\tGet the interface attribute.\n\n\tArgument:\n\t\tname (str): The name of the attribute to retrieve.\n\n\tRaises:\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn dict(self.__dict__)\n\n\tdef __setattr__(self, name, value): # pylint:disable=redefined-builtin\n\t\treturn object.__setattr__(self, name, value)\n\n\t@property\n\tdef interfaces(self):\n\t\t\"\"\"\n\t\t:return: the interfaces defined on the Interface\n\t\t\"\"\"\n\t\treturn dict(self.__dict__)\n\n\tdef add_interface(self, key, interface):\n\t\t\"\"\"\n\t\tAdd a new interface to the interface table\n\n\t\t:param key: Key associated with interface\n\t\t:type key: :py:class:`str`\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(self.iteritems(onlyLocal=all, localOnly=True))\n\n\tdef items(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn a list of (name, item) pairs of the attr data.\n\t\"\"\"\n\t\n\n\tdef keys(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tYield the attribute names and the values\n\t\t\"\"\"\n\t\tfor (attrName, attrValue) in filter(operator.attrgetter('__name__'), self._attrName.split('.')):\n\t\t\tyield (attrName, getattr(self, attrName, None"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.interface.__dict__\n\t\t#return vars(self.interface)\n\n\t@staticmethod\n\tdef get_attribute(name):\n\t\t\"\"\"Return the attribute with the interface name.\"\"\"\n\t\tif name in _cached_attributes:\n\t\t\treturn _cached_attributes[name]\n\t\tif name in _non_exposed_methods:\n\t\t\tif name in _exposed_methods or name.startswith('__'):\n\t\t\t\tif name.startswith('do_'):\n\t\t\t\t\tmethod = 'do_' + name[2:]\n\t\t\t\t\t_attr = partial(_unexpose_type, method, name[2:])"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\tret = []\n\tfor element in sequence:\n\t\tif isinstance(element, Declarions) or issubclass(element, Declarions):\n\t\t\tret.extend(_normalizeargs(element, output=output))\n\t\telif isinstance(element, Tuple):\n\t\t\tret.extend(list(_normalizeargs(element, output=output)))\n\t\telif isinstance(element, Declarions):\n\t\t\tret.extend(list(_normalizeargs(element, output=output)))\n\t\telif isinstance(element, tuple):\n\t\t\tret.extend(list(_normalizeargs(element, output="}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif not sequence:\n\t\t\treturn\n\tif output is not None:\n\t\toutput %= sequence\n\ndef normalize_signature(signature, output=None):\tprint \"Transforming signature %s\" % signature\n\tprint \"    %s -> %s\" % (signature[0], signature[1])\n\tprint \"#%s -> %s\" % (signature[1], signature[2])\n\ndef normalize_interface(source, output=None):\n\tprint \"Transforming interface %s\" % source\n\tprint \"    %s -> %s\" % (source[0"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\tif 'args' in list(sequence.keys()):\t# If only one entry, return args[0] -> []\n\t\targs = sequence['args']\n\t\tif isinstance(args, str):\n\t\t\targs = args.split()\n\t\tfor arg in args:\n\t\t\tif not isinstance(arg, tuple) or isinstance(arg, list):\t\t\n\t\t\t\traise ValueError('Invalid Arguments specification')\n\t\t\ttry:\n\t\t\t\tret=dict()\n\t\t\t\tfor arg_element in arg:\n\t\t\t\t\tret[arg_element.arg, arg_element.arg_type]+={arg_element,"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\tif output is None: output = []\n\telif isinstance(output, list): output = list(output)\n\ndef _expandargs(sequence, output=None):\n\t\"\"\"\n\tExpand declaration arguments\n\nExclusion arguments may contain Declarions or tuples, or single interfaces.\t\"\"\"\n\n\n\tif output is None: output = []\n\telif isinstance(output, list): output = list(output)\n\ndef _apply(sequence, funcptr, signature_types, **keywords):\n\t\"\"\"\n\tApply declarations to an instruction by applying a callable function onto said\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t # output is a hack to make sure we can ignore any errors in the\n\t\t# code that needs to dismiss them.\n\t\tif isinstance(sequence, Declarion):\n\t\t\tsequence = Sequence(*sequence)\n\t\treturn sequence\n\n\tif output == None:\n\t\toutput = []\n\n\tfor n in sequence.__class__.__mro__:\n\t\tif n in (Declarion, Sequence):\n\t\t\toutput += _normalizeargs(n, [])\n\t\telse:\n\t\t\toutput += [n]\n\n\treturn output\n\n# This only exists for backwards compatibility purposes...\n# Removed in Python"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\nclass DeclubFactory(object):\n\n\tdef create(self, *interfaces):\n\t\t\"\"\"\nCreates a new interface and registers it in self._decls, a set of\n\tinstances that this class may invoke.\n\"\"\"\n\t\t\n\t\tfor (instance, interface) in interfaces:\n\n\t\t\tfor deco in inspect.getmro(interface):\n\n\t\t\t\tself._decls[deco.split('.')[-1]].register(instance)\n\t\t\t\n\t\t\t\t# we want to keep track of all the instances alive when \n\t\t\t\t# the interface is killed\n\t\t\t\talive = not deco.alive\n\n\t\tself._de"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\timport warnings\n\t\twarnings.warn(\"decompose requires a full 'demangle' interface\",\n\t\t\t\t\t\t\tDeprecationWarning)\n\t\treturn sequence\n\t\n\tif output is None: output = []\n\n\tnumargs = 0\n\tfor arg in sequence:\n\t\tif isinstance(arg, tuple) and isinstance(arg[0], Declarions):\n\t\t\tnumargs += 1\n\t\telif isinstance(arg, str):\n\t\t\tnumargs += 1\n\t\telif isinstance(arg, Declarions):\n\t\t\t_normalizeargs(arg, output)\n\t\t\tnumargs = 1\n\t\telse:\n\t\t\tassert"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\tassert isinstance(sequence, tuple)\n\tassert len(sequence) > 0\n\tif output: \n\t\toutput.write(\"\"\"  :::   Aligned:::   %s\n\n\t\"\"\"%(list(sequence.keys())[0]))\n\t\toutput.write('\\n')\n\telse: \n\t\toutput.write(\"\"\"  ::: Aligned::: No Aligned:::   %s\\n\n\"\"\"%(\"\"))\n\t\n\t#-------------------------------------------------------------------------------\r\n# Name:        cmo.py\r\n# Purpose:\r\n#\r\n# Author:      Patrick"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n            return list(sequence)\t\n\t\n\tif output is not None:\n\t\timport warnings\n\t\twarnings.warn('No longer used \"verbose\" argument of the \"verbose\" '\n\t\t\t\t       'keyword in Glyph: use \"verbose:quiet=True\" instead',\n\t\t\t\t       DeprecationWarning)\t\n\t\n\tif len(sequence) == 1:\n\t\treturn sequence[0]\t\n\t\n\tif not isinstance(sequence[0], (tuple, list)):\t# Single element sequence\n\t\treturn tuple(sequence)\n\t\t\n\tif len(sequence) == 1:\n\t\treturn sequence[0]\n\t\n\ttry:\n\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n    output = output or [] # Avoid warning about unknown inputs\n\ndef _add_definitions(definitions):\n    output.extend(defs)\n\ndef _add_typedefs(definitions):\n    for base in definitions:\n\tfor typedef in re.findall(r'interface\\s+template\\s+(?P<type>[a-zA-Z]+)\\s+public', base):\n\t    if ':' in typedef.__name__:\n\t\toutput.append(typedef)\n\t    else:\n\t\toutput.append(typedef(typedef))\n\t\n\tdefinitions = [output]\n    \n    return definitions\n\ndef"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\ndef _optimize():\n\n\tsys.settrace(\n\t\tlambda *args: _c_optimize_opt(len(args))\n\t\t)\n\n\ttry:\n#\t\t_optimize_()\n\t\t_optimize_()\n\texcept ImportError:\n\t\traise ImportError(\"Optimizability support unavailable.\")\n\n#\texcept (AttributeError, ValueError):\n\t\tpass\n\n\t\t#except:\n\t\t#\tpass\n\n\treturn _c_optimize_opt(0)\n\n\ndef _optimize_opt(idx_x, idx_p):\n\t\"\"\"\n\tReturn the optimized optimization function.\n\nIf"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t if __debug__:\n\t\t\tfrom pycket.values_types import real_types_tree\n\t\t\treturn real_types_tree.values.get_optimized_by_name('OPTIMIZEDC')\n\t\traise ImportError(\"no optimizations\")\n\nif __name__=='__main__':\n\timport doctest; doctest.testmod()\n\n\t_optimize_with_opt_cache = _c_optimizations_available()\n\tprint(\"using optimize with optimization cache %s\" % repr(_optimize_with_opt_cache))\n\"\"\"Code to run tests using pytest.\n\nTo run tests"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\traise ImportError(\"To use the optimization modules, you \"\n\t\t\t      \"should try to import all of the following modules:\\n%s\"\n                % ', '.join(_c_module_names()))\n\ndef _set_optimizations():\n\t\"\"\"\n\tCheck the optimizations. Returns a string containing a description\n\tof the algorithm itself.\n\n\tThe optimizations are stored in the list _c_optimizations.\n\tIt must be a list containing the names of optimizations or None.\n\t\"\"\"\n\t_c_optimizations_available()\n\t_c_optimizations = [(name"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n\t\t\n\t\tif find_module('c_optimization') is not None:\n\t\t\treturn True\n\n\t\ttry:\n\t\t\treturn __optimizations__['c_optimizations'][0]\n\t\texcept KeyError:\n\t\t\traise ImportError('Cannot find the optimizations \"%s\"' %\n\t\t\t\t(__optimizations__['c_optimizations'][0],))\n\treturn False\n\ndef _c_optimizations_unavailable():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n\n\treturn __optimizations\n    \n\n\n\ndef _c_create_table(_table):\n    \"\"\"\n    Create a new empty C-accessible tables list in the given table.\n    \"\"\"\n    global _c_tables\n    _c_tables = [None] * len(_table)\n    for i in range(len(_table)):\n        table_name = _table[i]\n        try:\n            _c_tables[table_name] = _table[i]\n        except IndexError:\n            raise DatabaseError(\"%s is not a valid c-accessible table\" % table_name)\n        \n        \ndef _c_delete"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tguards = [\"gmp\", \"gmpy\"]\n\t\tfor gmp in guards:\n\t\t\ttry:\n\t\t\t\timport gmpy\n\t\t\texcept ImportError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\treturn gmp\n\t\traise ImportError(\"gmpy is required but not installed.\")\n\n_use_gmpy = _c_optimizations_available()\nif _use_gmpy:\n\tdel _use_gmpy\n\ndef _use_xperf():\n\tfrom distutils.spawn import find_executable\n\n\tfrom distutils.errors import DistutilsExecError\n\n\timport subprocess as sp\n\t"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn _c_optimizations() in [True, False]\n\ndef _get_optimize_flag():\n\t\"\"\"\n\tReturn the optimization flag, used to get running on a given\n\toptimization level.\n\n\tThis returns either 0 (normal), 1 (set optimal), or 2\n\t(optimizations available).\n\n\tThis is useful for debugging purposes.\n\t\"\"\"\n\treturn _get_optimization_level() == 1\n\n# The following functions and classes are used to define\n# OptimizationLevel objects.  They should be accessed\n# directly, rather than hard-coding them until"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\traise ImportError\n\n# This module is only used by the import system at the moment\n# since it can't be done when the __main__ module isn't yet\n# available (so it can't be copied to other files).\ndef get_optimization_module():\n\tif sys.hexversion < 0x3000000:\n\t\traise ImportError\n\treturn sys.modules[__name__]\n\n\n# Make our imports available as modules so they can be imported.\ndef _import_all_mod_modules(modules):\n\tglobal _imported_modules\n\t_imported_modules = []\n\t"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\t# It is important to check for the existence of the\n\t\t# built-in optimizations, so that one can always be\n\t\t# used when needed by the extension modules.\n\t\tif not _initialized:\n\t\t\treturn False\n\n\t\tmodules = []\n\t\tfor mod_str in _mod_strdb:\n\t\t\tmod, _ = load_module(mod_str)\n\t\t\tmodules.append(mod)\n\n\t\tif len(modules) < 2:\n\t\t\tif len(modules) == 0:\n\t\t\t\traise ImportError(\"Unable to load the C version\")\n\t\t\treturn  # This is"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\ttry:\n\t\t\timport greedy_clifford.optimize\n\t\texcept ImportError:\n\t\t\treturn None\n\t#if there are no optimizations in the searchList return...\n\tif _c_optimizations_available.success==False:\n\t\treturn None\n\telse:\n\t\treturn _c_optimizations_available.success\n\t\ndef _c_optimize_fct_wrapper(fct):\n\t\"\"\"\n\tA wrapper function to create the Optimizer class by performing basic\n\toperations on the C program.\n\n\tThese are wrapped to allow optimizations to be defined in conjunction"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn (sys.argv[-len(sys.argv) - 1] == 'bdist_egg' and\n\t\t\t\tos.environ.get('PURE_PYTHON') == 'python')\n\n\tif 'PURE_DISTUTILS' in os.environ:\n                return _should_attempt_c_optimizations()\n\n\tif 'PURE_ENABLE_EXAMPLES' in os.environ:\n                # Need an environment variable to control PURE_ENABLE_EXAMPLES, rather\n\t\t\t # than to set a default. The config module only supports a\n                #'set_options' target, which overrides this method,"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif _use_c_impl is not False:\n\t\t\treturn _use_c_impl\n\n\t\t# If we're on a CPython/PyPy-compiled / Vala, then do nothing.\n\t\t\n\t\tif sys.platform == \"win32\":\n\t\t\treturn None\n\t\telse:\n\t\t\treturn _True\n\n# Check whether Python supports a specific c-based interpreter.\n\n# I don't know if they're going to support any of the versions of Python installed,\n# so just use whatever's provided for now.\nif sys.hexversion >= 0x3030000:\n\tdef"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif is_py3:\n\t\t\treturn True\n\n\t\tenv_var = get_env_var()\n\t\tif env_var == 'NPY_DISABLE_PLUGIN_AUTOLOAD':\n\t\t\treturn True\n\n\t\tif env_var == 'IUSE_CMAKE':\n\t\t\t# On Windows, the _USE_CMAKE environment variable was set to\n\t\t\t# the new value (issue #2754) if a var is added under the C\n\t\t\t# section.  But we can't put the environment variable in\n\t\t\t# the.config subdirectory because it might not exist on the \n\t\t\t# filesystem."}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif os.environ.get(\"PURE_PYTHON\") is not None:\n\t\t\treturn os.environ[\"PURE_PYTHON\"] == \"python\"\n\t\treturn False\n\ndef get_c_optimizations():\n\t\"\"\"\n\tReturn a mapping of C optimization functions to their implementations.\n\t\"\"\"\n\tif not _use_c_impl:\n\t\treturn {}\n\telse:\n\t\treturn {\n\t\t\t'c_optimize': ('ccompiler', {'threads': 0}),\n\t\t\t'compiler.cpp': ('ccompiler', {'threads': 1}),\n\t\t\t'compiler.c':"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\twarnings.warn(\"C optimizer optimizations have been \"\n\t\t           \"deprecated, use 'optimizations_c' instead\",\n\t\t           DeprecationWarning, stacklevel=2)\n\t\treturn (self._use_c_impl and sys.flags.optimize == 2\n\t\t\t\tor self.c_overridable)\n\n\tdef use_c_runtime_warnings(self):\treturn self._use_c_impl or self.c_overridable\n\n\tdef run(self, show_progress=None, force=False):\n\t\t\"\"\"\n\t\tRun the module and display a progress message if the code is"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\ttry:\n#\t\t\treturn PURE_PYTHON is not None and os.environ[\"PURE_PYTHON\"] == \"true\"\n\t\t\t# The following is only used when the C implementation is\n\t\t\t# compiled with CPython.\n\t\t\treturn (sys.version_info >= (3, 8) and isinstance(PURE_PYTHON, int))\n\t\texcept AttributeError:\n\t\t\t# There aren't any C extensions...\n\t\t\treturn False\n_can_use_c_string = _can_use_c_impl\n\n\tif platform.python_implementation() == \"Jython\":\n\t\t_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif sys.platform!= \"win32\": return False\n\t\ts = os.environ.get('PURE_PYTHON')\n\t\tif s in ('true', 'ye') or s.startswith('ye'):\n\t\t\treturn True\n\t\tif s.startswith('distutils') and '64bit' in sys.version.lower():\n\t\t\treturn True\n\treturn False\n\ndef use_c_wrapper(old):\n\t\"\"\"\n\tDetect if we're on CPython. If so, we'll use c_api if available.\n\tThere is the file *c__name* (without the suffix"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif is_py3() or os.environ.get('PYPY', \"\").lower() == 'c99':\n\t\t\treturn True\n\n\t\treturn getattr(sys, 'python_implementation', None) is not None\n\ndef _need_to_set_interpreter_profile(func):\n\t\"\"\"Throw an error if we're trying to change the interpreter's\n\tprofile for PYTHONSTARTUP. This will result in the ``PYTHONSTARTUP`` environment\n\tvariable being set and a warning will be logged to stderr.\n\n\tWarning: The function call that uses ``PYTHONSTARTUP`` from the\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn (environ.get('PURE_PYTHON') and\n\t\t\tenviron.get('PURE_PYTHON_CFLAGS') == '-O2')\n\n\trerun_is_run = _should_attempt_c_optimizations()\n\n\tif not os.environ.get('PURE_ASSUME_SKIP', False):\n\t\treturn True\n\n\treturn bool(rerun_is_run)\n# Copyright 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tc_key = 'PURE_PYTHON'\n\t\twhile True:\n\t\t\tenv_val = os.environ.get(c_key, None)\n\t\t\tif env_val is not None:\n\t\t\t\treturn c_val\n\t\t\tuser_val = os.environ.get('USER', None)\n\t\t\tif user_val is not None:\n\t\t\t\treturn user_val\n\treturn None\n\n# ================================================================================\n# Optimizer registry\n# ================================================================================\n\nregistry = GlobalRegistry(USE_I18N=False)\n\n#------------------------------------------------------------------------------\n# Compiling options\n# ================================================================================\n"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn ()\n\n\tdef __create_options_from_options__(self):\n\t\t\"\"\"\n\t\tReturn a list of `Option` objects which are to be copied from\n\t\t`Options` instances, and the `process` attribute.\n\t\t\"\"\"\n\t\treturn []\n\n\tdef __init__(self):\n\t\tself._process        = None\n\t\tself._options_data = {}\n\t\tself._options_map = {}\n\n\t# -----------------------------------------------------------------\n\n\tdef _init_process(self, process_options):\n\t\t\"\"\"\n\t\tCheck that we were given any parameters and set the options. If"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\t_deprecated_attr(\"c_optimizations_ignored\", \"C code was renamed `c_optimizations_required` to `c_optimizations_used` and `c_optimizations_ignored`.\")\n\t## \tif _deprecated_attr(\"c_optimizations_used\", \"C code was renamed `c_optimizations_used` to `c_optimizations_used` and `c_optimizations_used`.\", DeprecationWarning) is None:\n\tif _deprecated_attr(\"c_optimizations_used\", \"This feature is experimental.\") and _deprecated_attr(\"c"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\tdef _c_optimizations_required(self):\t\"\"\"\n\tCheck if the Optimizations on this class should be automatically ignored.\n\t\n\tChecks a class's __requires__(__requires__ method) method for all the required methods.\n\tIf this method returns True, then the missing methods are removed from this module.\n\t\n\t\"\"\"\n\treturn True\n\n\t\"\"\"\n\tdef _c_optimization_enabled():\t\"\"\"\n\tEnable the method in this class as enabled or all its respective classes.\n\t\n\tThis function will be used to register the optimization function with a few useful features:"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn False\n\telif _C_OPTIMIZATION_IGNORED: # Ctrl+C - special handling of _specialize\n\telif _C_OPTIMIZATION_IGNORED_REXP: # Ctrl+C - special handling of _specialize\n\t\treturn False\n\telse:                                        # Ctrl+C\n\t\treturn True\n#!/usr/bin/env python2\n# vim:fileencoding=utf-8\nfrom __future__ import (unicode_literals, division, absolute_import,\n                        print_function)\nfrom future_builtins import map\n\n__version"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn '_c_optimizations_required'\n\n\td =  {\n\t\t'flags' : (_c_flags_required, _c_optimizations_required ),\n\t\t'path': (_filename_required,   _filename_required ),\n\t\t'divider_width': (_maxlevel - len(_prefix_chars)),\n\t\t'decoration'\t: _prefix_chars,\n\t\t'name': (_filename_required,   _filename_required ),\n\t\t'name_first'\t: _minlevel,\n\t\t'bidi_rtl' : decls,\n\t\t'line"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\n\n\nif not _c_optimizations_initialized:\n\t_c_optimizations_initialized = True\n\n\ndef p_optimizations_required(p):\n\t\"\"\"\n\t``p_optimizations_required``\n\n\t``\n\tThe return type is a tuple ``(p_p, *args)``.\n\t\"\"\"\n\n\t# noinspection PyProtectedMember\n\tparts = p.split(',')\n\n\tp_p = parts[0]\n\n\tfor p_p_i in parts[1:]:\n\t\tp_p = p_p_i.strip()\n\t\tif"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\n\t#\n\t#   _______________________________________________________________________\n\t#\n\t#   Q_c_optimize(self, q)\n\t#\n\t#   See Also\n\t#   --------\n\t#\n\t#\n\n\n###############################################################################\n#\n#   Base Optimization\n#\n\n\nclass BaseOptimize:\n\t\"\"\" Base class for the Optimizer.\n\t\"\"\"\n\n\tdef __init__(self, odesys, pstl_type):\n\n\t\t#\n\t\t#   Init\n\t\t#\n\t\tself.time_limit = None\n\n\t\t#\n\t\t#  "}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn _c_optimizations_required\n\n\tdef _c_optimizations_required(self, opt2):\t# pylint: disable=W0613,W0621\n\t\t\"\"\"\n\t\tTests for the given optimization, without requiring it for all the other instances of the same optimization.\n\t\t\"\"\"\n\t\treturn _c_optimizations_required(self, [opt2], opt1, opt2)\n\n\tdef _c_optimizations_reused(self, opt2):\t# pylint: disable=W0613,W0621\n\t\t\"\"\"\n\t\tTests for the given optimization"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn C.DEFAULT_OPTIMIZATIONS - frozenset([D.DEFAULT_OPTIMIZATIONS])\n\tdef _c_optimizations_required():\treturn C.DEFAULT_OPTIMIZATIONS - frozenset([D.DEFAULT_OPTIMIZATIONS, C.OPTIMIZATIONS])\n\tdef _c_validations_for(self, var):\n\t\t(i1, i2), (i3, i4), (i5, i6), (m1, m2),(m3, m4)) = self._unify((self.grammar.find(var), self"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tv = get_var_with_list(\n\t\t\t'OPTICS', 'Optimization', 'Version', 'Parameters', 'Optimizer', None\n\t\t) or get_var_with_default(\n\t\t\t'OPTICS', 'Optimization', 'Version', 'Parameters', 'Optimizer', None\n\t\t) or None\n\t\treturn v\n\n\t@_requires_kernel()\n\tdef _c_optimizations_required_for_msvc(self, options_version):\n\t\t\"\"\"\n\t\tReturns :obj:`True` if there are no matching - for a specific\n\t\tversion of MSVC.\n\t\t\"\"\""}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn \"PURE_PYTHON\" in os.environ\n\n_use_c_impl_specified = False\t\t# This is True if any C implementation is desired,\n\t# and is used. The value indicates that we really want C code if\n\t# running in a Python interpreter (at least with C code available).\n\t_use_c_impl = os.environ.get('PURE_USE_C_IMPL')\t# None when running standalone (C, C++,...)\n\t_use_python_impl = False\t\t\t# False when running on Python built with GNU"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif sys.version_info >= (3, 9):\n\t\t\treturn True\n\telif sys.version_info < (3, 5):\n\t\treturn sys.version_info >= (3, 5)\n\n\tc_supported_versions = [\"3.5.3\", \"3.5.4\", \"3.6.4\", \"3.7.0\"]\n\n\t# Try again\n\tc_supported = []\n\tfor version in c_supported_versions:\n\t\ttry:\n\t\t\tif c_support_code_map[version].has_key(c_supported"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn conf_opt.get(option) and conf_opt or False\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturn True if the C implementation is available\n\t(i.e., it looks in.c files) and False otherwise.\n\t\"\"\"\n\tif conf_opt.get(DISABLE):\n\t\treturn False\n\tif conf_opt.get(OPT_OPTIMIZATION):\n\t\tfor flag in conf_opt.get(OPT_OPTIMIZATION):\n\t\t\tif flag[:2] == \"-O\" and flag[2:3] == \"-arch"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _use_c_impl is not None\n#!/usr/bin/env python\n#\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\""}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn True\nc_optimizations_required = _use_c_impl\n\ndef _get_optimization_flags(flaglist, uselib_store=None):\n\t\"\"\"\n\tReturn a _OptimizationCache instance representing the current optimization flags\n\n\t@type flaglist: List of string or List of string\n\t@param flaglist: List of optimization flags\n\t@type uselib_store: L{eenv.uselib.EenvStore}\n\t@rtype: L{optimization_string_t}\n\t@return: The optimization flags flags\n\t\"\"\"\n\n\tif use"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn bool(os.environ.get(\"PURE_PYTHON\"))\n\ndef enable():\n\t\"\"\"\n\tEnable C extensions\n\t\"\"\"\n\tc_exts = dict((e[:-2], e[-2].split(\"-\")) for e in env_c_ext)\n\tconfigure(\"--without-docs\")\n\tconfigure(\"--enable=all\")\n\"\"\"Sphinx extension to create links to Python's documentation.\n\nTo create a Python API documentation, execute this module from your page:\n\nhttp://www.python.org/peps/pep-0352/\n\nor in a"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif not sys.flags.optimize:\n\t\t\treturn False\n\n\t\t# Some versions of CPython do not support the __stdcall\n        # implementation, so fall back to the pure Python implementation if possible.\n\t\timport platform\n\t\tif platform.python_implementation() == 'CPython' \\\n\t\t\tand sys.maxsize > 2**32:\n\t\t\timport warnings\n\t\t\twarnings.warn(\"the c runtime does not run with optimization <= 2.64. \" \\\n\t\t\t\t\"See gh-9749 for more information.\",\n\t\t\t\tDeprecationWarning)\n\n\t\tif len(os.uname()) >= 2 and sys.platform"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\twarnings.warn('use of c_optimizations_required is replaced by'\n\t\t\t\t\t\t' PURE_PYTHON and PURE_STATIC instead',\n\t\t\t\t\t\tDeprecationWarning, stacklevel=2)\n\t\treturn True\n\n\tdef _use(self, dist):\n\t\t\"\"\"\n\t\tReturn a bool indicating whether the environment requires to be made\n\t\twithin the context of an appropriate distribution. If the C\n\t\tenvironment requires requires, it is returned. Otherwise, this\n\t\tfunction does nothing.\"\"\"\n\t\treturn dist.location.needs_cache\n\n\tdef _clear_cache(self, dist):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _use_c_impl\n\n_use_c_impl = getc_supported_c_implementation()\n\ndef _uses_osmesa():\n\t\"\"\"Return if OS X is supported on the system.\n\nThis uses the *installed OSXYZ and *min_osmesa* system call. If you have\nOS X with NVIDIA installed, this returns True.\n\"\"\"\n\treturn _supported_oses\n\n\ndef _get_supported_oses():\n\t\"\"\"Get a list of the supported OS X installs.\n\nThis returns:\n\tA list of strings.\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t        #: Indicates if the C implementation takes care of C code\n\t        #: generation.\n\t        c_ok = True\n\n\tret = False\n\ttry:\n\t\tret = c_ok and _use_c_impl\n\t\t__USE_C_PROTECTION = 1\n\texcept ImportError:\n\t\tpass\n\n\treturn ret\n\n# For internal use only; no more special constants are used for variable usage.\nC_OPTIMIZED = 0x0001\nC_OPTIMIZE = 0x0002\n\ndef _needs_pxf():\n\treturn not_static(C_OPTIM"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself._init = {}\n\t\tself.set_data(initial_value, make_bins())\n\t\n\tdef set(self, initial_value=None, make_bins=True):\n\t\timport numpy as np\n\t\tif initial_value is None:\n\t\t\tinitial_value = {}\n\n\t\tif not hasattr(self,'max_bins'):\n\t\t\tself.max_bins = int(np.max(self.data['data']))\n\n\t\tif not (make_bins and isinstance(self.max_bins, int) and self.max_bins > 0):\n\t\t\traise ValueError"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\t# NOTE: this assumes that `self.binlabels` has already been filled by the other functions.\n\t\t# This is just for convenience and may be replaced later.\n\t\treturn False\n\n\tdef binlabels(self, *args):\n\t\treturn self.initial_value\n\n\tdef make_bkg(self, bkg_profile):\n\t\t\"\"\"\n\t\tGenerate a bidimensional array of bins and calculate\n\t\tthe bin centre.\n\t\t\"\"\"\n\t\tbin_centers, bin_widths = self.bin_centers(\n\t\t\tbkg_profile, self.bins,\n\t\t\tself"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tassert self.initialized, \"Can't reset an histogram before initialize\"\n\t\tself.histogram = {'init':self.initial_value}\n\n\t@classmethod\n\tdef from_param(cls, obj, section, option, default):\n\t\treturn cls(obj, section, option, default, 'b')\n\n\tdef _copy_param(self, obj, section, option, force_copy=False):\n\t\tif obj.has_key(section):\n\t\t\tobj[section].update(force_copy=force_copy)\n\t\telse:\n\t\t\tobj[section] = copy"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tpass\n\ndef make_bins(self):\treturn {}\n\ndef apply_hist(hist, data):\n\tif self.initial_value:\n\t\tif self.make_bins():\n\t\t\thist['bins'] = self.initial_value\n\t\telse:\n\t\t\thist['bins'] = [1.0]\n\treturn hist\n\ndef do_graph(n):\n        g = Graph()\n\t\tg.add_nodes_from([\"B\", \"M\", \"E\", \"F\", \"Ez\", \"M1\"], [\"x\", \"y\", \"z\"], [\""}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself._init_values = defaultdict(dict)\n\t\tself._make_bins()\n\t\tself._initial_values = {}\t# The initial state of the histogram.  \n\n\tdef update_initial(self):\n\t\tself._initial_values.clear()\n\t\tself._make_bins()\n\t\tself._initial_values = {}  # The initial state of the histogram.  \n\t\t\n\tdef _check_init_values(self):\t\n\t\tfor initial_vals_dict, bins in self._initial_values.iteritems():\n\t\t\tassert isinstance(initial_vals_dict, dict"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.histogram = {}\n\t\tself.initialized = False\n\n\tdef __str__(self):\n\t\t\"\"\"\n\tPrint a nice one line summary of the histogram.\n\nArgs:\n\tn -- the number of iterations\n\tf -- the fifth element\n\nReturns nothing\n\t\"\"\"\n\t\t# print the results\n\t\tprint \"Original histogram is\",\n\t\tprint self.histogram.keys()\n\t\tprint\n\t\tfor i in range(1, n+1):\n\t\t\tprint \",\".join([str(k) for k in self.histogram[i]])\n\t\t\tfor v in self."}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.clear()\n\t\tfor k in self.default:\n\t\t\tself[k] = self.initial_value\n\n\t@property\n\tdef bins(self):\n\t\t\"\"\"\n\tA list of bins.\n\nThe keys of the dictionary are the names of the bin names and counts:\n\n\tThe first (x) entry is the number of points.\n\tThe second (y) entry is the counts of the values that were the\n\tcounts at this bin.\n\tThe bins with the same name are returned in the keys\n\tcolumn and the density is returned in the value list."}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n        self.clear()\n        self.reinit()\n        self._initialize_values()\n\n        self.plot.setData( np.array([[ 0. ], [ 1. ], [ 0. ]]), self.yvals )\n        self.plot.draw()\n\n    def set_axis_title(self, title):\n        self.axis_title.setText(title)\n\n    def set_axis(self, values, force=False):\n        \"\"\"\n        Set the axis from the histogram.\n\n        Use this only after the first call to reset\n\n        * *values* is the values to use for the axis\n        * *force"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tfor key in self.inputs.keys():\t\n\t\t\tself.bins[key][:] = []\n\t\t\n\t\tfor key in self.outputs.keys():\n\t\t\tinputs = []\n\t\t\tfor node in self.bins[key]:\n\t\t\t\tinputs.append(self.bins[key][node])\n\t\t\t\t\n\t\t\toutputs = []\n\t\t\tfor edge in inputs:\n\t\t\t\ttry:\n\t\t\t\t\tnode = edge.getOutputs()[0]\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tnode.setInitValue(self.initial_value[node])\n\t\t\t\t\texcept:\n\t\t\t\t\t\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\ndef binedge(bins, start_value, end_value, bins_per_octave=None):\n\t\"\"\"\n\tCreate bins of len(bins) evenly divisible by start_value and\n\tend_value. Then creates bins from bins_per_octave to fill with of\n\tthe appropriate number of bins (bins_per_octave). For example, if the\n\tbins length is 12, then the bins are 12 bins. If the bins_per_octave\n\tis specified, then the width argument is used for the histogram.\n\t\"\"\"\n\t"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tlines = self._csv_reader.readlines()\n\t\tif header:\n\t\t\t# header is only valid for lines matching `separator`\n\t\t\tseparator = \"  \" if separator is None else \"\\n ..\".join(header)\n\n\t\tcolumns = []\n\t\tdata = []\n\t\tfor lines in lines:\n\t\t\tif separator is None:\n\t\t\t\tcolumns = lines\n\t\t\telse:\n\t\t\t\tfor column in _csv_to_list(lines, separator=separator):\n\t\t\t\t\tif len(data)!= len(columns):\n\t\t\t\t\t\tdata += [column.rstrip('.')]\n\t\t\t\t\t\tcolumns = ["}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.to_csv() if input_format \\\n\t\t\telse to_csv(self, separator, header)\n\n\tdef is_blank(self): self.blank = True\n\tdef is_edge(self):self.e_type=\"e\"\n\tdef is_complete(self):self.b_type=\"b\"\n\tdef is_complete_by(self):self.c_type=\"c\"\n\tdef is_cyclic(self):self.b_type=\"b_cyclic\"\n\tdef is_cyclic(self):\n\t\treturn self._cyclic\n\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.to_csv(separator, header)\n\n\tdef to_latex(self):\n\t\t\"\"\"\n\tConvert the graph to LaTeX.\n\t\n\tIf the graph data is structured in dot language, it will be\n\treturned as such: ::\n\t\t|--> node1_x || node1_x ||...\n\t               ...\n\t\t|--> node2_x || node2_x ||...\n\t               ...\n\t\t-|-->...\n\t\t-|--> node9_x ||...\n\t               ...\n\t\t-|-->...\n\t\t-|-->...\n\t\t-|-->..."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\timport csv\n\t\tif self.axis is None:\n\t\t\traise ValueError(\"axis undefined in to_csv\")\n\t\twriter = csv.writer(self.fh)\n\t\twriter.writerow(self.axis)\n\n\t\tif header is None:\n\t\t\tfor p in self.points:\n\t\t\t\twriter.writerow(p.to_csv(separator))\n\t\telse:\n\t\t\twriter.writerow(header)\n\t\t\t# TODO, check if length is correct\nfrom tests.base_test import BaseTest\nfrom zeeguu_core.rules.core import Node\n\n\nclass TestBlocked"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn sep.join(self.to_csv(separator, header))\n\n\nclass Node(object):\n\t\"\"\"\n\tNode class. This is a graph for representing nodes. Each point\n\tis transformed into a set of :class:`Graph` objects and then\n\talso acts like a :class:`Entity` class.\n\t\"\"\"\n\n\tdef __init__(self, points=None, graph=None):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.3 in API 0.3.3 (3D0) in version 0.4.0\n\t\t  Use :class:`lena.nodes"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\traise NotImplementedError\n\n# -*- coding: utf-8 -*-\n\n\"\"\"\nModule implementing :class:`TableBase` and its containers.\n\"\"\"\n\nimport abc\nfrom nailgun import entities\n\nfrom robottelo.constants import PRDS, REPOS, REPOSET\nfrom robottelo.datafactory import gen_string\nfrom robottelo.decorators import stubbed, tier1\nfrom robottelo.decorators import tier2, upgrade\nfrom robottelo.test import APITestCase\n\n\nclass TableBase(APITestCase):\n    \"\"\"Base class for :class:`TableMixin`"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t # TODO: add a header option for all nodes instead of just node names\n\n\tout = StringIO()\n\twriter = csv.writer(out, delimiter=separator)\n\theader = header or [True] * len(header or self.nodes)\n\twriter.writerow(header)\n\tedges = self.edges()\n\twriter.writerow(edges)\n\tfor node in self.nodes:\n\t\tif node:\n\t\t\tout.write(node.dumps(encoding=self.edge_encoding), newline=False)\n\tfor node in self.nodes:\n\t\trow = [node"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tfrom._base import to_csv\n\t\tfrom.. import formats\n\t\tfrom.util import write_table\n\t\tto_csv(self, format, separator, header)\n\t\t\n\tdef to_html_format(self, content):\n\t\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_html_format is not used.\n      Iterables are converted to html.\n\nConvert graph's points to HTML.\n\n*content* is a string of the content of the graph.\n\n\t\"\"\"\n\t\tfrom._base import to_html_format\n\t\treturn to_html"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self._to_csv(separator, header)\n\n\tdef _to_csv(self, sep=\",\", header=None):\n\t\t# If header is specified, it should be an interator\n\t\t# of :class:`lena.engine.Engine` instances.\n\t\tself.header = header\n\t\tsep = sep.join([(str(x),x) if isinstance(x, tuple) and len(x)==1 else x for x in self.header])\n\t\tsep = sep.replace(sep,\"\")\n\t\tassert sep\n\t\theader = [x.strip() for x"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tdef write_header_line(column, value):\n\t\t\tcolvalue = self[column]\n\t\t\tif not isinstance(colvalue, (list, tuple)):\n\t\t\t\treturn \"{}{}...\".format(separator.join(str(c) if c not in string.printable else \" \" for c in column), value.rstrip() if value else \"...\")\n\t\t\treturn \"{}{}...\\n\".format(separator.join(str(c) if c not in string.printable else \" \" for c in column), colvalue.rstrip())\n\n\t\tdata = list(self)\n\t\tif data"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\terr_indices = [self.get_coord(name) for name in self.coord_names]\n\t\tfor index in err_indices:\n\t\t\tyield index\n\t\n\tdef get_coord(self, name):\n\t\t\"\"\"\n\t\tGet the coordinate data given an identifier.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tcoord_name : str\n\t\t\tThe identifier of the coordinate.\n\t\t\n\t\tReturns\n\t\t-------\n\t\tcoord : tuple\n\t\t\tTuple of indices corresponding to the requested coordinate,\n\t\t\tor an exception if an attempt was made to access an\n\t\t\tunknown coordinate.\n\t\t\"\"\"\n\t\ttry:"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn self._err_dims.index(coord_name)\n\tdef get_err_indices(self, coord_name1, coord_name2):\n\t\t\"\"\"\n\t\tGet error indices corresponding to a coordinate.\tThe index\n        and a list of indices corresponds to the same coordinate.\t\"\"\"\n\t\treturn self._err_dims.index(coord_name1), self._err_dims.index(coord_name)\n\tdef get_err_index(self, coord_name1, coord_name2):\n\t\t\"\"\"\n\t\tGet error indices corresponding to a coordinate.\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord_num = self.coord_names.index(coord_name)\n\t\terrs = self.coord_errs[coord_num]\n\t\terr_index = list(set(errs))\n\t\treturn err_index\n\tdef _coord_check(self):\n\t\t\"\"\"\n\t\tCheck self for errors, return None in case of a missing\n\t\tcoordinate.\n\t\t\"\"\"\n\t\terr_indices = _get_err_indices('coordinate')\n\t\tif len(err_indices) > 0:\n\t\t\treturn err_indices\n\t\telse:\n\t\t\treturn None\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\ttry:\n\t\t\terr_indices = self._coord_err_indices[coord_name]\n\t\texcept KeyError:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn err_indices\n\n\tdef set_coord_coord_index(self, coord_name, coord in enumerate(self):\n\t\t\n\t\t\"\"\"\n\t\tSet the coordinate in the given coordinate\n\t\t\"\"\"\n\t\tself._coord_err_indices[coord_name] = coord\n\t\n\tdef set_coord_names(self, names):\n\t\n\t\t\"\"\"\n\t\tSet the names of the coordinate(s) to be used\n\t\t\"\"\"\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord_type = self._coord_type[coord_name]['type']\n\t\tif coord_type not in self._coord_types:\n\t\t\treturn []\n\t\treturn self._coord_types[coord_type]['coordinates'].keys()\n\t# --------------------------------------------------------------------------------\n\t# get ecomap indices\n\t# --------------------------------------------------------------------------------\n\tdef get_ecomap_index(self, coord_name):\n\t\tcoord_type = self._coord_type[coord_name]['type']\n\t\tif coord_type not in self._coord_types:\n\t\t\treturn []\n\t\treturn self._"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tif coord_name not in self._coord_names:\n\t\t\treturn -1, -1\n\t\telse:\n\t\t\treturn self._coord_names.index(coord_name), -1\n\t\n\tdef _find_err_index(self, coordinate, name, coord_value, errstring):\n\t\t\"\"\"\n\t\tFind the index of the matching coordinate, name/coord/value pair\n\t\tthat corresponds to the coord_value element, the number of errors\n\t\tand the error. The error number may be an exception type (e.g., integer).\t\n\t\tThe index returned is a"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn get_err_indices(self, coord_name)\n\n\tdef get_indices(self, coord_name):\t\"\"\"\n\tGet coordinates indices corresponding to a particular dimension.\t\n\n\t\t* coord_name*\n\t\t * Name of coordinate variable.\n\n\t\t* Returns\n\t\t\tIndices of coordinates.\n\n\t\tExample:\n\n\t\t\tcoords = get_spatial_indices(cube, \"grid_longitude\")\n\n\t\t\"\"\"\n\t\tcoordinates = [coord.name() for coord in coords]\n\t\tindex_map = {}\n\t\tfor coord in self.coords():\n\t\t\tcoord_name = coord.name"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tif coord_name not in self._coord_errors:\n\t\t\tself._coord_errors[coord_name] = []\n\t\t\tif self._dim:\n\t\t\t\tcoord_error = self._dim._get_error()\n\t\t\t\tself._coord_errors[coord_name] = coord_error._coord_err_mat\n\t\t\telse:\n\t\t\t\traise ValueError('Coordinates have no errors associated with dimension '\n\t\t\t\t\t\t\t\t '{}'.format(self._dim.name))\n\t\t\tself._coord_errors[coord_name].append({'name': 'x', 'coord_name': 'x'})\n\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = self.coord(coord_name)\n\t\t\n\t\tif coord is None:\n\t\t\treturn None, None\n\t\terr_indices = []\n\t\tfor i, d in enumerate(coord.points):\n\t\t\ttry: err_indices.append(idx)\n\t\t\texcept ValueError:\n\t\t\t\terr_indices.append(0)\n\t\treturn err_indices\n\t\t\n\t#TODO: check if self._err_indices() needs a method for\n\t#      calculating the error indices.\n\tdef _err_indices(self, idx):\n\t\t\"\"\"\n\t\tGet the error indices corresponding to a specified"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord1 = self._all_coords[coord_name]\n\t\tcoord2 = self._coords_out[self._all_coords[dim]]\n\t\terr_msg = 'coordinate %s is not present in source file'\n\t\tcoord_in_coords = coord1.common_coords()\n\t\terr_coord_in_coords = [coord1] * len(coord1)\n\t\tif coord2.coord_system.supports_real:\n\t\t\terr_coord_in_coords.append(coord2.index_in_core())\n\t\telse:\n\t\t\terr_"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\n\tif self.nversion > 0x040000 : self.nversion = 0x04000000\n\n\t# -------------------------------------------------------------------------\n\t# Create instance-specific attributes.\n\t# -------------------------------------------------------------------------\n\tself.store = None\n\tself.namespace = None\n\tself.graph = None\n\tself.keylist = None\n\tself.valuelist = None\n\tself.data = None\n\tself.errf = None\n\tself.is_valid()\n\t# we have not yet parsed the data yet\n\tif 0 < self.nversion : self.error = None\n\telse"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.context = context\n\t\tself._flow = None\n\t\tself.name    = \"\"\n\t\tself._path    = \".\"\n\t\tself._ctx   = None\n\t\tself.value   = \"\"\n\t\tself._error = None\n\t\tself._flow_stack = []\n\n\t@property\n\tdef flow(self):\n\t\t\"\"\"\n\t\tGet the flow object for the current block.\t:class:`.Flow` instance\n\t\t\"\"\"\n\t\treturn self.__flow\n\n\tdef restructure(self):\n\t\t\"\"\"\n\t\tRestructure this graph with other graphs.\n\n\t\t:class:`."}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\nclass Igraph(object):\n\tdef __init__(self):\n\t\t\"\"\"A dictionary of all information related to\n\t\tgraphing. The keys are strings. There is no need to\n\t\tspecify every field in the dictionnary\n\t\t:attr:`node` in the Graph class.\n\n\t\tThe values are stored in the \"value\" attribute of\n\t\t_G, and the \"attrs\" dictionary of all values.\n\t\t\"\"\"\n\n\t\tself.node = \"U\"\n\t\tself.attr = defaultdict(str)\n\t\tself.value = {}\n\n\tdef update(self):\n\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\ndef to_csv_graph(filename=None, format=None, name=None, context=None):\n\t\"\"\"\n\tExport a collection of values in :mod:`.tabular.core.flow`.\n\n*filename* is one of the supported data formats, including:\n\n - `numpy`, `scipy.sparse`, `pandas`, `pandas_gbq`\n - `matplotlib` (default)\n - `pydot` (default: matplotlib.backends.backend_pdf)\n - `pandas` (default: True)\n\n - `matplotlib.figure.Figure` of a graph\n\n -"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\timport networkx as nx\n\t\tnlist = nx.to_dict_of_lists(self.graphs)\n\t\tif len(nlist) == 0:\n\t\t\traise nx.NetworkXError(\"empty graphs\")\n\t\t# initialize data\n\t\tif \"x\" in self.graphs:\n\t\t\t# x is a node\n\t\t\tself.x = self.graphs[\"x\"]\n\t\t# y is an edge\n\t\tif \"y\" in self.graphs:\n\t\t\tself.y = self.graphs[\"y\"]\n\t\t# z is a property\n\t\tif \"z\" in self.graphs"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n#===============================================================================\n#\n# Graphs and Contexts\n\ndef _get_unique_edges():\n\t\"\"\"\n\tDeterimine unique edges so that all the nodes\n\tare unique.\n\t\"\"\"\n\t\n\treturn {}\n\t\ndef _get_unique_nodes():\n\t\"\"\"\n\tDeterimine unique non-unique edges so that all\n\tnodes are unique.\n\t\"\"\"\n\t\n\treturn {}\n\t\ndef _get_graph(G, name):\n\t\n\t\"\"\"\n\tFind a graph based on its name and id.\n\n\tThe returned graph object has ``context``, ``node``,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n        if context.error is None and len(self.error) == 2 and \\\n           isinstance(self.error[0], FlowGraphError):\t\n# \t    print \"Error at step\", len(self.error)\n\t    #   Note that we don't do anything with an edge, just pass\n\t   # to the parent's error-name property, since we don't want to set it\n\t   self.error = self.error[1]\n#        if not self.error:\n            if isinstance(self.node, ParseResults):\n                self.error = self.node\n            else:\n                self."}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\traise ValueError(\"%s not defined\" % (self.error))\n\n\tdef _update_edge(self, v1, v2):\n\t\t\"\"\"\n\tUpdate the value (from v1) of an edge. It will store the node\n\tthat was taken in the new value. This change as the new\n\tvalue for each edge is sent to the other node.\n        \"\"\"\n\t\tpass\n\tdef _update_edge_field(self, field, pos, new_pos=None):\n\t\t\"\"\"\n\tUpdate edge field, with or without pos in the input/output direction.\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tgraph = self.get_graph(context.value.value, context.subcontext)\n\n\t\tfor name, nodes in graph.edges(data = True):\n\t\t\tfor _, subnode in nodes.copy().iteritems():\n\t\t\t\tif name == context_name: continue\n\t\t\t\tfor subnodeset in subnode.value:\n\t\t\t\t\tif subnodeset in context.subcontext and subnodeset[0] == context.value.value[subnodeset].value:\n\t\t\t\t\t\tcontext.value.value[subnodeset].value[1] = name\n\n\t\tgraph.flush()\n\n\tdef"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\t# Note: if self.value has a key which does not\n\t# exist, we should rename that key.\n\n\tdel self.context[-1]\t\t\t\t\t\t\t# Delete this node\n\tdel self.context[-2]\t\t\t\t\t\t\t# Delete this node\n\ttry:\n\t\tself.value[0]                                 # The one on this line\n\n\texcept AttributeError:\n\t\tpass\n\t\n\tif '__warning__' in dir(self.context[-1]):\n\t\tif self.context[-1]['__warning__']:\n\t\t\twarning(\"Warning in %s: '%s'\" % (\n\t\t\t\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn (\n\t\t\t(_e.get_integrated_size(bins, edges, _e.get_integrated_size))\n\t\t\tfor _e in self.bins)\n\t\t\tfor _e in self.edges)\n\t\t\t\n\tdef intersections(self, bins, edges):\n\t\t\"\"\"\n\t\tIntersections.\n\t\t*bins* contain values, and *edges* form the mesh\n\t\tfor the integration.\n\t\tTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn (\n\t\t\t(_e.get_integrated_size(bins, edges,"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\t\n\tvals = n.outer((n.edges[:-1] - edges[1:]), (n.edges[1:] - edges[:-1]))\n\treturn n.sum(vals, axis=1)\n\ndef histogram_2(bins, edges1=None, edges2=None):\n\t\"\"\"\n\tCompute the :func:`nbins` and :func:`edges1` values of :func:`nbins` and\n\t:func:`edges2`. \n\t\n\t:arg bins: histogram bin edges\n\t:type bins: array\n\t:arg edges1: :func:`"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t return 0.5*log10(numpy.sum(binedge_bins[edges]*numpy.log(edges+1)))\ndef integral_at_edges(bins, edges):\n\t\"\"\"\n\tCompute integral at edges.\n\t\"\"\"\n\treturn 1.0*numpy.sum(hist_binwidth(binedge_bins[edges]*numpy.log(edges+1)))\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\"\"\"Test class for ``linuxf86`` and ``lal`` for\n`astropy.wcs` module.\n\"\"\""}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t return hist_integral(bins, edges, *args, **kwargs)[0]\n\n## histogram3 ----------------------------------------------------------\n#\n# Weighted density profile plotting using a histogram2d() function\n#\n\nHist2D = (Histogram3,)\nHist2D.__doc__ = \"\"\"\nHist2D creates a 2-D histogram, and optionally returns 2-D\nhistogram with one less dimension.\n\n.. seealso::\n    hist2d\n\"\"\"\n\nHistogram3D = (Histogram,)\nHist3D.__doc__ = \"\"\"\nHist3D creates a 2-D histogram with one less dimension"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tpoints = np.asarray([1, bins, 1, 1]).T - 0.5\n\t\tx, y = np.meshgrid(edges[:-1], edges[:-1])\n\t\txx, yy = np.meshgrid(x.ravel(), y.ravel())\n\t\txx, yy = np.meshgrid(xx, yy.ravel())\n\n\t\t# We must use triangles because the quadratures are parallel over each side,\n\t\t# which means there we have to do some parallelization using the bins and edges\n\t\t# of the mesh to get the integral of the density, i."}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\t\n\tr, g, b = bins\n\tR, G, B = _bins2nodes_C(g, B)\n\n\tgmin, gmax = B, B\n\tgrmin, grmax = _bins2nodes_C(r, g)\n\n\tA = B\n\tB = np.zeros(r, dtype='U1')\n\n\tfor i in range(len(bins)-1):\n\t\tb = bins[i+1]\n\t\tD = abs(G - A)\n\t\tB[i], D = D / G, D / A\n\t\t"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t return histogram(bins, edges)\n\n\tdef histogram(bins):\n\t\t'''\n\t\tCompute the curve.\n\n\t\t*bins* are 2-vectors (or 0-vectors if a single value),\n\t\t   and not a scalar value.\n\n\t\t   For example, the input is a 1-D array of N (x, y[,z])\n\t  or a 2-D grid.\n\n\t\tReturns:\n\t\t    a `num_elements` x `num_nodes` array of N arrays;\n\t\t   each is the element value of the curve evaluated\n\t\t   at the nodes.\n\t\t'''\n\t\treturn array([ ("}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\tassert len(bins) >= 2\n\tn_bins = len(bins)/2\n\tn_bins_up = numpy.ceil((n_bins - 1) / bins) * bins\n\tn_edges_up = numpy.ceil(n_bins - 1)\n\tfor i in range(n_bins):\n\t\tfor j in range(n_bins):\n\t\t\tif (bins[i] <= bins[j]):\n\t\t\t\tbins[i] = bins[j] - bins[i - 1]\t# -1 or 1; 1, 1 would be taken from"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n#\t\tbins and edges for the integration\n\tbins = np.array(bins)\n\tedges = np.array(edges)\n\tintegral = np.zeros_like(edges, dtype=np.int32)\n\n\tfor i in range(len(bins)-1):\t#bins[i] is the range of the points.\t\n\t\tfor j in range(i+1):\t#for all other points.\t\n#\t\t\tintegral[j+1] += bins[i]\n\t\t\t\tintegral[j] += edges[i]\n\n\t# For the moment,"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n# class IQCurveFitter(QObject):\n# \t\"\"\"\n# \tUsed to generate new IQ curves in a list.\n# \t\"\"\"\n# \t# This signal is sent whenever the curve profile is changed.\n\t\n\t# \tdef __init__(self, signal, axes):\n\t\t# signal:\n\t\t#\tSampling rate in us.\n\t\t# axes:\n\t\t#\tA list of indices into the signal.\n\t\t#\tThe value should be the number of points the curve takes out.\n\t\t#\"\"\"\n\t\t\n\t\t# self.__npoints"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif len(seq) > 1:\n\t\t\t# the one to be checked\n\t\t\treturn False\n\t\tif seq[0].value!= None:\n\t\t\t# the sequence contains only one element, and it \n\t\t\t# must be a Source Sequence\n\t\t\treturn False\n\telse:\n\t\t# empty sequence\n\t\treturn True\n\n\n#-------------------------------------------------------------------------------\n\nif TESTING:\n\t## Run tests.\n\tif \"TEST_TESTS\" not in locals():\n\t\tprint(\"Must run before test\")\n\t\tsys.exit(0)\n\tassert get_fill_seq([]) == []\n\tassert"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif isinstance(seq, (str, bytes)):\n\t\t\tif seq.startswith('fixed'):\n\t\t\t\treturn True\n\t\telif 'fill' in seq:\n\t\t\ttry:\n\t\t\t\treturn int(seq.replace('fill-', ''))\n\t\t\texcept ValueError:\n\t\t\t\treturn False\n\t\telif 'pad' in seq or 'end' in seq or 'pad' in seq or 'end' in seq:\n\t\t\treturn False\n\t\treturn True\n\treturn False\n\ndef check_for_replace(str1, str2):\t# check the replacements for a string of length 5\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\t# the Sequence element, or None\t\n\treturn len(seq) == 1 and seq[0].name == \"FillRequest\"\n\t\t\ndef is_source_sequence(seq):\t\"\"\"\n\tTest whether seq is a SourceSequence.\n\n\"\"\"\n\t# the Seq element, or None\n\treturn len(seq) == 0 and seq[0].name==\"SeqSource\"\n\"\"\"\nA test splinter.\n\n\"\"\"\nfrom __future__ import division, absolute_import, print_function\n\nimport warnings\n\nfrom numpy.testing import (assert_, assert_array_"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor elem in seq:\n\t\t\tif elem in (ElementToken.DeclarationEnd, ElementToken.DeclarationStart):\n\t\t\t\treturn True\n\n\t\treturn False\n\ndef is_fill_request(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequest.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is a Source sequence.\t\"\"\"\n\t\tfor elem in seq:\n\t\t\tif elem in (ElementToken.DeclarationEnd, ElementToken.DeclarationStart):\n\t\t\t\treturn True\n\n\t\treturn False\n\ndef trim_comment"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif seq.is_source:\n\t\t\treturn\t\t# already a source\n\t\telif seq.is_sink:\n\t\t\treturn\t\t# not a sink\n\n\tfor s in seq.sources:\n\t\t# check that all inputs (not outputs) are also\n\t\t# in the list (unless they must have same source)\n\t\tfor t in s.inputs:\n\t\t\tif t.is_source and t not in seq.source:\n\t\t\t\treturn 1\t\t# don't care more than one source\n\n\t\t# if all outputs are identical\n\t\tfor t1 in s.outputs:"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\t# FIXME: This is in fact ignored as it only affects\n\t\t# element and not element.\n\t\treturn True\n\n\tif not isinstance(seq, basestring):\n\t\treturn False\n\n\tsrc_name = _name_elements(seq)\n\tif len(src_name)<1:\n\t\treturn None\n\tfor s in src_name:\n\t\tif s in ('_','_', '+'):\n\t\t\treturn None\n\t\tif s=='_':\n\t\t\ts = ''\n\t\t elif s == '+':\n\t\t\ts =''\n\t\telif s=='_':\n\t\t\ts = ''"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tpass\n\n\tif isinstance(seq, FillRequest) or \\\n\t   hasattr(seq, 'getSource'):\t# if sequence is a FillRequest\n\t\t\t\treturn True\n\tif is_sequence_type_string(seq):\n\t\treturn False\n\n\treturn True\n\ndef get_sequence_item(element, seq_type):\n\tseq_item = None\n\n\ttry:\n\t\tseq_item = element.getSequenceItem(seq_type)\n\texcept PyOCDError:\n\t\tpass\n\n\tif seq_item is None:\n\t\tlogging.warning('Unknown sequence item \"%"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn (isinstance(seq, (LineSourceNode, StreamSourceNode)) and ((seq.getNumBytes() < 1) or (seq[0].getNumBytes() > 1)))\n\t# return True if they are any LineSource and StreamSource nodes\n\n\ndef wrapNodeClassFragment(builder, node):\n\t\"\"\" \n\t\"Simply wraps a Node class by replacing its child class's\n\t\"node['CLASS'] = klass\" (a normal Python represenation of\n\t\"cls = klass.clone()\").\t\n\tReturn the replacement for the <code> node in *"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn len([x for x in seq if x.get('isFillEvent')!= 1 and x.get('isFillInRequest')!= 1])== 1\n\t# the following tests must not actually occur but also\n# the checks here as we have already checked for that!\n\treturn not is_fill_request_seq(seq[0]) or \\\n\t   len(seq)==1 and len(seq) == 2\n\n################################\n# Unit Testing\n################################\nclass TestMedianInterval(TestCase):\n\t\"\"\" Unit test for is_median_interval() function. \"\"\"\n\n\tdef"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\t# if not isinstance(seq, basestring):\n\t\treturn False\n\n\treturn seq.get_source_sequence()!= None\n\t\t# return True\n\ndef is_filled_request_seq(seq):\n\t\"\"\"\n\tTests whether *seq* can be converted to a FillRequestSeq.\n\nSee also:\n\tSeq\n\"\"\"\n\t# if not isinstance(seq, basestring):\n\treturn False\n\n\treturn (not is_filled_request_seq(seq)) \n\ndef replace_child(old_node, new_node, child_node, is_input=True"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\ttype_string\t= obj.type_string\n\t\tif type_string in ['I','B','I']:\tself._isFilled = True\n\t\telif type_string in ['C','D']:\tself._isFilled = False\n\t\telse:\t\t\tprint('Error: invalid fill request element type:'+ type_string)\n\t\tif obj.buffer==None:\n\t\t\treturn False\n\t\treturn True\n\n\t# ---------------------------------------------------------------------------\n\t# Get name and mode of fill request element\n\t# ---------------------------------------------------------------------------\n\tdef GetFillMode(self):\n\t\treturn self._fill"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif not obj.attributes['request'].value.strip():\n\t\t\treturn False\n\t\tfill = str(obj.attributes['request'].value.strip()).strip()\n\t\treturn (\n\t\t\t(\"application/x-www-form-urlencoded\" if fill=='application/x-www-form-urlencoded' else \"form-data\" if fill=='form-data' else \"application/x-www-form-urlencoded-filtered\"),\n\t\t\t(\"multipart/form-data\" if fill=='multipart/form-data' else \"multipart/form-data\"),\n\t\t\t(\"application/json\" if fill=='"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn hasattr(obj, 'fill') and callable(getattr(obj,'request')) and callable(getattr(obj,'model'))\n\n\tdef is_filter(obj):\t\"\"\"\n\tObject contains executable methods'read','search' and 'update'.\t\"\"\"\n\t\treturn hasattr(obj,'read') and callable(getattr(obj,'search')) and callable(getattr(obj, 'update'))\n\n\tdef is_filter_el(obj):\t\"\"\"\n\tObject contains executable methods'select', 'deselect', and 'write'.\t\"\"\"\n\t\treturn hasattr(obj, '"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\t# FIXME:\n\t\tpass\n\n\tdef process_request_elt(el):\n\t\tif not el.is_exe:\n\t\t\treturn\n\t\tif el.in_app:\n\t\t\treturn\n\t\tif hasattr(el, 'is_runnable'):\n\t\t\traise Errors.WafError('exe action %r is not supported, skipping' % el)\n\t\tif not el.is_win32:\n\t\t\treturn\n\t\tif hasattr(el, 'runcode'):\n\t\t\t# We may find more than one runcode, from most recent runcodes.\n\t\t\t# If we find one"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.endswith(\".request\") and obj!= \"fill_request.el\"\n\ndef is_fill_request_elm(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\t\"\"\"\n\treturn obj.endswith(\".request\") and obj!= \"fill_request.elm\"\n\ndef is_drop_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'drop' and'request'.\t\"\"\"\n\treturn _get_drop_request_el(obj)\n\ndef is_drop_request_elm(obj):\t\"\"\"\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn (obj.get_attribute(\"fill_request\") == \"true\" or return_bool_str(obj.get_attribute(\"request_fill\")))\n\telse:\n\t\treturn (obj.get_code().endswith(\"fill\") or return_bool_str(obj.get_code()))\t\n\ndef get_fill_request_status(obj):\n\t\"\"\"\n\tGet fill request status (true if fill request is successfully finished or has entered a 'finished' attribute, False otherwise).\n\t\"\"\"\n\treturn (obj.get_attribute(\"fill_request_status\") =="}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tassert isinstance(obj, Element), \"Argument '%s' is not a valid object\" % type(obj).__name__\n\t\tif obj.has_key('fill'):\t\tassert obj['fill'] == obj.attrib['fill'], \"Fill must be equal to fill value\"\n\t\treturn obj.attrib['type'] == 'Solid'\n\n\tdef is_fill_request(obj):\n\t\t\"\"\"\n\t\tObject contains executable methods 'fill' and'request' and\n\t\tsupports 'fill' and retrieves it from 'fill-request'.\t\"\"\"\n\t\tassert isinstance(obj, Element"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif hasattr(obj,\"fill\"):\n\t\t\treturn obj.fill(iom.kernel.execute,iom.shell.process)\n\t\telse:\n\t\t\treturn obj.request(iom.kernel.execute)\n\tdef create_command_el(obj):\t\"\"\"\n\tReturns command instance filled with the command arguments\n\tfrom the 'fill' iom file. The'request' object contains the\n\tcommand arguments that can be used to submit the command:\n\t\t'fill': (command name, [optargs...], arguments)\n\t\t'request': (command name and arguments,"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.executable and obj.executable.fill\n\n\tdef is_fill_request_name(obj):\t\"\"\"\n\tObject contains method 'get_fill_name'.\t\"\"\"\n\treturn obj.method and obj.method.get_fill_name() == 'fill' and hasattr(obj,'request')\n\n\n\ndef get_script_logger(obj):\t\"\"\"\n\tReturns an instance of the logger class for the script.\n\t\"\"\"\n\n\tlogger_instance = script_logger_t(obj)\n\treturn script_logger(logger_instance)\n\n\ndef run_"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif len(obj) < 2: return False\n\t\tassert isinstance(obj[0], obj_el)\n\t\tassert obj[1] == None or obj[1] == 0\n\n\t\t# In some conditions, the objects in the binary is in two parts. Check.\n\t\t# Note: The output is never empty, it just contains a number,\n\t\t#  because all parts are binary characters (see below).\n\t\treturn len(obj[1].data) == 2\n\n\tdef is_fill_request_el_in1_el(obj):\t\"\"\"\n\tObject contains executable"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\trun = 0\n\t\tif obj.run:\n\t\t\tif obj.run.returncode!= 0:\n\t\t\t\treturn False\n\t\tif obj.has_run:\n\t\t\trun = obj.run.returncode\n\t\treturn run\n\tdef get_run_timestamp(self, node=None):\t\"\"\"\n\tGet the run timestamp for the specified node and its children.\n\tThe timestamp may be empty. Only nodes have a run if these run times are specified.\n\t\"\"\"\n\tif node:\n\t\treturn node.run.timestamp\n\tif self.is_parallel():\n\t\treturn self"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn isinstance(obj, waflib.Task.TaskBase)\n\n\n@conf\ndef get_cc_version_env(conf, cc):\n\tcmd = cc + ['-dumpversion']\n\tout, err = conf.cmd_and_log(cmd)\n\tif out:\n\t\tif out.startswith('gcc version'):\n\t\t\tcmd = cmd + ['--version']\n\t\tcmd = cmd + ['-V']\n\t\tout, err = conf.cmd_and_log(cmd)\n\telse:\n\t\tcmd = cmd + ['-version']\n\t\tout, err = conf.cmd"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.__self__ in (\"run\",)\n\tdef is_launch_node(self,obj):\t\"\"\"\n\tObject contains executable method 'launch_node'.\n\t\"\"\"\n\t\tif not is_run_el(obj):\n\t\t\treturn False\n\tdef is_launch_directory(self,obj):\t\"\"\"\n\tObject contains executable method 'launch_directory'.\n\t\"\"\"\n        if is_run_el(obj):\n\t\t\treturn True\n\tdef is_launch_directory_with_filename(self,obj):\t\"\"\"\n\tObject contains executable method 'launch_directory'."}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn is_python_object(obj) or is_shlib_file(obj) or is_shlib_library(obj)\n\n\tdef is_run_exe(obj):\n\t\t\"\"\"\n\t\tReturns true if run method is executable.\n\t\t\"\"\"\n\t\treturn is_runnable_by_other(obj) and is_runnable_by_other_python(obj)\n\n\tdef is_run_py(obj):\n\t\t\"\"\"\n\t\tReturns true if run method is Python.\n\t\t\"\"\"\n\t\treturn is_python_object(obj) and is_runnable"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif not hasattr(obj, 'run'):\n\t\t\treturn 0\n\t\treturn obj.run.func_code.co_argcount >= 2\n\tdef is_hook_executed(obj):\t\"\"\"\n\tObject contains executable method 'hook'. Return True if is hook\n\texecuted, False if not.\t\"\"\"\n\t\treturn hasattr(obj, 'hook')\n\tdef get_abi(self, regs, labels=None):\t\"\"\"\n\tCompile ABI string for this process, and return the binary ABI value.\n\t\"\"\"\n\tif not labels:\n\t\tlabels = self."}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\trun = obj.executable or ''\n\t\tcmd = self.path.find_best_executable_with_executable(\n\t\t\tos.path.join(os.path.dirname(obj.get_path()), 'run'))\n\t\texec_cmd = cmd if not self.path else ''\n\t\treturn not (run or cmd or exec_cmd or os.path.abspath(run or exec_cmd))\n\n\tdef use(self):\n\t\t\"\"\"\n\t\tInitialize the object for execution.\t\n\t\t\"\"\"\n\t\tos.chdir(os.path.dirname(__file__))\n\t\tpath ="}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.endswith('.el')\n\n\tdef is_run_cpp(obj):\n        \"\"\"\n\t\tObject contains executable method 'run.cpp'. We allow using \n\t\tthe cpp extension as well.\n\t\t\"\"\"\n        try:\n            subprocess.Popen([\"clang\", \".\", obj], stdout=subprocess.PIPE).communicate()\n        except OSError as e:\n            # no compiler found\n            return False\n        else:\n            return True\n\n\tdef can_run(self):\n        pass\n \n    def get_id(self):\n        return self.run.get('id')\n\n\n    def get_executable_args(self, fname"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tres, obj = parse_run(obj)\n\t\treturn res, obj\n\n\tdef is_run(obj):\t\n\t\t\"\"\"\n\t\tObject contains object identifier 'run'.\n\t\t\"\"\"\n\t\tres = False\n\t\tif obj.startswith(\"run:\"):\n\t\t\tobj, index = obj.split(\" : \")\n\t\t\trun = res, int(index)\n\t\telse:\n\t\t\trun = res, obj\n\t\t\t\n\t\treturn run\n\n\tdef get_run_path(obj):\n\t\t\"\"\"\n\t\tObject represents the path where is run, and the first word (the letter 'run"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif hasattr(obj,\"_run\"):\n\t\t\treturn obj._run\n\t\telse:\n\t\t\treturn False\n\tdef get_bin_files(self):\n\t\t_bin_files = []\n\t\tfor d in getattr(self,\"generator_lib_paths\",[]):\n\t\t\tif d:\n\t\t\t\tfor x in glob(d+\"/*\"):\n\t\t\t\t\tif not x.startswith(\"bin\"):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t_bin_files.append(x.replace(\"bin\",\"\"))\n\t\treturn _bin_files\n\tdef get_binaries(self):\n\t\tbin_files = self."}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\t# for executable methods check the environment\n\t\tkey = 'run_{0}_{1}'.format(myconf.host_string, obj.type)\n\t\tenv = get_env(key, raise_error=True, myconf=myconf)\n\t\tif not env or not obj:\n\t\t\treturn False\n\t\treturn os.path.basename(obj.path()).find('run') > -1\n\n\t# @classmethod\n\t# def kill (cls, pid):\n\t# \tenv = get_env(myconf.env_string)\n\t# \treturn env."}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# @@@ Not in c++!\n\t\tassert len(obj.command) == 3\n\t\treturn obj.command[0] == 'fill' and obj.command[2] == 'compute'\n\n\t# Get the executable and call C++ interpreter.\n\tobj = exe_wrapper.get_clr_object(\n\t\tc_object = obj,\n\t\tproject_folder = project_folder,\n\t\toutput_directory = output_directory,\n\t\tcommand = command\n\t)\n\t# Call the executable.\n\tret_val = obj.run_command(command)"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj.cmd.startswith('fill ') or obj.cmd.startswith('compute ')\n\n# -----------------------------------------------------------------------------\n# --- The actual commands ---\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# --- Python3-like commands ---\n\ndef find_commands(self, arg):\n\treturn (self.commands or []).__call__(arg)\n\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n# ---- Main program ----------------------------------------------------\n\ndef run():\n\tcli = Cli()\n\tif cli.cmds:\n\t\tcli.exit(0)\n\telse:\n\t\tcli.exit(1)\n\nif __"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif obj.is_unfilled: return False\t# Unfilled\n\t\tif obj.is_split: return False\t# Split\n\t\tif obj.is_array: return False\t# Array\n\t\tif obj.is_sub or \\\n\t\t\tobj.is_split or \\\n\t\t\tobj.is_scalar: return False\t# Scalar\n\t\tif obj.is_add: return False\t# Add\n\t\tif obj.is_subscript: return False\t# Subscript\n\t\tif obj.is_compare: return False\t# Compare\n\t\tif obj.is_unary_"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tfunc_names = [\"is_compute_\" + method for method in obj.get_methods()]\n\t\tif func_names:\n\t\t\treturn True\n\treturn False\n\nclass CompileObject:\n\tdef __init__(self, name, method, methodLines = [], getters = []):\n\t\tself.name = name\n\t\tself.method = method\n\t\tself.methodLines = methodLines\n\t\tself.getters = getters\n\n\tdef writeFile(self, f):\n\t\tf.write(self.get_method_str())\n\t\tf.write(\"\\n\")\n\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\ttry: return obj.compute\n\t\texcept AttributeError: return False\n\t\n\tdef is_fill_expr_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill', 'expr'.\t\"\"\"\n\t\ttry: return obj.expr\n\t\texcept AttributeError: return False\n\t\n\tdef is_expr_expr_el(obj):\t\"\"\"\n\tObject contains executable methods 'expr' and 'expr_expr'.\t\"\"\"\n\t\ttry: return obj.expr_expr\n\t\texcept AttributeError: return False\n\t\n\tdef get_compile_mode_name(cls, obj):\n\t\t\"\"\"\n\t\tReturns"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj is obj.getObject()\n\tif hasattr(obj, '_inst_is_fill_calc_el') and obj._inst_is_fill_calc_el:\n\t    return obj._inst_is_fill_calc_el\n\tif (obj.isDerivedFrom(\"App::ExtrusionLine\")):\n\t    return obj.getObject().isDerivedFrom(\"ExtrusionLine\")\n\treturn False\n\ndef is_fill_line(obj):\t\"\"\"\n\tObject contains lines defining a material areas. \n\n\t:param obj: Object to check for fill"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj in ['write', 'execute','read', 'write_header', 'execute_macro', 'open', 'close','seek','size', 'close_file']\n\n\tif hasattr(obj, 'is_fill'):\t# Check the type of objects\n\t\t# If it is a fill then it has an explicit type\n\t\t# We don't need to change the type but add an implicit type (object or type)\n\t\t# into the code for backwards compatibility\n\t\treturn True\n\n\treturn False\n\ndef is_calldef(obj):\t\"\"\"\n\tTrue if a"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tassert isinstance(obj, bool)\n\t\t\n\tdef check_is_fill_proc(proc):\n\t\tif proc.returncode:\n\t\t\treturn False\n\t\telif proc.returncode == os.EX_OK:\n\t\t\treturn True\n\t\treturn False\n\ndef is_fill_proc(proc):\n\tif proc.returncode:\n\t\treturn False\n\treturn True\n\ndef fill_proc(proc):\n\tis_filled_proc = False\n\tif proc.returncode:\n\t\treturn\n\tif is_fill_proc(proc):\n\t\tis_filled_proc = True\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj['fill'] or obj['compute']\n\n\n\t##\n\t# The first method in the interface.\n\t#\n\tdef readf(self, stream):\n\t\treturn self._read(stream)\n\n\tdef writef(self, stream):\n\t\treturn self._write(stream)\n\n\tdef execute(self, command_line):\n\t\treturn self.f(command_line)\n\n\n__all__ = [\n\t'f',\n\t'is_build_dir',\n\t'check_call',\n\t'ensure_dirs_created',\n\t'copy"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# make a node for the 'compute' output (notably an input)\n\t\tfor (i, x) in enumerate(obj.outputs): \n\t\t\tif isinstance(x.type, str):\t# file to execute\n\t\t\t\tfname = get_f2c_filename(obj.inputs, obj.outputs[i + 1].type + \".dat\")\n\t\t\t\twith open(fname) as fh:\n\t\t\t\t\t# this check is hackish, but no one can get the file\n\t\t\t\t\tcheck_f2c(obj, fh, fname)\n\t\t\t\t\treturn False\n\t\t# only nodes with '"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\t\n\td1 = deepcopy(d1)\n\tassert level >= 0 and level < len(d1)\n\t\n\td2 = deepcopy(d2)\n\t\n\tassert set(d1) == set(d2)\n\t\n\tkeys = [k for k in d1 if k in d2]\n\t\n\tdiff = {}\n\t\n\tfor depth, key in enumerate(keys):\n\t\tif depth < 0:\n\t\t\tdiff[level] = d1[key]\n\t\telse:\n\t\t\tfor d2_key in d2[key]:\n\t\t\t\td = d2[d2_key"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\tif level == -1:\treturn { \"dict\": d1 }\n\t\n\telif level == 1:\treturn { \"dict\": d2 }\n\t\n\telif level == 0:\treturn { \"dict\": d1, \"subdictionaries\": [], \"default\": d2[\"subdictionaries\"][0] }\n\t\n\telif level == 2:\treturn { \"dict\": d2 }\n\t\n\telif level == 3:\treturn { \"dict\": d2, \"default\": d1 }\n\t\n\t\ndef intersection(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif level < 0:\n\t\t\traise AssertionError(\"expected non-negative value for level, got: \"+str(level))\n\t\tif d1[level] is d2[level]:\n\t\t\treturn d1[level]\n\t\tif level >= 0:\n\t\t\tassert len(d1) >= 0\n\t\t\tassert len(d2) >= 0\n\n\t\t\td1.clear()\n\t\t\td2.clear()\n\n\t\t\tfor x in d1:\n\t\t\t\td1[x].clear()\n\t\t\tfor x in d2:\n\t\t\t\td2[x].clear()\n\n\t\t\treturn d1"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tresult = {}\n\t\tfor key, value in d1.items():\n\t\t\tif key!= '':\n\t\t\t\titem, is_item = _parse_kv_pair(key, value)\n\t\t\t\tcurrent_result = getattr(last_dict, key)\n\t\t\t\tif not is_item:\n\t\t\t\t\tif level > 0:\n\t\t\t\t\t\tnew_level = current_result\n\t\t\t\t\telse:\n\t\t\t\t\t\t# level 0 means 'No maximum recursion'.\n\t\t\t\t\t\tnew_level = -1\n\t\t\telse:\n\t\t\t\tnew_level = 0\n\n\t\t\tif level!= -1:\n\t\t\t\tnew_level = getattr"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n        # We want to do this so that all data structures that are passed in\n        # should either be primitive Python types or strings (depending on the\n        # type of the return value, not on the type of the return type).\n        if not isinstance(d1, (type, type(None))):\n            return d1 if d1 is not None else {}\n\t\n        if not isinstance(d2, (type, type(None))):\n            return d2 if d2 is not None else {}\n\t\t\t\n        if d1 == d2:\n\t\t\treturn {}\n\t\t\n\t\t# We know this is fine. But, if"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\tresult = dict()\n\tfor d in d1:\n\t\tfor f in d:\n\t\t\tdk = f.copy()\n\t\t\tdk.pop(f)\n\t\t\tdk.update((k, d[k]) for k in dk.keys() if k in d)\n\t\t\tresult[k] = dk\n\n\treturn result\n\n\ndef intersection(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* but not contained in\n *d2*.\n\n*level* sets the maximum depth of recursion."}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\t(key, val) = max((self._keys[1:], self._keys[:2*level]), key=lambda x: x[1], reverse=True)\n\t\treturn dict(iter([(key(v), v[1]) for v in d1.items()]),\n\t\t\t\t\titer([(key(v), v[1]) for v in d2.items()]),\n\t\t\t\t\titer([(key(v), v[1]) for v in d1.values()]))\n\n\tdef difference_2d(d1, d2, level=-1):\t\t# same function as d1"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\ndef removeDuplicates(l):\n\t\"\"\"\n\tReturn a list of each key that appears only once in *l*.\n\n\tThe *l* argument must be a *subset* of any dictionaries.\n\n\tBe aware that if an entry exists, that means:\n\t * Duplicates are removed recursively;\n\t* If more than one entry exists, those are recursively removed.\n\n\tIn any case, the returned list will contain at most one\n\tentry.\n\t\"\"\"\n\treturn filter(lambda k: not len(l) == 0 or l[0] in d2,  l"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n\n\ndef difference2(d1, d2, level=-1):\n\t\"\"\"\n\tLike :func:`difference`, but returns a dictionaries\n\n*d1*, *d2* are dictionaries with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in d1 and d2, it is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn recurse(d1.copy(), level)\n\n\ndef difference_no_recursion(d1, level):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*\n(including items in any level). If *threshold* > 0 then items\nnot in a dictionary with same items are recursively removed.\n\n\tIf *threshold* is -1 then a dictionary with less than\n\t*(differs than *threshold* levels), otherwise a\ndictionary with the same keys is returned containing the same\nvalues.\n\n\tIf *threshold*!= 0 then d1 is"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tweight = max(weight, 1)\n\t\ttry:\n\t\t\tmaxx = self.maxx\n\t\texcept: \n\t\t\traise AttributeError, \"The histogram is empty\"\n\t\tif weight > maxx:\n\t\t\traise AttributeError, \"The histogram is already outside the range\"\n\t\tif self.count:\n\t\t\traise AttributeError, \"Overwrite previous histogram\"\n\t\t\tself.count = 0\n\t\tend = weight\n\t\tself.count += 1\n\t\tif not self.is_empty():\n\t\t\twidth = end - start\n\t\t\tbins = numpy.linspace(start, maxx, num="}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\treturn self.binned[coord] if self.binned else self.binned\n\n\t# Bin boundaries, not inclusive\n\n\tdef bin_to_bin(self, bin_idx, weight=1):\n\t\t\"\"\"\n\tMap *bin_idx* to a list of bin edges. If *bin_idx* is a\n\tlist then all of the bin edges are used.\n\t\t\"\"\"\n\t\tif type(bin_idx) is list and len(bin_idx) == 1:\n\t\t\tidx = bin_idx[0]\n\t\telse:\n\t\t\tidx = bin_"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.copy_properties(coord, weight * self.weights)\n\n\tdef copy_properties(self, coord, weight):\t#copied from wiggle/wiggles.py\t\n\t\t\"\"\"Copy the given *coord* properties;\n\t\tthis re-scales `coord` (by multiplying the weights by 1, multiplying the inverse of the\n\t\tsum of weights).\n\t\t\"\"\"\n\t\tif isinstance(coord, Wiggle):\n\t\t\tassert 1 < len(coord.data.values) <= 3, \"Coordinates outside wiggle/wiggles must be provided as"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif coord == 0: return\n\t\t\n\t\t# We need to create a new vector for the histogram\n\t\tif size(size(self)) == 0: raise ValueError(size(self))\n\t\t\n\t\tif weight == 0:\n\t\t\tself.fill_vector = array([0]*size(self))\n\t\t#elif weight == 1:\n\t\t\t#self.fill_vector = array((self.size + 1) * size(self))\n\t\telse:\n\t\t\twhile not(self[coord] == 0):\n\t\t\t\ti = len(self)-1\n\t\t\t\tself[i] = self[i]"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif weight == 1:\n\t\t\traise NotImplementedError()\n\t\treturn\n\tdef find_and_clean(self, coord):\n\t\t\"\"\"\n\t\tLook in *coord* which should be replaced with the given *weight*. Any\n\t\tsubtable of the histogram must be done through the new table.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\nif __name__ == \"__main__\":\n\t# test cases\n\tfrom testUtils import test_logger\n\tfrom testSuite import Tester\n\ttester  = Tester()\n\ttester.exec_tests()\n\n#EOF\n\"\"\"Support for"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\treturn\n\n\tdef min_max(self, weight=1):\t\"\"\"Min and max distance between adjacent lines.\"\"\"\n\t\treturn\n\n\tdef min_mean(self, weights_list=None):\t\"\"\"\n\tCalculate minimum weighted mean for each of the points.\n\n\tweights_list = None -> compute all points, but weights from each of them has been calculated.\n\n\tReturns mean of two points: weighted mean or unweighted mean.\n\t\"\"\"\n\t\treturn\n    \n\tdef add_point(self, point, weight=1):\t\"\"\" \n\tAdd weighted average of the given point, weighted"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\t(x1,x2,y1,y2) = self.bbox(coord)\n\t\tself.fillRect(x1,y1,x2,y2,weight)\n\t\n\tdef __repr__(self):\n\t\treturn str(((self.x0, self.x1, self.y0, self.y1), self.xsize, self.ysize))\n\t\n\t# Return an iterator object that may be used to iterate over the\n\t# histogram.\n\tdef __iter__(self):\n\t\treturn iter(())\n\t\t\n\t\t\n# A simple bounding rectangle.\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.copy()\n\tif weight > 1:\t\t\t\t\t\t\t\t# Only edges longer than 1%\n\t\tfor j in xrange(len(self) - 1, -1, -1):\n\t\t\tif j > i:\n\t\t\t\tself[j].fill(coord, weight)\n\telse:\t\t\t\t\t\t\t\t\t\t\t# Only edges shorter than 0%\n\t\tfor j in xrange(len(self) - 1, -1, -1):\n\t\t\tlength = 1 - j\n\t\t\tfor i in xrange(0, length - 1):\n\t\t\t\tself[i, j].fill(coord)\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\treturn self.get_buffer().fill(coord, weight)\n\tdef fill_rect(self, coord, weight=1):\n\t\t\"\"\"\n\tFill rectangle at *coord* with the given *weight*.\n\nCoordinates inside the rectangular region are ignored.\t\"\"\"\n\t\treturn self.get_buffer().fill_rect(coord, weight)\n\tdef end_fill(self, coord, weight=1):\n\t\t\"\"\"\n\tFill the rectangle at *coord* with the given *weight*.\n\nCoordinates with respect to the rectangle edges are ignored.\t\"\"\"\n\t\treturn"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.histo[coord[0][0],coord[1][0]] = weight\n\n\t\"\"\"\n\tFill each histogram cell with an intersperse to *weight*.\n\nCoordinates outside the histogram are ignored.\n\"\"\"\n\tdef fill_histogram(self, bins, weights = (1,)):\t# @R\n\t\t\"\"\"\n\tFill all histuma cells with intersperse to *weights*.\n\n\tCoordinates outside the Histogram are ignored.\n\t\"\"\"\n\t\tfor h_bin, weight in bins.iteritems():\n\t\t\tself.histo[h_"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise\n#!/usr/bin/python\n#\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t_err_msg = _translate(\"Labels must be a list of strings or regular expressions, and they \"\n\t                 \"do not: <i>Expected strings or regular expressions</i>. \"\n\t                 \"Expected strings and regular expressions not: <i>None</i>\", err.messages)\n\t\t\n                raise ValidationError(_err_msg)\nimport contextlib\nimport datetime\nimport logging\nimport re\nimport sys\nimport time\nfrom email.utils import parsedate_tz\n\nimport pytz\n\nfrom django.conf import settings\nfrom django.core import signals\nfrom django.core.exceptions import ImproperlyConfigured"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfor label in labels:\n\t\t\t# If label is valid, then check that the pattern is correct or not\n\t\t\t# and raise ValidationError if the pattern does not validate correctly.\n\t\t\t\ttry:\n\t\t\t\t\tregex_msg = \"^%s$\" % label['key'] if 'key' in label else \"$$\"\n\t\t\t\t\tif regex_msg in label['value']:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ValidationError(regex_msg)\n\t\t\t\texcept KeyError:\n\t\t\t\t\t# If a Label does not match the regular expression, then throw\n\t\t\t\t\t# ValidationError if the label does not validate correctly.\n\t\t\t\t\tfor msg"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t_errors = []\n            for label in labels.values():\n                try:\n                    _validate_labels(label)\n                except ValidationError as err:\n                    assert err.messages == _errors\n\t_validate_keys(labels)\n\t_validate_values(labels)\n\t_validate_types(labels)\n\n\treturn [_validate_labels(label) for label in labels.values()]\n\n\n# Helper Functions\ndef validate_labels(labels):\n\t\"\"\"Compare the given labels to see if they have the required format, otherwise\n\traise ValidationError.\n\n\tArgs:\n        labels (dict): The labels"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tif len(list(labels.keys()))!= len(labels[None]):\n\t\t\traise ValidationError(\"\"\"\n\t\tLabels key is not of the expected type (not a dict). It is\n\t\tpossible that a label was given (string or bytes_like object for example,\n\t\ta :py:obj:`dict`).\"\"\"\n\t\tvalue_regex = None\n\n\t\tif isinstance(labels, dict):\n\t\t\tfor key, value in labels.iteritems():\n\t\t\t\tif not key in values:\n\t\t\t\t\traise ValidationError(\"\"\"Invalid labels for {}\nKeys: {}\"\"\".format(val, key))\n\t\t\tfor value in"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t# Validate keys and values in a single string.\n\t\tassert isinstance(labels[\"key1\"], six.string_types),\\\n\t\t\t\"Key must be a string.\"\n\n\t# Validate invalid labels for other keys.\n\tkeys = list(labels)\n\tlabels[\"key\"][keys[0]] = \"Value must be a string.\"\n\n\t# Validate missing labels.\n\tkeys = list(labels)\n\tlabels[\"not keys\"][keys[0]] = \"Value must be a string.\"\n\n\t# Validate values for keys that don't exist.\n\tfor k in [\"key\", \"value"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise\nimport datetime\nimport hmac\nimport logging\nimport os\nimport smtplib\n\nfrom boto import exception as boto_exception\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.image import MIMEImage\nfrom email.mime.image import MIMEImage\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\nfrom appengine_config import local_config\nfrom model.appsettings import CONFIGS\n\nDEBUG = CONFIGS[\"debug\"]\nSMTP_PORT = CONFIGS"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n    # Validate the labels keys against their regular expression or that the ones\n    # from the labels match the regular expression.\n    for k, v in labels.items():\n        if isinstance(v, str):\n            if v.startswith('$$') and not v.startswith('$'):\n                raise ValidationError('invalid label name: %s (key=%s)' % \\\n                                            (v, k))\n            if isinstance(v, dict):\n                _validate_labels(v)\n            else:\n                raise ValidationError('invalid label name: %s' % k)\n\n\n# Class used to represent the results of checks.\nclass Check:"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t_validate_labels({\"key\": \"key1\", \"key2\": \"invalid\"}.items()) # pylint: disable=redundant-keyword-arg\n\n\n\"\"\"\ndef _validate_type_int(type_):\n\t\"\"\"\n\tCheck that a type_ parameter is an integer.\n\t\"\"\"\n\tif type_ is None:\n\t\treturn _STR_OR_NONE\n\tfrom types import FunctionType\n\n\tif not isinstance(type_, FunctionType):\n\t\ttype_ = FunctionType\n\n\tif size(type_) > size(TypeDecorator):\n\t\traise ValidationError(\"type_ must"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tassert isinstance(err.messages, list)\n\t\tassert len(err.messages) == len(labels.iterkeys())\n\n\t\tfor label in keys:\n\t\t\tassert labels[label].keys() == labels[key].keys(), (\n\t\t\t\tlabel, keys[label].keys(), \n\t\t\t)\n\ndef _validate_keys(obj, field_name=None):\n\t\"\"\"\n\tCheck that objects are not empty (in the given fields), or if there\n\tare missing required key.\n\n\tAll fields of the object must be present, a key-value pair with \"all\" key."}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\t# This should match the path of the executable, or simply\n\t\t# check the name.\n\t\traise RuntimeError(\"Searching for resource name\")\n\n\t# Parse the source code and validate the resource name.\n\ttry:\n\t\twith open(rc_file) as rfile:\n\t\t\tcontent = rfile.read()\n\texcept EnvironmentError as ex:\n\t\traise ResourceNotFound(\"Failed to open {}!\".format(rc_file)) from ex\n\n\t# Check all required fields are present.\n\tfor field in (\n\t\t(\"resource_name\", str),\n\t\t(\"resource_version"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\t\n\timport re\n\tpatterns = [\n\t\tre.compile(r'^[a-zA-Z0-9_\\-]{1,25}$', re.I),\n\t\t]\n\n\tfor pattern in patterns:\n\t\tre_pattern = pattern\n\t\tpattern_compiled = re_pattern.pattern\n\t\t\n\t\ttry:\n\t\t\tresource_name = re_pattern.sub(\n\t\t\t\tr'\\1+', path_part)\n\t\t\tprint('Resource \"{}\" matched against \"{}\"'.format(pattern.pattern, resource_name))\n\t\t\t\n\t\t\tre_pattern = re.compile(r'^(?=.*)\\d+(?"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t return \"(?:\" + \"|\".join([item + \"(?:\\:\\:?\" + item + \")*?)\"\n\t\t\t   for item in _VALID_KRAKE_RESOURCE_NAMES])\n\ndef _parse_resource_name(resource_name):\t\t# pylint: disable=too-many-locals\n\t\"\"\"\n\tParse a resource name into a valid Krake resource name.\n\n\tSee also _get_resource_pattern above.\n\t\"\"\"\n\tmatch = _get_resource_name_regex()\n\tmatch.match(resource_name)\n\tnew_name = \"\"."}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\t# Build the regular expression using the named group (like the resource\n\t# name)\n\t# that was not defined using `regex_name`\n\tcompiled_pattern_group = re.compile('([\\*]{6,})\\s*;\\s*(.+?)(\\s*);')\n\n\t# Build the regular expression using the named group\n\t# (like the resource name but with a trailing $. So this pattern\n\t# doesn't match the trailing $).\n\t# See https://stackoverflow.com/questions/10327625/how-do-you-include-the-regular"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\t# Use a list here to support the legacy\n\t\t# version of the standard regex\n\t\tname_regex = re.compile(r\"%\\(([^\\)]+)\\)\")\n\t\t# Use a dict to pass into the regex with a single group\n\t\t# of regex matches.\n\treturn {'type': ['regex', None],\n\t\t\t'regex': [name_regex],\n\t\t\t'group': [False],\n\t\t\t'start': [False]\n\t\t\t}\n\n\ndef _get_resource_name_regex_and_group(pattern):\t\"\"\"\n\tBuild and return regular expressions that\n\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn None\n\nimport functools\nimport gzip\nimport json_utils as json\nimport logging\nimport os\nimport re\nfrom contextlib import contextmanager\n\n\nfrom django.conf import settings\nfrom django.db import IntegrityError, transaction\nfrom django.contrib.auth import models as auth_models\nfrom django.contrib.contenttypes import generic\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.contrib.auth.models import Group, AnonymousUser\nfrom django.core import validators\nfrom django.core.validators import MaxValueValidator, MinValueValidator\nfrom django.db import models, transaction\nfrom"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn [\n\t\t\tre.compile(\"^\\w+%$\"), # Regular expression\n\t\t\tre.compile(\"^\\w+%{?_!?}\") # Regular expression\n\t\t\t]\n\ndef _get_resource_path():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to\n\t\tvalidate the resource name of the Krake resources.\n\t\"\"\"\n\t\tpattern = _get_resource_name_regex()\n\t\treturn pattern.search(KrakeResource.BASE_PATH)\n\n_VALID_NAME_PATTERNS = [\n\tre.compile(r\"^"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    pattern = []\n    for pattern_object in resource_names:\n\tassert pattern_object[0:1] == \"\\\\\" or pattern_object[0:1] == \"\\\\:\\\\\"\n\tpattern.append(pattern_object)\n\n    return pattern\n\ndef get_class_regex():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe module class of modules.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the class name.\t\"\"\"\n\t\n    pattern = []\n    for pattern_object in class_names:\n\tassert"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\treturn (\n\t\t(Reg.get_pattern(resources, True) or Reg.get_pattern(resources, False))\n\t\t or Reg.get_pattern(resources, False)\n\t\t)\n\ndef _get_resource_name(regex):\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources using the regular expressions.\n\nReturns:\n    (str): the specified resource name, or raises\n\tRaises\n\tReturns None or str with a suitable regular expression (e.g.,\n\t'ns2resource:~"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn None\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Author: Simon Wu <simon@simonwu.edu.sg>\n#    Copyright (c) 2014 Acsone SA/NV (http://www.acsone.eu)\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version."}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    try:\t\t\t\t\t\t\t\t\n\tre.compile(value)\n\treturn True\n\ndef validate_ipv6_address(value):\t\t#TODO: Validate IPv6 address by trying to find valid IPv6 address \n\tif not value or len(value) < 2:\n\t\traise ValidationError('Invalid IPv6 address specified')\n\tif 0 < int(value) <= 255:\n\t\traise ValidationError('The provided IPv6 address could not be divided up into 16 bits')\n\treturn True\n\ndef validate_macaddress(value):\t\t#TODO: Validate MAC address by checking whether address is well"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\nclass ValidationError(Exception):\n    \"\"\"Error thrown when the validation check fails.\"\"\"\n    pass\n\n\n#######################\n#   USER\n#######################\nclass User(object):\n\t\n\t@staticmethod\t\n\tdef validate(username, password):\n\t\t\"\"\"\n\tValidates a username against the regular expression provided.\n\t\n\tArgs:\n\t\tusername   \tThe username that the username is validating.\n\t\tpassword\t\tThe password that the username is validating.\n\t\n\tRaises:\n\t\tValidationError: if the given username is not valid.\n\t\tError: if the given password is not valid.\n\t\t\"\"\"\n\t\t\n\t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif value and not regex.match(value):\t# match invalid regex\n\t\traise ValidationError(\"Value should only contain alphanumeric characters and '-' or '_'.\"\n\t\t\t\t\t, value)\n\ndef date(timestamp = None):\n\t\"\"\"\n\tReturn the current date (with timezone)\n\t\n\t:param timestamp: the timestamp to use as the format '%U-%m-%d %H:%M:%S.%f'\n\t\"\"\"\n\tif timestamp:\n\t\treturn datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\telse:\n\t\treturn"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\ndef validate_field(value, regex_validator, allow_empty=True):\n    \"\"\"\n\tValidate the given value against the corresponding regular expression.\n\n\tArgs:\n        value: the string to validate\n\t\tregex_validator: the given regular expression validator (that checks the supplied regex)\n\tallow_empty: by default, the attribute is required so you can only use False for a value that represents a non-existent value\n\t\"\"\"\n    match = regex_validator.match(value)\n    if match:\n\t\tif allow_empty:\n\t\t\tif match.groups()[2]:\n\t\t\t\treturn"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_size(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nReturns:\n    the validated value.\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_enum(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nReturns:\n    the validated value.\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\t\t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if \".\" not in value:\n\n#\t\traise ValidationError(\"\"\"value must be composed as string with separator.\n#                                                     e.g. \\n \\t\"\"\")\n\tif \"\\\"\\\"\" in value:\n\n#\t\traise ValidationError(\"\"\"value may contain at least one non space, beginning with \\n\"\"\")\n\n\tsplit_value = split_re.split(value)\n\tif len(split_value)!= 2:\n\t    raise ValidationError(\"\"\"value must have exactly two words, beginning with \\n\"\"\")\n\n\tresult = set(split_value)\n\tif len(split_value"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    m = REGEX_VALID.match(value)\n    if not m:\n        raise ValidationError('\"%s\" has an invalid regular expression.' % value)\n\ndef to_python(value):\n    \"\"\"\n\tConvert the given value to a regular python.\n\n\tArguments:\n    value  The value to be converted\n\n\tReturns:\n    The input value, converted to a python type (i.e. a python 3\n\ttype, such as `int`).\n\t\"\"\"\n    if value is None:\n        return 'None'\n    if value.startswith(('\"', \"'\")):\n        return value[1:-1]"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\t# this will raise a ValidationError if one of the fields is not blank\n\t\t# and the field is empty\n\t\tif value is None:\n\t\t\traise ValidationError(u\"%s must not be blank\" % value)\n\t\tif value in validators_valid_string(value):\n\t\t\traise ValidationError(u\"%s field contents are not allowed\" % value)\n\t\t# we check the field is of acceptable format\n\t\tfields_match = REGEX_FIELD_PATTERN.search(unicode(value))\n\t\tfor ffield in fields_match:\n\t\t\tif not ffield.match(unicode(value"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if value!= \"\" and value.startswith(\"'\"):\n        raise ValidationError(\"Value must start stripping the regular expression.\")\n\ndef valid_value(value):\n    if isinstance(value, str) and not value.startswith(\"'\"):\n\t\traise ValidationError(\"Value must not contain any symbols.\")\n\n\ndef unique_value(value):\n\treturn re.sub(\"[^a-zA-Z_]\", \"_\", value).lower()\n\n\ndef value_too_long_string(value):\n    return len(value) >= 250\n__author__ = 'karine'\n\nimport codecs\nimport"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\tcompiled = re.compile('^[A-Za-z0-9_?.]')\n\tif compiled.match(value):\n\t\traise ValidationError('A-Za-z0-9_ value has invalid characters')\n\n\treturn value\n\n\n\t\n'''\nCheck if input is a valid password\n'''\ndef is_valid_password(s):\n\tif len(s) == 0:\n\t\treturn True\n\tif re.search('^[-\\w\\._\\-]+$', s) is None:\n\t\treturn False\n\treturn True\n\n'''\nCheck if input is a"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    # Check if `key` is an ASCII character or not\n\tkey = validate_ascii_key(key)\n\n\t# Check if a key contains non-ASCII characters\n\ttry:\n\t\tre.match(key, '[^a-z0-9\\s]')\n\t\treturn True\n\texcept TypeError:\n\t\treturn False\n\n# Validate whether the given string can be parsed without failing;\ndef validate_ascii_key(s):\n\t\"\"\"\n\tValidates that `s` is a valid ascii string that can be in ASCII encoding\n\t(http://www.w3"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t#if len(key) < 5 or key[4]!= '0': raise ValidationError(\"An invalide key must be at least 1 character long\")\n        if regex.match('[\\d]+').match(_get_regex(), key): raise ValidationError('An invalide key must not contain [0-9a-zA-Z]', code=3)\n\ndef validate_pattern(patterns):\t\t\"\"\"\n\tValidate the given pattern against the given list of compiled regular expressions.\n\n\tArgs:\n\t    patterns: the list of compiled regular expressions (with no wildcards)\n\n\tReturns:\n\t    The"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\nclass ValidationError(Exception):\n\t\"\"\"\n\tValidation exception.\t\"\"\"\n\nclass EvaluationNotPossibleError(Evaluation):\n\t\"\"\"\n\tValidation function not possible (a value of this field can be omitted).\n\t\"\"\"\n\tdef check(self, value):\t\n\t\t\"\"\"\n\t\tValidate if value is in the field and check it is within the limit of it.\n\t\t\"\"\"\n\t\tif value < self.limit:\t\t\n\t\t\traise ValidationError(\n\t\t\t\t\"field {0} has reached {1}th record maximum of limit\"\n\t\t\t\t.format(self.field, value"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    re.compile(r'^[\\s]*$', re.IGNORECASE)\n\nclass Roster(models.Model):\n\t\"\"\"\n\tRepresents a single person or a set of people.\n\n\tMembers in Roster object may only be created directly through instances created via the Roster.\n\n\tFor convenience, the Roster class implements the same interface and should not be used directly\n\tby the application directly. To use this class, an Roster.to_xml may be used to create\n\ta xml document from a string containing XML in it.\n\n\tThe Roster class behaves like a Roster instance"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif not key:\n\n\t\traise ValidationError('A key is required')\n\t\n\tfor entry in key.split(' '):\n\t\tif not validate_regex.match(entry):\n\t\t\traise ValidationError(\"Invalid key, please see the docs for valid keys\")\n\n\ndef validate_user_from_key(key):\t\n\t\"\"\"\n\tValidate the given key for a given user by its regex on its home directory ($HOME).\n\t\n\tIf this is a valid user, try to validate the user against the given key.\n\tOtherwise, it updates their user_id in the database\n\t@"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# Check that the regular expression is in the set of allowed characters\n\t\tset_allowed_charset(key)\n\ndef validate_text(text, charset='utf-8'):\n\t\"\"\"\n\tValidate text with given charset.\n\nArgs:\n    text: The text to validate (unicode or str).\n\tcharset: Optional Characterset of strings indicating the charset to use from the text.\n\t\"\"\"\n\tif isinstance(text, basestring):\n\t\ttext = unicode(text)\n\tif charset:\n\t\tif not isinstance(charset, set):\n\t\t\traise ValidationError(\"Invalid charset to"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    if not isinstance(key, basestring):\n\t\traise ValidationError(messages.KEY_ERROR_KEY.format(key))\n\t\n\t# TODO: Make sure that some exceptions are raised if:\n\t#   a) The user has supplied a non-valid regular expression (including brackets\n\t#   and quotes) that is longer than 140 characters long\n\t#   b) The user has been given too many revealed words than 140 characters\n\t#   c) The user has been given too many revealed words\n\t#   d) The user has been given a word which is truncated"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# Split the given kkey into its pieces\n\t\ttry:\n\t\t\tpattern = r'\\s*(.*)\\s*\\$(%s)' % key.split(key.lower())\n\t\texcept UnicodeEncodeError:\n\t\t\t# For Python 3.x no longer provides the `i` prefix, which would lead\n\t\t\t# to strange non-ASCII characters for keys longer than 255 bytes.\n\t\t\tpattern = r'(?<!\\w)' + key.replace(r'\\w', r'\\s*')\n\t# Split the given char from its pieces\n\tidx = key.find(pattern)\n\tif idx"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# TODO error handling!\n\t\tpass\n\n@validate_key\ndef validate_number (number):\n    \"\"\"\n\tValidate the given number against the corresponding regular expression.\n\nArgs:\n    number: the string to validate\n\nRaises:\n    ValidationError: if the given number is not conform to the regular expression.\t\"\"\"\n\t\t# TODO error handling!\n\t\tpass\n\n@validate_key\ndef validate_time (time):\n        \"\"\"\n\tValidate the given time against the corresponding regular expression.\n\nArgs:\n    time: the string to validate\n\nRaises:\n    ValidationError: if the given"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t#raise ValidationError(\"Value can not be empty.\")\n\n\tif not VALID_KEY.search(key):\n\t\traise ValidationError(\"%s is not a valid regular expression.\" % key)\n\tif len(key) > 20:\n\t\tvalidation_error = ValidationError(\"Value must be between 4 and 20 characters long: %s\" % key)\n\treturn key\n\ndef validate_label(label):\n\tif len(label) > 250:\n\t\tvalidation_error = ValidationError(\"Value must be between 250 and 310 characters long: %s\" % label)\n\t\n\tif label[0]"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tif not first_level:\n\t\t\tdict_copy = {'first_level': [] }\n\t\telse:\n\t\t\tdict_copy = {'first_level': first_level }\n\t\treturn dict_copy\n\telse:\n\t\tdict_copy = {'first_level': None }\n\t\treturn dict_copy\n\ndef generate_list_of_observer_paths(config):\n\treturn list(generate_default_observer_schema_dict(config['manifest_dict']))\n\ndef generate_list_of_namespace_paths(config):\n\treturn list(generate_default_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n    def dict_to_bundle(dict):\n        manifest = []\n        for field_name, field in list(dict.items()):\n            if not field_in_manifest(dict, manifest_dict, field):\n                field_name = field[0]\n                field_value = manifest_dict.get(manifest_dict.keys()[0], {})[field_name]\n                manifest.append((field_name, field_value))\n        res_dict = {}\n        if len(manifest) > 1:\n            res_dict = {manifest[0]:manifest[1]}\n        return res_dict\n\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\t# Set defaults\n\t\tmanifest_dict = manifest_dict \\\n\t\t\tif first_level \\\n\t\t\telse {\n\t\t\t\t# If no initial dict is passed in it's empty, append a dummy dict\n\t\t\t\t# This is used later to fill in empty entries from ``manifest_list``\n\t\t\t\t# which actually have values to be ignored but preserved with a list\n\t\t\t\tif not manifest_dict \\\n\t\t\t\telse {}\n\t\t\t}\n\n\tgenerated_observer_schema = {}\n\tfor field in MANIFEST_MANIFEST_FIELDS:\n\t\tfield_default = field in manifest_dict \\\n\t\t\tif first"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n    new_dict = {}\n    non_list_fields = []\n    fields = manifest_dict.get(None)\n    if fields is None:\n\t    if first_level:\n\t\tfor key, value in manifest_dict.items(include_hidden=False):\n\t\t    non_list_fields.append((\"exclude\", key, value))\n                    else:\n                        new_dict[key] = manifest_dict\n        else:\n            new_dict[\"include\"] = fields\n    else:\n        if first_level:\n            for key, value in manifest_dict.items(include_hidden=False):\n                new_dict"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tif (manifest_dict is None) or (not is_resource_list(manifest_dict))):\n\t\t\traise ArgumentTypeError(\"{0}: '{1}' field dictionary must be provided as manifest dictionary\"\n\t\t\t\t\t.format(self.__class__.__name__, self._class_name))\n\n\t\treturn dict()\n\telif isinstance(manifest_dict, list):\n\t\tif first_level or not is_resource_list(manifest_dict):\n\t\t\tfor i in range(len(manifest_dict)):\n\t\t\t\tif first_level:\n\t\t\t\t\tfirst_level_copy = copy.copy(manifest_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tmanifest_dict = {}\n\tif first_level:\n\t\tif isinstance(manifest_dict, dict):\n\t\t\tresult = {}\n\t\t\tfor entry in manifest_dict:\n\t\t\t\tmanifest_dict[entry] = {}\n\t\t\tfor entry in manifest_dict:\n\t\t\t\tif isinstance(manifest_dict[entry], list):\n\t\t\t\t\tmanifest_dict[entry].append(result)\n\t\t\t\telse:\n\t\t\t\t\tresult[entry] = manifest_dict[entry]\n\t\t\tmanifest_dict = result\n\t\telse:\n\t\t\tresult = build_observer_schema_dict(manifest_dict)\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tif first_level:\n\t\t\tif 'Observer' not in manifest_dict:\n\t\t\t\treturn dict()\n\t\t\tschema_dict = {'Observable': manifest_dict['Observer']}\n\t\t\tif 'Observables' not in manifest_dict:\n\t\t\t\tname_prefix = 'RemapObservable'\n\t\t\t\textension_key = name_prefix + '*'\n\t\t\t\textension_value = 'Observer'\n\t\t\t\tschema_dict[extension_key] = extension_value\n\t\t\t\textension_value = extension_value + '%'\n\t\t\t\tschema_dict[name_prefix + extension_key] = extension_value"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t                                                        \n\tdef _replace_item(key, value):\n\t\t\"\"\"\n\t\tReplaces a key in file path so that path is not modified\n\t\tafter it's been copied to the manifest.\n\t\t\"\"\"\n\t\tnew_path = get_full_path(value)\n\t\t# if key is already a non-list key, remove the item from the existing dictionary\n\t\tif key in entry_schema[key] and len(entry_schema[key][key].keys()) == 0:\n\t\t\t\tentry_schema[key][key] = value\n\t\t\telse:\n\t\t\tentry_schema"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\treturn { k : v if first_level else v[k] for (k, v) in manifest_dict.items() }\n\telif isinstance(manifest_dict, collections.Mapping):\n\t\treturn { k : generate_default_observer_schema_dict(v, first_level=bool) for (k, v) in manifest_dict.items() }\n\telif isinstance(manifest_dict, list):\n\t\treturn [generate_default_observer_schema_list(x, first_level=bool) for x in manifest_dict]\n\treturn None\n\ndef generate"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\ndef get_publisher_dict(namespace):\n    \"\"\"\n    This function takes a namespace with Kube's \"service\" and returns a dictionary of \n    the publisher classes from their names. This is useful for passing\n    Kube context information into the event that a resource has been created in Kube.\n\n    Kube has no publisher.\n    It contains the list of the publisher classes used.\n    \"\"\"\n    api_version = api_version_tag.release_string\n    publisher_list = _get_kube_context().publisher_list\n\n    # Iterate all the possible values of the namespace\n    for item_list in publisher"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\ndef clear_all_previously_applied_manifests_from_resp(\t\"\"\"\n\tTogether with :func:``clear_all_previously_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``previously_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function is a callback"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor metadata in _update_last_applied_manifest(observer_schema,\n                                                                response.body['last_applied_manifest']) :\n\t\t\t# set last_applied_manifest_id and etag\n\t\t\tif 'last_applied_manifest_id' in metadata:\n\t\t\t\tdel metadata['last_applied_manifest_id']\n\t\t\tif 'etag' in metadata:\n\t\t\t\tdel metadata['etag']\n\n\t# now go through all observed values, and add new content\n\t_update_content_with_resp(\t\"\"\"\n\n\t\tMERGE (  # copy this to the last applied response\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\tlast_applied_manifest = set(last_applied_manifest)\n\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\"\"\"\n\t\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\t\tfunction is called recursively to updatea partial ``last_applied_manifest``\n\t\tfrom a partial Kubernetes response\n\t\t\"\"\"\n\t\n\n\tupdate_last_applied_manifest = set(last_applied_manifest)\n\tobserver_schema = set(observer_schema)\n\tresponse = set(response)\n\tupdate_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfor r in response[0]:\n\t\tr['last_applied_manifest'] = dict([[e.get('last_applied_manifest', {})\n\t\t\tfor e in r['last_applied_manifest'] if e['last_applied_manifest']])\n\n\t\tfor e in r['observer_schema']:\n\t\t\te['last_applied_manifest'] = observer_schema['last_applied_manifest']\n\n\treturn response\n\n\n@time_this('update_last_applied_resource_version')\ndef update_last_applied_resource_version(\n\t\tversion_id "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfor field in previous_pending_manifest_fields:\n\t\tif previous_pending_manifest_fields[field]:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tprevious_pending_manifest_fields[field]\n\t\t\t)\n\t\t\tif field in observer_schema:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tobserver_schema[field]\n\t\t\t\t)\n\t# If no list of fields, update_last_applied_manifest is not done\n\tif len(previous_pending_manifest_fields):\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\treturn\n\n\tlast_applied_manifest_json = get_latest_applied_manifest()\n\n\tfor field in filter(filter_dict_values,\n\t\t\tmap(lambda e: (e, field),\n\t\t\t\tmap(lambda x: (x.get('key'), x.get('name'), x.get('namespace'))),\n\t\t\tlast_applied_manifest_json.items())):\n\t\tif 'value' not in field:\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tlast_applied_manifest_value = field.get('value')\n\t\t\tif not isinstance(last_applied_manifest_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\treturn\n\n\tif len(last_applied_manifest) <= 0:\n\t\treturn\n\n\tlast_applied_manifest_version = last_applied_manifest[0].get('version')\n\ttry:\n\t\tlast_applied_manifest_key = last_applied_manifest[0].get('key')\n\texcept:\n\t\tpass\n\n\tif last_applied_manifest_key is None:\n\t\tlast_applied_manifest_key = last_applied_manifest_dict_list['key']\n\n\tif last_applied_manifest_version is None:\n\t\treturn\n\n\tif"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\t# Inject the last_applied_manifest items\n\tfor item in last_applied_manifest:\n\t\t# Add everything relevant to the response\n\t\tresponse += item\n\n\t# Update our list of the latest changes\n\tprint \"Updating next changed manifest items\"\n\n\tif observer_schema and update_observer_schema:\n\t\tif observer_schema[0].get('type') == 'object':\n\t\t\tif len(jsondata['items']) == 1 and '_created_at' in jsondata['items'][0]:\n\t\t\t\t# We only want to update a single change if a list is"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\n\tif hasattr(last_applied_manifest, 'next') and len(last_applied_manifest) > 0:\n\t\tlogger.info('Update last applied manifest list {}..'.format(last_applied_manifest))\n\t\tif hasattr(observer_schema, 'next') and len(observer_schema) > 0:\n\t\t\tlogger.info('Update last applied manifest list {}..'.format(observer_schema))\n\t\t\tif hasattr(response, 'next'):\n\t\t\t\tlogger.info('Update last applied manifest list {}..'.format(response))\n\t\telse:\n\t\t\tlogger."}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor field in ('last_applied_manifest', 'observer_schema'):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tfield,\n\t\t\t\t\tresponse['last_applied_manifest'])\n\t\t\tupdate_last_applied_manifest_dict_from_obj(\n\t\t\t\t\tfield,\n\t\t\t\t\tresponse['observer_schema'],\n\t\t\t\t\tresponse['response']['observer_schema'])\n\n\nif __name__ == '__main__':\n\t\"\"\"\n\tUpdate the API as needed for the next generation\n\t\"\"\"\n\tprint_api_version()\n\tapi = KubernetesAPIClient("}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\n\n\ndef get_observer_from_response(response, observer_schema=None, response_schema=None):\n\t\"\"\"Returns the `observer` that corresponds to a single provided result.\n\n\tIf ``observer_schema`` input is not provided, the result from the server is\n\treturned as-is, and ``response_schema`` is not provided.\n\t\"\"\"\n\n\tif 'body' not in response:\n\t\traise Exception('missing message: body')\n\n\tif observer_schema is None:\n\t\tobserver_schema = get_observer_schema('response')\n\n\tif'result"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tif not observer_schema:\n\t\t\t# Nothing to do here\n\t\t\treturn\n\t\t\t# Nothing to do here\n\t\tif not hasattr(observer_schema, 'keys'):\n\t\t\traise KeyError(\n\t\t\t\t'The observer schema was not provided so we cannot '\n\t\t\t\t'update it in the partial results dict')\n\n\t\tfor k in observer_schema:\n\t\t\tif hasattr(observer_schema[k], 'previous_version_list'):\n\t\t\t\tif k not in observer_schema[k].previous_version_list:\n\t\t\t\t\t# We already have a previous version: ignore\n\t\t\t\t\tcontinue"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\n\t\tfor k in last_applied_manifest.keys():\n\t\t\tif k not in observed:\n\t\t\t\tobserved[k] = last_applied_manifest[k]\n\n\t\tfor k in observed.keys():\n\t\t\tif k in last_applied_manifest.keys():\n\t\t\t\tif k not in response.keys():\n\t\t\t\t\tresponse[k] = last_applied_manifest[k]\n\n\ndef get_last_applied_manifest_dict_from_resp(resp):\n\n\tlast_applied = copy.deepcopy(resp)\n\tif 'items' in last_applied and last"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\told_value_to_update = json.loads(response.get(\"last_applied_manifest\"))\n\t\tlast_applied_manifest.update_last_applied_manifest(\n\t\t\t\tjson.dumps(old_value_to_update))\n\ndef create_last_applied_manifest_dict_from_resp(message, \n\t\t\t\t\tobserver_schema, response):\n\t\"\"\"\n\tCreates a ``last_applied_manifest`` dictionary from a Kubernetes API response.\n\tThe fields for these dicts are:\n\t- ``observer_schema`` (required): the name of an Agent's"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tif 'last_applied_manifest' in last_applied_manifest:\n\t\t\tif 'observer_schema' in last_applied_manifest['last_applied_manifest'][observer_schema]:\n\t\t\t\tupdated_at = last_applied_manifest['last_applied_manifest'][observer_schema]['updated_at']\n\t\t\t\tif updated_at is not None: # pylint: disable=no-else-return\n\t\t\t\t\tupdated_at = str(datetime.utcfromtimestamp(int(updated_at)))\n\t\t\t\telif updated_at == '': # don't need the timezone on the value, just"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\n    # Find the last event, add it to the dictionary\n    last_applied_manifest_dict = last_applied_manifest_dict_from_resp(\n            '{}/{}'.format(observer_schema['event_name'], observer_schema['version']))\n\ndef update_last_applied_manifest():\n    \"\"\"\n    Updates the last applied_manifest field in the Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest``\n        observer_schema (dict): partial ``observer_schema``\n\n    Raises:\n        KeyError: if the observed field is not present in the Kubernetes"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t                                                        \n\t                                                        \n\t                                                        \n\tif 'last_applied_manifest' in cherrypy.serving.request.known_headers:\n\t\tlast_applied_manifest = cherrypy.serving.request.get_header('last_applied_manifest')\n\t\t\n\n\tfor observed_field_name in last_applied_manifest:\n\t\tupdated_field_val = last_applied_manifest[observed_field_name]\n\t\tdict_val = last_applied_manifest[observed_field_name]\n\t\t\n\treturn dict_val\n\t\n\t\ndef update_last_applied_manifest(*args, **kwargs"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t# We first pull the non-empty value off every field - we'll need this\n\t\t# to know when the json_body is written out to ensure that later\n\t\t# lookups can succeed in order to make one more valid JSON body\n\tresult = last_applied_manifest.copy()\n\tif \"last_applied_manifest\" in result:\n\t\tlast_applied_manifest.pop(\"last_applied_manifest\")\n\treturn result\n\ndef add_to_pending_manifest(response):\n\t\"\"\"\n\tParses a JSON response into a podcast-encoded string and adds it to"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\twith closing(StringIO(resp.text)) as f:\n\t\tresp_obj = json.load(f)\n\n\tdef add_last_applied_manifest(key, value):\n\t\tlatest_field = resp_obj.get('latest_manifest', {}).get(key)\n\n\t\t# if the latest field isn't already present it's likely that\n\t\t# there's not an expiration time (e.g. a valid token after\n\t\t# midnight)\n\t\tif v2.validate_on_pending_field(latest_field, value):\n\t\t\tif latest_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t# check the last_applied_manifest is available\n\t\tif 'last_applied_manifest' in resp:\n\t\t\tlast_applied_manifest = resp['last_applied_manifest']\n\t\telse:\n\t\t\traise Exception(\"No last_applied_manifest field has been initialized\")\n\n\t\t# go through the list of dicts for this field\n\t\tlast_applied_manifest.update({'last-applied-manifest': last_applied_manifest})\n\n\t\tresp['last-applied-manifest'] = last_applied_manifest\n\n\ndef update_last_applied_manifest_str_from_resp(resp"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    # Build a map from resource type to class to initialize, populated from\n    # the default handler and exposed on the service handler.\n    resource_map = {\n       'status': 'list',\n        'host': 'kube-apiserver',\n        'namespace': 'kube-system namespace',\n       'spec_version': 'kube-system apiserver spec version',\n       'status_reason':'status_reason',\n       'spec': {\n            'v1': 'v1',\n            'v2': 'v2',\n            'kubelet': 'kubelet',\n            'kubes': 'kubes"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n        # Initialize source information\n        source_path = get_absolute_path(app.config['source'])\n        if source_path is None:\n            raise Exception(\"{0} is not a valid source path.\".format(app.config['source']))\n\n        # Generate a default observer schema (for no event sources)\n        return generate_observer_schema(source_path, app.config)\nfrom __future__ import unicode_literals\n\nimport datetime\nfrom operator import attrgetter\nimport random\nimport re\nimport warnings\nfrom weakref import WeakKeyDictionary\nfrom unittest import skipUnless\nfrom xml.dom.minidom"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\t\n\tnamespace = 'v1.0'\n\tpattern ='metadata/annotations/$namespace/pods/' + namespace + '/events/*/'\n\t\n\tfor event in spec.traverse(lambda event:\n\t\tif 'objectId' in event and 'operationId' in event['objectId']):\n\t\tschema = generate_observation_schema(event)\n\telse:\n\t\treturn None\n\t\n\tfor event in spec.traverse(lambda event:\n\t\tif'schemaVersion' in event and event['schemaVersion'] == 'v1.0':\n\t\t\tschema = generate_metadata_schema(event"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\t\t\n\tgenerate_default_observer_events_schema(app)\n\nif __name__ == \"__main__\":\n\tapp = App(\"\")\n\n\twith app.app_context():\n\t\tapp.logger.info(\"Initializing kraken controller\")\n\t\tapp.set_data(app.config[\"PRODUCTION_CONFIG\"])\n\t\tapp.logger.info(\"Loading Trait routes\")\n\t\tfor tracer in app.config[\"TRACING\"]:\n\t\t\ttracer_endpoints = []\n\t\t\tfor endpoint in tracer.get(\"endpoints\", []):\n\t\t\t\ttracer_endpoints.append(Endpoint(endpoint"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    for k, v in spec.manifest.items():\n        if v['status']!= 'deleted':\n            if not v['labels'] or not v['last_observed']:\n                # create the resource schema for the given Kubernetes resource label\n                for label in v['labels']:\n                    app.logger.info('[ {0} in resource] Creating default observer schema {1}.{2} for label {3}'.format(label, k, v['labels'], v['last_observed']))\n                    v['last_observed'] = ''\n\n                for label in v['last_observed']:\n                    app.logger.info('[ {0"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tassert len(app.kraken_apis) == 0\n\t\tassert not app.kraken_apis[0].schema[0].in_clusters\n\n\t# Create the required service names (not unique, that we don't use as\n\t# a group) under which all of our services can be placed in the\n\t# Kubernetes pods\n\tapp_desc = generate_service_desc(app)\n\tapp_defs = get_service_defs(app)\n\tapp_defn = app_defs.pop('AppDef')\n\tservice_names = []\n\t"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\t# Generate the root resource namespace\n\tns = APP_KIND.format(APP_NAME, APP_VERSION, APP_BUILD_ID, APP_RELATIVE_PATH)\n\n\t# Add the root resource manifest\n\tif'metadata' in ns and ns['metadata']:\n#\t\t\tns['metadata'].append(ManifestResource('/metadata',\n#                                                   '/namespace'))\n            ns['metadata'] = [\n                {\"name\": \"metadata\", \"namespace\": \"kubernetes\"}\n            ]\n\t\t\n        return [\n            Resource('/' + ns['name'] + '/' + k,\n             Endpoints('data',"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n        # Generate the default observer schema\n        k3resource = K3Resource()\n        k3resource.load()\n\n        # Initialize the default observer\n        r = k3resource.observer.Core(\n            'ns0:observer:DefaultObserver',\n            kube_apis=[k3resource]\n        )\n\n        # Set up the client to receive responses from the config server\n        r.config.K3_ENDPOINT_URL = app.k3kv.io.server.port\n        return r\n#!/usr/bin/env python\n__author__ = 'greghines'\nimport numpy as np\nfrom numpy."}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tdef _build_application_schema(self):\n\t\tresource_path = get_resource_path(app.name,'resource')\n\t\twith open(resource_path) as f:\n\t\t\tdata = f.read()\n\n\t\t\t# TODO(mjanuszczak): we are currently using legacy namespaces here, but may not yet be supported...\n\t\t\tnamespaced = Namespace(namespaces[data[22:30]], data[30:])\n\n\t\t\treturn {\n\t\t\t\t'apiVersion': 'v1',\n\t\t\t\t'kind': 'Api',\n\t\t\t\t'version': {'spec': '"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    def generate_event(self):\n        \"\"\"\n        Generate a kubernetes event object, formatted as a dict\n        - name: event name\n        - namespace: namespace we are creating the corresponding event; \"namespace\" can be\n          used to reference the name\n        - description: general description\n        - status: status of event (True|False)\n        \"\"\"\n        event_schema = {\n            'event_type':'Kubernetes.Event',\n           'status':self.get_status()}\n       # For the creation of a Kubernetes event\n        event_schema['name']=self.get_info().get('name',None)\n        event_"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\n\tout_props = ('sql', 'params', 'in_style') + tuple(out_params) + tuple(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif isinstance(p, str) else tuple(p)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor p in param_values)\n\tparam_tuples = tuple([(p, q) for p, q in zip(params, param_pairs)])\n\treturn tuple(out_props + param_tuples)\n\n\ndef to_select(\n\tquery: Sequence,\n\tquery_args: Mapping[Tuple[str, str], List[str]],\n\treturn_type"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\traise NotImplementedError()\nfrom datetime import datetime\n\nfrom django.db import models\nfrom django.conf import settings\nfrom django.utils.timezone import now\n\nfrom allauth.socialaccount import providers\nfrom allauth.socialaccount.providers.base import ProviderAccount\nfrom allauth.socialaccount.providers.oauth2.client import OAuth2Client\nfrom allauth.socialaccount.providers.oauth2.views import (OAuth2Adapter,\n                                                          OAuth2CallbackView,\n                                                          OAuth2LoginView,\n                                                          OAuth2CallbackView)\nfrom allauth.socialaccount.providers.oauth2.views"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tpass\n\n\tdef __str__(self):\n\t\treturn repr(str(self))\n\n\tdef __repr__(self):\n\t\treturn '%s(%s)' % (\n\t\t\tsuper().__repr__(),\n\t\t\t', '.join('%s=%r' % (repr(key), repr(val))\n\t\t\tfor (key, val) in self.items())\n\t\t)\n\n#---- public api interface\n\ndef get():\n\t\"\"\"\n\tEither returns :class:`~collections.abc.Empty` or throws :exc:`IndexError`\n\tif not all are loaded. Otherwise returns :class:`~"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\n\nclass SQLSelectBuilder(object):\n\t\"\"\"\n\tConvert the SELECT query to a `SELECT... FROM... WHERE...\n\tquery as an inner value and as :class:`.ColumnReference`.\n\n\t:param query: The SELECT query :class:`~sqlite3.Cursor.execute` query\n\t\tto be converted to :class:`.ColumnReference`.\n\t:param params: The parameters to the SELECT query.\n\t:param kw: The keyword arguments to :class:`.Query.compile` when compiling.\n\t:param context: The global or application context.\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\ti = 0\n\t\tfor name, val in self._in_style.items():\n\t\t\ti = i + 1\n\t\t\tif not isinstance(val, (int, float, complex, str)):\n\t\t\t\traise TypeError(\"expected an identifier\")\n\t\t\tif hasattr(val, \"_params\"):\n\t\t\t\tfor param in val._params:\n\t\t\t\t\tif not isinstance(param, Mapping) or param._name not in param._params:\n\t\t\t\t\t\traise TypeError(\"non-keyword list of mappings\")\n\t\tfmt = \"{{{0}}}{1}{{{2}}}\".format(\n\t\t\t[\"_\" + str("}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\traise NotImplementedError\n\n\n@typecheck(sql: str, parameters: Optional[str])\ndef match(param):\n\t\"\"\"\n\tCheck the given SQL command against the input parameters.\n\n\t- :class:`str` or :class:`bytes` :class:`bytes`\n\t- :class:`dict` of :class:`str` :class:`int`\n\t- :class:`str` or :class:`bytes` :class:`str`\n\t- :class:`int` or :class:`bytes` :class:`int`\n\t- :class:`str` or :class:`"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tassert (isinstance(self.raw, _TYPEFLAG_TYPES) or isinstance(self.sql, _TYPEFLAG_TYPES)), self.raw\n\t\tresult = []\n\t\tfor name, (param_type, param_name) in self.sql.sql_params.items():\n\t\t\t# we ignore parameter name for now (it would raise if there are\n\t\t\t# multiple SQL parameters inside a single keyword arg that could be\n\t\t\t# a parameter type that could also be a key in a tuple). Therefore\n\t\t\t# if the param_name has a colon and a colon plus the colon, use that"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t return (\n\t\t\tsql,\n\t\t\ttuple(params)\n\t\t)\n\n\n#:.. versionadded:: 1.2\n#:.. versionchanged:: 1.3\nquote_name = _bind_params(None, \tUnicode)\n\n\ndef _parse_like_term(s):\n\tquote = _quoted(s)\n\tif not quote:\n\t\treturn None\n\treturn unicode(quote)\n\n\ndef _parse(s):\n\t\"\"\"\n\tParse an SQL query, splitting query into parts. The default\n\tis a ``\\\\n``-separated list of terms.\n\t\"\"\""}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn self.__out_style.copy()\t\t# set in-style parameters; must be shallow copy\n\t\n\n\t@abstractmethod\n\tdef execute(self):\t\t# TODO: implement in-style queries\n\t\traise NotImplementedError\n\n\n\t@abstractmethod\n\tdef fetchone(self):\t\t# TODO: implement fetchone\n\t\traise NotImplementedError\n\n\n\t@abstractmethod\n\tdef fetchmany(self, size=None):\t# TODO: implement fetchmany\n\t\traise NotImplementedError\n\n\n\t@abstractmethod\n\tdef fetchall(self):\t\t# TODO: implement fetchmany\n\t\traise NotImplementedError\n\n\n\t@abstractmethod"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tout = dict()\n\n\t\tparams = self._parse_params(params)\n\n\t\t# convert numeric parameters to regular or named parameterised parameters\n\t\tfor param in params:\n\t\t\tif isinstance(param, _NumericParameter) and param.name in params:\n\t\t\t\tout[param.name] = params[param.name]\n\n\t\t# handle unknown parameters we don't know about\n\t\tfor key, value in out.items():\n\t\t\tif value is not None:\n\t\t\t\tself.error(\"Unknown keyword %r for query %r\" % (key, value))\n\n\t\treturn tuple(out"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn iter(self.query(params, multi=True))\nfrom django.core.urlresolvers import resolve, Resolver404, get_script_prefix\n\nfrom django.test.testcases import TestCase\n\nfrom wagtail.wagtailcore.models import Page, Site\nfrom wagtail.tests.wagtailusers.models import Group\nfrom wagtail.wagtailcore.tests.utils import WagtailTestUtils\n\n\nclass TestUtilityFunctions(TestCase):\n    def test_resolve_single_page(self):\n        self.assertIsInstance(resolve('/bar/').func, Page)\n\n    def test_resolve_simple_page(self"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn tuple(self._formatmany(i) for i in self)\n\n\t##\n\t# String representation helpers\n\t##\n    _sqlquote = b'\"'\n\n\t##\n\t# Iterator over all entries in the table.\n\t##\n\t@property\n\tdef entries(self):\n\t\t\"\"\"\n\t\tAn iterator of (name, value) tuples containing entries from the\n\t\tsame order the parameters were given.\n\t\t\"\"\"\n\t\treturn list(zip(*self.aslist()))\n\n\tdef names(self):\n\t\t\"\"\"\n\t\tAn iterator of the names of all entries.\n\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn format(query,\n\t\t\ttype_ = type(query),\n\t\t\tparam_names = (param_names or (),),\n\t\t\tmultiple_params = len(sql_params),\n\t\t\tmultiple_params_defaults = [None] * len(params)\n\t\t\t)\n\ndef formatmany_params(*params):\n\t\"\"\"\n\tFormat parameters in \"many-style\" style.\n\n\tParameters may be :class:`collections.abc.Mapping` or :class:`collections.abc.Sequence`::\n\n\t\t# The length of *parameters*.\n\t\t#\n\t\tquery = formatmany()"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttry :\n\t\t\tout = {}\n\t\t\tfor i in range(len(params)) :\n\t\t\t\tout[params[i]] = params[i]\n\t\t\treturn out\n\t\texcept Exception as e :\n\t\t\tif hasattr(e,'message') :\n\t\t\t\tif 'closed' in e.message :\n\t\t\t\t\traise MyError(\n\t\t\t\t\t\"Error when converting SQL query, \"\n\t\t\t\t\t\"you must close it before passing it to \"\n\t\t\t\t\t\"your sql parser\",\n\t\t\t\t\te\n\t\t\t\t)\n\t\t\tif isinstance(e, tuple) :\n\t\t\t\traise MyError(\"'tuple' object is not"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn (\n\t\t\tlambda items:\n\t\t\tparam(tuple(\n\t\t\t\tself.to_sql(item).to_fmt(tuple(item).to_param(item))\n\t\t\t\tif isinstance(item, dict) else item\n\t\t\t\tfor item in items\n\t\t\t))\n\t\t)\n\n\tdef from_sql(self, sql):\n\t\t\"\"\"Convert the `sql` data into a :class:`dict` of ``str`` to :class:`list` containing\n\t\tthe arguments defined within the :class:`~sqldict.sqldict` data.\n\t\t\"\"\"\n\t\tparams, params_or_"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\n\n\ndef formatdict(\t\"\"\"\n\tFormat the output of a SQL query into its individual form.\n\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Iterable` of :class:`~collections.abc.Mapping`)\n\n-       *params* (:class:`~collections.abc.Sequence`) contains each set of\n        converted out-style parameters (:class:`dict` or :class:`list`).\n\n\nReturns a :class:`list` containing:\n\n-       The formatted SQL query (:class:`str`"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn tuple(self.formatext(v, in_style) for v in self)\n\n\tdef format_in(self, **kwargs):\n\t\t\"\"\"\n\t\tConvert the SQL query in-style parameters to a new format suitable\n\t\tfor consumption by the :class:`~collections.abc.Sequence`.\n\n\t\t*kwargs* (optional): Any additional keyword parameters accepted\n\t\tby the :class:`~collections.abc.Mapping` and :class:`~collections.abc.Sequence`\n\t\tinstances, which then come into play when generating the SQL\n\t\tquery.\n\t\t\"\"\"\n\n\t\treturn \"\\n\"."}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttbl = []\n\t\tfor p in params:\n\t\t\tif isinstance(p, Mapping):\n\t\t\t\ttbl.extend(paramToDict(p))\n\t\t\telif isinstance(p, Sequence):\n\t\t\t\ttbl.extend(range(int(p[0]), int(p[1])+1, 1))\n\t\t\telse:\n\t\t\t\traise TypeError(\"p should be either mapping or sequence, got %r\" % (p,))\n\t\tretval = (TABLE(tbl_name, tbl), )\n\t\treturn retval\n\n\tdef sql(\n\t\t\ttable_name, sql_query_params, many_params"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\n\n\treturn (\n\t\t\"sqlquery\",\n\t\tNone,\n\t\t_format_params(\n\t\t\tparams,\n\t\t\t_format_many(\n\t\t\t\tmany_params,\n\t\t\t\t_fmt_params(multi=True),\n\t\t\t\t_make_params(vals)\n\t\t\t),\n\t\t\t),\n\t\t)\n\t)\n\ndef _format_params(params, multi=False):\n\t\"\"\"\n\tReturn the SQL parameters (as tuples) for a SQL query *params*.\n\n\tExample:\n\n\t    >>> sqlparam = SQLParam()\n\t    >>> sqlparams = SQLParams([\n\t        ('"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tif 'params' in kwargs:\n\t\t\titer = kwargs.pop('params')\n\t\t\toutlist = iter.next()\n\t\t\toutdict = iter.next()\n\t\t\tfor out in outdict: \n\t\t\t\tout['out'].update(outlist.todict())\n\t\t\t\tyield out\n\ndef formatsql(*sql,**kwargs):\n\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif self.fs is None: return False\n#\treturn True\n\ndef add(obj, pfx):\n\t\"\"\"\n\tAdd object to path.\n\n\t@type  obj: object\n\t\n\t@param  obj: object to add\n\t@type  pfx: object\n\n\t@return  return True if added, False otherwise.\n\t\"\"\"\n\treturn add_object(obj, pfx, '')\n\nclass AddObject(object):\n\t\"\"\"\n\tAdd object to pyfs.object.\n\t@type obj: object\n\t\n\t@param obj: object to add"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif not self.isOCFSFile(path):\n\t\t\treturn None\n\t\twarnings = [\"\", \" \", \".\"]\n\t\tfor line in self.ioObject.readlines():\n\t\t\tif line.startswith(\"    \") and not line.strip():\n\t\t\t\twarnings.append(line[2:].lower())\n\t\t\telif warnings[-1].split(\" \")[-1] in line:\n\t\t\t\twarnings[-1] = line.strip()\n\t\tif warnings:\n\t\t\treturn True;\n\t\treturn False\n\t\n\t# Returns (bool, int) or (None, string) if one or more items were"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n#\t\tif not os.access(path, os.R_OK):\n#\t\t\traise ValueError(\"No such file or directory: %s\" % path)\n#\t\t\n#\t\ts = os.stat(path)\n#\t\tos.chmod(path, s.st_mode | stat.S_IWRITE | stat.S_IREAD | stat.S_IXOTH)\n#\t\treturn True if not os.access(path, os.W_OK) else False\n#\"\"\"Unit tests for the ``maasserver.view.users``"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn True\n\ndef main():\n\t\"\"\"\n\tProgram entry point; reads OCFL file and checks if required files are present and returns a list of errors.\n\t\n\tThe 'OCIFL' program allows for both OCFL- and vocabs-python libraries or in the same directory.\n\t\n\tIf the script exits with a non-zero return status code (0), a general program exits with error code 128.\n\tIf not all files are present, a program exit status 0 is returned instead of an error code\n\t(1) If no errors were found, a special program exit status"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tvalidate_pyfs(path, validate_bom)\n\t\tself.pyfuse.validate(self.root + path)\n\n\nclass Archiver(object):\n\t\"\"\"Wrapper for pyfs build archive for creating or removing archive.\n\tIn particular if it is a standalone archive, this class acts as an abstraction for creating an archive and removing\n\tit when no archive is being created.\n\t\"\"\"\n\n\tdef __init__(self, path):\n\t\t\"\"\"Build Archiver object at archive path.\"\"\"\n\t\tself.pyfuse = Pyfuse(path)\n\n\t\tif not self"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\n\nclass OcflException(Exception):\n\t''' Custom exception object for OCFL class.\n\t'''\n\tpass\n\t#\n#   def __init__(self, value):\n #    self.value = value\n #    self.traceback = value\n #\n #   def __str__(self):\n #       return str(self.value)\n #\n #   def __repr__(self):\n #       return self.__str__()\n\n\ndef decodeMutex(buf):\n #   mutexVal = unpack('<L', buf.read(4))[0]\n #"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tassert path and os.path.exists(path)\n\t\troot = Root(path)\n\t\tif not root.exists:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\ndef create(path = None, path_is_root = False):\n\t\"\"\"\n\tCreate an OCFL object at path.\t\n\tIf path is None will create a new root. Otherwise path must have been root.\t\n\t\"\"\"\n\n\ttry:\n\t\treturn Ocfl(root = None if path_is_root else None, path = path)\n\texcept RuntimeError as e"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n    #check that file exists\n    if not path or not os.path.exists(path):\n        msg = (\"Requested path does not exist: %s\") % path\n        l.warning(msg)\n        print(\"Unable to find file: %s\" % path)\n        return False\n\n    #check that file is valid\n    if not os.access(path, os.R_OK):\n        msg = (\"Requested path is invalid: %s\") % path\n        l.error(msg)\n        print(\"Unable to read: %s\" % path)\n        return False\n\n    #check that file has a valid header\n   "}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\terrors = 0\n\t\twarnings = []\n\t\tif not os.path.exists(os.path.dirname(path)):\n\t\t\terror = \"Invalid OCFL object at %s\" % (path)\n\t\t\terrors += 1\n\t\tif not os.path.isdir(path):\n\t\t\terror = \"Invalid OCFL object at %s\" % (path)\n\t\t\terrors += 1\n\t\tif errors > 1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\t\t\n\tdef _getPath(self, path):\n\t\tif not \"./\" in path:\n\t\t\tpath"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn super(OCFL, self).validate(path)\n\n\n# =============================================================================\n# OCFLParser\n# =============================================================================\n\nclass OCFLParser(Parser):\n\t\"\"\"\n\tParser for OCFL.ocf files.\n\n\tNOTE: a lot of this code duplication is required, since it can not be reused.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitializes OCFLParser.\n\n\t\t:param filename: File name.\n\t\t:type filename: string\n\t\t\"\"\"\n\t\tsuper(OCFLParser, self).__init__('ocf"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif not self.is_valid(): return ''\n\t\tfor validator in itertools.chain(self.validators, self.formatters):\n\t\t\tif not validator(self, prefix):\n\t\t\t\treturn ''\n\t\treturn '%s:' % prefix\n\n_validator_msg_tmpl = '''Hello...\n\nThis is just a comment.\n\nSome sample code.\n\n:param str prefix: text to prefix the message with [default: %(default)s] (if prefix not in message, this will override default %(default)s).\n\nMessage has been: '%(msg_prefix)s'\n'''"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn str(getattr(self, \"status_%s_str\" % prefix, \"\"))\n\t\t\n\tdef _log(self, *args, **kwargs):\n\t\tself.debug(time.strftime('[%H:%M:%S]'), *args, **kwargs)\n\n\t@property\n\tdef status_str_no_result(self):\t\n\t\t\"\"\"\n\t\tReturn string of error message, without optional error message.\n\t\t\"\"\"\n\t\treturn self._log(self.status_str)\n\n\n#: (:class:`bool`) `True` if successful and `False` if not.\n#:"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self._status_string(prefix, self._status)\n\n\tdef _status_string(self, prefix='', status_prefix=''):\n\t\tif prefix:\n\t\t    status_prefix += prefix + ':'\n\t\treturn self._status_prefix + status_prefix\n\nclass _StatusGetter(object):\n\tdef __init__(self, status_getter, status_prefix='', msg=None, *args, **kw):\n\t\tself._status_getter = status_getter\n\t\tself._status_prefix = status_prefix\n\t\tself.msg\t= msg\n\t\t#print status_getter.__"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn '%s:%s' % (prefix,'status_code')\n\n\tdef verify_url(self, url):\n\t\t\"\"\"\n\t\tVerify that the url has the correct format and returns status.\n\t\t\"\"\"\n\n\t\turl = strip_url(url)\n\t\tparse = urlparse(url)\n\n\t\t# Check if url is a valid http(s) url-encoded HTTP url\n\t\ts = \"{SCHEME}://{url!s}{PARAMS}\".format(SCHEME = split_scheme(parse.scheme)[0], url = url)\n\n\t\t# If it's not, try to decode it,"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus_str = _('success') + prefix + _(self)\n\t\tif self.__class__.error_label is not None:\n\t\t\tstatus_str = self.__class__.error_label\n\t\treturn u\"%s%s\" % (status_str, self.as_dict())\n'''\r\nCreated on Dec 25, 2014\r\n\r\n@author: sean\r\n'''\r\nimport unittest\r\nfrom..utils.xmltest import XMLTestBase\r\n\r\nfrom.basetestcases import *\r\n\r\nclass TestBase(unittest.TestCase, XMLTestBase):\r\n\tdef setUp(self):\r\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = []\n\t\tstatus.append(\"Ok\")\n\t\tstatus.append(\"= \")\n\t\tstatus.append(\"Validation failed\")\n\t\tstatus.append(\"\")\n\t\tif prefix:\n\t\t\tstatus.append(\"Prefix:\")\n\t\t\tstatus.append(prefix)\n\t\telse:\n\t\t\tstatus.append(\"Prefix\")\n\t\treturn \"\".join(status)\n\n\tdef get_description(self):\n\t\treturn \"\"# Copyright (c) 2014 NetApp, Inc.  All rights reserved.\n# Copyright (c) 2015 Ben Swartzlander.  All rights reserved.\n# Copyright (c"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = self.status()\n\t\tif status == 'ok':\n\t\t\treturn status\n\t\telif status == 'BAD_PASSWORD_ERROR':\n\t\t\treturn errors.TinyMngr.generate_failure_msg(0)\n\n\t\treturn \"invalid: \" + prefix + status\n\n\t@staticmethod\n\tdef status_html_button_color(self):\n\t\t\"\"\"Return color of status of validators checkbox with optional\n\t\t   \"success\" label. \"\"\"\n\t\tfor state, label in (\n\t\t\t (\"OK\", \"OK\"),\n\t\t\t (\"FAIL\", \"FAIL\"),\n\t\t\t (\"PASS\", \"FAIL\")):\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\ts = ''\n\t\tif self.flags & self.FLAG_ALLOW_UNICODE:\n\t\t\ts = self.__unicode__().encode(None, 'ignore')\n\t\telse:\n\t\t\ts = self.__unicode__().encode(None, 'ignore')\n\t\treturn 'invalid' if (not prefix) else ('\"' + prefix + s + '\"')\n\n\tdef __iter__(self):\n\t\treturn iter(self.__list__())\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn length of list (length of self.__list__()).\n\t\t\"\"\"\n\t\treturn len(self.__list__())"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn prefix+self.status\n\n\tdef __repr__(self):\n\t\t\"\"\"Return string of current status as string.\"\"\"\n\t\treturn str(self.name)+' ('+self.status_str(prefix) + ')'\n\n\nif __name__==\"__main__\":\n\tif len(sys.argv)<=2:\n\t\tprint \"Please pass a file and a directory to the validator.  \"\n\t\traise Exception\n\ttry:\n\t\tcf = ConfigParser.ConfigParser()\n\t\tcf.read(sys.argv[1])\n\t\tcf.get(\"DEFAULTS\",\"output_dir\")\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\ttstr = _safe_unicode_obj(self)\n\t\tif not isinstance(tstr, unicode):\n\t\t\ttstr = tstr.decode('utf-8')\n\t\ts = prefix + s + tstr\n\t\treturn s\n\n\tdef __unicode__(self):\n\t\treturn u'<%s %s>' % (self.__class__.__name__, str(self))\n\n\tdef __str__(self):\n\t\treturn self.__unicode__()\n\n\tdef __hash__(self):\n\t\treturn self.__unicode__().encode('sha1')\n\n\tdef get_config(self, config_"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn '%s' % (prefix and self.__dict__[prefix].status_str())\n\nclass TestActionResult(object):\n\t\"\"\"Class which stores result of testing actions. When performing validation check passes the expected parameters, a result will be stored in the self.expected_results dict. If failed, the self.results dictionary contains the expected results and a log.\"\"\"\n\tdef __init__(self):\n\t\tself.expected_results = None\n\t\tself.results = None\n\tdef __str__(self):\n\t\tprint(self.__dict__)\n\t\tif (not self.expected_results"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn self.get_name() + ':'+ prefix + status +': '\n\n\tdef _get_statusstring(self):\n\t\t# Statusstr is only used for logging from codeml\n\t\treturn self.status\n\n\tdef get_name(self):\n\t\tresult = \"\"\n\t\tif self.status!= 'FAIL':\n\t\t\tresult = \"(OK), Status = %s\" % self.status\n\t\telif hasattr(self, 'name'):\n\t\t\treturn self.name\n\t\treturn result\n\n\tdef log(self, event, msg, *args):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tif not self:\n\t\t\treturn super(Validator, self)._status_str(prefix + ': no validation performed')\n\t\telse:\n\t\t\treturn self._get_status_str(prefix + ': validation performed')\n\n# -------------------------------------------------------------------------------------------------------------\nclass Filter(Validator):\n\t\"\"\"\n\tObject representing a Filter that can be defined either with a filter name or both.\n\tCurrently supported filters:\n\t- Filter with a list of items (e.g. 'title', 'description'),\n\t- filter with more than one item (e.g. 'date', 'duration'),\n\t-"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn '\\n'.join([\n\t\t\t('Step', self.step),\n\t\t\t('Case', self.case or ''),\n\t\t\t('Step Duration', self.step_duration if self.step_duration else ''),\n\t\t\t('Type', self.type or '')\n\t\t])\n\n\t@property\n\tdef validation_passed(self):\n\t\t\"\"\"\n\t\tReturns True if there were any validation passed for this step.\n\t\t\"\"\"\n\t\treturn not self.errors_passed\n\n\n\tdef add_error(self, validation_error):\n\t\t\"\"\"\n\t\tAdds an error to this validation log with a short"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tbuf = \"-----------------------\\n\"\n\t\tbuf += prefix + 'VALIDATION\\n'\n\t\tbuf += \"-----------------------------------------------\\n\"\n\t\tbuf += self._format_info()\n\t\tret = buf + \"\\n\"\n\t\tret += prefix + 'VALIDATION\\n'\n\t\treturn ret\n\n\tdef test_log(self, name=None):\n\t\t\"\"\"\n\t\tReturn string value of validation log, with optional name.\t\n\t\t\"\"\"\n\t\tret = self.status_str(name)\n\t\t\n\t\tif isinstance(ret, tuple):\n\t\t\t# multiple elements - concatenate with"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"[\" + self._prefix + \"VALIDATIONLOG]: \" + self._validate\n\n\tdef debug(self, txt='', level=logging.DEBUG):\n\t\t\"\"\"\n\t\tDebugging output.\n\t\t\n\t\tUse it to monitor how many steps and metrics print information. (see the\n\t\tLogging module for more information.)\n\t\t\"\"\"\n\t\tif self.debugFlag:\n\t\t\ttxt += \"\\n\" + self.id + ':'+ txt\n\n\t\ttry:\n\t\t\tlogger.debug(txt)\n\t\texcept AttributeError:\n\t\t\tpass\n\n\t@property\n\tdef statusType(self):\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\ttry:\n\t\t\ts = ''\n\t\t\tprefix_len = len(prefix)\n\t\t\tfor item in self.parameters:\n\t\t\t\tif len(item) > prefix_len:\n\t\t\t\t\ts += prefix.ljust(prefix_len) + \" - \"\n\t\t\t\t\ts += item\n\t\texcept Exception as e:\n\t\t\t# TODO: add more information when logging the data from all the functions\n\n\t\t\ts = 'Invalid parameters ({})'.format(str(e))\n\n\t\treturn s\n\t\n\tdef makeReport(self):\n\t\t\"\"\"\n\t\tReport parameters.\n\t\t\n\t\tThe parameter values are used to construct the"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\ts = \"\"\n\n\t\tif len(prefix) == 0:\n\t\t\ts = prefix\n\n\t\tfor entry in self._validation[s]:\n\t\t\tif entry[0] == '':\n\t\t\t\tentry[1] = True\n\n\t\t\telse:\n\t\t\t\tentry[1] = False\n\n\t\treturn \"Validation log status:  %s\" % (s)\n\n\tdef summary_str(self, prefix=''):\t\t\treturn \"Summary:\" + (self._summary_header().replace('\\t','')[:4] if self._is_summary_header() else \"\")\n\tdef summary_int(self"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tv = self\n\t\tv.prefix = prefix\n\t\treturn v.dump().rstrip()\n\tdef __str__(self):\n\t\tif not self.value:\n\t\t\tv = self\n\t\t\tv.dump()\n\t\treturn str(v)\n\tdef has_key(self, v):\n\t\t\"\"\"\n\t\tCheck whether log contains specified key and value\n\t\tReturn boolean\n\t\t\"\"\"\n\t\treturn v.has_key(self.serialize_value())\n\tdef has_many(self, v, keys):\n\t\t\"\"\"\n\t\tCheck whether log contains specified mutiple keys and"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tstatus_list = [x['message'].get('status') for x in self.event.get_status_log()]\n\t\tstatus_list.reverse()\n\t\tstatus =''.join(status_list)\n\t\tif status == 'ERR' and not self.is_default:\n\t\t\t# This is a \"primary error\", \"secondary error\", \"no error\" etc. error\n\t\t\tstatus = 'OK'\n\t\tif self.source and self.data:\n\t\t\treturn \"%s: %s\" % (self.event.message or self.event.data['level'], status"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\traise Exception('must be implemented in subclass')\n\n\tdef is_derived_from(self, other):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if ``other`` is derived from ``self`` implementation, both from the\n\tsubclass as well as the subclass, or the subclass' implementation.\n\t\"\"\"\n\treturn self == other or ((self.is_derived_from(other) and other.is_derived_from(self)) or (self.is_derived_from(other) and other.is_derived_from(self)))\n\ndef validate_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef is_valid_str(_string):\n\t\treturn True\n\n\tdef is_valid_int(_string):\n\t\treturn True\n\n\tdef is_valid_float(_string):\n\t\treturn True\n\n\tdef is_valid_uuid(_string):\n\t\treturn True\n\n\tdef is_valid_date(_string):\n\t\treturn True\n\n\tdef is_valid_time(_string):\n\t\treturn True\n\n\tdef is_valid_bytes(_string):\n\t\treturn True\n\n\tdef is_valid_path(_string):\n\t\treturn True\n\n\tdef is_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef get_identifier(self):\n\t\t\"\"\"\n\t\tThe representation of the identifier used by this object.\n\t\t\n\t\t@returns: str\n\t\t\"\"\"\n\t\treturn None\n\n\t@staticmethod\n\tdef split(path, parent=None):\n\t\tif parent is not None:\n\t\t\tsubpath_matches = subpath_re.match(path)\n\t\t\tsubpath_parts = subpath_matches.groups()\n\t\t\tif len(subpath_parts) > 3:\n\t\t\t\tparent = parent[0:3]\n\t\t\telse:\n\t\t\t\tparent = None\n\t\t\tparent"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\tdef is_complete(self):   # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is a possible completion value.\t\"\"\"\n\t\treturn False\n\nclass Signature(object):\n\t\"\"\"\n\tClass for signature of a particular signature.\n\n\tWhen we evaluate an object with a magic number, like this:'m_xxxxx-xxX'\n\tThe major version is one plus a minor signature which will be used to validate the\n\tsignature and use the object's object_type field to get the proper version.\n\tThe minor version defines fields"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\traise NotImplementedError(\"Subclasses must implement is_valid()!\")\n\n\tdef __getattr__(self, attr):\n\t\t\"\"\"\n\t\tProvides access to a subclassed container class. Subclasses should need to implement this method.\n\t\t:param string attrname: sub-class name of the container class\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif attr == \"__subclasses__\":\n\t\t\t\treturn self.subsubclasses\n\t\t\telse:\n\t\t\t\tsuper(Submodule, self).__getattr__(attr)\n\t\texcept AttributeError:\n\t\t\treturn self.subsubclasses.__getattribute__(attr)\n\n\tdef call(self, identifier):\n\t\t\"\"\""}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef update_user_metadata_cache(self, user_metadata_cache):\n\t\t\"\"\"\n\t\tUpdates the given fields from the given user metadata cache and saves on the metadata_cache\n\t\tobject.\n\n\t\tThis will update all fields in USER_METADATA_CACHE to all values in USER_METADATA_CACHE_VERSION,\n\t\tregardless of user_metadata_cache_version values.\n\n\t\t:param dict user_metadata_cache: Dictionary of user metadata for all users that are being updated.\n\t\t\"\"\"\n\t\tfor key in user_metadata_cache.keys():\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn bool(identifier)\n\n# TODO (cpennington): This is just a convenience function, and it would probably make more sense to use\n#  collections.Counter to find unique permutations.\ndef make_unique_id(prefix, string_id=True):\n\t\"\"\"\n\tReturns a unique id starting from the string.\n\n\tIf a string is passed to a function, this will be the result of calling\n\ta function with the unique id. For the identity function, this must return the string\n\tidentity for a message that is meant to be hashable (e.g. \""}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True;\n\n\tdef check(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn whether this identifier is valid (using a regular name\n\t\tinstead of just a number).\n\n\t    The check relies on the fact that self.basename_check_callback\n\t    has already checked if this identifier has been rejected by\n\t   calling CheckForValidIdentifier again.\n\t    \"\"\"\n\t\t# check the identifier hasn't been rejected.\n\t\tif identifier!= IDENTIFICATION_VALID:\n\t\t\t# yes it does\n\t\t\treturn self.basename_check_callback(identifier, self)\n\n\tdef"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef create_from(self, namespace, type_id, default):  # pylint: disable=unused-argument\t\"\"\"\n\tCreate a new type of a typedef in this module.\n\n\t:param namespace: Namespace containing the typedef we want to create a new type for.\n\t:param type_id: The class to create.\n\t:param default: The default value of this should be, default is None.\n\t:return: True if successful, False if not.\n\t\"\"\"\n\treturn TypeReference.TypeReference.create_from(self, namespace, type"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\nclass MetadataStore:\n\t\"\"\"Abstract interface for Metadata Stores.\n\tUsed internally by the L{create} method.\"\"\"\n\n\tdef __setitem__(self, path, data, validate = True):\n\t\t\"\"\"Ensure that the item identified by path is valid. This method\n\t\tmust not write anything into an existing item; it can be called in\n\t\ta thread-safe way. The path is first checked for a valid item;\n\t\totherwise an error is raised during the creation of the item.\n\n\t\t@param path: A string or a L{Path} object.\n\t\t@"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tret = True\n\t\tif self.version and self.version!= format_version(self.version):\n\t\t\tret = False\n\t\treturn ret\n\n\tdef mark_as_changed(self):\n\t\t\"\"\"\n\t\tMark this as changed and update inventory, it's safe to persist\n\t\tmodification time as the value were read.\n\n\t\tWe don't want to update the variable before it's marked as changed, so\n\t\twe need to do it here. When updating an inventory, we need to be sure that\n\t\tthe version number will be updated correctly. The inventory of the\n\t\tinventory does not"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tresult = None\n\t\tif not isinstance(self.spec_version, _AnsibleVersion):\n\t\t\tself.spec_version = _AnsibleVersion(self.pkg_manager)\n\t\tif extract_spec_version:\n\t\t\tself.spec_version = self.spec_version.split('.')\n\t\t\tif validate_version(self.spec_version) > parse_version(self.version):  # pylint: disable=no-member\n\t\t\t\tself.record_exception(\"Version %s of %s is not valid, should be %s\" % (self.spec_version,\n\t\t\t\t\tself"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not self.version:\n\t\t\traise TestParseError('No version given')\n\t\telif extract_spec_version:\n\t\t\ttry:\n\t\t\t\tself.version = self.spec_version.format(self.version_separator)\n\t\t\texcept ValueError as error:\n\t\t\t\traise TestParseError(\"Error parsing '%s' into '%s'\" %\n\t\t\t\t\t\t\t\t\t\t\t  (self.type, error))\n\n\tdef validate_min(self, inventory, extract_spec_version=False):\n\t\t\"\"\"\n\tValidate a given inventory. If the given version is less than the\n\tspecification version_separator, ValueError"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\tversion = self.spec_versions.get(provider, (None, None))[1].version_id\n\t\t\tif version is not None:\n\t\t\t\ttry:\n\t\t\t\t\tre.compile(version)\n\t\t\t\texcept re.error:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tre.compile(b'disable')\n\t\t\t\t\texcept re.error as e:\n\t\t\t\t\t\traise InvalidConfig('Invalid type value (%s): %s (%s): %s at %s\\n' %\n\t\t\t\t\t\t\t(version, type_name(version),\n\t\t\t\t\t\t\tprovider, e.__class__.__"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tspec_version = self.spec_version\n\t\tif spec_version is not None:\n\t\t\tif extract_spec_version:\n\t\t\t\tspec_version = spec_version.upper()\n\t\tvalidate_version(self.version, spec_version)\n\n\n\tdef match(self, version, allow_empty=False):\n\t\t\"\"\"\n\t\tSearch the specified version for the given version string or set current\n\t\tcontainer version if unspecified since that could be the case when no\n\t\tother parameters were explicitly provided for the version argument.\n\t\t\"\"\"\n\t\tif allow_empty and not self.version."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\ttype_spec = self.spec_version\n\t\telse:\n\t\t\ttype_spec = self.spec\n\n\t\tif type_spec is None:\n\t\t\t# If we are given a version, test that one is a package being uninstalled\n\t\t\tto_test = None\n\t\telif type_spec in self.uninstalled:\n\t\t\t# A version was installed - the test must be skipped if installed before test\n\t\t\tto_test = self.uninstalled[type_spec]\n\t\telse:\n\t\t\t# Either a version is missing from IUSE\n\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not extract_spec_version:\n\t\t\t# no spec version specified?\n\t\t\tself.spec_version = self.get_galaxy_spec(\"Version\")\n\t\telif not extract_spec_version:\n\t\t\t# spec version is optional\n\t\t\tif not self.spec_version:\n\t\t\t\traise EBSCliException(\"A spec_version must be specified (e.g. '3.4' or '3.4.3'\\n\")\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.spec_version = self.spec_version.replace(\"-\", \".\")\n\t\t\t\texcept"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version is False:\n\t\t\tself.spec_version = self.spec_version.capitalize().strip()\n\t\telse:\n\t\t\tself.spec_version = extract_spec_version\n\nclass ValidateInteger(Validator):\n\t\"\"\"\n\tValidate an integer.\n\t\"\"\"\n\tfrom openmdao.main.api import VariableTree, Resource, Link\n\tfrom openmdao.lib.datatypes.api import Float\n\n\tdef validate(self, value, variables):\n\t\ttry:\n\t\t\treturn int(value)\n\t\texcept ValueError:\n\t\t\t# Don"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tspec_version = self.spec_version\n\t\tif spec_version is None:\n\t\t\tspec_version = 1\n\t\tif not extract_spec_version:\n\t\t\traise errors.WafError(\"version not of expected: %s\" % self.spec_version)\n\t\tif not version.spec_from_tree(self.tree):\n\t\t\traise errors.WafError(\"version does not match spec_from_tree: %s\" % self.tree)\n\n\t\t# A version was found. Now set the type.\n\t\tself.set_type(spec_version)"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version!= extract_spec_version:\n\t\t\tself.spec_version = extract_spec_version\n\n\tdef validate_field(self, inventory, field):\n\t\tif self.spec_version:\n\t\t\tversion = inventory.get_field(field, None)\n\t\t\tif not inventory.field_exists(version):\n\t\t\t\traise ValueError('Missing %s for required field %s: %s' % (repr(field), repr(version), inventory.get_name(self)))\n\t\t\telif version!= field:\n\t\t\t\traise ValueError('Version mismatch for field %s"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tdigests_used.sort()\n\t\tchecked_manifest_digests = list(manifest_files)\n\t\tused_digests = set(self._get_manifest_digests(manifest_files))\n\t\tfor digest in digests_used.difference(checked_manifest_digests):\n\t\t\tif digest in used_digests:\n\t\t\t\tself.log('Digest {0} is already present but will be used'.format(digest))\n\t\t\t\tused_digests.remove(digest)\n\t\tif len(used_digests) > 0:\n\t\t\tself.log('Not all"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor digest, hashname in manifest_files:\n            \tprint \"   %s - %s\" % (digest, hashname)\n            \tmanifest = os.path.join(self.dest, hashname)\n            \tprint \"   %s - %s\" % (digest, manifest)\n\n            \twith open(manifest) as file:\n\t\t\t\tcontent = file.read().splitlines()\n\n\t\t\tfor digest in content:\n\t\t\t\tused = False\n\t\t\t\tfor line in digest:\n\t\t\t\t\t#print \"line: %s\" % digest\n\t\t\t\t\tif not line in guids_used:\n\t\t\t\t\t\tused"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tself.assert_true(file[6] == '0' or file[6] == '1')\n\t\t\tdigest = calculate_digest(file[8], file[4], self.hash_type)\n\t\t\tif digest!= digests_used[digest].hash:\n\t\t\t\tprint >>sys.stderr, \\\n\t\t\t\t\t\"ERROR checksum mismatch: checksum mismatch (%s: %d!= %d)\" \\\n\t\t\t\t\t% (file[1], file[3], digests_used[digest].hash)\n\t\t\t\tprint >>sys.stderr, \"file"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\twith open(manifest_file) as f:\n\t\t\t\tfor line in f:\n\t\t\t\t\tif line.startswith('digests: '):\n\t\t\t\t\t\tdigests = line.split('=')[2].split(',')\n\t\t\t\t\t\tdigest = digests[-1][1:-1]\n\t\t\t\t\t\tdigest_used = digests[-1][-1]\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\traise AssertionError(\"\"\"\nUnable to find digests in \"manifest\" but were not found.\"\"\")\n\n\tdef get_manifest(self, manifest_files, digests_used):\t\n\t\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor digest_file in manifest_files:\n\t\t\tmd5hash = compute_md5_hash(digest_file)\n\t\t\tdigest_contents = self.get_digest_contents(digest_file)\n\t\t\tused_digests = set()\n\t\t\tfor digest_offset in digest_contents_offsets:\n\t\t\t\tdigest_offset_entry_key = digest_offset ^ 0xf00000000\n\t\t\t\t# Only check a portion of the file and if so, check for a match.\n\t\t\t\tif digest_offset_entry_key in self.digest_offsets_by_type:\n\t\t\t\t\tdigest_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor f in manifest_files:\n\t\t\tf = f.upper()\n\t\t\tf = getattr(self, f)\n\t\t\tif f not in digests_used:\n\t\t\t\treturn False\n\t\treturn True\n\n\n\tdef find_digest_files(self):\t\n\t\t\"\"\"\n\t\tFinds the md5 digests in the manifest file. Returns md5-digest hash.\n\t\tReturns None if it can't find the file.\n\t\t\"\"\"\n\t\tf = None\n\t\tf_contents = self.manifest_contents()\n\t\tself.md5_digests = []\n\t\tif f is"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\twith self.__mutex:\n\t\t\twith self.__tlock:\n\t\t\t\tif self._fingerprint is not None:\n\t\t\t\t\twhile not self.__finished.is_set():\n\t\t\t\t\t\tfor key in manifest_files.iter_keys():\n\t\t\t\t\t\t\tif not manifest_files.get(key):\n\t\t\t\t\t\t\t\tself.__finished.set()\n\t\t\t\t\t\t\t\traise AssertionError(\"'%s' has been deleted before. \"\n\t\t\t\t\t\t\t\t\t   \"Please create or set the manifest entry with the proper \"\n\t\t\t\t\t\t\t\t\t   \"properties.\" % str(key))\n\t\t\t\t\t\tbreak\n\t\t\t\tfor manifest in manifests_used.difference(digests_available):"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tdigests_missing = []\n\t\tfor manifest_file in manifest_files:\n\t\t\twith open(manifest_file, 'r') as f:\n\t\t\t\ttext = f.readlines(max(0, 256), 256)\n\t\t\t\tdigests_used.append(text)\n\t\t\t\tdigests_missing.append(text)\n\n\t\t# Determine which digests are left in the file and that the manifest file contains only\n\t\t# those files we're about to include\n\t\tfor digest in sorted(digests_used):\n\t\t\tdigest_file = digests_used[digest]\n\t\t\trelative"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tverify = False\n\t\tfor file_path in manifest_files:\n\t\t\ttry:\n\t\t\t\tmanifest_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n\t\t\texcept OSError:\n\t\t\t\tcontinue\n\t\t\tverify = True\n\t\t\tmanifest_digests = read_data(file_path)\n\t\t\tif manifest_digests is None:\n\t\t\t\tverify = True\n\t\t\telif not file_path.endswith(\".data.sha256\"):\n\t\t\t\tverify = False\n\t\t\tif verify:\n\t\t\t\tdigests = digests_used.get(manifest_digests[\"name\"])\n\t\t\t\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor manifest_file, manifest in manifest_files.items():\n\t\t\tdigests = manifest_file.digests\n\t\t\ttotal_digests = digests.matches + digests.mismatches\n\t\t\tif total_digests >= digests_used:\n\t\t\t\traise ApkWorkLoadError(\n\t\t\t\t\t\"The following of a manifest file is in manifest '{}', but not the one found in the manifest '{}'.\"\n\t\t\t\t).format(manifest_file, manifest)\n\t\n\tdef fetch_manifests_from_archives(self):\n\t\t\"\"\"Fetch manifests from the archivename, if possible.\"\"\""}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\ttry:\n\t\t\tobjectValidator = InventoryValidator(self, prior)\n\t\t\tif not objectValidator.isOkToValidate(self):\n\t\t\t\traise ValueError\n\t\texcept ValueError:\n\t\t\traise ValueError('Failed to validate for prior version {0}.'.format(prior.version))\n\n\tdef check_version(self):\n\t\t\"\"\"\tCheck that the current inventory version is valid.\n\nThe input variable version can be parsed to validate and both self\nand prior inventories are assumed to have been checked for internal consistency.\t\"\"\"\n\t\timport Globals\n\t\timport sys\n\t\tif sys.hex"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise DataError(\"Specified prior is not a valid InventoryValidator type.\")\n\t\tif not isinstance(prior.root_inventory_name, str):\n\t\t\traise DataError(\"Specified prior is not a InventoryValidator type.\")\n\t\tif not isinstance(prior.priority_priority, int):\n\t\t\traise DataError(\"Specified prior is not a valid Priority type.\")\n\n\tdef set_prior(self, prior):\t\t\t# This is to be overwriten when there is no prior\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif not self._init_vars:\n\t\t\traise exception.IncorrectVersionError(self, prior)\n\t\tif not self._check_prior_version(prior):\n\t\t\traise exception.IncorrectVersionError(self, prior)\n\t__call__ = validate_as_prior_version\n\n\tdef _validate_for_required(self, value):\n\t\tif not value:\n\t\t\traise exception.RequiredValueMissing(self._name)\n\t\tif value is not None and not utils.is_identifier(value):\n\t\t\traise exception.RequiredValueInvalid(self._name, value)\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tself._validate_as_prior(prior)  # validate prior version\n\t# we need to do this here because the \"self\" attribute always represents\n\t# valid prior versions but we need to know the name from prior\n\tself._validate_prior_name(prior)\n\treturn prior\n\n\tdef _validate_as_prior(self, prior):\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise InvalidInventoryConstructor('The requested object is not of type %s' % type(prior))\n\t\tif prior.errors or prior.warnings:\n\t\t\traise InventoryValidator(\""}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# We need to check if anything about the current inventory version\n\t\t# is newer than the prior version.\n\t\tif isinstance(prior, InventoryValidator):\n\t\t\treturn prior._version == self.inventory_version\n\t\telse:\n\t\t\traise ValueError,'The current inventory format changed, but'\\\n\t\t\t\t\t\t'the prior and the current inventory version do not match.'\n\ndef validate_as_level(level):\n\tvalidate_as_prior_version(level)\n\tvalidate_as_inventory(level)\n\ndef validate_as_inventory(level):\n\tglobal _validate_level"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif hasattr(prior,'version'):\n\t\t\tpass\n\t\telif isinstance(prior, Inventory):\n\t\t\tif (self.current_inventories is not None and\n\t\t\t\t\t\t    \tself.current_inventories.version!= prior.version):\n\t\t\t\treturn (True, False)\n\t\t\treturn (False, False)\n\t\treturn (False, False)\n\n\tdef is_valid(self, allow_empty_inventory=False):\t\n\t\"\"\"\n\tCheck the current inventory for the existence of this inventory.\n\tIf allow_empty_inventory is False, only inventory that do exist\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tvalidated_prior = self._validate_member_versions(prior)\n\t\tif validated_prior.version_is('1.0'):\n\t\t\tlog.error(\"Your current inventory schema has version '1.0'.\\n\"\n\t\t\t\t\"This requirement should be specified in order for \"\n\t\t\t\t\"purchasing the latest version of MiP. Well, if you \"\n\t\t\t\t\"want to upgrade the system prior to 1.0, you could \"\n\t\t\t\t\"adjust using a new YAML mapping to get the recommended\"\n\t\t\t\t\" version by loading '%s'.\" % self.filename_to"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tself.validate(self._db_root, prior)\n\n\tdef validate(self, db_root, prior):\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise ValidationException(\"{0}: Expected type {1} Found {2}\".format(\n\t\t\t\tself.collection, \"expectedInventory\", type(prior)), pri)\n\t\t# If prior is present then it is assumed to be in the Inventory object\n\t\t# (with no incoming inventory objects), but only the current inventory object\n\t\t# is needed by the inventories. Otherwise if prior is not present then\n\t\t# it is"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tcurrent = self\n\t\twhile current!= prior:\n\t\t\tif self._validate is privilege.NEUTRAL:\n\t\t\t\tif current is not prior:\t# Necessary for a global check, see the module documentation for details\n\t\t\t\t\tself._validate(next(iter(current.hosts)))) \n\t\t\t\tself._validate(next(iter(prior.hosts))))\n\t\t\telse:\n\t\t\t\tif current is not prior: # Necessary for a global check, see the module documentation for details\n\t\t\t\t\tself._validate(next(iter(current.hosts)))\n\t\t\t\tself._validate(next(iter(prior"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tcheck_var(prior,\n\t\t\t {\"key\": \"local_var\", \"type\": \"string\", \"convert_to\": self.convert_to, \"validate_fn\": self.validate_as_prior_version})\n\n\tdef validate_when_missing_key(self, name, key):\n\t\ttry:\n\t\t\tcheck_var(name,\n\t\t\t\t\t\tself._get_validator(\"required_keys\"),\n\t\t\t\t\t\t{ key : [self[name][\"_missing_keys\"], key]}\n\t\t\t\t\t)\n\t\texcept KeyError, err:\n\t\t\t_logger.error(\"Missing key for required group"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tassert inventory is not None\n\t\tassert version is not None\n\t\tresult = {}\n\t\tfor filename in inventory.list_versions():\n\t\t\tif filename == version:\n\t\t\t\tresult[filename] = True\n\t\treturn result\n\treturn store.union_update_existing([get_logical_path_map(inventory, version) for version in inventory.list_versions()], version)\n\n# XXX: remove after moving this function:\ndef merge_dicts(target_dict, overwrite_dict=None):\n\t\"\"\"\n\tTake a dictionary of merge-dict, e.g. merge_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tresult = {}\n\t\tfor entry in inventory['_entries']:\n\t\t\tif entry == 'content':\n\t\t\t\tresult['content_files'] = []\n\t\t\t\tfor entry_name in entry['vars']:\n\t\t\t\t\tcontent = get_file_contents(entry_name)\n\t\t\t\t\tresult['content_files'].append(content)\n\t\t\t\tcontinue\n\n\t\t\tentry_content = \\\n\t\t\t\tdict(ast.literal_eval(\"[\\\"\\\"\" + entry[\"vars\"][entry_name][\"content\"] + \"\\\"]\"))\n\n\t\t\t# Do not compare for files of same version as last version, so that\n\t\t\t# we can"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tif not os.path.isfile(\"/etc/ansible/modules.d/softwares.d/modules.d\"):\n\t\t\treturn set()\n\t\treturn set(i[:-3] for i in open(\"/etc/ansible/modules.d/softwares.d/modules.d\").read().split(\"\\n\") if i[-3:] == [\"\", \"/\"])\n\n\tresult = {}\n\tfor path in lsmodtool(\"/bin/lsmod\").stdout:\n\t\tl = lsmodtool(\"/bin/lsmod\").stdout.split(\"\\n\")\n\t\tdata = {}\n\t\tfirst"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\treturn inventory[\"_meta\"][\"hostvars\"][version][\"content_files\"]\n\n\n\t_config_options = __salt__['config.option']\n\n\tif \"virtualMachineIdentifier\" not in _config_options:\n\t    raise SaltInvocationError('missing required parameter: \"virtualMachineIdentifier\"')\n\t\n\tvirtual_machine_iden= int(re.sub(r\"\\D\", \"\", os.urandom(11).encode('hex'), 16))\n\n\tif virtual_machine_iden < 1 or _config_options[\"virtual_machine_identifier\"]:\n\t\traise SaltInvocationError('invalid virtual machine identifier: '"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\n\tlogical_path_map = set()\n\tfor (root, dirs, files) in os.walk(inventory.root):\n\t\tfor file in files:\n\t\t\tif file.endswith(\".data\"):\n\t\t\t\t# only consider files ending in.\n\t\t\t\tif file in logical_path_map:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# not a symlink, just process it.\n\t\t\t\tsymlink_path = os.path.join(root, file)\n\t\t\t\ttry:\n\t\t\t\t\tst = os.lstat(symlink_path)\n\t\t\t\texcept os.error as e:\n\t\t\t\t\tif e.errno!= errno"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tfor state in inventory.get_hosts(include_version=True):\n\t\t\tpath = get_file_path(os.path.join(inventory.host_list, state, 'files'))\n\t\t\tfile_hash = hash_file(path)\n\t\t\tfile_map[state] = [os.path.relpath(path, inventory.path)]\n\t\t\tif file_hash not in file_map[state]:\n\t\t\t\tfile_map[state].append(os.path.relpath(path, inventory.path))\n\n\t\treturn file_map\n\ndef get_host_item_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t # FIXME:  This would only work correctly for the first two\n\t #        versions contain the same physical path.  So there is\n\t\t#        no problem if the logical path has such a file that\n\t\t#        has an identical content_file in another version.\n\t\tlogical_files = {} # type: Dict[str, Set[str]]\n\n\t\t# First, check if we've already checked that the version really changed.\n\t\tif 'physical-diff' not in inventory:\n\t\t\treturn logical_files\n\n\t\t# Second, check if we've already loaded the inventory.\n\t\tif inventory['"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tmapped = {}\n\n\tdef write_mapping(logical_path_in_state):\n\n\t\tfor item in contents.keys():\n\t\t\tfor f in state[item]:\n\t\t\t\tif f not in mapped:\n\t\t\t\t\tmapped[f] = set()\n\n\t\t\t\tmapped[f].add(item)\n\n\tdef load_all_physical_paths():\n\n\t\tfor file in listdir(directory):\n\n\t\t\tif file.startswith('.'):\n\t\t\t\tcontinue\n\n\t\t\tfile = join(directory, file)\n\t\t\tif isfile(file):\n\n\t\t\t\twith open(file, 'r') as f:\n\t\t\t\t\tlogical_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\ttry:\n\t\t\timport gc\n\t\t\timport pwd\n\t\texcept ImportError:\n\t\t\treturn\n\tinventory_json = {}\n\tfor path in PathFinder.get_inventory_paths():\n\t\ttry:\n\t\t\tif path!= inventory:\n\t\t\t\tcontinue\n\t\t\twith open(path, 'rb') as f:\n\t\t\t\tinventory_json.update(pickle.load(f))\n\t\texcept IOError:\n\t\t\tpass\n\n\t# TODO: If more than one copy of the inventory can be affected,\n\t# do not update the persistent inventory with any existing one.\n\tfrom octoprint._util"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tlogical_path_map = {}\n\n\t# Iterate over files to try against, to avoid duplicates\n\t# with identical file\n\tfor f in inventory.walk('file'):\n\t\tlogical_path_map[f] = fullpath_filter(f, version)\n\n\tfor f in inventory.walk('folder'):\n\t\tpath = f.split('/')\n\t\tif len(path) < 2:\n\t\t\tcontinue\n\t\tcurrent_level = path[0]\n\n\t\tif current_level not in ['.', '..'] \\\n\t\t\t\tand f!= '..' \\\n\t\t\t\tand get_all:\n\t\t"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor file in self._get_files(manifest_files, check_only=True):\n\t\t\tif not os.path.exists(os.path.join(self.settings.webroot, \"public\", file)):\n\t\t\t\traise util.ProcessException(\"Block {0} is not referenced in {1}\").format(fixity, file)\n\n\t\tfor entry in os.listdir(self.settings.webroot):\n\t\t\tif not entry.startswith(\".\"):\n\t\t\t\tcontinue\n\t\t\tif not os.path.isdir(os.path.join(self.settings.webroot, entry)):\n\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor f in manifest_files:\n\t\t\tnode = self.get_raw_node(f)\n\t\t\tif node is None:\n\t\t\t\traise Exception(\"Missing inventory reference: \" + \\\n\t\t\t\t\t\tfixity + \" in \" + self.filename)\n\t\t\tif node not in list(self.files_by_name.keys()):\n\t\t\t\traise Exception(\"Missing \" + fixity + \\\n\t\t\t\t\t\"in [\" + fixity + \"] in \" + self.filename)\n\t\t\tself.files_by_name[node].append(f)\n\t\treturn True\n\n\tdef detect_fixity"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\t# check for duplicate items in manifest files\n\t\tfor f in manifest_files:\n\t\t\ttry:\n\t\t\t\tkey = str(hash((f+' in the repo) and (f+1))))[-10:]\n\t\t\texcept ValueError:\n\t\t\t\tcontinue\n\t\t\n\t\t\tf_manifest = open(f, \"rb\")\n\t\t\tmanifest_items = dict((k, self.decode(self.get_version(v))) for v in yaml.safe_load_all(f_manifest))\n\t\t\tif key in manifest_items:\n\t\t\t\tlogger.debug(\"Fixity %s in %s: duplicate"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\tfile_name = os.path.basename(manifest_file).strip(\".tar.bz2\")\n\t\t\tif not self.tree.has_node(file_name):\n\t\t\t\tself.errors.append(\"file '%s' contains no.tar.bz2 entry\" % file_name)\n\t\t\t\treturn 2, \"invalid data for %s\" % file_name\n\n\t\t# Validate if new and old manifests are well-formed\n\t\tfor node, blobset in self.manifests.items():\n\t\t\tfor x in range(len"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\t# check to see if the root folder exists\n\t\tif os.path.exists(self.root_folder):\n\t\t\tfiles = glob.glob(self.root_folder + \"/**/*\")\n\t\t\tfiles += glob.glob(self.root_folder + \"/**/*/*.cpp\")\n\n\t\t\tfor file in files:\n\t\t\t\tif not validate_fixity_file(file):\n\t\t\t\t\tself.errors.append(\"Forbidden file '{}' not found in fixity block!\"\n                                     .format(file))\n\n\t\t\t# check to see if the parent folder exists\n\t\t\tif os.path.exists(self"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor key, val in fixity.items():\n\t\t\tif len(val)!= 4:\n\t\t\t\traise Exception(\"Missing mandatory fields on fixity block: %s\" % key)\n\t\t\tif key == \"name\":\t\t# name used in manifest for old entries\n\t\t\tself.__validation_check(val[0], val[1])\t# Check the fields are not emptys\n\t\t\tif key == \"size\":\t\t# size in MB after fixity checks\n\t\t\t\tself.__validation_check(val[2], val[3]) # Check the size are a 1-byte field\n\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tif type(fixity) is tuple:\n\t\t\t# A tuple like (filename, marker)\n\t\t\tfilename, marker = fixity\n\t\telif type(fixity) is int:\n\t\t\t# An int, the actual filename\n\t\t\ttry:\n\t\t\t\tfile_offset = self.repo_manager.get_offset_from_line(fixity.entry_start)\n\t\t\t\tfilename, marker = fixity\n\t\t\texcept ValueError:\n\t\t\t\t# A relative path\n\t\t\t\tpass\n\t\t\tif filename is None:\n\t\t\t\t# A relative path\n\t\t\t\trepo_file = self.repo_manager.get"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tif not isinstance(manifest_files, (str, unicode)):\n\t\t\traise TypeError(\"'fixity' must be a string or a file object.\")\n\n\t\tfor manifest_file in manifest_files:\n\t\t\t# The file is either the empty string or UTF-8-encoding\n\t\t\t# byte-string, or is encoded as a bytestring.\n\n\t\t\tfrom.base import FileType\n\t\t\tfrom. import Block\n\t\t\tfrom.utils import compute_check_segment\n\n\t\t\tif not isinstance(manifest_file, FileType):\n\t\t\t\traise TypeError(\"'fixity' must be a file object.\")\n\n\t\t\ttry:"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor node in yaml.load_all(self.config_file):\n\t\t\tif 'files' in node: continue\n\t\t\tfor sub in node['files']:\n\t\t\t\tfull_path = os.path.join(node['path'], sub)\n\t\t\t\tif not os.path.isfile(full_path):\n\t\t\t\t\tcontinue\n\t\t\t\tmd5hash = self.load_dublinCore(full_path)\n\t\t\t\tif not self.validate_fixity_file(md5hash,\n\t\t\t\t\t\tnode['sha256']):\n\t\t\t\t\tself.logger.debug(\"Fixity mismatch for {path}\"."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor node in self.nodes_by_name.itervalues():\n\t\t\tparent = node.parent_id\n\t\t\tif not parent.children:continue\n\t\t\tif parent.type == \"fixity\" and not isinstance(parent, fixity):\n\t\t\t\tif parent.parent_id == None:\n\t\t\t\t\traise Exception(\"Nodes to be assigned should belong to a specific fixity!\")\n\t\t\t\t\t#raise Exception(\"Nodes in a specific fixity should not linked to this node! %s\"%(node))\n\t#\t\tfixity_tree = {\"nodes\": [],\t\t\t\"nodes_by_name\": {},"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn listdir(path)\n\ndef copy_files_from_source(source, dest):\n\t\"\"\"\n\tCopy file from source to destination with all its files\n\tto/from home directory.\n\n\t:param source: File to copy from\n\t:param dest: File to copy to\n\t\"\"\"\n\tif os.path.isdir(source):\n\t\tfor fname in listdir(source):\n\t\t\tcopy_file(os.path.join(source, fname), dest)\n\telse:\n\t\tfor item in os.listdir(source):\n\t\t\tsource_path = os."}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tout = filter(None, findall(path, '*.png'))\t\n\t\treturn out\n\telse:\n\t\treturn {}\n\ndef files_list_update(path):\t\"\"\"\n\tUpdate the image list in `path` to reflect the latest changes\n\tfile.\n\n\tArgs:\n\n\t\tstr: Pathname to path to update the current image list.\n\n\tKeyword Args:\n\n\t\tpath (str): Path to the file to be updated.\n\t\"\"\"\n\tout = filter(lambda x: x['name']!= '', files_list(path))\t\n\tout.sort"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\ndef sort_nicely(l):\n\t\"\"\"\n\tSort and return a list with the N most important elements first.\n\t>>> l = [\"a\", \"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"a\", \"a\", \"b\", \"b\"]\n\t>>> sort_nicely(l)\n\t['a', 'b', 'b', 'b', 'a', 'b']\n\t"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn os.listdir(path)\n\t\t\n\n# def folders_list(path):\n# \t\"\"\"\n# \tReturn the folders in `path`.\"\"\"\n# \treturn os.listdir(path)\n\ndef list_files(path):\n\t\"\"\"\n\tReturn the files in `path`.\"\"\"\n\treturn [os.path.join(path, f) for f in os.listdir(path)]\n\t\n\t\n# def exists_empty(path):\n#\treturn os.path.exists(path)\n\t\n# def remove(path):\n# \t\"\"\""}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n\n\ndef get_md5(filename):\n\t\"\"\"\n\tGet the md5 hex digest of the given file\n\t\n\t:param filename: Name of file in question\n\t:return: md5 hex digest of given file (in hex)\n\t\"\"\"\n\twith open(filename, 'rb') as f:\n\t\treturn hashlib.md5(f.read()).hexdigest()\n\n\ndef save(data):\n\t\"\"\"\n\tSave the contents of"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [f for f in os.listdir(path) if f.endswith('.py')]\n\ndef copy_images(files_list, folder, num):\n\t\"\"\"\n\tCopy a list of images from the passed list to a folder\n\t@param files: the images to copy\n\t@param folder: the folder to copy the images to\n\t@param num: the number to copy the contents of the images\n\t\"\"\"\n\t# Copy each file\n\tfor file in files_list:\n\t\t# Copy the file\n\t\tshutil.copy(os.path.join(folder,"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n\t\tfiles = []\n\t\tfor d in dirs:\n\t\t\tp = os.path.join(path, d)\n\t\t\tif os.path.isfile(p):\n\t\t\t\tfiles.append(File(p))\n\t\tfiles.sort()\n\t\treturn files\n\tdef clean_files(directory, pattern):\t\n\t\t\"\"\"\n\t\tDelete all files matching `regex` in the specified location.\n\t\t\"\"\"\n\t\tfor p in walk_files"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn get_files(path, 'files')\n\n\ndef mkdir_p(path):\n\t\"\"\"\n\tMake the directory `path` exist\n\t\"\"\"\t\n\tif not path or os.path.isdir(path):\n\t\ttry:\n\t\t\tmakedirs(path)\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\tensure_dirs(path)\n\n\ndef replace_lines(filepath, pattern, replacement):\n\t\"\"\"\n\tReplace lines from filepath by line by replacement lines\n\t\"\"\"\n\tregex = re.compile(pattern, re.MULTILINE)\n\twith open"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [n for n in listdir(path) if isfile(join(path, n))]\n\ndef get_project_setting(key):\t\"\"\"\n\tGet the project setting as an argument\n\t\t:param key: project setting name \n\t\t\t\t\t\t(example_value=\"MyProjectName\")\n\t:return: project setting: the key value as a string\n\n\t\"\"\"\n\tglobal _project_setting\n\treturn _project_setting.setdefault(key, \"No config found\")\n\ndef read_project_setting(key):\n\t\"\"\"\n\tReturn the value of the project setting as"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\timport os, stat\n\t\td = os.path.join(path, \".git\")\n\t\treturn [f for f in os.listdir(d) if os.path.isfile(os.path.join(d, f))]\n\n\tdef get_file_list(path):\n\t\tpath = expand_path(path)\t\t# join with common directory\n\t\tret = []\n\t\tfor f in files_list(path):\n\t\t\tadd_to_ret = filter(lambda x: os.path.join(path, x), os.listdir(f))\n\t\t\tfor x"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\n\tpackage = package + \".\" + source\n\tif xml_filename not in files:\n\t    files.append(xml_filename)\n            return False\n\ttry:\n\t    doc = minidom.parse(xml_filename)\n\texcept Exception, e:\n\t    print \"\\nXML file: \", xml_filename, \"is not a valid XML file. \\n\"\n\t    print \"XML error: \", str(e)\n\n\t    return False\n\tkeys = doc.getElementsByTagName(\"key\")\n\tvals = []\n\n\tfor key in keys:\n\t    vals.append(key.firstChild.data)\n\n\treturn dict"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t# open the source and read the file(s)\n\t\tsource = open(source, 'rb')\n\t\txmls = pickle.load(source)\n\t\t# loop through the xml files\n\t\tfor file in files:\n\t\t\tname, ext = os.path.splitext(file)\n\t\t\tif not ext in xmls.keys():\n\t\t\t\tcontinue\n\t\t\txmls_keys = xmls[ext].keys()\n\t\t\tfor xml_key in xmls_keys:\n\t\t\t\tname_val = xmls[xml_key][0].decode('utf-8')[:-4]\n\t\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n        # read in files and parse XML and return dict\n        xml_files = [] # a list of files read (as XML if more than one)\n        for xmlfile in xmls:\n            # try to read xml file\n            try:\n                xml_filename = file(xmlfile)\n                xml_file = xml_filename.read()\n                xml_content = xml_file.decode()\n                xml_file.close()\n            except IOError:\n                # no xml file exists: we continue\n                continue\n            # break down XML file\n            xml_filename = xml_filename.replace('  ','')\n            if xml_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tname, ext = os.path.splitext(os.path.basename(xmls[0]))\n\t\tfiles = [name + '/' + f for f in files]\n\t\treturn {'name': name, 'files': files}\n\t\n\tdef get_files(sub):\t #Get the xml filenames\n\t\txmls = xml_filename.findall('./{0}file'.format(sub))\n\t\tfiles = []\n\t\tfor _sub in xmls:\t\t\t #iterate all xml filenames and use regex in the regular expression\n\t\t\tname, ext = os.path.splitext(_sub).split('/')[-"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t#get all xml files of this level\n\t\txml_folder_path = os.path.join(source, xml_filename)\n\t\txml_files = listdir(xml_folder_path)\n\t\t#return data\n\t\t#return empty dict\n\telse:\n\t\t# get xml files matching the pattern\n\t\txml_filename = fnmatch.filter(files, \"%s*.xml\" % xml_filename)\n\t\txml_files = listdir(xml_folder_path)\n\n\t\t#return empty dict\n\t\t#return empty empty dict\n\treturn json_to_group(xml_files"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n     md5sum = hashlib.md5()\n     for x in xmls:\n    \t  md5sum.update(x.split(\"/\"))\n     data_map = _get_data_map(files, xmls)\n     data = {}\n     data['md5sum.xml'] = data_map\n     data['name'] = md5sum.hexdigest()\n     data['source'] = source\n     data['sha1'] = md5sum.hexdigest()\n     return data# Copyright (c) 2014 by Ecreall under licence AGPL terms \n# available on http://www.gnu.org"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\n\t# Initialize a dictionary of data and file data sets.\n\tdata = {}\n\n\tfor xml_filename in files:\n\t\twith open(xml_filename, \"r+\") as fp: \n\t\t\txml_content = fp.read()\n\t\t\tfp.seek(-1, 2)\n\t\t\t\n\t\t\t# For security reasons there is a bug in xml parsing at the moment\n#\t\t\t# we can only parse XML, so only parse files that contain XML inside.\n#\t\t\tfp.seek(-1, 2)\n\n\t\tdoc = ElementTree.parse(xml_content)\n\t\troot = doc."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tgroups = ET.parse(xmls).findall('package')\n\t\tfor group in list(groups):\n\t\t\tname = group.get('name', None)\n\t\t\tfile = group.get('file', 'Unknown')\n\t\t\tfilepath = group.get('filepath', 'Unknown')\n\t\t\text = group.get('ext', 'Unknown')\n\t\t\t\n\t\t\tif not (name and file and filepath):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\ttry:\n\t\t\t\tdata = extract_pkg_data_from_zip(file, filepath=filepath, ext=ext)\t\n\t\t\texcept Exception, e:\n\t\t\t\tdata ="}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t#get_group_files_and_files_by_filename(source, [xmls]).values()\n\t\tgroups = _load_xml_files(xmls, files)\n\t\treturn groups\n\telse:\n\t\treturn None\ndef extract_package_list(args):\n\t\"\"\"\nExtract list of packages from the xml file(s)\n\n\tParameters\n\t------\nxml_filename : str\n    XML filenames\n\nReturns\n-------\nlist\n        list of packages in the folder or zipfile\n\n\"\"\"\n\txml_files = parse_xml_list(args.xml_filename"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\txmls = _get_xmls_for_filenames(xmls, files)\n\tname = \"groups-by-xml-filename\"\n\treturn load(name, None, _get_xml_from_files(files))\n\ndef _group_file_by_dirname2filename(dirname, files):\t\"\"\"\n\tGroup files by their dirname\n\nGroups file by their dirname and returns data in dict format.\n\nParameters\n----------\ndirname : str\n    parent directory for the files (relative to the parent directory)\n    files : list\n    list of files in the folder or zipfile"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t#FIXME: should be more elegant for each file found\n\t\t#return False\n\n\tfor prefix_part in prefix.split(os.sep):\n\t\tif os.sep == prefix_part:\n\t\t\tif match_file(file_path, os.sep):\n\t\t\t\treturn True\n\t\telse:\n\t\t\tif match_file_by_prefix(prefix_part, file_path):\n\t\t\t\treturn True\n\treturn False\n        \t\t\ndef main():\n\n\n\tfiles = os.listdir(\".\")\n\t\n\tif len(sys.argv) < 2:\n\t\tprint \"No input folder"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t\n\t\tfrom. import DocumentFiles\n\t\tdocument_files = DocumentFiles()\n\t\treturn document_files.get_file_by_prefix(prefix, file_path)\n\n\t# No file is found by the correct name pattern\n\treturn not file_path.startswith(prefix)\n\n\ndef match_package_containing_name (path, prefix = \"\", ignore_root = False):\n    \"\"\"\n\tIdentify if a `path` belongs to a package by a given `prefix`\n\n\tIdentify if a `path` belongs to a package by a given `prefix`.\n\nParameters"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\ndef test_data_package(document, filename, package):\n\t\"\"\"\n\tCheck if a specified `document` belongs to the package by filename\n\n\tReturns `True` if the document belongs to the package, else `False`.\n\t\"\"\"\n\twith open(filename, \"r\") as fp:\n\t\tfor line in fp:\n\t\t\tdata = line.split()\n\t\t\tif len(data) == 7:\n\t\t\t\tpart = document + \"_\" + data[1]\n\t\t\t\tif package in part:\n\t\t\t\t\treturn True, package, part[len(part)-1]\n\t\t\telif"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\t#test\n\tdoc = Document(None, prefix=\"\")\n\tdoc[\"teststring\"] = \"testing\"\n\tdoc[\"test\" : \"testing\"}\n\tdoc2 = Document(None, prefix='\\t')\n\tdoc2[\"teststring\"] = \"testing\"\n\tdoc2[\"test\" : \"testing\"}\n\tdoc[\"test\" : \"testing2\"]\n\tdoc[\"test2\"] = \"testing2\"\n\tdoc[\"test\" : \"testing22\"]\n\tdoc[\"test/testtwo.py\"] = \""}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\n\ndef read_file(file_path, return_value, ignore_if_not_existing = True, no_header):\n\n\tfrom os.path import basename, splitext\n\timport fnmatch\n\timport fnmatch\n\timport re\t\t\t\n\n\tfrom collections import OrderedDict\n\t#import zipfile\n\n\tif ignore_if_not_existing:\n\t\tfilename = splitext(file_path.lower())[0]\n\telse:\n\t\tfilename = file_path\n\n\t# read to prevent opening and reading from stdin\n\tf = open(filename, 'r')\t\t\t\t\t\t\t\t\t\t\n\tf"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn _match_file_by_prefix(prefix, file_path)\n\ndef _match_file_by_prefix(prefix, file_path):\n    import os\n    from datetime import datetime\n    t0 = datetime.now()\n    d = os.path.dirname(file_path)\n    f = os.path.basename(file_path)\n    r = os.path.relpath(file_path, d)\n    d = os.path.join(d, \"\")\n    while True:\n        p1 = p_dirs(os.path.join(d, f))\n        p"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn os.path.exists(file_path) and os.path.isfile(file_path)\ndef extract_metadata_from_file_on_disk(file_path):\n\t\"\"\"\n\tExtract all metadata from a disk file into a dict (dictionary)\n\n\tParameters\n\t----------\nfile_path : str\n    File path\n\t\"\"\"\n\tmetadata = {}\n\tmetadata_data = dict()\n\tif os.path.isfile(file_path):\n\t\t# file exists; load metadata from disk\n\t\tmeta_file = open(file_path, 'r')"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t                                                               \n\tfile_name = '{prefix}_{file_path}'.format(**locals())\n\treturn file_stats.file_in(file_path, file_name, max_length=86400)\t\n\n\ndef match_file_by_id(id):\n\t\"\"\"\n\tIdentify a `document_id` belong to a document package by given `id`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nid : str\n    Document id\n\nReturns\n-------\nbool\n    True - document belongs to"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn os.path.isfile(os.path.join(file_path, prefix + '.docx'))\n#! /usr/bin/env python\n# This source file is part of Tautulli.\n#\n#  Tautulli is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Tautulli is distributed in the hope that it will be useful"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# Get the file path or raise an error\n\t\ttry:\n\t\t\tif os.path.exists(file_path) and os.path.isfile(file_path):\n\t\t\t\treturn True # file exists and is a file\n\t\texcept IOError: # File doesn't exist\n\t\t\t# Try the same as above, ignoring prefix case\n\t\t\treturn True # file does not exist\n\t\texcept Exception:\n\t\t\t# Other errors are raised:\n\t\t\t# Not a permission error:\n\t\t\t# Unexpected error:\n\t\t\t# No data:\n\t\t\t# No permissions\n\t\t\traise\n\telse:\n\t\t#"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tpass\n\n\tfrom os.path import splitext\n\n\tall_filenames = [i for i in files if splitext(i)[1].lower() == prefix.lower()]\n\tif all_filenames:\n\t\tfiles = all_filenames\n\telse:\n\t\tfiles = [x for x in files if x.startswith(prefix)]\n\t\tfiles.extend(glob.glob(prefix + \"*\"))\n\n\treturn files\n\ndef get_all_files_by_prefix(prefix):\n\t\"\"\"List files which match by prefix\"\"\"\n\tfiles = []\n\tfor (dirpath, dirnames"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\t\n\tfiles = listify(files)\n\tfiles = [f for f in files if re.match(prefix, str(f))]\n\t\n\tfiles.sort()\n    return files\t\n\t\nif __name__ == '__main__':\n\t\n\tdirectory_name = 'test'\n\t\n\tfile_names = select_filenames_by_prefix(file_prefix = directory_name + '/', files = [file_name])\t\n\t\n\toutput_file = open(os.path.join(directory_name, file_names[0].split('/')[-1]))\n\toutput_file.write(\"\\n\"."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\timport re\n\t\tregex = r\"((?!%s).)*\" % prefix\n\t\tfiles = re.split(regex, files);\n\t\tfiles = [f.strip() for f in files];\n\t\treturn files\n\ndef select_filenames_by_suffix(suffix, files):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com suffix\t\n\nParameters\n----------\nsuffix : str\n    Filename suffix\nfiles : str list\n    Files paths\nReturns\n-------\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t# print(files)  # for testing\n\n\tfilenames = []\n\tprefixes = []\n\n\t# Get files files\n\tfor (dirpath, dirnames, filenames) in os.walk(os.path.join(prefix, \"documents\")):\n\n\t\tfor f in filenames:\n\t\t\tfilenames.append(os.path.join(dirpath, f))\n\n\t\tprefix = os.path.join(prefix, prefix)\n\t\tfiles_to_yield = glob.glob(os.path.join(prefix, \"docs2\", \"*\"))\n\t\tfor fname in files_to_yield:\n\t\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t# FIXME: refactor using string.replace (from a seperate module):\n\nimport os\nimport sys\nimport re\n\n\nclass FindFiles(object):\n\n\tdef __init__(\n\t\tself, project, prefix='.',\n\t\t\texclude_folders=None, skip_directories=None,\n\t\t\tverbose=0, exclude_prefixes=None,\n\t\t\t\tignore_files=None, ignore_prefixes=None):\n\t\t\t\"\"\"\n\t\t\tGet files in directory tree of given name. \n\t\n\t\t\t:param project:\t\t\t\tThe project to find files in\n\t\t\t:type project:\n\t\t\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n        files = sorted(files)\n        \n        return files\nprint(__file__)\n\ndef select_filenames_by_file_no(fileno, files):\n    \"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `fileno`\n\nParameters\n----------\ndirectory nome do ficheiro do ficheiro para a comandos das \n\nParameters\n----------\nfileno : str\n    nome do ficheiro do ficheiro para a comandos das \n\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tfile_paths = []\n\t\tfor fname in files:\n\t\t\tfname = fname.split('/')\n\t\t\tif len(fname)>1 and fname[1].__contains__('@pythonmodule'):\n\t\t\t\tcontinue #todo: cargar podereta os par\u00e2metros que vai ser n\u00e3o seja pelo argumento\n\t\tfor fname in dir(prefix):\n\t\t\tif fname.startswith('_') or fname.startswith('_')+'\\\\':\n\t\t\t\tcontinue #todo: cargar podereta os par\u00e2metros que vai ser n\u00e3o seja pelo argumento"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\n\tdata = pd.read_csv(files)\n\treturn [join_fname(prefix, f) for f in files]\n\n\ndef split_names_by_length(filename, length):\n\t\"\"\"\n\tYields filenames.\n\nParameters\n----------\nfilename : str\n    Filename to enumerate (may contain wildcards)\t\nlength : int\n    Yields filenames.\n\nReturns\n-------\nlist\n    filenames list containing filenames\n\"\"\"\n\n\tcurrent = 0\n\tnames = []\n\tfor i in range(0, (length-1)):\n\t\tname = filename[:i+1"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\treturn [get_files_by_prefix(prefix, f) \n\t\t\t\tfor f in files]\n\ndef get_files_by_prefix(prefix):\t\n\treturn [f for f in glob.glob(build_doc_dir_list(prefix)) if os.path.isfile(os.path.join(f, \"*.py\"))]\n\ndef get_files_by_prefix_ext(prefix, ext):\n\treturn [f for f in glob.glob(build_doc_dir_list(prefix, include_suffix=True)) if os.path."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n        #print 'Select', prefix\n        files_paths = [ os.path.expanduser( f ) for f in files if prefix in f ]\n        return files_paths\n\ndef select_files_by_pattern(pattern, files):\t\"\"\"\n\tGet files which can be used as filter expressions \n\nRetorna os arquivos da lista `files` cujos nomes \n iniciam com `pattern`\n\nParameters\n----------\npattern : str\n    File name regex\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which matches pattern\t\"\"\"\n\t\n       "}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\ndef _explore_files(path,folder_path):\n\t\"\"\"\n\tGet packages' data from given folder and explore it\n\t\"\"\"\n\n\n\tfolder_path_split = folder_path.split(\"/\")[0]\n\n\tdirectory_path = '/'.join(folder_path_split.split('/')[:-1])\n\n\tfiles_path = './'\n\n\tdata = {}\n\n\tfiles = list(sorted(os.listdir(folder_path)))\n\n\tfor f in files:\n\t\tfile_path = path + '/' + f\n\t\tfile_ = codecs.open(file"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tdef _checkFolder():\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tpass\n\t\n\t\t\n\t\t\n\tcurrent_path = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n\t#print folder\n\t\n\t#print os.path.basename(current_path)\t\n\tdata = {}\n\tfor file in os.listdir(current_path):\n\t\tif not file.endswith(\".xml\"):\n\t\t\tcontinue\n\t\t#print file\n\t\txml=ParseXml(current_"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n# Get a list of categories that are supposed to be processed\ncat_names = data_dict['categories'][:]\t# contains ordered categories and their description \n# Extract the name of the category\ncat_names.insert(0,'Categories')\n\n# Extract the names of the files which have this category before processing\ncat_files=glob.glob('../data/Category/'+cat_names[0]+'/categories/*.xml')\t# This list contains all categories, which are supposed to be processed\n\n# Generate a dictionary which contains all the files which have files that match\n# The keys are the"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _get_packages(folder):\n\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\timport xml.etree.ElementTree as ET\n\tres = {}\n\tfor elem in os.listdir(folder):\n\t\tif elem.endswith(\".xml\"):\n\t\t\txml_filename = os.path.join(folder, elem)\n\n\t\t\t# We skip directories\n\t\t\tif not os.path.isdir(xml_filename):"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n        json_dict = {}\n\twith codecs.open(os.path.join(folder,'_packages.json'), encoding='utf8') as f:\n                json_dict = json.load(f)\n        \n        json_dict['folders'] = folder\n        \n        return json_dict\n# Copyright (C) 2010-2019 The ESPResSo project\n#\n# This file is part of ESPResSo.\n#\n# ESPResSo is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation,"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tpath = Path(folder).absolute()\n\t\treturn dict( [(file_obj.name, (file_obj.exists() and file_obj.is_file()) and path.name(file_obj)) for file_obj in find_all_files(path)])\n\ndef file_exists(a_file):\t\t\t\t# return True if a_file exists, else False\n\treturn os.path.exists(a_file)\n\ndef get_all_files(folder):\t\t\t\t# Return a list with all files under a folder\n\t\"\"\"\n\n\t\"\"\"\n\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tif not os.path.exists(folder):\n\t\t\tprint 'folder is not exist:', folder\n\t\t\treturn None\n\n\t\tfolders_info = os.listdir(folder)\n\n\t\tfor index_file in range(len(folders_info)):\n\t\t\tfolder_name = folders_info[index_file].replace('.xml', '')\n\t\t\tindex_file_data = get_data_from_folder(folder_name)\n\t\t\tif index_file_data is not None:\n\t\t\t\tpath = get_path_from_file_data(index_file_data)\n\t\t\t\tif path:"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\t_list = map(lambda x: os.path.join(x, '.xml'), os.listdir(folder))\n\n\n\t\t_folder = [_x for _x in _list if os.path.isdir(_x)]\n\t\t_files   = [os.path.join(_folder, fn) for fn in _list]\n\n\t\t_folders  = set(_files).difference(_list)\n\t\t_files   = set(_files).difference(_list)\n\n\t\treturn {'files', 'folders', 'files2', 'files3', 'files4', 'files"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tif not os.path.isdir(folder):\n\t\treturn dict()\n\t\t\n\t#parse xml files into dict format\n\ttree = xml.etree.ElementTree.parse(folder)\n\troot = tree.getroot()\n\t\n\t#parse packages into dict format\n\troot \t= _explore_package(root.attrib[\"xml\"])\n\t\n\t#flatten data\n\troot['pkgs'] = _get_flattened_list(root)\n\troot['pkgs'] = _sort_pkgs(root['pkgs'])\n\t\n\t#reverse dict order using yaml order\n\tpkglist = [\n\t\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n        # Get all groups\n        groups = []\n        for data_json in os.listdir(folder):\n                group = {}\n                path = os.path.join(folder, data_json)\n                with open(path, 'r') as data_file:\n                        js = data_file.read()\n                        # Lookfor a group declaration\n                        if '<group_name>' in js:\n                                group['name'] = js.split('>')[1].split('\"')[0].strip()\n                        else:\n                                pass \n                group['children'] = []\n                _explore_folder(group)\n\t\t\n\t\t# Get all files/"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _eval_tree(prefix, data, _map):\n\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote as linhas de resultados\n\n\tParameters\n\t----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\n\nReturns\n-------\ndictionary\t\"\"\"\n#    The core of this python program is AMDA\n#    Copyright (C) 2008  Nathanael C. Fritz, Lance J.Thompson\n#\n#    The program is free software; you"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n        # variables globais com dados de cada arquivos\n        root = ET.parse(file_path).getroot()\n\n\n        # restaurar os dados do pacotes e coloca atualizadas do pacote\n        for x in root:\n\t\n\t\t# obt\u00e9m o tamanho nome do arquivo com dados do pacote\n\t\tif 'asset' in x.keys():\t\n\t\t\tasset_tree = ET.ElementTree(ET.fromstring(x['asset']))\n                        # remover os dados, caso, pacote"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t       \n\tdocument_file = open(file_path)\n\tdocument = ElementTree()\n\troot_element = document\n\troot_element_name = re.sub('\\.xml$', '', document_file.read())\n\troot_element_id = root_element.find(prefix + \"ID\").text\n\n\tdocument_file.seek(0) # Esfera vai o tempo\n\n\tdata_elements = self._get_data_elements(xml_file=document_file, xml_declaration=True)\n\tif data_elements is not None:\t\n\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tret = []\n\t\tfile = open(file_path, 'r')\t\n\t\tfor line in file:\n\t\t\tstr = line\n\t\t\tline = line[:-1]\n\t\t\tline = line.lower()\n\t\t\tret.append(str)\t\n\t\treturn ret\n\n\t\"\"\"\n\tidentifica o tipo do pacote e seus parte de arquivo em arquivo\n\tdo arquivo n\u00e3o especificada\n\nParameters\n----------\nin_path : str\n    n\u00e3o \u00e9 poss\u00edvel identificar as path da classe ou atual"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t    \n\tfile_path = \"/home/user/Desktop/python/picard/data/\" + file_folder + \"/\"\n\tfolder_path = os.path.join(file_path, prefix + '/' + filename)\n\tprint(\"Arquivo comando... \\n\")\n\texecfile(folder_path)\n\tjson = run_command(\"python -c 'import picard; picard.cli.process_file(picard.cli.parse_dir(\" + file_path + \"/ \" + filename + \"))'\")\n\tjson = eval(json)\n\tprint"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\twith open(file_path, encoding = \"utf-8\") as file_handle:\n\t\t\tasset = etree.parse(file_handle)\n\t\t\trendition = asset.getetag(\n                    './rendition'\n                )\n                packages = asset.getetag(\n                    './packages'\n                )\n\n                t = dict(\n                    title = prefix,\n                    identifier = asset.get('identifier'),\n                    asset_number = asset['asset_number'],\n                    asset_label = asset['asset_label'],\n                    year = int(asset['year_min']),\n                    month = int(asset['year_max']),\n                    publisher"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tlista_arquivos_info = os.listdir(file_path)\n\tstrinas_data = (prefix+prefix +'s.txt' +  extension for extension in  lista_arquivos_info)\n\tos.chdir(file_path)\n\t\n\twith open(file_path, 'r') as arquivos:\n\t\ts = arquivos.read()\n\t\t\n\tr = {}\t\n\tfor x in s.split('\\n'):\n\t\tdata = x.split('\\t')\n\t\tt = data[0]\n\t\tr"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _read_xml(filename):\n\t\"\"\"\n\t\tRecebe o tempo xml no arquivo de seguintes a cada arquivo;\n\t\tsao contr\u00e1rio uma lista com os tags dos objetos da tags de um arquivo extens\u00e3o.\n\t\"\"\"\n\txml = ET.parse(filename)\n\t\n\treturn xml\n\ndef read_xml(filename, file_folder, encoding=\"utf-8\"):\n\t\"\"\"\n\t\tRecebe o tempo xml para armazenar o arquivo.\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\treturn _eval_xml(file_path, prefix)\n \ndef _eval_xml(file_path, prefix):\t\t\t\n        if not os.path.isfile(file_path):\n            raise IOError(\"Erro ao conte\u00fado acesso!\" % file_path)\n\ttry:\n    \twith open(file_path, \"rb\") as f:\n    \tcontent = f.read().decode('utf-8')\n    \treturn {'xml_code': content,\n\t\t'tag': prefix}\n\texcept Exception as ex:\n       print repr(ex)\n       content ="}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\ntemplate_env = Environment(loader=FileSystemLoader(settings.TEMPLATES_PATH))\n_locals = {\n}import requests, json\nfrom pprint import pprint\n\ndef getUserId():\n    # Get API user id.\n    query = \"https://social.twitch.tv/accounts/show?format=json\"\n    res = requests.get('http://api.twitch.tv/users/lookup', params={'ids': query})\n    try:\n        return res['users']['ids'][0]['uid']\n    except:\n        return None\n\ndef getSearchHistory(searchId):\n   "}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\ttry:\n\t\t\tself.fileManager.createFile(file_path)\n\t\texcept FileAlreadyExists:\t\t\t\n\t\t\t\n\t\t\tlogging.error(\"File already exists, this means you failed to add rendition\")\n\t\t\treturn\n\n\t\tresult = self.fileManager.getRendition(lang)\n\t\tlogging.info(\"Resolved rendition {0}.\".format(lang))\n\t\tlogging.info(result)\n\t\t\t\n\tdef remove_rendition(self, lang):\n\t\t\"\"\"\n\t\t{\n        \"original\": \"artigo01.pdf\",\n        \"en\": \"artigo01"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tf = open(file_path, 'rb')\n\t\ttry:\n\t\t\tpdf_text = f.read()\n\t\tfinally:\n\t\t\tf.close()\n\t\tif not pdf_text:\n\t\t\treturn None\n\t\tpdf_file = codecs.open(self.file_path, 'w', 'utf-8')\n\t\tpdf_file.write(pdf_text)\n\t\tpdf_file.close()\n\nclass CabalCrystalLexicon():\n\tfile_path = 'cabalcrys.txt'\n\tfile_name = 'cabal"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tnew_file = os.path.join(self.storage_path, \"original.pdf\")\n\t\tos.renames(file_path, new_file)\n\t\ttext = u\"\"\n\t\ttext = text + _(\"The source code of {} is ready to be downloaded on {}\").format(self.type, lang)\n\t\tprint_error(text)\n\t\tfile_path = os.path.join(self.storage_path, self.type, lang, \"en.pdf\")\n\t\tf = open(file_path, \"w\")\n\t\tf.write(text"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself._revise(file_path, {}, [lang])\n    \n\t@classmethod\n\tdef _add_text_tag_by_tag(cls, tag):\n\t\t\"\"\"\n\t\tfor word in tag.text: \n\t\t\tif word.isupper() or word.islower():\n\t\t\t\treturn word\n\t\t\"\"\"\n\t\ttag_split = tag.text.split(' ')\n\t\tif len(tag_split) < 2:\n\t\t\treturn \"\"\n\t\ttagged = []\n\t\t#tag = tag_split[0]\n\t\tfor i in range(1, len(tag_"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tf = open(file_path, 'w+')\n\t\tf.write(str(self.original_lang_id))\n\t\tf.close()\n\n\t\tfor i in range(1,len(self.content)):\n\t\t\tcontent = self.content[i]\n\t\t\tif(content!= None):\n\t\t\t\tf = open(file_path, 'a+')\n\t\t\t\tf.write(content)\n\t\t\t\tf.close()\n\n\t\tf = open(file_path,'w+')\n\t\tf.close()\n\n\tdef check(self):\n\t\tprint \"check\",self.original"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\treturn os.path.join(self.path, self.path + \"rendition.jpg\")\n\n\tdef add_content(self, lang, content, file_path):\n\t\t\"\"\"\n\t\t{\n        \"original\": \"artigo02-en.pdf\",\n        \"en\":\"artigo02-en.pdf\",\n}\t\"\"\"\n\t\treturn os.path.join(self.path, self.path + \"content.pdf\")\n\t\n\n\tdef get_language_file(self, file_path):\n\t\tfile_name = os.path.basename(file_"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.update_current_file(source_path=file_path, language='artigo')\n\t\tfile_name_original = os.path.basename(file_path)\n\t\tif file_name_original.endswith('.ps') or file_name_original.endswith('.pdf'):\n\t\t\tself._add_to_rerun_pdf(lang_name=lang, file_name=os.path.basename(file_path), target_path=\"_Rendition.pdf\")\n\t\tif file_name_original.endswith('.pdf') or file_name_original.endswith"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfile_name, file_ext = os.path.splitext(file_path)\n\t\tif (file_ext == '.pdf'):\t\t\t\n\t\tfile_name = '{0}.pdf'.format(file_name)\n\t\tprint file_name\n\t\tself._check_path(lang, file_name)\n\t\tif (self.check_files(lang, file_path)):\n\t\t\tif (not self._upload_file(lang, file_path)):\t\t\t\n\t\t\t\tself._add_file('artigo', file_name)\n\t\t\telse:\n\t\t\t\tprint 'Upload Error"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tnew_file = open(file_path, \"w\")\n\t\tfor line in fileinput.input(file_path, inplace=True):\n\t\t\tif line.startswith(\"#\"): continue\n\t\t\tnew_file.write(line)\n\t\tnew_file.close()\n\tdef write_rendition_html(self, lang, rendition_html_file):\n\t\t#if self.is_page_available():\n\t\t#\tpage_head = self.create_page_head(lang)\n\t\t#\tpage_head.save(rendition_html_file)\n\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.reponse[\"original\"] = open(file_path, \"rb\").read()\n\n\t\tuploadcode = {\"file\":(file_path, file_path)}\n\t\tresponse = requests.post(self.url, files=uploadcode)\n\n\t\turl = response.json()[\"upload_url\"]\n\t\tfile_url = re.findall(\"<a href=\\\"(.+?)\\\">\", url)[0]\n\t\tif file_url!= \"\":\n\t\t\trendition_url = file_url.rsplit(\"/\", 1)[1]\n\t\telse:\n\t\t\trendition_url = \"\"\n\n\t\tself."}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tfor c, a in self.assets.items():\n\t\t\ta.source_path = os.path.normpath(\"/\" + basename + \"/\" + file_path)\n\n\t\t\tprint(\"{0} - {1} - {2}\".format(\"artigo \" if not c else \"\", a.source_path, file_path))\n\n\tdef create_new(self, file_path):\n\t\tfor c, a in self.assets.items():\n\t\t\tif (self.files_changed(file_path, a.source_path, c, \"artigo02-gf03."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tpath, filename = os.path.split(file_path)\n\t\tfilename, ext = os.path.splitext(filename)\n\t\tif (ext[1:].lower() not in [\".tif\", \".jpg\"]) and ext[1:] not in [\".tif\", \".jpg\"]:\n\t\t\tfilepath = find_image_filename(file_path)\n\t\telse:\n\t\t\tfilepath = self._get_relative_path(basename, file_path)\n\t\tfpath = os.path.join(self._path, filepath)\n\t\tif not os.path.exists(fpath):\n\t\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tpath_to_images = self.image_dirs\n\t\tname = basename\n\t\tif os.path.isfile(file_path) :\n\t\t\timage_path = path_to_images + file_path\n\t\t\ttry :\n\t\t\t\timage_name = os.path.basename(image_path)\n\t\t\t\t# Create the base path\n\t\t\t\tfile_basepath = os.path.basename(image_path)\n\t\t\t\tself.image_dirs[ image_path ] = os.path.dirname(file_basepath )\n\n\t\t\texcept Exception as e :\n\t\t\t\tlogger.error(\"Could"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif basename not in self.asset_list:\n\t\t\tself.asset_list[basename] = Asset(file_path)\n\n\tdef findAsset(self, textureName):\n\t\t\"\"\"\n\t\tReturns an asset or None\n\t\t\"\"\"\n\t\tfor asset in self.asset_list.values():\n\t\t\tif asset.texture == textureName:\n\t\t\t\treturn asset\n\ndef create_texture(basename, file_path, textureType, num_frames=None, color=None):\n\t\"\"\"\n\t:param basename: name of the image\n\t:param file_path: path of"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tname, ext = os.path.splitext(basename)\n\t\tself.assets_paths[-1].append(file_path)\n\t\tself.output_paths[1].append(self.output_path, file_path)\n\n\tdef run(self):\n\t\t\"\"\"\n\t\t@brief\tGenerate a zip file with artigo-specific information.\n\t\t@return\tNone.\n\t\t\"\"\"\n\t\twith zipfile.ZipFile(self.assets_paths[-1][0], 'w') as myzip:\t\t\t# use the big zipfile object to save these files\n\t\t\tmyzip"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tfile_ext = os.path.splitext(basename)[1]\n\n\t\tif file_ext not in MEDIA_TYPES:\n\t\t\treturn None\n\t\t\n\t\tself.add_header(u\"Content-Disposition\", file_ext)\n\t\tself.add_header(u\"Filename\", basename)\n\t\n\t\tfpath = dirname(file_path)\n\t\tif self.dryrun or (self.force or not exists(fpath)) or not isdir(fpath):\n\t\t\tself.make_path(fpath)\n\t\t\n\t\tif not basename.startswith(\"http://\"):\n\t\t\tbasename = \"http://\" + basename\n\t\t\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\t#!!!!\n\t\t# try:\n\t\t# \tos.remove(file_path)\n\t\t# except:\n\t\t# \tpass\n\t\t# \t# return False\n\t\t#\n\t\tpass\n\tdef remove_asset(self, basename):\n\t\tpath = os.path.join(self.output_dir, basename)\n\t\tos.remove(path)\n\t\treturn path\n\t#\n\t#\n\t#\n\t#\tdef convert(self):\n\t#\t\treturn\n\t\n\t#\tdef convert_list(self, path):\n\t#"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.assets.append((basename, file_path))\n\n##\t\t\tself.assets.sort(key=lambda x: len(x[1]))\n##\t\t\tprint \"self.assets: \" + str(self.assets)\n\n##\t\t\tfor item in self.assets:\n##\t\t\t\tif len(item[0]) >= 10:\n##\t\t\t\t\tprint \"item_index: \" + str(item[0].index(str(item[0][1])))\n\n##\t\t\t\tfor file in item[0]:\n##\t\t\t\t\tprint \"file: \""}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\ta = AstroPy.AstroPy()\n\t\tc = a.new_camera_stream(str(basename))\n\t\t\n\n\t\tself.artigo.tracks.add_tracks(name= basename,\n\t\t\tx= 0,\n\t\t\ty= 0,\n\t\t\tz=-8,\n\t\t\tfilename=file_path)\n\n\tdef remove_asset(self, basename):\n\t\t\"unmount directory\"\n\t\tself.artigo.delete_track(str(basename))\n\n\tdef enable_cam_cam(self, basename, file_path):\n\t\t\"enable camera"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\treturn os.path.basename(basename) + '_' + '.jpg'\n\n\tdef __get_filepath(self, filename):\n\t\tpath = os.path.join(self.export_path, \"pic1\", filename )\n\t\tprint(path)\n\t\treturn path\n\n\tdef __get_full_filename(self, filename):\n\t\treturn os.path.join(self.export_path, filename)\n\n\tdef __get_basename(self, filename):\n\t\treturn os.path.split(filename)[-1]#!/usr/bin/env python\n# --"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tprint(\"Creating data structure...\")\n\t\tzip_file = zipfile.ZipFile(zip_path)\t\n\t\tfor zip_name in zip_file.namelist():\n\t\t\t#print(\"Importing {}\".format(zip_name))\n\t\t\tzipdata = parse(zip_file.open(zip_name))\n\t\t\txml_root_node = os.path.basename(zip_file.filename)\n\t\t\t#print(\"Importing {}\".format(xml_root_node))\n\t\t\txml_tree = ET.parse(xml_root_node)\n\n\t\t\t#print (\"Parsing {}\".format(os"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t return json.load(codecs.open(zip_path, 'r', 'utf-8'))\n\n\ndef _get_package_file_data(zip_name):  ##\n\t\"\"\"\nReturns.zip file data if it doesn't exist.\n\nParameters\n----------\nzip_name : str\n    zip file name\nReturns\n-------\ndict\t\"\"\"\n\t file_path = os.path.join(project_home,'package', zip_name+'.dat')\n\t return json.load(codecs.open(file_path, 'r', 'utf-8'))\n\n\ndef _read_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tzip_path = zip_path.split(os.sep)\n\t\tzip_path[-1] = 'zip.zip'\n\t\tret = dict()\n\tif not zip_path[-1].endswith('zip'):\n\t\tzip_path[-1] = file_ext\n\t\tzip_path = zip_path[:-1]\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\treturn ret\n\ndef _expand_paths(path):\n\tos.path.expandvars(path)\n\treturn path.split(os.sep)\n\n"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\t#get_zipfile_from_zip(zip_path)\n\t\tzf = zipfile.ZipFile(zip_path)\n\t\tinfo = zf.infolist()\n\t\tinfo.sort(key=lambda info: info.filename.index('_'))\n\n\t\treturn info[0]\n\ndef _explore_zip(zip_path):\t\"\"\"\n\tExplore zip file structure (zip_path) and return file name\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\nname\tfile name\n\"\"\"\n\tname = None\n\tzip_dir ="}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ttree = etree.parse(zip_path)\n\t\troot = tree.getroot()\n\n\tpackages = {}\n\tfor package in root.findall('package'):\n\t\tname         = package.find('name').text\n\t\tversion      = package.find('version').text\n\t\tsummary      = get_summary(os.path.join(root.findall('filename'), name + '.xml'))\n\t\t\n\t\tpackage        = {\n\t             \"name\"       : name,\n\t             \"version\"     : version,\n\t             \"summary\"    : summary\n\t    }\n\t\tpackages[name]\t= package"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n    import zipfile\n\n######################\n##########  # # ## # ##   # ##           ##   # ##  # ##\n##########  # ## ## # # ##           ##   # ##     # ## ##\n######################\n\ndef unzip(zip_path, dest_path): \n        \"\"\"\n        Unzip a zip file and return a list of data.\n\n        Parameters\n        ----------\n\n            zip_path:str\n                absolute path to zip file\n            dest_path:str\n                dest path of the folder.\n            overwrite:bool\n                overwrite existing files\n        Returns\n        -------\n            dict of {str:dict} \n                \"\"\""}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\tnamelist = zip_file.namelist()\n\tpkg_dict = {}\n\tpath = None\n\t\n\tfor namelist_file in namelist:\n\t\tif name_extract(namelist_file) == None:\n\t\t\tcontinue\n\t\tpkg = dict()\n\t\tfor name in namelist_file:\n\t\t\tif name_extract(name) == None:\n\t\t\t\tcontinue\n\t\t\tpkg[name] = name_extract(name)\n\t\tzip_file.extract(pkg_file, pkg)\t\t\t\t\t\t\t#extract all"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tmanifest_dict = {}\n\t\t# read in binary mode\n\t\twith open(zip_path, \"rb\") as zf:\n\t\t\ttry:\n\t\t\t\tmanifest_dict = read_binary_manifest(zf)\n\t\t\texcept (BadZipfile, PermissionDenied):\n\t\t\t\tprint \"Manifest not readable: \" + zip_path\n\t\treturn manifest_dict\n\t# read in xml archive and return dictionary in memory\n\twith zipfile.ZipFile(zip_path, mode='r') as zf:\n\t\tmanifest_dicts = read_xml_manifest(zf)\n\t\t# loop for each entry in manifest"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tzip_ref = zipfile.ZipFile(zip_path, 'r')\n\t\n\tif _verbose:\n\t\tprint \"[{0}] Opened {1}\".format(_time.strftime(\"%c\"),\n\t\t\tround(time.time()-_start_time, 3))\t\t\t\n\t\t\t\n\tpackages_data = {}\n\tpackage_refs = []\n\tfor root, dirs, files in os.walk(zip_ref.namelist()[0]):\n\t\tfor file in files:\n\t\t\tif file.endswith(\".xml\"):\n\t\t\t\tfilename, file_extension = os.path.splitext(file)\n\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tnamelist = zip_path.namelist()\n\t\tnamelist.remove('.zip')\n\t\treturn_dict = {'packages': {},'strings': {}}\n\t\tfor f in namelist:\n\t\t\tf_root = os.path.join(zip_path, f)['root']\n\t\t\tif zip_path.endswith(f):\n\t\t\t\treturn_dict['strings'] = read_from_zip(zip_path, f)\n\t\t\telif zip_path.endswith(f+'.txt'):\n\t\t\t\treturn_dict['strings'][f] = read_from_zip(zip_path, f+'.txt')"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\tzipf = zipfile.ZipFile(zip_path)\n\tfiles = zipf.namelist()\n\treturn files\t\n\n##### Read and save file\n# This is the part where we read the file\nif not os.path.isfile(filepath):\t\n\tprint('The file you wanted to read is unknown.')\n\texit(0)\n\n\nwith open(filepath, 'r') as f:\t\n\tfiles_read = f.readlines()\n\n# I'm just going to use an iterator for the files\nfor file_name in files_read:\n\t# Get"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfiles = []\n\t\tname_list = []\n\t\tfor root, dirs, files in os.walk(zip_path):\n\t\t\tfor file_name in files:\n\t\t\t\tfiles.append(os.path.join(root, file_name))\n\t\t\t# add folders to check if they exist\n\t\t\tpath_list = [f.split(folder_path) for folder_path in dirs]\n\t\t\tfiles.sort()\n\t\t\tfor path in path_list:\n\t\t\t\told_path = path_list.pop(0)\n\t\t\t\tif old_path.endswith(\".zip\"):\n\t\t\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\treturn zipinfo[0]\n\t#with zipfile.ZipFile(zip_path) as zip_file:\n\t#\treturn map(unicode, files_list_from_zipfile(zip_file))\ndef get_file_md5_for_checksum(file_path):\n\t\"\"\"\n\tReturns the MD5 checksum for the files in file_path\n\t\"\"\"\n\treturn hashlib.md5(open(file_path, 'rb').read()).hexdigest()\ndef zipinfo_get_file_md5_for_checksum(file_path):\n\t\"\"\"\n\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\nimport os, zipfile\n\npaths_to_unzip = [path for path in os.listdir('.') if path.startswith('zip') and path.endswith('.zip')]\n\nfor path in paths_to_unzip:\n\tif os.path.isfile(path):\n\t\tzip_path = path\n\t\tzip_base_path = path\n\telse:\n\t\tzip_base_path = os.path.dirname(path)\n\tzip_path_without_zip = os.path.join( zip_path, '_{0:04}'.format(datetime.datetime.now()."}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\ttry: \n\t\t\timport lxml.etree as etree\n\t\texcept ImportError:\n\t\t\traise ImportError('lxml is not available')\n\n\t\ttry:\n\t\t\tfiles = [join(root, file) for root, subdirs, files in os.walk(zip_path)]\t# get path of all files\n\t\t\tfiles_list = [file for file in files if file not in ('0889', 'tinf')]\t\t# get a list of files\n\t\t\tfiles_list = [file for file in files if file not in ('0889', 'tinf', 'g01', 'g02')]\t\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\treturn ['2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t\t\t\t'2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t\t\t\t'2318-0889-tinf-33-0421/2318-0889-tinf-33-e200061.xml',\n\t\t\t\t'2318-0889-tinf-33-0421/2318-0889-tinf-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\treturn list(zip(filename_list_path, file_list))\n\n\ndef check_file_list(filepath_list):\t\n\t\"\"\"\n\tCheck if the files in `filepath_list` are in the file system.\n\n\tArgs:\n\t\tfilepath_list (list): file(s) to be checked\n\n\tReturns:\n\t\tbool: True if all files are in the file system.\n\t\"\"\"\t\n\treturn not any(\n\t\tcheck_file_list(f)\n\t\tfor f in filepath_list)\n\"\"\"\n#    Majic\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\nlista_archivos=list(os.listdir(os.getcwd()))\n\n# Lista de archivos\nfor item in lista_archivos:\n\tif item.lower() in f:\n\t\tprint(\"{}-{}-{}\".format(item,f,str(len(lista_archivos)-1)))\n\t\n\t\t\n# Lista de archivos del archivo contenidos\nf_archivos=os.listdir(os.getcwd())\nf_archivos.sort()\n\n# Contenedor\nf_arch"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\t# list contents of a zip file\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\t\t\n\t\t# list the file names\n\t\tfiles = [f for f in zip_file.namelist() if not fname.startswith('.')]\n\t\t\n\t\t# return the found files\n\t\treturn files\n\n\t# list the names of files in the zip\n\tzip_file = zipfile.ZipFile(zip_path)\n\t\n\t# for each file, give it a list as a tuple and return\n\t# it as a tuple\n\t#\n\t# [('2318-0889-tinf-33-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tzipf = zipfile.ZipFile(zip_path)\n\t\tfor filename in zipinfo.namelist():\n\t\t\tzipf.read(filename)\n\treturn zipinfo\n\n\ndef files_list_from_zipfile_with_metadata(zip_path, metadata):\n    \"\"\"\n\tReturn the files in `zip_path` according to the metadata dictionary, either from\n\tthe contents of `metadata`_.\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tline_w = content.partition(\"=\")[2]\n\t\tline_w.replace(\"'\",\"''\")\n\t\tline_w = line_w.replace(\"\":\",'\")\n\t\tline_w = line_w.replace('\"','')\n\t\tline_w = line_w.replace(\"'\",\"'\")\n\t\tline_w = line_w.replace('\"','')\n\t\tline_w = line_w.replace(\"'\",\"'\")\n\t\treturn line_w\n\n\tdef fix_namespace_prefix_f(content):\t\"\"\"\n\tMake a namespace_prefix of the form 'namespace:key"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tfrom frappe.utils import md5\n\t\treturn md5(content.replace(r'\\\\', '\\''))\n\n\treturn re.sub(\"\\s*(<|>)\\s*:([^>]*)>\", fix_namespace_prefix_w, content)\n\n\ndef get_comment_list(doctype, name):\n\t\"\"\"Comment a doclabel\n\t\"\"\"\n\treturn frappe.db.sql(\"\"\"select name, subject, content\n\t\tfrom tabDocField where (fieldtype=\"Text\" and\n\t\t\tfieldsize <= 99 %s or fieldlength <= 999)\"\"\" % doctype, as_dict=1"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn content.replace(\":\", \" \").replace(': \" \"').replace('w:st=\"w:st=', 'w:')\n\treturn content\n\n\n_meta_charset_regex_cache = {}\n\"\"\"\nExample:\n\t- _meta_charset_regex_cache['iso-8859-4'] = \"iso-8859-4\"\n\"\"\"\n\ndef meta_charset_regex(pattern):\n\treturn _meta_charset_regex_cache.get(pattern) or re.compile(pattern).search\n\ndef charset_regex(content):\n\t\"\"\"\n\tConvert a"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tfor k, v in settings.WALK_NAMESPACE_PREFIX_REPLACEMENTS.items():\n\t\t\tcontent = content.replace(v, k)\n\treturn content\n\"\"\"\n.. module: lemur.plugins.base\n    :platform: Unix\n    :copyright: (c) 2015 by Netflix Inc., see AUTHORS for more\n    :license: Apache, see LICENSE for more details.\n\n.. moduleauthor:: Kevin Glisson <kglisson@netflix.com>\n\"\"\"\nimport collections\nimport math\nimport re\nimport random\nimport struct\nimport"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn content.replace(\"w:\", 'w:')\t\n\tdef fix_namespace_prefix_x(content):\t\treturn content.replace(\"x:\", 'x:')\t\n\tfix_namespace_prefix_i(content)\n\tif content==\"\":\n\t\treturn \"\"\n\tfix_namespace_prefix_o(content)\n\treturn \"<encoding %s >\" % (content)\n\nif __name__=='__main__':\n\tfrom pprint import pprint\n\timport sys\n\tif len(sys.argv)!=4:\n\t\tprint(\"Usage python gen_ns3."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\timport codecs\n\t\tif config.is_win32:\n\t\t\ttry:\n\t\t\t\tfrom subprocess import getstatusoutput, DEVNULL, Popen\n\t\t\texcept:\n\t\t\t\tfrom commands import getstatusoutput, DEVNULL, Popen\n\n\t\t\tdef my_popen(cmd, mode='t'):\n\t\t\t\timport subprocess\n\n\t\t\t\tif not getstatusoutput(cmd):\n\t\t\t\t\t\traise\n\t\t\t\ttry:\n\t\t\t\t\tprocess = Popen(cmd, shell=True, bufsize=0,\n\t\t\t\t\t\tstdout=subprocess.PIPE, stderr=DEVNULL)\n\t\t\t\texcept:\n\t\t\t\t\tp = Popen(cmd, shell=True,"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif '\"' in content: return re.sub('\"','\"','\"', content)\n\t\treturn re.sub('w:st=','w-st=','',content)\n\treturn content\n\ndef apply_tag_to_id_and_name(id_tag, name_tag):\n\txml_node = etree_parent (xml_parent)\n\ttag_nodes = xml_node.findall(\".//tag\")\n\tid_tag_node  = tag_nodes[0].findall(\".//%s\" % (id_tag))[0]\n\tname_tags = name_"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\timport re\n\t\tre_namespace_prefix = re.compile('^\\w:st\\s*:$')\n\t\treturn re_namespace_prefix.sub(' ', content)\n\ndef run(apiurl, query):\n\t\"\"\"Run a search query to get the results for a list of results\"\"\"\n\tapiurl += \"?q=\" + query\n\tresponse = http_request(apiurl)\n\tentries = []\n\twhile(response):\n\t\tif 'query_continue' in apiurl:\n\t\t\tbreak\n\t\tentries.extend(json.loads(response))\n\treturn entries\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\t#TODO: make this really Unicode\n\t\treturn re.sub(\":\",\":\",content)\n\n\tdef merge_setting(self, p,m):\n\t\t'''Prune empty lines, collapse adjacent empty lines, make unique entries for\n\t\tidentical properties in p'''\n\t\tt = m[0]\n\t\tif len(t) == 0:\n\t\t\ti = 1\n\t\telse:\n\t\t\ti = len(t)-1\n\t\twhile i >= 0 and m[i][-1] == '\\\\':\n\t\t\ti -= 1\n\t\twhile m[i][-1] =='':\n\t\t\ti"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif not content:\n\t\t\tcontent = \"<html><head></head><body>Sorry, the document definition is invalid; HTML tags can't contain w:st=\"'>.\"\n\t\tcontent = re.sub(\"<W:st.*>\", \"\", content)\n\t\treturn replace_ampersands(content)\n\treturn re.sub(\"prefix=\\\"w:st *;?><w:st *;?>?\", fix_namespace_prefix_w, content)\n\ndef fix_section_namespace(content):\n\tsections = re.split(r\"<(hd|x|img|form|"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor n in node.childNodes:\n\t\t\tif n.uri == XMLNS_PUBDATE:\n\t\t\t\treturn n\n\t\n\tdef match_rating(node, rating_xpaths):\t\"\"\"\n\t\tReturns the first match in the rating_xpaths list\t\"\"\"\n\t\tfor n in node.childNodes:\n\t\t\tif n.uri == XMLNS_RATING:\n\t\t\t\treturn n\n\t\n\tdef match_publisher(node, publisher_xpaths):\t\"\"\"\n\t\tReturns the first match in the publisher_xpaths list\t\"\"\"\n\t\tfor n in node.childNodes:\n\t\t\tif n.uri"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor ytpa in pubdate_xpaths:\n\t\t\tif xtox_pattern.match(yopa + '.') == 0:\n\t\t\t\treturn ytpa\n\t\treturn None\n\n\tdef get_pubdate(self, node):\n\t\t\"\"\"\n\t\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor ytpa in self.match_pubdate(node, self.pubdate_xpaths):\n\t\t\treturn ytpa\n\t\treturn None\n\n\tdef get_pubdate_suffix(self, title, prefix):\n\t\t\"\"\"\n\t\tReturns the match etag"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tmatch = None\n\t\tfor xpath in pubdate_xpaths:\n\t\t\tif match:\n\t\t\t\tif xpath.endswith(text_type(node)):\n\t\t\t\t\tmatch = xpath\t\t\t\n\t\t\telse:\n\t\t\t\tmatch = xpath\n\n\t\treturn match\n\treturn None\n\nclass TestXPaths(unittest.TestCase):\n\n\tdef test_parse_xpaths_simple_test(self):\n\t\tfor text in [\"The Test\", u\"\u00a1\u00a1\u00a1\u00a1_\u00a1\",\n\t\t\t\t\tu\"Hello World\"]:\n\t\t\tf = text_type(text)\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif len(node.get(tag, [])):\t# the pubdate is already defined\n\t\t\tif pubdate_xpaths:\n\t\t\t\tfor xpaths in pubdate_xpaths:\n\t\t\t\t\tif (  len(xpaths) == 1\n\t\t\t\t\t\tand node.get(xpaths[0])[4] == pubdate_xpaths[0]\n\t\t\t\t\t\tand len(xpaths[1].get(tag, [])):\n\t\t\t\t\t\treturn xpaths[0]\n\t\t\telse:\n\t\t\t\tfor xpaths in pubdate_xpaths:\n\t\t\t\t\tif (len(xpaths) == 1\n\t\t\t\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor xp in pubdate_xpaths:\n\t\t\tif _pubdate_is_earlier(piw, xp, node.get('pubdate')):\n\t\t\t\treturn xp\n\t\treturn None\n\n\tdef match_pubdate_date(node, pubdate_xpaths_is_earlier):\n\t\t\"\"\"\n\t\tReturns true if the node text is in the pubdate_xpaths_is_earlier of this node's.pubdate file.\n\t\tIt is used in place of this node's.pubdate with a date value that is in the document's.article's.txt file\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor pubdate in node.findall(pubdate_xpaths[0]):\n\t\t\treturn pubdate\n\n\tlast_xn_filename = match_filename(node, filename_xpath_str)\n\n\tif last_xn_filename is None:\n\t\treturn None\n\n\treturn last_xn_filename.text\n'''\nTask Coach - Your friendly task manager\nCopyright (C) 2004-2013 Task Coach developers <developers@taskcoach.org>\n\nTask Coach is free software: you can redistribute it and/or modify\nit under the terms of the GNU General"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor x in pubdate_xpaths:\n\t\t\tif isinstance(node.get(x), dict):\n\t\t\t\tmatch = True\n\t\t\t\tfor y in node[x]:\n\t\t\t\t\tmatch = match and node[x][y] == x\n\t\t\t\t\tif node[x][y] == y:\n\t\t\t\t\t\treturn True\n\t\t\tif not match:\n\t\t\t\treturn False\n\t\treturn False\n\t\n\t# check for date range and month names\n\tdef match_pubdate_and_month_name(self, node, pubdate_xpaths):\t\n\t\tfor x in pubdate_xpaths:\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn (node.get(PUBFILE) or node.get(TITLE).get(PUBDIR) or {}).get(XHTML)\n\ndef process_feed(url):\n\t'''Extract the text from a URL and call this procedure on each entry.\n\t'''\n\ttitle = url and url.get(ENTRY) or \"\"\n\tparser = make_parser()\n\tparser.feed(url)\n\tparser.close()\n\n\tfor entry in parser.entries:\n\t\tmatch_title = pattern.search(entry.get(TITLE))\n\t\tif match_title:\n\t\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif node:\n\t\t\tdate = node.get(settings.ID_FIELD, None)\n\t\t\tif not date or not pubdate_xpaths:\n\t\t\t\treturn None\n\t\t\tif '/' not in date:\n\t\t\t\treturn None\n\t\t\tpubdate_path, pubdate_intp = parse_pubdate(date)\n\t\t\tif not pubdate_intp or not pubdate_intp.integer:\n\t\t\t\treturn None\n\t\t\treturn pubdate_intp.number\n\t\treturn None\n\nregister.tag(CMSStaticPattern(\n    function='replace_all', \n    inputs=('^"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tpubdate_xpaths = pubdate_xpaths[:]\n\t\tfor node, n in pubdate_xpaths:\n\t\t\tif n.find(\"pubDate\"):\n\t\t\t\tif node.find(\"node\") is None:\n\t\t\t\t\tnode.tail = None\n\t\t\t\telif n.find(\"pubDate\").text in pubdate_xpaths:\n\t\t\t\t\tbreak\n\t\treturn node\n\n\tdef matches(self, node):\n\t\t\"\"\"Returns True if the node matches the context node\"\"\"\n\t\tmatches = {}\n\t\tfor n in node:\n\t\t\t#print n.tag, n.text, node.get"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tif (issue_ellis_match):\t\n\t\t\tissue_ellis = issue_ellis_match.group(1)\n\t\telse:\n\t\t\tissue_ellis = ''\n\n\t\t# Cleaning in case there are multiple spaces following the issue's number\n\t\tnumber_val = issue_ellis_clean.search(issue_ellis)\n\t\tif number_val:\n\t\t\tif len(number_val) > 1:\n\t\t\t\tnumber_val = number_val.group(1)\n\t\treturn int(number_val)\n\t\t# return issue_ellis_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tfor item in issue.getchildren():\n\t\t\tnumber=item.getName()\n\t\t\tsuppltece=item.getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].text\n\t\t\treturn number,suppltece\n\t\t\t\t\nclass Issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\ttag = issue.find(ns_tag(\"item\")).find(ns_tag(\"number\"))\n\t\tvalue = issue_el.find(ns_tag(\"suppl\", value_id=\"number\"))\n        return (value.text, value.text)\n\n    def is_issue_valid_date(self, issue):\n        \"\"\"\n        Verify this issue is a valid ISO8601-formatted string.\n        \n        :param issue: a issue element that contains a valid ISO8601-formatted string\n        :type issue: a :py:class:`lxml.etree._Element` instance.\n        \"\"\"\n        val_xml ="}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber,suppl,suppl_num = issue\n\t\tif number and (not number.strip() and len(suppl) > 0):\n\t\t\tnumber = string.replace(number,',','--')\n\t\tif number and (not number.strip() and len(suppl) > 0):\n\t\t\tnumber = string.replace(number,',',string.join(list(suppl),',')).replace(',','\\n')\n\n\t\texpo_number,suppl_num = issue\n\t\treturn number,suppl,suppl_num\n\n\n#====================================================================#\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n# \t*"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\t_extract_number_and_supplment = {\n\t\t\t'director': -1,\n\t\t\t'type': -1,\n\t\t\t'title': 'issue_number',\n\t\t\t'text': issue[1].text,\n\t\t\t# 'volume': comment.volume,  # CALE\n\t\t\t# 'issue_num': issue[3].number,\n\t\t\t# 'number': issue[3].number,\n\t\t}\n\t\tissue_number = issue[3].number.translate(None, '()').strip()\n\t\tnumber = issue_number[-1]\n\t\tsuppl"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\ttry:\n\t\t\tline, number = issue.find('span').replace('.', '').replace(',', '').split('\\t')\n\t\t\tsuppl = issue.find('p')\n\t\t\tissue_number = utils.extract_number(line)\n\t\t\tissue_suppl_number = int(number)\n\t\texcept ValueError:\n\t\t\tissue_number, issue_suppl_number = '0', '0'\n\t\treturn issue_number, str(suppl)\n\n\tdef _create_journal_in_issue(self, issue):\n\t\t\"\"\"\n\t\tCreates a Journal object from the issue.\t\n\t\t\n\t\t:"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = issue.find(\"{%s}number\" % _ns_uri)\n\t\tsclass = issue.find(\"{%s}supersede\" % _ns_uri)\n\t\tsbody = issue.find(\"{%s}body\" % _ns_uri)\n\t\tpclass = issue.find(\"{%s}class\" % _ns_uri)\n\t\tpname = issue.find(\"{%s}name\" % _ns_uri)\n\t\tif number:\n\t\t\tif number.find(sbody) is not None:\n\t\t\t\tnumber_text = number.find("}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tif issue is not None:\n\t\t\tnumber = issue.xpath(\"issue/issue[@number]\")[0].text.strip()\n\t\t\tvolume = issue.xpath(\"issue/issue[@volume]\")[0].text.strip()\n\t\t\tsuppl = issue.xpath(\"issue/issue[@supplement]\")[0].text.strip()\n\t\t\tnumber = re.sub(r\"\\s+\", \" \", number)\n\t\t\tnumber = re.sub(r\"\\s+\", \" \", number).replace(\" \", \"\")\n\t\t\tjournal = issue.xpath(\"issue/journal/issue[@journal]\")[0].text."}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber_part, suppl_part = issue_element_parser(issue, './issue/number', {'number': None,'suppl': None})\n\t\tissue_number = str(int_part.text.strip().replace(',', ''))\n\t\tissue_suppl = str(int_part.text.strip().replace(',', ''))\n\t\treturn [issue_number, issue_suppl]\n\n\n\tdef _extract_issue_identifier_and_volume(issue):\t# IssueID, Volume and Date\n\t\t# IssueID is optional; volume is optional.\n\t\tissue_name = 'issue'"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = ''\n\t\tsupplt = ''\n\t\tfor el in issue.findAll():\n\t\t\tif el.tagName == 'number':\n\t\t\t\t\tnumber = el['value']\n\t\t\tif el.tagName =='suppl':\n\t\t\t\t\tsupplt = el['value']\n\n\t\tnumber = number.strip()\n\t\tsupplt = supplt.strip()\n\t\treturn number, supplt\n\t\t\n\tdef _parse_issue_element(element, issue_element):\n\t\t\"\"\"Parse the information from the element 'issue' \"\"\"\n\t\tdef find_attribute(el, attr_name):\n\t\t\t\"\"\""}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tindentStr = indent if indent == self.indent else \"\"\n\t\tl = []\n\t\tfor h in self.head:\n\t\t\tl.append(\"%s%s\" % (\" \" * indentStr, h))\n\t\treturn '\\n'.join(l)\n\n####################################################################################################\n# Utils for the file input parameters\n####################################################################################################\n\ndef getFileFormat():\n\t\"\"\"\n\tThe method returns the input file format (see inputFiles in InputManager) as string format for\n\tthe python process.\n\t\"\"\"\n#  for example usage take this line from the root directory where __"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\t# TODO: implement\n\t\t# indent = 6 => indent every 6 spaces\n\n\t\tprint (\"   \"*indent)+pretty_repr(self, self.depth, indent, debug)\n\t\treturn pretty_repr(self.root)\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn self.to_string(indent=indent, debug=debug)\nfrom __future__ import unicode_literals\n\nimport logging\nimport mimetypes\nimport posixpath\nimport uuid\n\nfrom mopidy import backend, exceptions, protocol\nfrom mopidy.core.utils import unix_filename\nfrom mopidy.core.utils.utils import parse_url\n\ntry:\n    from urllib import parse as urllib_parse\nexcept ImportError:\n    import urllib as urllib_parse  # pylint: disable=import-error\n\nLOG = logging.getLogger(__name__)\n\n\nclass"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\tdef encode(obj, context):\n\t\t\t\ttry:\n\t\t\t\t\treturn encode_canonical(obj, context)\n\t\t\t\texcept Exception:\n\t\t\t\t\tlog(None, True)\n\t\t\t\ttry:\n\t\t\t\t\treturn make_repr(obj, context)\n\t\t\t\texcept Exception:\n\t\t\t\t\tlog(None, True)\n\t\t\t\treturn 'None'\n\t\timport sys\n\t\treturn indent*indent + repr(self.to_dict())\n\nclass JSONObjectEncoder(JSONEncoder):\n\t\"\"\"\n\tCustom JSON encoder which serializes itself.  This serializes out objects that are found inside it,"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t    \tif debug:\n\t        \treturn \" {m0:#x} |-> {m1:#x} {m2:#x} |-> {m3:#x} \".format(m0=self.m0,m1=self.m1,m2=self.m2,m3=self.m3)\n\t    \telse:\n\t        \treturn \" {m0:#x} |-> {m1:#x},{m2:#x} \\\\\\\\\\n\".format(m0=self.m0,m1=self.m1,m2=self"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tret = []\n\t\tret.append(' '*indent + str(self.header))\n\t\tfor field in self.fields:\n\t\t\tret.append(self.__dict__[field])\n\t\tdef make_header(fields):\n\t\t\tret.extend([\"'\" + x[0] + \"':\" + str(x[1]) + \"'\" for x in fields])\n\t\treturn \"[\" + \", \".join(ret) + \"]\\n\"\n\n\tdef __repr__(self):\t\treturn repr(self.header)\n\n\nclass PlainDocString(object):\n\t\"\"\"\n\tA documentation"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\treturn \"n\" + indent + \"\\n\" + self.dump()\n\t\tif self.is_empty():\n\t\t\treturn \"\\n\".join([child.__str__() for child in self]) + \"\\n\"\n\t\tresult = \"n\" + indent + str(self) + \"\\n\"\n\n\t\tdef dump(x):\n\t\t\tif x is not None:\n\t\t\t\tresult += \"\\t\"\n\t\t\t\tx.show()\n\t\t\treturn result\n\n\t\tlines = self.dump()\n\t\tif lines:\n\t\t\tresult += indent + \"commands:\\n\" + indent +"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn tostr(self.__dict__)\n\n\t# ----------------------------------------------------------------------\n\t# Exception formatting.\n\n\tdef __str__(self):\n\t\t\"\"\"Print out the message.\"\"\"\n\t\treturn self.as_string(sort_keys=True, indent=indent,\n\t\t\tout=DEBUG)\n\n\t# ----------------------------------------------------------------------\n\t# Utility functions.\n\n\tdef get_pretty_info(self, max_depth=5, file=DESCRIPTOR_FILE,\n\t\tshow_file_header=True, show_line_numbers=False):\n\t\t\"\"\"\n\t\tPrint out all the information as human readable\n\t\t(i"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif len(self.children) == 0:\n\t\t\treturn \"\"\n\n\t\tdata = self.children[0].pretty(indent, debug)\n\t\tif len(data) > 0:\n\t\t\treturn data\n\n\t\tfmtstr = self.__class__.pretty.__doc__\n\t\tfor line in self.children[1].pretty(indent+2, debug):\n\n\t\t\ts = line.strip().strip(' ')\n\t\t\tif debug:\n\t\t\t\ts += \" > \"\n\n\t\t\tif len(line) > 0:\n\t\t\t\tdata += \"\\n\"\n\t\t\t\tline  = line.replace(\"  \","}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tprint (indent)+self.pretty_tags(indent, debug)\n\t\t\t\n\tdef to_string(self):\t\n\t\treturn str(self)\t\n\tdef get_id(self):\n\t\t return(str(self.id))\n\tdef get_string(self):\n\t\treturn str(self.toString())\t\n\n\tdef get_pretty_tags(self):\n\t\treturn(str([k for k in self.pretty_tags()]))\t\n\t\t\t\n\tdef make_id(self):\n\t\tself._id = int(self.get_id())\n\t\treturn("}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t  \n    \tself.checkRecursion( self.createExpr(args[\"expr\"]), \n\t\t\t[self.createExpr(i) for i in (args[\"expr\"], [])])\n\n    ################################################################################\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nfrom __future__ import absolute_import\n\nfrom os import path\nfrom textwrap import dedent\n\nfrom mach.decorators import (\n   "}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        sols = list(zip(args[::2], args[1::2]))\n        sols.append(\"\") \n\n        for i in xrange(len(args[2::2])):\n            sols.append(self.replace(sols[-1], \"^\", \"^\"))\n        return self.replace_list(sols, \"!\", \"^\")\n\n\tdef a_and_A_B_and_A_neg1(self, expr):\n        \"\"\"\n        Performs A & (A & B) AND A & (-A & B);\n        Returns True if and only if there are no repeated substitutions at all."}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        new_args = []\n        for arg in args:\n            if arg.__class__!= type(args[0]): new_args.append(arg)\n            else:\n                new_args.append(arg.__class__(args[0], args[1]))\n        return self.operate_combine(tuple(new_args))\n\t\t\n\t# ----------------\n\t# Removal rules, which are not well supported\n\t# ----------------\n\tdef __mul__(self, other):\n\t\treturn self.operate_combine(other * (-1))\n\t\t\n\tdef __add__(self, other):\n\t\treturn"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t        \n        s = []\n\tfor a in args:\n\t    if not a in self._constraints:\n\t\traise LogicError(\"%s is not a valid symbolic constraint\" %\n\t\t\t\t\t\t  (a))\n\t\telse:\n\t\t    s.append(self._constraints[a][0])\n        self._constraints = s\n\n\n##################################################\n\nclass Fun(*Logic, Assignment):\n\n    \"\"\"\n    An Fun variable, a Fun variable in a form, where variables are\n    variables or expressions.\n\n    \"\"\"\n\n    def __new__(type, *elements):\n        if not elements:\n            raise LogicError(\"Fun"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        expr = [ self.absdiff(self._expr, arg) for arg in args ]\n        expr.append( self.absdiff(self._absorb, args) )\n        return expr\n\n    def complex(self, args):\n        \"\"\"\n        Given an expression containing complex numbers, return a new list\n        of expression that recieve complex expressions.  Also do not\n        alter any variables, if any, which has a default value of b's\n        value.\n        \"\"\"\n        # XXX Would be nice to handle cases like\n        #   c1*a + b1 == c2*a + 1 ="}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    absorb_map = {}\n    pos = 0\n    currentA = None\n    for item in args:\n        temp = absorb(item)\n        # print \"   currentA = %s\" % currentA\n        # print \"   temp = %s\" % temp\n        if (temp in absorb_map):\n            currentA = absorb_map[temp]\n        else:\n            tmp = currentA + absorb(item)\n            absorb_map[temp] = tmp\n        pos = pos + 1\n    absorb_map[currentA] = currentA\n    return"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    def filter(self, i):\n        \"\"\"\n        Returns the expression whose logical AND of the arguments in\n        i are bounded by a coefficient in the formula. The expression\n        of an element is returned. If the expression is univariate,\n        return an element containing the classical expression.\n\n        If A and A'X' and a'Y' are both unknown, return one of \"A\" and \"\".\n        If a and A'X' and b'Y' are not known, return an arbitrary expression.\n        If a and B'X' and 'X' are real numbers, return another numeric expression.\n        \n       "}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    # A | B is the zero-indexed expression; no proof for 'A'\n    if len(args) == 3:\n        # One or two proofs to the left of B\n        proof = args[2]\n        if proof.type!= BOOL and not proof.value in [L_AND, L_OR]:\n            return\n        elif proof.type!= BOOL and proof.value in [L_NOT, L_AND]:\n            return\n        return list(args[0])\n\n    # A & B are zero-indexed, so remove those\n    else:\n        return list(args"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    result = []\n    for expr in args:\n        if expr in self:\n            result.append(expr)\n        else:\n            for expr2 in self:\n                result.append(expr2 ^ expr)\n       \n    \n    for expr in args:\n        self._reduce_constants(expr, result)\n    return result\n\n\nclass AbsorptionException(Exception):\n    \"\"\"\n    Exception raised when a non-zero value is encountered. Can be raised\n    at any time by setting the attributes `is_defined`,\n    `is_in_range`, etc.\n    \"\"\"\n    def __init__(self, val):\n        self."}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n      return []\n\n    def cst_subexpr(self):\n\t\"\"\"\n\t\"\"\"\n\treturn self.args[0]\n\n    def cst_apply(self, args, cst_subexpr):\n\t\"\"\"\n\tApply a sequence of expressions to a new absorbing group.\n\nArgs:\n\n    args = list of Expression expressions\n\n    cst_subexpr = new expression to apply to subexpression\n\n    Example:\n        >>> absorb(args).cst_subexpr()\n        a b c + 1\n        \n\"\"\"\n\treturn tuple()\n\n    def cst_subexpr_str"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    if self.__handlers.has_key(hook):\n\t\tself.__handlers[hook]._add(self)\n\n\ndef init_hooks(self):\n    \"\"\"\n\tDecorate all of the hooks to handle changes in the hooks registry.\n\t\"\"\"\n    for k, v in _get_hooks_config().items():\n\t\tif \"on\" in k:\n\t\t\tself.__config[k] = on\n\t\t\tfor hook in v:\n\t\t\t\tself._hooks[hook] = getattr(self, hook)\n\t\t\t\n\n\t\t\n\t\telif v:\n\t\t\tself.__config[k] = v"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        # Register the handler\n        hooks[Hooks[hook]] = hook\n\n\t# Return the newly added handler\n\treturn hook\n\n\t\"\"\"\n\tAdds a new handler to the store.\n\nArgs:\n    hook (HookType): Hook attribute for which to add the new handler.\nReturn:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n\t\n\n\t\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#  main.py ---\n#\n# Filename: main.py\n# Description:\n# Author: Subhasis Ray"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        def wrapper(f):\n            self.hooks.add(f)\n            return f\n        \n        return wrapper\n\n\nclass MessageBus:\n    \"\"\" \n    Broadcasts messages to another bus.\n    \"\"\"\n\n    def __init__(self, bus):\n        self.messages = Queue(maxsize=32)\n        self.bus = bus\n        self.stop = Event()\n        self.start = Event()\n        self.queue = {}\n\n        def add(msg):\n            # Check if the message is a message set or a single message\n            if len(msg.data) == 0:\n                if msg.type =='status' or \\"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        _LOGGER.debug(\"Registering listener %s\", hook.name)\n        self.add_listener(hook)\n        return hook\n    \n# encoding: utf-8\n\"\"\"\n@author    : Scott Moser\n@author    : Nicholas H.T. LATTER\n@copyright : WTFPL http://www.wtfpl.net\n@license : LGPL - Lesser General Public License\n\"\"\"\n# Package Imports\nimport urllib2\n\n# Local Imports\nfrom eWRT.ws.Common import Url\nfrom eWRT.ws.Common import WS"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\ndef on_all(hooks):\t\n\t\"\"\"\n\tDecorator for registering listeners for all registered hook types.\n\n\tArgs:\n    hooks (list): List of all hook types to register listeners for.\n\nReturns:\n    callable: Decorator for registering listeners for all registered hook types.\n\n\"\"\"\n\tdef wrapper(func):\n\t\tfor hook in hooks:\t\t\t\n\t\t\tsetattr(func, hook, hook)\n\t\ttry:\n\t\t\tfunc()\n\t\texcept Exception:\n\t\t\t# We know this isn't anything that happens.\n\t\t\timport traceback\n\t\t\treturn traceback.format_exc()"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\t@self.on_plugin_loaded\n\t\tdef load():\n\t\t\thandler = self.plugins.get(hook.name)\n\t\t\tif handler:\n\t\t\t\tyield handler\n\ndef set(self, name, callback):\n  \"\"\"\n  Sets the specified handler name.\n\nArgs:\n    name (str): Name of the handler (e.g.'message')\n    callback (function): Function to execute when the handler is\n        registered (e.g. 'on_message' hook).\n\nReturns:\n    None.\n\nRaises:\n    AttributeError: When a hook is already registered before the handler\n       "}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        from. import dispatcher\n        from. import _default\n\n        def wrap(fn):\n                @dispatcher.handler(hook)\n                @hook(self)\n                def cb(message):\n                        method=getattr(self, \"on_\"+hook.__name__, None)\n                        if method is None:\n                                return\n                        method(*args, **kwargs)\n                return wrap\n            return cb\n        return wrap\n\n\ndef get_events():\n        \"\"\"\n    This function returns a list of all `hook` callbacks registered\n    \"\"\"\n        return [self] + [w for w in dispatchers]\n\nfrom random import randint\nfrom itertools import chain"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n        self.handlers.append(hook)\n\n\tdef before(self, args, context):\n\t\t\"\"\"\n\t\tAdd an hook to the registry.\n\n\t\tArguments:\n\t\targs (list, tuple): Arguments for the hook to add.\n\t\tcontext (str): Current name of the context from which this\n\t\thook has been registered.\n\t\t\"\"\"\n\n\t\t# Get all the arguments from command\n\t\targs = [arg if isinstance(arg, str) else arg.split()[0] for arg in args]\n\n\t\t# Ensure this hook is registered with our implementation\n\t\tfor hook_class, hook"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tdef inner(func):\t\t\t\t\t\t# pylint:disable=C0103\n\t\t\thooks.register(hook, func)\t\t\t\t\t# pylint:disable=E1101\n\t\treturn inner\n\treturn decorator\nimport base64\nimport json\nimport re\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport decimal\nimport hashlib\nimport itertools\nimport time\n\nimport boto3\nimport django.utils.functional as core_util\nfrom django.conf import settings\nfrom django import forms\nfrom django.utils.crypto import get_random_string\nfrom django.utils.translation import ugettext_lazy as"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\nclass CommandHandler(RegistryHandler):\n\t\"\"\"\n\tDecorator function to add a new command handler to your registry.\n\nArgs:\n    arg (CommandParam): Command param.\n\nReturns:\n    callable: Decorator for registering command handlers for the named\n    command handler.\t\"\"\"\n\nclass MultiCommandHandler(RegistryHandler):\n\t\"\"\"\n\tDecorator function to add multiple registered command handlers\n\nArgs:\n    commands (MultiCommandParam): Commands to register to.\n\nReturns:\n    callable: Decorator for registering commands for the same command.\n\nExample:\n  @on(MultiCommandHandler"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t      \n\treturn {\"user\": user,\n\t\t      \t\"etcd_host\": etcd_host,\n\t\t      \t\"etcd_port\": etcd_port}\n\t      \ndef get_data_from_config(config, section, key, default):\n\t\"\"\"\n\tRetrieves a configuration value from a configuration section\n\tand key pair.\n\tIf section is not in ini file it will be created.\n\tIf the key does not exist it will be created.\n\tReturns a dict.\n\t\"\"\"      \n\tif not config:\n\t\treturn None\n\t\t\n\ttry:\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\n\t#create a default configuration of a user if none exists\n\tcreate_configuration(user=user)\n\t\n\t#get parameters for creating a database\n\tcreate_database(hostname=user,user=user)\n\n\t#set and configure our instance\n\tdb_config = create_instance_config(etcd_host=etcd_host, etcd_port=etcd_port)\n\tdb_config[\"hostname\"] = etcd_host\n\tdb_config[\"etcd_url\"] = etcd_url\n\tdb_config[\"etcd_username\"] = etcd_username\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    configs = {\"host\": etcd_host, \"port\": etcd_port}\n    \n    if user!= \"\":\n\t\n      configs[\"user\"] = user\n    if etcd_host!= \"\":\n        configs[\"host\"] = etcd_host\n\n    if etcd_port!= \"\":\n        configs[\"port\"] = etcd_port\n\n    return configs\n        \ndef configure_static_configs():\n    \"\"\"\n    Starts the configuration by running the static database config.\n\n    Returns:\n    \n    dict: the created configuration.\t\"\"\"\n    \n    configs = {}\n\n    env.user = env.config_get(\"user\")\n    env.password ="}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\tconfig_str = \"\"\n\t\tconfig_str += \"server={server}\\n\"\n\t\t\n\t\tconfig_str += \"hostname={host}\"\n\n\t\tconfig_str += \"database=epcd,etcd,database={db}\\n\"\n\t\tconfig_str += \"user={user}\\n\"\n\t\tconfig_str += \"password={passwd}\\n\"\n\t\t\n\t\tconfig_str += \"database_user={db_user}\"\n\t\t\n\t\tconfig_str += \"directory={directory}\"\n\n\t\tconfig_str += \"host={host},\"\n\t\tconfig_str += \"port={port}\\n\""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    return requests.post(BASE_CONFIG_URL, auth=(user, etcd_host, etcd_port))\n    #return(requests.post(BASE_CONFIG_URL, auth=(\"root\", etcd_host)))\n\ndef get_user_and_password():\n\treturn  get_user_and_password_for_user()\n\ndef get_user_and_password_for_user():\n\tparams = { \"root\" : \"/root/open_hummingbird/config\" }\n\turl = \"{}/\".format(BASE_URL) \n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    etcd_config = u\"\"\"#\n# Tethys config.\n#\n# You can add your own authentication parameters, either in this order (not yet used):\n#\n# - OUO_HOST, OUO_APIKEY, OUO_PASS, and OUO_DBNAME must include the OUO_APINAME\n#   and OUO_DATABASE as the authentication username.\n# - OUO_USER, OUO_PASSWORD, and OUO_HOSTNAME must include a trailing space.\n##\n # Example configuration\n#\n#"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    config = configparser.ConfigParser()\n    config.read(etcd_host)\n    user_info = config['user_info']\n    user = user_info['user']\n    user_password = config['user_password']\n    if user == '':\n        user = '@' + user\n    # password to be used if user exists, otherwise use the password specified in the user.\n\n    # get password from etcd_config.ini.\n\n    # get the user password from etcd_config.ini file\n\n    #if the user doesn't exists, then make the default value\n\n    try:\n        with open(etcd_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\tjson = {'cmd': 'create_static_configuration','message': 'Error: Failed to create static configuration.'}\n\t\treturn json\n\t# If the user has no credentials (the config has not been created yet), exit with an error.\n\tif user == 'not':\n\t\tprint 'Error, you must provide credentials to connect through the API.\\n'\n\t\treturn {'cmd': 'create_static_configuration','message': 'Error: Need to create the static configuration first.'}\n\n\t# Parse the args; for example, if you are looking for the user in the settings file"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    try:\n        conn_string = \"host={0};port={1} dbname=django\".format(etcd_host,\n                                                    etcd_port)\n        # use 'psycopg2' for username and 'password' for password.\n        conn = psycopg2.connect(\"dbname=\" + etcd_config[\"db_name\"] + \" user={0} password={1}\".format(user,\n                                                        etcd_config[\"db_password\"])).cursor()\n\n        # get table and column list\n        column_list = get_database_list(conn)\n\n\t# get fields list for SQL query\n\ttable"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    try:\n        client = MongoClient(host=etcd_host, port=etcd_port)\n    except Exception as e:\n        raise ValueError('Cannot connect to etcd_host: \"%s\"' % etcd_host)\n\n\ttry:\n        db = client.get_database(user)\n        db.authenticate(user, etcd_host, etcd_port)\n        client.admin.authenticate(user, etcd_host, etcd_port)\n\n    except Exception as e:\n        raise ValueError('Cannot log in as user \"%s\"' % user)\n\n    else:\t    \n        return {key"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    # We allow either of the previous two \"fold\" entries in the\n    # previous version of UTC and naive time, even if the exact datetime\n    # is in the previous day.\n    if not self._normalize:\n        return dt\n    else:\n        return dt.replace(tzinfo=self)\n\n  #\n  # We do not need to do any conversion to a naive datetime.\n  #\n  def __sub__(self, other):\n    assert isinstance(other, tzlocal)\n    if not other or not self:\n      return tzlocal\n\n    args = self._parse_delta(other)\n    dt = tzlocal.get"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t    \n\tif dt.tzinfo is None:\n\t\treturn None\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt\n\treturn dt + timedelta(0, 0, dt.tzinfo)\n\ndef _fold_fromutc(dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nThe time between dt and the given 'fold' state can be calculated\nusing the fold state.\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object."}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\t# We'll find this timezone (or one of its subclasses)\n\t# first, then our offset from UTC and the next.\n\t(offset_from_utc, offset_next) = (self.localize(dt) for i in range(7))\n\toffset = max(0, offset_from_utc - offset_next)\n\t\t\t\n\ttz = self._create(offset, dt)\n\ttz.utcoffset(dt)\n\treturn tz\n\t\t\n\t# At this point forward, consider the fold-checking.\n\tstart = tz is not None\n\t\n\tends_after ="}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    naive_tz_dt = dt\n    tz = tzinfo(naive_tz_dt.tzinfo)\n\n    same_timezone = self.combine(self.tz, tz) == tz\n\n    # If we're using a fuzzy-based pattern the datetime is naive\n    if (self.fuzzy and same_timezone):\n        return naive_tz_dt.replace(tzinfo=None)\n    else:\n        return naive_tz_dt\n\ndef _normalize_offset(offset, dt):\n    \"\"\"\n    Convert an offset (a datetime.timedelta) to a normalized\n    offset (a pytz Time, a datetime.datetime object,"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n     pass\n\"\"\"SCons.Tool.dvipdf\n\nTool-specific initialization for dvipdf.\n\nThere normally shouldn't be any need to import this module directly.\nIt will usually be imported through the generic SCons.Tool.Tool()\nselection method.\n\n\"\"\"\n\n#\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\t# We have an unambiguous TZInfo object. This doesn't make much\n\t\t# sense with dates, the exact start of UTC is in effect, so we\n\t\t# just use the same timezone where dt.replace(tzinfo=tzutc()) would\n\t\t# result in an OverflowError.\n\t\treturn None\n\n\tself._tzobj = dt\n\n\treturn self\n\n def _check_time_conflict(self, dt, other):\n\t\tif not self._conflict(dt, other):\n\t\t\treturn False\n\n\t\tif dt.replace(tzinfo=self._tzobj) >= other:\n\t\t\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t # We need to do naive arithmetic on the datetime object, as we're\n\t\t# interested in the correct time offset for the ambiguity.\n\t # This won't work for mixed units.\n\t return datetime.date.fromordinal(dt.toordinal())\n\n\nclass Timezone:\n\t\"\"\"\n\tRepresents a timezone. The internal representation of an informal\n\trepresentation of a timezone.\n\t\"\"\"\n\t@staticmethod\n\tdef parse(datestring, time=None):\t\t\t# pragma: no cover\n\t\t'''Return the :class:`TimeZone` of a string date.\n\t\tThis"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n    # At this point, we use the following rules:\n\n    #   Is there a time of e.g. Europe/Prague\n    #   Is there a time of e.g. UTC\n    #   Is there an epoch of type Timestamp\n    #   Is there a certain timezone?\n    #   Is there an amount of time left before Eastern/Cacah.\n    #   Is there a certain timezone?\n    #   Is there a certain timezone?\n    #   Is there a \"fold\" state?\n    #   Is there a \"fold\" timestamp?\n    # A similar pattern is used so that"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    localtz = _LocalTimezone()\n    now = datetime.datetime.now(localtz)\n    delta_seconds = timedelta(minutes = 1)\n    delta = now - delta_seconds\n    # We calculate and return the delta that contains the local\n    # time in UTC. That is the same for our purposes. Time deltas\n    # are considered as if they are local to a transition period, so a\n    # transition period starts at midnight UTC.\n    return delta.replace(tzinfo=None)\n#\n# Copyright (c) SAS Institute Inc.\n#\n# Licensed under the Apache License, Version"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\traise NotImplementedError()\n\n\nclass tzical(datetime.tzinfo):\n\t\"\"\"\n\tDatetime class for pytz\n\t(though it's needed for compatibility)\n\t\"\"\"\n\t# TODO:\n\t# - more tests\n\t# - more tests should be added for every single timezone.\n\tZERO = datetime.timedelta(0)\n\tHOUR = datetime.timedelta(hours=1)\n\n\n\tdef dst(self, dt):\n\t\tdst = datetime.datetime.combine(dt.astimezone(), self.ZERO)\n\t\tdst = dst.replace(tzinfo=None) # not used?\n\t"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\tyear, month, day, hours, minutes, seconds, microseconds = \\\n\t\t\ttimestamp.fromtimestamp(timestamp_int()), \\\n\t\t\ttimestamp.fromtimestamp(timestamp_int(self.year))\n\t\n\t\t# We need to do this ourselves\n\t\treturn relativedelta(weekday=dateutil.relativedelta.SU(+weekday))\n\ndef relative_weekday(date: DateObject, *args, **kwargs):\n\t\"\"\"\n\tReturn a RelativeWeekday object from a :class:`date` object.\n\t:param date: Either a :class:`date` object or a string representing a\n\t\tnumber of minutes"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\t# See: http://www.dateutil.org/docs/month.html:strftime-behavior\n\t\t# The documentation says that a number may not be appropriate\n\t\tyears = (self.years_start_yr, self.years_end_yr)\n\t\tmonths = (self.months_start_mr, self.months_end_mr)\n\t\tdays = self.days_of_week * 60 * 60 + self.days_of_month * 100 + \\\n\t\t\tdays\n\t\tmonths_rounded = abs(self.days_of_week - self.days_of"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n# ______________________________________________________________________________\n\n\ndef now(*args):\n\t'''\n\tReturn a timezone-aware :class:`datetime.datetime` object or a\n:class:`datetime.date` object (which will accept timezones and\ndates without timezones). This function can be used in two ways.\n\n>>> now(datetime.datetime(2009, 10, 7, 4, 15, 0))\ndatetime.datetime(2009, 10, 7, 4, 15, 0)\n>>> now(time(10, 1, 1).replace(tzinfo=local_tz))\ndatetime.datetime("}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ndef iso_relativedelta(*args,**kwargs):\n    raise NotImplementedError(\"iso_relativedelta class not defined\")\n\n\nclass tz(object):\n    def __init__(self,tztuple=None,default=None):\n        if tztuple == None:\n            tztuple = default\n        if isinstance(tztuple,str):\n            self.name = tztuple     # for example, \"GMT\"\n\n        elif len(tztuple) == 1:\n            self.name = tztuple     # for example, \"EST\"\n            self.tzval = None\n\n        elif len(tztuple) == 2:\n            self.name = tz"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\nclass relativedelta(object):\n\t\"\"\"\n\tA class that represents a relativedelta implementation.\n\n\tIt can be used as a `relativedelta`_ class method for relativedelta\n\tinstances.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tdateutil.__init__(self, *args)\n\t\tself._special_values = self._special_values\n\n\t\t# These values must be cached to avoid the __getattribute__ dance. Using them\n\t\t# results in deadlocking, which would be too expensive.\n\t\tself._value_cache = {}\t\t\n\t\tself._dirty"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ttry :\n\t\t\treturn relativedelta(self.years, self.months, self.days, self.hours, self.minutes, self.seconds, self.microseconds)\n\t\texcept Exception as error:\n\t\t\traise e\n\n\t\tif isinstance(self.__original_value, datetime.datetime) :\n\t\t\treturn self.__original_value.date()\n\t\telif isinstance(self.__original_value, datetime.date) :\n\t\t\treturn self.__original_value\n\t\telse :\n\t\t\treturn relativedelta(self.__original_value, self.__original_value.start(), self.__original_value.end"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    @staticmethod\n    def days(delta):\t\treturn delta.days\n\n\t@staticmethod\n\tdef hours(delta):\t\treturn delta.seconds/3600+delta.days * 24\n\n\t@staticmethod\n\tdef minutes(delta):\treturn delta.seconds/60-delta.days * 24\n\n\t@staticmethod\n\tdef microseconds(delta):\t\treturn delta.microseconds\n\n\t@staticmethod\n\tdef weekdayname(weekday, width):\treturn WEEKDAYS[weekday] * DAYNAMES[width]\n\n\t@staticmethod\n\tdef weekdayname_short(weekday, width):\t"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    __doc__ = relativedelta.__doc__ + \"\"\"\nThe Absolute :class:`dateutil.relativedelta` supports weeks\n14 days, weeks seconds and milliseconds:\n\n    >>> relativedelta(days=14).relative().normalize().as_weeks_number()\n    '14 weeks, 26 weeks, 27 weeks, 12 months, 30 days'\n    >>> relativedelta(days=15, weeks=4).relative().as_weeks_number()\n    '15 days, 13 weeks, 12 months, 20 years, 3 days, 3 hours, 4 minutes'\n    >>> relativedelta(days=15, weeks=4).relative().as_weeks"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn self.__class__(*[getattr(self, attr) for attr in self._attributes])\n\n\ntry:\n    set\nexcept NameError:\n    # Python 2.3\n    from sets import Set as set\n\n\nclass Set(set):\n    \"\"\" A set is a finite, collection of unique elements. It has a high\n    overhead and a low overhead that it can be convenient to have a\n    significant amount of data for it's items only (e.g. timestamps).\n    Note that it has a high probability of overloading or truncation or\n    zero data, so setting a high overhead of 0 won't hide very"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ttry:\n\t\t\tx = as_int(self)\n\t\texcept (ValueError, TypeError):\n\t\t\tx = NotImplemented\n\n\t\treturn relativedelta(x=x)\n\n\t@classmethod\n\tdef current_date(cls, naive=False, dow=None):\n\t\t\"\"\"\n\t\tReturn the current date as a :class:`date` object, representing the\n\t\tcurrent time this :class:`DateTime` instance represents.\n\n\t\t:param naive:\n\t\t\tWhen the calculated date is naive, then the date returned will be\n\t\t\tnaive\n\t\t:return:\n\t\t\tThe current date as a :class"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t return namefunc.replace(u'\\u0300', '\\u203f').decode('UTF-8')\n\n################################################################################    \n\n_RE_EOL = re.compile(r'\\n|\\r|(\\r\\n|\\n)')\n\n################################################################################    \n\nclass TimeUtilsMixin (object):\n\n\t@abstractmethod\n\tdef time_to_seconds(self, unixtimestamp = None):\n\t\traise NotImplementedError\n\n\t@abstractmethod\n\tdef sec_to_time(self, timestamp = None):\n\t\treturn time.mktime(timestamp.timetuple())\n\n\tdef unixtime_to_timestamp"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(encoding='utf-8').encode('ascii')\n\n\ndef tzname_from_python3():\n\t\"\"\"\n\tReturn unicode output back to bytes for Python 3. It used to return bytes, but was converted\nto unicode strings  \"\"\"\n\t\treturn namefunc(encoding='latin-1').encode('ascii') \n\n\nif __name__ == \"__main__\":\t\n\t\n\ttest_data = { \"english\" : \"english-inn\", \n\t\t\t\"\u4e2d\u6587\" : \"Tuesday\",\n\t\t\t\"\u4e2d\u6587, \u0433\u043e\u043b\u043e\u0432\u043e\": \"Hi Tue"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn unicode(namefunc())\n\n\ndef _to_int(input, name_func):\n\t\"\"\"\n\tDealing with unicode string conversions based on the number of bytes\n\treturned by namefunc\n\ninput = list of 2 byte integers\nname_func = a function which will be used to extract strings in strings, and\nwill then translate them back to a byte list\n\"\"\"\n\toutput_str = u\"\"\n\tfor i, e in enumerate(input):\n\t\tn = len(e)\n\t\tif n < (2*n):\n\t\t\toutput_str += uchr"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(unichr(0xd800))\n\n# This is only needed for python2 compatibility\nPY3 = sys.version_info[0] >= 3\n\nif PY3:\n\tdef unicode_from_out(out, encoding='utf8', errors='surrogateescape'):\n\t\t\"\"\"\n\t\tIf the string is text or bytes, returns the original string.\n\t\tIf encoding is 'None' (the default), returns bytes unchanged. Otherwise,\n\t\treturns encoded string.\n\t\t\"\"\"\n\t\tif out is None:\n\t\t\tout = \"\"\n\t\tif type(out)"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif sys.version_info[0] < 3:\n\t\t\tnames = namefunc(0)\n\t\telse:\n\t\t\tnames = namefunc(namefunc(0).decode('ascii'))\n\t\treturn str(names, encoding='ascii')\n\nclass Test_Py2_unicode(Test_unicode_file):\n\t\"\"\"Run Unicode routines via PyPNG library\"\"\"\n\tdef setUp(self):\n\t\tTest_unicode_file.setUp(self)\n\t\timport png\n\t\ti = png.Reader(bytes=x.encode('ascii'),\n\t\tdeflate=False, width="}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(ord(namefunc[1]))\n\nunicode_output = partial(unicode_output_encode, encoding)\n\n# map ctypes.WINFUNCTYPE and ctypes.LFUNC_TYPE\ntry:\n\tLPDWORD = POINTER(wintypes.HANDLE)\nexcept TypeError: # requires python 2.7\n\tLPDWORD = c_short\n\nif PY3:\n\tGetCommandLineW = WINFUNCTYPE(BOOL, HWND, LPDWORD)\n\n# Note: requires python 2.7 - but you have to make sure we get unicode\n# messages when using a unicode"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\timport re\n\t\timport sys\n\t\tif not sys.stdout.encoding:\n\t\t\treturn re.sub(b(\"coding[=:]\\n\"), \"\", namefunc(sys.stdout.encoding))\n\t\telse:\n\t\t\treturn re.sub(b(\"coding[=:]\\n\"), \"\", namefunc(sys.stdout, sys.stdout.encoding))\n\n\t@staticmethod\n\tdef add_aliases(code):\n\t\t\"\"\" Add possibly aliases to the code\n\tReturns the code with alias names added (e.g. %0A becomes lower-case)\n\tThis is useful, for"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\n__all__ = [\n\t'codec_helpers',\n\t'get_display_encoding',\n\t'_tounicode',\n\t'normalize_encoding',\n\t'normalize_string',\n\t'slugify',\n]\n\nimport re\nimport sys\n\ndef reformat(pattern, table, fdict=None, ldict=None):\n\t\"\"\"\n\tReformat compiled re object from str() to format strings\n\t\n\tpattern\t\tstring or compiled regular expression\n\ttable\t\tlist, tuple or string holding sequence of 2 items or strings\n\tfdict\t\ta mapping"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif isinstance(namefunc, unicode):\n\t\t\tnamefunc = namefunc.encode()\n\t\ttry:\n\t\t\treturn namefunc.decode()\n\t\texcept AttributeError:\n\t\t\treturn unicode(namefunc, 'utf_8')\n\tdef str_decode(self):\n\t\tsval = self.unicode_as_ascii_string()\n\t\treturn self.unicode_from_native(sval)\n\t\t\n\tdef unicode_as_ascii_string(self):\n\t\tsval = self.unicode_from_native(u'utf_8')\n\t\treturn self.str_decode()."}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tresult = \"\"\n\t\tpos = None\n\t\t# replace all % escapes with their single-character equivalent\n\t\t# (is there no equivalent conversion (because there are no \n\t\t# backslashes in the string) or % escaped)\n\n\t\t# get rid of any % escapes, and replace them with the real ones:\n\t\t\n\t\treplacements = [\"\\\\\", \"/\"]\n\t\t# map escaped escapes to slashes to prevent problems with backslash\n\t\t# substitution (this is why we keep them in double-quotes!)\n\t\tfor c in result :\n\t\t\tc = re.sub(c, '\\\\' + c, re."}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversions = {}\n\t\twith open(os.path.join(get_base_path(), 'dist','version.txt')) as file:\n\t\t\tfor line in file:\n\t\t\t\tname, ver = line.split(None, 1)\n\t\t\t\tif 'enterprise' in ver.lower():\n\t\t\t\t\t# e.g. nginx 3.6.7\n\t\t\t\t\tversions[name] = ver.split(' ', 1)[1].replace('.', '')\n\t\t\treturn versions\n\n\treturn get_versions()\n\ndef get_base_version(fpath):\n\t\"\"\"\n\t\tGet base version from"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn LATEST_VERSION\n\n\nclass TestError(Exception):\n\t\"\"\"\n\t\tBase class from which all other exceptions derive.\n\t\"\"\"\n\t#: The cause of the error. This is used once if an exception has\n\t#: no relevant message at the application level.\n\tcause = None\n\n\nclass InvalidData(Error):\n\t\"\"\"\n\t\tRaised when data in the data are invalid.\n\t\"\"\"\n\n\tmessage = None\n\tcode = None\n\tstatus_code = 200\n\tbody_class = 'NotEnoughData'\n\tresponse_class = 'MalformedResponse"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversion_file ='version_info.txt'\n\t\tversion = '0.1'\tif os.path.exists(version_file):\n\t\t\twith open(version_file, 'rb') as f:\n\t\t\t\ts = f.readline().strip()\n\t\t\t\tif s == '':\n\t\t\t\t\tversion = '0.0.1'\n\n\t\t\tif not s:\n\t\t\t\ttry:\n\t\t\t\t\tf = open(version_file, 'rb')\n\t\t\t\texcept Exception:\n\t\t\t\t\treturn version\n\n\t\t\ts = f.read().strip()\n\t\t\tf.close()\n\t\treturn version\n\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tif not os.path.exists(VERSION_FILE):\n\t\t\treturn None\n\t\ttry:\n\t\t\treturn read_version_from_file(VERSION_FILE)\n\t\texcept:\n\t\t\treturn None\n\t\treturn None\n\n#\n#  Copyright (c) 2013 Daniel Fokkeke (daniel@freokkeke.de)\n#\n#  This file is part of HaloPoin.\n#\n#  HaloPoin is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tpass\t\n\ndef check_prereqs():\n\t\"\"\"\n\tCheck for required prerequisites\n\t(python >= 2.6.28)\n\t\"\"\"\n\tpass\t\n\ndef check_license():\n\t\"\"\"\n\tCheck that the LICENSE file exists. If not, the default\n\tbehaviour for the extension is checked.\n\t\"\"\"\n\tpass\t\n\ndef check_gls():\n\t\"\"\"\n\tCheck the library gls\n\t(python >= 2.6.28)\n\t\"\"\"\n\tpass\t\n\ndef check_m4(gls="}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn pkg_resources.get_distribution('django-pgjson').version\n\tif pkg_resources.parse_version(get_versions)==pkg_resources.parse_version('1.2'):\n\t\treturn 'Django-1.2'\n\telif pkg_resources.parse_version(get_versions)==pkg_resources.parse_version('1.1.0'):\n\t\treturn 'Django-1.1.0'\n\telif pkg_resources.parse_version(get_versions)==pkg_resources.parse_version('2.0.2'):\n\t\treturn 'Django"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\timport os, sys, re, subprocess\n\t\tp = subprocess.Popen([\"gcc\", \"-print-multiarch\"], stdout=subprocess.PIPE)\n\t\tout, err = p.communicate()\n\t\tif err:\n\t\t\tprint(err)\n\t\t\tprint(\"gcc -print-multiarch: Unable to get version of gcc.\")\n\t\t\treturn False\n\t\tm = re.match(r'([0-9]+)\\.([0-9]+)\\.([0-9]+)\\.([0-9]+)', out)\n\t\tif m:\n\t\t\treturn tuple(map(int, m.groups()))\n\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tfile_str = local('which %s > /dev/null' % file_path).strip().lower()\n\n\t\tif file_str and file_str == '/dev/null':\n\t\t\tversion = None  # this is the same version returned.\n\t\telse:\n\t\t\ttry:\n\t\t\t\tversion_obj = os.listdir('/dev/null')\n\t\t\texcept OSError:\n\t\t\t\tversion_obj = None\n\n\t\t\tfile_obj = 0\n\t\t\ttry:\n\t\t\t\tversion_obj = len(version_obj)\n\t\t\texcept OSError:\n\t\t\t\tversion_obj = 0\n\t\t\tversion ="}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tif no_version: return None\n\t\tif not verfile: return None\n\t\twith io_open(verfile,\"rt\", encoding='utf-8') as f:\n\t\t\tself._write_version(f, version_text); return f.read()\n\n\tdef get_versions_json(self):\n\t\tif len(verfile)==0:\n\t\t\treturn None\n\t\n\t\twith io_open(verfile,\"rt\", encoding='utf-8') as f:\n\t\t\treturn json.load(f)\n\t\n\tdef get_version_texts(self,verfile):\n\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tvers = \"%d.%d\" % sys.version_info[:2]\n\t\tif len(vers) == 3:\n\t\t\tvers = vers[:3]\n\t\treturn vers\n\tfinally:\n\t\tsys.exit()\n\nfrom datetime import date as module_date\nclass _LazyInitContext(object):\n\tdef __init__(self, *largs, **kwargs):\n\t\tself.enabled = False\n\t\tself.ignore_dependencies = False\n\t\tself.depends_cache = set()\n\t\tself.trace = False\n\tdef __enter__(self):\n\t\t"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn \"\\n\".join(pieces+[\"\"])\n\nclass BootstrapCompressor(Compressor):\n\tdef __init__(self, **kwargs):\n\t\tself.template = \"\"\"#!{base_indent}\"\"\" if kwargs.get(\"base_indent\") else \"\"\"#!/bin/sh\n{env_indent}\n\"\"\".format(env_indent=env_indent)\n\n\t\tCompressor.__init__(self, **kwargs)\n\n\tdef write(self, lines, append=False):\n\t\tlines = map(lambda x: type(x), lines)\n\t\treturn write_with_newline("}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\trendered_version = []\n\n\t\t# render the label for the current version\n\t\trendered_label = \"%s%s\" % (style['vertical_tag'], style['horizontal_tag'])\n\n\t\tif pieces['type'] == 2 and style['vertical_tag'] in (\"baseline\",\"super\",\"both\"):\n\t\t\trendered_label += \"* %s\" % style['title']\n\n\t\tif pieces['type'] == 2 and style['vertical_tag'] == \"super\":\n\t\t\trendered_label += \" super script\"\n\n\t\tif pieces['type'] == 3 and style['vertical_tag'] in (\""}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tparsed = list(pieces)\n\t\tif len(pieces) > 1 and pieces[-1] in ('alpha', 'beta', 'candidate', 'final'):\n\t\t\tnext_alpha = pieces[-1]\n\t\t\tif get_int_param(style.precision) < 3:\n\t\t\t\tif (len(style.lines) > 1) or pieces[-2].startswith('alpha'):\n\t\t\t\t\tnext_alpha += float(style.lines[-1]) / 100.0\n\t\t\t\telse:\n\t\t\t\t\taddstr(next_alpha, 1.00, style.lines[-1])\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\traise NotImplementedError\n\n\nclass HTMLConverter(Component):\n\t\"\"\"\n\tConvert all supported HTML markup into its equivalent format.\n\t\"\"\"\n\tdef extract_toc(self, soup_class):\n\t\ttoc = super(HTMLConverter, self).extract_toc(soup_class)\n\t\treturn toc.replace(r'\\xa0','')\n\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nfrom django.conf import settings\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# The render() method is called recursively. This allows multiple\n\t\t# renderable rendering pass over the rendered pieces.\n\t\t# As a side effect, if the user has not overridden render()\n\t\t# on the current page, the result needs to be built twice.\n\t\treturn pieces.render(style)\t\t\n\n\nclass PageRenderer(object):\n\t\"\"\"\n\tThe render method of a single page. For rendering in the\n\tcontext of a single page, you must use a page template that\n\treturns a compiled template. When called to render a page\n\twhich is in turn passed"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn \"\".join(map(style.color,\n\t\tpieces[:pieces.index(\"#\")])\n\t\t\tfor pieces in styles[-1][\"pieces\"])\n\nclass UndoRenderingState(object):\n\tdef __init__(self):\n\t\tself.lines = []\n\t\tself.head_cursor = (0, 0)\t# Cursor's start/end cursor\n\t\tself.head_line = 0\t\t# Current line. It is not always on header.\n\t\tself.column = -1\t\t# Column is visible\n\t\tself.content = []\t\t\t# Contents.\n\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# Parse style.\n\t\tformatter = style.get(\"formatter\", \"\")\n\t\tif not formatter:\n\t\t\t# No formatter specified, so assume none is present for this output\n\t\t\tformatter = style[\"default\"]\n\t\t# Parse pieces.\n\t\tformatter = self._init_formatters(formatter)\n\t\toutput = []\n\t\tfor piece_type, piece_count in formatter.enum_types.items():\n\t\t\t# Skip `None` elements inside format.\n\t\t\tif piece_count is not None:\n\t\t\t\t# An enumerated type:\n\t\t\t\tif \"enum\" in piece_count:\n\t\t\t\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tcontext = \" \".join(pieces[:len(style)])\n\t\treturn '<style type=\"text/css\">{0}</style>'.format(style).format(\n\t\t\tu' '.join('<{0}>{1}</{0}>'.format(*pair) for pair in pieces))\n\n\n\n\n#!/usr/bin/env python\n\n\"\"\"\nScript to download the GDRL-ICO images and extract the GDGRL data\n\nTo be run\n\n- python scripts/download.py\n\nThe GDGRL data is stored in a folder called 'GDSR20120100200202"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn pieces\n\n\tpieces = render_pieces(pieces)\n\trendered = []\n\n\tif not pieces:\n\t\traise Exception(pieces)\n\telif style == QUARTOOLS:\n\t\trender_pieces_quick(pieces, style, pieces[0])\n\t\treturn pieces\n\n\tif pieces:\t\n\t\trender_pieces(pieces, style + QUARTOOLS[style])\n\t\tchunks = []\n\n\t\tfor i in pieces:\n\t\t\tchunks.append((i, i))\n\n\telif style == LATEST:\n\t\trender_pieces_latest(pieces, style"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tfrom pygments import lexers\n\t\t\n\t\toutput = []\n\t\tfor index, piece in enumerate(pieces):\n\t\t\tstyle_class = lexers.guess_lexer_for_style(pieces[index])\n\t\t\ttry:\n\t\t\t\tparser = style_class()\n\t\t\t\tformatter = parser.formatter.get_style_by_name(\"bold\")\n\t\t\t\toutput.append(formatter.get_style_defs(parser.get_style_defs()))\n\t\t\texcept Exception as e:\n\t\t\t\toutput.append(str(e))\n\t\t\n\t\treturn '\\n'.join(output)\n\n\n\tdef render_version"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces)==1:\n\t\t\treturn pieces[0]\n\t\telse:\n\t\t\treturn os.path.join(*pieces)\n\treturn _concat(pieces)\t\n\ndef reroot_path(path):\n\t'''reroot a file path to /dir'''\n\tif path.startswith('/'):           # absolute path\n\t\tpos = path.find('/')   # directory-relative path\n\t\tpath = path[pos+1:]      # drive\n\ti = _splitpath(path)\n\tif i[0] == '/':            # root\n\t\tpath = path[1"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) == 1:\n\t\t\treturn 0\n\t\treturn pieces[0] + \".\" + pieces[1]\n\n#--- CONVERT_CHARGE -------------------------------------------------------------#\n\ndef convexa(word):\n\t\"\"\"\n\tTurns a word into a convexa cell, and if it is one of the \"happy\", presumed\n\tconvexaes, will put it in a cell.\n\t\"\"\"\n\treturn \"happy\" if word in _VERB_STOPS else word\n\n#--- CONVERT_WORD --------------------------------------------------------------#\n\ndef con"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces[0] in \"0123456789\":\n\t\t\treturn \".\" + pieces.pop(0)\n\t\telse:\n\t\t\treturn \".\" + pieces.pop(0)\n\ndef build_list(item, key):\n\t\"\"\"Make a list out of a dictionary for easier indexing and searching.\n\n\t:param item: The dictionary to index.\n\t:param key: Dictionary key\n\t:returns: List of values as string.\n\t\"\"\"\n\tif item is None:\n\t\tlist_string = \"\"\n\telse:\n\t\tlist_string = build_list1(item[key"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) == 1:\n\t\t\treturn pieces[0]\n\t\tif pieces[0].isdigit():\n\t\t\t# a + b  -> a\n\t\t\tpieces[0] = parts[0] + pieces[0]\n\t\telse:\n\t\t\t# a + b -> a, a - b\n\t\t\tpieces[0] = str(int(pieces[0]) + str(pieces[1]))\n\t\treturn pieces\t\t\n\t\t\t\ndef plus_or_dash(pieces):\t\"\"\"\n\tDivide the argument into pieces, and if needed pad the result into\n\tthe rightmost edge"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif not pieces: return ''\n\t\tif len(pieces) == 1: return pieces[0]\n\t\treturn pieces[:-1] +'' + pieces[-1]\n\treturn ''\n\ndef remove_duplicate_words(words):\n\t\"\"\"\n\tFor a list of words, delete those that have the same length.\n\tWe can't use words[0] as we need to remove words[1:] because this\n\tfunction only contains one word. It seems to work for word_len=2.\n\t\"\"\"\n\tdups = []\n\tfor i in range(len"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t #print('plus_or_dot(pieces: {}, 1)', pieces)\n\t if not len(pieces): return ''\n\t elif 1 == len(pieces):\n\t\t return pieces[0]\n\t\telse:\n\t\t tmp = pieces[:]\n\t\t tmp[0] += if_match(pieces)\n\t\t return tmp\n\n\t#if we have a slash, then we might want to split \n\t#into multiple sections, but we shouldn't touch these out yet but we\n\t#can't use that\n\tif pieces[0][0] == '/':\n\t\treturn ''\n\n\t#if we are at the end of a"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif not pieces:\n\t\t\treturn None\t\t\n\t\tfirst = pieces[0]\n\t\tfor first_seg in pieces[1:]:\n\t\t\tif not first:\n\t\t\t\treturn first_seg\n\t\t\tfirst = first_seg.strip()\n\t\t\tif first[-1] == '.':\n\t\t\t\tfirst = first[:-1]\n\t\treturn first\t\n\t\n\tdef set_version(self, key, value):\n\t\t\"\"\"\n\t\tSet/change the Key's version, so we know what to set.\n\t\t\"\"\"\n\t\tself.key = key[:key.find('.')]\n\t\tif self.key"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces in _plus_or_dot_cache:\n\t\t\treturn _plus_or_dot_cache[pieces]\n\t\tpiece = _safe_by_regex(pieces)\n\t\tif piece is None:\n\t\t\traise RuntimeError(\n\t\t\t\t'\"{cls.pattern}\" does not appear to be a valid '\n\t\t\t\t'regular expression: {pieces!r}'.format(\n\t\t\t\t\tcls=self.__class__.__name__, words=pieces))\n\t\tif pieces in _dot_cache:\n\t\t\treturn _dot_cache[pieces]\n\t\tpiece = _safe_by_regex(pieces)"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tindex = 0\n\t\tfor piece in pieces:\n\t\t\tif pieces[index]!= \".\":\n\t\t\t\tpieces[index] += \"+\"\n\t\t\tpieces.insert(index, piece)\n\t\t\tindex += 1\n\t\treturn \" \".join(pieces)\n\n\t@staticmethod\n\tdef build_list_of_tuples(l, l_index, l_len):\n\t\tret = \"\"\n\t\tfor item in l:\n\t\t\tif \"\\n\" in item:\n\t\t\t\tret += \"\\t\" + item\n\t\t\telse:\n\t\t\t\tret += \", \"\n\t\tif l_index + l_"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) == 1:\n\t\t\treturn pieces[0]\n\t\treturn join_chars_plus_or_dot(pieces)\n\treturn plus_or_dot(pieces)\n\n#\n# Classes for finding symbols\n#\n\n\nclass Symbol:\n\n\t#\n\t# Get the base (sub)symbol value of token\n\t#\n\t# @param name       The name to parse\n\t# @param token     The token to be parsed for symbol\n\t#\n\tdef get_name(self, token):\n\t\tif token == \"=\":\n\t\t\treturn self"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t# run_command() can take a list or str with args like\n\t\t# ['ls', '-la']\n\t\t# for now, shlex is probably better\n\t\tif isinstance(commands, list):\n\t\t\tcmds =''.join(commands)\n\n\t\t# in the case we run with a non-literal list we have to quoted it.\n\t\tif (isinstance(args, str) or isinstance(args, basestring)):\n\t\t\targs = shlex.split(args)\n\n\t\tverbose=False\n\t\tif hide_stderr:\n\t\t\tstdout = None\n\t\telse:\n\t\t\tstdout = subprocess.PIPE"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tassert isinstance(commands, (list, tuple)), \"Internal error!\"\n\t\trunner = Runner(os.getcwd(), dirname(cwd or os.path.abspath(sys.argv[0])), commands, args, cwd, verbose, hide_stderr)\n\t\trunner.run()\n\n\t@staticmethod\n\tdef _get_tool_name(tool):\n\t\t\"Return the fully qualified tool name for a given tool (if any).\"\n\t\ttool_name = tool if isinstance(tool, str) else os.path.basename(tool) if isinstance(tool, unicode) else tool[6:]"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif not quiet: print(args)\n\n\tcwd = cwd if cwd is not None else os.getcwd()\n\tcmd_str =''.join(cmd_list)\n\n\t#print 'Running: cmd = %r' % cmd_str\n\tstatus = subprocess.call(cmd_str, shell=True)\n\n\tif status!= 0:\n\t\tsys.stderr.write(\"\\nError trying to run command '%s' (%d): %s\\n\" %\n\t\t\t(cmd_str, status,''.join(cmd_list)))\n\t\treturn status  # Return False"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\treturncode, output = utils.run_local_command(commands, cwd, cmd_input=args, env=env, verbose=False)\n\t\tif returncode:\n\t\t\traise UtilsExitException(returncode, output)\n\t\treturn output\n\n\tdef update_job_stats(self):\n\t\tif self.job_stats is None:\n\t\t\tif not self.params.get(\"no_job_stats\", False):\n\t\t\t\traise UtilsError(\"No job stats available (use --help for more info). Please use --list-stats command.\")\n\t\t\telif os.path.exists(self."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = None\n\t\tfor c in commands:\n\t\t\ttry:\n\t\t\t\tlogging.debug(\"- Executing: %s\" % c)\n\t\t\t\tp = subprocess.Popen(c, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env, shell=True)\n\t\t\texcept Exception, e:\n\t\t\t\tlogging.error(\"Exception running command %s: %s\" % (c, e))\n\t\t\t\treturn\n\n\t\tif not isinstance(p, str) or not isinstance(p, unicode):\n\t\t\tlogging.warn(\"Expected str or unicode, found None, instead"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif isinstance(commands, str): # is a string (could be a command\n            commands = split_args(commands)\n        logging.debug(\"Running command: %r\", args) \n\t\n        proc = Popen(args, cwd=cwd, env=env, stdin=stdin, stdout=stdout, stderr=stderr)\n        stdoutdata, stderrdata = proc.communicate(commands)\n        returncode = proc.returncode\n        if returncode!= 0 and verbose: \n\t    print stdoutdata.replace('\\n', '')\n\t    print stderrdata, \"\\n\\nCommand output:\\n\", stdoutdata, \"\\n\\n\"\n\t   "}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t# We use unicode_literals here because that is what distutils does.\n\t\tcmd = \" \".join(map(pipes.quote, commands))\n\t\tif verbose: print(cmd)\n\t\n\t\tif cwd!= None:\n\t\t\ttry:\n\t\t\t\tos.chdir(cwd)\n\t\t\texcept OSError:\n\t\t\t\traise Exception(\"Working directory not found: {}\".format(cwd))\n\t\tc = _check_output(args, cmd, cwd=cwd)\n\t\tif c.returncode!= 0:\n\t\t\traise Exception(\"Command: {} returned non-zero exit code {}\".format(cmd, c.returncode))"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tuse_shell_wrap = False\n\n\t\t# We use this import mechanism for Python 2.7 on Python >= 2.4.\n\t\ttry:\n\t\t\tif not use_shell_wrap: from subprocess import Popen, PIPE, check_output\n\t\t\telse:\n\t\t\t\targs = [sys.executable] + args\n\n\t\t\ttry:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(' '.join(args))\n\t\t\t\tif env: env = dict(os.environ, **env)\n\t\t\t\telse: env = None\n\t\t\t\t\n\t\t\t\tif hide_stderr:\n\t\t\t\t\tcheck_output(args, cwd=cwd)"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif debug:\n\t\t\tsys.stderr.write(colors.header('COMMAND_HEADER', 'running'))\n\t\tif verbose:\n\t\t\t\t\tsys.stderr.write(colors.header('COMMAND_HEADER', 'running: '))\n\t\tfor command in commands:\n\t\t\tcommand = command.rstrip()\n\t\t\ttry:\n\t\t\t\tfunc = globals().get(command)\n\t\t\texcept Exception:\n\t\t\t\tif hide_stderr:\n\t\t\t\t\tsys.stderr.write(colors.header('ERROR_UNDEFINED_COMMAND', 'command:'+ command)) \n\t\t\t\t\tsys.stderr.write(colors.header('"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = None\n\t\tcommands = split_args(commands)\n\t\tif not commands: return\n\t\tif verbose:\n\t\t\ttry:\n\t\t\t\tp = subprocess.Popen(args, cwd=cwd, env=env)\n\t\t\t\tmesg(\"Executing `%s'...\" %''.join(commands))\n\t\t\texcept OSError as e:\n\t\t\t\tmesg( \"Failed: `%s' [%s]\" % (e, \" \".join(commands)))\n\t\t\t\treturn\n\t\tfor l in commands:\n\t\t\tif l.startswith(\"-\"):\n\t\t\t\tprint_header()\n\t\t\telif l.startswith(\">>\"):"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\n    from setuptools import setup\n    cfg = get_config()\n    version = cfg.VCS\n    format = cfg.format\n    #format = \"%(name)s (@date:%(date)s) %(author)s @date:end\"\n    date = cfg.date  \n\n    # Do not use split_post_release_version.py for the version group\n    # version.\n    version('0.0.0', 'q',        format='%(tag)s')\n    version('0.0.1', 'q',        format='%(tag)s-%(date)s')\n\n    return cfg\n\n\nclass NotThis"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"None\"\n\t\tconfig.versionfile_source = \"src/iris-core/_version.py\"\n\t\tif os.path.exists(\".git\"):\n\t\t\tconfig.git_source = \"src/iris-core/_version.py\"\n\t\ttry:\n\t\t\tconfig.tag_prefix = \".\"\n\t\texcept AttributeError:\n\t\t\tpass\n\t\treturn config\n\n\nclass NotThis"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n\t\t# _version.py\n        cfg = VersioneerConfig()\n        cfg.VCS = \"git\"\n        cfg.style = \"%(STYLE)s\"\n        cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n        cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n        cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n        cfg.verbose = False\n        return cfg\n\n    def setup_package_data(self, filenames):\n        \"\"\"Setup routine for package_data\"\"\"\n        assert isinstance(filenames"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"\"\n\t\tconfig.versionfile_source = \"versioneer.py\"\n\t\tmeta = \"git\"\n\t\toptions, args = parser.parse_args(sys.argv[1:],\n\t\t                             [\"git\", \"build\", \"--git-dir\"],\n\t\t                             [\"install\", \"versioneer.py\", \"versioneer.py\"])\n\t\treturn config\n\ndef versions_from_parentdir("}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = get_config()\n\t\t# '%%' is hackish: it's the wrong way to get the option\n\t\t# destpath, for example, 'git+git://github.com/pypa/piptools.git'\n\t\tsrc_path = os.path.abspath(os.path.dirname(__file__))\n\t\thave_setup_cfg = (\"setup.cfg\", \" setup.cfg\")\n\t\tf = open(src_path, 'w')\n\t\tf.write((\"version = '%s'\\n\" % version) + \"\\n\")\n\t\tf.write((\""}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"biofuel-\"\n\t\tconfig.versionfile_source = \"biofuel/_version.py\"\n\t\tconfig.verbose = False\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\tpass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tcfg_dir = os.path.join(base_dir, \"versioneer.cfg\")\n\t\twith open(cfg_dir, 'w', encoding='utf-8') as f:\n\t\t\tf.write(CONFIG_FILE_TEMPLATE % globs)\n\t\tconfig.versionfile_build = os.path.join(base_dir, \"versioneer\", \"git_build\")\n\t\tconfig.tag_prefix = \"\"\n\t\tif setup_cfg.versionfile_source not in _CONFIG_FILE_SOURCES:\n\t\t\t"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tfrom._version import get_versions\n\t\t# these strings will be replaced by git during git-archive.\n\t\ttag_prefix = \"\"\n\t\tparentdir_prefix = \"quattorcexternalembeddedoc\"\n\t\tversionfile_source = \"quattorcexternalembeddedoc/_version.py\"\n\t\ttry:\n\t\t\t# references to previously-defined.version file\n            except NameError:\n\t\t\t\tpass\n\t\t\tfinally:\n\t\t\t\ttag_prefix = \"quattorcexternalembeddedoc_\"\n\t\t\t\tversionfile_source = op.abspath(versionfile_source"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tcfg = VersioneerConfig()\n\t\tprepare_parser(cfg.version)\n\t\ttry:\n\t\t\treturn cfg.parse_args()\n\t\texcept ParseError:\n\t\t\treturn cfg\n\n\tdef get_config_from_root(root):\n\t\t\"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n\t\troot = os.path.abspath(root)\n\t\tsetup_cfg = os.path.join(root, \"setup.cfg\")\n\t\tif not os.path.exists(setup_cfg):\n\t\t\tprint(\"Could not find setup.cfg at %s\" %"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"pastaxtransform-\"\n\t\tconfig.versionfile_source = \"pastaxtransform/_version.py\"\n\t\tfrom pastaxtransform._version import get_versions\n\t\tget_versions()['version'] = get_versions()['version']\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrap(f):\n\t\t\tdef wrapper(vcs, *args, **kwargs):\n\t\t\t\tVCS = VCS.bind_vcs(vcs, method, *args, **kwargs)\n\t\t\t\t# Only the VCS will have had a name\n\t\t\t\t# as this is the handler, so we don't add\n\t\t\t\t# the same handler more than once!\n\t\t\t\tif VCS is None:\n\t\t\t\t\treturn 0\n\n\t\t\t\t# Add the requested VCS to _already_ traversed\n\t\t\t\t# Otherwise, we'll accumulate the results into one list\n\t\t\t\t# until we find one that has a name already\n\t\t\t\t# encountered and"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t                       # if vcs is a string, then the vcs object\n\tdef wrap(f):                          # function or method\n\treturn decorate(vcs, f)                                   # decorator\n\ndef run_command(commands, cwd=None, verbose=False, hide_stderr=False):   # decorator\t\"\"\"\n\tRun commands. Return stdout if command was successfull, otherwise return stderr if no errors or\n\tthe return code if a command encountered error.\n\tIf verbose is True, additional info for the VCS command are displayed.\n\tIf hide_stderr is True, also output to stderr and print to stdout"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrapped(self, *args, **kwargs):\n\t\t\ttry:\n\t\t\t\tvcs.handlers[method](*args, **kwargs)\n\t\t\texcept TypeError:\n\t\t\t\t# if self.debug: print(\"VCS HANDLER: %s(*%r, **%r)\" % (method, args, kwargs))\n\t\t\t\tpass # decorator does nothing\n\t\treturn wrapped\n\n\treturn register_vcs_handler\n\n\n# vim: fileencoding=utf-8 et sw=4 ts=4 tw=80 :\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef create_decorator(checkobj):\n\t\t\tvcs.register_vcs_handler(method, checkobj)\n\t\t\treturn method\n\t\treturn create_decorator\n\treturn register_vcs_handler\n\ndef get_cached_vcs(vcs_dict):\n\t\"\"\"\n\tCache the current VCS and its data from the config.pickle\n\tCall on init.py whenever the file that was loaded in memory.\n\tTries caches in the config, caching its internal data if the config\n\tchanges.\n\n\t@param vcs_dict: The dict of all files.\n\t@type vcs_"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(fn):\n\t\t\tvcs.register_vcs_handler(vcs, method)\n\t\t\tvcs.add_callback(fn)\n\t\t\treturn fn\n\t\treturn decorate\n\n\t@staticmethod\n\tdef create_vcs_instance_(path):\n\t\treturn VCS.singleton.create_vcs_instance_(path)\n\n\t@staticmethod\n\tdef register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\t\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(fn):\n\t\t\tvcs.register_vcs_handler(vcs, method)"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrap(f):\n\t\t\t@wraps(f)\n\t\t\tdef inner(self, *args, **kwargs):\n\t\t\t\twith self._lock:\n\t\t\t\t\t\tvcs_handler_args = (vcs,) + args\n\t\t\t\t\t\tvcs_handler_kwargs = {k: kwargs[k] for k in kwargs if k in kwargs}\n\t\t\t\t\t\tif method not in vcs_handler_args:\n\t\t\t\t\t\t\traise NameError('not supported method: {0}'.format(method))\n\t\t\t\t\t\t\tvcs_handler_args = None\n\n\t\t\t\t\tfunc = getattr(self.handle, method)\n\t\t\t\t\tres = func(*vcs_handler"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(vcs, method)\t# decorator\n\t\treturn decorate\n\n\tdef run_command(self, command_name, *args, **kwargs):\n\t\t\"\"\"\n\t\tRuns the command in the VCS.\n\n\t\t:param str command_name: Name of the particular VCS command to run.\n\t\t:param kwargs: Additional optional keyword arguments to pass to ``subprocess.Popen``.\n\t\t:return: The output of the command in a subprocess.\n\t\t\"\"\"\n\t\tkwargs.setdefault('cwd', self.path)\n\t\tkwargs.setdefault"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(vcs, method)\n\t\t\treturn f\n\t\treturn decorate\n\treturn register_vcs_handler\nimport numpy as np\nimport scipy.ndimage\nfrom tqdm import tqdm\nfrom scipy.optimize import curve_fit\nfrom pyseidon.pyseidon import pyseidon\nfrom pyseidon.bseid.seidel import *\n\n\ndef make_grid_coords(image, ref_shape, target_shape, borders=True):\n    \"\"\"\n    Make a grid"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\t\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n\t\t\tif vcs not in HANDLERS: handlers = HANDLERS[vcs]={}\n\t\t\tif method not in HANDLERS[vcs]: handlers[vcs][method]={}\n\t\t\t HANDLERS[vcs][method][f.im_class]=[f]\n\t\treturn decorate\n\n\treturn register_vcs_handler\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n\t\"\"\"Call the given command(s).\"\"\"\n\tif verbose:\n\t\tprint"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t    if vcs not in vcs_handlers:\n\t\t\t\tvcs_handlers[vcs] = []\n\t\t\tvcs_handlers[vcs].append(method)\n\t\t\treturn f\n\t\treturn decorate\n\n\tif len(sys.argv) == 1:\n\t\tregister_vcs_handler(sys.argv[0], method)\n\telse: \t\tvcs_handler = register_vcs_handler(sys.argv[1], method)\n\t\treturn vcs_handler\n\n\n##############################################################################\n# Public interface\n##############################################################################\n\nlog = logging.getLogger('zen.zenpy.repository')\n\n"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\ndef split_oid(obj):\n    \"\"\"\n    Split OID from string representing OID, first and last '/' separated.\n    If split on both sides, first number will be included, and remaining number will be excluded.\n    E.g.:\n\n        2A1,3,4A2,5\n        2f5,6,7A3,8A4,10A5\n    would return '2f5', '6', '7', '8' respectively.\n\n    Raises ValueError if no splitting was possible.\n    \"\"\"\n    oid, attribs, key, value = None, [], obj\n    tokens ="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# objects are valid - first, check if they are contained in the\n\t\t# hierarchy\n        good_objects = self.tree.filter(\n            oidnum__in=[x.oid for x in self.tree.object_set])\n        if len(good_objects) == 0: # no obj's were found\n            if validate_objects:\n                return 0\n                # if we have only one valid obj root object, use the first one\n\t\t\t# and assume it contains the proper object_num\n\t\t\tmax_obj = self.tree.object_set.all()[0].oidnum\n\t\t\tfor x"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n\t\n\tcheck_info = validate_objects if validate_objects else self.object_info\n\tshow_warnings = show_warnings if show_warnings else self.warning_info\n\n\t\t\n\t\t\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects:\n\t\t# check if a name already exist\n\t\tself.object_check(obj)\n\n\n\t\t# check if the object_type is valid for the storage root\n\tif isinstance(obj, StorageObject) or isinstance(obj, StorageRoot) or obj.type!= StorageRootType.name:\n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n        # validate objects (in case we can)\n        if self._validate_objects:\n                self.log.debug(\"Validating objects - only objects checked\")\n\t\n                # determine how many objects will be checked, if available by caller\n                if self._objects_check_digests and self.__used_digests!= 0:\n                        self.__used_digests += 1\n                if validate_objects:\n                        self.__used_objects_check_digests += 1\n\t\tif validate_digests:\n                        self.__generate_digests_for_objects(verbose_output=False)\n\t\tself.log."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n\tif validate_objects:\n\t\tvalid_keys = [ \"path_info\", \"size\", \"type\"]\n\t\tfor k in sorted(self.__dict__.keys()):\n\t\t  if k in valid_keys:\n\t\t\t\tobj = self.__dict__[k]\n\t\t\t\t# Check if path exists.\n\t\t\t\tif not os.path.exists(obj.storage_path):\n                      raise ValueError(\"Storage path (%s) not found\" %obj.storage_path )\n\t\n\t\t\t\t\n\tif show_warnings:\n\t\tif self.get_verbose():\n\t\t\tself.show_warnings()\n\t\telse"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n    def _get_all_names(self):\n\t\"\"\"\n\tReturn list of objects as a string with a space separated list\n\tof names.\n\t\"\"\"\n\t# build list of object names as requested\n\t# useful in case storage does not support objects that\n\t# are not available directly within an object\n\tall_obj_names = set()\n\n\tobjects = self.object = self.container = None\n\tcontainers = self.container_acl = list()\n\n\t# get object names and check content of each acl object\n        # is available\n        for obj_info in objects:"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t    \n\tobjs_checked = 0\n\ttotal_objects = len(self.storage_roots)\n\tif self.storage_roots:\n\t\tfor obj in self.storage_roots:\n\t\t\tif obj.hash_object() in self.bad_objects:\n\t\t\t\tcontinue\n\t\t\tif obj.contains_type(\"META/DATA/DB\"):\n\t\t\t\tcontinue\n\t\t\tif (obj.get_content_type()!= 'application/x-directory'):\n\t\t\t\tcontinue\n\t\t\tobj_stat_info = self._get_stat_info(obj)\n\n\t\t\tif obj_stat_info"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n    for part in self.parts.values():\n        if part.storageobject:\n            # If storageobject is None there are no good objects, so we validate\n            # everything else\n            try:\n                # If we have a StorageObject, a storage object if our container has no\n                # storageobject, otherwise we assume this is a misconfig object\n                if part.storageobject == self and not hasattr(self, \"valid_storle\"):\n                    self.valid_storle = None\n                # If we have a Storage_Object, a string describing what's in it is\n                elif validate_storage_object(self,"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n    if validate_objects:\n\t\n    return (len(self.root) - self.root.count(storage_root_name + suffix) +\n              len(storage_root.objects))\n \n \n \n\ndef storage_root_exists(storage_root_name):\n\t\"\"\"\n\tCheck if the storage root exists.\n\t\n\tArgs:\n\t    storage_root_name - name of the storage root\n\t    \n\tReturns:\n\t    True  - if storage_root was found\n\t\"\"\"\n\t\n\tfound = False\n        try:\n            for i in range(1, 12):\n                if storage_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\t\n\tif self.root_object is None:\n\t\treturn (-1, -1, \"\")\n\n\t# Validate data\n\tcheck_hashes = False\n\tif validate_objects and len(self.root_object.get_children())!= 0:\n\t\tfor child in self.root_object.get_children():\n\t\t\tif child.has_id() and child.has_name() and \"md5\" in child.get_name():\n\t\t\t\tif self.root_object.get_children()[0].get_name() == child.get_name():\n\t\t\t\t\tself."}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.clear_db()\n\t\tself.clear_db()\n\n\tdef clear_db(self):\n\t\t\"\"\"\n\t\tClear out all entries, entries of OCFL storage that have not been used again.\n\t\tTo use the empty database, call the read() method, with no arguments.\n\t\t\"\"\"\n\t\tself.db = {}\n\n\tdef create(self, tablename = None, fields = [], indexes=[], \n\t\t\tconstraints = [], condition = None, \n\t\t\ton_conflict = None, ondelete = None, cache = True):\n\t\t\"\"\"\n\t\tCreate an empty OCFL"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\nclass OCFLStorageRoot(FileOrPath):\n\n\tdef __init__(self):\n\t\tself.root = RootContainer()\n\t\tself.tree = []\n\t\tself.storage_root = root\n\t\tself.read_storage_root()\t\n\t\tself.create_tree()\n\t\t\n\tdef read_text(self):\n\t\twith open(self.storage_root.file_name, 'r') as storage_file:\n\t\t\tfor line in storage_file:\n\t\t\t\tif line[0] == \"#\":\n\t\t\t\t\tcontinue\n\t\t\t\tif line[0] == '+':"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tpass\n\n\tdef mount(self, mount_point):\n\t\t\"\"\"\n\t\tMounts the given mount point.\n\t\t\"\"\"\n\t\tpass\n\n\tdef unmount(self, mount_point):\n\t\t\"\"\"\n\t\tUnmounts the given mount point.\n\t\t\"\"\"\n\t\tpass\n\n\tdef resize_fs(self, mount_point, filesystem_name):\n\t\t\"\"\"\n\t\tResize the given filesystem.\n\t\tIf a filesystem is missing, it is created.\n\t\tIt is assumed, that the filesystem is not part of the filesystem\n\t\t(eg. root on a file system).\n\t\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tlogging.info(\"Initializing OCFI storage root...\")\n\t\tself.data_dir = self.config.get('DEFAULT', 'data_dir')\n\t\troot = self.find_root()\n\t\tif root is not None:\n\t\t\tself.default_template = os.path.join(root, \"templates\", \"base.template\")\n\t\telse:\n\t\t\traise RuntimeError(\"No storage root in configuration file\")\n\n\tdef start(self):\n\t\t\"\"\"\n\t\tStart OCFL storage engine.\n\t\t\"\"\"\n\t\tself.output_filename = \"ocf.data\""}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\t# This is a bit complicated code, but we must create a root\n\t\t# in the correct location without the storage root, otherwise we can't\n\t\t# initialize the storage root under a prefix of anything\n\t\tos.mkdir(self._root_path)\n\n\t\t# Set up the root as the root path and add a trailing slash if\n\t\t# necessary. The root will remain visible at / if there is no leading\n\t\t# slash.\n\t\tself._root_path = self._root_path[1:]\n\n\t\t# Set up local cache\n\t\tself._cache = oz.ozutil"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.path = self.config.get(\"path\")\n\t\tif not os.path.exists(self.path):\n\t\t\tos.mkdir(self.path, 0o700)\n\t\tself.dir = os.path.join(self.path, \"storage\")\n\n\tdef initializeWithConfig(self, config_string):\n\t\tself.config = ConfigParser.SafeConfigParser()\n\t\tself.config.read(config_string)\n\t\tself.storage_dir = self.config.get(\"storage\", \"root\", \"storage\")\n\t\tself.directory = self.config."}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself._log(\"initializing storage root...\")\n\t\tself._o = OBJCFetch(None, self)\n\t\tself._o.add_type(OBCFetch.OBJCF_CACHE)\n\t\tself._o.add_type(OBCFetch.OBJCF_BUFFER)\n\t\tself._o.set_data(self._buffer)\n\t\tself._root = None\n\n\tdef init_namespace(self, prefix, uri):\n\t\t\"\"\"\n\t\tInitialize the namespace.\t\n\t\t:param prefix: The namespace prefix.\n\t\t:param uri: The uri of the"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\t# Set debug level if used\n\t\tself.setDebug()\n\n\t\t# Create config object\n\t\tself.storage = storage.storage(self.storageConfig)\n\t\t# Update the storage object with the new storage config\n\t\tself.storage.setRoot(self.storage.getRoot())\n\n\t\t# Re-initialize root\n\t\tself.initializeStorageRoot()\n\n\tdef updateStorageRoot(self):\t\t\t\n\t\t\"\"\"\n\t\tUpdate the storage root by setting the storage.root and a new storage config,\n\t\tand remove old one if it exists.\t\n\t\t\"\"\"\n\t\t# Disable the"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.createSubDir(name=\"OCFS-StorageRoot\")\n\t\tself.createFile(\"OCFS-StorageRoot.xml\")\t# the ocfs root\n\t\tself.createFile(\"OCFS-StorageRoot.bin\")\t\t# ocfs Root\n\t\tself.createDir(name=\"OCFS-StorageRoot\", path=\"/tmp/oscrcc\")\n\t\tself.addDir(name=\"OCFS-StorageRoot\", path=\"/tmp/oscrcc/OSC4S\")\n\t\tself.writeManifest()\t\t\t# make the root"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tLog.i(5, \"OCFFL storage root init\")\n\n\t\tself.isNew = True\n\t\tself.fsList = dict()  # path -> FileEntry\n#\t\tLog.i(5, \"OCFFL storage root init done\")\n\n\tdef find(self, fileId):\n\t\t\"\"\"\n\t\tFind a file by its ID. Will raise an OSError if the entry not found.\n\t\t:param fileId: int with desired file identifier\n\t\t:return: FileEntry if found, None otherwise\n\t\t\"\"\"\n\t\tlog.i(5, \"OCFFL"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n\n\t\tpattern = '^(?P<major>[0-9]+)'\n\t\tpattern += '(?:[1-9][0-9]*\\.?[0-9]*)+(?P<minor>[\\-0]+)'\n\t\tpattern += '(?:[1-9][0-9]*\\.?[0-9]*)-(?P<revision>[\\-0]+)'\n\n\t\tmatch = re.match(pattern, version)\n\t\treturn match.group('major'), int(match.group('minor')), match.group('revision')\n\n\n\t# NOTE: For Python 2 compatibility, ignore the error when the \"git\" module is\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tfor i in (1,2,3): # 1.3a with prefix in the version number\n\t\t\tif version.startswith(str(i)):\n\t\t\t\tyield i\n\t\t\telif version == str(i) and (len(str(i))==1 or str(i)[0]==\">\"):\n\t\t\t\t\tyield 2\n\t\t\telse:\n\t\t\t\tfor v, v_end in [(i,i) for i in reversed(list(enumerate(version))):\n\t\t\t\t\tif version[0]==v[0]: # prefix match\n\t\t\t\t\t\tif i<=i+len(version): # already prefixed\n\t\t\t\t\t\t\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t # Version numbers:\n\t #   0.2\t[x]\t0.2\n\t #   0.1\t\t0.1.0\n\t #   0.0.0\t1.2\t4.0.2\n\t #   1.2.3\t0.3.0\n\t #   1.2.3-1\t1.2.3\n\t #   1.2.3-1[post]\t1.2.3-1\n\t #   1\t1.2.3-post[pre]\t1.2.3-post[post]\n\t #"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ndef get_suffix(source):\n\treturn \".\".join(source.split(\".\")[:-1])\n\n\n# Return the short version (i.e. \"v1.3.0\") in which the current file is in\n# the current source directory.\n\ndef version_tuple(short):\n\treturn tuple([int(x) for x in version.split('.')][:2])\n\ndef file_is_locked(source, name):\n\tfile_path = os.path.join('.lock', name)\n\treturn os.path.isfile(file_path)\n\ndef"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n        i += 1\n        assert(i > 0)\n        \n        return version\n\n\ndef next_suffix(suffix):\n        ''' Return version suffix string. '''\n    \n        if len(suffix) < 3:\n            return \"0\"+suffix\n\n        return suffix\n\n\ndef end_of_line(filename):\n\t''' \n\tReturn end of last line from file. '''\n\t\n\tf = open(filename)\n\twhile(True):\n        l = f.readline()\n\tif not l: break\n\telse:\n\t\tif l[0] == '\\n':\n            l = l[1:]\n        if l"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn \".\".join(version.split(\".\")[:2])\n\ndef split_version(version):\n\t\"\"\"\n\tSplit version string into three parts: major-minor,\n\tminor-patch, micro-patch, and serial-number.\n\t\"\"\"\n\ti=int(version[len(version)-1])\n\tif i<10:\n\t\treturn \"0.0%s\" % version\n\tb=int(version[len(version)-2])\n\treturn \"%d.%d\" % (a+b,b)\n\ndef int_or_str(version"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn _parts[version.key(*_parts)[1]]\n\ndef prev_version(version):\n\t\"\"\"\n\tPrevious version identifier following existing pattern.\n\nMust implement __eq__ on final instances of Version.\n\t\"\"\"\n\treturn next_version(version)\n\ndef __str__(self):\n\tif self.__doc__:\n\t    if self.__doc__.startswith(self.__desc__) :\n\t\treturn \"Documentation for %s\" % version_to_string(self.__doc__)\n\t    else :\n\t\treturn self.__doc__\n\telse:\n\t    if self.get(\""}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn version[-2:]\n\n@TaskGen.feature('cshlib','cxxshlib','dshlib')\n@after_method('process_source')\n@before_method('process_use')\ndef apply_d_compiler_flags(self):\n\t\"\"\"\n\tTask generator method to ease building.\n\t\"\"\"\n\tif self.env.CC_NAME =='msvc':\n\t\treturn\n\tfor var in self.v:\n\t\tidx = self.env.CC[var]['CXX']\n\t\ttry:\n\t\t\tidx = idx.replace(' ','')\n\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn _next_version_index(version) + 1\n\n\ndef getNextVersion(version, prefix = ''):\t\"\"\"\n\tGet next version from filename (usually path), prefix string and inorder from version in filename.\n\n\tIf prefix is not empty, use only prefix string and inorder from version in filename.\n\n\t@param version: a number of ints to be looked up (starting at 0)\n\t@param prefix: a prefix string to search\n\t@returns: a 2-tuple of (version_number, next_version)\n\t\"\"\"\n\tif prefix == '':\n\t\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ndef parse_version(version_file):\n\tversion = \"\"\n\ttag = \"v\"+version_file['tag'].upper()\n\tversion = version_file['version'].strip()\n\t\n\tif not version:\n        \treturn version\n\n\tif re.match(r'^v'+version_file['tag'].upper(), version):\n\t\tassert(tag == \"v0\")\n\t\tversion = version[len(tag):]\n\n\treturn version\n\ndef write_version(version_file, version):\n\tversion_file.write((\"%\"+version_file['tag'].upper()"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tself.version_dirs = version_dirs\n\n\t@property\n\tdef check_version_files(self):\n\t\t'''\tThe check we enforce is actually performed in a list of files for us'''\n\t\treturn self.version_dirs[0].split(';')\n\n\tdef check_and_repair_inventory(self, inventory):\n\t\t'''\n\t\tMake sure no hosts are available and are in all inventory files\n\n\t\tThis is a helper function.\n\t\t'''\n\t\tused_hosts = []\n\n\t\t# FIXME: it belongs somewhere to the inventory\n\t\tused_inventory = []\n\n\t\tfor"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tassert len(version_dirs) > 0, \"Inventory inventories must contain 0 or 1 elements.\"\n\t\tfor i in range(len(version_dirs)-1):\n\t\t\tif version_dirs[i] <= 0:\n\t\t\t\traise ValueError(\"Version directory number {} is not valid.\".format(version_dirs[i]))\n\n\t\t\tself.version = version_dirs[i]\n\n\t\tself.num_all_inventories = len(self.version)\n\n\t\tfor version_dir in version_dirs:\n\t\t\tself.validate_version(version_dir)\n\n\t\tif len"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tvalidation_errors = []\n\t\tfor version_dir in version_dirs:\t\t\t\n\t\t\tversion_dir = os.path.abspath(version_dir)\n\t\t\tfor root, dirnames, filenames in os.walk(version_dir):\n\t\t\t\tif 'digests.' not in filenames:\n\t\t\t\t\tcontinue\n\t\t\t\tif not version_dir.startswith(root): # exclude all folder trees at the end of the directory\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tfor hash_name in filenames:\n\t\t\t\t\tfile_path = os.path.join(root, hash_name)\n\t\t\t\t\tif not os.path.isfile"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\n\tif not version_dirs:\n\t\traise Exception(\"Invalid version_dirs, need upgrade.\")\n\n\tversion_dirs_str = \"\".join([\"/\".join(x) for x in version_dirs])\n\tfor version_dir in version_dirs:\n\t\tallversions = [x.split(\"/\")[0] for x in os.listdir(version_dir)]\n\t\tfiles = set([x.split(\"/\")[0] for x in allversions if len(x.split(\"/\")[-1]) == 0])\n\t\tversions = []\n\t\tfor file in files:\n\t\t\ttry:\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif len(version_dirs) < 2:\n\t\t\traise ValueError(\"Invalid version directory path encountered\")\n\n\t\t# Split the version directory path into 3 pieces\n\t\tversion_dir_parts = version_dirs.split(\"/\")\n\t\t\n\t\tif len(version_dir_parts[0]) < 2:\n\t\t\traise ValueError(\"Invalid version directory path encountered: \" + version_dir_parts[0])\n\n\t\t# Ensure that the version list exists\n\t\tif not isinstance(version_dir_parts[0][1], list):\n\t\t\traise ValueError(\"Invalid version list path encountered: \" + version_dir_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tseen = set()\n\t\tseen_names = set()\n\n\t\tfor version_dir in version_dirs:\n\t\t\tfor subdir in version_dir.split(\"/\"):\n\t\t\t\t# Remove the prefix and suffix on the prefix, if there is\n\t\t\t\t# already one, add a suffix to the prefix if not already there\n\t\t\t\t# If so, there is no suffix present at all possible versions\n\n\t\tfor name in seen:\n\t\t\tvalid = False\n\n\t\t\tfor part in name.split(\"/\"):\n\t\t\t\tfull_part = os.path.join(part, name)\n\n\t\t\t\tif full_part in seen"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif len(version_dirs) <= 1:\n\t\t\tself.logger.error(\"Above should've looked at version_dirs %s in fact. Not checking for inventory upto one.\" % \n\t\t\tversion_dirs)\n\t\t\treturn\n\n\tdef process_inventory(self, basedir, versions):\n\t\t\"\"\"\n\t\tProcesses each version directory. \n\t\t- It checks which directories are a subdirectory of the inventory,\n\t\tand in those subdirectories, then adds them to the inventory.\n\t\t- It adds a list of valid releases per release.\n\t\t- It adds the release directory to the inventory, \n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version_dir in reversed(version_dirs):\n\t\t\troot_entries = self.find_entries_in_version(version_dir)\n\t\t\tif root_entries is None:\n\t\t\t\traise YunohostException(\n\t\t\t\t\t'The inventory version (%r) does not exist in version_dir %s'\n\t\t\t\t\t% (version_dir, version_dir))\n\t\t\tyield (version_dir, None,''.join(sorted(self.versions[root_entries.t_mtime])))\n\n\tdef __init__(self, versions, root_entries, t_mtime=None"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version in version_dirs:\n\t\t\tprefix = \"v\" + version\n\t\t\tif os.access(self._root_directory + prefix + \"/version\", os.F_OK):\n\t\t\t\t\t# check the version in the inventory since it might be different:\n\t\t\t\t\t# a) if it is, it's safe to use it, since we're running it through\n\t\t\t\t\t# b) if it doesn't, since we're running it through a different instance of\n\t\t\t\t\t#    this script and that version is the same, and the inventory only gets\n\t\t\t\t\t#    a hash containing this whole file as an"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tassert isinstance(version_dirs, list)\n\t\tversions = [\"1\",\"2\",\"3\"]\n\t\tversions.extend(version_dirs)\n\t\treturn versions\n\t\t\n\tdef update_inventory(self, filepaths):\n\n\t\t\"\"\"\n\t\tWhen processing a file, it turns them into a Python inventory:\n\t\t- Make a list of all files in the package.\n\t\t- Generate a list of all hashes and size hashes.\n\t\t- Sort the lists so that the hashes for the \"top\" version trees are always first\n\t  in the list (because they don't start with a version number).\n\t\t"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# print \"searching path '%s'\" % path\n\n\t\tif path == '/usr/share/X11/Xinit':\n\t\t\treturn 'root'\n\t\telif path == '/usr/local/X11/Xinit':\n\t\t\treturn 'object'\n\t\telse:\n\t\t\traise OCFLError('Not found path: \"%s\"' % path)\n\n\ndef extract_file_type(path):\n\t\"\"\"\n\tReturn the type of thing at the given path.\n\t\"\"\"\n\t\t# print \"looking path '%s'\" % path\n\n\t\tif path == 'root':"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tif not (path.startswith('/')) and path.endswith('/'):\n\t\t\ttype = 'root'\n\t\telse:\n\t\t\ttype = 'object'\n\t\t\tpath = path[1:]\n\t\tparts = path.split(\"/\")\n\t\tif len(parts) > 1:\n\t\t\ttype = parts[1] + \"/\"\n\t\telse:\n\t\t\ttype = 'root'\n\treturn type\n\t\ndef find_mount_dir(path):\n\t\"\"\"\n\tReturn an OCFL mount directory.\n\t\"\"\"\n\t\n        # First remove the drive letter from the path, since we assume that\n\t\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tresult = ['0=','','']\n\t\tresult.extend(get_paths_on_host(path,type='object'))\n\t\tresult.extend(get_paths_on_host(path,type='file'))\n\t\tresult.extend(get_paths_on_host(path,type='inventory'))\n\t\tresult.extend(get_paths_on_host(path,type='folder'))\n\t\tresult.append(path)\n\t\tresult.append('')\n\t\treturn '\\n'.join(result)\n\texcept subprocess.CalledProcessError,err:\n\t\t\t\treturn err\n\t\t\t\ndef"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# The following is an array in bytes, just to be sure...\n\t\t# I've had to figure out how big we would be to check.  Note that\n\t\t# this is always zeroed out at this point, but for some reason\n\t\t# the above shouldn't be a problem.\n\t\tlen_str = os.stat(path)[stat.ST_SIZE]\n\t\tname_str = path\n\t\tif len_str.startswith(name_str):\n\t\t\t# path is a file, walk to it and find the type\n\t\t\tfor i in range(0, len("}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# the file is the first entry in inventory and starts at \"File:\"\n\t\t# find the string \"File: Namaste files\"\n\t\tparts = path.split(os.sep)\n\t\t# the split string \"Namaste files\" consists of 2 tokens, first is the pathname\n\t\t# of the file, and second is a string in the form xxxx - where xxxx is a number\n\t\t# from where the file name is...:\n\t\t# \n\t\t#   0: The string \"Namaste files\" starts at the offset - this refers \n\t\t#     to the"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t      \t\n\ti=0\t\t\t\t\n\twhile (i < len(path)-6):\n\t\tif (path[i]=='\\0'):\n\t\t\ti+=1\n\t\t\tbreak\n\tif not i:\n\t\tpath=\"root\"\n\telif (path[i]==\"\\\\\"):\n\t\ti+=1\n\t\tc=path[i]\n\t\tif (c==\"\\\\\\\\\"):\n\t\t\ti+=1\n\t\t\tl=i-6\t\n\t\telse:\n\t\t\ti+=1\n\t\t\tl=i-8\n\t\tif (l==0):\n\t\t\traise \"error getting path object"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tsplitted = path.split(',')\n\t\tif len(splitted) < 2:\n\t\t\treturn 'OBJECT'\n\n\t\tpath = splitted[0]\n\t\tif path[0] == 'object':\n\t\t\treturn 'FOLDER'\n\t\tif path[0] == 'folder':\n\t\t\treturn 'OTHER'\n\treturn 'UNKNOWN'\t\n\nclass Path:\n\t\"\"\"\n\tClass which represents a 'path' in a folder and it's file type.\n\tUsable as a folder in a different directory.\t\"\"\"\n\tdef __init__(self, path):\n        self."}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\ttry:\n\t\t\t# Try the path.  Should work either as a file or a path.\n\t\t\t# If it is a path, then we have some extra crap as well as our magic flag\n\t\t\tpath_type = os.path.isfile(path)\n\n\t\texcept:\n\t\t\tpath_type = 'object'\n\n\treturn path_type\n\n\ndef get_path_to_node(node):\t\t# (string) -> string\n\t\t\"Converts OCFL Resource Node to node_string\"\n\t\tglobal _node\n\t\t_node = node.to_string()\n\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t       #return os.path.dirname(path)\n\tp = ocl.find_file_node(path)\n\tif p:\n\t\treturn ocl.get_string_attrib(p, \"type\")\n\treturn ocl.get_string_attrib(path, \"type\")\n\ndef find_file_types( path, file_types=None ):\n\t\"\"\"\n\tReturn a list of supported file types in a given path.\n\tIf <file_types> is specified, returns only file types specified.\n\t\"\"\"\n\t       try:\n\t\t\tocl."}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# this string should match what the most common filesystem\n\t\t# is starting at\n\tif path == '/':\n\t\treturn 'file'\n\telif path[0] == '!':\n\t\treturn 'root'\n\telif path[-1] == \":\":\n\t\treturn 'object'\n\telse:\n\t\treturn path\n\ndef find_paths(top):\n        paths = [os.path.normpath(x) for x in os.listdir(top) \\\n\t\t\t       if os.path.isfile(x) \\\n\t           and os.path.join(top, x) not in"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tlogging.debug(\"[%s] Params: %s\" % (self._index, params))\n\t\t\n\t\tif int(params['count']) > 0:\n\t\t\n\n\t\t\t# Branch Name\n\t\t\tbranch = params[\"id\"]\n\t\t\tif str(params[\"component\"]) == branch:\n\n\t\t\t\t# Component\n\t\t\t\tcomponents = \"Component\"\n\t\t\telse:\n\t\t\t\n\t\t\t\t# Component\n\t\t\t\tcomponents = \"Component\"\n\t\t\t\n\t\t\t# Check Bug id\n\t\t\tif bug_ids is None:\n\t\t\t\tif str(params[\"bug_id\"]) == bug_ids['id']:\n\t\t\t\t\tlogging.debug(\"[%s"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams['action'] = 'amend'\n\t\tparams['id'] = ','.join(bug_ids)\n\t\treturn self._request(params, ['Amend', 'Bugzilla'], 'POST')\n\n\t@_needs_auth\n\tdef update_bug(self, bug_id, **kwargs):\n\t\t\"\"\"\n\t\tTry to update the bug and return the modified Bug object.\n\n\t\tBUG_ID: The unique bug identifier.\n\t\t\"\"\"\n\t\tbug_id = to_unicode(bug_id) #pylint: disable=unexpected-keyword-arg\n\t\tif not bug_id"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\t# Remove any existing parameters.\n\t\tdel params['bugzilla_id']\n\n\t\t# Build up the Bugzilla URL\n\t\tparams['bugzilla_url'] = self.bugzilla_base + \\\n\t\t              params['bug_params'].format(**params)\n\t\t\n\t\t# Append data with the bug id.\n\t\tparams['bug_id'] = bug_ids\n\t\t\n\t\t# Execute the server using Django's SMTP server.\n\t\tself.smtp.sendmail(str(self.mail), [self.mailto],\n\t\t                  msg.as_string())\n\t\n\tdef get_bug_url(self, id):"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tquery = \"UPDATE bug_params SET bug_ids=? WHERE id=?\"\n\t\tif params is not None: query += \" AND id=?\"\n\t\t\tnum_of_params = 0\n\t\t\ti = 0\n\t\t\tfor i in range(len(params)):\n\t\t\t\tparams[i] = params[i] + \",\"\n\t\t\t\tif num_of_params == len(params[i]): num_of_params = 0\n\t\t\t\tif not params[i]: continue\n\t\t\t\tparam = params[i]\n\t\t\t\tcount = num_of_params\n\n\t\t\t\t# A list of bug id's to"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif (self.bug_id is not None) and (not params or not bug_ids):\n\t\t\treturn\n\n\t\tself.info(\"setting attachments\")\n\t\tfor (name, email, url) in self.attachments:\n\t\t\t# set the upload to the specified email address\n\t\t\tattachment = self.bz.change_attachment(\n\t\t\t\tbug_id, 'fileset',\n\t\t\t\tself.name, self.email, params, url\n\t\t\t\t)\n\t\t\t# store the full contents of the attachment in the database\n\t\t\t# in the 'url' parameter\n\t\t\tattachment.url = url\n\t\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tquery = 'bugzilla_set %s' % \",\".join(tuple(bug_ids))\n\t\tquery = self._log_bzparams(query)\n\n\t\treturn self.base.add_request(url=query, data=params, method='POST')\n\n\tdef add_bugs(self, bug_ids):\n\t\t\"\"\"\n\tSave a Bugzilla bug ids in a set of bug_ids\n\t@param bug_ids: list of Bugzilla IDs to be added\n\t@param params: A dictionary containing all valid parameters\n\t\"\"\"\n\t\tif not isinstance(bug_ids,"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif Bug.objects.filter(id__in=bug_ids).count() == 0:\n\t\t\treturn False\n\t\tparams.pop(\"id\")\n\t\tif Params(**params).count() == 0:\n\t\t\tparams.pop(\"id\")\n\t\treturn True\n\n\tdef ajax(self, request):\t\n\t\treturn HttpResponse(\n\t\t\tjson.dumps({\"bug-id\": self.b._bug_id, \"status\": \"ok\"}),\n\t\t\tcontent_type=\"application/json\"\n\t\t)\n\n\tdef put(self, request):\t\t\n\t\ttry:\n\t\t\tif self."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\turl = self.__makeBugUrl()\n\t\tif not bug_ids:\t\t\t# No bug ID list!\n\t\t\treturn\n\t\tfor bug in bug_ids:\n\t\t\ttry:\n\t\t\t\tbody = self._getURLBuildData(params, bug)\n\t\t\t\th = httplib.HTTPConnection(\"sourceforge.net\")\n\t\t\t\th.request(\"POST\", url, body, {'User-Agent': 'Mozilla'})\n\t\t\t\tresp = h.getresponse()\n\t\t\t\tprint resp.status, resp.reason, resp.status_desc\n\t\t\texcept (URLError, httplib.HTTPException):\n\t\t\t\traise\n\t\tprint"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams.update(self.server.get_params())\n\t\tif \"bzparams\" not in params:\n\t\t\traise BugzillaError(\"No bugzilla parameters!\")\n\t\tself.server.add_params(params)\n\t\tself.server.flush()\n\t\tself.connection.db.execute('SELECT * FROM BugzillaParams')\n\t\tbug_ids = map(int, params['bzparams'][0].split())\n\t\tself.connection.db.executemany(\"INSERT INTO BugzillaParams (Id, Name, Description) VALUES (%i, %i, '%s?')\", bug_ids"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams = params.copy()\n\t\tparams['id'] = bug_ids[0]\n\t\tparams['summary'] = u'%sthe %s-%s' %(params['id'], params['version'], int(time.time()*1000000))\n\t\treturn params\n\tamend_bzparams.url = 'http://bugzilla.redhat.com/xmlrpc.cgi'\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(AmendingBug, self).__init__(*args, **kwargs)\n\t\tself.user = None\t\t\t# Username\n\t\tself"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\nimport yaml\nimport datetime\n# TODO: write a unit test based on how this file should be written\n\nyaml_type2tag = {'tag': 'tag:yaml.org,2002:str', 'value': '5'}\nyaml_type3tag = {'tag': 'tag:yaml.org,2002:int', 'value': '24'}\n\ndef deep_merger(ds, merge_list):\n\t\"\"\" \n\tGiven ds, merge_list, return the result of merging. If ds is None, then no merging\n\twill be made. If ds is not None"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n__all__ = [\n\t'deepfy',\n]\n\nimport re, os, re.subn, codecs\n\n# ------------------------------\n\ndef deepfy(path):\n\t\"\"\"\n\tpath - full path including filename, can be absolute or relative or in current working directory\n\tReturns root normalized path path. If path is absolute or relative, the relative path will be\n\tmade absolute. If path isn't absolute or relative, it is added. If relative, it must end in '/'.\n\tReturns the normalized path.\n\t\"\"\"\n\t\n\treString = os.path.abspath("}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef create_config_section(borgmatic_file):\n    \"\"\"\n    For a given borgmatic_file containing:\n\n    [\n        \"a\",\n        \"b\",\n        \"b\",\n        \"a..b\",\n        \"a...\",\n        \"...\",\n        \"...a\",\n        \"...a..b\",\n        \"x\",\n        \"x..b\",\n        \"X..b\",\n        \"...a..b\"\n    ]\n    \"\"\"\n\n    yaml_string = to_yaml(borgmatic_file)\n    new_file = create_tmp_config_temp_file(yaml_string)\n\n   "}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tif isinstance(nodes, list) and not isinstance(nodes[0], MappingNode):\n\t\t\tnodes = list(node for node in nodes if node.tag in nodes[0].value)  # type: ignore # copy of list\n\t\t\tnodes = list(nodes)\n\t\t\tnodes[0].value = [Nodes.Clone()] + nodes[1:]\n\t\tif isinstance(nodes, list) and not isinstance(nodes[0], Node):\n\t\t\tnodes = [nodes]\n\treturn nodes\n\nclass TestNode(object):\n\tdef __init__(self, tag, value):\n\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tif isinstance(nodes, MutableSequence):\n                merged_node = nodes\n            elif isinstance(nodes, MutableMapping):\n                merged_node = deepcopy(nodes)\t# shallow copy, don't overwrite\n                nodes = merged_node\n            del nodes\t# doer!\t\t## debug\n\n        nodes = nodes_to_dict(nodes)\t# flatten the nested borgmatically\n        merged_node = {**merged_node, **nodes}\t# do the actual merge\n        if merged_node['tag:yaml.org,2002:str'] in merged_node['tag:yaml.org,2002:map']:"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef is_single_line(yaml_data, line): # lint: +ELLIPSIS\n\t\"\"\"\n\tReturns True if `yaml_data` has a key, else False.\n\t\"\"\"\n\tfor k in yaml_data:\n\t\tif line == None or k == line:\n\t\t\treturn True\n\treturn False\n\t\t\t\t\n\ttry:\n\t\tkey, value = next(iter(yaml_data))\n\texcept StopIteration:\n\t\traise BadYamlYAML(\"YAML file doesn't have a key and cannot end in an xblock YAML file. \"\n\t\t                     \"Therefore the YAML file"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(self.items())\n\n#### Read-only section of borgmatic config file\n# This has been implemented in the file \"config.yaml\" by the parse_borgmatic_config method\n# in order to be able to do some indexing using the parsed dictionary (or the list of tuples in\n# the document) rather than indexing them, we also want to remove some of that, otherwise\n# we'd remove this mess, since we'll be passing a copy of the dictionary that we'll use in our\n# merge_nodes dict and only store the config file section"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n        if not isinstance(nodes, list):\n\t\t\tnodes = [nodes]\n\t\tif len(nodes) < 2:\n\t\t\traise Errors.ConfigurationError, 'Configuration format is only allowed for configuration node values'\n\n\t\t# check that the configurations section is not an empty list\n\t\tfor node in nodes:\n\t\t\tif node['tag']!= 'configuration' or not node['value']:\n\t\t\t\traise Errors.ConfigurationError, \"Missing configuration node tag\"\n\n\t\t# convert all YAML nodes to the native Python types\n\t\tnodes = [n.value for n in nodes]\n\n\t\t# iterate over the configurations"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tif 'yaml' not in nodes:\n\t\t\traise ValueError(\"nodes must contain an 'yaml' section\")\n\t\tr = _deep_merge_nodes(nodes['yaml'])\n\t\tif r is not None: return r\n\t\t\n\t\tnew_config = copy.deepcopy(nodes['yaml'])\n\n\t\tnew_config.setdefault('reverting_to', None)\n\t\tif'reverting_from' in nodes: new_config['reverting_from'] = nodes['reverting_from']['content']\n\n\t\tnew_config.setdefault('reverter', None)\n\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(deep_merge_nodes([(group, key) for key, val in node.value.items()]for node in nodes))\n\n\nclass Node:\n\t\"\"\"\n\tRepresents a Python class of information from a configuration file, including\n\tthe values for individual configuration options parsed from other code,\n\tincluding the options for config files, the values for individual borg values, and\n\tthe values for individual config file attributes extracted from other sources\n\t\"\"\"\n\n\tdef __init__(self, tags=None, values=None, config=None):\n\t\t\"\"\"\n\t\t\t"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog='python-mintty-runtests.py',\n\t\t                                description='Executes the script with test arguments.',\n\t\t                                epilog='Written by Jason Nelson. This script does two things,'+\n\t\t\t\t\t\t\t\t\t\t\t              'to run the test suite, gather information about and'+\n\t\t\t\t\t\t\t\t\t\t\t              'whether tests for the entire suite were successful,'+\n\t\t\t\t\t\t\t\t\t\t\t              'and how well they run correctly.'+\n\t\t\t\t\t\t\t\t\t\t\t              'The actual run must be on the system PATH indicated'+\n\t\t\t\t\t\t\t\t\t\t\t              'in the current directory. Example usage: python -m"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser()\n\n\tgroup = parser.add_argument_group(\"Logging/Logging Options\")\n\tgroup.add_argument('-v', action='version', version='%(prog)s 0.0.1')\t\t\n\tgroup.add_argument('--logfile', dest=\"logfile\", default=\"./log/logfile\", help=\"file to log the debug output to\", metavar=\"LOGDIR\")\n\n\targs = parser.parse_args()\n\targuments = args._parse_args(args=args)\n\targuments.logfile = logfile = arguments._logfile\n\treturn instance(arguments"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(description=__doc__, epilog=EPILOG, formatter_class=RawDescriptionHelpFormatter)\n\treturn parser\n\nclass Argument(object):\n\t\"\"\"\n\tA parameter to be passed in an argument-file. Argument defaults to the first argument supplied.\n\tAny arguments not passed at all include the value set to None.\n\tIf the argument is left out, the first argument will be passed\n\tto corresponding argument. This is used for providing custom defaults\n\twhen missing required arguments. For example:\n\n\t(def my-method (self, x: int = 1)):\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\targ_parser = argparse.ArgumentParser(description=\"Pegasus - Python program for using British Earth Engine API\")\n\t\targ_parser.add_argument(\"-d\", \"--database\", default=None)\n\t\targ_parser.add_argument(\"-c\", \"--connect\", default=None)\n\t\targ_parser.add_argument(\"-u\", \"--use-database\", default=True, action=\"store_true\")\n\t\targ_parser.add_argument(\"-p\", \"--port\", default=5000, action=\"store\", type=int)\n\t\targ_parser.add_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\nusage = \"usage: %prog [options] [args]\"\ndescription = \"This script will be automatically called when the arguments given on the commandline are processed.\"\nversion = \"%prog 0.1\"\t\t\t# Print the version number and exit\n\nparser = OptionParser(usage=usage, description=description,version=version)\nparser.add_option(\"-v\",\"--verbose\",dest=\"verbose\",action=\"store_true\",default=False,help=\"tell me what you are doing\")\n(options, args) = parser.parse_args()\n\nif not options.verbose:\n\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tparser = ArgumentParser(description=\"Utility to help with getting data from the PASCAL Vantage\")\n\n\tparser.add_argument('--server',\n\t                       action='store', \n\t                       nargs=2, \n\t                       # action='store', \n\t                       help='Server to query. Either a file containing the server connection file or the URL of an arduino server.')\n\n\tparser.add_argument('--arv',\n\t                       action='store', \n\t                       nargs=2, \n\t                       #action='store', \n\t                       help='Arv AVR data.')\n\n\tparser.add_argument('--debug',\n\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\t# Create a command-line argument parser for the given parameters (by\n\t# overriding whatever arguments were the program arguments in __init__)\n\tparser = argparse.ArgumentParser(\n\t\tdescription = 'Run Molcas test suite with the specified settings and run the following scripts:')\n\tparser.add_argument('-t', '--target', dest='target', help='Path to test package; e.g. /usr/share/doc/package')\n\tparser.add_argument('-p', '--parallelism', dest='parallelism', help='Maximum number of processors to use. Defaults to the number"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(add_help=False, conflict_handler='resolve')\n\t\tparser.add_argument(*arguments, **arguments)\n\t\treturn parser\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n#\n# The MIT License (MIT)\n#\n# Copyright (c) 2016 Bartosz Janda\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n#"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(description=\"This help\")\n\t\tparser.add_log_file()\n\t\tparser.print_usage()\n\t\treturn parser.parse_args(*arguments)\n\treturn parser\n\n#-----------------------------------------------------------------------------\n# Main\n#-----------------------------------------------------------------------------\nif '__main__' == __name__:\n\tsys.exit(main())\n# -*- coding: utf-8 -*-\n#################################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2013-2015 Julius Network Solutions SARL <contact@julius.fr>\n#\n#    This program is"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tif arguments:\n\t\t\tparser = argparse.ArgumentParser(add_help=arguments['-h'])\n\t\telse:\n\t\t\tparser = argparse.ArgumentParser(add_help=arguments['-h'])\n\t\tparser.add_argument('command', nargs=argparse.REMAINDER)\n\t\treturn parser.parse_args()\n\n\t@staticmethod\n\tdef parse_command_line_and_argv(arguments):\n\t\t\"\"\"\n\t\tTake argv and interpret them.\n\t\t\"\"\"\n\t\t# Set the command for use by other programs.\n\t\targs = parse_arguments(arguments)\n\t\targuments"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tflags, args = parser.parse_known_args()\n\tflags = \"\".join(flags)\n\t\n\t# filter options\n\topts = [item for item in opts if item.get(\"flags\", None)!= None] + \\\n\t\t  [item for item in args if item.get(\"flags\", None)!= None]\n\tflags = \" \".join(opts)\n\t\n\treturn flags\n\ndef find_all_project_files(dir=\".\"):\n\t\"\"\"\n\tSearch for all *.cproject files, not just their.sip files.\n\t\"\"\"\n\t\n\tfor root,"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tdef get_arg_name():\n\t\t\"\"\"The name in front of the flag.\"\"\"\n\t\t\n\t\treturn ''\n\t\n\treturn parsers[get_arg_name()](parser)\n\n\ndef register(subparsers):\n\t\"\"\"Register and parse the command line argument parser.\"\"\"\n\t\n\treturn subparsers.register(*parser_flags(parser))'''\nGiven a binary tree and a sum, count the number of root-to-leaf paths from\nthe root to the leaf of the tree. \nReturn its sum and its root.\n\nExample 1:\n\n\n    \n    \r\n    >>> from root import TreeNode"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tflags = []\n\tdef add_argument(arg):\n\t\tflags.append(\"--\" + arg.strip())\n\t\n\tparser.add_argument(\"--help\", action=ArgparseAction, dest=\"help\", help=\"show this help message and exit\")\n\tparser.add_argument(\"--quiet\", action=ArgparseAction, dest=\"quiet\", help=\"do not display status messages\")\n\tparser.add_argument(\"--force\", action=ArgparseAction, dest=\"force\", help=\"force update, do not commit any output\")\n\n\treturn flags\n\n\n# -*- coding: utf-8 -*-"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tp1 = parser.add_argument_group(title='p1',\n\t\t description='Options for p1')\n\tp1.add_argument(\"-v\", \"--verbose\", action=\"count\", \n\t\thelp=\"Set verbosity of output.\", default=0)\n\tp1.add_argument(\"-q\", \"--quiet\", action=\"count\", \n\t\thelp=\"Set verbosity of detail.\", default=0, const=0)\n\tp1.add_argument(\"--verbose-count\", dest=\"verbose\", \n\t\taction=\"store_true\", help=\"Be more verbose (e.g., -"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"-d\", \"--delimiter\", help=\"Delimiter string\",\n\t\t\tdest=\"delimiter\", default=\",\",\n\t\t\thelp=\"Delimiter string to use for the names of the columns (default: all \"\n\t\t\t\"columns).\", nargs=\"+\")\n\n\tparser.add_argument('--order')\n\tparser.add_argument('--sort', dest='sort', default=None,\n\t\thelp='Name of type of fields to sort by (default: \"id asc\").',\n\t\tchoices='id asc'.split())\n\tparser.add_argument('-f', '--delimiter', help=\""}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tdef _sub(match):\t# type: (Match[str]) -> str\n\t\t\"\"\"\n\t\tReturns the appropriate substring of the supplied argument\n\t\t\"\"\"\n\t\treturn parser._sub(match, options.flags)\n\t\n\tfor name, parser, description in _parser_actions:\n\t\toption_parser_actions[name] = parser_flags\n\n\tfor short, long_flagfunc in _sub.patterns:\n\t\tl = optparse.Option(\n\t\t\toption_strings=[\"-\" + short],\n\t\t\tlong_flagfunc=long_flagfunc,\n\t\t\taction=lambda"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = parser.parse_args()\n\t\tfor k,v in map(lambda x:x.strip(), flags):\n\t\t\tyield \"--\"+k.replace(\"_\",\"-\")\n\ndef main():\t\n\tmain_parser = ObfuscatedArgumentParser(prog=\"parser\", add_help=True)\n\n\tif len(sys.argv) > 1:\n\t\toptions = parser_flags(sys.argv[1:])\n\t\tmain_options = parser_options(options)\n\telse:\n\t\tmain_options = parser_options()\n\n\tmain_options.func(main"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tdef parse_flags(args):\t# pylint: disable=W0613\n\t\t# Get all arguments without a space.\n\t\targs = [\"\"] * len(args)\n\t\t\n\t\tfor i in xrange(len(args)):\n\t\t\targ = args[i]\n\t\t\tif len(arg) == 0:\n\t\t\t\tcontinue\n\t\t\t\t\n\t\t\tassert arg[0] == \"=\"\n\t\t\tif \"?\" in arg:\n\t\t\t\targ, val = arg.split(\"?\", 1)\n\t\t\t\targ = arg.strip()\n\t\t\t\tif arg == \"all\":\n\t\t\t\t\targs[0] = \"*\"\n\t\t\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tdef parse_flags(argv):\n\t\toptparser = MyArgumentParser(add_help=False)\n\t\toptparser.add_argument_group(None, 'Options')\n\t\topts, args = optparser.parse_known_args(argv)\n\t\t\n\t\targprs = ('help', 'argc')\n\t\tfor opt, argp in zip(opts, args):\n\t\t\tif opt in ('--help', '-h'):\n\t\t\t\tprint Help\n\t\t\telif opt in ('--argc', '--argp'):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tprint \"Error: unknown argument `%s"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t \n\topts, args = parser.parse_known_args(sys.argv)\n\topts.positional_args = getattr(args, 'args', None)\n\ttry:\n\t\topt_string_flag = getattr(opts, 'positional_args', None)\n\texcept AttributeError:\n\t\topt_string_flag = 0\n\n\tprint(opt_string_flag)\n\tif len(args) == 0:\n\t\toptions, args = parser.parse_known_args(sys.argv)\n\tcommand = args[0]\n\tflag_args = 0\n\n\tfor"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\t# Parse the arguments given in the command line and return\n\tnamespace, args = unparsed_arguments + [\"-h\"]\t\t# add the default header line, if none was given\n\tif len(args) > 0:\n\t\tnamespace, args = args[0], args[1:]\n\n\t# First, the namespace object will contain the arguments as attributes to parse.\n\t# The Namespace object can then access information about the arguments for further processing.\n\tns = argparse\n\t\n\t\t\n\t# Iterate through the commands and parse the arguments from the commands.\n\tcount = 0\t\t# initialize"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='A program that can be used to invoke some'+ \\\n\t\t\t\t\t\t'other subcommand')\n\n\t\tparser.add_argument('command', metavar = 'command', help = 'The commandline to run')\n\t\treturn parser.parse_args(unparsed_arguments)\n\n\tdef main(self, argv):\n\t\targuments = parse_arguments(self.command.split())\n\t\treturn_obj = self.construct(arguments)\n\t\t\n\t\tif return_obj == {}:\n\t\t\treturn 1\n\n\t\tstdout_data = return_obj['stdout']\n\t\tstderr_data"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog=\"py-pulsar_client.py\", description=\"<b>pulsar-client> Command-line interface for the pendulsar server. See the tutorial on tutorial for more information about the pendulsar software, and the online docs for how to use py-pulsar on your own Pulsar cluster. An idiomatic script for simple client applications is available at http://pulsar.sourceforge.net.\\n\\nCopyright (c) 2017 Thomas Hannon <thomas@thomas.se> & contributors."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Command-line utility to retrieve or generate a python class' )\n\n\t\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\", dest=\"debug\", help=\"Show debugging output\", default=False)\n\t\tparser.add_argument(\"-o\", \"--output\", action=\"store\", dest=\"output\", help=\"Path to output file directory\")\n\n\t\tsubparsers = parser.add_subparsers(help='Sub-commands')\n\n\t\tadd_subparser = subparsers.add_parser('add', help='Add subparser')\n\t\tadd_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t # Parse the commandline arguments to be passed to this script.\n\t parser = argparse.ArgumentParser(description=\"This scripts parses a local JSON file, given as input a.json file for \" + \\\n\t\t\"visualization, and returns the output of these scripts as a parsed JSON object. All subparser names \" + \\\n\t\t\"of the same subparser will be named 'local_parsed_arguments', to see how the arguments are passed to \" +\n\t\t\"this script.  If --subparser-arguments is not provided, sys.argv[1] in the arguments will be passed \" + \\\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tsub_parsers = {}\n\n\t# parse command line\n\tglobal_args = []\n\thelp_args = list()\n\tfor name, arg_str in zip(arguments, list()):\n\t\targ = arg_str.decode(\"utf-8\").strip()\n\t\tif arg == \"--help\":\n\t\t\thelp_args = list()\n\n\t\telif arg == \"--version\":\n\t\t\twith open(arg_str, 'r') as f: # parse_file() will read from the whole file\n\t\t\t\tfor line in f:\n\t\t\t\t\tparts = line.split(\" \")\n\n\t\t\t\t\tif"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\toptions = dict(\n\t\t\tlist(reversed(arg_iter))\n\t\t\tfor arg_iter in\n\t\t\tUnparsedArgumentsArgumentParser().parse_args(\n\t\t\t\tunparsed_arguments\n\t\t\t)\n\t\t)\n\t\treturn options\n\n\tdef make_subparser(self):\n\t\treturn optparse.OptionParser()\n\n\tdef parse(self, args):\n\t\targuments_dict = vars(self.parse_arguments(*args))\n\t\toptions = self.make_arguments(arguments_dict)\n\t\tif options.print_help:\n\t\t\tself._print_help()\n\t\t\tsys.exit("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\treturn {arg[0]: arg[2] for arg in unparsed_arguments}\n\ndef load_app(app_name):\n\t\t\"\"\"\n\t\tLoads an app with the given name. If no application matches the given name,\n\t\treturns None. If the supplied app does not yet exist, it will be created.\n\t\t@param app_name: the name that the application is running.\n\t\t@type app_name: str\n\t\t\"\"\"\t\n\tfrom core import command_line_arguments as _command_line_arguments_module\n\tfrom core import arguments as _arguments_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tdef _main(arguments=None, subparsername=None):\n\t\tparser = argparse.ArgumentParser(*arguments, usage='usage: %prog [OPTIONS] [TARGETS]')\n\t\t#parser.add_argument(\n\t\t#\t'-i', '--interactive-mode',\n\t\t#\taction='store_true',\n\t\t#\thelp='run a single interactive session. Useful for debugging.',\n\t\t#\tdefault=False\n\t\t#)\n\t\tparser.add_argument(\n\t\t\t'-m', '--main-modality',\n\t\t\tchoices=['true', 'false'],\n\t\t\tdefault='"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\t\t\t\t\t\t\t\t\tdescription=description,\n\t\t\t\t\t\t\t\t\t\t\tepilog = epilog,\n\t\t\t\t\t\t\t\t\t\t\tformatter_class=argparse.RawTextHelpFormatter)\n\t\tparser.add_argument(\"-r\", \"--repo\",\n\t\t\t\t\t\t\t\t\t\t\thelp=\"Enter repo URL (e.g. repo/something/someone/branch)\")\n\t\tparser.add_argument(\"--type\",\n\t\t\t\t\t\t\t\t\t\t\thelp=\"Enter repository type ('repo' or 'url')\")\n\t\tparser.add_argument(\"--name\",\n\t\t\t\t\t\t\t\t\t\t\thelp='Enter repository name (e.g. repo/something/someone/branch)')\n\t\t"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\targs = [] # a list of subparser name strings\n\tparser_args = {} # arguments to be used for subparsers\n\tfor action in subparsers.choices:\n\t\targs = action.parser_args(args or []) # add the args that were not specified\n\t\tif args:\n\t\t\targs += args # we now have a copy of the list\n\t\t\tkw_prefix = args[-1]\n\t\t\twhile True:\n\t\t\t\ttry:\n\t\t\t\t\tkw = action.parser_get_arg_metadata(kw_prefix)\n\t\t\t\t\tif kw:\n\t\t\t\t\t\tparser_args[kw_prefix +"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# Get the options into the global namespace\n\t\tdest_subparser_name = None\n\t\tfor key, value in vars(unparsed_arguments).items():\n\t\t\tdest_subparser_name = value\n\t\tnamespace_parser = subparsers.add_parser(dest_subparser_name,\n\t\t                                         help = _(\"Sub-command\"))\n\t\tnamespace_parser.add_argument('repository',\n\t\t                                              metavar = None,\\\n\t\t                                              nargs = '+',\\\n                                default = None,\n\t\t                                              help = _(\"Repository to validate\"))\n\t\tnamespace_parser.set_defaults(func ="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\ndef arguments_from_namespace(namespace, namespace_map_in_args, subparsers):\t\n\t\"\"\"Return a list of parser arguments by the given namespace for additional parsers. This\nreturns as a tuple of (args parsed from argparse) for the next namespace arg.\n\t\"\"\"\n\treturn list(namespace_map_in_args if namespace in namespace_map_in_args\n\t\telse [namespace]) + list(copy.copy(namespace_map_in_args)) \n                                    # [namespace]\n\t\ndef add_subparser_arguments(subparsers):\n\t\"\"\"Return all arguments"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tresult = []\n\t\tfor subparser in subparsers:\n\t\t\tresult += parse_subparser_arguments(subparser, subparsers)\n\t\treturn (result, unparsed_arguments)\n\treturn ()\n\ndef parse_args(arguments):\n\tresult = []\n\targs = iter(arguments)\n\n\tparser = argparse.ArgumentParser(prog='roster', description = 'A command that is a roster service. Note that'+ self.short_name, epilog = 'See help topics for complete documentation on a command.', epilog_epilog = '\\n\\n')\n\tparser.add_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tdef _parse(subparser_args):\n\t\tsubparser = None\n\t\twhile True:\n\t\t\tkey = subparser_args.pop(0)\n\t\t\t\n\t\t\tif key in parsers:\n\t\t\t\tsubparser = parsers[key]\n\t\t\t\tbreak\n\t\t\t\n\t\t\taction_name = key\n\t\t\tif hasattr(subparser, 'dest'):\n\t\t\t\taction_name = subparser.dest\n\t\t\t\n\t\t\toption_string = None\n\t\t\tif hasattr(subparser, 'option_strings'):\n\t\t\t\toption_string = subparser.option_strings[0]\n\t\t\t\n\t\t\tif action_name and action_name!="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tresult = subparsers.add_parser(\"resolve-subparser\", \n\t\t\thelp=\"resolve args to a repository from a dict\")\n\t\n\tdef resolve_subparser_action(subparser_dict):\n\t\tunparsed_arguments.remove(subparser_dict['name'])\n\t\tparsed_arguments = parse_subparser_arguments(unparsed_arguments, subparser_dict)\n\t\tif subparser_dict['action'] in _aliases_for_action[parsed_arguments[0]]:\n\t\t\taliases = _aliases_for_action[parsed_arguments[0]][subparser_dict['"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tsubparser_actions = subparsers.add_parser(\"repository\", help=\"Parse an existing SCM repository in order to construct a new repository\")\n\tsubparser_actions.add_argument(\"repository_url\", nargs = \"?\", metavar = \"URL\", help=\"The repository URL to parse\")\n\tsubparser_arguments = add_argument_actions(subparser_actions, \"repository_url\")\n\tsubparser_arguments.add_argument(\"local_args\", nargs = \"?\", help=\"An optional argument (e.g. location,...) that the repository will parse\")\n\t\n\tsub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tcommand = subparsers.add_parser('repository', parents=[common_args])\n\tcommand.set_defaults(func=repository)\n\n\t# Set up some helpful arguments which can allow a subparser to be called if missing\n\t# arguments are present on the command-line, as well as having a shot at parsing.\n\tparser_args_required = parser_subparser_required(parser)\n\trequired_arguments = set(map(lambda x: getattr(args, x), required_arguments))\n\n\tdef add_optional_argument(parser, name):\n\t\tparser_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tresult, args = subparser_parser(\n\t\t'proctranslating',\n\t\t**unparsed_arguments\n\t)\t\n\t# Filter the result based on the subparser, skipping the first arg and continuing\n\t# the rest.\n\tfor i, arg in enumerate(args):\n\t\t# Replace \"repository\" and \"repository\" as arguments\n\t\targ = arg.replace('repository','repository_{}'.format(i))\n\t\t\n\t\t# Extract the named arguments from the named arg dictionary\n\t\tresult.add_argument(*arg)\n\t\n\treturn result, args\n\ndef parse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tname = unparsed_arguments['<repository>']\n\tnargs = len(unparsed_arguments['<args>'])\n\tremaining_args = unparsed_arguments['<args>']\n\t\n\tsubparsers = [argparse.ArgumentParser()]\n\n\tdef parser(action, subparsers):\n\t\t\"\"\"\n\t\tGiven a argparse.ArgumentParser instance and a action and a subparsers,\n\t\treturn a callable that handles any additional/required arguments.\n\t\t\"\"\"\n\t\tsubparser = subparsers.add_parser(name, help=subparsers[action])\n\t\tsubparser.set_defaults(func=getattr(lambda p"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tprog_parser = ArgumentParser(description='PypeShell (http://mickeytools.sourceforge.net)')\n\tprog_parser.add_argument('name', nargs=1, metavar='COMMAND')\n\n\tsubparsers = prog_parser.add_subparsers(dest='command', help='Commands')\n\n\tfor command in SUBCOMMANDS.keys():\n\t    subargs = add_parser_argparser(command, subparsers)\n\t    subargs.func = command\n\t    subargs.name = command\n\t    setattr(subargs, 'action', command.split('_')[0])\n\t    if not isinstance(command,"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='A quick and dirty tool for using the FASTA file parser as a template.')\n\t#group = parser.add_argument_group('Search')\n\tgroup_output = parser.add_mutually_exclusive_group()\n\tgroup_output.add_argument('--search', type=str, nargs='+', dest=None, required=False, help='If no string is set, scan all entries.')\n\tgroup_output.add_argument('input', type=str, help='Path to the input file.')\n\tgroup_output.add"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=\"This script takes either a bam and a bam and saves it to a file.  Also does a random sampling of the input.\")\n\tparser.add_argument(\"-o\", \"--output\", \n\t\thelp=\"The name of the output.\",\n\t\ttype=argparse.FileType('w'),\n\t\trequired=True)\n\tparser.add_argument(\"-c\", \"--contamination\", \n\t\thelp=\"This script takes a number for plate continuum data, a number for background.\",\n\t\ttype=int)\n\tparser.add_argument(\"-l"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef get_argument_parser():\t\"\"\"\n\tConstruct the argparser for building options for this script.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Builds YARA benchmarks for the YARA benchmarking project.\", prog=\"yara\")\n\tparser.add_argument(\"project_dir\", help=\"the root path for the project directory \" ) #default: the base dir\n\tparser.add_argument(\"--benchmark_name\", help=\"the name of the benchmark\") #default: the name of the parser\n\tparser.add_argument(\"--benchmark_arguments\", help=\"additional arguments"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef handle_args():\n\tglobal args\n\tparser = argparse.ArgumentParser(prog=sys.argv[0],\n\t\t\t\tusage=main_usage,\n\t\t\t\tdescription=main_description)\n\t\n\t# Parse the options \n\tparser.add_argument(\"-a\", \"--all\", dest=\"all\", help=\"Find all objects\",\n\t\taction=\"store_true\", default=False)\n\tparser.add_argument(\"-e\", \"--edges\", dest=\"edges\", help=\"Search inside edges\", metavar=\"FILE\")\n\t\n\tparser.add_argument(\"-l\", \"--list\", dest=\"lists\", metavar"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tgroup = parser_class()\n\tgroup.add_argument(\"-f\", \"--config\", dest=\"config_file\", required=True, help=\"Specify a specific conf file. (default: %default)\")\n\tgroup.add_argument(\"-p\", \"--path\", dest=\"path_to_config_file\", required=False, help=\"Specify path to the configuration file\")\n\tgroup.add_argument(\"-r\", \"--restart\", dest=\"restart_method\", choices=['continuous','maximize'], default='continuous', type=str.lower, help=\"Continuous or maximizing.\")\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\targparser = argparse.ArgumentParser(\n\t\t\tdescription = 'Generate a BERT pipeline in PSL format files')\n\t\targparser.add_argument('-n', '--non-interactive', dest='non_interactive', \n\t\t\taction='store_true', default=False, help = 'Do not prompt the user for a non-interactive submission.')\n\t\targparser.add_argument('--debug', dest='non_interactive', action='store_true', default=False, help = 'Just print out the configuration information.')\n\t\targparser.add_argument('pipeline', help = 'Filename with PSL"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"input\", help=\"input file\")\n\tparser.add_argument(\"output\", nargs=1, help=\"output file (.fasta)\")\n\tparser.add_argument(\"-a\", \"--allele_file\", help=\"list of alleles to use\")\n\tparser.add_argument(\"-b\", \"--blastoutput\", help=\"blast output file\")\n\n\tparser.add_argument(\"-x\", \"--filter\", help=\"filtering expression (or not-set)\", type=str)\n\tparser.add_argument(\"--regex\","}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tsubpar = subparsers.add_parser(\"generate_data.py\")\n\t\tsubpar.add_argument(\"input\", \n\t\t\thelp=\"Path to the input data files (*.xml)\")\n\t\tsubpar.add_argument(\"output\", \n\t\t\thelp=\"Path to which output data will be generated (*.json)\")\n\t\treturn parser, subparsers\n\ndef main():\n\targs = create_parser().parse_args()\n\tassert isfile(args.input) and isfile(args.output)\n\n\t# Load all the modules and their classes\n\treader = CVS_CSVParser(args"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(help='Subcommands')\n\tparser_compare = subparsers.add_parser('compare')\n\tparser_compare.add_argument('-c', '--compare', required=True)\n\n\tsubparsers.add_parser('compare_table', help='Compare a table.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser_compare.add_argument('table', choices=table_choices,\n\t\tdefault=None, nargs='?',\n\t\tmetavar='table', help='Table to compare')\n"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t)\n\n\t\tout_file = StringIO(test_configuration)\n\n\t\twith open(\"test_configuration\", 'w') as f:\n\t\t\tf.write(out_file.getvalue())\n\t\twith open(\"test_configuration_new\", 'w') as f2:\n\t\t\tf2.write(out_file.getvalue())\n\n\tdef test_merge_configuration(self):\n\t\t# We need to merge a config file to some old format\n\t\ttest_configuration_tmp = StringIO(test_configuration)\n\t\toutput = StringIO()\n\t\tself.assertTrue((True, 'test_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tnew_nodes = []\n\n\t\t# First we look for the actual dictionary elements.\t\n\t\tfor key, value in nodes.items():\n\n\t\t\t\t# Try to convert strings and booleans to booleans.\n\t\t\t\tif type (value) is str:\n\t\t\t\t\tnew_value = eval(value)\n\t\t\t\t\tnew_nodes.extend([(u'\\'${' + key + '}', new_value)])\n\t\t\t\telse:\n\t\t\t\t\tnew_nodes.extend(value)\n\n\t\t\t# Also if we are replacing a string with a special character, then\n\t\t\t# merge values seperately.\n\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n# We use the same format as the above code - the first one is a\n# backwards-compatible implementation.\nclass Borg:\n\n\tdef __init__(self, path, *args, **kwargs):\n\t\t# If using a path as str, then the current directory is assumed to be\n\t\t# a path without an extension.\n\t\tif isinstance(path, str):\n\t\t\ttry:\n\t\t\t\tdirectory, file_ext = path.rsplit('.', 1)\n\t\t\texcept ValueError:\n\t\t\t\tdirectory = ''\n\t\t\tfilepath = os.path.join(directory, path)\n\t\telse:\n\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t # TODO: rework for now...\n\tdef flatten_configuration_tree(mylist):\n\t\"\"\" Takes an unprocessed nested borgmatic configuration tree\"\"\"\n\treturn [(value.value_type, value) for key, value in mylist.items()]\n\n\tout = []\n\tif hasattr(yaml, 'RoundTripDumper'):\n\t\tout.extend(yaml.RoundTripDumper(indent=4).dump(flatten_configuration_tree))\n\telse:\n\t\tif hasattr(yaml, 'RoundTripLoader'):\n\t\t\tout.extend(yaml.RoundTripLoader(flatten_configuration"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n# FIXME: there's no actual dental implementation for merging borg-style\n\nclass CheetahTemplateYamlConfigParser:\n    \"\"\"Main parsing for the YAML Cheetah configuration data.\"\"\"\n\n    # FIXME: there's no actual dental implementation for merging borg-style\n    # configuration files into another. We'll need to parse a special\n    # configuration file so we can just parse it again if it's missing.\n    class YamlCheatConfigParser(YamlReadConfig):\n        def __init__(self, *args, **kwargs):\n            super(CheetahTemplateYaml"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\nimport yaml\nfrom yaml.errors import YAMLError\n\nif __name__ == \"__main__\":\n    configs = [\n        ('a=b,c=d'),\n        ('b=c,d=1,f,g,h=i,j=0'),\n        ('i=foo,bar,baz=3,g,x,y'),\n        ('None,None,None,1'),\n        ('s=s1,s2=s3'),\n        ('', ''),\n        ('{ a }=a,'),\n        ('a,b=c'),\n    ]\n    for config in configs:\n        print"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\"\"\"\nFor example, replace the \"dry-run\" section from \"dry-run\" into \"dryRun-config\":\n\n- config section:\n    dry-run: true\n\nTo see what type of configuration is the same as the default value, use the function\n:func:`deep_merge_nodes` instead.\n\"\"\"\n\n\"\"\"\n\tdef deep_merge_configs(nodes):\t# <node> may be None, but we want to do it\n\t# <node> may be <tag> (not a node tag) or <element> (only node tag)\n"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\nimport logging\n\nfrom rebulk import Rebulk\nfrom rebulk.pattern import (\n    AbstractPattern, Dict, LazyRebulk\n)\n\nfrom. import utils\n\n\n\n\n\n\n\nclass AbstractConfiguration(object):\t# inherit from Rebulk class\n\n\tlogger = logging.getLogger(__name__)\n\n\tdef __init__(self):\n\n\t\tself.rebulk = None\n\n\t\t# This is the object that we do not use to represent a given node.\n\t\tself.root_config = None\n\n\tdef load(self, config_file=None):\n\t\traise NotImplementedError()\n\n\tdef load_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_dictionaries(nodes) + deep_merge_nodes(nodes)\n\n#\n# YAML\n#\n\nclass SafeYamlConfigParser(SafeConfigParser):\n\t\"\"\"\n\tA specialised version of SafeConfigParser, that remembers unloaded contents which\n\tthe current context remains unchanged.\n\n\tUse a `SafeConfigParser.read`, to access the configuration options as a\n\tdict. This is equivalent to:\n\n\t\tparser.read_dict(content)\n\n\tinstead of:\n\t\t\tparser.readfp(content, filename=filename)\n\n\tThis is necessary because we"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn yaml.multimap(deep_merge_nodes, nodes, deep_settings)\n\ndef deep_merge_node(node):\t\t#pylint: disable=W0613\n\n\titems = []\n\tif node.tag not in node.attribs: \n\t\treturn items\n\tif node.attribs[node.tag] == 'keep_hourly':\n\t\titems.append(yaml.load(node))\n\telif node.attribs[node.tag] == 'keep_daily':\n\t\titems.append(yaml.load(node))\n\tif node.attrib and node"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=\"Script to parse a sequence of sequences of mate pairs within an alignment. This can then be generated using the MALLET library from Python scripts using the MALLET executable (http://mall.tumblr.com)\", prog=\"python mallet_script\", epilog=\"#\")\n\t\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", dest=\"verbose\",\n\t\t\t\t\t\thelp=\"be very verbose\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", dest=\"debug\",\n\t\t\t\t\t\thelp=\""}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n    parser = argparse.ArgumentParser(description=description, epilog=epilog, formatter_class = argparse.RawDescriptionHelpFormatter)\n\n    common_args = common_arguments()\n\n    # If --verbose is not enabled, then we will not print to stdout\n    if '-v' in common_args or '-vv' in common_args or '-vvv' in common_args:\n        # Verbose output should be disabled with -vvvv\n        parser.add_argument('-v', '--verbose', help='Turn on debug outputs -vvv')\n    if '-vvv' in common_args:\n        # Set verbosity based on the command line arguments"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\n\t\tparser = _fixargparse(parser, arguments, options)\n\n\t\tparser.error = _fixargparse(parser, arguments, options)\n\n\t\treturn parser\n\n\treturn parse_arguments\n\n# =============================================================================\n# Command line handling\n# =============================================================================\n\ndef getOpts():\n\t\"\"\" Parse the command line. \"\"\"\n\tparser = createOptionParser()\n\n\tparser.description = \"Runs the sieve of a Siesta and returns a plot of the sieve. Useful for plotting data without changing the plot width with the original. Does NOT attempt to parse data"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tif len(arguments)<1 or arguments[0]==\"help\": \n\t\tprint (\"Usage: $0 [-h] [-c] [-r] [-s] [-p] [-l] [--debug] [--config=config]\")\n\t\texit(-1)\n\t#parse command line arguments\n\ttry:\n\t\toptions, arguments = getopt.getopt(arguments,'hg:c:o:r:l:s:p:i:lc:',[\"help\"])\n\texcept getopt.GetoptError, err:\n\t\t# print help information and exit:\n\t\tprint (\"Missing options: \" +"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = get_arg_parser()\n\t\tparser.description = arguments[PHELP]\n\t\treturn parser\n\t\n\t# Get command line arguments\n\tglobal args\t\t\t# The current arguments\n\targs = parse_argument(*arguments[1:])\t# These can be parsed from command line args\n\t\n\tparser = get_arg_parser()\t# Get arg parser\n\targs = parser.parse_args(args + args[1:])\n\t\n\t# Make sure only command line arguments are returned\n\tif not (args.help and args.quit):\n\t\tparser.print_help()\n\t\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tparser = make_argument_parser()\n\tparser.add_argument('command', metavar='command', type=str, nargs='?', help='command to run')\n\tparser.add_argument('args', metavar='argument', nargs=argparse.REMAINDER, help='argument arguments to pass to the command')\n\treturn parser.parse_args(arguments)#!/usr/bin/python\n# -*- encoding: utf-8; py-indent-offset: 4 -*-\n# +------------------------------------------------------------------+\n# |             ____ _               _        __  __ _  __           |\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\n\t\t# Add subparser.\n\t\tparser.add_argument(\"command\", \n\t\t\tnargs='*',\n\t\t\tdefault=[], \n\t\t\thelp=\"command-line arguments to pass to %s\" % COMMANDS)\n\n\t\t# Add default arguments\n\t\tfor i in xrange(len(arguments)):\n\t\t\tparser.add_argument(arguments[i],\n\t\t\t\tdefault=arguments[i+1],\n\t\t\t\tnargs=DEFAULT_ARGS,\n\t\t\t\thelp=argparse.SUPPRESS)\n\n\t\t# Add arguments after the required arguments\n\t\tfor i in xrange(len(arguments)-1):"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n# Set up logging\nfrom logging.handlers import RotatingFileHandler\nfrom logging import Logger, Formatter\nfrom signalprocessing import RabbitMagic\nfrom collections import namedtuple\nfrom time import strftime, gmtime, strptime\nimport os\nimport re\n\n\n# Regular expressions to match the beginning of a string.\n_STRINGRE = re.compile(r\"[A-Za-z0-9\\$\\-]+\")\n\n# Make sure we have a directory named \"tools\"\ntools_path = lambda path: os.path.join(path, '..')\nassert tools_path(__file"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tif len(arguments)!= 3:\n\t\t\tprint 'Usage: {} <command-line> <arguments> <inputfilename>'.format(argv[0])\n\t\t\tprint 'Example: {} /path/to/myproject.py myprojectname.png'.format('\"output.png\"')\n\t\t\texit(1)\n\t\t# First argument is filename and second is the command-line arguments: split on whitespace\n\t\tsplitargs = [''] * len(arguments[0].split())\n\t\t# The first argument should be the directory of the argument\n\t\tsplitargs[0] = arguments[0]."}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\targs = ArgumentParser().parse_args(arguments)\n\t\treturn args\n\n\ndef print_versions():\n\tfor ver in version_to_parse():\n\t\tprint (\"{} {}\".format(ver_to_string(ver), ver))\n\n\ndef call_with_output(*args, **kwargs):\n\t\"\"\"\n\tWrap the call to run_output with an optional parameter to suppress printing output.\n\t\"\"\"\n\treturn run_output(*args, **kwargs)\n# Copyright 2012 OpenStack Foundation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\targs = parser.parse_args()\n\n\targ_strings = []\n\tfor i in args.args:\n\t\targ_strings.append(i)\n\n\treturn arg_strings\n\n\nif __name__ == \"__main__\":\n\tmain()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as ticker\nimport matplotlib.ticker\nfrom matplotlib import gridspec\nfrom matplotlib import rcParams\nimport matplotlib.animation\n\nplt.rc('font', family='serif')\n\nrcParams[\"xtick.major.size\"] = 8\nrcParams[\""}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\treturn parser.parse_args()\n\t\ndef arguments(argv=None):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, parse an argument list using an\n\targparse.ArgumentParser instance as the only argument.\n\t\"\"\"\n\n\tparser = argparse.ArgumentParser(\n\t\tconflict_handler=argparse.ArgumentDefaultsHelpFormatter, \n\t#  prog=args.prog,  epilog=args.epilog,  description=argv[0], epilog=None\n\t)\n\t\n\t# positional arguments\n\targpar = vars(parser.parse_args(argv))\n\t\n\t\nprint \"==============================================================\"\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn [\n\t\tOption(\"-b\", \"--bytes\", action=\"store\", help=\"parse FILE as many bytes as possible\"),\n\t\tOption(\"-n\", \"--numbers\", action=\"store\", dest=\"numbers\", metavar=\"N\",\n\t\t\thelp=\"number of times to extract text from files\")\n\t]\n\n\tdef parse(self, args=None, namespace=None):\n\t\t\"\"\" Parse the commandline arguments, and make sure we have everything we need.\n\t\t@param args: argument list to parse; if None, returns all the arguments. Otherwise\n\t\t\twe call parse with --help for usage.\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\n\t\t\t\t'modelfile',\n\t\t\t\tdefault='/data/opt/opt8',\n\t\t\t\tnargs = '?',\n\t\t\t\tmetavar  = 'FILE',\n\t\t\t\thelp      = 'The location to be read from')\n\t\tparser.add_argument(\n\t\t\t\t'output',\n\t\t\t\tnargs = '?',\n\t\t\t\tmetavar  = 'FILE',\n\t\t\t\tdefault = None,\n\t\t\t\thelp     = 'The file to write results')\n\t\tparser.add_argument(\n\t\t\t\t'-g',\n\t\t\t\taction ='store_true',\n\t\t\t\tdest    = 'G',\n\t\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\targs = parser.parse_args()\n\n\tflags = parse_flags(args.flags)\n\n\n\treturn flags\n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t# Use the built-in ArgumentParser API to avoid importing \"unused function\" errors\n\tfrom argparse import SUPPRESS_HELP\n\n\tclass CParser(object):\n\t\tdef __init__(self): \n\t\t\tself.group = HelpGroup(parser, \"C\", [])\n\t\t\tself.group.add_argument('-v', '--verbose', action ='store_true')\n\n\tparser = argparse.ArgumentParser(description=\"Creates a new package from an existing package or an existing module.\")\n\tparser.add_argument('-c', '--new', action=CParser)\n\tparser.add_argument"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument(\"--test\", dest=\"tests\",\ttype=str, default=None,\n\t\thelp=\"Path to file with tests to run, or None. If not provided, \"\n\t\t\"sys.argv for nosetests will be used.\")\n\t\n\tparser.add_argument(\"--run-parallel\", dest=\"runs\", action=\"store_true\", help=\"Run all jobs. If false, \"\n\t\t\"this script will use all cores, on the local machine to ensure all cores are used.\")\n\n\t# Parse the args and check if the user provided all the necessary flags.\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tdef get_flags(args):\n\t\t\"\"\"\n\t\tGiven a list of arguments, return a dictionary of argument names and\n\t\tvalues.\n\t\t\"\"\"\n\t\tkeys = None\n\t\tvalues = None\n\t\tfor arg in args:\n\t\t\tif arg in ('-s', '--source'):\n\t\t\t\tkeys = arg.split('=',1)\n\t\t\telif arg in ('-s', '--source-language'):\n\t\t\t\tkeys = arg.split(\"=\",1)\n\t\t\telif arg in ('-s', '--source-extension'):\n\t\t\t\tkeys = arg.split(\"=\",1)\n\t\t\telif arg in ('-s',"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t # TODO: handle 'f' and 'F' as kwargs and require these to be defined\n\t parser.add_argument('files', action='store', nargs='*', help='files to check, must be directories')\n\tparser.add_argument('--log-file', action='store', nargs='?', const=None,\n\t\thelp='name of log file (default: <logfile>.log)')#args.logfile is used to see if this is set to None\n\treturn parser\n# coding=utf-8\n#\n# Copyright 2014 Red Hat, Inc.\n# Copyright 2013 Hewlett-Packard Development"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t# Default flags\n\tflags = {\n\t\t'action'    :\tTrue,\n\t\t'action_option' :\t'a',\n\t\t'descr'     :\t\"as\",\n\t\t'descr_option' :\t\"aye\",\n\t\t}\n\t\t\n\t# Get the flags in the order in which the parser will be called\n\tflags_order = ['action','action_option','action_value']\n\tfor flag in flags.keys():\n\t\tflags_order.remove(flag)\n\t\tflags[flag] = getattr(parser.values, flag)"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tbash_completers = []\n\t\tfor arg_splitter in BORG_COMPLETIONS_PROMPT_SPLITTERS:\n\t\t\targs = arg_splitter(commandline)\n\t\t\tif len(args) == 0:\n\t\t\t\tlogger.error('The command line \"%s\" is empty. Command \"%s\" failed to run.' % (arg_splitter.cmd, commandline))\n\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\tbash_completers.append(args[0])\n\t\tif not bash_completers:\n\t\t\tlogger.error('The command line \"%s\" does not appear"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tcmd = '%s/borgmatic/bin/completion' % (cfg['borgmaticDir'])\n\t\tif cmd in cfg['commands']:\n\t\t\treturn cfg['commands'][cmd]['cmd']\n\treturn ''\n\ndef configure(cfg):\n\t\"\"\"\n\tReturn a bash configuration script for the borgmatic command. Produce this by\nreading a configuration file and executing it (creating the file). Return a string\n\tinterpreted as a (interactive) shell script.\n\t\"\"\"\n\t\n\t\n\tprint 'borgmatic configured script: %s' % cfg['borgmatic']\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\n\tborg_str = Borg('borgmatic')\n\n\t# borgmatic_cmd = 'borgmatic-'+borg_str.branch_name\n\tborg_cmd = get_borgmatic_full_cmd(borg_str.name, \n\t\t\t\t\t\t\t\t\t'bash -c'\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n\t\n\t# get the args for all 'bash' commands\n\t# this is useful for checking the \"no arguments are specified with '-q' in borg_cmd\n\tbash_args = borg_cmd.split(\" \")\n\tprint \"\\n\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\t\n\treturn r'''set -e \n\nrm borgmatic; touch borgmatic\n\nrm borgmatic_v23d.bat\n\nrm borgmatic.bat\n\nrm borgmatic_v23d.bat <<EOF\n'''\n\n# test commands\n\nprint(\"borgmatic_v23d.py\")\nbash_completion()\n\n############\n# Test that the bash completion code has the expected number of arguments specified\n\ncommands = r'''\nset -e\n\nrm -f *~borgmatic.txt\nrm -f *~borg"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\timport argparse\n\t\tparser = argparse.ArgumentParser(description='''Script that borgmatic command line utility functions must use''')\n\t\tparser.add_argument('-a','--all',action='store_true', help='Print all commands on one line of text'); parser.add_argument('-t','--threads',dest='threads',action='store',type=int,default=1,help='Specify number of threads for multi-threaded processing')\n\t\tparser.add_argument('-s','--source',nargs='+',help='Specify a source')\n\t\targs = parser.parse_args()\n\t\treturn args"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tfor arg in sys.argv[1:]:\n\t\t\tif(arg.startswith('-')==0 and arg!= '-m' and len(arg) < 13):\n\t\t\t\targ = arg.replace('-m','*');\n\t\t\targ = arg.strip()\n\t\t\t#print(arg)\n\t\t\t\n\t\t\t\n\t\t\tif (re.match('^([^%]*?)0*([0-9]+)/',arg) == None):\n\t\t\t\tprint(arg)\n\t\t\telse:\n\t\t\t\tprint(dir(arg))\n\t\t\t\tprint(cmd_completion_line(arg))\n\n\ndef get_completion_dict(file_"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\treturn 'borgmatic -h -t'+ BorgmaticCommandLineArgParser().commandline_for_borgmatic\n\t\ndef create_environment_variables(verbose=False):\n\t\"\"\"\n\tReturn a shell environment variable listing the borgmatic command and the current\n\tworking directory. The 'borgmatic_env' property holds the environment variables which\n\tare meant to be changed. This property is updated with any modifications on the\n\treturned context by passing in the -e or --show-env=value options through the configuration\n\tfiles.\n\t\"\"\"\n\tcontext ="}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tborgmatic_parser.parser.add_argument(\"--borgmatic_args\", required=True)\n\t\tborgmatic_parser.parser.add_argument(\"--debug\", dest=\"loglevel\",\n\t\t\t\t\t\t\t\t\t\tchoices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n\t\t\t\t\t\t\t\t\t\tdefault=\"DEBUG\")\n\t\tborgmatic_parser.parser.add_argument(\"-c\", \"--count\", action=\"store_const\", const=\"count\",\n\t\t\t\t\t\t\t\t\t\tdest=\"verbosity\", default=\"count\",\n\t\t\t\t\t\t\t\t\t\thelp=\"count verbosity\")\n\t\tborgmatic_parser.parser.add_argument(\"--"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=__doc__,\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t\t)\n\t\tparser.add_argument(\n\t\t\t'--version',\n\t\t\taction='version'\n\t\t)\n\t\treturn parser.parse_args()\n\ndef parse_args():\n\t\"\"\"\n\tParse command line arguments.\n\t\"\"\"\n\tparser = make_parser()\n\targuments = parser.parse_args()\n\n\t# Load borgmatic command\n\tborgmatic = CommandBorgmatic(\n\t\tcommand_name=arguments.command_name,\n\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\treturn'source %s && %s' %(opts.source, opts.target)\n\tpass\n\n#------------------------------------------------------------------------------\ndef bash_completion_and_path(verbose):\t\"\"\"\\\n\tReturn a bash completion and a path to the completion of the borgmatic command. Will raise an CommandError if completion cannot be found, else prints an error message.\n\t\"\"\"\n\tsys.stderr.write('\\033[31m[-] Completing borgmatic command\\033[0m')\n\tsys.stderr.write('source: %s\\ntarget: %s\\n\\n\\033["}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='')\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\t\n\tparser_all = subparsers.add_parser('all', parents=[parser], description='')\n\tparser_all.set_defaults(func=set_all)\n\n\tparser_all.add_argument(dest=\"subcmd\", metavar='subcmd', type=str, nargs='+', help=\"The sub-command for this function\")\n\tparser_all.add_argument('-n', '--count', action=\"store_true\", dest='count', help=\"Count the"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=description, epilog=\"Example: %s %s\" %\n\t\t\t\t\t(os.path.basename(__file__), sys.argv[0]), add_help=False)\n\t\tparser.add_argument('-v', '--verbose', dest=\"verbosity\", action='count', default=0, help=\"Increase verbosity of logs in.\")\n\t\tparser.add_argument('--debug', dest=\"debug\", action=\"store_true\", help=\"Debugging mode.  Default is off.\")\n\t\tparser.add_argument('--output', dest=\"output_dir\", default=None"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\ndef cmd_parse():\n\t\"\"\"\n\tGiven a configuration file generated by'make.py -h' command-line parameter, parse it and return\na dict mapping from Namespace types to Namespace objects, using the default Namespace type\nmapping found in make.py.\n\t\"\"\"\n\tparser = argparse.ArgumentParser( \n\t\t\tdescription=\"Tool for generating API documentation from API documentation. Can be called from outside or outside of a Python application.\",\n\t)\n\tparser.add_argument('--version', action='store_true', \n\t\thelp=\"print program's version number and exit\")\n\tparser."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n    # Parse arguments\n\targs = parse_arguments()\n\t\n\t# Parse command line arguments\n\targs.command = [sys.argv[0],]\n\t\n\t# Parse additional command options and parse their arguments\n\tfor opt in parsed_arguments[1:]:\n\t\tcmd, args = opt.split('=')\n        cmd = cmd.strip()\n        args = args.strip()\n\toptions = vars(parser.parse_args(args, namespace=args))\n\n\treturn options\n\n\nclass Cli(object):\n\t\"\"\"\n\tA class for interacting with the cli. The actual implementation uses the"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tfrom argparse import Namespace\n\t\tnamespace, unknown_args = get_args(unparsed_arguments)\n\n\t\targs = namespace, unknown_args\n\n\t\tcommands = [\n\t\t\targparse.ArgumentParser(\n\t\t\t\tprog=self.prog,\n\t\t\t\tadd_help=False)\n\t\t]\n\n\t\t# define default subcommands\n\t\tif args['action'] is None:\n\t\t\targs['action'] = self.help\n\n\t\t# parse the args with `sys.argv[1:]`\n\t\telse:\n\t\t\tfor cmd_name in args['action']:\n\t\t\t\tcmd = getattr(namespace, \"commands"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\timport argparse\t# Python 2\n\timport json\n\tfrom argparse import Namespace\n\timport datetime\n\n\t# Make sure we aren't using the correct version of the arguments\n\tparsed_arguments = ParseArgs([parse_file_in_arg(unparsed_arguments[0], args.command) for unused_i in range(len(unparsed_arguments))]\n\t\t\t\t\t\t\t\t\t  \t)\n\n\t# Load the json dictionary from the input string\n\tglobal_arguments_json_name = json.loads(parsed_arguments[1])\t\n\targuments = dict()\n\tfor k in global_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\t\n\t\tfor arg, val in unparsed_arguments.items():\n\t\t\tparser.add_argument(arg, **val)\n\t\t\n\t\treturn vars(parser.parse_args())\nfrom collections import OrderedDict\nimport sys\n\n\ndef _test(*_):\n    from libtbx import _utils\n    from libtbx.utils import (assert_raises_ex, _assert_invariants,\n                             _check_for_none,\n                             _assert_no_none,\n                             is_type)\n\n\ndef test_is_type():\n    for i in _test('_utils"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=description,\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t\tfor name, arg in unparsed_arguments.items():\n\t\t\tif not arg.startswith(\"-\"):\n\t\t\t\tparser.add_argument(name, arg, nargs='?', help=argparse.SUPPRESS)\n\t\tparser.set_defaults(func=do_command)\n\t\treturn parser\n\n\t# -------------------------------------------------------------------\n\t# Parse command-line arguments.\n\t# -------------------------------------------------------------------\n\tdef commandline_arguments(self, prog):\n\t\tparser = self.parser_factory(prog)"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\timport argparse\n\t\timport argparse\n\n\t\tparser = argparse.ArgumentParser(prog='unpkg-parse', description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)\n\t\tparser.add_argument = unparsed_arguments\n\t\treturn parser\n\n\n\tparser = parse_arguments(sys.argv)\n\ttry:\n\t\treturn args._parser\n\texcept AttributeError:\n\t\treturn None\n\t\n\n\n'''\ndef main():\n\tmain()\n\n\nif __name__ == '__main__':\n\tmain()\n\n\"\"\"\nAmazon S3 uploader that uploads images to Amazon S3 to replace old"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs = ArgumentParser().parse_args(_args)\n\t\tfor subparser_name, subparser_args in _args:\n\t\t\tif subparser_name in _argparser_name_dict:\n\t\t\t\tnamespace = _argparser_name_dict[subparser_name]\n\t\t\t\tparser = _argparser_class_dict[namespace]\n\t\t\t\tfor subargs in subparser_args:\n\t\t\t\t\tif subparser_args in parser.add_subparsers(subparser_name):\t\n\t\t\t\t\t\tfor key, val in vars(parser.parse_args(subargs)):\n\t\t\t\t\t\t\tparser["}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs = sys.argv[1:]\n\tversion_tuple = parse_args(actions_dict[args[0]])\n\topts = {}\n\ti = 0\n\twhile i < len(args) and version_tuple:\n\t\tp = args[i]\n\t\tif p in options_list:\n\t\t\tp = options_list\n\t\tif p in subcommands:\n\t\t\tdel args[i]\n\t\telse:\n\t\t\topts[p] = args[i]\n\t\t\ti += 1\n\t\t\targs.pop(i)\n\t# args should now hold parsed namespace"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tepilog = \"\"\"\nExamples:\n\n\t./argparse -r jason/cli\n\t./argparse -l jason/cli list\n\t./argparse -m jason/cli munch\n\t./argparse -r jason/cli --repository foo bar baz baz --type baz\n\t./argparse -r jason/cli list --repository foo\n\t./argparse -r jason/cli --repository foo --type baz\n\t./argparse -r jason/cli --repository foo bar --type baz\n\t./argparse -r jason/cli --repository foo --type bar --repository foo\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tresult = (\n\t\t# (args, namespace)\n\t(\n\t\t([\n\t\t\t('--repository', action_manager.subparsers.get_action_by_name('repository').register\n\t\t]),\n\t),\n\t# args\n\t(\n\t\t(['config', 'db']),\n\t),\n\t(\n\t\t([\n\t\t\t('--config-file', action_manager.subcommands.get_action_by_name('config.yml').get_config_path(),\n\t\t\t\"Configuration file path\"\n\t\t]),\n\t),\n# general\n\t(\n\t\t(['config',"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tresults = []\n\t\tfor argdict in unparsed_arguments.values():\n\t\t\tif argdict[\"dest\"] not in parsers:\n\t\t\t\tparser = ArgumentParser(description=argdict[\"description\"], parents=[argdict[\"dest\"]], add_help=False)\n\t\t\t\targument_actions = parser.add_subparsers(**argdict[\"parents\"])\n\n\t\t\t\tfor key, value in argdict[\"actions\"]:\n\t\t\t\t\targuments = key.split(\".\")\n\t\t\t\t\tactions = argument_actions[0].add_argument(value, **argdict[\"kwargs\"])\n\n\t\t\t\t\tif len(arguments)!= len(actions):\n\t\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n    # If arg_parser is in args, then parse that too. Else default to\n    # a parser instance (that will take one) if no subparser specified.\n    if len(args) == 0:\n        args = [ ]\n\n    if arg_parser_args:\n        for arg in arg_parser_args[0]:\n            if hasattr(arg, 'dest'):\n                args.append(arg.dest)\n        args.extend(arg_parser_args[1:])\n    else:\n        # Default to the same parser instance as no argument passed via\n        # parser (by default there's no need to pass"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\t# Extract the action\n\taction = subparsers.add_parser(\"action\", help=\"Execute actions of this action\")\n\taction.add_argument(\"args\", \n\t\thelp=\"The action arguments, each argument will be added to the namespace in the result.\")\n\taction.set_defaults(func=execute_action)\n\taction.add_argument(\"--repository\", \n\t\thelp=\"The command-line or configuration for the action repository\")\t\t\t\t\t\n\t# TODO: what action do we want?\n\t# Parse the remaining arguments (at a minimum)\n\tremaining_arguments = parser_sub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tdef add_usage(subparsers, dest_parsers, parents):\n\t\t\"\"\"A helper to add some common usage messages for argparse.\"\"\"\n\t\t\n\t\tepilog = textwrap.dedent(\"\"\"\n\t\t\n\t\tAvailable subparser types: %(valid_subparsers)r\n\t\t%(invalid_subparsers)s\n\t\t%(no_args_message)s\n\n\t\t\"\"\" % {'valid_subparsers': ', '.join(subparsers), 'invalid_subparsers': '\\n'.join(reversed(subparsers))})\n\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=dest_parsers.get('description', ''),\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tdef parse_action_arguments_from_subparsers(subparsers, parent_group_name, repo_group_name, action_name, repository_type):\n\t\tactions = {}\n\t\tfor name, kwargs in actions_namespace[repository_type].items():\n\t\t\tif name in subparsers.actions:\n\t\t\t\tactions[name] = subparsers.actions[name]\n\t\t\telif name in subparsers.actions:\n\t\t\t\tactions[name] = subparsers.actions[name]\n\t\t\t\n\t\treturn (actions, {}, {})\n\n#################################################\n#-*- coding: utf-8"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\tfrom optparse import OptionParser\n\tfrom rez.exceptions import RezBindError\n\tfrom os.path import basename as F\n\tfrom pprint import pformat\n\t\n\tclass Error(RezBindError):\n\t\t\"\"\"\n\t\tRaised when a repository error occurs. The error must be assigned the\n\t\tfirst time to run the command.\n\t\t\n\t\tIf this error was caused due to a bind/parse error during the run, this\n\t\tis the place to do the right thing with error checking the repository.\n\t\t\"\"\"\n\t\t\n\t\tdef __init__(self, repo_path):\n\t\t\tRezBindError.__"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\t# First, parse the arguments and initialize an argparse ArgumentParser.\n\tpargs = _parse_args(unparsed_arguments, [], _subparsers)\n\n\t# Now parse all arguments and use their values to create a namespace to use with argparse.ArgumentParser.\n\targs, remaining_args = parser.parse_known_args(pargs)\n\n\t# Then parse all of the arguments. The results will include a namespace\n\t# for all sub-parser invocations.\n\n\trepository = {}\n\t\n\tfor arg, value in remaining_args.items():\n\t\t# If we have a namespace we"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = optparse.OptionParser(\n\t\tusage=\"usage: %prog [options] inputfile outputfile\")\n\tparser.add_option(\"-a\", \"--all\", action=\"store_true\", dest=\"all\", default=False, \n\t\thelp=\"Generate all source files and header files\")\n\tparser.add_option(\"-a\", \"--headers\", metavar=\"N.H\", dest=\"headers\", nargs=\"+\", \n\t\thelp=\"The first N.H header must contain N.H header files.\")\n\tparser.add_option(\"-b\", \"--header\", metavar=\"N.H\", dest=\""}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Convert an Xlsx and XML file to HTML/XML.')\n\tsubparsers = parser.add_subparsers()\n\tsubparsers.required = True\n\t\n\tparser_make_xml = subparsers.add_parser('make_xml',\n\thelp='Create HTML/XML test report')\n\tparser_make_xml.add_argument('infile', help='Path to Xlsx input file.')\n\tparser_make_xml.add_argument('outfile', help='Path to XML test report.')\n\tparser_make_html"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\t# Build the argument parser and define the function object\n\targuments = docopt(__doc__, version=VERSION)\n\n\targuments.add_argument('host', action='store', nargs=1, metavar='remote host port')\n\t\n\targuments.add_argument('-S', action='store', dest='socket_timeout', type=int, default=60)\n\n\tparser = subparsers.add_parser(name='webdav', parents=[arguments], description='Run webdav server')\n\tparser.add_argument('-H', action='store', dest='debug', action='store_true')\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\t# Set the command line option\n\tparser = argparse.ArgumentParser(description='Test the CLI')\n\tparser.add_argument(\"-l\", \"--list\", action='store_true', dest=\"list\", help=\"List available plugins\")\t\t\n\tparser.add_argument(\"-p\",\"--profile\", action='store_true', dest=\"profile\", help=\"Profile the client\")\n\tparser.add_argument(\"-r\",\"--remove\", action=\"store_true\", dest=\"remove\", help=\"Remove existing plugin\")\n\tparser.add_argument(\"-l\",\"--list-plugins\", action=\"store"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\treturn _make_parser_parser().parse_args()\nfrom __future__ import unicode_literals\n\nimport datetime\nimport json\nimport re\n\nfrom django.contrib.admin.templatetags.admin_static import static\nfrom django.contrib.admin.templatetags.admin_urls import add_preserved_filters\nfrom django.contrib.admin.templatetags.admin_urls import change_list_order\nfrom django.contrib.admin.utils import (\n    display_for_field, display_for_value, label_for_field, lookup_field,\n)\nfrom"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n# ------------------------------------------------------------------------------\n# Class that handles the command-line arguments, which will\n# be consumed by argparse (as with argv or file-type modules)\n# If no arguments are provided, use argparse\n# Parse Command\nif __name__ == '__main__':\n    # Define the ArgumentParser and parse the arguments\n    if len(sys.argv)!= 3:\n        sys.stderr.write(\"Error: You needed to specify an command\\n\")\n        parser.print_help()\n        sys.exit(2)\n    # Parse the arguments, for the help subcommand and its help function\n    args = _cmdline()"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the CUDA CUDB test suite.\"\n\t\t\t  \t\t  \t      \t\t)\n\tparser.add_argument(\"-d\", \"--disable-cleanup\",\n\t                help=\"remove created files after the test completes\",\n\t                action=\"store_true\"\n\t                )\n\tparser.add_argument(\"--cleanup\", \"--clean\",\n                        help=\"clean up created files\",\n                        action=\"store_true\"\n                        )\n\tparser.add_argument(\"--check\", \"--check-logs\",\n                        help=\"check the logs for errors\",\n                        action=\"store"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\t# Define a new argparse formatter\n\tdef format_help(self):\t\n\t\t\"\"\"\n\t\t\tThis function will return the help for the available commands and\n\t\tthe usage of the command for which that command description is printed.\n\t\t\"\"\"\n\t\n\n\t\tdef add_header(name, description, epilog=None):\n\t\t\t\"\"\"\n\t\t\tAdd the help to the header, making use of the description and\n\t\t\tepilog strings.\n\t\n\t\t\tThis method will add the help into the help string for those commands,\n\t\t\tin the same format as add_usage can be used to determine the\n\t\t\tcommand usage"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\t# The main method to the main parse loop.\n\tdef parse(self):\n\t\t#print \"parse\"\n\t\tif(self.args.input):\n\t\t\tself.inputFilename = self.args.input\n\t\telif(self.args.output):\n\t\t\tself.outputFilename = self.args.output\n\t\telif(self.args.input!= None):\n\t\t\tself.inputFilename = self.args.input\n\t\telif(self.args.output!= None):\n\t\t\tself.outputFilename = self.args.output\n\t\t\n\t\tself.parse_file() \t\t\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tfrom argparse import _check_argument_types\n\n\t\tparser = argparse.ArgumentParser(description=\"Automate the output of automate-output.\")\n\t\tparser.add_argument(\"-d\", \"--automate-output\", action=\"store\", type=str, dest=\"output_path\", help=\"The output file.\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"If not provided, an output will be generated.\"\"\")\n\t\tparser.add_argument(\"-j\", \"--dry-run\", action=\"store_true\", dest=\"dry_run\", help=\"Run the process and exit\")\n\t\targs = parser.parse"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\titems = response.xpath(util.nspath_eval('f:SearchResponse/*/f:Items/f:Item'))\n\tif not items:\n\t\treturn\n\tfor item in items:\n\t\tif item.length!= 10:\t# don't return too long\n\t\t\treturn\n\t\tpaging_result = util.nspath_eval('f:SearchResponse/*/f:Page/' + util.nspath_eval('f:Items/f:Item/' + util.nspath_eval('f:ItemName')), True, True)\n\t\t# We first check whether the"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n        for obj in response:\n\t\tyield obj\n\t\nWAPI2_ENDPOINT_BASE = \"/wapi2/url/paging\"\t # api url for paging\n###############################################################################\n# This file is part of openWNS (open Wireless Network Simulator)\n# _____________________________________________________________________________\n#\n# Copyright (C) 2004-2007\n# Chair of Communication Networks (ComNets)\n# Kopernikusstr. 5, D-52074 Aachen, Germany\n# phone: ++49-241-80-27910,\n# fax"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\tfor i in range(max_results):\n\t\tapi_call = get_next_page_api_call(i)\n\t\tnext_page_api_call = get_next_page_api_call(0)\n\t\tresponse.extend(api_call['response'])\n\t\tresponse.extend(next_page_api_call['response'])\n\n\treturn response\n\ndef main(argv=None):\n\t\"\"\"main function\"\"\"\n\n\tif argv is None:\n\t\targv = sys.argv\n\n\tparameters = docopt(__doc__)\n\n\tif parameters[\"action\"]["}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\ndef _get_paging_data(p: str) -> str:\n\t\"\"\"\n\tUtility function to make Request object to paginate.\n\t\n\tArgs:\n        p (str): Input parameters.\t\t\nReturns:\n    str: Queries params.\n\t\"\"\"\n\t# Get data.\n\treturn {'paging': p, 'pagingType': 'page', \n\t\t\t'nextPageToken': p + 1, 'cursor': '0', 'previousLink': '0', 'total_count': '0','result_count': '0',\n\t\t\t'select_page': p + 1"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tfor i in range(1, max_results):\n\t\t\tyield response\n\nclass Response(object):\n    \"\"\"Generates a single page of objects of various types and values.\n\tCurrently supports JSON responses.\n\t\"\"\"\n\tdef __init__(self, resp, content, http.client, status, headers=None):\n\t\tself.content = content\n\t\tself.status = status\n\t\tself.content_type = 'application/json'\n\t\tself.headers = headers or {}\n\t\tself.body = self.json_body(resp, content)\n\t\tself.http_headers"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n        if(response.status_code == 200):\n                response_status_code = response.status_code\n\t\t\t\n                if(response_status_code==200):\n                        response_content = create_page(response)\n                        try:\n                                while (response_content.status_code<2000):\n                                        page2 = response_content.read()\n                                if(\"WAPI Response content\" not in page2) or(\"WAPI Response error response\" not in page2):\n                                        page2 = response_content.read()\n                                        continue\n\n                                        json_data = json_loads(page2)\n                                        json_data ="}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t\n\tpaging_result = None\n\tfor result in response:\n\t\ttemp_page = result.next_page\n\t\tif temp_page is not None:\n\t\t\t# If next page isn't the last one, set paging result as null object.\n\t\t\t# Else iterate through the rest of results as long as there are not >2 results in one page and the\n\t\t\t# last one of the objects was the next page.\n\t\t\t# If paging result is not less than max_results, increment paging result as one object\n\t\t\t# Also, adjust next page as paging result for"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    if max_results < 0:\n        max_results = 1\n    \n    offset = 0\n    if max_results > (8 * max_results):\n        offset = max_results - 8 * (4 - 1)\n        \n    for i in range(max_results):\n        response.append([offset, \"\", \"\", 0])\n        response.append([\"Page\", \"\", \"\", 1])\n        response.append([\"Content Type\", \"text/html\", \"text/html\", \"application/xhtml+xml\", \"application/xml\"])\n        offset = offset + len(response[0])\n        \n    return(generate_response(response, offset"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t\t\n\t# Get objects from results\n\tobjects = []\n\t\n\t# Check for paging\n\ti = 5\n\twhile i <= max_results:\n\t\tpage = wapi.get_page({'pagingList': {'page': page_num}})\n\t\t\n\t\t# Print a message\n\t\tpprint.pprint(page, width=25)\n\t\tnum_pages = 0\n\t\tif len(page['pages']) == 0:\n\t\t\tpprint.pprint(page, width=25)\n\t\t\ti = 0\n\t\telse:\n\t\t\tnum_pages = page['pages_count']\n\t\tpprint"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n@ndb.transactional()\ndef find(request):\n\t\"\"\"\n\tWAPI:Find;\tRequest\n\tDescription:\n\t\tThis will provide a full page, list of objects and properties.\n\t\t\tIt returns array of object search results.\n\tExample usage:\n\t\t1. Find(name=Foo, age=9, location=Street, url=http://www.mappoint.co-city.edu/roads/marblets for Lake Wood, CA )\n\t\t2. Find(name=\"Bernard\", age=12)\n\t\t3."}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tsize = size.upper()\n\tif size[0] == '-':\n\t\tsize = size[1:]\n\tdecimal = 0\n\twhile size:\n\t    decimal += int(size.replace(\",\", \"\")) *.0001\n\t    size /= decimal\n    decimal = int(decimal)\n    \n    return decimal\n\n\n\n\n\n_logLevel = LOG_INFO[0]             # Log level of operation\n_logFormat = None                 # Name of file format\n_logDate = datetime.datetime.today().strftime(_logLevel) # Log date\n_logLevel = LOG_INFO[1]             # Log level of operation"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tdecimal = 0\n\t\tfor value, name in re.findall(r\"(\\d+)([BkMGT]?)\", str(size)):\n\t\t\tdecimal += value\n\t\t\t\n\t\treturn int(decimal)\nfrom. import *\n\nfrom. import *\nfrom. import constants\n\nimport os\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nimport xml.etree.ElementTree as ET\n\nclass XMLParser(object):\n  \"\"\"\n  This class is the base class for parsing xml to XML files.\n\n  It should be used with the C++ parser and can be used in both"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ts = str(size)\n\t\tg = s.split(\".\")\n\t\tif len(g)!= 2:\n\t\t\traise ValueError(\"Size must have exactly two digits!\")\n\t\tfor i in range(1, len(g[0]) + 1):\n\t\t\tif i!= len(g[0]):\n\t\t\t\traise ValueError(\"Size should start with a 0!\")\n\n\t\tif size[-1]!= \"000\":\n\t\t\traise ValueError(\"Invalid value for size. Should be: '000'\")\n\n\t\tif size[-7] < 50:\n\t\t\traise ValueError(\"Invalid size value. Should be: '"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tdef _str_int(size: str, sign_decimal: bool) -> str:\n\t\t\trpt = Decimal(size.replace('\\\\', ''))\n\t\t\tif abs(rptr.as_tuple().exponent) >= 2:\n\t\t\t\texponent = '-0.' + str(rpt.as_tuple().exponent)\n\t\t\telse:\n\t\t\t\texponent = '0'\n\t\t\treturn f\"{exponent}\" + str(rpt.as_tuple().to_integral_value()) +'' + exponent\n\t\treturn _str_int\n\n\t@classmethod\n\tdef _parse_int(cls,"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n        size_in_bytes = size_to_bytes(size)\n        if size_in_bytes <= 10:\n\t\treturn \"%dB\" % (size_in_bytes / 1000)\n\telif size_in_bytes <= 100:\n\t\treturn \"%.1f\" % (size_in_bytes / 1000)\n\telse:\n\t\treturn \"%.0f\" % size_in_bytes\n\nclass Filesize:\n    \"\"\"\n    Class with a basic size and a simple format on the logfiles.\n    \"\"\"\n\n    def __init__(self, size: float, format=None):\n        self"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn str(int(size[:-2]))\n\t\t# return \"%3.1f\"%(bytes / pow(1024,size[-1]/1024))\n\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n#!/usr/bin/env python\n# encoding: utf-8\n\n# Copyright (C) 2010\n#     Gabes Jean, naparuba@gmail.com\n#     Gerhard Lausser, Gerhard.Lausser@consol.de\n#     Gregory Starck, g.starck"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n        _size_byte = int(size)\n        decimal_point = size - len(_size_byte) / pow(10, 10)\n\n        return '{0:.2f} k'.format(decimal_point)\n\n\n\"\"\"\nUtility functions for manipulating raw data and loading into the database\n\n\"\"\"\n\nCACHE = {}\ndef convert_raw(data: dict, table_data: dict, date_columns: list) -> bool:\n    \"\"\"\n    Takes a data dictionary and converts it to a raw data dictionary.\n    The data is a list with data and the header is the column label,\n   "}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\t# strip \"k\"\n\t\tif size[-2:]==\"k\":\n\t\t\tsize = size[:-2]\t\n\n\tsize_split = size.rsplit(\" \")\n\tif len(size_split)!= 2:\n\t\traise ValueError(\"Incorrect size format\")\n\tif size_split[0] == \"k\":\n\t\treturn long(size_split[1])\n\telse:\n\t\treturn long(size_split[1])\n\nif __name__ == \"__main__\":\n\tmain()\n\n'''\nCreated on Dec 26, 2015\n\n@author: GaoR"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n\n\t\tif not size:\n\t\t\traise ValueError(\"No size specified and unable to calculate size of \"\n\t\t                \"file. Either supply a file size explicitly or use.get \"\n\t\t                \"size = '.000K'\")\n\t\tsize = int(size.strip(\"mM\"))\n\t\tif size <= 0:\n\t\t\traise ValueError(\"no size given and unable to calculate size of \"\n\t\t\t                \"file. Either supply a file size explicitly or use.get \"\n\t\t\t                \"size=\" + size)\n\t\tif size >= 1048576:\n\t\t\treturn Decimal(size, 2)\n\t\treturn Decimal("}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tp = float(size)\n\t\tif (p >= 0.0):\n\t\t\treturn round(size * p, 1)\n\t\telse:\n\t\t\treturn 0\n\traise TypeError(\"size argument must be a number: (1K)\") # pragma: no cover\n\n\nclass Misc:\n\t\"\"\"\n\tSimple interface for basic operations\n\t\"\"\"\n\tdef __init__(self, verbose: bool = False):\n\t\tself.verbose = verbose\n\n\tdef check_unique_name(self, name: str) -> bool:\n\t\t\"\"\"\n\t\tCheck if the supplied file ID exists already"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\td = {}\n\t\tfor d in dicts:\n\t\t\td.update(d)\n\t\treturn d\n\n#--- REGULAR METHODS ---\n\ndef find(value, candidates):\n\t\"\"\"\n\tSearches an item's dict for values that it is most similar to value.\n\tReturns the key that is least similar to value if found.\n\t\"\"\"\n\tfor k, v in candidates.items():\n\t\tif v == value:\n\t\t\treturn k\n\treturn None\n\ndef find_last(iterable, candidates):\n\t\"\"\"\n\tReturns the last item in an iterable"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tlists = ['[']\n\t\tfor dict in dicts:\n\t\t\tlists.append(dict[0] + '=' + dict[1][0])\n\t\tfor list in lists:\n\t\t\tn = '%d' % (len(list))\n\t\t\tif len(list) > 1:\n\t\t\t\tn = '[%s]' % n\n\t\t\tprint(n, end=' ')\n\t\treturn lists\n\nif __name__ == '__main__':\n    import sys\n    a = [1,2,3,4,5,6,7]\n    print([dict(a, i) for i in a"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tn = len(dicts)\n\t\tsum = 0\n\t\tprev = None\n\t\ttotal = 0\n\t\tfor d in dicts:\n\t\t\tsum += d\n\t\t\tif prev == d:\n\t\t\t\ttotal += 1\n\t\t\t\tprev = d\n\t\t\telse:\n\t\t\t\tprev = d\n\t\t\ttotal += prev\n\t\tif last is not None:\n\t\t\tsum += last\n\t\t\ttotal += last\n\t\t\tsum += last\n\t\t\tprev = d\n\t\treturn {'total': total,'sum': sum}\n\n\n\ndef _dictminus(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(dicts, ())\n\telse:\n\t\treturn reduce(dict._dictsum, dicts, [])\n\n\n\n#####################################################################\n# String representation of dictionary data\n\n# class for basic string representations like str, bytes and\n# other iterable objects\n\nclass _String(object):\n\tdef __init__(self,_data):\n\t\tself._data = _data\t\t\n\t\tself._data_len = len(_data)\n\t\tself._str_types = types.StringTypes\n\t\t\n\tdef __repr__(self):\t\t\n\t\tldata = {}\t\n\t\tfor keystr, data in"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\t\n\tresults = []\n\tfor index, value in enumerate(dicts):\n\t\tif len(value)!= 0:\n\t\t\tresults.append((dictsum, value[index]))\n\t\telse:\n\t\t\tresults.append(dictsum)\n\t\t\t\t\n\treturn results\n\t\ndef _listsum(list):\t\"\"\"\n\tCombine values of the list supplied by iterable list.\n\t\n\t>>> _listsum([1,2,3])\n[1, 2, 3]\t\"\"\"\n\tnew_list = []\n\tfor index in range(len(list)-1):\n\t\tif len("}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tr = {}\n\t\tfor k,v in dicts.iteritems():\n\t\t\tr[k] = reduce(add, v, {})\n\t\treturn r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\t# I just started with the loop that's failing, but this was causing a few bugs.\n\treturn sum([d[1] for d in dicts])\n\n\n################\tTesting data structures ################\n################\nimport pandas as pd\nfrom garage.utils import file_reader as file_reader\nimport logging, os #@UnresolvedImport\n\nclass PfamUtils():\n\t# Class used for testing PfamReader (i.e. PfamUtils)\n\tdef get_input_types(self, input_types):\n\t\t\"\"\"\n\t\tExtracts input type definitions from a csv file"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tdictrep = []\n\t\tfor d in dicts:\n\t\t\tvalue = d.copy()\n\t\t\tif isinstance(value, defaultdict):\n\t\t\t\tvalue = list(value.items())\n\t\t\telif isinstance(value, list):\n\t\t\t\tif len(value) > 0:\n\t\t\t\t\tif isinstance(value[0], basestring):\n\t\t\t\t\t\tif value[0].lower() == 'true':\n\t\t\t\t\t\t\tvalue = True\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tvalue = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tvalue = [int(d) if len(d) == 1 else d for d in value]\n\t\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\n\"\"\"\n    Tests for Blockstore-based XBlock.\n\n    See /xblock_config/xblock.py for more information.\n\"\"\"\nimport unittest\nimport ddt\nfrom uuid import uuid4\n\nfrom xblock.field_data import DictFieldData\nfrom xblock.fields import ScopeIds\n\nfrom xblock.test.tools import (\n    assert_equals, assert_raises, assert_not_equals, assert_raises_rpc_error,\n    assert_raises_json, assert_raises_rpc_error, assert_true, assert_false\n)\nfrom xblock.test.tests import TestRuntime"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\t\n\t_keys = ', '.join(sorted(dicts.keys()))\n\t_values = list(dicts.values())\n\t\n\treturn '%s(%s)'%(\n\t\t(type(self.lists[0]).__name__,)+'.'.join(_keys),\n\t\t_values)\n\t\ndef _items(dicts, keys):\n\t\"\"\"\n\tAggregate all the values of several dictionaries by the given keys.\n\t\n\tCombinatorial is performed like \"items\", but using the key name for any \n\tvalue.\n\t\"\"\"\n\t_dict1 = {}\n\t_dict2 = {}\n\t\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor arg in url_args:\n\t\t\targ_found = False\n\n\t\t\tif is_url(arg):\n\t\t\t\targ = arg.split(u'/')\n\n\t\t\telse:\n\t\t\t\t# no url found\n\t\t\t\tif debug:\n\t\t\t\t\tprint(u'no custom arg to replace: %s' % arg)\n\t\t\t\targ_found = True\n\n\t\t\tif arg_found:\n\t\t\t\ttry:\n\t\t\t\t\turl_obj[arg] = get_arg(arg)\n\n\t\t\t\texcept KeyError:  # this is optional!!!! check\n\t\t\t\t\t# if the arg is not in url_dict then\n\t\t\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tresult = []\n\t\tfor i in range(0, len(url_args)):\n\t\t\tresult.append(url_args[i] + url_args[i+1])\n\t\t\tif i!= len(url_args) - 1 and i!= 0:\n\t\t\t\tresult.append(url_args[i+1] + '=' + url_args[i])\n\t\treturn''.join(result)\n\treturn url_re.sub(_replace_url_args, url)\n\n#\n# A simple command-line parsing class that allows us to run any of\n#"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif isinstance(url, str):\n\t\t\turl_args = urlparse(url)\n\t\t\turl_args_list = parseUrlArgs(url_args)\n\t\t\turl = urlparse(url)\n\t\t\tfor key in url_args_list:\n\t\t\t\tif key == 'query_string':\n\t\t\t\t\turl_args_dict = urlparse(url_args)\n\t\t\t\t\turl_args = dict()\n\t\t\t\t\tfor key in url_args_dict:\n\t\t\t\t\t\tif key == 'path':\n\t\t\t\t\t\t\turl_args[key] = url_args_dict[key]\n\t\t\t\t\t\telif key == 'args"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif url_args:\n\t\t\ti_url_args = iter(url_args)\n\t\t\tif isinstance(i_url_args, list):\t# if len(i_url_args) == 1 and i_url_args[0]:\n\t\t\t\ti_url_args = i_url_args[0]\n\t\t\tfor url_arg in i_url_args:\n\t\t\t\tif isinstance(url_arg, str): \n\t\t\t\t\turl_arg = url_arg.decode(\"us-ascii\")\n\t\t\t\tfor url_seg in url_arg.split('/'):\n\t\t\t\t\tif"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\treturn replace(_url, url_args, '\"')\n#!/usr/bin/env python2.5\n#\n# Copyright 2010 the Melange authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#   http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\t#if args is an empty dict return original text\n\t\t#url_args = {}\n\t\t#try:\n\t\t\t#url_args.update(url_args)\n\t\t\t#if len(url_args) > 0 and str(url_args[0]).endswith(\"/\"):\t#if exists, append / if not, convert /\n\t\t\t\t#\targs_converted = list(args)\n\t\t\t\t#if converted!= str(args_converted):\t#we don't even try to go any further\n\t\t\t\t#\tprint \"replace\", url, args\n\t\t\t#except Exception:\n\t\t\t#"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif url in url_args and isinstance(url_args[url], basestring):\n\t\t\tif url_args[url]!= DEFAULT_URL_PATTERN:\n\t\t\t\turl_args[url] = DEFAULT_URL_PATTERN % url_args\n\t\t\treturn url + url_args[url]\n\t\telse:\n\t\t\traise ValueError(\"Invalid url: '%s'\" % url)\n\n\treturn urllib.parse.urlunparse(url_args)\n\ndef _encode_multipart_formdata(fields, files):\n\t\"\"\"\n\tEncode each value in the multipart/form-data payload into a multipart"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif hasattr(url, \"items\"):\n\t\t\tfor i in range(len(url.items)):\n\t\t\t\t_replace_url_args(url.items[i][1:], url_args[i])\n\t\telse:\n\t\t\tif \"#\" not in url_args:\n\t\t\t\ttry:\n\t\t\t\t\t# Python 3\n\t\t\t\t\tfrom urllib.parse import urlparse\n\t\t\t\texcept ImportError:\n\t\t\t\t\t# Python 2\n\t\t\t\t\tfrom urlparse import urlparse\n\t\t\t\ttry:\n\t\t\t\t\t# Python 3\n\t\t\t\t\tfrom urllib.error import URLError\n\t\t\t\texcept ImportError:\n\t\t\t\t\t# Python 2\n\t\t\t\t\tfrom urllib2 import URLError\n\n\t\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\turl_args = [escape(arg) if arg is not None else str(arg) for arg in url_args]\n\t\treturn url_concat(url, url_args)\n\treturn url._replace(xpath, replace_text)\n\nfor path, title in [(\"link\", \"link\"), (\"description\", \"description\")]:\n\tpage = frappe.new_doc(\"Page\", {\"page_title\": title})\n\tpage.path = frappe.utils.get_url(path)\n\tpage.set_show_table(True)\n\tpage.load_links()"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\treturn url_args.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;'), url\n\treturn url\n\ndef create_user (username, password, domain):\t\n\t\"\"\"\n\tGet a user object\n\t\"\"\"\n\t#Create an empty post request\n\tparams = {\"password\":password,\"domain\":domain }\t\t\n\treturn requests.post(url=u\"https://api.reddit.com/user\", params=params)\n\ndef get_author(username):\n\t\"\"\"\n\tGet a particular user\n\t\"\"\"\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\t# str(bool) is True if val is none.\n\t\t# Note that'str(False)' is not 'True'.\n\t\treturn not val.strip()\n\telse:\n\t\ttry:\n\t\t\treturn validate(val)\n\t\texcept (ValueError, TypeError):\n\t\t\treturn False\n\n\ndef is_string(val: Any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn isinstance(val, str) or isinstance(val, unicode)\n\n\ndef split_list_string(val: Any) -> List[str]:\t\"\"\""}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif not isinstance(val, str):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn to_bool(val)\n\t\n\n\tdef check_type(val: Any) -> bool:\t\"\"\"\n\tCheck if a value represents a string of any type.\t\"\"\"\n\t\tis_none_string(val)\n\n\tdef check_type_list(val: Optional[Iterable[Any]]) -> None:\n\t\t\"\"\"\n\t\tCheck that values are a list of strings.\t\"\"\"\n\t\tif not isinstance(val, list):\n\t\t\treturn\n\n\t\tfor item in val:\n\t\t\titem ="}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn is_none_type(val) or val is None\n\n\ndef get_module_context() -> None:\n\t\"\"\"\n\tReturn the context containing all of the module global variables\n\n\t:return: the context\n\t\"\"\"\n\tctx = dict(\n\t\t__builtins__ = globals(),\n\t\t**_MODULE_CONTEXT  # copy of the globals dict to avoid repeated lookups\n\t\t)\n\n\tfrom frappe.sandbox import context\n\tctx.update(context_processors.__dict__)\n\treturn ctx\n\n\n_standard_modules: Dict[str, type] = {}\n\ndef"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, string_type)\n\n\ndef is_iterable(val: Iterable) -> bool:\t\t\n\t\"\"\"\n\tCheck if a iterable (string or list) represents an untyped sequence of values.\n\t\"\"\"\n\treturn isinstance(val, list)\n\n\ndef flatten(obj: Any) -> Union[str, Iterable]:\n\t\"\"\"\n\tFlattens values or tuples recursively.\n\t:param obj: Any instance, like a sequence, list or tuple.\n\t:return: An iterator where flattened values are converted from iterable into strings.\n\t\"\"\"\n\tif isinstance"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, six.string_types)\n\n\tdef format(self) -> str:\n\t\t\"\"\"\n\t\tFormat the value according to the formatting rules.\n\t\t\"\"\"\n\t\tif self.min_width is not None and len(str(selfmin_width)) < 7:\n\t\t\tself.min_width = '16pt'\n\t\tif self.max_width is not None and len(str(self.max_width)) < 7:\n\t\t\tself.max_width = '16pt'\n\t\tif isinstance(self.min_value, six.string_types"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tmatch = re.match(r\"^\\s*(None|False|True|False)\\s*\", val.strip())\n\t\tif match and match.groups()[0].strip()!= \"\":\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\n\t@staticmethod\n\tdef extract_value(key_value: Any, key_str: str, key_int: int):\n\t\t\"\"\"\n\t\tGet the specific value if it exist or return None.\n\t\t\"\"\"\n\t\tif not bool(key_value) and not bool(key_str):\n\t\t\treturn None\n\t\tif not bool"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn is_nonestr(val) or isinstance(val, str)\n\n\ndef dict_constructor(name: str) -> Any:\n\t\"\"\"\n\tCreate a dictionary from a dictionary, given a `dict` which may be a\n\tdeserialized dict.\n\t\"\"\"\n\tassert is_none_string(name)\n\td = {}\n\tif name in reserved_names:\n\t\t# We support the following syntax:\n\t\t# 'a' becomes:\n\t\t#      a = { b0 }\n\t\t#      {} becomes:\n\t\t#      c = { a:"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is None or (isinstance(val, string_types) and len(val) == 0)\n\treturn check_bool(val)\n\nclass Config(ValueSpec):\n\t\"\"\"Configuration data for the configuration option of ``yedit``.\n\tThese are options that only apply to configs which do not correspond to a specific option, but do are not allowed to be\n\tlisted here, as plugins may define it to other Config subclasses.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tself.default"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str)\n\telse:\n\t\treturn type(val) is str\n\ndef is_not_boolean(val: bool) -> bool:\t\"\"\"\n\tCheck if a string, representing a boolean value.\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() in ['true', 'false']\n\ndef is_not_empty_string(val: bool, allow_empty: bool) -> bool:\t\"\"\"\n\tCheck if a string, representing a non-empty value.\n\n\tIf the type of the val is 'empty', it"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn is_not_empty_val(self, val, \"empty string\")\n\n\n#-----------------------------------------------------------------------------\n# Internal utility methods\n#-----------------------------------------------------------------------------\n\n# NOTE: the functions here use these two functions seperately,\n# in order to improve performance, they are available in several\n# places and we should probably refactor the implementation in a separate\n# module instead...\n\n# TODO: the linter should be implemented as a factory for the linter\n# factory function, as there are multiple ways in which a file is\n# opened.\n# A better implementation would probably require extending the\n# linter factory"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t# This works best on Python 2.6+\n\tdef string_types(prefix):\n\t\treturn dict((x, k) for k, x in globals().iteritems() if k.startswith(prefix))\n\t\n\t\n\treturn sorted(string_types(\"char\")) - sorted(string_types(\"ushort\"))\n#!/usr/bin/env python\n\"\"\"\nThis example shows how to use the `wradlib.as_matrix()` function.\n\nTo load it using this module, specify it with a ``load_module()``\nkeyword argument:\n\n::\n\n    from wradlib import"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\ndef command(opt):\t\"\"\"Parse a command line argument\n\t\n\tReturns:\n\t\t(options, args)\n\t\"\"\"\n\tparser.add_argument(\n\t\t'-H',\n\t\tdest = 'hostname',\n\t\taction ='store',\n\t\tmetavar = 'HOST',\n\t\ttype = str,\n\t\tdefault = '127.0.0.1',\n\t\thelp = 'Hostname for web server.')\n\tparser.add_argument(\n\t\t'-p',\n\t\tdest = 'port',\n\t\taction ='store',\n\t\tmetavar = 'PORT',\n\t\ttype ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n    @parser.register_argument(\"-f\", \"--filter\", help=\"Filtering mode, defaults to \" +\n\t\t\"'<mode> = <filter>. Only items which are to be filtered are \" +\n\t\t\"returned\",\n\t\tdefault='filter')\n\t@parser.add_argument(\"-e\", \"--exclude\", help=\"Exclude these items and exit\",\n\tdefault='',\n\t\tnargs=\"*\",\n\t\t)\n\t@parser.add_argument(\"-d\", \"--debug\", help=\"Print all log messages\",\n\tdefault=False,\n\t\taction=\"store_true\" )\n\t@parser"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = {\n\t\t\t'help': '[options] -h ',\n\t\t\t'default':    '[options] -e ',\n\t\t\t'version':    '[options] -v ',\n\t\t}\n\t\tp = {\n\t\t\t'setup':    None,\n\t\t\t'setup2':    None,\n\t\t\t}\n\t\tfor k in flags:\n\t\t\tif k!= 'help' and flags[k] is not None:\n\t\t\t\tp[k] = flags[k]\n\t\tif p['version'] is not None:\n\t\t\tp['version'] = p['version'].replace"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n \n\tparser.add_argument('-v','--version', help = 'Display version information', action ='version', version = version.getVersionInfo(sys.argv[0]))\n\tparser.add_argument('-s','--silent',   action ='store_true', help = 'If specified, any existing output files in the output are deleted.', dest ='silent')\n\n\t#parser.add_argument('-C','--nocolor',  action ='store_true')\n\n\t#parser.add_argument('-v', '--version_type',  type = str, dest ='version_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser._flag_actions[0].arguments.__iter__()\n\n\tdef set_flag(self, flag):\t# pylint: disable=unused-argument\n\t\t\"\"\"\n\tSet the default value of the passed flag.\n\tThe argument is represented by a dictionary mapping flag name (a list of\n\tstrings) to a flag-like value.  An optional second argument is passed into the\n\tparser in place of a flag.\n\t\"\"\"\n\tparser._flag_actions = [[]]\n\tparser._flag_arguments = flag_arguments(self.parser)\n\n\tdef add_flag"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tdef p(args):\n        \tfor idx, arg in enumerate(args):\n\t\t\tif arg.startswith('--'):\n\t\t\t\tp.arg('Required argument #%d found. Ignored.' % idx)\n\t\t\telse:\n\t\t\t\tp.arg(arg, nargs='-'+str(argparse.OPTIONAL))\n\t\t\t\tp.arg(arg, help='Required argument #%d found.' % idx)\n\n\n\n\ndef get_commands(args):\n\tif not isinstance(args, dict):\n\t\traise TypeError('Command argument is not a mapping.')\n\tcmds = []\n\t\n\tfor name in args:\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument('--outputMode', metavar='MODE', default=0,\n\t\thelp='This parameter specifies how to write the data to the file. '\n\t\t'Some modes accept an argument that is then evaluated at each time. '\n\t\t'It is similar to {+FLAGS.{default,min,max}}. '\n\t\t'All modes accept an argument that is set from one to many. This is '\n\t\t'really only necessary for the programmatic use case where all the '\n\t\t'args specified are integer. If you are passing an argument to a '\n\t\t'function"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t # Initialize an argparse.ArgumentParser instance.\t\t\n\t parser.add_argument('--help',\n\t\t     action='store_true', dest='help',\n\t\t     help='Display this help message and exit.')\n\n\t# Initialize an argparse.ArgumentParser instance.\t\n\tdef parse_args(args):\t# For parsing help message.\n\t\tparser.print_help()\n\t\tsys.exit(1)\n\t\n\t# Parse the arguments.\n\targs = parser.parse_args(args)\n\treturn args\n\n# Parse the arguments.\nargs = parser_flags(parser)\n\nprint \"This"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tparser.add_argument(\"-t\", \"--time\", dest=\"timestring\", default=\"%Y-%m-%d %H:%M:%S\",\n\t\t     help=\"Simulation time [format: mm/dd/yy]\")\n\t\t\n\tparser.add_argument(\"-a\", \"--amplitude\", dest=\"amplitude\", type=float, default=1,\n\t\t     action= \"store\", help=\"Amplitude [1-6]\")\n\t\t\n\tparser.add_argument(\"-v\", \"--vheight\", dest=\"vheight\", type=float, default=1,\n\t\t     action= \"store\", help=\"Vertical Height ["}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # Check if the given path name has already been processed\n    if processed.isdisjoint(path_name) == False:\n\t\n    # If not, add the entry to the set\n\tis_excluded = True\n\n    while is_excluded:\n\t     # If we have processed two items in the set (do not continue),\n\t     # we need to add a new entry to the set.  This will be\n\t     # uncommented later.\n\t     if len(processed) > 1:\n\t\t     # We have two items in the set\n\t\t# For simplicity, append the entry, and remove the"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # print_verbose('Processing %s..' % path_name)\n    # Paths should be absolute, but the absolute paths should\n\t# be converted to absolute paths.\n\t\n\tparts = processed.partition('/')\n\tif parts[2]:\n\t\tif verbose:\n\t\t\tprint_verbose('Processing entry: %s' % path_name)\n\t\t# Pathname cannot have a trailing '/'\n\t\tif not parts[2]:\n\t\t\treturn False\n\t\t# Get relative, so relative paths may be represented as '..'.\n\t\trelative_path = '/'.join( parts[2:] )\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # If a entry exists then it's not yet processed\n    if path_name in processed:\n\t\n    # We want this path\n    return False\n    \n\n\ndef recurse_on_file(path_name, verbose):\t\n\t\"\"\"\n\tRun recursively and display all files and folders that match the\n\tprovided path.\n\n\tArgs:\n\t\tpath_name: Path to a directory\n\t\tverbose: True if verbose output is requested\n\t\"\"\"\n\tfor root, dirs, files in os.walk('.'):\t\n\n\t\t# We want this path\n\t\tfound_entries = []\n\n\t\tfor file in"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if processed:\n\t# Already in the set \n\treturn False\n\t\n\t# Make sure we haven't processed it already\n\talready_processed = False\n\t\n\tfor entry in processed:\n\t    if entry.startswith(path_name):\n\t\t# The path exists\n\t\talready_processed = True\n\t\tcontinue\n\n\t    if os.path.isdir(entry):\n\t        # A directory\n\t        shutil.rmtree(entry)\n\t        continue\n\n        processed.add(entry)\n\n\treturn already_processed\n\t\n\ndef filter_files(path):\n    \"\"\"\n    Filter file paths using glob or globq, which"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\n\tfor x in processed:\n\t\tcurrent = os.path.join(path_name, x)\n\n\t\tif ( not os.path.exists(current) or not os.access(current, os.R_OK)) :\n\t\t\treturn False\n\n\treturn True\n\n\ndef is_python_file(name, test):\n\tname = os.path.splitext(name)[0]\n\treturn test in ('.py') and not os.path.isdir(os.path.join(name,'resources', name))\n\n\ndef get_config_path(conf):\n\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\t# this means test if a file was already processed\n\t\t# if not processed, just move on\n\n\t# we have two options: process and path\n\t# (if found) and a prefix\n\t# (if not)\n\n\tif len(processed) > 1:\n\t\t# handle duplicates\n\t\tif path_name in processed:\n\t\t\treturn True\n\t# file name exists.. call this function\n\telse:\n\t\treturn False\n\n\ndef process_path(path):\n\t\"\"\"\n\tProcess the file or file name with a filter for a given prefix.\n\t\n\tIf it begins"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # Check if the name of the entry already ends with a number\n\t# greater than two, just return False\n\t# (the path name + path_name). If 2 < lenghts\n\t# or > 2, just return False\n    stripped_name = stripped_name.rstrip('.txt')\n\tif len(processed) > 2 and stripped_name[0]!= 'f':\n\t\treturn False\n\t\n\tif len(path_name) < 2:\n\t\tfor name in processed:\n\t\t\tif path_name.endswith(name):\n\t\t\t\treturn False\n\t\t\t\n\tif any"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if path_name in processed:\t\t# Already has it so raise an error\n        return True\n    else:\n        try:\n            file_obj = file(path_name)\n            # Verify files\n            assert file_obj.isvalid()\n        except AssertionError:\n            # Check whether it's a directory\n            return False\n        return not was_processed(file_obj, path_name, verbose)\n\ndef find_duplicates(a, b):\t\t# Returns unique elements in a without duplicates\n\tduplicates = []\n\n\t# Check if a contains a duplicate.\n\ta_set1 = set()"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\ndef get_files_and_dirs(dir_path):\n\t\"\"\"\n\tObtain the absolute path of all the directories \n\tto a file or directory.\n\n\tArgs:\n\tdir_path: Path to a directory\n\treturn: set of all the files and directories in path\n\t\"\"\"\n\t\n\t#TODO - find it more efficient in a function\n\n\tall_files = set([])\n\ttotal_files = 0\n\n\t#walk through all paths\t\n\twhile dir_path!= 'root':\t\n\t\tdir_children = os.walk(dir_path)\n\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    from collections import defaultdict\n    assert path_name not in processed ## WTF?\n##\tassert is_valid_directory(path_name), '%s is not a valid path name' % str(path_name)\n        \n    for entry in processed:\n        if verbose: print 'Checking ', entry\n        # Looking for path_name, recursively checking\n        path_to_check = path_name + ':' + entry\n        if not os.path.exists(path_to_check) or \\\n\t\t    (entry in processed) or \\\n\t\t    (entry['path'] == path_to_check):\n            \n            # Print"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\ta = 0\t\t\t\t\t\t\t# vertex number\n\t\tb = 1\t\t\t\t\t\t# the two vertices\n\n\t\tt = (vertices[0][0] * vertices[0][0]) / 3.0\n\t\tx = (vertices[1][0] * vertices[1][0]) / 3.0\n\t\ty = (vertices[2][0] * vertices[2][0]) / 3.0\n\t\tz = ( vertices[3][0] * vertices[3][0]) / 3.0\n\n\t\t# point to new vertex\n\t\tp = (x*y + y*z"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\tA = (vertices[0][0], vertices[0][1])\n\tD = (vertices[1][0], vertices[1][1])\n\n\tB = (vertices[0][0], vertices[0][1])\n\tC = (vertices[1][1], vertices[1][0])\n\n\tCDA = (vertices[0][0], vertices[0][1])\n\tCDB = (vertices[0][1], vertices[1][0])\n\t\n\tDAB = (vertices[1][0], vertices[1][1])\n\tDAC = ("}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tn1, n2 = vertices\n\t\tA, B, C, D, A2, B2 = vertices\n\n\t\t# The second vertex contains no points (except for the third)\n\t\tA = (A2 + A) >> 1\n\t\tB = fmod(A, n1)\n\t\tc = fmod(B, n1)\n\t\td = fmod(C, n1)\n\t\ta = fmod(A2, n2)\n\t\tb = fmod(B2, n1)\n\n\t\treturn 3, n1, n1 + n2,"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\treturn (vertices[0][0], vertices[0][1], vertices[0][0]+vertices[1][0], vertices[1][1])\n\n\ndef polygon_xytuple_2D(polygon):\t\"\"\"\n\treturn 2 points for each segment of the polygon. This will include the segment and the 2 points on the 3 sides.\n\n\t>>> from grendel.gmath import polygon_xytuple_2D, point3tuple\n\t>>> print polygon_xytuple_2D((0,0),(1,1))\n\t(0.0,0.0)\n"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tv1=(vertices[0]-vertices[2]).as_tuple()\n\t\tv2=vertices[1].as_tuple()\n\t\tv3=(vertices[1].as_tuple()+vertices[2]).as_tuple()\n\t\treturn (v1, v2, v3)\n\n\tdef getVertFromSpline(self,spline):\n\t\t\"\"\"\n\t\treturn the vert a line through a spline line \n\t\tNOTE:  the spline must be closed.  if not, then it will return [None,-1].  otherwise it will return [[None,0], [None"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tC = np.zeros((1,3))\n\t\tfor verts1 in vertices[0 : 3]:\n\t\t\tC[verts1] = Vertex(verts1)\n\t\tfor verts2 in vertices[1 : 3]:\n\t\t\tC[verts2] = Vertex(verts2)\n\t\treturn C\n\n\tdef vertebent(a, b, c):\n\t\t\"\"\"\n\t\tCalculate how much we're going to divide the geometry from the vertices\n\t\tThis would be a good place to do the work of intersection\n\t\tand return the sum of every 3 points and 2 points"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tnpoints=len(vertices)\t#number of points in polygon\n\t\tv = vertices[0]\t\t#top vertex\n\t\tv1 = v\t\t\t\t\t#bottom vertex\n\t\tv2 = vertices[1]\t\t#right side vertex\n\t\tv3 = v1\t\t\t\t\t#left side vertex\n\t\tnpoints_v=len(vertices)-2*v2+2*v3\t#number of points of the polygon\t\n\t\tnpoints=npoints_v\t#create list with the outer vertices\n\t\tv_a = vertices[0]\t\t#vertex id"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tvertices = (vertices+vertices.tolist())/.5\n\t#A    C    D\n\t#B    D\n\t#C    D\n\t#D\n\t#A\n\t#B\n\t#C\n\t#D\n\t#A\n\t#B\n\t#C\n\t#D\n\treturn (vertices,vertices,vertices,vertices)\n\ndef vertex3tuple2(vertices):\t\"\"\"\n\treturn 2 points for each vertex of the polygon.\n\npolygon with vertices ABCD\nwill return\nDAB, ABC, BCD, CDA -> returns 2"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tif len(set(vertices)) < self.max_sides_per_side :\n\t\t\treturn\n\t\tpoints = [[0,0,0] for i in range(self.v)]\n\t\tfor v1,v2 in zip(vertices[0::2], vertices[1::2]):\n\t\t\tpoints[0][0] += v1[0]\t\t\t# add vertex UBV v1 and VBV v2\n\t\t\tpoints[1][1] += v1[1]\t\t\t# add vertex UBV v1 and VBV v2\n\t\t\tpoints"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tA = len(vertices)//3\n\tB = len(vertices)//3\n\tC = len(vertices)%3\n\t\n\t#calculate point\n\tP = [[A for j in range(A)] for i in range(A)] \n\tP = [P[i//3] for i in range(B)]\t\n\t\n\treturn P\t\n\n\t\n\t\n\t\ndef polylist(polylist, num_points):\t\t\"\"\"\n\tgiven a list of points and a list of polylist\n\treturn a list, in descending order of points\n\tA list of x,y tuples with the following"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif padding is None:\n\t\t\tpadding = number * 10\n\n\treturn alphabet[min(padding, len(alphabet) - number)]\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, FileField, DecimalField\nfrom wtforms.validators import Required, Length, Email, Regexp, EqualTo, Lengths\nfrom wtforms_sqlalchemy import Model\nfrom wtforms.ext.sqlalchemy.orm import model_form\n\n\nclass NameForm(FlaskForm):\n    name = StringField('What is your name?', validators=[Required()])\n    submit = SubmitField('Submit')\n\n\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif not number:\n\t\t\treturn ''\n\t\tif encoding <= 10:\n\t\t\tencoding = 0\n\t\tif encoding > 0 and ord(number) - encoding > (len(alphabet) - 1):\n\t\t\treturn alphabet[number] + alphabet[len(alphabet) - 1]\n\t\tif padding is not None and number!= 0:\n\t\t\tif len(padding) - len(alphabet) > padding and len(alphabet) - padding <= (len(alphabet) - len(number) - 1) * padding:\n\t\t\t\treturn alphabet[len(alphabet) - count - padding - alphabet_index("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tint_chars = ''\n\t\tdigit_cnt = 0\n\t\tfor n in alphabet:\n\t\t\tint_chars += n\n\t\t\tdigit_cnt += 1\n\tif padding:\n\t\tint_chars += '+-' * padding\n\tint_str = str(int_chars)\n\treturn int_str\n\n\ndef string_to_int(string: str, alphabet: List[str] = None) -> int:\n\t\"\"\"\n\tConvert a string to an int, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tstring_chars ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\ttry:\n\t\t\treturn alphabet[(number >> shift) % len(alphabet)] + alphabet + alphabet\n\texcept IndexError:\n\t\tif not padding:\n\t\t\tpadding = \"0\"\n\t\treturn \"0\"\n\n\ndef string_to_int(string: str, length: int) -> int:\n\t\"\"\"\n\tConvert a word or a string to an integer encoded with the given length.\n\n\tThe given string will be converted to an integer using this encoding.\n\tIf the length of the string is smaller than the length in characters, the\n\tnumber is padded.\n\t\"\"\"\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif padding is None:\n\t\t\t# Padding is just to make sure we return a string like \"0\"\n\t\t\tdata = \"%d\" % number\n\t\telse:\n\t\t\t# Padding is padded to make sure we pad \"0\" to the right by padding\n\t\t\tdata = \"{0:d}{1:d}\".format(number, padding)\n\t\treturn data\n\nclass Message:\n\t\"\"\"A base class for all messages in Slack.\n\t\"\"\"\n\t# A list of ignored_words. These are:\n\t#\n\t#   * None\n\t#     *"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\treturn ''.join(\"\".join(alphabet[:number])\n                       if alphabet else alphabet)\n\n\ndef random_string(alphabet: str=None: List[str], padding: Optional[int] = None) -> str:\n\t'''\n\tGenerate a random string.\n\n\t:param alphabet: a generated string, or None.\n\t:param padding: The number to pad the string to, so that it can be of any length.\n\t:returns: The generated string.\n\t'''\n\tnumber = random.randint(1, len(alphabet))\n\tif number == 0: return ''\n\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\ts = str(number).zfill(alphabet + [padding])\n\t\tif len(s) > length(alphabet):\n                        raise ValueError(\"The length of the output string is longer than the alphabet.\")\n                        if padding:\n                                return s[0:padding]\n                        else:\n                                return s\n\ndef bitstring_to_int(bitstring: BitString) -> int:\n        return int.to_bytes(bitstring, byteorder='big') if isinstance(bitstring, BitString) else bitstring\n\ndef validate_bitstring(b, bitstring: BitString) -> bool:\n        \"\"\"\n       "}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\n\t\n\tif not number: return \"\"\n\n\tstring = \"\".join([alphabet[int(number / 10)], pad_chars(number)])\n\treturn string\n\ndef int_to_alphabetical(number: int) -> str:\t\"\"\"\n\tConvert a number to a string in which all the ASCII equivalent\n\tASCII characters have been used to indicate each of their alphabets.\n\nThe output has the most significant number of each character in its\nalphabet. This means that each character is equally likely to be a valid \ndigit (1 to 9 digits). The digits are then put at"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tinteger = 0\n\t\tbits = []\n\t\twhile integer < len(str(number)):\n\t\t\tbits.append(alphabet[number % 8])\n\t\t\tinteger = integer + 1\n\t\treturn ''.join(bits)\n\ndef hex(number: int) -> str:\n\treturn ''.join([\"{:02x}\".format(digit) for digit in unicode(number)])\n\ndef addBytes(string:'str', bytes: Optional[Sequence]) -> str:\n\t# Note: this currently only adds one byte at a time; the byte\n\t# itself can be any Python string (eg"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif padding is None:\n\t\t\tpadding = \"\"\n\t\t\tfor char in number[::-1]:\n\t\t\t\tpadding += \" \"+char\n\t\t\tpadding = \"\".join([\"0\" for x in range(len(padding))]).zfill(padding)\n\n\t\treturn str(number)[:padding]\n\t\n\t### MATCHING ###\n\tdef match(self, pattern: str, string: str) -> bool:\n\t\t\"\"\"\n\t\tMatch a string to a match.\n\t\t>>> match(\"aabbc\", \"aab\", \"a\")\n\t\tmatches True\n\t\t>>> match(\"abcacac\", \"abcbb"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif'register_number' not in flow_params.keys():\n\t\t\traise Exception(\"No flows have been defined for '%s'\" % register_number)\n\t\telif register_number!= register_value:\n\t\t\traise Exception('Register number doesn\\'t agree\\\n\t\t\t    to register value: %d vs %d' % (register_number, register_value))\n\t\telse:\n\t\t\tflow_params['register_number'] = register_number\n\t\treturn\n\telif register_number < len(flow_params['flows']['register']):\n\t\treturn\n\n\t# For each existing flow ("}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tflow_params[register_number] = register_value\n\n\t@classmethod\n\tdef __setitem__(cls, register_number, register_value):\n\n\t\tkey = cls._get_key(register_number)\n\t\tif key not in flow_params.__dict__:\n\n\t\t\tflow_params[register_number] = register_value\n\n\tdef replace_register_value(self, value):\n\t\t\"\"\"\n\t\tReplaces register value from all existing keys in flow_params\n\t\tin-place (which still exists in flow params)\n\n\t\t:param value: Value to be replaced\n\t\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tregistered_flows_dict = {}\n\t\tregistered_flows_dict.update({register_number:register_value})\n\t\tflow_params[register_number] = register_value\n\ndef extract_param_and_value_from_params(param_name, param_value, flow_params):\n\tparam_value = _replace_register(flow_params, param_name, param_value)\n\treturn param_value\n\ndef _extract_register(flow_params, register, register_number, register_value):\n\tregister_value = _replace_register"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tglobal _REGISTER_MAP\n##\tprint \"Replace with register %s, number: '%s'\" % (register_number,register_value)\n\t\tfor _key in _REGISTER_MAP.iterkeys():\n\t\t\tvalue,mod_number = _REGISTER_MAP[Register(register_number, register_value)]\n\t\t\tFLOW_PARAMS[value] = mod_number\n\tdel _REGISTER_MAP[Register(register_number,register_value)]\n\treturn FlowParams(flow_params)\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n#"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_number not in flow_params.keys():\n\t\t\tfor i in range(0,register_number):\n\t\t\t\tflow_params[register_number][i] = register_value\n# Author: echel0n <echel0n@sickrage.ca>\n# URL: https://sickrage.ca\n#\n# This file is part of SickRage.\n#\n# SickRage is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tfor i in range(register_number, len(flow_params)):\n\t\t\tflow_params[i] = register_value\n\n\ndef register(flow_params, register_number, register_value, debug=False):\n\t\"\"\"\n\tReplaces a flow with registers\n\n'register_number' key in dictionary will be replaced\nby register number\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n\tparameter: list of tuples of ("}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_number not in flow_params.keys():\n\t\t\treturn \"\"\n\t\n\t\tif register_value not in flow_params[register_number]:\n\t\t\tflow_params[register_number][register_value] = \\\n\t\t\t\tregister_value\n\t\telse:\n\t\t\treturn flow_params[register_number][register_value]\n\n\ndef _replace_load(flow_params, load_number, load_value):\n\t\"\"\"\n\tReplace value from flows to given normal load number\n\n'reload_value' key in dictionary will be replaced by load number\ngiven by 'load"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tdef_value = flow_params.get(register_number)\n\t\t# if the parameter is None, delete the existing entry\n\t\tif def_value is not None:\n\t\t\tdel flow_params[register_number]\n\n\tdef _get_register_value_for_prefix(flow_params, prefix):\n\t\t# Find register by prefix and replace with flow_params value\n\t\t# (if present) to given prefix + ':'\n\t\treturn flow_params.get(prefix + ':' + register_number, None)\n\n\tdef _replace_register_for_prefix(flow,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t# Remove regulators that use the same field name multiple times\n\t\tif register_number in flow_params:\t\t\t\n\t\t\t# First search through flow params\n\t\t\tindex = i = 0\n\t\t\twhile i < len(flow_params[register_number]):\t\n\t\t\t\t# search through flow params\n\t\t\t\tif reg_name in flow_params[register_number][i]:\n\t\t\t\t\tindex = i\n\t\t\t\t\tbreak\n\t\t\t\telif reg_num_to_get(flow_params[register_number][i], register_number):\n\t\t\t\t\tif register_value == int(flow_params[register_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tfor step in flow_params:\n\t\t\tif step[register_number] == register_value:\n\t\t\t\tnum = step[register_number]\n\t\t\t\tdel flow_params[num]\n\ndef _load_model_file(filename):\n\t\"\"\"\n\tLoad the model file\n\n:param filename: The name of the model file\n\t:type filename: str\n\n\t:return: The loaded model object\n\t:rtype: m3t.core.tmg.tmg_model.TmgModel\n\t\"\"\"\n\ttry:\n\t\treturn TMG3Tmg."}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tfor k in arg:\n\t\t\tv = arg[k].replace(arg[k],'')\n\t\t\tif not re.match('\\\\s*', v):\n\t\t\t\tnew_str = value+str(v)\n\t\t\t\targ[k] = new_str\n\t\treturn arg\n\treturn value\n\n\t\t\n# Check if the command exists or not. \n\tdef is_command(self, name): \n\t\treturn self.commands.get(name,'NotExist') or self.commands.get(name,'NoExist') or \\\n\t\t\t\tself.commands.get(name,'Nonexistent') or \\"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn arg.replace('.', '_').replace('-', '_')\n\n\tdef quote(str_):\n\t\"\"\"\n\tConverts the given string into a unicode string for use in RSS feeds\n\t\"\"\"\n\treturn xml.sax.saxutils._unescape(str_).replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#39;', \"'\").replace(\n\t\tr'[^;\\n\\r]+', '&#x0026;').replace('&quot;', '\"')\n\ndef add_namespace_to_cmd(command_name):\n\t\"\"\"\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif type(value) is unicode:\n\t\t\treturn re.sub('\\\\d+|^\\\\.', '_', value)\n\t\tif type(arg) is unicode:\n\t\t\targ = arg.encode('ascii', 'ignore')\n\t\tvalue = _unicode_encode(arg, True)\n\t\tif type(value) is dict:\n\t\t\tret = dict((k.encode('ascii', 'ignore'), v) for k, v in value.iteritems())\n\t\telse:\n\t\t\tret = unicode(value)\n\t\tassert isinstance(ret, str)\n\t\treturn ret\n\ndef unicode_encode("}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn str(arg).replace('.','').replace('*','').strip()\n\n\ndef get_local_id(host, source=\"mysql_database\"):\n\tsql = \"\"\"\n\t\tSELECT `master` FROM `%(db_host)s`.`master_id` \n\t\tWHERE `master` ='master' AND `source` = '%(source)s'\n\t\t\"\"\"\n\tif source == \"mysql_database\":\n\t\tresult = run_mysql_query(conf.mysql_query, sql, as_dict = True)\n\t\treturn result and result[0] or ''\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn re.sub(r'(\\.|_)*$','', value).lower()\n\n\t@staticmethod\n\tdef expand_path_to_listformat(path):\n\t\t\"\"\"\n\t\tExpands a path to each entry of the entry-source file with the \n\t\texpanded entry in the path list format, so that it is a valid \n\t\tvalue for the \"format\", not the raw value used within the entry.\n\t\t\n\t\tThis function converts string entries to a list for use later.\n\t\t\"\"\"\n\t\tlist = list()\n\t\t# Create an entry object with a list of string entries for each entry"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tfor i in value:\n\t\t\tif i not in PATH_TERMINALS + string.digits:\n\t\t\t\treplace(i, arg)\n\treturn arg\n\n# \tThis is the core of a module in the v-subversion programmer who are inspired from the Python standard library.  So if we import the module as if it was\n# \t\"from module import *\":\n#\n#\n# \tfrom module import *\n#\ndef find_package(name, search_path=None):\n\t\"\"\"\n\tChecks if the given name can be imported in"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tnew_args = []\n\t\tindex = 0\n\n\t\tfor c in re.split('\\$', value):\n\t\t\tif c == '.':\n\t\t\t\tnew_args.append(result)\n\t\t\t\tindex = index - 1\n\t\t\telse:\n\t\t\t\tnew_args.append(c)\n\n\t\t\tindex += 1\n\n\t\tnew_args.append(value)\n\n\t\treturn ''.join(map(lambda c: result.replace(c, '.', 1), new_args))\t\n\treturn value.replace('.', '\\\\')\n\ndef remove_quotes(search, value, is_keyword=False):\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tvalue = value.replace('.', arg);\n\t\treturn value\n\n\tdef is_file(value):\t\t# Test if filename is a string instead of a file\n\t\tif not os.path.isfile(value):\n\t\t\treturn False\n\n\t\treturn os.path.isfile(value)\n\tdef replace_dot(value, arg):\t\t# Replace all dots in path\n\t\tif not os.path.splitext(value)[1] in ('.txt','.pdf'.lower()):\t\t# Add dot\n\t\t\tvalue = value + '.'\n\n        return value.replace('.', arg)\n\n\n\n#\n#"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif'' in arg and ')' in arg:\n\t\t\treturn arg.split(' ')\n\n\t\treturn arg.split('.')\n\n\ndef load_json(filename, default_flow_style=False):\n\t\"\"\"\n\tLoads a JSON file, creating and opening it. Assumes the encoding specified appears in the given json file.\n\n\t**Required argument**\n\n\tfilename\n\t\tThe JSON file to load\n\n\t**Optional argument**\n\n\t**Optional argument**\n\n\t**Optional argument**\n\n\t**Return**\n\tThe JSON object with the loaded data\n\t\tif the filename was a JSON file\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\ti = 0\n\t\twhile i < len(value):\n\t\t\tif value[i]!= '.':\n\t\t\t\tbreak\n\n\t\t\tif arg[i]!= '.':\n\t\t\t\ti += 1\n\t\t\t\targ = arg[:i] + arg[i+1:]\n\t\t\t\tvalue = value.replace(arg, '')\n\t\t\telse:\n\t\t\t\ti += 1\n\t\t\t\targ = ''\n\n\treturn value\n\n\n\n\"\"\"\nmainline\n\nInitiates each of the supported command in this file.\nEach command is itself the main function of this file.\n\"\"\"\ndef main(argv):\n\tglobal output_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = cls.__subclasses__()\n\t\tfor subclass in subclasses:\n\t\t\tyield subclass\n\treturn subclasses(getattr(mixins(), cls.__name__.lower()))\n\ndef all_subclasses(klass):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\n\t\"\"\"\n\tall_subclassess = subclasses(klass)\n\twhile subclasses.__subclasses__():\n\t\tsubclasses_list = [\n\t\t\tc for c in all_subclassess\n\t\t\tif not issubclass(c, subclasses_list)\n\t\t\tand issubclass(subclasses_list, subclass_that)\n\t\t]\n\t\tsubclasses = subclasses"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = cls.__subclasses__()\n\t\tfor subclass in subclasses:\n\t\t\tyield subclass\n\n\tdef __iter__(self):\n\n\t\tfor subclass in self.__subclasses__():\n\t\t\tyield subclass\n\t\t\n\n\n\"\"\"\nimport datetime\nimport uuid\nimport itertools\n#import sys\nimport os\nimport random\nimport time\nimport subprocess\nimport socket\nimport logging\nimport sys\nfrom collections import deque\nfrom threading import Thread\nimport logging\nLOG_LEVEL = logging.INFO\nLOG_FILE = '/var/log/panda/panda.log'\nVERBOSE = False\nif VERBOSE:\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn tuple(subsubclasses(cls))\n\n\nclass Node(Base):\n\t\"\"\"\n\tA node.\n\t\n\tThis class represents a node in a tree. A node stores information about the node's type. It holds\n\tinformation about the current input source, the output source and a list of the inputs\n\tfor that input at this node.\n\t\n\tAttributes\n\t----------\n\tparent : Union\n\t\tThe parent node, specifying the node type, if the node has an implicit input.\n\ttype : str\n\t\tThe node type. Defaults to `None` (optional).\n\t\n\t\"\"\"\n\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tall_subclasses = []\n\n\tsubclasses = subclasses()\n\tfor class_ in subclasses:\n\t\tall_subclasses.extend(classes(class_))\n\n\treturn all_subclasses\n\nclass AutoGen(metaclass=AutoGenMeta):\n\tdef __init__(self, gen_type, name, template, bases=None, methods=None):\n\t\t_AutoGenMethod.autogen_type = gen_type\n\t\t_AutoGenMethod.template = template\n\t\t_AutoGenMethod.bases = bases if bases else {}\n\t\t_AutoGenMethod.methods = methods if methods else {}\n\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses=set()\n\t\tsubclasses_add=False\n\t\tsubclasses_remove=False\n\t\twhile subclasses_add==False:\n\t \t\tsubclasses_add=True\n\t \t\tnew_subclasses=[]\n\t \t\tfor e in bpy.data.objects: \n\t \t\tnew_subclasses.append(e)\n\t \t\t  for cc in subclasses_set: pass\n\t \t\tsubclasses_remove=True\n\t \t\n\t \tsubclasses.add(new_subclasses)\n\t \tsubclasses_add=False\n\t  \n\t \tsubclasses.remove(cls)\n\t\tsubclasses_remove=True\n\t \tsubclasses_add=False\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tsubclasses += cls.__subclasses__()\n\t\tfor sub in subclasses:\n\t\t\tsubclasses += sub.__subclasses__()\n\t\treturn subclasses\n\n\tdef appendClass(cls, c):\n\t\t\"\"\"\n\t\tAppend a class (and its subclasses) to the list of \n\t\t\tsubclasses\n\t\t\"\"\"\n\t\tif cls in c.__subclasses__():\n\t\t\treturn\n\t\tsubclasses = subclasses.append(cls)\n\t\tc.__subclasses__ = subclasses\n\t\tc.__typesubclasses__ = tuple(subclasses)\n\n\tdef _process(self, parent, name, *args, **kwargs):\n\t\t#"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tret = set()\n\t\tsubclasses = cls.__subclasses__()\n\t\tfor x in subclasses: \n\t\t\tret.add(x.__name__)\n\t\treturn ret\n\n\nclass BaseController(object):\n\t\"\"\" Controller\"\"\"\n\n\tdef __init__(self, controller, request):\t\t\n\t\tself.controller = controller\n\t\tself.request = request\n\n\t@overrides([IResource])\n\tdef index(self):\n\t\t\"\"\" This is the main index-page \"\"\"\n\n\t\trequest = self.request\n\t\tcontroller = self.controller\n\t\tself.request.matchdict[\"controller\"] = controller\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tclasses = [\n\t\t\tcls for cls in cls.__subclasses__()\n\t\t\tif not cls.__name__.startswith('_')\n\t\t\tif not cls.__name__ in cls.__dict__\n\t\t\t]\n\t\treturn classes\n\n\tdef get_subclasses(cls):\t\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = [\n\t\t\tcls for cls in cls.__subclasses__()\n\t\t\tif cls.__name__.startswith('_')\n\t\t\tand cls.__dict__.get('subclass', False)\n\t\t]\n\n\t\treturn subclasses\n\n\tdef get_class(cls):\t\t\"\"\"\n\tReturn the"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn [_class for _class in cls.__subclasses__() if _class.__metaclass__ is cls and issubclass(inspect.getmro(cls), cls)]\n\ndef getclass(name):\n\t\"\"\"\n\tLoad a class by its name or the absolute path if not registered. If the object has a function named `_class`, it calls that function.\n\tIf the module has a class with the same name, the second argument of the function is used to retrieve the class.\n\t:param name: the name of the class\n\t:return: the given baseclass\n\t\"\"\"\n\treturn getfull"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn cls.__subclasses__()\n\n\tdef __init__ (self):\n\t\tself._subclasses = {}\n\n\tdef register (self, name, subclass):\n\t\tif name not in self._subclasses:\n\t\t\tself._subclasses[name] = set()\n\t\tself._subclasses[name].add(subclass)\n\n\tdef remove (self, name):\n\t\tif name in self._subclasses:\n\t\t\tself._subclasses[name].remove(self)\n\n\tdef load_class (cls, name, path):\n\t\tif name in self._subclasses:\n\t\t\treturn self._subclasses[name]\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tint_sequence = list(string)\n\t\tint_int_sequences = list()\n\n\t\tfor i in range(len(int_sequence)):\n\t\t\tint_int_sequences.append(int_sequence[i:i+len(alphabet)])\n\t\t\t\n\t\tint_sequence = sorted(int_int_sequences, key=lambda list_: int_sequence if list_ in alphabet else alphabet.index(list_))\n\t\treturn int_sequence[0]\n#encoding=utf-8\n\nimport numpy as np\n\n#data is a python list of sentences\ndata = [line."}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn len(alphabet) * 8 * alphabet.index(string)\n\n\nclass Solution:\n    def hammingDistance(self, s1: str, s2: str) -> int:\n        i1 = 0\n        i2, j1 = len(s1), len(s2)\n        while i1 < i2:\n            s1[i1], s1[j1] = s1[j1], s1[i2]\n            i1 += 1\n            j1 += 1\n        while j1 < j2:\n            s1[i1], s1[j1]"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\t# Ensure that there is a unique representation of the number\n\t\tstr_num = ''\n\t\tstrs = alphabet[:]  # For clarity, also unique only\n\t\tfor st in strs:\n\t\t\tif st not in str_num:\n\t\t\t\tstr_num += st\n\t\treturn str_num\t\t\n\n\tdef parse_str(str: str, alphabet: List[str]) -> int:\t\t\"\"\"\n\tCreate a random string in the given alphabet. Note that,\n\tit's difficult to check for the existence of a unique number to start\n\twith (e.g., \""}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n#\talphabet = [int(c) for c in alphabet if c.isalpha()]\n\talphabet = ''.join([c for c in alphabet if c.isdigit()])\n\n\t#\talphabet = ''.join([x for x in alphabet if x.isalpha()])\n\n\tif alphabet == alphabet.upper():\n\t\treturn len(string)\n\t\n\t# 1: if the alphabet length is bigger than 10,\n\t# the string can contain any characters.\n\t# 2: if the alphabet is not longer than 11, the string cannot contain\n\t# any characters.\n\t# 3: if the"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n# Faster thanks to http://code.activestate.com/recipes/578228/\n# It would be better (if we ever used it):\nclass Solution:\n\t\n\t# @param s, a string\n\t# @return the sum of the number of different occurrences of s in s\n\t# For example, if s = \"Aaaa\", i.e. return 7, because there are only\n\t# two occurrences of s = \"Aaaa\", 3 each occurence is 6. \n\t# @return the maximum possible number of occurrences of s in s.\n\tdef max"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t return max(k for k in string.lower())\n\ndef naturalsize(string: str) -> int:\n\t\"\"\"\n\tReduce a string to all its characters first, except the last\n\tcharacter, then convert it as long as possible.\n\t\"\"\"\n\treturn len(string) // len(string.lstrip('-'))\n\nif __name__ == '__main__':\n    string = input().lower()\n    print(string, naturalsize(string))\n    print(naturalsize(string, 5))\n# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation."}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\ta = int(0)\n\t\tcount = 1\n\t\tfor c in alphabet:\n\t\t\ta = a * count % 10\n\t\t\tcount += 1\n\t\treturn a\n\n\n############################################################################################################################################################################################################################################\n#\n# Unit tests\n\n############################################################################################################################################################################################################################################\n\nif __name__ == \"__main__\":\n\tmain()\n\n####################################################################################################################################################################################################################################\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n# --- BEGIN_HEADER ---\n#\n# migrate-api [-h] [-u] [-p PATCH_REMOTE_NAME] [-r"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tn = len(string) # type: ignore\n\n\tif n < 2:\n\t\treturn alphabet[0] * n\n\n\t# For i in range(4 - n + 2):\n\t#\talphabet = alphabet[2:9]\n\n\treturn alphabet[1] * n + alphabet[0]\n\n\ndef int_to_alphabet(n: int) -> List[str]:\t\"\"\"\n\tTake the number up (positive) and convert the string it into a list of letters.\n\n\tIf n is negative, it does not count the spaces, and assumes all letters are of the"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\t# pylint: disable=too-many-arguments\n\t\tascii_alphabet = \"\".join(filter(lambda x: x not in string, alphabet))\n\t\tdigits = len(string) - len(ascii_alphabet.replace(\"z\", \"\"))\n\t\tif digits < 3:\n\t\t\tdigits = 3\n\t\tint_value = int(digits + ASCII_DIGITS_MIN) % 26 # pylint: disable=invalid-name\n\t\treturn int_value\n\n\tdef string_to_int_dict(string: str) -> dict:\t# pylint: disable=invalid-name\n\t\t\"\"\"\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tresult = 0\n\t\tfor c in string:\n\t\t\tresult *= 26 ** (len(c) - 1)\n\t\treturn result\n\n\n\treturn list(string_to_int(string_to_int(string, alphabet)))\n\n\n\n\n\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Extracts numbers from the given text file.')\n\n    parser.add_argument('-i', default='-', help='Filename containing raw data.')\n    parser.add_argument('-o', default='-', help='GZIP or GZIP archive.')\n    parser.add_argument('-p',"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef extract_tgz(url, destination_path):\n\t\"\"\" Downloads.tar.gz into its directory and extracts it to a path \n\twhich contains 'desc', file for each package.\n\t\n\tArgs:\n        url: url of the.tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\t\"\"\"\n        print(\"Extracting.tgz archive...\")\n        response = requests.get(url, stream=True)\n        with open(destination_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=1024"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\"\"\"\nMake the \"desc\" file\n\"\"\"\ndef make(path,destination_path):\n\t\"\"\"\n\tMake a tar.gz file for the given path,\n\tand put all files in that directory\n\tto the path\n\t\"\"\"\n\n\t#path is the abs path to the.tar.gz tar file\n\t# destination_path is the location of extracted archives\n\tdest_compressed_path=destination_path[:-3] + '.tar.gz'\n\tf=open(path, \"w\")\t\n\tf.write(requests.get(url).content)"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n    try:\n        if os.path.isdir(destination_path):\n            print(\"Error: path %s already exist, use -d to skip. (Is %s)\" \\\n            % (destination_path, 'exists', str(sys.argv[1])))\n        else:\n            os.makedirs(destination_path)\n    except OSError, msg:\n        print(msg)\n        sys.exit(1)\n        \n    files = os.listdir(destination_path)\n    for fname in files:\n        if \".tar.gz\" in fname:\n            os.remove(fname)\n        if \".tar.bz2\""}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef install(): # to install the library\n    print \"Starting to install library, will use existing path as destination\"\n\n    if os.path.exists('.tar.gz'):\n        os.remove('.tar.gz')\n\n    filename = 'core/extra/community.txt'\n    with open('./version.txt', 'w') as f:\n        f.write(__version__)\n\n    url_parts = urlparse.urlparse(url)\n    # extract all desc files from \"desc\" file\n    try:\n        desc_file = open(os.path.join(os.getcwd(), filename))\n    except OSError: # could"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef get_dest(src_path, dest):\n\treturn '{0}/{1}'.format(dest.path, src_path)\n\t\ndef extract(src_path):\t\n\tsrc_dir, name_ = os.path.split(src_path)\n\textract_path = os.path.join(src_dir, \"extract.tar.gz\")\t\n\tif not os.path.exists(extract_path):\n\t\tcreate_file(extract_path, name_, os.path.getmtime(extract_path))\n\telse:\n\t\textract_tar"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef download_packages(url, destination_path):\n\t# Retrieve the name of the.tar.gz archive we extracted from\n\tpackage = 'core' + url.split('/')[-1].split('-')[0]\n\t\n\t# Download the.tar.gz package.\n\t\n\ttry:\n\t\treq = requests.get('https://github.com/' + destination_path + '/archive/core/' + package + '/', headers={'Accept':'application/octet-stream'})\n\t\twith open(destination_path + '/desc.dat', \"wb\") as fout:\n\t\t\tfout.write(req."}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef check_if_dir(path):\n\tif os.path.isdir(path) == False:\n\t\tprint \"Error! path [\" + path + \"] is not a directory\"\n\telse:\n\t\treturn path    \n\ndef get_tar(path):\n\tfilename = path+\"/\"+ filename_format()\n\trequests.packages.urllib3.disable_warnings()\n\tdirectory = get_tar_dir()\t\n\twith zipfile.ZipFile(filename, 'r') as zf:\n\t\tcontent = zf.read('desc')\n\t\tzf.close()\n\t\treturn"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef getTars(arch):\n\t\"\"\"Gets the arch tar for this package.\n\t\n\tArgs:\n        arch: the name of the arch (eg: 'core')\n\t\"\"\"\n\tresponse = requests.get(arch)\n\tdata = response.content\n\t\t\n\tprint('Downloading ', arch)\n\n\t# extract the tar archive for this archive\n\tfilename = 'gen-' + arch + '.tar.gz'\n\ttar = tarfile.open(filename)\n\ttar.extractall('')\n\t\t\n\t# extract the archive and return them as a string\n\treturn [os.path.join"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n#get the tar.gz archive\nurl_archive = url.split(\"/\")\nurl_arch = \"arch-\" + url_archive[len(url_archive)-3:len(url_archive)-2]\n#get list of arch tar.gz\ntar_file     = url.replace(\".tar.gz\", \"\").split()\n#tar file name\ntar_file      = os.path.basename(tar_file)\n\n#get archive information\narchive_name_list = []\nif tar_file[2] == \".tar\":\n\t archive_name_list.append("}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\n    download_tools = os.path.join(os.getcwd(), '../', os.path.basename(url))\n    \n    if os.path.exists('extract-' + destination_path):\n        \n        # Download\n        urllib.urlretrieve(url, download_tools + '.tar.gz')\n        \n    elif os.path.exists(download_tools):\n\n        # Extract\n        tarfile.open(download_tools + '.tar.gz').extractall('build' + source_folder_path, recursive=False)\n\n        # Extract\n        tarfile.open(download_tools + '.tar.gz').extractall(source_folder"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\t# os_osx_ver_str = (platform.mac_ver()[0] + '-' + str(platform.mac_ver()[1])).replace('v', '')\t\n\t\t# if os_osx_ver_str.startswith('10.3.'):\n\t\t# \tos_mac_ver = '10.3'\n\t\t# \tif platform._sys_platform.startswith(('freebsd', 'darwin')):\n\t\t# \t\tos_mac_ver_str = platform._sys_platform.replace('freebsd','mac')\n\n\t\t# print platform.os_ver_"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef CheckMac():\n\t\"\"\"Check if Mac OS and its os is windows\n\n:return: True is linux\n:rtype: bool\n\n\"\"\"\ndef CheckMacRamanager():\n\t\"\"\"Check if the os is Ramanager for ROMANAGER\n\n:return: True is Ramanager, otherwise False\n:rtype: bool\"\"\"\n\t#os = GetOs()\n\tif os.path.exists('/etc/default/rcS') \\\n\t\tand os.path.access('/etc/default/rcS',os.R_OK):\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\n\t# Check for OS\n\tif os.name == \"posix\" or \"darwin\" in platform.platform():\n\t\treturn True\n\t\n\t# Check for linux\n\telif os.name == \"posix\":\n\t\ttry:\n\t\t     import sys \n\t\t    \tprint sys.platform \n\t\t    \tfrom os.path import exists as os_exists\n\t    \texcept ImportError, e:\n\t\t    \tprint e\n\t\t return False\n\t\t\t\n\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\t\tif os_exists(\"../lib/python2.7/dist-packages/macOS/\"):\n\t\t\tfrom os import system\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\tif os.name.lower() in OS_MAPPING.keys():\n\t\t# check if macOS\n\t\tmacOS = os.uname()[2]\n\t\t\n\t\tif ((macOS == \"mac os\") or (macOS == \"win\")):\n\t\t\t# check if user is root\n\t\t\troot_pass = os.getuid() == 0\n\t\t\treturn True\n\t\telse :\n\t\t\tpass\n\n\n\telif \".\" in OS_MAPPING.keys():\n\t\t# check if macOS\n\t\tmacOS = os.uname()[2][0]\n\t\t\n\t\tif ((macOS == \"OS X\") or"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef getOS():\n\t\"\"\"\n\tChecks if the OS is linux or macOS\n\t\"\"\"\n\treturn sys.platform!= 'linux' and __platform__!= 'darwin' and os.environ.get('MACOSX_OS', None)!= None\n\ndef getArch():\n\t\"\"\"\n\tChecks the architecture of the os\n\t\"\"\"\n\tif not os.environ.has_key(\"OS\"):\n\t\traise Exception(\"No os environment found. Install os-osx or os-windows first\")\n\treturn __arch__\n\ndef getGreeks():\n\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(\"/etc/lsb-release\"):\n\t\treturn False\n\t\t\t\n\tf=open(\"/etc/lsb-release\",\"r\")\n\tfor line in f:\n\t\tsplitted=line.split(\"=\")\n\t\tif splitted[1].startswith(b\"VERSION\"):\n\t\t\treturn True\n\treturn False\n\t\t\t\ndef get_iso_dir():\n\t\"\"\"\n\tChecks if the iso is installed\n\tIf not get the latest iso dir from /etc/rhevm-update.\n\tIf not, it sets the environment variable DESTINATION\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef systeminfo():\n\t\"\"\"\n\tGets the systeminfo command\n\n:return: The systeminfo command\n:rtype: str\t\"\"\"\n\tcommand=\"systeminfo\"\n\tresponse=executeCommand(command)\n\treturn response[1][\"result\"][\"systeminfo\"][\"command\"]\n\ndef systemstatus(): return systeminfo()['current_status']\n\t\t\ndef is_windows():\n\t\"\"\"\n\tChecks if the os is windows.\n\n:return: True or False is windows\n:rtype: bool\t\"\"\"\n\t\n\ttry:\n\t\tsystemstat()\n\t\tos.system(\"start x"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef get_mac_os_version():\n\tos_version = 0\n\tif os.environ.has_key('MACOSX_OS_VERSION'):\n\t\t# We can have a macOS (1,2 or an older macos)\n\t\tif os.environ.get('MACOSX_OS_VERSION').find('A') > -1:\n\t\t\tos_version = 1\n\t\t\tos_version = int(os.environ.get('MACOSX_VERSION')[4:])\n\treturn os_version\n#!/usr/bin/env python\n#\n# Copyright (C"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\tos.system (\"echo $(readlink /proc/meminfo | head -1; \"\n\t\t\t\"awk '/NR >/dev/null 2>/dev/null'), 0)\n\t\treturn True\n\n\tdef build(self):\n\t\t\"\"\"\n\t\tBuilds the kernel source and build.c, linking for gcc and\n\t\t# calling it binary\n\t\"\"\"\n\n\t\tif not os.path.isfile('./gcc'):\n\t\t\tos.system(\"cp -r../../../Makefile./gcc; make\")\n\t\t\tos.system(\"make install\")\n\n\t\tif not os.path.isfile('./g++'):"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\tif os.name == \"mac\":\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\n#def createDirectoryIfNotExists(dirName):\ndef createDirectoryIfNotExists(dirName):\n\tos.system(\"rm -rf %s\" % (dirName))\n\tif dirName == '':\n\t\tos.mkdir(dirName)\n\n\n#def createImageBin(width, height):\n#\tim_width = im.size[0]\n#\tim_height = im.size[1]\n#\tim_im = Image.new(\"RGB\", ("}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\treturn dict((k, re.compile(v)) for k, v in item.items())\n\ndef regex_replace(rule_dict, file_name, pattern, ignorecase=False):\n\t\"\"\"\n        Apply regex replace, replace all rules found in file_name that\n        match with the pattern.\n        \n        Arguments:\n\trule_dict : { regex_key: (regex_value, pattern_value, ignorecase) }\n        file_name : string for the file to replace\n\tpattern : (string) the regex to search for\n\tignorecase : (booleans) whether or"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef convert_includes(item):\n\t\"\"\"\n\tGiven a dict where the keys are all filenames\n\twith wildcards, convert only the keys into equivalent regexes \n\twith includes. These are the file name (with wildcards removed)\n\t\"\"\"\n\tif ('includes.cpp' in item):\n\t\treturn False\n\tif ('includes.h' in item):\n\t\treturn False\n\tif ('includes.o' in item):\n\t\treturn False\n\treturn True\n\nfor file in glob.glob('*.cpp'):\n\tfile = file.rstrip('\\r\\n')\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\n\ndef remove_comments(items_to_check, key, value):\n\t\"\"\"Removes comment from a dict by setting it to ''\n\tThis function works mainly to deal with the comments of the file\n\t\"\"\"\n\tout_dict = {}\t\n\tlines = key.split('\\n')\n\t\n\tfor line in lines:\n\t\tif re.search('%s=[ \\t]*%s' %(value, lines[0]), line):\t\t\n\t\t\tif re.search('[ \\t]*\"%s\"' %(value), line):\n\t\t\t\tout_dict[value] = lines[3]"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n    regex = {}\n    regex.update(regex_keys)  \n     \n    for item_regex in regex_dict.iteritems():\n        for key, re_pattern in item_regex.iteritems():\n            re = re.compile(re_pattern)\n            regex[key] = re            \n    return regex \n\n\n\"\"\"    \n\"\"\"\ndef search_rules(item, regex, target_dict):\n    return regex\n\ndef do_replacements(rules, text, replacement_dict=None):\n    replacement_dict = replacement_dict or {}\n    pattern = r\"\"\"(?:%s)[$]{}\"\"\""}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n        re_regex_keys = re.compile(ignore_file(item['a']).lower())\n               \n\t\tret = {}\n\t\tfor m in re_regex_keys.findall(item['a']):\n            try:\n                ret.update({m: item['a']})\n               \t\tret.update({re.compile(ignore_string(v) + \".*\") : v})\n\t\texcept IndexError:\n                print (\"%s is not a valid key for item %s\" % (item['a'], item['a']))\n\n\t\tret.update({item['h']: item['a']})\n\t\tret"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\npatterns = {\n    '.cpp': re.compile(r'\\.(h|cpp|hpp|C|h|C++|HPP|CPP)\\Z'),\n    '.h': re.compile(r'\\.h|'),\n    '.c': re.compile(r'\\.c|'),\n    '.hpp': re.compile(r'\\.(h|pp|hppcc|H|c|C\\Z)', re.I)\n}\nregex_keys = 'pcrel|pch|cref|hcl|hxx|m|mm|inc|java"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\nkeys = {}\t\nwith open(path_to_file, 'r') as file:\n\tfor item in file:\n\t\tk = item.split('/')[-1].split('.')[0]\n\t\tif k in ['*.h', '*.cpp']:\n\t\t\tkeys[k] = regex_keys(item.replace('{}'.format(i), ''))\n\t\telif k in ['*.c', '*.cpp', '*.h']:\n\t\t\tkeys[k] = regex_keys(item)\n\nreturn keys\n\n# -------------------------------------------------------------------------------------\n\ndef dict_equal(dict1, dict2):\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef load_matches(file_matches):\n\t\"\"\"\n\tLoad files and make a set of matched regexes and load them.\n\n\tArgs:\n\t    file_matches (dict): with every file containing regexes\n\n\tReturn:\n\t\tlist containing regexes which match a given regex\n\t\"\"\"\n\n\tfile_dict = {}\n\n\tfor file_name, regex in file_matches.iteritems():\n\t\tif re.match(regex[2], file_name):\n\t\t\tfile_dict[regex[3]] = file_name\n\n\treturn file_dict\n\n\ndef write_matches("}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n        regex_dict = {}\t\t\n\n        for key in regex_keys:\n            compiled = []            \n            for exp in regex_dict:\n                # regex only has key in dict\n                # print key\n\n                if exp['a'] == key:\t\t\t\n                    compiled.append(exp.copy())\n                    regex_dict[key]['a'] = exp['a']\n                    compile_regex_keys(compiled, regex_dict)\n                    print \"compile key \", key\n            compiled.append(regex_dict[key])\n            # print compiled\n\n            # compile all values in compiled\n            compiled += compiled\n\t\t\n        return regex_dict\n\ndef"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\n\t# TODO : Check for bad matches\n\n\tfor key, regex in regex_keys.items():\n\t\ttry:\n\t\t\tval = re.findall(regex, item[key])\n\t\t\tif len(val) == 0:\n\t\t\t\tcontinue\n\t\texcept Exception:\n\t\t\tprint(\"Error while decoding regex key:\", key)\n\t\t\tcontinue\n\t\t_dict = {}\n\t\t_dict['regex'] = regex\n\t\tvalue = []\n\t\tfor v in val:\n\t\t\tv = v.strip()\n\t\t\tif not v.startswith('\\\\'):\n\t\t\t\tv = v.lstrip('\\\\')\n\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tname = name.strip() # remove quotation marks\n\t\tname = name[0:len(name)-2]\n\t\tif name.endswith('\"'):\n\t\t\tname = name[:-1]\n\t\treturn name\n\n\tname = re.sub(r'\\s*([^\\\\])', r'\\1\\1', name)       # remove quotes\n\tname = re.subn(r'^\\s*;|(?=\\w)&', r'<\\1>', name)       # remove spaces\n\tname = name.replace(\"\\''\", \"'\")      # remove spaces\n\tname = name.replace('\\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef remove_quote(s):\n\t\"\"\"\n\tRemove quote from the given name.\n\t\nSyntax\n\tRemove quote from the given name, like so:  \"x-y, z, t\\t1,2,3-x\" -> \"x-y, z, t\\t1, 2, 3-x\"\n\"\"\"\n\ndef replace_string(s):\n\t\"\"\"\n\tReplace string and returns the passed string, as a single string.\n\tIf the same key exists, as two strings, a string is returned unchanged (useful\n\twhen saving, in"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tresult = re.sub(\"\\$(?!\\w)\", \"\", result.lower())  # Strip $ signs from string names\n\t\tresult = re.sub(\"^'%s\\w+$\" % os.environ[\"HOME\"] if 'HOMEPATH' in os.environ else \":\", result).upper()\n\t\treturn result\n\t\t\n\t@property\n\tdef name_without_leading_chars(self):\n\t\t\"\"\" Return the trailing semicolon and space stripped of name\n\t\t\"\"\"\n\t\tresult = self.name[:-len(self.name_prefix)]\n\t\tresult = result.strip()\n\t\treturn result\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\nelif __name__ == '__main__':\n\t'''\n\t\tGet a list of all the names in the database, create their tables if they do not exist.\n\t\t'''\n\n\ttables = []\n\tfor t in tables_to_create:\n\t\tname = get_unique_table(schema_name=\"doctests\", prefix=\"public\", name_length=10, max_length=20)\n\t\tname += '_id'\n\t\tnames_in_use = get_names_from_table( schema_name=\"doctests\", prefix=\"public\", name_length=10, max_"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\treturn re.sub(r\"'|\\\"|\\'|\\\" \")',replaceStr,val, re.M)\n\tdef removeQuote(self,value,replaceStr=\"'\",re=re):\n\t\t\"\"\"\n\t\tRemove the quote from the string to avoid problems with the regex\n\n\t\treplaceStr: string to be used for the replacement\n\t\tre: regular expression matching the values.\n\t\t\"\"\"\n\t\tif re.search(replaceStr,value):\n\t\t\tself.value = re.sub(value,'\\'','',re.M)\n\t\t\tself.value = re.sub(r"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\nclass Msgs(base.Mesgs):\n\n\t@staticmethod\t\n\tdef command(params, body):\n\t\t\n\t\t\"\"\"\n\t\tFind a quote on the Msgs database.\n\t\t\n\t\t:argument params:\n\t\t\n\t\t:body:\n\t\t\tA dictionary object containing all\n\t\t\tkeys which can be searched without a name\n\t\t\tvalue or a list containing search parameter\n\t\t\tdefinitions.\n\t\t\n\t\t:returns:\n\t\t\tA dictionary object containing the result\n\t\t\tfrom query.\n\t\t:raises:\n\t\t\t:class:`KeyError` when the parameters contain no value.\n\t\t\"\"\"\n\n\t\tif not ('where"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\twords   = self.split(line)\n\t\tself.remove_quotes(words)\n\n\tdef split(self,line):\n\t\t\"\"\"\n\t\tSplit <delim> and split line at delimiter.\n\t\tReturns (delim, line).\t\"\"\"\n\t\tline = self.rest.split(line)\n\t\tif line == '': line = None\n\t\treturn (line[0],line[1:])\n\n\tdef is_equal(self,line1,line2):\n\t\tm1,n1=self.maxsplit(line1)\n\t\tm2,n2=self."}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n#----------------------------------------------------------------------\n\nfrom\tcommons \timport *\n\n#----------------------------------------------------------------------\n\nclass\tReplaceWordTest(unittest.TestCase):\n\n\tdef setUp(self):\n\t\tif hasattr(sys,'frozen'):\n\t\t\tself.oldargv = sys.argv\n\t\t\tself.newargv = sys.argv\n\t\t\tself.reclass()\n\t\telse:\tself.oldargv = sys.argv[0]\n\t\t\tself.newargv  = sys.argv[1:]\n\t\t\tself.reclass()\n\t\t\n\tdef reclass(self):\n\t\t\n\t\t\"\"\"\n\t\tRedefine functions not to call"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n#class HierarchyNode(object):\n\t#\"\"\"\n\t#Wrapper around the name in hierarchy\n\t#\"\"\"\n\t\n\tdef __init__(self, node):\n\t\tself.name = node.text\n\t\tself.children = []\n\t\t\n\tdef add(self, child):\n\t\tself.children.append(child)\n\t\tchild.parent = self\n\t\t\n\tdef remove(self, child):\n\t\tself.children.remove(child)\n\t\tchild.parent = None\n\t\n\tdef __str__(self):\n\t\treturn self.name\n\t\t\n\tdef __repr__(self):\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tl=re.sub(\"<\", \"\", name) \t# get rid of quotation marks\n\t# this regex matches the original code\n\tre.sub(\"[s]+\", \"\", name) \t\t\t# replace quotes so that they are not too confusing with regex\n\treturn re.sub('<.*?>', '', name)\t  \t\t\t# get rid of extra whitespaces\n\ndef convert_idio_command (name, idio):\n\t\"\"\"\n\tGiven the name of the output idio, convert it to a string,\n\tand return it.\n\t\"\"\"\n\tl=i"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\t\n\tclass Cmd(string.Cmd):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tfor k, v in args[0]:\n\t\t\t\tassert type(k) is str, k\n\t\t\t\tself.__k = k\n\t\t\t\tassert ((sys.platform == 'win32') or (sys.platform == 'cygwin')), (\n\t\t\t\t\t\"non-Windows or Cygwin arguments not allowed: cmd.exe supports\"\n\t\t\t\t\t\" these (args: %r)\"%list(kwargs.keys()))\n\t\t\n\t\tdef pipe_command(self, *args):  # pipe-command uses popen"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\t\n\nclass Rex:\n\n\t\"\"\"A more real class which overrides the normal (non-Windows) shlex\n\tcommand-line.  This class simply overrides the shlex.join() method as\n\tex.  Argparse is much more straightforward than cmd.Cmd, and will be\n\tused directly as Cmd's __init__ and set Cmd's.raw_input_loop.\"\"\"\n\n\tif platform == 'windows':\n\t\tpcmd = '/bin/sh -c'+ cmd\n\t\tdef __init__(self, cmd):\n\t\t\tself.cmd = cmd\n\t\t\tself.f, self"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\"\"\"\nclass MultiSystem(object):\n\t\"\"\"\n\ta multi system object (also called shell).\n\t\"\"\"\t\n\tdef __init__(self):\n\t\tself.cmds = []\n\t\tself.cwd = 0\n\t\tself.env = None\n\t\tself.cwd_args = None\n\t\tself.env1 = {}\n\t\tself.env2 = {}\n\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn string representation of this multi system object (which may\n\t\tbe unicode)\n\t\t\"\"\"\n\t\tres = []\n\t\tmap = dict()\n\t\tmap["}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tdef find_exe(exe, path=None, _cache={}):\n\t\t\t\"\"\"If executable not found return None.\"\"\"\n\t\t\tcmd_output = Utils.cmd_output(exe +'/? -f --version', shell=True, verbose=False)\n\t\t\tif path in _cache:\n\t\t\t\tresult = _cache[path]\n\t\t\telse:\n\t\t\t\tresult = which(exe)\n\t\t\t_cache[path] = result\n\n\t\t\tcmd_output = [x.replace(CMD_LINE_ENV_VAR, x) for x in _cache.values()]\n\t\t\tself._log('"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n    if os.name == 'posix':\n        # use specific platform specific code\n        for c in [\"/bin/sh\", \"/sbin/sh\"]:\n            if c in os.environ:\n                platform = c\n                break\n        platform = '/'.join(platform.split(\":\")[0.index(\".\"))]\n        \n    if not (platform == '' or platform == 'win32'):\n        print(\"platform name may only be None, or a known compiler\")\n        sys.exit(1)\n\n    if not re.match('^[0-9a-z_]*$', platform):\n        print(\"unsupported platform: [0-9a-"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tsys.stderr.write(dedent(r'''\n\t\t\t'''))\n\t\tsys.stderr.write(dedent(r'''\n\t\t\t[cmd]\n\t\t\tshell = msys.unix.shell\n\t\t\tcmd = praw.auto\n\t\t\tcmd = praw.auto 'hello.txt'\n\t\t\tshell.exec_cmd(cmd)\n\t\t'''))\n\t\tp = Parser(); p.execute_line()\n\t\tdef myexec(cmd):\n\t\t\tprint cmd\n\t\tp.execute_line(myexec)\t\t\t\t\n\t\tp.compile()\n\t\twhile True:\n\t\t\tline ="}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\n\tdef set_attribute(self, path, key, value):\n\t\t\"\"\"Sets a key value pair\"\"\"\n\t\t\n\t\ttry:\n\t\t\tassert type(path) in (type(''), type(u''), type(b''))\n\t\t\tassert type(key) in (type(''), type(u''))\n\n\t\t\tassert not re.match(\n\t\t\t\tr'^\\#?\\s*(%s)\\s*:\\s*$' % key.strip(),\n\t\t\t\tpath)\n\n\t\t\tvalues =''.join([\n\t\t\t\t'%s %s\\t%s\\t%s\\t%s' % ("}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\n\n__all__ = [\"shlex\"]\n\n\nif sys.version_info > (3, 0):\n\tdef _cmd_string_to_list(arg):\n\t\treturn shlex.split(arg)\nelse:\n\tdef _cmd_string_to_list(arg):\n\t\treturn shlex.split(arg, posix=True)\n\ndef parse_args_and_files(args):\n\t\"\"\"Parse command to list of arguments including files.\n\n\tThese are non-standard arguments and return `None`.\n\t\"\"\"\n\trargs = []\n\tfor arg in"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\t# this works on Linux and OSX\n\t\tif re.search(\"^\\s*$\", argv[0], re.M):\n\t\t\tcmdline = argv[0] + \" \" + argv[1].strip(\"\\\"\").strip()\n\t\t\targc = len(argv)>2 and argv[2] or \"\"\n\t\t\targv = argc+\" \" + argv[0] + \" \" + argv[1].strip(\"\\\"\").strip()\n\t\t# elif re.search(\"^\\s*$\", argv[0]):\n\t\tcmdline = argv[0]\n\telse:\n\t\targc = 1"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nimport re\nfrom collections import OrderedDict\nimport string\n\nfrom.utils.platform import platform, Command\nimport sys\nimport shlex\n\n__all__ = [\"shlex\", \"split_command_string\", \"shlex_quote\"]\n\n# Used for splitting commands with space (or without \\n that way), for use in the path.  \n_split_args = shlex.split\n\nclass shlex_quote(Command):\n\t\"\"\"\n\tA string substitution function that unescapes quotes in given environment\n\tvariables into list of strings.  \n\n\tSubstitutions should map to the space character,"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef _check_file_repo(archive_path):\n\t\"\"\"\n\tGiven an archive_path, decompress it and check if the archive already exists.\n\t\"\"\"\n\ttar_cmd = [\"tar\", \"-zxvf\", archive_path]\n\tret = subprocess.call(tar_cmd)\n\tif ret == 0 :\n\t\tprint \"tar.gz extraction failed; archive does not exist\";\n\t\ndef main():\n\tif len(sys.argv)!= 4:\n\t\tprint \"usage:\\n\t  python %s <archive_path> [to folder]\" % \"\\"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef _unpack(tarpath, target_dir):\n\tsubprocess.call(\"tar -xf %s \" % tarpath, cwd=target_dir)\n\t\n\tresult = False\n\ttry:\n\t\tresult = subprocess.check_output(\"tar -xf %s \" % tarpath, cwd=target_dir)\n\t\tprint(result)\n\texcept:\n\t\tpass\n\t\n\tif result:\n\t\tprint(\"error, unpacking archive:  %s\" % tarpath)\n\t\t\n\t\tsubprocess.call(\"rm %s\" % tarpath, cwd=target_dir)\t\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\tcmd = ['gpg','archive','-v','--assumeyes']\n\t    proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\t    out, _ = proc.communicate(input=None)\n\t    return out[:-1]\n\t    \n   def upload(self, path):\n\t    repo = self.get_repo(path)\n\t    if repo:\n\t\t    url = self.get_origin('upload', repo['id'])\n\t\t    msg = \"Adding '%s' to location '%s'...\\n\" % (path, url)\n\t\t    if self._debug:\n\t\t\tself.log.write"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef is_root_writeable(path):\n\t\"\"\" Check if path is a read-only file in read-only mode. \"\"\"\n\treturn os.access(path, os.R_OK)\n\t\ndef get_local_mirror_dir(user):\n\t\"\"\" Get the folder containing mirror of user based on /etc/apt/apt.conf. If it doesn't exist, then return None. \"\"\"\t\n\t# FIXME: Need to deal with /etc/apt, for more information.\n\ttry:\n\t\treturn os.path.dirname(user.file.replace(\"."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\t\n\tsubprocess.call(['tar','-tf',archive_path+'/*'],stdout=logfile,stderr=logfile)\n\tsubprocess.call(['mv','-v'],input='tmp',output='/etc/apt/sources.list.d/{}.iso.rpm'.format(archive_path),stderr=logfile,shell=True)\n\treturn '/etc/apt/sources.list.d/{}.iso.rpm'.format(archive_path)\t\n\t\n\t\n\"\"\"\n\t\tThis function will create the mirror configuration for mirror.\n\tTo generate a mirror config, it needs to be used following a template:\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef uncompress(archivePath: str) -> str:\n\t\"\"\"\n\tUncompress the archive in a given path of archive_path.\n\t\"\"\"\n\tcmd = \"unzip \" + archivePath\n\tsubprocess.check_output(cmd, shell=True)\n\t# remove.zip\n\tcmd = \"rm \" + archivePath\n\tsubprocess.check_output(cmd, shell=True)\n\t\n\treturn archivePath\n\ndef copyUncompress(archivePath: str, src_path: Optional[str]=None):\n\t\"\"\"\n\tCopy the archive_path to a temporary"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\nclass ArchiveUrl:\n\n\tdef __init__(self, working_directory):\n\t\tself.working_directory = working_directory\n\n\tdef get_archive_path(self):\n\t\t\"\"\"Return a path to the.tar file for the archive being compressed.\n\t\t\"\"\"\n\t\treturn os.path.join(self.working_directory, self.archive_type)\n\n\tdef compress_files(self):\n\t\t\"\"\"Compress, uncompress, and delete original archive to it.\n\t\t\"\"\"\n\t\tif not os.path.exists(self.get_archive_path()):\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\tprint ('Downloading {0}'.format(archive_dst_name))\n\tsubprocess.call(shlex.split(archive_path), cwd=temp_dir)\n\tos.rename(archive_dst_name, archive_dst_name + '.orig')\n    os.rename(archive_dst_name, archive_dst_name + '.new')\n    return os.path.splitext(os.path.basename(archive_dst_name))[0] + '.orig'\n    \n\n\ndef get_remote_tarball_dir(host_url):\n        \"\"\" Extracts the tarball dir to save"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef uncompress(path):\n\t\"\"\"Uncompress file path\"\"\"\n\tcmd = \"%s -d -xjf %s\" % (path, options.mirror_url)\n\tsubprocess.call(cmd, shell=True)\n\ndef create_zip_file(source_zip_file_path, source_filename, zip_size):\n\t\"\"\" Creates a zip file \"\"\"\n\tcmd = \"%s -y zip %s --chunk 1024\" % (options.archive_path, source_filename)\n\tsubprocess.call(cmd, shell=True)\n\t# zip"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\tdef unzip_archive(self,archivepath):\n\t\t\"\"\"\n\t\tUncompress a tar.gz archive into an archive_path.\n\t\t\"\"\"\n\t\tif \"windows\" == sys.platform or \"darwin\" == sys.platform:\n\t\t\t# os.unlink would unlink tmp dir -> uncompress zip on windows\n\t\t\tlogging.warning(\"os.unlink doesn't functional\")\n\t\t\tarchive_path_tmp = re.sub('.zip$', '%s.zip'%archivepath, os.path.basename(archive_path))\n\t\t\tshutil.make_archive(archive_path_tmp,"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\n\t#get_status : get file size and return it as a string\n\tdef get_status(self,filename):\n\t\tcmd = 'git ls-files --full-name | grep \"%s\"' % filename\n\t\tself.__log('cmd is : %s' %''.join(cmd))\n\t\tres = subprocess.call(cmd, shell=True, universal_newlines=True)\n\t\treturn res\n\n\t\t\n\n\n\n\n\nif __name__ == '__main__':\n\n\tfile_list =[]\n\n\tfile_pattern = '.*\\.mp3' #default file pattern\n\t\n\tall_file"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\t\n\tfilesList = list()\n\tindex_list = ['.', '?', '*', '.git', '?', '.hg','']\n\tif type(files)!= str:\t# Check is file list\n\t\tfilesList = os.listdir(sys.argv[2])\n\t\tfilesList.sort()\n\telse:\n\t\tfor f in files:\n\t\t\tif not f in index_list:\n\t\t\t\t\tfilesList.append(f)\n\tif len(filesList) == 1:\n\t\treturn filesList[0]\n\telse:\n\t\treturn filesList\n\t\t\n\ndef file"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\ndef run_git_cmd(cmd):\t\n\t#print(\"Running:\",cmd)\n\tstdout, stderr = subprocess.Popen(cmd,stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()\n\treturn stdout\n\ndef git_list(src_dir):\n\t#print(\"Running:\", src_dir)\t\n\tretcode, output = run_git_cmd(\"git ls-files %s | grep 'ChangeLog' | grep -v '%s'\" % (src_dir, '%s' % src_dir)).communicate()\n\treturn retcode,output\n\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tdef git_ls_files( self, name ):\n\t\t\"\"\"\n\t\tOutput a list of files\n\t\t\"\"\"\n\t\t\n\t\tlines = self.git(\"ls-files -z %s\" % name)\n\t\treturn lines\n\t\n\n\tdef git_ls_dirs( self, name ):\n\t\t\"\"\"\n\t\tDo a recursive search for a directory\n\t\t\"\"\"\n\t\tlines = self.git(\"ls-dirs %s\" % name);\n\t\treturn lines\n\n\t\n\n\tdef git_ls_files_with_matching_files( self, name, regex, dirs, verbose=False):\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\nclass git_args:\n  def __init__(self):\n  def __init__(self):\n  def __init__(self):\n  def __init__(self):\n  def __init__(self):\n\t\n        try:\n            self.GIT = which('git')\n        except IOError:\n            print 'ERROR: Cannot find git'\n        self.GIT_PATH='.'\n  def find(self):\n        if self.GIT_PATH=='.':\n           git = which('git')\n           for line in git:\n               print line\n        else:\n           path_to_find = os.environ.get('PATH', '').split(os"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n        return _git_fetch_stdout(command)\n\n\n\n#\n# main entry point\n#\n\n#\n#def main(argv):\n#    (opts, args) = cmd.parse_args(argv[1:])\n#\n#    if opts.verbose:\n#        util.log(25,\"git: %s\" % (repr(opts.git_binary),))\n#\n#    (head, _) = _git_fetch(cmdoptions.git_binary)\n#    (head, _) = _git_fetch_stdout(cmdoptions.git_binary)"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n##\n# Helpers\n#\n\ndef cleanUpLine(line):\n\ti = line.index(line.rstrip('\\n'))\n\treturn line[:i].strip()\n\ndef main():\n\tp = subprocess.Popen(\"git rev-list HEAD...\", stdout=subprocess.PIPE, shell=True, bufsize=1)\n        branch = \"git rev-parse --abbrev-ref HEAD\"\n        commit = \"commit\"\n        branch_desc = run_command(branch)[1].decode(\"ascii\").strip()\n        return cleanUpLine(p.stdout.readline())\n\ndef update"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tif not self.git_executables:\n\t\t\tself.git_executables = self.find_git_executables()\n\t\t#command = \"git describe --tags\"\n\t\t#status, output, error = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE).communicate()\n\t\t#self.__git_command = self.git_executables\n\t\tself.git_command = \"git rev-parse --verify HEAD\"\n\t\tself.clean_git_command = \"git clean -x --cached -q --quiet\"\n\t\tstatus, output, error ="}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t     \n\tdef getFiles(self, files):\n\t\tcommand \t= \"git diff --name-only --diff-filter '+'\"\n\t\tfilenames = \"\".join(files) + \";\"\t\t\n\t\tcommand +=''+filenames\n\t\tprint command\n\t\tsubp = subprocess.Popen(command, shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\t\t(stdoutdata, stderrdata) = subp.communicate()\n\t\tfor line in stdoutdata:\n\t\t\tif line[:-1]!= \"\":\n\t\t\t\tlines = line.split(\"\\t\")\n\t\t\t\tfilenames = lines["}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\ttry:\n\t\t\tproc = subprocess.Popen(['git','-C',git_command],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\t\texcept OSError,e:\n\t\t\treturn []\n\telse:\n\t\tret = []\n\t\n\ttry:\n\t\tfor file in [f for f in os.listdir('.') if os.path.isfile (os.path.join(f,\"./.git/objects\"))]:\n\t\t\ttry:\n\t\t\t\tif os.path.basename(file) == 'objects' and not file.endswith('.py'):\n\t\t\t\t\tcmd = \"git log --format=%"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\nelif os.path.exists('.py'):\n\tfilename=os.path.abspath('..\\\\..\\\\'+filename,2)\nelse:\n\t'''Given in the location of this file return true'''\n\treturn False\n\t\t\nif __name__ == '__main__':\n    import time\n\tfrom datetime import datetime\n\t\n\tdate_now=datetime.now()\n\t#print date_now.strftime('%Y-%m-%d %H:%M')\n\t#print '\\n==\\n==\\n==\\n' \n\t#test_string='this is a string'\n\t#"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\n\t#Check for extension\n\t#TODO: implement check if file has an extension.\n\t\n\texts = [\".py\", \".c\", \".h\"]\n\t\n\tfor ext in exts: \n\t\tif filename.endswith(ext):\n\t\t\treturn True\n\treturn False\n\ndef create_module():\n\t\"\"\" Creates the module\"\"\"\n\treturn import_module(__name__, __file__)\n\n\ndef test_file_exists():\n\t\"\"\" Checks if filename exists to ensure the path to the file exists\n\t\"\"\"\n\tprint \"Test file does not exist\"\n\treturn os.path.exists"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef getfilename(filename):\n\t\"\"\"\n\tReturn the file path that mandates this filename\n\tto be scanned.  Note that both.txt and.py extensions might\n\thave.py suffix so we need to check both\n\n\tReturns a list of module names for the given filename\n\n\tAssumes filename has one of the extensions.  \n\t\"\"\"\n\tfilename = filename.split(\".txt\")\n\tfilename = \".\".join(filename[:-1])\n\tfilename = filename.split(\".\")\n\tfilename = filename[0][:-3]  \t# Strip.py and..\n\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\n\ndef __is_in_git_repository(filename):\n    \"\"\"\n    Checks if given file is already on the git repository\n\n    Args:\n    filename: Filename to match\n    Returns:\n    False if not a match, True if supported\n    \"\"\"\n    \n    from os.path import isdir\n    if is_git_repository(filename):\n        return False\n \n    repo_path = join(dirname(__file__), \"..\", \".git\")  \n    if isdir(repo_path):  \n\t    return True\n    return False\n\n    return get_git_file_exists(filename)\n \n\ndef __is_in_python_dir(dirname"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef is_valid_module_file(filename):\n    \"\"\"\n    Check if a filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n\tReturn: True if we match, or an error if not found\n    \"\"\"\n        if not filename.endswith('.py'):\n                return False\n        base_mod_name, extension = os.path.splitext(filename)\n        if extension in ['.pyf', '.so', '.pyd']:\n                return True\n        elif base_mod_name + '.pyc' in filename:\n                return True\n        return False\n\ndef check_filepath_path("}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef check_extension(header):\n    # Check if the extension is known to ECLIPSE header files\n    # If the extension is an ECLIPSE header file, check its mime type\n    root, ext = os.path.splitext(header)\n    if ext in [\".EDC\",\".ELF\"]:\n        ext = \".ELF\"\n    if ext + \".tbz2\":\n        return True\n    else:\n        return False\n\ndef check_file(filename):\n    # Check if the filename is a type that this module supports, if it is in the correct format\n    root, ext=os"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef __formatType(obj, type):\n\t\"\"\" This function formats a type and returns a string type that this object supports\n\t\"\"\"\n\t# If the type is a string, this function returns the object as a string\n\ttry:\n\t\tif type=='str':\n\t\t\treturn obj\n\t\telif type=='string':\n\t\t\timport unicodedata\n\t\t\timport warnings\n\t\t\twarnings.warn(\"The string module is deprecated, please use unicode as a pattern, such as \\\n\t\t\tUnicodePattern or %s. This is now deprecated. Use unicode as type.\" % type)\n\t\t\treturn unicode(obj)"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef exists(filename):\n\t\"\"\"\n\tTest if filename exists\n\t\"\"\"\n\treturn os.path.exists(filename)\t\n\ndef is_exe(file):\n\t\"\"\"\n\tTest if filename is an executable\n\t\"\"\"\n\treturn os.path.isfile(file)\t\n\ndef is_dll(file):\n\t\"\"\"\n\tCheck if file is a DLL\n\tReturns False if not a dll\n\t\"\"\"\n\treturn os.path.isfile(file)\t\n\t\t\ndef get_dll(file):\n\t\"\"\"\n\tGet dll from dll file"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef IsPythonModule(filename):\n\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\"\"\"\n\ttry:\t\n\t\tname = os.path.split(filename)[1]\n\texcept Exception,e:\n\t\treturn True\n\t\t# If we can't find the name, that's the case if there is an unknown module\n\t\t# the module is of interest\n\t\t# if not filename.find(\"'\")==-1:\n\t\t# \treturn True\n\t\t# else:\n\t\t# \treturn False\n\ttry:\n\t\tfile = open"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef is_dir_or_file(filename):\n    if os.path.isfile(filename):\n        return True\n    if os.path.isdir(filename):\n        return True\n    return False\n       \ndef is_package_dir(filename):\n    return is_dir_or_file(filename)\n\ndef is_module_dir(filename):\n    return is_dir_or_file(filename)\n\ndef is_dir_or_file(filename):\n    return is_module_dir(filename)\n\ndef get_module_files(directory):\n    f = []\n    if os"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef validate_frequency(frequency):\n\t\"\"\"\n\tGiven a string a frequency (like a number/unit), return an integer or None\n\traise a ValueError in case of an invalid frequency.\n\n\tRaises ValueError if not valid.\n\t\"\"\"\n\tif frequency == '':\n\t\treturn None\n\tfrequency = frequency.lower().strip().upper()\t\t# Remove all special terms from freqs.\n\tfrequency = frequency.split('.')\n\tfrequency = len(frequency)\n\tfrequency = frequency[0:frequency.find('.')]\t# Remove 0, 1 from the end.\n\tfor"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_interval(frequency, start_time=None):\n\t\"\"\"Parse an interval represented as a string of the form 1..12.\t\n\tIf the frequency is empty but the given start time is not, return None.\"\"\"\n\ttry:\n\t\tif start_time is None:\n\t\t\tstart_time = datetime.datetime.utcnow()\n\t\tstart_date, start_time_plus_one = start_time.split(\" \")\n\t\tstart_date = datetime.date(year=int(start_date), month=int(start_time_plus_one))\n\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef monthrange(year, month):\n\t\"\"\"\n\tConverts a string into a month numbers string.\n\n\tThe year must be in the range 1 - 12 and the month must be in 0 - the month\n\tmust be 0 - the month must be a number including 0.\n\t\"\"\"\n\tmonths = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n\t\t  \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n\tstr_date = \"%s-%s-%s\" % (year, month, months[month-1])\n\tnum = months"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n@memoize\ndef _get_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\t\n\tif isinstance(frequency, basestring):\n\t\tfrequency = get_default_frequency(frequency)\n\tif frequency == \"never\":\n\t\treturn None\n\tif isinstance(frequency, int):\n\t\tfrequency = timedelta(weeks=frequency)\n\tif frequency == \"year\":\n\t\tstart, end = get_fiscal_year(date=frequency)[1]\n\t\treturn start\n\tif frequency == \"month\":\n\t\tstart, end = get_fiscal_year(date"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef now():\n\t\"\"\"\n\tTimezone specification.\n\tReturns a string in the format \"4 weeks 4 days\" \"seconds\" or \"9 mseconds\".\n\t\n\tUsage:\n\t>>> now()\n\t'days since 12.00 AM'\n\t>>> now().replace(hour=6, minute=4, second=15, microsecond=123)\n\t'0:00:00 PM'\n\t\"\"\"\n\treturn datetime.timedelta(days=nowHours or 1, seconds=1)\n\ndef current():\n\t\"\"\"\n\tReturn current timestamp.\n\tUsed in the"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\n\tif len(freq)==0:\n\t\treturn None\n\ttry:\t\n\t\treturn datetime.timedelta(weeks = float(freq.split(\" \")[0]))\n\texcept ValueError as e:\n\t\traise_with_traceback(ValueError(\"Invalid frequency: %s\" % str(e)))\n\ndef parse_timestamp_string(timestamp):\t# From http://web.archive.org/web/2006071235/http://www.iana.org/assignments/timestamppicker/\n\tif timestamp[-1]==\"Z\":\n\t\ttimestamp=datetime.datetime.utc"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef daterange(start_date, end_date):\n\t# Check if start date can be parsed\n\tif end_date.count('/')!= 1:\n\t\traise ValueError('Start date can only have a slash.')\n\t\n\t# Use dateutil's daterange function.  This is actually fairly cheap;\n\t# the goal is to calculate the difference between the start and the\n\t# end date and return the resulting timedelta.\n\tdays_since_epoch = datetime.date(int(start_date[0:4]), int(start_date[5:7]), int(start"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef datetime_to_milliseconds(ms):\n\t\"\"\"\n\tConverts milliseconds to milliseconds.\n\t\"\"\"\n\tmilliseconds = ms/1000\n\treturn milliseconds\n\t\ndef  seconds_to_datetime(second):\n\t\"\"\"\n\tConverts seconds to a datetime.timedelta instance.\n\t\"\"\"\n\treturn datetime_to_milliseconds(second)\n\t\ndef timestamp_to_time_delta(timestamp):\n\t\"\"\"\n\tConverts a `timestamp` to a `float`.\n\t\"\"\"\n\treturn datetime_to_milliseconds(timestamp) * 1000.0\n\ndef time"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\traise NotImplementedError, 'Frequency type `{}` not implemented.'.format(type(freq))\n\nclass Interval(object):\n\t\"A class representing a time interval.\"\n\n\t__slots__ = ('end','begin','end_fixed')\n\tdef __init__(self,start=None,end=None,begin_fixed=None):\n\t\t\"Start and end values need to be given in a format used by Python strftime.\"\n\t\tif (start is not None):\n\t\t\traise ValueError('Start must not be given together.')\n\t\tif (end is not None):\n\t\t\traise ValueError('End"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n# =============================================================================\n#                               Sample classes\n# =============================================================================\n\nclass Sample:\n\t\n\tdef __init__(self, unit=\"days\", start=None, end=None, periods=None, \n\t\t\t\t\ttdelta=timedelta(1), name=None,\n\t\t\t\t\tdescription=None,\n\t\t\t\t\ttags=None, \n\t\t\t\t\tcategory=None,\n\t\t\t\t\tbackground=None,\n\t\t\t\t\tbackground_color=None,\n\t\t\t\t\tbackground_image=None,\n\t\t\t\t\tbackground_alpha=None,\n\t\t\t\t\tbackground_alpha_fill=None,\n\t\t\t\t\tbackground_fill=None,\n\t\t\t\t\tdisplay_unit=None"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\t  \n        sock.connect((host, 80))             \n        #sock.setblocking(0) \n        s = sock.makefile(\"rb\")\n        s.write(\"Welcome\\r\\n\")\n        s.flush()\n        result = sock.recv(1024)\n        #print (result)\n        if result == \"Welcome\":       \n            print(\"[+] Confirmed connection from \" + host)\n    except socket.error, msg:\n        print(\"[!] Error : \" + str(msg))        \n            \n    sock."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\treturn (socket.getfqdn(host) == \"localhost\" or\n\t\t\t\tsocket.getfqdn() == socket.getfqdn(socket.gethostname()))\t\t\n\t\n\nif __name__ == \"__main__\":\n\tsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\thost = socket.getfqdn(socket.gethostname())\n\tprint(host) + \" is up: \" + socket.getsockname()[0]\n\tif \"localhost\" in sys.argv:\n\t\tlocalhostname = socket.getfqdn(sys.argv[\"localhost\"])\n\telse:\n\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\n\tconn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tconn.connect((host, 80))\n\n\tdata_borrower = conn.recv(2048)\n\n\tif data_borrower[:2] == b\"200\":                ## Check if the host is 192.168.1.33\t\n\t\tconn.close()\n\n\tif data_borrower[:2] == b\"250\":\n\t\tif data_borrower[0:2] == b\"250\" :       ## Check if this host contains 223 bytes\t\t\t\t\n\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef isLocalHost(host):\n\t\"\"\" Checks if the host's ip is reachable (not remote) \"\"\"\n\ttry:\n\t\tsocket.gethostbyname(host)\n\texcept socket.error:\n\t\treturn False\n\treturn True\t\t\t\ndef readConfig():\n\tconfig=ConfigParser.ConfigParser()\n\ttry:\n\t\tfd = open(configFile, 'r')\n\t\tconf = fd.readlines()\n\t\tfor s in conf:\n\t\t\ts=(s.replace(\"#\", \"\").replace(\" \", \"\"), str(Config.get(s, \"ip\").lower()))\n\t\t\tfp."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef check_ipaddress(host):\n\t\"\"\"This functions checks if an ipaddress is available in the system\"\"\"\n\ttry:\n\t\tsocket.inet_aton(host)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\nif __name__ == '__main__':\n\tglobal HOST\n\tHOST = 'localhost'\n\tprint\"Checking hostname:\", HOST\n\t# Hostname\n\thost_test = \"localhost.localdomain\"\n\thosts_file = '/etc/hosts'\n\tif not os.path.exists(hosts_file):\n\t\tprint \"Host"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef print_to_screen(host, port):\n\ts=socket.socket()\n\ts.connect((host, port))\n\ts.send(\"show host\\n\")\t\n\ts.recv(500)\n\ts.close()\t\n\t\ndef load_host_file(host_file):\n\tf=open(host_file, 'r')\n\tall_hosts=f.read()\n\tf.close()\n\treturn all_hosts\n\t\ndef get_hosts():\n\thosts=[]\n\thostfile=open(hostfile_name,"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef isLocalIP(host):\t\t# Checks if an IP is local (IP v6)\n\t\n\ts = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\t# Create a UDP socket and bind it\n\ts.bind((host,0))\t# Bind the socket to the port\n\t\n\t\n\tfor ip in s.getsockname()[0]:\t\t\t\t\t# Query the IP address to find the correct host\n\t\t\n\t\tprint 'Checking if the IP is local to %s...' % ip\n\t\t# Check the version of the OS\n\t\tresult = os.system('ping '"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t\n\t# If the hostname does not contain a forward slash\n\tif host.find(\"/\") == -1:\n\t\treturn False\n\t\t\n\t# If port/url are empty/port/url is empty\n\tif (host.find(\"://\") == -1  or host.find(\"http\") == -1):\n\t\treturn False\n\t\t\n\t# If the port isn't http/https/, don't get the host\n\tif host.find(\"https\") == -1:\n\t\treturn False\n\t\t\n\t# Check if this host is the localhost \n\tif host == \"\" or host.find(\"[\")"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\twith socket.socket(socket.AF_INET, socket.SOCK_STREAM, socket.IPPROTO_TCP) as s:\n\t\t\tres = s.connect_ex((host, 80))\n\t\t\treturn True\n\ndef startListening(server, port):\n\t\"\"\" Checks if the host is listening\n\n\tReturns: returns True if the socket exists and a connection exists \"\"\"\n\tif server is not None and port is not None and server!= \"\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\" Sends the message to the server (socket) in loop \"\"\"\ndef sendMsg("}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef isLinux():\n\t\"\"\"Check if sorry is a Linux system\n\n:param host: The hostname or ip\n:return: True if the host is not localhost else False\"\"\"\n\timport commands\n\tp = subprocess.Popen([\"/bin/df\"], stdout=subprocess.PIPE)\n\tp.wait()\n\treturn\n\ndef ip():\n\t\"\"\"Returns the system ip on the host (this is always 1.2.3.5)\"\"\"\n\treturn commands.getoutput(\"/usr/sbin/sysctl -p\")[1].split()[0]\n\t\nclass PcapListener"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\ndef build_re(pattern, prefix, repl):\n\t\"\"\"\n\tConverts a regexp pattern into a regular expression. See re.compile for for\n\texplanations.\n\t\"\"\"\n\n\t# replace 'foo\\\\(.*\\\\){bar}' part with new-line\n\tregexp = ''\n\tfor index, p in enumerate(prefix):\n\t\tregexp += '(' + p\n\t\twhile True:\n\t\t\tindex = find_end_of_pattern(pattern, regexp)\n\t\t\tif index > -1: break\n\t\tregexp += ')'\n\t\tprefix = pattern[index:]\n\treturn re"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\n\n# Path Matcher class\nclass PathMatcher(PatternMatcher):\n\t\"\"\"\n\tA pattern matching against a list of paths.\n\tMatches the given glob patterns if they are on the filesystem; by default this matches\n\tthe whole directory path::\n\n\t\t$ find /bin/foo.txt\n\t\t$ find /bin/**/*\n\n\t\"\"\"\n\tname = 'path'\t# Name matched against\n\tdisplay = 'path'\n\tpattern = re.compile('^[*?[]\\.$|/.+$]')\n\n\tdef __init__(self, patterns):\n\t\t\"\"\"\n\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nclass CommandRunner(object):\n\t\"\"\"CommandRunner is an abstract class, that will be called any time a script needs\n\tto find a file, and it will get an environment instance used to determine the path, a\n\tdictionary of substitutions that will be used to fill in the path.\n\t\"\"\"\n\tsettings_spec = {\n\t\t'command' : 'command',\n\t\t'argv_split': 'args',\n\t\t'env': 'env',\n\t}\n\n\tdef __init__(self, working_dir, paths, file_patterns, env):\n\t\t\"\"\"Initializes a Command"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\treturn [re.compile(p) for p in patterns]\n\n\tdef recurse(node, patterns):\n\t\t\"\"\"Recursively search patterns tree into paths of nodes.\n\n\t\tThe purpose is to handle the \"walking through\" of the search path before it's\n\t\tthe children. (These are called 'walking' through and are the children of\n\t\tevery node.)\n\t\t\"\"\"\n\t\tfor pattern in patterns:\n\t\t\tsearch_paths = node.findall(pattern)\n\n\t\t\tfor i in search_paths:\n\t\t\t\trecurse(i, patterns)\n\t\treturn node\n\n\trecurse(list"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\t# TODO: reintroduce a'recursive replacement'\n\ndef ensure_directory(path):\n\t\"\"\"Ensure that 'path' is a directory.\"\"\"\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\ndef main():\n\t\"\"\"This should be called to generate the example app.  Call it like this:\n\tpython3 extract_regex('^a\\.py$', r'a\\.py$', 'lib/*.lib')\n\t\"\"\"\n\tp = argparse.ArgumentParser()\n\n\tp.add_argument('-d', '--directory',\n\t\tdefault="}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tdef recurse_dir(path):\n\t\t\t\"\"\"\n\t\t\trecursively convert os-specific directory entries into\n\t\t\tsub-directories:\n\n\t\t\t@param path: a path (unicode or bytes)\n\t\t\t@return: the given path as a list of strings, joined by slashes\n\t\t\t\tstarting with '/'.\n\t\t\t@param type: either L{b'').split} or L{b'/')}\n\t\t\t@raise AssertionError: in case of an invalid path, where subpath\n\t\t\t\tdoes not exist\n\n\t\tdef recurse(path):\n\t\t\t\"\"\"\n\t\t\trecursively convert unix style directory entries into\n\t\t\tsub-"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tif \"replaces\" not in mysettings:\n\t\t\tif not \"patterns\" in mysettings:\n\t\t\t\traise SafeException(\".patsubst requires at least one pattern to be replaced\")\n\t\t\tpatterns = mysettings[\"patterns\"]\n\t\telse:\n\t\t\tpatterns = mysettings[\"patterns\"]\n\n\t\tresult = []\n\t\tsep = os.sep\n\n\t\tfor pat in patterns:\n\t\t\tresult.append(re.compile(expr, re.IGNORECASE))\n\t\tfor pfx in myglobber([pattern.replace(\"\\\\*\", os.sep) for pattern in patterns]):\n\t\t\tfor path in glob.glob"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tpath_fragments = []\n\t\tfor fragment in options.get('find_paths'):\n\t\t\tpath_fragments.extend(\n\t\t\t\tre.findall(\n\t\t\t\t\tpattern,\n\t\t\t\t\tpath_fragments,\n\t\t\t\t\tflags=re.M)\n\t\t\t\t)\n\t\tif path_fragments:\n\t\t\tfor fragment in path_fragments:\n\t\t\t\tyield fragment\n\t\treturn\n\n\traise RuntimeError(\"No files found matching: {!r}\".format(\n\t\t', '.join(options.get('find_paths'))))\nfrom pypov.pov import Texture\n\nclass TestTexture:\n    \"\"\"\n   "}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t\n\tdef __init__(self, glob_patterns):\n\t\tself.glob_patterns = [re.compile(p) for p in glob_patterns]\n\n\n\t__call__ = lambda self, str_data: self.glob_patterns\n\tdef match_path(self, path_data, **kwargs):\n\t\tfor pattern in self.glob_patterns:\n\t\t\tmatched = path_data.find(pattern)\n\t\t\tif matched:\n\t\t\t\treturn path_data[:matched.end()]\n\n\ndef _filter_by_regex(data, keys=('root', ), match_type='"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\n\tdef myformat(p):\n\t\t\"\"\"\n\t\tReturn p formatted for logging to stderr. Usefull for debug\n\t\tprint statements or errors\n\t\t\"\"\"\n\t\n\t\tdef pfn(s):\n\t\t\ts = re.sub(r'\\W', '', s)\n\t\t\ts = s.replace(r'\\s','')\n\t\t\treturn s\n\t\t\n\t\treturn pfn\n\t\n\tpatterns = [p for p in patterns.split('\\n') if p]\n\tpaths = [p for p in paths if p]\n\n\tmatchers = [re.compile(p) for p in patterns]"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\t#if 'C:/Users/mario/Documents/Git/githash/src/test_git_1/setup.sh' not in sys.path:\n\t\t\t#sys.path.append('./')\n\t\treturn subprocess.call([self.run_cmd,'setup.sh'])\n\n\tdef check_tag(self, tag):\n\t\tif os.path.isdir('./git-stash'):\n\t\t\treturn os.path.isdir(os.path.join(self.git_checkout(), 'git-stash'))\n\t\telse:\n\t\t\treturn os.path.isdir('./"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_user_id():\n\t\"\"\"Returns UserID\"\"\"\n\t\n\ttry:\n\t\treturn os.getuid()\n\texcept:\n\t\treturn -1\n\t\t\n\t\t\ndef get_path(path):\n\t\"\"\"Gets path\"\"\"\n\t\n\treturn os.path.abspath(os.path.expanduser(path))\n\t\ndef get_user():\n\t\"\"\"Gets User ID\"\"\"\n\t\n\ttry:\n\t\treturn pwd.getpwnam(get_user_id()).pw_uid\n\texcept:\n\t\treturn \"\"\n\t\t\n\t\t\n\ndef check_command(command):\n\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef check_path(root, path):\n\t\"\"\"\n\tif this dir does exist => True else False\n\t\"\"\"\n\tcur_dir = os.path.abspath(os.curdir)\n\twhile next_dir!= root:\n\t\tnext_dir, name = os.path.split(next_dir)\n\t\tif name == path: # current path exists, stop looping\n\t\t\tprint(\"\\n!! This directory exists, but is empty --> \" + next_dir + \".\")\n\t\t\tbreak\n\t\tif os.path.isfile(os.path.join(next_"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\treturn os.getenv(\"GIT_TEST\")!= None\n\ndef is_running(cmd):\n\treturn os.system(cmd) == 0\n\ndef is_dirty(cmd):\n\treturn os.system(cmd) == 1\n\ndef get_version():\n\treturn os.environ.get(\"GIT_TAG\")\n\ndef get_log(cmd):\n\treturn os.system(\"%s --pretty=format:%%ai %s\", cmd)\n\ndef gitinit(dir):\n\t\"Initializes git-multimails directory\"\n\n\tif check_version(git"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ntry:\n    from git import *\n    from git import Git\nexcept:\n    pass\n\nclass GitCmd(object):\n    \"\"\"\n    This code is created from the git_cmd.pl script included for git_archive. As the following is provided\n    use the commands as well.\n\n    @author  : Liangjun Zhu\n    @changelog : \n    \"\"\"\n\n    def __init__(self, git_cmd):\n        super(GitCmd, self).__init__()\n        self._git_cmd = git_cmd\n        self._status = os.system('./git_cmd.sh ')        \n\n    def initialize(self"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_all_configs(directory):\n\t\"\"\"This function gets all configs from a certain directory\"\"\"\n\t\n\ttry:\n\t\tsubprocess.call(['git', 'pull', '--only-tags', '--prune-empty', '-z'],\n\t\t\t\t\t\tcwd=directory)\n\n\t\tfor cmd in ['git', 'diff', 'HEAD']:\n\t\t\tsubprocess.call(cmd, cwd=directory)\n\t\tsubprocess.call(['git', 'checkout','master'], cwd=directory)\n\t\t# Get all files in the folder\n\t\tfor this_file_path in os.listdir(directory):\n\t\t\tif"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef do_git(executable):\n\tf = open(os.path.join(git_root_dir, \"config\", \"config.yaml\"), \"r\")\n\t\n\tf_raw = f.readlines()\n\t\n\tf_raw = \"\".join(f_raw[:-2])\n\t\n\tfor line in f_raw:\n\t\t\tif \"include-path\" in line:\n\t\t\t\tfor part in executable_path:\n\t\t\t\t\t\tif part == line:\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\n\t\tbreak\n\n\treturn False\n\t# encoding: UTF-8\n\n'''"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef getGitbinPath():\n\t\"\"\"\n\treturns the path to the git binary in the current working dir\n\tshould have \"git\" and \"hg\" keys with path\n\t\"\"\"\n\t\n\treturn r\"{0}{1}\".format(RsrcFile(), BgSrcFile())\n\t\ndef checkRepoExists(path):\n\t\"\"\"\n\tChecks if path exists\n\tchecks if path exists and returns the flag\n\t\"\"\"\n\t\n\tif not os.path.exists(path):\n\t\tprint('Path does not exist')\n\t\t\n\t\treturn False\n\t\n\telif not os.path.isdir(path"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\treturn os.getenv('ANNEXUS_GIT_PYTHON_VERSION') is not None\n\n#===================================================================================================\n# Utils\n#===================================================================================================\n\ndef get_env(key='PATH'):\n\t\"returns a dictionary with the given key/value pairs expanded\"\n\treturn os.getenv(key) or {}\n\n\ndef get_host_tag():\n\treturn os.getenv('ANNEXUS_HOST_TAG')\n\ndef cleanup(prefix=None):\n\t\"\"\"\n\tClears the output, if there are any.\n\tIf this keyword is empty, clears"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_last_commit_revno():\n\t\"\"\"\n\treturns the last git commit that was run last (or -1) before calling run as a bash shell\n\t\"\"\"\n\t# TODO: make all these non-interactive to work with python 2.x\n\treturn read_file(\"../.vimhistory/.git/history\")[-2]\n\ndef get_cur_file_path(current_file):\n\t\"\"\"\n\tReturns the absolute file path of the last selected file\n\t\"\"\"\n\t# TODO: make all these non-interactive to work with python 2.x"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n \tif not os.path.exists(output_dir):\t\n\t\tos.makedirs(output_dir)\n\t\t\n\tfp=open(\"%s/%s\"% (os.path.dirname(__file__), output_filename))\n\t\n\toutfile = codecs.open(\"%s/%s.yaml\"%(os.path.dirname(__file__), output_filename), \"w\",\"utf-8\")\n\tif overwrite:\n\t\tfor path in paths:\n\t\t\t# make a folder for the output and output dir if it does not exist\n\t\t\ttry:\n\t\t\t\tos.makedirs(os.path.join(output_"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tif (os.path.exists(ConfigSubsFilename)):\n\t\t\t# File already exists\n\t\t\tConfigSubExists = True\n\t\telse:\n\t\t\t# Create it\n\t\t\tcreate_ConfigSubsFile()\n\n\telif (os.path.exists(ConfigFileSubsFilename)):\n\t\t# File already exists\n\t\tConfigFileExists = True\n\t\tConfigSubExists = False\n\telse:\n\t\t# Create it\n\t\tConfigFileExists = True\n\t\tConfigSubExists = False\n\n\t# Write file\n\ttry:\n\t\t# Write file to disk\n\t\ttry:\n\t\t\tfd ="}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\t\n\tconfig = load_config(cfg_source)\n\troot_path, ext = os.path.splitext(cfg_source)\n\troot_dir = root_path + \"_themes/Theme\"\n\tsource_dirs = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n\tfor source_dir in source_dirs:\n\t\tdest_dir = root_dir + \"/\" + source_dir[:-5]\n\t\tif not os.path.exists(dest_dir):\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n     def create_config_dirs():\n\t\t# Create required directories if necessary so that we can properly create them\n\t\tif not os.path.exists(config_dir):\n        \tos.makedirs(config_dir)\n        \tprint \"Created default configuration directory at: \" + config_dir + \" \" + os.environ['TEMP'] + \"; \"\n        \n         # For each of the directories in the config yaml, create all the needed directories\n        \tfor config_dir in os.listdir(config_dir):\n        \t\tif not config_dir.endswith(\"\\\\\"):\n        \t            dir_path_template"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\t#If self.config and not os.path.isfile(self.config):  #No config, but no config.yml => create it.\n\t\tpass\n\n\t\t#if not os.path.isdir(self.configdir):\n\t\t#\traise Exception(\"Configuration dir does not exist and is not a file!\")\n\n\t\t#Save self.config as a dictionary to the new config\n\t\td={}\n\t\tself.config=d\n\n\t\tdef _write_data(d={},k='',v=''):\n\t\t\t\"\"\"Write data to file.\"\"\"\n\t\t\twith open(self.config) as file:"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\t\n\tdef write(self, target):\n\t\twith open(target, 'w') as fh:\n\t\t\tself.dump(self.config, fh)\n\t\t\n\tdef dump(self, target, stream=None):\n\t\tdata = self.env.get_template(\"config_%s.json.j2\" % target)\n\t\tif not stream:\n\t\t\tshutil.copy2(os.path.join(self.target_folder, target), data)\n\t\telif os.environ.get('DOCKER_DEBUG', None) == '1': # for testing purposes\n\t\t\tprint(\"DEBUG"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t \n\tif os.path.isfile(path):\n\t\t# If a file already exists, delete it.\n\t\tif os.path.isfile(path):\n\t\t\tos.remove(path)\n\t\t\t\n\t\t\t# Else, overwrite the existing config file.\n\t\t\tos.rename(path, filename)\n\telse:\n\t\tdata = {}\n\t\tdata[\"name\"] = \"config\"\n\t\tdata[\"sections\"] = yaml.load(filename)\n\t\t\t\n\t\t# Check if the file is not yet exists, create new config as necessary.\n\t\tif data[\"sections\"][\"version\"] > 1:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\t#check in case of an override:\n\tif not os.path.isfile(os.path.join(os.path.expanduser(\"~\"), \".opal.conf\")) and not overwrite:\n\t\traise IOError(\"Configuration file %s does not exist.\"%os.path.join(os.path.expanduser(\"~\"), \".opal.conf\"))\n\t\t\t\n\ttry:\n\t\twith open(os.path.join(os.path.expanduser(\"~\"), \".opal.conf\"), \"r\") as conf_file:\n\t\t\tconf_string = conf_file.read()\n\t\t#"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\tif not os.path.isfile(sys.argv[0] + \"/config.yml\"):\n\t\tprint(\"File path \" + sys.argv[0] + \"/config.yml does not exist!\")\n\t\tsys.exit(1)\n\twith open(sys.argv[0] + \"/config.yml\") as conffile:\n\t\tconfig = yaml.load(conffile.read())\n\t\tglobal c_path\n\t\tc_path = path_abspath + \"/\" + sys.argv[0] + \"/\" + \\\n\t\t\t\tsys.argv[1].split('.')[0] +"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\nclass ConfigFiles(object):\n\t\"\"\"\n\tThis is the implementation of this class. I can pass in both config files and\n\tconfig file paths to be read.\n\t\"\"\"\n\tdef __init__(self,config,path):\n\t\tself.config\t\t= config\n\t\tself.path\t\t= path\n\n\t\tif not os.path.exists(path): path = os.path.join(self.path,\"/..\")\n\n\t\tself.source\t\t= os.path.join(path,self.config[\"source_dir\"],self.config[\"key_prefix"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\n\n# coding=utf8\n\n\"\"\"\nThis script reads a list of strings (with comments) from a file and writes\neach of them to a text file with all relevant features, namely with punctuation,\nlanguage, and an integer ID line per line.  (To get a rough match, use a line for\nthe comments inside a file. Use this module carefully before using it)\n\nIt is intended for use as an auto-incrementing script.\n\"\"\"\n\nimport fileinput\nimport re\n\n# read the lines from a file\nwith open('README.txt', mode='"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n#\t\treturn re.sub(\"\\s+\", \" \", script)\n\treturn \"\\\"\" + script + \"\\\"\\n\"\n\n#\tdef script_line(script, args):\n#\t\t\"\"\"\n#\n#\t\tExecutes a single ssh command.\n#\n\ndef script_line(script, args):\n\t\"\"\"\n\tExecutes a single ssh command with args and send the command\n\tto the server with a 0 exit code if run successfully.\n\t\"\"\"\n\treturn script_no_end(script + \"\\n\" + \"\\n\".join(args)) if"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tscript = str(script) if script else \"\"\n\t\tprint \"using '%s' as script...\" % script\t\n\n\t# create a dictionary of commands\n\tcommands = {}\n\tfor _script_id, script in script.items():\n\t\tcommands[_script_id] = str(__import__('commands.%s' % script).COMMANDS[_script_id])\n\n\n\t# run scripts\n\twith open (\"%s.commands\" % script, \"w\") as f:\n\t\tf.write(\" \".join(commands.values()))\n\n\ndef get_user(username):\n\t\"\"\"Return the"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef get_name(path):\n\t\"\"\"\n\tlooks the current and parent folders for names that will be folders\n\t\"\"\"\n\ti= 0\n\ttry:\n\t\twhile(True):\n\t\t\tname=path+\".\"+str(i)+\"_\"+str(len(path)+1)\n\t\t\t#print(name)\n\t\t\ti=i+1\n\t\t\tif os.path.isfile(name):\n\t\t\t\tbreak\n\t\treturn name\t\t\n\texcept:\n\t\tprint(\"Error, failed to read file\")\n\t\treturn \"\"\n\ndef gen_name(name):\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn script + \"\\n\"\n\ndef run(commands, shell=True):\n\t\"\"\" execute commands given one or more commands for execution.\n\tShell defaults to False if you don't want them.\n\n\t:param commands:\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tp = Popen(shlex.split(commands), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tout, err = p.communicate()\n\texcept Exception, ex:\n\t\tprint ex, ex.output\n\t\tprint err\n\t\treturn\n\n\tif shell:\n\t\tfor line in"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\n\twith io.StringIO() as file:\n\t\tcommand_line = textwrap.dedent(script)\n\t\tprint(\"command line:\\nprom {0}\".format(\" \".join(command_line)))\n\t\tprint(\"exiting\")\n\t\tsys.stdin = file\n\t\tprint(file.read())\n\n\tresult = runCommand(command_line)\n\tprint(\"result:\", result)\n\n\ndef executeCommand(command):\n\tif len(command) < 1:\n\t\tprint(\"entering\")\n\t\ttry:\n\t\t\texec(command)\n\t\texcept Exception as e:\n\t\t\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef write(text, fn, encoding=\"utf-8\"):\n\tfn = open(fn, \"w\")\n\tfn.write(text.encode(encoding))\n\tfn.close()\n\t\ndef get_shell_cmd(session, cmd, cwd=None, env={}, no_input=False,\n\t\tsilent=False, error_to_stderr=True):\n\t# session must be a callable to be used as a string and not a function that returns the same\n\t# number.\n\n\tif hasattr(session, \"prefork\"):\n\t\tif not \"_SHELL\" in"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\t\n\ttmp = script.split(\" \")\n\tret = \"\"\n\tidx=0\n\twhile idx < len(tmp):\n\t\tif (idx + 1 < len(tmp)):\n\t\t\tret = ret + \"  \" + tmp[idx + 1]\n\t\t\tidx = idx + 1\n\t\tidx = idx + 1\n\treturn ret\n\nclass ssh:\n\t\n\tglobal __user_cmds\n\tglobal __hostname\n\tglobal __password\n\tglobal __port\n\tglobal __ssh_pass\n\n\tdef __init__(self,host,user,pw,name,password,port="}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\twith open(script) as f:\n\t\t\tfor j, line in enumerate(f):\n\t\t\t\tprint f'{j}: #{line.rstrip().rstrip()}'\n\t\t\tprint ''\n\n\t\t\n# -*- coding: utf-8 -*-\n\nfrom django.db import models, migrations\n\nimport oioioi.oioioi.fields\nimport oioioi.base.fields.ranges\nfrom django.conf import settings\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('odk_viewer', '0023_remove_ocr',\n                 '0006_remove_"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\t\n\tscript = script.rstrip(script)\n\tif script.strip().endswith('>'):\n\t\tscript_split = script.split('\\n', 1)\n\t\tfor script_split in script_split[1:]:\n\t\t\tscript_split[0] = script_split[0].rstrip()\n\t\toutput_path = './' \n\telse:\n\t\tf = open(script, 'r')\n\t\toutput_path = script\n\t\tfor line in f.readlines():\n\t\t\tline_split = line.split(' ')\n\t\t\tif len(line_split)!= 3:\n\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\tresult = subprocess.run(['python2', sys.executable, '-c', 'print(sys._getframe(1).f_code.co_name)', argfile], **cmdargs)\n\treturn result.stdout\n\n\n\n\ndef check_output(code, expected):\n\t\"\"\"\n\tCheck that code produced `expected`, containing expected code.\n\t\"\"\"\n\tassert code == expected\n\ndef main():\n\t\"\"\"\n\tExecute commands and look for any exit codes that we need.\n\t\"\"\"\n\tassert subprocess.call(['python2','--help'], stdout=subprocess.PIPE).returncode"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\tif extra_env is not None:\n\t\t\tsubprocess.run(extra_env, shell=True)\n\t\t\treturn\n\t\n\tif args is None:\n\t\targs = [ '-c', 'import time; time.sleep(\\'0\\');']\n\t# Call the function\n\tif sys.platform.startswith('win'):\n\t\tproc_args = subprocess.Popen(args, stdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess.STDOUT)\n\telse:\n\t\tp = subprocess.Popen(args, shell=True, **subprocess_options)\n\n\t# Check result\n\tstdout,"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\nclass RunOutput:\n\t\"\"\"\n\tObject to hold a result of an call of an external program.\t\"\"\"\n\tdef __init__(self, stdout=None, stderr=None):\n\t\t\"\"\"\n\t\tInitialize a new RunOutput.\n\t\n\t\tArgs:\n\t\t\tstdout (str, optional) :\n\t\t\t\tstdout of the result.\n\t\t\tstderr (str, optional) :\n\t\t\t\tstderr of the result.\n\t\t\"\"\"\n\t\n\t\tif stdout is not None or stderr is not None:\n\t\t\tself.stdout = stdout\n\t\t\tself.stderr = stderr\n\t\telse:\n\t\t\tself.stdout ="}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run_func(func, *args, **extra_env):\n    \"\"\"Run a function and return the subprocess.call command.\n    \n    Parameters\n    ----------\n    func: function\n    tuple|list|dict|str|str\n      The functions that will be tested. If set to None\n    args, dict\n        A tuple containing any command line arguments to be passed in.\n        If set to None, no extra arguments will be passed.\n    extra_env: dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n\n    Returns\n    -------\n    tuple|list|dict|str\n        The return values"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n@run_async\ndef launch(cmd, args, extra_env, stdout=None):\n\t\"\"\"Run a command and return its stdout.\n\n\tArgs\n\t---------\n\tcmd : str\n\t\tCommand to be run.\n\targs : str\n\t\targs in the string to pass to the process.\n\textra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\t\"\"\"\n\ttry:\n\t\tproc = subprocess.Popen(args=args, shell=True, cwd=os.getcwd(),\n\t                                  universal_newlines=True, stdin"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\nimport subprocess\n# Define the error object\ndef errorHandler(exceptiontype, exceptionvalue, tb):\n    traceback.print_exception(exceptiontype, exceptionvalue, tb)\n\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2010-2015 C\u00f3digo Sur Sociedad Civil\n# All rights reserved.\n#\n# This file is part of Cyclope.\n#\n# Cyclope is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run(*args, **kwargs):\n\t\"\"\"\n\tRun a function in a sub-process.\n\n\tThis function uses the subprocess module to run the function, and makes sure the subprocess is being used.\n\t\"\"\"\n\tif extra_env is None:\n\t\textra_env = {}\n\t\t\n\tprint args\n\tprint kwargs\n\tp = subprocess.Popen(args, *args, **kwargs)\n\tp.wait()\n\tif p.returncode!= 0:\n\t\traise Exception(\"Function failed : {0}\".format(p.returncode))\n\n# A function to test the"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t    \n\tif extra_env is None:\n\t\textra_env = {}\n\tparams = [\n\t\t\"import platform\",\n\t\t\"if platform.linux_distribution\",\n        \"   dist_name = 'openmpi', version = openmpi\",\n\n\t    \"setup (install)\",\n\t\t\"if platform.linux_distribution\",\n\n\t    \"  (only on target)\",\n\t\t\"  (not available on macOS)\",\n\n\t    \"  (not up-to-date)\",\n\n\t    \"  (only on target)\",\n\t\t\"  (not available on linux-i686)\",\n\n\t    \"\""}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\nimport os, os.path, glob\n\ndef start(args=None):\n    \"\"\"\n    Invoke the process and return a tuple of its return code and the\n    signal.\n\n    The return is a tuple of the following form:\n        ret_code (int) = (0, None), the value of the return code.\n        signal (str) = the signal received by the child process if any.\n        exit_code (int) = (0, None) the process's exit code if any of the\n        return code is non-zero.\n    \"\"\"\n\n    if args is None:\n        args = ()\n   "}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\"\"\"Default commands\"\"\"\n# ------------------------- #\n# --- Function --- #\n# ------------------------- #\ndef main():\n\tprint sys.argv\t\t\t\t# Arguments used by the command-line parse code\n\tglobal extra_env\n\twith open('commands','w') as commands_file:\n\t\tfor arg in sys.argv:\n\t\t\tif arg:\n\t\t\t\tif arg[0] == \"--\":\n\t\t\t\t\tcommands_file.write(\"-- Command line arguments. \\n\")\n\t\t\t\telif arg[0] == \"--make-cmd\":\n\t\t\t\t\tmakecmd()\t\t\t# Write all custom commands\n\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\t# TODO\n\t\t# 1. use the global variables as a fallback default.\n\t\tdef getValue(name, default=True):\n\t\t\tif default:\n\t\t\t\treturn os.environ.get(\"${\" + name.upper() + \"_}\" + name + \".${\" + name.upper() + \"_}\")\n\t\t\telse:\n\t\t\t\treturn os.environ.get(name.upper() + \"_\")\n\n\t\tenv, matcher = getVar(name, False)\n\t\tif not env:\n\t\t\tenv = os.environ.get(name.upper(), os.environ.get(\"${\""}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\n\twith open('env') as file:\n\t\tstr_default = file.read().replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n\t\t#print str_default\n\t\twith open(os.path.expanduser(param_file), 'w') as file:\n\t\t\tfile.write(str_default)\n\t\t\n\nif __name__==\"__main__\":\n\tmain()\"\"\"\nSupport for Ring Door Bell/Chimes.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/ring/"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif not matcher:\n\t\t\traise Error('No matcher name given')\n\t\telse:\n\t\t\t# Try to match\n\t\t\tvalue = matcher.group(1).strip().strip('\\n')\n\t\t\tif len( value ):\n\t\t\t\tdefault = value\n\t\t\t\tlogging.debug('Found config option %s', matcher)\n\t\t\telse:\n\t\t\t\tlogging.debug('No key/value supplied')\n\t\t\treturn default\n\n\tdef get(self, name, matcher, default = None, throw = False, default_value = None):\n\t\tif not name:\n\t\t\t#raise Error('Match keyword is missing"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\nclass BagConfig:\n\t\"\"\"\n\tGet the command line arguments for BAG-BAG-DB-REPORT\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\"\"\"\n\t\t#print(\"INIT BAG-BAG-DB-REPORT\")\n\t\tprint(\"Parsing config...\")\n\t\tself.argc=0\n\t\tself.arg1=0\n\t\ttry:\n\t\t\topts, args = getopt.getopt(sys.argv[1:], \"i:\")\n\t\t\tself.configfiles=[]\n\t\t\tfor opt, arg in opts:\n\t\t\t\tif opt == '-h':\n\t\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\n\tenvVar = ''\n\tif(not variable):\n\t\tprint \"The specified variable is not defined.\"\n\t\tprint os\t\n\t\tsys.exit()\n\n\telif(variable == ''):\n\t\tprint \"The specified variable is used as default.\"\n\t\tprint \"\"\n\t\tprintos.system(['bash', 'echo \"$ENV_OSTYPE\" > ~/dsp/env.d/bin/python2.7.27.py'])\t\n\t\tos.system('python2.7.27.py')\n\t\t\n\telse:\t\n\t\tprint \"The specified variable must be defined"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tenv_keys = []\n\t\tenviron_type = 'all'\n\n\t\tif env_list:\n\t\t\tfor k in env_list:\n\t\t\t\tif k in os.environ:\n\t\t\t\t\tenv_keys.append(k)\n\t\t\t\telse:\n\t\t\t\t\tenviron_type = 'all'\n\t\t\t\t\tprint('%s was not given in environment, using default %s.' % (k, os.environ[k][1]))\n\n\t\telse:\n\t\t\tenv_keys.append(None)\n\n\t\tif env_type and (env_keys[-1] == None):\n\t\t\tenv_keys[-"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tvalue = os.getenv(key)\n\t\tif not value:\n\t\t\traise Error(\"[global] Environment variable \\\"%s\\\" not provided.\" % key)\n\t\telse:\n\t\t\treturn value\n\ndef getLocalVar(key):\n\t\"\"\"\n\tGet the value from the environment given an explicit name reference\n\tthat was saved in the local environment file.  If the variable is not\n\tdefined in local environment it will search using the environment default\n\tvalue.\"\"\"\n\tvalue = os.getenv(key)\n\tif not value:\n\t\traise Error(\"[global] Environment variable \\\"%s\\\""}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\n\tdef getvalue(self, value, default=None):\n\t\t\"\"\"\n\t\tGet a value from environment parameters.\n\n\t\tUsage: getvalue(${var})\n\n\t\tParameters:\n\t\t${var}\t\t(str) The value to get from env['var', if it exists, or a default value in the environment, the default value\n\t\t${default}\t\t(str) The value to return if variable is empty.\n\n\t\tReturns:\n\t\t${value}\t\t(any) The value if env[var] exists and has a value else None.\n\n\t\tRaises:\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tfor key in \"PATH env-variable PATH\".split():\n\t\t\tvalue = os.getenv(key)\n\t\t\tif value:\n\t\t\t\tif re.match('^[\\\\w.]*$',value)!= None:\n\t\t\t\t\tsetattr(self, key, os.path.expandvars(value))\n\t\t\t\telse:\n\t\t\t\t\tprint(\"warning: %s not defined in env variable:%s\"%(key, value))\n\t\tif not os.path.exists(os.path.expanduser(\"~/.my/cnf_defaults/\") + '/cnf'):\n\t\t\tprint(\"warning: Not using default configuration!\")\n\t\tif"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\t\n\tval=None\n\tfor k in os.environ:\n\t\tif os.environ[k] is not None:\n\t\t\tval = os.environ[k]\n\treturn val \n\ndef setenv(var,val):\n\tos.environ[var]=val\n\ndef getenv(var):\n\treturn os.environ[var]\n\n\n# -*- coding: utf-8 -*-\n##############################################################################\n# For copyright and license notices, see __openerp__.py file in module root\n# directory\n##############################################################################\nfrom openerp import models, fields, api, exceptions\n\n\nclass AccountAnaly"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\t# split on spaces\n\t\treturn image_href.split('/')\n\n\n##### EXCEPTIONS ##################################################################################\n\nclass Except:\n\t\"\"\"\n\tExcepts\n\trepresent the exception traceback.\n\n\tThe constructor is a very thin wrapper over the\n\t:doc:`traceback` class from Python's standard library,\n\tlike so::\n\n\t\twith Except('/tmp/tmp_file.txt') as exc:\n\t\t\traise exc\n\t\texcept ZeroDivisionError, e:\n\t\t\tprint('Ouch!')\n\tand optionally accept any number of arguments::\n\n\t\tfor exc in _throw(*sys"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\t\n\treturn (None, '', image_href)\n\t\n\t\n\ndef processImgUrls(image_href):\t\n\t\"\"\"Parse an image href into image name, host, path\n\ttuple items.\n\n:param image_href: href of an image\n:returns: tuple of the form (image_name, host, path)\n:raises ValueError:\t\"\"\"\n\t\n\t\n\timage_name = os.path.basename(urllib.parse.urlparse(image_href).path)\t\t# remove protocol or domain\n\thost = urllib.parse.urlparse(image_href).netloc\t\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\n\timport urlparse\n\tif urlparse.urlparse(image_href)[:2]!= ('http','https'):\n\t\timage_href = 'http://%s' % image_href\t\t\n\n\tm = re.search(r'http://(.*?)/',image_href)\n\tnetloc = m.group(2)\n\tuse_ssl = False\n\n\t#if netloc:\n\t#\tuse_ssl = True\n\ti = netloc.rfind(':')\n\tnetloc = netloc[i+1:] if i!= -1 else netloc\n\twhile netloc.find(':') == -1:\n\t\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef is_valid_url(url):\n\n\t# Remove leading /\n\turl = url[1:]\n\turl = re.sub(\n\t\t'[/\\\\:;,?;]+',\n\t\t'',\n\t\turl)\n\n\tresult = urlparse.urlparse(url)\n\treturn result.scheme == '' and result.netloc == '';\n\n\ndef url_to_netloc_tuple(url):\n\t\"\"\"\n\tParse a url into the internal and external parts.\n\n\tThis function parses the output of the url, specifically to get\n\tthe netloc and internal parts (i.e.,"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef parse_url_components(url, context=None):\n\t# FIXME: parse\n\tcomponents = []\n\tif context == None:\n\t\tcontext = {}\n\n\t# read URLs\n\twhile 1:\n\t\ttry:\n\t\t\tresponse = urllib.urlopen(url)\n\t\texcept ValueError:\n\t\t\tbreak\n\t\ttry:\n\t\t\tscheme = response.info().get('Content-Type').split('charset=')[-1]\n\t\texcept:\n\t\t\tbreak\n\t\telse:\n\t\t\ttry:\n\t\t\t\tcharset = response.info()['Content-Type'].split('charset=')[-1]"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef urlsplit(url, scheme='', allow_fragments=True):\n\t\"\"\"\n\tParse the image href into composite parts\n\n:param url: an image href\n\t:param scheme: \"http\" or \"https\"\n\t:param allow_fragments: If set to True and the user\n        did not specify a protocol, Trellis, Swift, or HTTP\n        will insert the http://schema.org/ImageObject\n        in the returned tuple, if it isn't already.\n\t\"\"\"\n\tproto, rest = urllib.splittype(url)\t# url parser is new in Python 2"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\t# Image does not exist\n\t\t_logger.debug(\"Image '%s' does not exist\", image_href)\n\t\tif \":\" in image_href:\n\t\t\t(username, password, host) = image_href.split(\":\")\n\t\t\tuse_ssl = True\n\t\telse:\n\t\t\t(_logger.debug(\"Image '%s' does not look like a hostname and a path\", image_href))\n\t\t\tuse_ssl = False\n\t\tnetloc_result = netloc.parse(use_ssl=use_ssl)\n\t\tif not netloc_result: raise ValueError\n\t\tnetloc = netloc_result"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef get_image_from_href(href):\n\t\"\"\"\n\tReturn image path from a href pointing to another image.\n\n\tThe method is useful for determining the path to download,\n\timagine checking whether the image exists, or if it's up to date.\n\t\"\"\"\n\tif get_image_is_local(href):\n\t\timage = urllib.urlopen(href)\n\t\timage_id = image.info().get(u'image-id')\n\t\tif not image_id:\n\t\t\traise ValueError\n\t\treturn image_id, False\n\timages_path"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\t# Quick sanity check\n\t\tif '/' in image_href:\n\t\t\timage_href = image_href.rsplit('/', 1)\n\n\t\t# Don't forget the image itself\n\t\tif image_href[0:2]!= \"http://\":\n\t\t\timage_href = \"http://\" + image_href\n\n\t\tuser_domain_match_obj = re.match(r\"\\w+://[\\w\\d\\.\\-]+\", image_href)\n\t\tif user_domain_match_obj is None:\n\t\t\traise ValueError(\"Could not parse the image href: \\\"%s\\\"\" % image"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\t_ = image_href.startswith('urn:brocade.com') \\\n\t\t\t\tand image_href.endswith('/') \\\n\t\t\t\tand len(image_href) == 36\n\tif image_href:\n\t\tparts = image_href.split('/', maxsplit=1)\n\t\treturn parts[0], parts[1].rstrip('/')\n\telse:\n\t\treturn (None, None, None)\n\ndef is_valid_uri(s):\n\t\"\"\"This function returns True if the given string is a well-formed URI\"\"\"\n\treturn bool(re.search('://', s"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef is_windows():\n\t\"\"\"check whether this platform is Windows/Mac\"\"\"\n\twin_flag = False\n\tif os.name == 'nt':\n\t\ttry:\n\t\t\timport win32process\n\t\t\twin32process.TerminateProcess(win32process.GetCurrentProcess(), 0)\n\t\t\tlocal_encoding = win32api.GetSystemEncoding()\n\t\t\tbuffer_encoding = win32api.GetConsoleTextBufferLength() # 2\n\t\t\tret_list = []\n\t\t\tfor str_path in win32api.EnumDisplayTypes():\n\t\t\t\tif len(str_path) >"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef is_url(data):\n\t\"\"\"\n\tComprueba si un dict si el archivo al diccionario de la funci\u00f3n de argumentos a la url.\n\tSi una cadena (ex. /foo/bar.html) si la funci\u00f3n es None, se devuelve None.\n\t\n\tArgs:\n\t\tdata: dictionary \n\t\"\"\"\n\treturn (data == None or data == '') or data.isspace()\n\ndef get_list_data(input_list):\n\t\"\"\"\n\tGenera la lista para modificar los valores en"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef get_file_mtime(filepath): # TODO: This function should be used instead\n\t\"\"\"Take a string and return the last modification time.\n\n\tArgs:\n\t    filepath:  path to the file\n\t\n\tReturns:\n\t      (str) modification time\n\t\"\"\"\n\n\ttry:\n\t\tmytime = os.stat(filepath)[8]\n\texcept OSError:\n\t\traise TypeError(\"File has not been created.\")\n\n\treturn mytime\n\n\ndef delete_file(filepath): # TODO: This function should be used instead\n\t\"\"\"\n\tDelete a file.\n\nArgs:\n  "}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    # Check the inputs\n    if not isinstance(input_list, list):\n\t\traise TypeError(\"%s must be a list\" % (type(input_list)))\n\t\n\t# Build an ordered dictionary from each string, which is what the\n\t# string (after modification) represents.\n\toutput = {}\n\tfor string in input_list:\n\t\t\n\t\t# Clean string\n\t\twhile string and not string[:1].isdigit():\n\t\t\tstring = string[1:]\n\t\tif len(string) < 2:\n\t\t\treturn {}\n\t\t\t# End of string\n\n\t\t# Remove trailing os seperator"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\treturn\n\tif len(input_list) >= 1:\n\t\tinput_list.pop(0)\n\ndef _remove_trailing_trailing(s, pos):\n\t'''Removes trailing os seperator from file.'''\n\tif pos.endswith('('):\n\t\tpos = pos[:-1]\n\treturn s[pos:]\n\ndef get_filename(input_list):\t\t#input_list is a list of files\n\tfor string in input_list:\n\t\ttry:\n\t\t\ts = os.stat(string)\n\t\texcept OSError:\n\t\t\tcontinue\n\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    if len(input_list) is 0:\n        raise TypeError(\"the given list is empty.\")\n    if not os.path.isfile(input_list[0]):\n        raise TypeError(\"the given file \" + input_list[0] + \" does not exist!\")\n    if len(input_list[0]) < 2:\n        raise TypeError(\"the given list does not contain a separator.\")\n    return [input_list[0], input_list[1]]\n\ndef remove_end_of_str(input_list, idx, end_str):\n    \"\"\"Return string between end_str start"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\treturn None\n\ndef remove_trailing_path(input_list):\n\tif os.path.isdir(input_list[0]):\n\t\tinput_list.pop(0)\n\tfor file_name in input_list:\n\t\tif os.path.splitext(file_name)[1] not in (\".\", \".\\\\\"):\n\t\t\tfile_name = file_name[0:file_name.rfind(\"\\\\\")-1]\n\t\tfile_name = os.path.join(input_list[1], file_name)\n\t\tif os.path.exists(file"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef read_file(filename):\n\t\"\"\"\n\tRead lines of file to remove trailing os seperator characters.\n\t\"\"\"\n\n\tlines = []\n\n\twith open(filename, \"r\") as f_input:\n\t    for i in f_input:\n\t\tthis_line = i.strip()\n\t\tif this_line and not this_line.endswith(os.linesep):\n\t\t\tline = this_line[:-1]\n\t\t\tlines.append(line)\n\n\treturn lines\n\t\t\n\n\ndef remove_whitespace(lines, char='-'):\n    \"\"\"\n    Remove redundant whitespaces from"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    def list_files(path):\t\t# type: (str) -> List[str]\n    paths = []\n    for pathname in input_list:    \n        if os.path.sep not in pathname: \t# Not a pathname\n            paths.append(pathname.rpartition(os.path.sep)[0])\n    return paths\n\ndef remove_suffix(file_name):\n    \"\"\" Return a list of the files below a certain path. \"\"\"\n    file_name = os.path.abspath(file_name)\n    new_list = []\n    while(not os.path.exists(file_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef string_remove_trailing_os_delims(input_list):\n\t\"\"\"\n\tRemove trailing os seperator character(s) from the list.\n\t\n\tParameters\n\tinput_list: List with trailing os seperator character(s) from the input file.\n\t\n\tReturns\n\tProcessed string list with trailing os seperator character removed.\n\t\n\tRaise:\n\t\tTypeError\n\t\t\"\"\"\n\t\n\t\ndef string_split_files(input_list, separator_char = ':'):\n\t\"\"\"\n\tSplit a list of paths into a list of strings.\n\t\n\tParameters\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ndef rtrim(s, n):\n    \"\"\"\n    This method strips the strings (which you may want to strip)\n    from the given string.\n    \n    s: the string to be processed\n    n: the integer containing the n-th character.\n    Returns: a list containing the trimmed string\n    \"\"\"\n    return re.sub(\"(?<!\\\\)%s\\\\s*[^a-zA-Z0-9\\,\\+\\-\\(\\)\\/\\.\\-\\_\\$:\\_\\-_\\\\\\[\\]\\:\\;\\:\\?\\@\\$]\",\n                 \"\",\n                 s,\n                 flags=re.MULTILINE).split(n)  "}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ndef search_term(input_string):\n\t\"\"\"\n\tThis method converts the given string to a compiled regular expression\t\n\t\"\"\"\n\treturn re.compile(input_string,re.IGNORECASE)\n\t\n# Function to strip unwanted characters\ndef dedupe(input_string):\n\t\"\"\"\n\tThis method strips the given character from a string (with more characters)\n\t\"\"\"\n\treturn re.sub('\\W+', '', input_string)\n\n# Function to insert text into list\ndef strlist(input_list,strlistname):\n\t\n \t\"\"\"\n\tThis"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t \n\ts = str(s);\n\t#re.sub(r'(?<=\\[)\\{.*?\\}', '', s.replace('\\{','').replace('[','{'))\n\t#regex_pattern = '[\\{[\\{]'\n\t#regex_pattern = re.escape(regex_pattern);\n\t#regex_pattern = re.sub(r'(?<!\\{)[\\-_\\]\\}]', '', regex_pattern);\n\t#regex_pattern = re.sub(r'(?<!\\{[\\}]+|\\}|\\(|\\)|\\!|\\{|\\$|\\%|\\%(?!\\"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\t\n\t@staticmethod \n\tdef get_regex_pattern():\n\t\tre_pattern = re.compile(r'#define\\s+(define)')\n\t\treturn re_pattern\t\n\t\n\t\n\t\n\t\t\n\t\"\"\"\n\t\tThe class constructor which is used to construct \n\t\tan object instance.\t\n\t\tThis is done so the objects (self) can be constructed \n\t\tusing self.create_object or self(object).\t\n\t\tIn particular, it also sets properties of the object such\n\t\tas:\n\t\t\tself.name:\t\t\tthe name of the object\n\t\t\tself.description:\t\ta description"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\t\t\n\t# remove comments and empty lines\n\tcomments = (comment.split('/'))\n  \n\ttemp = []  # list for saving temporary entries and entries that will be removed when creating a new list \n\t\n\t# split a set of lines into a new list \n\tfor i, entry in enumerate(lines):  \n\t\ttemp.append(['# ', entry])  # append all text before the entry\t\t\n\t\ttemp[i][0] = entry[0].strip()      # remove the leading # character which separates the line from the actual text\n\t\ttemp[i][1] = re.split"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\treturn pattern.search(strval, strval2)\n\tdef _strval2intstr(self, strval):\n\t\t\"\"\"Given a string, try and int the string value\n\t\tin the current locale\n\t\t\"\"\"\n\t\tstrval2int = val2int(strval)\n\t\t## make sure the string is valid\n\t\t## if a match is found...\n\t\t## use the dictionary to calculate the integer value\n\t\ttry:\n\t\t\t# match the value (or default if it does not exist)\t\n\t\t\treturn locale.atof(strval, val2int)"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef convert(self,input):\n\t\t\"\"\"\n\t\tThis method converto the given string\n\t\tThis method should reformat to \n\t\tmake sure the regex pattern is correct\n\t\tThis method can be used for converting the pattern to a \n\t\tstring\n\t\"\"\"\n\t\tif type(input) is unicode: \n\t\t\tfor i in range(len(input)):\n\t\t\t\t#print str(self.encode(input[i],'UTF-8')) + \" -> \"+str(self.encode(input[i:], 'UTF-8'))\n\t\t\t\tinput[input.index(str(i))] ="}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\treg = ''\n\t\treg = reg + re.escape(name) + r''\n\t\treturn re.compile(reg,re.M)\n\n####################################################################################\n\nclass File(object):\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself.__dir = 'data/'\n\t\tself.__name = ''\n\t\tself.__size = 0\n\t\tself.__type = 'file'\n\n\tdef __str__(self):\n\t\treturn ''.join(self.__dir + '\\n' + self.__name + '\\n')\n\n\tdef setName(self"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef regex_from_patterns(self, patterns):\n\t\t\"\"\"\n\t\tThis method checks if the passed patterns are string and then\n\t\tconverts the result of this method to a regex\n\t\t\"\"\"\n\t\t#print(pattern)\n\t\tresults = []\t\n\t\tfor pattern in patterns:\n\t\t#\tprint(pattern)\n\t\t\ttry:\n\t\n\t\t\t\t#print(\"pattern: \" + unicode(pattern))\n\t\t\t\tregex_word_found = re.findall(pattern, self.input_str)\t\n\t\t\t\t#print(\"regex_word_found: \" + unicode(regex_word"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tregexp2 = re.compile(\"^[^0-9A-Za-z'\\-]*$\")\n\treturn re.sub(regexp2, strm, text)\n\t\ndef getTextParts(text):\n\t\"\"\"\n\tThis method returns a list of the matched string elements \n\tfrom the given text.\n\n\t\"\"\"\n\tresult = []\n\n\t# the initial part\n\t# pattern\n\tm = regexp2.search(text)\n\tif not m:\n\t\treturn result\n\n\t# the regex part\n\tr = m.group()    \t\t# the starting"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass TaskThread:\n\tdef __init__(self, id):\n\t\tself.id = id\n\t\tprint(\"[task][task-threading]: [%i]\" %id)\n\t\tself.out=None\n\t\n\tdef run (self):\n\t\twhile self.out==None:\n\t\t\ttry :\n\t\t\t\tcmd = subprocess.Popen(self.id + \" -w -n 1\", shell=True)\n\t\t\t\tself.out=int(cmd.communicate()[0]) \n\t\t\texcept KeyboardInterrupt:\n\t\t\t\traise\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Exception from task-%i\" %"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass PrepError (Exception):\n\tdef __init__ (self, msg):\n\t\tself.s = msg\n\tdef __str__(self):\n\t\ts = self.s\n\t\tif not s:  \t   # short out\n\t\t\treturn self.s + \" (no stdout)\"\n\t\telse:\n\t\t\treturn s\n\nclass Checksum:\t\n\tdef __init__(self):\n\t\tself._args = None\n\t\tself.__stdout = subprocess.PIPE\n\t\tself.returncode = None\n\t\tself.stdoutData = \"\"\n\t\t\n\tdef run(self):\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ndef copy_and_upload_to_s3(local):\n\t# This command allows for the execution of commands in a local directory.\n\t# It should be assumed the local directory is the root\n\tprint \">>> COMMAND ARGUMENTS...\"\n\tprint \">>> \" + local\n\tprint\n\n\t# Copy\n\tif useS3Copy:\n\t    ssh_copy_command = \"cp \" + local\n\t    check_command = \"ssh -o StrictHostKeyChecking=no \" + local + \" \" + SSH_LOG_FILE\n\t    subprocess.call(check_command, shell="}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass GetTime:\n\t\"\"\"\n\t@brief Get the elapsed time after running the command(s).\n\t\"\"\"\n\tdef __init__(self,command):\n\t\t\"\"\"\n\t\t@param command Command to execute.\n\t\t@return Time in seconds.\n\t\t\"\"\"\n\t\tself.command = command\n\t\tself.start =\ttime.time()\n\t\t\n\tdef getTime(self):\n\t\t\"\"\"\n\t\t@return The elapsed time.\n\t\t\"\"\"\n\t\treturn self.start\n\t\t\n\t\t\t\nclass LogReader:\n\t\"\"\"\n\tRead a syslog and log lines.\n\t\"\"\"\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ndef main():\n\ttry:\n\t\topts, args = getopt.getopt(\n\t\t\t\tsys.argv[1:], \"shtn:l:\",[\"help\",\"version\"])\n\texcept getopt.GetoptError:\n\t\tusage()\n\t\tsys.exit()\n\t\t\n\tif len(opts) == 0 or len(args) == 0:\n\t\tusage()\n\t\tsys.exit()\t\t\n\tversion = opts[0][0:-1]\n\toptions = {}\n\t\n\tfor opt, arg in opts:\n\t\tif opt in (\"-h\", \"--help\"):\n\t\t\tusage()\n\t\t\tsys"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp=subprocess.Popen(args,cwd=self.root_dir,args=cmd)\n\t\treturn p\n\t\t\n\tdef start(self):\n\t\tself.running=True\n\t\t\n\tdef stop(self):\n\t\t\"\"\"\n\t\tStop running all services.\n\t\t\"\"\"\n\t\tself.running=False\n\t\t\n\t\n\tdef isactive(self):\n\t\tif self.active:\n\t\t\treturn True\n\t\treturn Falsefrom unittest import skipIf\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.forms.widgets import (\n    RadioSelect,\n    ChoiceInput"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nargs_object = [sys.executable] + sys.argv[1:]\n\ntest_run = [ ]\ntest_errors = [ ]\n# test_run = []\n\nfor cmd in args_object:\n\tif cmd == \"--check\": test_run.append(TEST_CHECK)\n\telse: test_skip = [ test_skip_string + \".*\" ]\n\ttest_run.append(TEST_RUN_PATH)\n\t\n\tif cmd == \"--gen-seed\":\n\t\ttest_run.append(TEST_RANDOMIZE_SEED)\n\telse"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\n\tdef run(self=None, args=None):\n\t\t\"\"\"\n\t\tSend the pre and post action to the execution queue.\n\t\tReturn the tuple of the output and return code.\n\t\t\"\"\"\n\t\tif self.debug==1 and self.dry_run:\n\t\t\tif self.verbose==1:\n\t\t\t\tprint \"Execute:\", args \n\t\t# Create an output queue and pass its command to execution.\n\t\tself.out_queue = Queue()\n\t\tself.out = []\n\t\tif args is None:\n\t\t\targs = []\n\t\tif self.verbose>1:\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t        \n\tdef start(self):\n\t\t\"\"\"\n\t\tStart the container, waits for it to have started properly\t\"\"\"\t\n\t\tprint \"Starting container\"\n\t\tif sys.version_info[0:2] == (2, 7):\n\t\t\treturn\n\t\tsubprocess.check_call([ \"../../build/setup/docker-compose\", \"--entrypoint=/usr/local/bin/node-docker:/usr/lib64/nodejs\", \"start\" ])\t\n\t\n\tdef stop(self):\n\t\t\"\"\"\n\t\tStop the container, waits for it to have stopped properly\t\"\"\""}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ndef main():\t\n\t# Check if the path exists.\n\tif (not os.path.exists('data/data.txt') or \n\t\tnot os.path.exists('data/data2.txt') or\n\t\tnot os.path.exists('data/data3.txt') or\n\t\tnot os.path.exists('data/data4.txt')):\n      \tprint('Data folder does not exist!')\n\telse: \n\t      \t# Use the subprocess module to check the program output.\n\t        proc = subprocess.Popen([sys.executable, 'data/data.txt"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n    def IP(self,network):\n\t\"\"\"\n\t\tTest if given ip\n\t\t:param network: \n\t\t:return: \n\t\t\"\"\"\n\tdef CheckIp(self,ip):\n\t\"\"\"\n\t\tTest if IPv4 address\n\t\t:param ip:\n\t\t:return: \n\t\t\"\"\"\n\t\t\n\tdef IP_to_range(self,IP):\n\t\"\"\"\n\t\tConvert a string to integer\n\t\t\n\t\t:param IP:\n\t\t:return: \n\t\t\"\"\"\n\t\t\n   \t# Check for range:\n\t#\tprint \"Testing range string format (192.168.1.25"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t \n\tdef checkIp6addr4(self, ip):\n\t\t\"\"\"\n\t\tIP6 addr4\n\t\tCheck if IPv6 address\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttmp = str(netaddr.IPNetwork(ip)[\"cidr\"])\n\t\texcept :\n\t\t\treturn False\n\t\tif not netaddr.valid_ipv4(tmp):\n\t\t\treturn False\n\t\treturn True\n\t\n\tdef checkIp4addr4(self, ip, ip6):\n\t\t\"\"\"\n\t\tIP4 addr4\n\t\tCheck if IP address\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttmp = str(netaddr.IPNetwork"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass ip6address:\n\t\"\"\"\n\tTest if IPv6 address or not\n    \t\"\"\"\n\tdef __init__(self,ipaddr,netmask,if_type,state):\n\t   self.iftype = if_type\n\t\tself.ipaddr  = ipaddr\n\t\tself.mask   = netmask\n\t\tself.state\t= state\n\t\tself.addr_len = 24\n\t\tself.last_addr  = \"255.0.255.255\" \n\t\tself.default_l2_len = 36\n\t\tself.default_l2_addr = \"\"\t\t\n\t\tif self"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n@ddt\nclass TestIP6Address(unittest.TestCase):\n    \"\"\"\n    Test IP6 Address\n    \"\"\"\n\n\n    @ddt\n    def __init__(self, ipv4_addr):\n        \"\"\"\n        Init\n        \"\"\"\n        unittest.TestCase.__init__(self)\n        self.ipv4_addr_int_list=ipv4_addr\n        self.ipv4_addr_list=range(6,0,-1)\n\n    def tearDown(self):\n        \"\"\"\n        \"\"\"\n        pass\n\n    def testValidIp4(self):\n        \"\"\"\n        Valid IPv4 address\n        \"\"\"\n        ipv4_addr = ipaddress.ip_address"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\taddr = None\n\t\ttry:\n\t\t\tipaddr = socket.gethostbyname(host)\n\t\t\taddr = str(ipaddr[0])\n\t\texcept:\n\t\t\taddr = \"127.0.0.1\"\n\treturn  addr\n\ndef isIpAddr(addr, addrType='IPv4'):\n\t\"\"\"\n\tCheck if given IPv4 address or not\n    \t\"\"\"\n\ttry:\n\t\tipaddr = ipaddress.ip_address(addr)\n\texcept:\n\t\treturn False\n\tif addrType in addrTypes and ipaddr in addr:\n\t\treturn True\n\treturn False\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\n\ttry:\n\t\tipaddress.IPv4Address('192.168.3.1/24') \n\texcept ValueError:\n\t\tprint(\"\\n\\t\\t IPv4 address test failed\")\n\t\n\ttry:\n\t\tipaddress.IPv6Address('2001:db8::beef') \n\texcept ValueError:\n\t\tprint(\"\\n\\t\\t IPv4 address test failed\")\n\n\ttry:\n\t\tipaddress.IPv6Address('2001:db8::beef') \n\texcept ValueError:\n\t\tprint(\"\\n\\t\\t IPv4 address test failed\")\n\n\ttry:\n\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n        if self.ipv4_addresses and not self.address in self.ipv4_addresses:\n                return True\n#        return False\n\n    #\n    # ---------------------------------------------------------------------------------\n    def __str__(self):\n        \"\"\"\n         return a string representation of the IPv4 address (excluding port number) (string)\n        \"\"\"\n        return str(self.address)\n\n    #\n    # ---------------------------------------------------------------------------------\n    def __repr__(self):\n        \"\"\"\n         return a representation of the IPv4 address (excluding port number) (string)\n        \"\"\"\n#        return \"IP(IP(address=%s, port=%d)\" %"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass ipv4(ipaddress._BaseIP):\t# Use ipaddress._BaseIP\n\t \"\"\"IPv4\"\"\"\n\n\tdef _test(self, ipaddr, numbytes, addrtype=0):\n\t\t\"\"\"Test IP address with or without subnet\"\"\"\n\t\tif not netaddr: \n\t\t\tself.fail(\"No host or broadcast specified\")\n\t\t# Convert host to integer even in case it is not valid\n\t\tif addrtype:\n\t\t\treturn ipaddr.int_to_addr(ipaddr)\n\t\telse:\n\t\t\treturn ipaddr\n\n\tdef search(self, numbytes=None, exclude=None):\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass ipv4_addr_class_test(unittest.TestCase):\n\n    ipv4_addr_class_values = [0x42, '1F65:6F67:5E87:5D18',\n                            ipaddress.IPv4Address('0:0:0:0:1'\n                        ), 4294967295,\n                            ipaddress.IPv4Address('1::2:3::4'),\n                            ipaddress.IPv4Address('0:0:0:0:255')]\n\n    def test_ipv4_addr_class_1(self):\n\t\"\"\"\n\tIPv4_addr_class()"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ndef main():\n\t\n\t\"\"\"docstring for test_ipv4_addr\"\"\"\n\t\n\t\nif __name__ == \"__main__\":\n\t\tmain()\n# -*- coding: utf-8 -*-\n\"\"\"QGIS Unit tests for QgsServer.\n\n.. note:: This program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\"\"\"\n__author__ = 'Nyall Dawson'\n__date__ = '18/"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\tr = []\n\t\t\n\tfor subgraph_itr in subgraphs:\n\t\tif not ((subgraph_itr.get(\"rdflib:subClassOf\") in rdflib.RDFS.subClassOf) and\n\t\t\t(subgraph_itr.get(\"rdflib:broader\") in rdflib.RDFS.broader)):\n\t\t\tcontinue\n\t\t\n\t\tfor child_itr in triples:\n\t\t\tr.append((child_itr[\"obj\"], rdflib.RDF.type, rdflib.URIRef(child_itr[\"value\"])))\n\t\n\t\n\treturn r\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tif rdflib.RDF.type in [ rdflib.RDFS._BNode, rdflib.RDFS._DataObj]:\n\t\t\troot_name = rdflib.RDFS.subClassOf\n\t\telse:\n\t\t\troot_name = rdflib.RDFS._data[rdflib.RDFS.subClassOf][rdflib.RDFS._node.data]\n\t\tfor child in graph.objects(root_name):\n\t\t\tif child[rdflib.RDFS.subClassOf] in (rdflib.RDFS._BNode, rdflib.RDFS._DataObj"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\tdef find_roots (graph, uri):\n\t\tglobal g\n\t\ttry:\n\t\t\tall_roots = set(graph.subject(uri, RDFS.subClassOf))\n\t\t\tfor uri in sorted(all_roots):\n\t\t\t\tall_roots = g.all(self.find_matches(uri)).union(all_roots)\n\t\texcept rdflib.exceptions.NamespaceNotFound:\n\t\t\tg.add(uri)\n\n\t\t# TODO: need list of all RDFS.subClassOf\n\t\t# TODO: how do we get non-RDFS triples?\n\t\t"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, rdflib.RDFS.subClassOf, prop):\n\t\"\"\"Find all roots in rdflib\n\n\tGiven an rdflib URI with an rdflib query and a triple with\n\tproperty subClassOf and property predicate for each root, it will generate a\n\tset of all roots of the sub-class and all those that root is in\n\trdflib.RDFS.subClassOf.\n\t\n\tIf the root matches an rdflib URI and all of its subClassOf or\n\tprop predicates is empty, no roots will be added to the set.\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t#print(tripleOfRDF(tripleOfSKOS(subClassOf(skos.broader))))\n\t\treturn set(p for p in tripleOfSKOS(self, subClassOf(skos.broader)))\n\ndef find_root(graph, nss, parent):\n\t\t\"\"\"find_root(graph, nss, parent)\n\t\tSearches for the root node matching the given triple (parent), returns None as not found\n\t\t\"\"\"\n\t\t#print(\"searching node\",nss,[n.value for n in nss.predicates(nss[-1"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\nclass RecursiveRDFS:\n\t\"\"\"\n\tRecursive version of the RDFS.\n\tWill iterate through the RDFS to see if any of the paths\n\tare not already visited. If so, its a set of all non-visited paths \n\t\"\"\"\n\t\n\tdef __init__(self, root_class, paths_to_be_visited=set(), recurse=False, \n\t\t                   max_depth=2):\n\t\t\n\t\t\"\"\"\n\t\tIt will try all the paths in root_class/paths_to_be_visited\n\t\t\tto see if any of the entries already be visited or"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\troots = [rdf_from_resource(i) for i in find_roots(graph, rdflib.RDFS.subClassOf)]\n\t\t\n\tif roots[0]:\n\t\treturn roots\n\telse:\n\t\treturn []\n\ndef get_parents(graph, graph_with_parent, path=[]):\n\t\"\"\"returns a list of the parents of the given path (list of the parents of a given node)\n\t\n\tThis is similar to :meth:`find_roots()` except that it returns a list where each\nelement will only have 1 parent and 0 its parent. For example...\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tdef find_roots_from_prop(prop):\n\t\t\t# TODO: should be rewritten in a more general fashion?\n\t\t\tif isinstance(prop, rdflib.URIRef):\n\t\t\t\treturn set([ (prop, None, None, None) ])\n\t\t\treturn set()\n\t\tpath = find_roots_from_prop(prop)\n\n\t\tlist = list(path)\n\t\troots = set([ path[0] ])\n\n\t\t# make sure the list is sorted by descending order\n\t\tlist.sort(reverse=True)\n\t\treturn roots\n\n\tresult = set()\n\n\tfor prop"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\tp = list(graph.triples((None, None, None, None)))\t\n\ta = list(graph.triples((None, p[0][0], p[1][0])))\n\tb = list(graph.triples((None, None, None, None)))\n\tb1 = list(graph.triples((p[0][0], p[1][0], p[0][1])))\n\tc = list(graph.triples((None, None, p[1][1], p[1][2])))\n\tc1 = list("}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\treturn set([o.subClassOf for o in graph.objects(subject, NS.RDFS.subClassOf, SKOS.broader)])\n\n\ndef main():\n\tq = rdflib.Graph() # initialize graph\n\tq.bind(\"http://example.org/\")    # binds to root node\n\tq.bind(\"www.example.org/ns\")   # attach as rdflib namespace for rdflib\n\tQ=Triple(\"rdfs:subClassOf\")\n\tB=Triple(\"http://example.org/foo:rdflib:object\")\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\nif (__name__ == \"__main__\"):\n\n\tpass\n\n# Copyright 2012 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\treturn yaml.dump(obj, encoding = 'UTF-8')\n\ndef string_replace(sold, old, new):\n\t\tif type(sold)==string_types or old is None:\n\t\t\treturn sold\n\t\tif isinstance(sold, string_types):\n\t\t\treturn sold.replace(old,new)\n\t\t\n\t\telse:\n\t\t\treturn string_replace( sold, str(new ), str(sold))\t\n\t\n\t\n\t\n\t\nyaml.SafeLoader.add_constructor(u'!unicode', u''.encode)\nyaml.SafeLoader.add_constructor"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry :\n\t\t\tfp = open(fn, \"w\") \n\t\t\tfp.write(yaml.dump(self.yaml(),default_flow_style=False))\n\t\t\tfp.close()\n\t\texcept (IOError, yaml.YAMLError) as err : \n\t\t\tprint \"Could not dump to %s: %s\" %(fn, err)\n# -*- coding: utf-8 -*-\n\n\"\"\"\n***************************************************************************\n    v_net_load.py\n    -----------\n    Date                 : August 2012\n    Copyright            : (C) 2012 by Victor Olaya\n    Email                : volay"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry:\n\t\t\timport yaml\t\n\t\texcept ImportError:\n\t\t\traise ImportError('You need either pyyaml (version >=2.8) or py3-yaml (version <=3.2)')\n\t\t\n\t\tif not hasattr(yaml, 'RoundTripLoader'):\n\t\t\traise ImportError('You need either python-2.6 or python-3-python')\n\t\t\t\n\t\timport yaml\t\n\t\tdump = yaml.dump\n\t\tdump = yaml.dump\n\n\t\tdef ordered_dump(dump, stream=None, indent=None, allow_unicode=False, default_flow_style=False):\n\n\t\t\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\nimport yaml, sys\n\ndef pretty(data):\n\t\"\"\"\n\tPretty print the given Data object or dictionary\n\t@param data: \n\t@type data: py2-unicode or py3-string\n\t@returns: pretty-printed string\n\t\"\"\"\n\tif isinstance(data, dict):\n\t\treturn \"dict:\" + dict2py3(data)\n\telif isinstance(data, list):\n\t\treturn repr(data[0]) + \":\" + dict2py3(data[1:])\n\telse:\n\t\treturn pretty_printer(data)\n\t\t\t\ndef pretty"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\twith open(filename, 'w') as f:\n\t\t\tyaml.dump(config, f, default_flow_style=False)\n\n# #\tload data\ndef load(filename):\n\t\"\"\"\n\tLoad the data from a yaml filename.\n\n\t:param filename: The yaml filename for the file to load.\n\t:rtype: list | dict\n\t:return: The loaded data.\n\t\"\"\"\n\twith open(filename, 'r') as f:\n\t\treturn yaml.load(f)\n\n# #\tLoad\ndef save(filename, data):\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\timport cStringIO\n\t\timport yaml.reader\n\t\timport os\n\t\t\n\t\tif self.indent:\n\t\t\tindent = self.indent\n\t\telse:\n\t\t\tindent = None\n\n\t\tif self.sort_keys:\n\t\t\tprint('sorting: keys')\n\t\t\tkeys = self.sort_keys\n\t\telse:\n\t\t\tkeys = self.keys\n\t\tkeys.sort()\n\t\tfor key in keys:\n\t\t\tvalue = self.data.get(key, '')\n\t\t\tprint('%r' % (key), end=' ', file=value, end_with_newline=False)\n\t\tfor ("}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\t\t\t\n\tdef dump(self):\n\t\tpass\n\t\n\t\t\n\tdef create(self):\n\t\t\n\t\t\"\"\" create_network \"\"\"\n\t\ttry:\n\t\t\tpass\n\t\t\t#from houdini import houdini\n\t\t\t#hdir = houdini(\"file.txt\")\n\t\t\t#file = open(\"file.txt\", \"w\")\n\t\t\t\n\t\t\t#print >>file, hdir\n\t\t\t#print >>file, \"type=%s\" % self.type\n\t\t\t#if not self.type in ['string','int','float','bool']:\n\t\t\t\t#file.write(\"#%s\""}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\t_yaml=yaml.dump(self, stream, Dumper=MyDumper, default_flow_style=False)\n\t\t_yaml=yaml.load(_yaml)\n\t\treturn yaml_to_py2unicode(_yaml)\n\t\n\tdef save(self, filename, as_unicode=True, indent=2, sort_keys=True, check_circular=True, cls=None):\n\t\t\n\t\t\"\"\"Save this configuration to a file.\n\t\t\t\n\t\tIf as_unicode=True:\n\t\t\tSaves current configuration as a text file.\n\t\t\tIf as_unicode=False\t\t(default"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\n\tdef __init__(self, **kwargs):\n\t\t# Call the parent constructor\n\t\tBaseNode.__init__(self, **kwargs)\n\n\t\t\n\t\t# Make sure the file exists\n\t\tif not os.path.isfile(settings.config.get('py2ini','file')):\n\t\t\traise IOError(settings.config.get('py2ini','file'))\n\n\t\t# Copy to temp folder if it doesnt exist\n\t\tself.dest = settings.config.get('py2ini','folder')\n\t\t\n\t\t# Create list of files to ignore when processing\n\t\tignoreFiles = []\n\t\tfor"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nlogger = logging.getLogger(name=__name__)\ndef getFileLog(fileName,logName=\"error.log\"):\n\t\"\"\"\n\tGets a FileLog instance from a file and returns it as a Logger obj\n\t\"\"\"\t\n\timport logging\n\t#FileLog.__init__()\n\t#Logger = logging.getLogger(\"log_file\")\n\t#Logger.critical(\"The 'FileLog' object has been deprecated. You should use \\\"FileLog(\\\" + logName + \"\\\"). If you are sure that you want to use the new Logger class, use \\\"FileLog(filename,level"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\n\tdef __init__(self, name = None, verbose = 0, path = \"\", level = logging.DEBUG):\n\t\tself.logger = logging.getLogger(name)\n\t\tself.verbose = verbose\n\t\n\t\tif path[-1] == \"/\":\n\t\t\tself.path = path\n\t\telif path[-1] == \"\\\\\":\n\t\t\tself.path = path[:-1]\n\t\telse:\n\t\t\tself.path = path + \"/\"\n\t\t\n\t\tself.level = int(level)\n\t\tif self.level > 1:\n\t\t\tself.logger.setLevel(self.level)\n\t\telif self"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef get_logger(self):\n\t\tself.__logging__\t\t= logging.getLogger(__name__)\n\t\treturn self.__logging__\n\t\t\n\tdef get_logger_for_loglevel(self,level):\n\t\tfor logger in self.logger:\n\t\t\tif not logger.level >= level:\n\t\t\t\treturn logger\n\t\n\n\tdef get_output_location(self):\n\t\treturn self.__output_location\n\t\t\t\t\t\n\tdef get_output_location_for_current_experiment(self):\n\t\treturn self.__output_location_for_experiment\n\n\tdef get_output_location"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef logInfo(self, format, *args):\n\t\t\"\"\"\n\t\tLog basic log information. This will also log the info to the logging module \n\t\t\"\"\"\n\t\tlogging.info(\"%s - - [%s] %s\" % (self.name,format % args))\n\n\tdef fatalError(self, failure):\n\t\t\"\"\"\n\t\tRaise an exception of the last error captured\n\t\t\"\"\"\n\t\tif failure is None:\n\t\t\tpass\n        else:\n\t\t\traise failure\n\n\n#=======================================================================================\n# File System Management\n#=======================================================================================\nclass File(Base):\n\t\"\"\"\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef log(self,message):\n\t\ttry:\n\t\t\tself.logger.info(message)\n\t\t\tfile_name, file_extension = os.path.splitext(message)\n\t\t\tf = open(self.log_filepath,'a')\n\t\t\tf.write(time.strftime('%Y-%m-%d %H:%M:%S') + \": \" +message)\n\t\t\tf.close()\n\t\texcept:\n\t\t\tlogging.error(\"Could not write log to \" + self.log_filepath)\n\t\t\t\n\tdef logger2str(self):\n\t\treturn \"\\n\\"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tlogger = logging.getLogger(__name__)\n\tlogger.setLevel(logging.DEBUG)\n\tfh = logging.FileHandler(\"logfile.txt\",\"w\",encoding='utf8')\n\tfh.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\t# create logger\n\tlogger_log = logging.getLogger(__name__ +\".log\")\n\tlogger_log.setLevel(logging.DEBUG)\n\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t     \treturn 0\t# Copyright 2012 Nebula, Inc.\n# Copyright 2013 IBM Corp.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#   "}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nLOG_FILENAME = 'vcp_log.txt'\nERROR_LOG_FILENAME = 'vcp_error.txt'\n\ndef print_error_to_file(error,*args):\n    \"\"\" Print the exception to the console. \"\"\"\n    print \"An exception has been raised with the following info:\"\n    print \"---------------------------------------------------------------------------------------------------------------------\"\n    traceback.print_exception(*args)\n    print \"---------------------------------------------------------------------------------------------------------------------\"  \n    quit()\n\ndef log_error(text):\n    \"\"\" Show error into the file. \"\"\"\n    sys.stderr.write(text+\"\\n\")\n    \n# Define a main function"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nlogger = logging.getLogger(\"TweetLogger\")\nlogger.setLevel(logging.DEBUG)\n\nfrom lda.nlp import NLPCorpusReader,NLPTaggerCorpusReader,NLPLemmatizer,NLPLTaggerCorpusReader,TaggerReader,NLPICorpusReader,NLPPoemCorpusReader,NLPICorpusReader\n\nclass TweetLogger(object):\n\t\"\"\"Class for logging individual data. It logs its arguments through a logger object\"\"\"\n\tdef __init__(self, path, mode, min_freq, max_freq, mode_t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef __init__(self,name=\"systemlogger\"):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself.name   = name\n\t\tself.log    = logging.getLogger(name)\t\t\n\n\tdef write(self,text=\"Error\"):\n\t\t\"\"\"\n\t\tSimple function that allows printing to the screen \n\t\t \n\t\tIt will print to screen \n\t\t\"\"\"\n\t\tself.log.info(text)\n\n\tdef writeError(self,text=\"Error\"):\n\t\t\"\"\"\n\t\tSimple function that allows writing to the screen \n\t\t \n\t\tIt will print to screen \n\t\t\"\"\"\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    return np.array([0],dtype=dtype)\nimport copy\n\nfrom. import _base, ffi\nfrom.error import PyAsn1Error\nfrom.type import typeof, newstr\n\n\nclass error(Exception):\n\n    def __str__(self):\n        return self.message\n\n    def __repr__(self):\n        fmt = [newstr(self.message)]\n        for name, value in vars(self).items():\n            if name.startswith('__') and typeof(value) is tuple:\n                fmt.append('%s=%r' % name)\n        return '\\n\\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\nclass Multiply():\n\t\n\tdef __init__(self, a, b):\n\t\tself.a = a\n\t\tself.b = b\n\t\n\tdef __add__(self, arr):\n\t\tself.a += arr\n\t\tself.b += arr\n\n\tdef __radd__(self, arr):\n\t\tself.a += arr\n\t\tself.b += arr\n\n\tdef __sub__(self, arr):\n\t\tself.a -= arr\n\t\tself.b -= arr\n\t\n\tdef __rsub__(self, arr):\n\t\tself.a -= arr\n\t\tself.b -="}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n_TOLERANCE = 10**(-9)\ndef makeArray(basearr, dtype=np.float):\n    \"\"\"\n    Returns an array with a specified base array of dimensions and contents\n    filled with zero.\n\n    >>> makeArray([1,2,3]).shape\n    (3,)\n\n    Returns\n    -------\n    A new array with the specified base array of dimensions and contents\n    filled with zeros. The length of the new array is a variable.\n    \"\"\"\n    try:\n        basearr.shape\n        return basearr\n    except AttributeError:\n        pass\n    newarr = np.zeros(basearr.shape[0],"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef writemat_array(filename, arr, dtype=np.dtype('>f4')):\n\t\"\"\"\n\tWrites ndarray `arr` to file.\n\n\tParameters\n\t----------\n\tfilename : str\n\t\tName of input file\n\tarr : ndarray\n\t\tndarray with shape and dtype\n\tdtype : `numpy.dtype`\n\t\tdata-type of the array\n\t\"\"\"\n\tnp.savez(filename, arr=arr, mdict={'arr': arr, 'arr': arr.dtype.descr + (dtype)})\n\t\ndef create_array(name, arr, shape=None"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tpass\n\ndef reshape(x,shape):\n\t\"\"\" reshapes an array to a new shape.\n\nParameters\n----------\nshape : tuple\n    shape of the array to be reshaped to, i.e. (2,2)\n\"\"\"\n\tpass\n\t\t\ndef add_sub_tensor(x,y,shape):\n\t\"\"\"\n\tFunction to add a sub-tensor for an array.\n\nParameters\n----------\nx : numeric type\n    shape of the array to be added to the sum of the elements \ny : type of the numeric type to add to the array\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef createZeros(shape,dtype):\n\t\"\"\"\n\tCreate 2D arrays from the given shape and data type.\n\nParameters\n----------\nshape : ((d2c,n),(y,n)) array\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tl = []\n\tfor i in range(shape[0]):\n\t\tl.append(np.arange(i).reshape(shape[0],1))\n\treturn np.reshape(np.array(l),[shape[0],np"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n#--------------------------------------------------------------------------------\ndef imsave(arr, filename=None, format = \"JPG\"):\n\t\"\"\"\n\tFunction which saves an array to the clipboard.\n\t\n\tParameters\n\t----------\n\tArr : `np.ndarray`\n\tfilename : string or `pyu.file`\n\t\tThe filename of the file to save the array\n\tformat : {\"JPG\", \"BMP\", \"PNG2\", \"*\"})\n\t\tFormat of the output array(s)\n\t\t\n\tExample\n\t-------\n\t>>> from pyu.io import StringIO\n\t>>> o = StringIO()\n\t>>>"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\tpass\n\n\ndef rand(size):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nsize : tuple\n    size (element-wise) from where to return\n\"\"\"\n\treturn np.random.random(size)\n\t\ndef randn(size):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nsize : tuple\n    size (element-wise) from where to return\n\"\"\"\n\treturn np.random.random(size)\n\t\ndef randc(size):\n\t\"\"\"\n\tFunction to"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    M.ndarray.__init__(self, dtype=dtype, shape=shape)\n\t\n\t# return a pointer to the data\n\tdef _read_single(self):\n\t\tm = self._shape[0]\n\t\tn = self._shape[1]\n\t\tnDtype = np.dtype(float)\n\t\treturn np.ndarray.array( (m,n,nDtype), self.dtype, align=m, \n\t)\n\t\n\t# return an array of the requested dimensions\n\tdef _read_array(self):\n\t\tsize, m,n,nDtype"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t    \ndef array_write(self, array, shape=None, dtype=None):\n\t\t\"\"\"Function to write an array with shape and dtype to the file.\"\"\"\n\t\tif shape is None:\n\t\t\tshape = array.shape\n\t\tif dtype is None:\n\t\t\tdtype = self.dtype\n\n\t\t# save the data to file\n\t\t\n\t\tfile = open(self.filename, \"%d\" % shape)\n\n\t\t# write the data - first line: the array header\n\t\tfile.write(\"#%d\\n\" % self.line_num)\n\t\tfile.write(\"%d\\t%"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\t\"\"\"\n\tDefine the number of data points and the corresponding Gaussian.\n\t\"\"\"\n\tM=50\t\t#Number of data\n\tN=10000\t\t#Points per point\n\tL=5\t\t#Length of each data point\n\tTol=1\t\t#Tolerance of the test points in the GP\n\t\n\t\"\"\" Initialize the gaussian model using gaussian_noise_like(y,mu,sigma) \"\"\"\n\tmuhat=np.zeros(M); muT=np.zeros(M); sigmaT=np.zeros(M);\n\ty=np."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef func(a, b):\n\t\treturn np.exp(-0.5 * (b/a) + np.log(b/a))\n\n\tobj = blaze.array([-1, -1, -1], x0_bounds=(-2,2), lb=[-2, -2], ub=[-2, 2])\n\n\tdef grad(expr):\n\t\treturn (expr[0] - 2) * expr[1]\n\n\tobj_grad = obj(0., 0.)\n\tobj_grad.plot_f(func)\n\tblaze"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\nclass gaussian_gaussian_cdf(object):\n\t\"\"\"\n\tGaussian, not unshifted\n\t\"\"\"\n\tdef __init__(self, mean=None, sigma=None, mean_stddev=100., sigma_stddev=50., mode = 'normal'): \n\t\t\n\t\tself.mean = mean\n\t\tself.sigma = sigma\n\t\tself.mean_stddev = mean_stddev\n\t\tself.sigma_stddev = sigma_stddev\t\t\n\t\tself.mode = mode\n\t\t\n\tdef pdf(self, x, mean, sigma):\n\t\t\"\"\"\n\t\tFunction to be integrated with the pdf"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\nfrom math import sqrt\ndef CircularGaussian(l, sigma, x0=1., eps=1.e-6):\n\treturn np.exp(-((x0-l)/2.)*sigma**2.\n\n'''\ndef LLc2(ll, ctl, alpha=(0.03,0.1,0.015)):\n\treturn ((ll+tctl)/2.)*(alpha/2.)+(-(ll-cttl)/2.*alpha*alpha)\n'''\ndef LGlp(ll, ctl, alpha=(0.03,0."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\ndef gaussian_kernel(data, mean=lambda x: x,sigma=1):\t\n\n\t\"\"\"\n\tGaussian centered around 0.1 with sigma * 0.1\n\t\"\"\"\n\n\n\t\n\treturn np.exp(-(data-mean(data))**2/(2*sigma**2))\n\n\n\"\"\"\n\n\"\"\"\nData Generation Function\n\"\"\"\n\ndef generate_data(N, M):\n\t\n\t\"\"\"\n\tGenerate a random dataset of dimension M, given a desired number of data points\n\tN (int): number of data points to generate\n\tM (int-array,"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\t#def gaussian_pdf( x, mu, sigma_x, c, s, z, theta) :\n\t#\treturn [ 0.2/(1+np.exp(-(x-mu)**2 / 2) + 1/sigma_x + theta)  for x in x ]\t\n\n\nclass TestGaussPoisson1D(unittest.TestCase) :\n\tdef setUp(self) :\n\t\tpass\n\tdef runTest(self) :\n\t\tclass TestGaussPoisson1D(unittest.TestCase) :\n\t\t\tdef test_gaussian_pdf( self)"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\n\tdef gen_h_sample (self, x) :\n\t\treturn ( x - self.mu )**2 \n\t\n# -------------------------------------------------------------------\n# MCMC analysis from \n# \n# y-axis = g.h\n# y = x0\n#\n# The fitness values of a Gaussian distribution can be expressed as\n#   C^{(k)} = k *  ( sqrt(-k*||sigma)/C^{(k)} )\n#\ndef fit_gmm (x0) :\n    x = np.arange ( x0 )\n    return y - ( g."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\n\tdef __init__(self, width1, width0, width2, N):\n\t\t\t\"\"\"\n\t\t\tInitialize the Gaussian instance\n\t\t\n\t\tInputs:\n\t\t\twidth1:\t\twidth of Gaussian 1D\n\t\t\t\tAmplitude and width of Gaussian 2D\n\t\t\t\tAmplitude and width of Gaussian 3D\n\t\t\t\tAmplitude and width of Gaussian 4D\n\t\t\t\n\t\t\twidth0:\t\t\twidth of Gaussian 1D \n\t\t\t\tAmps\n\t\t\twidth2:\t\t\twidth of Gaussian 2D\n\t\t\t\tAmps\n\t\t\tN:\t\t\t\tnumber of gaussian 1D components\n\n\t\tOutputs:\n\t\t\tself"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\n\tdef kernel(self, x, mean, cov):\n\t\treturn 0.2 * (x - mean)**2.\t\n\t\n\t\n\tdef test_kde_kde(self):\n\t\t\"\"\"\n\t\tkde(array)\t\n\t\tTests KDE with a gaussian kernel of mean =0\n\t\t\"\"\"\n\t\tkde = gaussian_kde(self.kern)\n\t\tkde.fit(x)\n\t\tassert_almost_equal(kde.kernel_, np.ones(x.shape[0])/float(x.shape[0]), decimal=15)\n\t\tself.assertRaises(NotImplementedError,"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t    \n\tdef __init__(self, length=10, sigma=0.1):\n\t\tsuper(Gaussian, self).__init__(length)\n\t\tself.sigma = np.sqrt(sigma*np.random.rand(number_of_parameters)*sigma)\n\t\t\n\tdef __call__(self, x):\n\t\tr = 0.5*self.sigma*np.exp(-((np.pi/4-self.sigma)**2)*(x-x)**2)/self.length**5\n\t\treturn r\n\t\n\tdef likelihood(self, x, theta):\n\t\t#print(\"\\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\tdef __init__(self, configDir, outputDir, parser, path=None, maxLog=3):\n\t\t\"\"\"\n\t\t@param configDir: Path to configuration files, or the directory containing the configuration files\n\t\t@type configDir: string\n\t\t@param outputDir: Path to output directory where the JSON error will be stored\n\t\t@type outputDir: string\n\t\t@param parser: Name of the parser module to use\n\t\t@type parser: string\n\t\t@param path: Path to the configuration file\n\t\t@type path: string or None\n\t\t@param maxLog:"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t\n\n\tdef validateConfigs(self,configs):\n\t\t\"\"\"Takes all (if any) config sequences (including non-configurable) and validates them before\n\t\tacting on them when passed a list of configs \"\"\"\n\t\tfor (base_path,config_filename) in configs:\n\t\t\tfile_name = os.path.basename(base_path)\n\t\t\t\n\t\t\t# If an explicitly provided config filename is missing for parse errors\n\t\t\tif not os.path.isfile(config_filename):\n\t\t\t\traise Exception(\"Failed to find config file: {0}\".format(config_filename))\n\n\t\t\t#"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\t# Load configuration file into memory.\n\t\tlogging.info(config)\n\t\tlogging.basicConfig(level = logging.DEBUG, format = config.get(\"logging\", \"format\"), filename = config.get(\"logging\", \"filename\"))\n\n\t\t# Validate configuration.\t\n\t\tvalidated = True\n\t\tresult = self._validate_config(config.get(\"validation\", \"errors\"))\n\t\t\n\t\t# Extract and parse errors.\n\t\terror_count = 0\n\t\tmax_errors = -1\n\n\t\tif len(result) > 0:\n\t\t\tlogging.error(result)\n\t\t\tlogging.error(\""}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\tdef __init__(self, config_filenames):\n\t\t\n\t\tself.config_paths = {}\n\t\tself.is_parsed = False\n\t\tself.config_files = {}\n\t\t\n\t\t#Parse all config files\n\t\t#This code assumes that there is exactly one directory named config file(s),\n\t\t#and this variable has no corresponding \".yaml\" extension. \n\t\t\n\t\tfor config_filename in config_filenames:\n\t\t\tconf_file = config_filename.split('.')[1]\n\t\t\t#Get the full path to the config file\n\t\t\tconfig_file = os.path.join(os"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\nclass Config(object):\n\t\"\"\"Object representing the configuration parsed by a config file. We assume that the first line is a space separated list\n\tof values separated by = or ';' character.\n\t\"\"\"\n\tdef __init__(self, name, lines = (), sep = '\\n'):\n\t\tself.name = name\n\t\tself.lines = lines\n\t\tself.sep = sep\n\t\t\n\t@property\n\tdef lines(self):\n\t\tr = list(self.lines)\n\t\tr.extend(list(line.rstrip() for line in self.lines.split(self.sep"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n    # Initialize configuration\n\tconfigMap = {}\n\tfor configFileName in configFiles:\n\t\tprint(\"Reading configuration file \" + configFileName + \"...\")\n\t\twith open(configFileName, encoding=\"utf-8\") as fStream:\n\t\t\tinputConfig = fStream.read()\n\t\t\tparser = TskEventParser(inputConfig, \"config\", \".*\", True)\n\t\t\t#logger.debug(\"Adding new parse error handler\")\n\t\t\tp = TskEventParser(list(parser.warnings.keys()), \"config\", \".*\", True, {}) \n\t\t\tparser.parseConfigurationFiles(config"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tret = self.get_parse_errors(self.parse_errors, self.get_parse_warnings, log_parser, debug=debug, filename=filename, parse_log=parse_log)\n\t\treturn ret or ()\n\tdef load_configs(self, filenames, parse_errors, parse_warnings, debug=False):\t\n\t\t\"\"\"\n\t\tGiven a sequence of configuration filenames, load and validate each configuration file.\n\n\t\tReturns: a tuple of:\n\t\t[Configuration instance], dict of parsed configuration\n\t\t@throws: ConfigParser.NoSectionError\n\t\t\"\"\"\t\t\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t_logger=logging.getLogger(self.__class__.__name__)\t\t\n\ndef parseArgs(arguments):\n\t\"\"\"\n\tParse the arguments passed to it, where each parameter's name is\n\ta tuple of argument values.\n\tThis is a helper function intended to simplify code since it is much\n\tdifferent if the script reads the configuration files into a bunch of different\n\tkinds of arguments...\n\t\n\tThe following is an internal method called by parseArgs, but returns an\n\t\n\t(configuration_filename, configuration_parser, exception_class) tuple; the exceptions\n\tof the parsing are"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\traise NotImplementedError\n\tdef log_and_save(self, filename, parse_errors, *args, **kwargs):\n\t\t\"\"\"\n\t\tSave a parse error to a configuration file. This method will save the parser (ie.\n\t\tif no errors are raised by parse_errors), and restore the parsing errors.\n\t\t@param filename: the configuration filename\n\t\t@param parse_errors: True, if errors were encountered, False otherwise\n\t\t@param *args: additional parameters to be passed to the parser\n\t\t@param **kwargs: additional keyword arguments\n\t\t@return: the log record object ("}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\tsuper(ConfigurationLoader,self).__init__()\n\n\t\t# define default settings\n\t\tself.settings = dict(\n\t\t\tdict(\n\t\t\t\tshort_name='general',\n\t\t\t\tdescription='Use this plugin to manage the OpenStudio Project, OpenStudio or OpenShiva in a variety of different ways.',\n\t\t\t\ttype='list',\n\t\t\t)\n\n\t\t# define command line options\n\t\tself.parser.add_option('--log',\n\t\t\tdest='log', default=True,\n\t\t\taction='store_true',\n\t\t\thelp='Dump all configuration files to"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n#    print \"string: \" + obj\n\tbyte_str= \"\"\n\tobj=obj.encode(\"utf-8\").strip(chr(0)).encode(encoding)\n\t#\tprint \"string: \" + obj \n\tbyte_str += \"     \"\n\n\n\t# print \"string: \" + byte_str\n\t# byte_str_len\n\t# byte_str_len = len(str(byte_str))\n\tbyte_str_len = byte_str_len * 5\n\t\n\t# print \"1\" * (byte_str_len*5)"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\n\tf = open(obj, \"rb\");\n\tbytes = f.read();\n\tf.close();\n\n\tif len(bytes)!= 4:\n\t\tsize = 4;\n\telse:\n\t\tsize = 4;\n\t\n\n\tf = open(obj, \"rb\");\n\tcontent = f.read();\n\tf.close();\n\n\treturn content;\n\ndef readString(obj):\n\t\"\"\"\n\tread the data from the string using UTF-8.\t\n\t\n\tParameters\n\t----------\n\tobj : file\n\t\tThe file object\n\tReturns\n\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t\n\t## Return bytes object that contains file data.\n\tfile1=open(str(src),'rb')\n\tbuf=file1.read(1024)\n\twhile len(buf)!=0:\n\t\tfile1.readline() # Skip null character\n\t\tif not len(buf):\n\t\t\tbreak\n\t\tprint buf\n\t\tbuf=file1.read(1024)\n\twhile len(buf)!=0:\n\t\tfile1.readline() # Skip null character\n\t\tbuf=file1.read(1024)\n\t\n\t## Return the bytes object corresponding to the data found in"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\n\treturn obj.decode(\"utf-8\")\n\n\n\n\n\n\ndef get_from(file_name, type):\n\t\"\"\"A generator function to get data from the given type.\n\t\"\"\"\n\twith open (file_name, 'rb') as file:\n\t\tvalue = bytearray(file.read())\n\treturn value\n\n\n\n\n\n\nclass AVATools:  # Class with helper functionality to find files\n\t\"\"\"A Class containing all the classes of the VATools class.\"\"\"\n\tdef __init__(self):\n\t\tself.init_vampy()\n\t\tself.read_"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef uint8(obj):\n\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8\n\t\"\"\"\n\tif type(obj)==bytes:\n\t\treturn obj.decode(\"utf-8\")\n\telse:\n\t\treturn obj\n\n# End functions\n\n\ndef _isstr(obj):\n\t\"\"\"\n\tThis function performs the operation of the string function ``obj``\n\treturns ``True`` for empty strings.\n\t\"\"\"\n\tif type(obj)==str:\n\t\tif obj:\n\t\t\tif not obj"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n#\tdef read_int(f):\n#\t\tr\"\"\"\n\tReturns the next integer from ``f``, and the remainder of bytes\n\tin ``fp`` (which may be ``str``).\n\tThe bytes object must have the following attributes:\n\n\t  * ``length`` (``int``) - The number of bytes to read\n\t\t* ``offset`` (``int``) - Position inside the file where the bytes\n\t\t\t  starts, from where the start and the end characters\n\t\t     read.\n\n\tThe bytes objects pointed to by the string object ``f`` may be a byte\n\tstring of an"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef u(obj=-1):\n    \"\"\"\n    Return the Unicode string representation of an object.\n\n    For example:\n\n    >>> pystring = u'Hello'\n    >>> int(pystring[-3:])\n    u'Hello'\n    \"\"\"\n    if isinstance(obj, type(None)):\n        if sys.version_info >= (3, 0):\n            obj = str(obj)\n        else:\n            obj = unicode(obj)\n    return obj\n# Copyright (C) 2002-2006 Python Software Foundation\r\n# Author: barry@zope.com (Barry Warsaw)\r\n#"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t\n\tresult = \"\"\n\twhile obj:\n\t\tresult += obj\n\t\tobj = obj[1:]\n\treturn result\n\t\n\ndef bytessplit(a, n):\n\treturn list(map(a.rsplit(' '), n))\n\t\n\"\"\"\n\tthis function returns an array containing a list of 8 bit floats\n\tthat represent each byte in the given sample.\n\tIf more than one byte is available, output the same array.\n\"\"\"\n\nclass SamplesGenerator(object):\n\t\"\"\"Samples Generator\"\"\"\n\t\"\"\"\n\t#def __init__(self, length):\n\t#"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n    # get the number of bytes needed\n    nbBytes = ( (len(obj)) + 7 ) / 8\n    # return the bytes length\n    return nbBytes\n\n# this function reads the length of an object, returning the\n# number of seconds since the Unix Epoch.\n#\n# Note that the UNIX Epoch contains an integer, so need to\n# convert to string before calling the function.\n#\n# The UNIX Epoch may be a string of the form N:H, where N is a timulus\n# interval (integer between 0 and 23), and C may be a string of"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef read_str3(file,dtype=None):\n\t\"\"\"\n\tThis function returns the bytes object corresponding to the \n\tcontent of the file. \n\t\"\"\"\n\tf=open(file,'rb')\n\tret=f.read(64000).decode(encoding='latin-1','ignore')\n\tf.close()\n\treturn ret\n\ndef read_str4(file,dtype=None):\n\t\"\"\"\n\tThis function returns the bytes object corresponding to the \n\tcontent of the file. \n\t\"\"\"\n\tf=open(file,'rb')\t\n"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\ndef timedelta_microseconds(v):\n\tv = int(v)\n\tif v < (86400000000000L):\n\t\tv += 964000000000L    \n\tticks = int(v % 1 / 864000000000L)\n\tvalue = ticks / float(ticks)\n\treturn datetime.utcfromtimestamp(v) + timedelta(microseconds=value)\n\ndef time_to_days(t, tz_utc):\n    t_utc = t - datetime.fromtimestamp(0)\n    return timedelta_microseconds(t_utc - tz_utc)\n\ndef time_to_num"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\ndef _time_to_days(dt):\n\t\"\"\"\n\tConvert a datetime object to a 1d numpy.timedelta64\n\t\"\"\"\n\tdays = (dt - datetime.datetime(1970, 1, 1)).days\n\tdays = int(days)\n\treturn days\n\ndef to_seconds(hours=0, minutes=0, seconds=0, milliseconds=0,\n\t\t\t seconds_precision=None):\n\t\"\"\"\n\tConvert a datetime object to a 1d numpy.timedelta64\n\t\"\"\"\n\t\n\thours = hours and int(hours) or 0\n\tminutes = minutes"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n        if ticks > 0:\n            sign = -1 # not really a negative number to start\n            return Time(ticks, sign * -1)\n        if tz is not None:\n            utctimetuple = lib.myround(int(ticks), -8).replace(hour=0, minute=0, second=0)\n            return Time(utctimetuple.utctimetuple()) + tz\n        else:\n            return Time(0, 0)\n\n    def __str__(self):\n        \"\"\"return string representation of the object\"\"\"\n        s = self.__str__()\n        assert '\\n' not in s, \"input string must not contain"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\t\n\tif not scale: raise ValueError('ticks parameter is out of bounds')\n\t\t\n\tticks2 = fixticks(ticks, tz)\n\t\n\treturn cls(*ticks2)\n\t\ndef utc_from_hms(h, tz='Australia/Sydney'):\n\timport pytz as utc\n\t\n\treturn utc.localize(datetime.utcfromtimestamp(h))\n\t\ndef iso_from_eur(d, format=None):\n\t\"\"\"Convert an ISO 8601 (Epoch) (datetime.date) to a date,\n\tusing a local timezone as a UTC timezone.\n\n\tFor dates outside"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\n\"\"\"Helper to make it easy to find out a path from $PATH. \nSee http://www.dietpepsi.org/wiki/DietPEP%20API for more details\"\"\"\n\nimport os, re\nimport __builtin__\n\ntry:\n  from setuptools.extern.six.moves import map\nexcept ImportError:\n  # This error will be raised if distribute isn't installed,\n  # but we must exit with error message\n  import sys\n  sys.dont_write_bytecode = True\nelse:\n  map = map\n\ndef locate_python_lib(path, prefixes="}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    if ticks > 86400000000000:\n        raise ValueError(\"ticks out of bounds (86400000000000)\")\n\n    if tz:\n        return cls(ticks, tz=tz)\n    else:\n        return cls(0, 1440)\n\n\ndef to_ticks(ticks, tz=None):\n    \"\"\"\n\tCreate a tick from ticks (tick as seconds past midnight), timezone\n\tor time of midnight, no matter which format is used.\n\n:param ticks: Ticks, tz, or datetime.tzinfo\n\tNote that no timezone is used here.\n\t@see datetime module functions."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    from datetime import datetime, time\n    from pytz import timezone\n    import time\n    try:\n        # make sure ticks is in UTC\n        if 0 < ticks < 86400000000000:\n            ticks += 86400000000000\n        # make sure it isn't already a number (just in case)\n        if not ((0 < ticks) and (ticks < 1000) and \n              ticks >= 10000000):\n            raise ValueError\n    except ValueError:\n        raise ValueError(\"ticks must be > 0 and not %d\" % ticks)\n\n    tz = _tzstr(tz)\n    return Time(ticks, tz=tz)\n    \ndef to"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\n\tif ticks < 0:\n\t\traise ValueError(\"Ticks must be in range 0 - 98400000000000.  \" +\n\t\t\t \"Received: %d\" % ticks)\n\t\n\t# ensure our value is sufficiently far in UTC, if we're on POSIX\n\tif tz is None:\n\t    tz = time.timezone / datetime.timedelta(seconds=(ticks * -1))\n\t# We're on windows, not MB so adjust timezones to UTC (they're more consistently\n\t# the same as the C standard -> Windows ones)\n\t# tz = pytz.timezone('UTC')\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n# --------------------------------------------------------------------\n# Support classes & methods\n\nclass Timezone:\n\n\tdef __init__(self, name, offset, local=False):\n\t\t\"\"\"\n\t\tUse 'local' to indicate UTC/None tzinfo differences.\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.offset = offset\n\t\tself.local = local\n\n\tdef local_to_datetime(self, dt):\n\t\t\"\"\"\n\t\tConvert naive time 'dt' into 'dt.local' format and adjusts offset\n\t\tif needed.\n\n\t\t:param dt:\n\t\t\tdate/time with UTC timezone\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\traise ValueError(\"ticks out of bounds\")\n\t\traise ValueError(\"ticks out of bounds\")\n\t\treturn cls(ticks=ticks)\n\n\tdays, seconds = divmod(ticks, 68400000000)\n\tminute = (seconds + 6) // 60\n\thour = (minute + 0) // 24\n\n\treturn cls(days=(days * 24 + hour) * 60, seconds=(seconds + 60) * 60)\n\n\ndef from_intervals(intervals, tz):\t\"\"\"\n\tCreate a time from intervals (intervals since midnight).\n\n:param intervals: an interval object\n:type intervals"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \n\tversion_arg_list = [None] * ((len(protocol_version) + 1) -\n                                len(version_tuple_list))\n\tif len(version_arg_list) == 1:\n\t\tversion_index = version_arg_list[0]\n\telse:\n\t\tversion_index = -1\n\thandlers = {}\n\n\t# Iterate through the protocol version list and use it to find\n\t# the protocol handler for each available version\n\tfor version_tup in version_index.iter_version():\n\t\t# Get protocol version\n\t\tversion, handler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\n\n\t# No support for default protocol version\n\tif not (protocol_version is None and\n\t\tprotocol_version[0] == 3):\n\t\t\traise TypeError(\"protocol version not accepted: %s\" % protocol_version)\n\n\tprotocol_handlers = {}\n\tfor protocol in protocol_version:\n\t\tif protocol in cls._protocol_map:\n\t\t\tprotocol_handler = cls._protocol_map[protocol]\n\t\t\tprotocol_handler(protocol)\n\t\telse:\n\t\t\tlog.warn(\"Using protocol %s %s\"\n\t\t\t% (protocol, protocol_version))\n\t\t\tprotocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise TypeError(\"protocol_handlers takes one or two \"\n\t\t\t\t\t \"parameters (e.g. '3', '5')\")\n\tprotocol_version = protocol_version or (3,)\n\n\tfrom.protocol_handlers import handler_registry\n\t\n\tresult = {}\n\tversion = sys.version_info\n\t__all__ = [\n\t\t# protocol handler dictionary key\n\t\t'DEFAULT',\n\t\t'trap_if_protocol_version_is_supported',\n\t\t# bolt version\n\t\t'diff_default',\n\t\t'handler_registry',\n\t\t'handler',\n\t\t# handlers used"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n        # assume protocol version matches version tuple\n        # only take first tuple version\n        # since proto_version is not None means that we have\n        # a version provided...\n        protocol_version = (3, 5)\n\n    for version in protocol_version:\n        # convert string into tuple\n        (protocol, (cversion_tuple, port)) = (\n            protocol_version[:2] + cversion_tuple,\n            protocol_version[2:] + cversion_tuple + [\n                port])\n        # construct dictionary (see note here we are ignoring tuple)\n        protocol_handlers_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    result = {}\n    if protocol_version:\t# if not specified use current version\n    for version in protocol_versions(protocol_version):\n        result[version] = handler_for_version(version)\n    return result\n\n\nclass RubyHandlerAdapter(object, BoltHandlerAdapter):\n    \"\"\"\n    An adapter for Ruby's Ruby protocol. It provides the correct \n    implementation and can wrap any Ruby class which has more options:\n      1. ProtocolHandler -> new subclass that will load config\n      2. BoltHandler -> new subclass that will handle the protocol handler\n    \"\"\"\n    def __prepare(self"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tpass\n\nclass BoltHandlerRegistry(object):\n\t\"\"\"\n\tA registry that stores handlers for different versions of various\n\tBoltHandler implementations.\n\t\"\"\"\n\t_map = {}\n\tby_protocol = {}\n\n\tdef register_version(self, protocol_version):\n\t\tprotocol_handlers = self._map.get(protocol_version, None)\n\t\tif protocol_handlers:\n\t\t\tfor protocol_handler in protocol_handlers:\n\t\t\t\tself.find_by_protocol[protocol_handler.protocol_version] = protocol_handler.__func__\n\n\tdef register"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\treturn dict(\n\t\t\t(name, cls)\n\t\t\tfor name in protocol_version\n\t\t\tif isinstance(protocol_version, tuple))\n\n\nclass FlinkClient(object):\n\t\"\"\"\n\tA client for bolt.\n\n    This client is capable of fuzzing the Flink client, as it supports\n    transactions of transactions and does not require the client to call Bolt\n    using the Flink protocol, such as a MySQL FLINK client.\n\n    The client is also capable of fuzzing the Flink protocol when creating\n    transactions, such as a MySQL Flink connection and receiving\n    transactions"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    handlers = {\n           'send-request': (handlers.MsgTypes.SEND_REQUEST, cls.send_request_handler),\n           'send-reply': (handlers.MsgTypes.SEND_REPLY, cls.send_reply_handler)\n        }\n\n    if protocol_version is None:\n        return handlers\n\n    else:\n        if not isinstance(protocol_version, (tuple, list)):\n            raise TypeError(\"protocol_version must by type tuple, not %s\" % (str(protocol_version)))\n        if len(protocol_version) == 2:\n            return {\n                key: ((val, )\n                       "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n        protocol_version = (3, 5)\n    if isinstance(protocol_version, tuple):\n        protocol_version = protocol_version + (0,)\n    handlercls = {}\n    for version in protocol_version:\n        hv = lambda f: False\n    for version in protocol_version:\n        try:\n            version = handlers[version]\n        except KeyError:\n            raise TypeError(\"no such handler for version %s\", (version, ))\n        if not hv():\n            raise TypeError(\"unsupported version %s in protocol_version\", (version, ))\n        version = protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    result = {}\n\n    if isinstance(protocol_version, tuple):\n        if len(protocol_version)!= 2:\n            raise Error(\"protocol_version should be a tuple\")\n        version = protocol_version[0]\n        major, minor, patch = protocol_version[1:]\n        \n        if protocol_version[0]!= 0:\n            raise Error(\"protocol_version should be a '0', not '%s'\"%(protocol_version[0])) \n        elif not isinstance(version, int):\n            raise Error(\"protocol_version should be an 'int', not '%s'\"%version) \n        else:\n            if"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tt = parse_bookmark_raw(values)\n\t\tcls.extend(t)\n\n\ndef from_path(path):\n\t\"\"\"Create a Bookmarks object from a path string.\n\n\tYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param path: Path string\n:type path: str\t\"\"\"\n\tif isinstance(path, str):\n\t\tpath = parse_folder_path(path)\n\treturn Bookmarks(path)\n\n\ndef parse_folder_path(path):\n\t\"\"\"\n\tParse a single folder path.\n\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n        return cls(values)\n\n    # Create a Bookmark object from a list of raw bookmark strings values.\n    @_decode_value\n    def from_bookmark_string(self, value):\t\treturn Bookmark(value)\n\n    # Creates an instance of Bookmark from raw values.\n    @_decode_value\n    def from_raw_values(self, values):\n        return Bookmark(values)\n\n    # Creates a Bookmark by parsing from the raw values.\n    @_decode_value\n    @staticmethod\n    def _decode_value(values):\n        (marker, values) = values.split(\":\")\n\n        return Marker("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n\n\tvalues = [s.strip() for s in values]\n\n\t# make sure values has the right format\n\tif len(values) % 3 not in [2, 3]:\n\t\traise ValueError(\"Length of values must be a multiple of 3\")\n\n\tvalues = list(zip(values[::2], values[1::2]))\n\t\n\treturn cls(raw=values)\n\nclass Bookmark(object):\n\t\"\"\"A bookmark for the book.\n\n:param raw: the bookmarks string (formatted as an array of arrays)\n:see: `parse_bookmarks_as_arrays"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tvalues = [val.replace('\\\\', '\\\\\\\\') for val in values]\n\n\t\t# Try and validate the input. Any errors are caught before the block\n\t\t# is started and the error is raised.\n\t\ttry:\n\t\t\tvalues = values or set([u'diff --bookmark-name=%s' % values])\n\t\texcept ValueError as err:\n\t\t\traise ValueError('Invalid string values: {0}'.format(err))\n\n\t\tif set([u'bookmarks', u'diff --bookmark-name=%s' % values]).issubset(values):\n\t\t\traise ValueError('Invalid bookmark names: {0"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tpass\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor value in values:\n\t\t\tvalues = value.split(BIT_LENGTH)\t\n\t\t\tif len(values)!= (BIT_LENGTH*BITS):\n\t\t\t\traise TypeError('Bookmarks data must have exactly %d bytes:' %\n\t\t\t\t\t\t(BIT_LENGTH*BITS,))\n\t\t\tbid      = int(values[0],16)\n\t\t\tdid      = int(values[1],16)\n\t\t\tbtype    = int(values[2],16)\n\t\t\tname    = values[3].strip('\\x00')\t\t\n\t\t\tvalue   = values[4]\n\t\t\tvalue"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfrom roamer import bookmarks\n\t\tfrom collections import namedtuple\n\t\timport pytoml\n\t\tself = pytoml.loads(values)\n\t\tself._tracks = []\n\t\ttry:\n\t\t\tname = bookmarks.extract_bookmarks_name(values[0])\n\t\texcept IndexError:\n\t\t\treturn\n\t\ttracks_info = []\n\t\ttracks_info.append((self, name))\n\t\tnext_id = 1\n\t\tfor track_idx, track in enumerate(tracks_info):\n\t\t\tnext_id = next_id + 1\n\t\t\t# skip past header\n\t\t\tif next"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\titerable = iter(values)\n\t\tlines = (line.strip() for line in cls.lines(iterable))\n\t\tdata = {}\n\t\tfor line in list(lines):\n\t\t\tdata[line] = line\n\t\treturn cls(dict_from_raw_values(data))\n\n\tdef as_dict(self):\n\t\treturn dict(self.items())\n\n\tdef items(self):\n\t\treturn dict(self.items())\n\n\tdef __str__(self):\n\t\tvalues = \"\"\n\t\tfor item in (self.items() if isinstance(self.items(), basestring)"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif values is not None:\n\t\t\tif not values.endswith(u'\\x00'):\t# skip NULL bytes\n\t\t\t\traise ValueError(\"Bookmarks values must end with '\\\\0': %r\" % values)\n\t\t\tfor item in values:\n\t\t\t\titem = item.strip(u'\\0')\n\t\t\t\tif item:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tbookmark = cls.read_null_bookmark(item)\n\t\t\t\t\texcept BookmarkError as e:\n\t\t\t\t\t\tlogging.getLogger(\"Bookmark\").warning(\n\t\t\t\t\t\t\tu\"Couldn't parse bookmark %s: %s\"\n\t\t\t\t\t\t\t%(item, e))"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif isinstance(values, str):\n\t\t\tfor bookmark_str in values:\n\t\t\t\tbookmarks.append(Bookmark(bookmark_str))\n\telse:\n\t\traise TypeError(\"Unsupported value type for bookmarks\")\n\n\tdef to_strings(self):\n\t\t\"\"\"\n\t\tConvert raw bookmarks to a dictionary in the form:\n\n\t\t{\n\t\t\t'bookmark_string': 'bookmarks'\n\t\t}\n\n\t\t\"\"\"\n\t\tbookmarks = {}\n\t\tfor bookmark_line in self.split_bookmarks_line(BOOKMARKS_KEY):\n\t\t\tbookmark_str = bookmark_line.strip()\n\t\t\t"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\n\tif seq and bufsize is None:\n\t\t# bufsize can be None since in Python 2, \n\t\t# with default (preferred) size is smaller:\n\t\treturn ((seq, _check_seq(seq, bufsize)),\n\t\t\t       _check_seq(seq, bufsize))\n\n\tif isinstance(seq, str):\n\t\t# Sequence should be a string according to size --\n\t\treturn ((seq,), _check_seq(seq, None, None))\n\n\timport io\n\n\tif isinstance(seq, list):\n\t\treturn ([seq], _check_seq(seq, None, bufsize))\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n# A better description of this method is:\n# \n# A *long* sequence may share one or more elements. For example:\n# \n# * A series of numbers with different lengths and types - e.g. \n# \n# * A series of sequences with different lengths but different types - e.g. \n# \n# * A series of sequences with different lengths - e.g. \n# \n# * A series of sequences with different lengths but different types - e.g. \n# \n# * A series of sequences with different lengths."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tif len(seq) < bufsize:\n\t\t\traise NotImplementedError\n\t\titems = []\n\t\twhile 1:\n\t\t\tif len(items) == 0:\n\t\t\t\treturn items\n\t\t\titems.append(items.pop(0))\n\t\tif len(items) == 0:\n\t\t\traise ValueError\n\t\tseq = tuple(items)\n\t\tif len(seq) == 1 and type(seq[0]) is not tuple:\n\t\t\treturn seq[0]\n\t\telse:\n\t\t\treturn (seq, type(seq[0"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t \n\t# TODO: I feel like this should be simpler\n\tif bufsize is None:\n\t\tbufsize = -1\n\n\tif   seq == None:\n\t\treturn (None, None)\n\n\tif   (len(seq) < bufsize):\n\t\tif   (len(seq) > bufsize):\n\t\t\treturn (None, None)\n\t\tif   (len(seq) < bufsize-1):\n\t\t\treturn (None, None)\n\t\tif   (len(seq) < bufsize-2):\n\t\t\treturn (None, None)\n\t\t# XXX This should be a better way to"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\t# The test cases are all (floats)\n        f = lambda: F(seq, bufsize)\n        return _test(f)\n\nclass BasicFp(object):\n    \"\"\"\n    This class can be used as a context manager to implement a (thread-safe)\n    parallel (non-recursive) synchronization of multiple processes.\n\n    The basic workflow is as follows:\n\n   .. testcode::\n\n     with BasicFp(\"Hello, World\") as p:\n      ...     p.put(HelloWorld(123))\n      ...     p.get()\n     \"\"\"\n\n    def __init__(self, target, name):"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tif ((len(seq) == 1 and type(seq[0]) is type) or\n\t\t\t((len(seq) == 2 and type(seq[1]) is type) and seq[0] in STR_TYPES)):\n\t\t\treturn (((seq[0],) + subtype) if (subtype is None) else (seq[:1],))\n\t\telse:\n\t\t\traise TypeError(\"wrong argument type\")\n\tctype = type(seq[0])\n\tif ctype is tuple:\n\t\treturn tuple(ctype[:3]),"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None: bufsize = len(seq)\n\t\tif seq in seqs: return seq\n\t\tsize = len(seq)\n\t\tif 0 < size < bufsize: return (seq, 1)\n\t\tbufsize = int( bufsize.bit_length() or 1)\n\t\t# Note: a loop with an array of size 1 has the desired interpretation\n\t\t# of the argument. This is a little buying that allows the real\n\t\t# sequence to be memory-efficient, but not a big deal for the number of sequences in\n\t\t# practical tests.\n\t\tfor seq_len"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = 1 << 16\n\t\tif len(seq) == 2:\n\t\t\tseq[:1] = seq[-2:]\n\t\t\tseq[-2:] = seq[-1:]\n\t\telse:\n\t\t\tseq = chain(seq[:-1], seq[:1], seq[1:])\n\tseq = map(seq.__getitem__, seq)\n\tif len(seq) == 1:\n\t\treturn seq\n\treturn (seq,)\n\ndef _convert_other(s):\n\t\"\"\"Convert other (single) value to :\n\t\t`basestring` if `s"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t return islice(seq, 0, bufsize)\n\nseqtype = c_int # enum\n\nclass _Types(Structure):\n    _fields_ = [\n        ('next', c_void_p),\n        ('pa1', c_size_t),\n        ('pa2', c_ssize_t),\n        ('p1', c_char*psize_t)\n    ]\n\nclass StructureIter:\n    _fields_ = [\n        ('head', POINTER(Structure)),\n        ('tail', POINTER(Structure)),\n    ]\n\nclass union_iter2(_Iter2):\n    def __init__("}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = 30\n\t\ts = \"\"\n\t\tfor i in range(0, len(seq), bufsize):\n\t\t\tif len(s) + i < bufsize:\n\t\t\t\ts = s + seq[i:i + bufsize]\n\t\t\telse:\n\t\t\t\ts = s + \" \"\n\t\treturn s\n#   Copyright 2013 OpenStack Foundation\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#   not use this file except in compliance with the License. You may obtain\n#   a copy of the License"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\traise ValueError(\"cannot use scale 'other' with scaled histogram\")\n\t\n\tif other is None: return\t\n\tself._assert_compatible_dtype(other)\n\tother = self._constrainList(other, \"scale\", \"other\")\n\t\n\tif recompute:\n\t\ttry:\n\t\t\treturn self.__class__(self[0], float(other[0]))\n\t\texcept IndexError:\n\t\t\traise IndexError(\n\t\t\t\"histogram with default scale must have integer entries, or have different lengths\")\n\t\n\t# In theory, the _other_ method in the documentation for\n\t# histogram.scale"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n\n\tif other is None:\n\t\tother = self\n\n\trescaled = False\n\ttry:\n\t\tdscale = other.do_scale\n\t\tif dscale!= other.scale:\n\t\t\traise ValueError(\"cannot use unscaled histogram\")\n\texcept ValueError:\n\t\tdscale = self.scale\n\t\n\n\tif dscale!= other.scale:\n\t\traise ValueError(\n\t\t\t\"scale %s should have been %s\" % (\n\t\t\t\tself.scale, other.scale,\n\t\t\t))\n\n\tif dscale < 0:\n\t\traise ValueError(\n\t\t\t\"when using"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is not None:\n\t\t\tother = self.norm()\n\t\t\tvalue = other * other\n\t\t\tfor sub, scale in self:\n\t\t\t\tvalue = value if math.isnan(value) else value/other\n\t\t\tself.scale_value = self[0]*self.scale + value*scale\n\t\telse:\n\t\t\traise ValueError(\"Either a normalized or \"\n\t\t\t\t\"single value must provide an int, \"\n\t\t\t\t\"list, or a tuple.\")\n\ndef mean(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set mean.\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\traise LevParse(\"Cannot rescale '%s'\"% other, 0)\n\n\tself.scale_set(other, recompute)\n\tif recompute: self.recompute()\n\treturn self\n\n_scale = scale\n\n\ndef get_scale(other=None): return _scale\n\ndef _get_base(other=None): return _base\n\ndef _set_base(other=None, recompute=True): _base = None\n\ndef get_base(other=None): return _base\n\ndef _get_base_not_set(other"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\t# We can't do a histogram with no rescale=True here, but then\n\t\t# just check if it doesn't match.\n\n\t\ttry:\n\t\t\t_assert_(_all_same(self, *other))\n\t\texcept ValueError:\n\t\t\treturn None\n\n\tif isinstance(self, NumberHistogram):\n\t\treturn float(self)\n\n\tif not self.is_numeric_dtype and other is not None:\n\t\traise ValueError(\"Only numeric type containers are supported.\")\n\n\tif recompute and numpy.all([a is self for a in [self, other]]):\n\t\traise"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tself._init_base(other, overwrite=False, copy=False,\n\t\t\t\t\t\t   ignore_scale=False, new_size=0,\n\t\t\t\t\t\t   strict_min_value=True, ignore_min_size=True,\n\t\t\t\t\t\t   strict_max_value=False)\n\t\tself.clear()\n\n\t\tif type(other) is not ExponentialScale:\n\t\t\treturn self\n\t\tself.size += other.size # Use to be consistent!\n\n\t@property\n\tdef min_key(self): return np.min(self)\n\n\n\n\tdef has_key(self, min_"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif self is None or other is None:\n\t\t\treturn\n\tif self.sum > 0:\n\t\tself._raise_if_error(\"cannot rescale by undefined object\")\n\n\tself.total = sum(self.itervalues())\n\t# If not scaled by zero, return 0.\n\tif self.sum == 0:\n\t\tself.n = 0\n\telse:\n\t\tself.n = self.sum / float(self.total)\n\n\tif  self.sum in [0,1] and \\\n\t   not self.allclose(self.mean, self."}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t    \n\tif not self._data: raise TypeError(\"A histogram of the same dim %s '%s', could \"\n\t\t\t    \"not be expanded further, or this histogram would be \"\n\t\t\t    \"overspecified.\" % (self.name(), other.name()))\n\tif other is None: raise TypeError(\"You need to compute the other histogram object.\")\n\tmy_data = self._data\n\tif my_data.size == 0: return\n\tif other is None: \n\t\traise LengthError(\"Cannot rescale %i histogram (size zero).\" % \\\n\t\t\t\t  (self.size()))\n\t#"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\tother = self.copy()   # self can be None\n\t\treturn other.set_scale(self)\n\t\t\n\tdef copy(self):\t\t# new copy not deep, we're not sharing\n\t\traise ValueError, \"Can't copy a histogram\"\n\n\tdef new_hist(self):\n\t\t\"\"\"\n\t\tCreate a new histogram and fill it with the\n\t\tnew histogram, without making the changes for\n\t\tanother histogram.\n\t\t\"\"\"\n\t\tself.check_hist(self)\n\t\tself._new_item = 1\n\n\tdef create_group"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t    \nclass Labelframe(object):\n\t\"\"\"\n\tHelper object for Labelframe objects.\n\t\"\"\"\n\t_default = object()\n\t_current_index = None\t\t# index of currently loaded item\n\t_new_index   = None\t\t# index of the new item (in tree form)\n\t_min_index   = None\t\t# index of minimum item (in tree form)\n\t_max_index  = None\t\t# index of maximum item (in tree form)\n\t_default_scale = None\t\t# value of scale"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\treturn Vector3d(\n\t\t\t1, # scalar\n\t\t\t2, # vector\n\t\t\t3, # vec3\n            )\n\n_Vector3dVector = Vector3d\n\ndef copy(self, other=NONE):\n    \"\"\"\n\tDeep copy the graph.\n\n:Parameters:\n\tother : another :class:`graph_tool.Node` object that\n\t support the copy operation.  If *other* is specified, use\n\t      the given *type* of *other* before copying.\n\n\tAfter this method is called, the arguments are identical to\n\tthis tree, except that"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tdef re_x(x):\n\t\t\tif isinstance(x, (basestring,int,Float)):\n\t\t\t\treturn _core_.GraphicalForm.X(\n\t\t\t\t  [], [], x.n3(), None\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\treturn _core_.GraphicalForm.X(  [], [], x, None )\n\n\t\tdef re_y(y):\n\t\t\tif isinstance(y, (basestring,int,Float)):\n\t\t\t\treturn _core_.GraphicalForm.Y(\n\t\t\t\t  [], [], y.n3(), None\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\treturn _core_."}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\traise TypeError(\"cannot compute scale if this object is empty graph or the given vector of the other object is no more than two objects.\")\n\t\tif __debug__:\n\t\t\tif not isinstance(other, Graph) or not isinstance(other, (int, long, int, float)):\n\t\t\t\tif __debug__ :\n\t\t\t\t\tb = Graph()\n\t\t\t\t\tb.add(hash(self))\n\t\t\t\t\tb.draw(other);\n\t\t\t\traise TypeError(\"cannot compute scale while rescaling to different objects.\")\n\t\t\tif len(self)!= other.length():\n\t\t\t\traise"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\treturn self.scale_extrapolate_info(other)\n\n\tdef scale_vector(self, other=None):\t\"\"\"\n\tGet or set the scale vector of the graph.\n\nIf *other* is ``None``, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has 0 values and 3 values,\nthen rescaled, and for a 3"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None and self == self.graph:\n\t\t\treturn Graph.SCALE\n\t\tother = {k:v for k, v in self.coords.items() if v is not Undefined}\n\t\tif other:\n\t\t\treturn Graph.SCALE(self.x, self.y, *other.values())\n\t\treturn Graph.SCALE(self[0], self.y, *other)\n\n\tdef transform(self, *e):\n\t\treturn Graph.SCALE(getattr(self.graph, 'transform', e[0]), self.x, self.y, self, *e"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif isinstance(other, PosGraph):\n\t\t\tself.graph = copy(other.graph)\n\t\telse:\n\t\t\tself.graph = PosGraph(other)._graph\n\t\tfor a, b in self.graph.adjacency_iter():\n\t\t\tif b < other.x and b >= other.y:\n\t\t\t\tcontinue # no change in for some reason\n\t\t\tif self._coord_exists(a):\n\t\t\t\tv = self._value(a)\n\t\t\t\tif v!= v:\n\t\t\t\t\tif self.graph.scale(v, other):\n\t\t\t\t\t\traise ValueError(\"Not rescaled."}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other == None:\n\t\t\tother = self\n\t\txscale, yscale = self.getScale(other)\n\t\tfor (xval, yval) in utils.pango_to_gdk_coords(X11.client_to_screen_coordinate_format(), yval.client_to_screen_coordinate_format())[1:]:\n\t\t\tself.scale(other, xscale)\n\t\tif (self.getDataExtents()!= (xscale, yscale) and\n\t\t\tmath.isnan(xval)):\n\t\t\terrstr = _(\"X coordinate outside view range.\")\n\t\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\tself._unit = 1.\n\t\t\treturn\n\t\tassert len(other) == 2\n\t\tassert self._unit == self._max_unit\n\t\tscale_y = self.scale(self._x)\n\t\tscale_x = self.scale(self._y)\n\t\tassert len(self._x) == 2\n\t\tassert len(self._y) == 2\n\t\trescale_x = float(self.scale(other[0])) * self._unit\n\t\trescale_y = float(self.scale(other[1])) * self"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif not isinstance(other, NoneType):\n\t\t\traise TypeError\n\t\tassert self.is_in_view() or isinstance(other, type(None)), type(other)\n\n\t\tself._scale_to(other)\n\t\treturn self\n\n\tdef _scale_to(self, other):\n\t\tif other is None:\n\t\t\tif self.is_grid_x or self.is_grid_y:\n\t\t\t\tself._scale_x(other)\n\t\t\telif self.is_grid_z:\n\t\t\t\tself._scale_z(other)\n\t\telif isinstance(other,"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tvalues = self.get(\"xy\")\n\t\tif values is None: return None # no xy\n\t\treturn tuple([int(i) - 1 for i in values])(*other)\n\n\tdef normalize(self):\n\t\t\"\"\"\n\t\t:return: a normalized representation of this graph.\n\t\t\"\"\"\n\t\tx, y = self.get(\"xy\")\n\t\treturn (x * self.scale())/self.scale()\n\n\tdef _addPoint(self, point):\n\t\t\"\"\"\n\t\tIndicate to add one point to the given graph point.\n\t\tDoes not raise :exc:`."}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tassert isinstance(hist, Histogram)\n\t\tif make_value is not None:\n\t\t\thist = make_value(hist)\n\t\t\n\t\tif make_value is None:\n\t\t\t# make one variable, useful for generating graphs.\n\t\t\thist = hist.context(make_value)\n\t\t\n\t\tgraph = HistogramGraph(hist)\n\t\t\n\t\tfor v in make_value(hist):\n\t\t\tgraph.add_value(v)\n\t\t\n\t\t# update histogram with all existing information\n\t\t# and graph\n\t\tfor v in make_value(hist):\n\t\t\tgraph.add_value(v)"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\n\tif make_value is None:\n\t\tmake_value = lambda value: None\n\n\tif hist == None:\n\t\thist = hist_to_graph(hist)\n\tif make_value:\n\t\tmake_value_error = make_value_error_bar(hist)\n\telse:\n\t\tmake_value = lambda value: hist_to_graph(value, make_value)\n\treturn hist_to_graph(hist, make_value)\n\n#################################################################\n\ndef make_graph_x(range, x_scale, name, values, title ="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\traise NotImplementedError(\"hist to_graph\")\n\n\td = dict(\n\t\t_fill_options=dict(side='left', fill='toZero'),\n\t\t_edge_color=color,\n\t)\n\tif make_value:\n\t\tvalues = hist\n\telse:\n\t\tvalues = [hist]\n\n\tfig = make_graph('histogram', values, values)\n\tfig.xaxis.axis_label = 'value'\n\tfig.yaxis.axis_label = 'error'\n\n\treturn fig, None\nimport sys\nimport copy\nimport operator\nimport py\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\treturn make_value(hist, make_value)\n\n\tdef __init__(self, name, context, default_title=None):\n\t\tself.name = name\n\t\tself.context = context\n\t\tself.default_title = default_title or name\n\t\tself.label = None\n\t\tself.min = None\n\t\tself.max = None\n\t\tself.bins = None\n\n\n\tdef __repr__(self):\n\t\treturn '<Histogram object with %d-%d attributes>' % \\\n\t\t   (len(self.bins)+1, len(self.labels))\n\n\tdef"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t# Check that hist is an instance of histogram.\n\t\tif not isinstance(hist, h5py.highlevel.Hist):\n\t\t\traise ValueError(\"The hist argument must be a h5py.highlevel.Hist instance.\")\n\n\tif make_value:\n\t\thist = make_value(hist)\n\n\t# If we have a single value, then we want it to be a single value.\n\tif len(hist) == 1:\n\t\thist = hist[0]\n\n\tif make_value:\n\t\t# Create a histogram.\n\t\thist = h5py.highlevel"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t_check_class(hist, histogram, make_value, \"hist\")\n\n\t\t_make_value = make_value\n\n\t\t# validate and set default\n\t\tif callable(_make_value):\n\t\t\t_make_value = _make_value()\n\t\t\tif type(_make_value) is bool:\n\t\t\t\t_make_value = _make_value\n\t\t\tassert _make_value is None\n\n\t\tif not (hist is None and isinstance(hist, Hist)):\n\t\t\traise TypeError(\"hist has to be either \\\"hist\\\" or \\\"%s\\\" \" % self.__class__.__name"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t))\n\n\n#\n# Create a basic histogram\n#\n\ndef hist(bins, make_value=None, get_coordinate=False, scale=None, hist_bin=None):\t\t# pylint: disable=arguments-differ\n\t\"\"\"\n\tCreate a histograms for a list of :class:`.Histogram` objects.\n\tThis function is mostly the code for creating histograms (i.e. what makes sense).\n\n\t*bins* is a list of (index, value) pairs, where *value* is the value\n\t(or a histogram object itself if *hist"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\n\tdef __init__(self, hist=None, make_value=True, field_names=None, scale=1, **kwargs):\n\t\t\"\"\"\n\t\tCreate a new custom :class:`.histogram` object.\n\n\t\tIf hist is None, the histogram instance will be created after\n\t\treading data. If make_vbins is True, a histogram instance\n\t\twill be built automatically on the fly. In the case of an histogram\n\t\tinstance, the `make_value` parameter determines the bin content to use\n\t\tfor x, y, values for values.  If make_vbins is"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\n\t_range = [None, None]\n\tbins = []\n\n\t_fields = _guess_from_kwargs(**kwargs)\n\tif kwargs.get(\"do_range\", None):\n\t\t_range = kwargs[\"do_range\"]=_range\n\tif \"bins\" in kwargs:\n\t\tbins = kwargs[\"bins\"]\n\telif \"bins_required\" in kwargs:\n\t\t_required = True | kwargs[\"bins_required\"]\n\t\t_bins = kwargs[\"bins_required\"]\n\t\tif not (0<=_required):\n\t\t\traise GraphError(\"bins_required must be"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n        import numpy as np\n\timport matplotlib\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport scipy.stats\n\t\n\t#import random\n\tfrom array import array\n\timport operator\n\tfrom math import floor, ceil\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\timport scipy.cluster\n\t\n\t#this is the edge length; this needs to be adjusted\n\tN = 100000\n\tg = scipy.cluster.hierarchy.ward(np.array([0,0,N/2.]), [0,0,N/2. + N/"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\n\t#\n\t# The candidate's class declares to the interface. This is where we\n\t# have the interface.\n\t#\n\t# It will go inside an instance of the class it was defined on.\n\t#\n\t# There is no way to check if there are no methods or attributes. This\n\t# is why there's not just one such a method.\n\t#\n\t# To handle this, this method is called indirectly through the instance\n\t# and the name (well, what happens on the assumption of the *type* of\n\t# the candidate."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n# Local interfaces are defined here as they are only needed to find\n# local interfaces.\n\n# Exceptions that should be raised if they can't be inferred.\n\nclass DeclaimTypeError(TypeError):\n\tdef __init__(self, iface, wanted):\n\t\tTypeError.__init__(self, '{0} was a type {1}, but wanted {2}'\n\t\t\t.format(iface.name, iface.providedBy, wanted.__name__))\n\t\tself.iface = iface\n\t\tself.wanted = wanted\n\nclass CannotDeallocateType(DeclaimTypeError):\n\tdef __init__(self,"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t    \n\tif not candidate in iface.implementedBy(iface.providedBy(candidate)):\n\t    raise zope.interface.Invalid(iface.name, iface=candidate)\n\n\tif tentative and iface.implementedBy(candidate):\n\t    raise zope.interface.Invalid(iface.name, iface=candidate)\n\n\tattrs = list(iface.getAttributeValues(ns=dict(**attrs) ))\n\tattrs = [(n, l, t) for (n, t) in attrs if t is not None]\n\tif len(attrs) > 1:\n\t    raise zope.interface.Vetoure"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    ifaceattr = iface.providedBy(candidate)   \n    if (not ifaceattr) and (not tentative) and isinstance(candidate, interfaces.IProvide):\n\traise InterfaceError(\"Interface class cannot provide a %s which is\"\n\t\t\t        \"not declared by %s. It is used by a %s of %s.\" % \\\n\t\t\t\t\t\t\t\t  (typ, iface, candidate, ifaceattr))\n    elif tentative and  ifaceattr:\n\twarnings.warn(\"The interface %s has provided by %s using %s, which is not\"\n\t\t\t           \" declared by %s.\" %"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    zope.interface.alsoProvides(iface)\n    \n    if not candidate.providedBy(iface):\n\traise exc.Invalid('candidate does not provide %s' % iface)\n        \n    if not hasattr(candidate, 'providedBy'):\n\tif not (hasattr(candidate, '__path__') and \n\t\t\tcandidate.__path__.find('zope')!= -1):\n\t    raise exc.Invalid('candidate does not provide __path__ '\n\t\t    'and __name__ == \"%s\"' % candidate.__name__)\n\t\n\t\n    if not hasattr(candidate, '__qualname__'):\n\t    raise exc.Invalid('candidate does not specify"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn True\n\n\tif not candidate.providedBy(iface):\n\t\tif vtype in iface.names():\n\t\t\treturn True\n\t\tif not tentative and not (iface.providedBy(iface) and is_valid_iface(iface)):\n\t\t\traise Invalid(iface, candidate)\n\ndef is_valid_iface(iface, tentative=False):\n\t\"\"\"Checks if a class provides a `iface`.\"\"\"\n\tfor iface_info in iface.ifaces():\n\t\tif iface_info.providedBy(iface):\n\t\t\treturn True\n\n\treturn False\n\n\ndef collect("}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if vtype:\n\t\tmembers = ', '.join(iface.providedBy(candidate))\n\t\traise Invalid(('%s: %s.%s requires %s.%s'\n\t\t\t\t% (memberName, iface.__name__, memberType, vtype, members)))\n\n## Local Variables:\n## fill-column:78\n## End:\n# -*- coding: utf-8 -*-\n#\n# This file is part of FogLIMS\n# Copyright (C) 2015 Lucas Hoffmann\n#\n# This program is free software; you can redistribute it and/or\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if isinstance(candidate, type) and issubclass(candidate, Interface):\n\tfor name in dir(candidate):\n\t    candidate_attr = getattr(candidate, name)\n\t    if _matches(candidate_attr, tentative, vtype, True):\n\t\treturn True\n\traise Invalid(type(candidate), \"%s is an Interface and %s \"\n            \"doesn't\" % (candidate, tentative))\n\t\n\ndef _matches(attr, tentative=False, vtype=None):\n    \"\"\"\n    Searches a candidate for *attr* in a class hierarchy and returns\n    the class object. Returns"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # This step is only valid if iface accepts only'required' and the\n    # required attribut is a key in iface specification. For interface\n    # methods, nothing gets returned.\n    if iface.implementedBy(iface.IInterface):\n        return 0 # It is always acceptable to have this condition\n    if candidate is None:\n        if tainted is not None:\n            raise Invalid(\n                \"Interfaces implementing '%s' required by %s\" %\n                (repr(iface), repr(tweak)), [iface])\n        return 1\n    else:\n        # It is only acceptable to have this condition\n        if tain"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if iface.__spec__.keyword == 'zope.interface.interfaces' and \\\n        iface.__name__ == 'zope.schema.interfaces':\n            zapi.provide(iface, iface.__name__, interfaces.IInterface)\n        if 'zope.annotation' in iface.__annotations__:\n            raise zope.interface.Invalid('Multiple methods or annotations '\n                                    'detected')\n            \n    if not candidate in __zope_candidates:\n        raise zope.interface.Invalid(\n                'No such candidate: %r' % candidate\n                )\n    \n    candidate_obj = __zope_candidates[candidate]\n\n    # Test if"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn False\n\n\tfor attr in iface_attributes(iface):\n\t\tif not attr.implementedBy(candidate):\n\t\t\tif attr.requiredBy(candidate):\n\t\t\t\traise Invalid(node,\n\t\t\t\tmessage=\"Descriptor %s not provided in interface %s.\" %(attr.identifier(), iface_id(attr)))\n\t\telse:\n\t\t\tif not attr.name:\n\t\t\t\traise RequiredBy(node,\n\t\t\t\tmessage=\"Descriptor %s required by %s.\" %(attr.identifier(), iface_id(attr)))\n\telse:\n\t\tif not candidate.unbounded():\n\t\t\traise Invalid("}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # The interface provided by the candidate should have the same\n    # interface, but some interfaces do, use it. Otherwise no checks\n    # are performed here.\n#    assert iface.providedBy == iface.providedBy(candidate), \"Got different interface in\"\n    \n    # First check that the interface itself declares.\n#    iface2 = ifaceClass.interfaceFor(candidate)\n#    checkClass(iface2, iface)\n    if not isinstance(candidate, iface2):\n        raise Invalid(\"interface %s not subclass of %s\" % (\n            iface2.__name__, candidate.__name__))\n    \n   "}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\t   \n\tif not isinstance(candidate, iface.implementedBy):\n            if tentative and candidate.isInterface():\n                return True\n\t    else:\n\t\t# XXX When we don't get to the problem, we could have a\n\t\t#     situation where a user attempts to access an\n\t\t#     interface that's not the interface that the method is\n\t\t#     intended to be. Instead of calling isinstance() in the\n\t\t#     sense that we simply let ``isinstance()`` do nothing.\n\t\t#     It would seem odd that ``isinstance()`` never\n\t\t#     would be used if we have a"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    return True\n \n\n__all__ = [\n   'verifyObject',\n    'getObjectFromInterface',\n   'verifyClass',\n   'verifyObjectWithAttribute',\n    'Adapter',\n    'compareTypes',\n    'convertKeywords',\n    'addAdapter',\n    'compare',\n    'check',\n    'Unsupported',\n\n    \"getAdapter\",\n    \"getUtilities\",\n    \"getUtilitiesFor\",\n    \"getWSDL\",\n    \"getRequest\",\n    \"getRequestClass\",\n    \"Request\",\n    \"Response\",\n    \"ResponseWithAttribute\",\n\n    \"verifyObjectAndWSDL\",\n    \"verifyObjectFromInterface\","}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if __debug__: yield \"zope.interface.verifyObject(iface, candidate)\"\n\n    def requiredAttrs(self):\n\t\"\"\"Return two-tuple of the name/value pairs for all the interface attributes\"\"\"\n    try:\n        if self.__providedBy__: return (self.__name__, self.__value__)\n        else: return ()\n    except AttributeError:\n        raise AttributeError, \"zope.interface.verifyObject() object has no attribute '%s'\" % self.__name__\n\n    def optionalAttrs(self):\n\t\"\"\"Return two-tuple of names and values for all the interface attributes\"\"\"\n    return (self.__"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\n    candidate = checkInterfaces(candidate)\n    if candidate.providedBy(iface) and \\\n            not (candidate.providedBy(iface.interface) and \\\n                len(candidate.interfaces) == 1):\n\traise Invalid('Incompatible interface object', iface, candidate)\n\n    for iface2, iface3 in interfaces(candidate):\n\ttry:\n\t    if iface3.implementedBy(iface, candidate):\n\t\t    return True\n\t    else:\n\t\tpass\n\t    candidate = iface3\n\texcept Invalid:\n\t    pass\n    raise Invalid('Incompatible interface object', iface, candidate)\n\n    def __compareAttributes(self, iface"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    interfaces = iface.interfaces()\n    return cls.implementedBy(iface, interfaces)\n  \n  def providedBy(self, iface, requirements=None):\n\tif not requiredArguments:\n\t\trequiredArguments = []\n\tif self.iface:\n\t\tprovidedBy = iface.providedBy(self.iface)\n\telse:\n\t\tprovidedBy = iface\n\n\tdef _getDependentMethods(self, iface, tentative):\n\t\tfor iface_methods in iface_methods:\n\t\t\tif tentative is True and \\\n\t\t\t\tself.implements(*iface_methods): return\n\t\t\tif ("}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # NOTE: All of the previous non-special methods may return this \"chained\"\n    ##         chain.  So use keyword arguments to change the behaviour\n    #         in the future.\n#         chainedMethod(iface.providedBy, iface.name).returns(iface)\n\n    if not candidate.providedBy(iface, None, True):\n        raise zope.interface.Invalid('interface:'+ repr(iface))\n\n    if tentative and iface.provides.get('IProvide')!= ['Zope 2.0']:\n        raise zope.interface.Invalid('requires:'+ tentative"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    ret = zope.interface.methods(iface, tentative=tentative)\n    for iface_name, iface in ret.interfaces.iteritems():\n        try:\n            checker = zope.interface.verify.interfaces.Checker(iface_name)\n        except Invalid, ex:\n#            import pprint\n#            pprint.pprint(ex)\n#            raise Invalid(ex.args[0])\n        # check the interface\n        try:\n            iface_to_check = zope.interface.verify.interfaces.Not(iface, iface_name, candidate)\n            checker.check()\n        except interfaces.Invalid as"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    iface_spec = (iface.__spec__ if hasattr(iface, '__spec__') else\n                  getattr(iface, '__class__', type(iface)))\n    \n\n    # Check for a single base class\n    base = iface_spec.pop(-1)\n    if base is object:\n        raise Invalid('%s is not an interface' % iface)\n    if not issubclass(interface.Class, base):\n        raise Invalid('%s is not a base class' % base.__name__)\n\n    # Check if the first error was reported\n    if not issubclass(iface_spec[0], interface.Invalid):\n        raise Invalid('%s: %"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\treturn verifyClassIter(iface, candidate, tentative)\n\n########################################################################\n#\n# Python 3 support.\n\nxrange = six.integer_types + ( types.MethodType, )\n\ndef safe_range(val, rng=None):\n\t\"\"\"\n\tGenerate a range from an iterable of numeric values and a\n\trange of numbers.  This function doesn't do a direct comparison\n\tover all the comparison operators (since all comparisons are numeric,\n\tintegers and floats).  It can normalize the ranges so they work\n\tthe same as normalization, i.e., a list of"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\treturn iface == candidate or isIETF(candidate)\t# True if we are an interface for the candidate\n\n\tif not hasattr(candidate, '__call__'):\t# Not a candidate, so assume it's a method\n\t\tifaceType = getIfaceType(iface)\n\t\tif ifaceType is None:   # No interface type available.. this has no definition on the interface (use the whole iface)\n\t\t\tif DEBUG: print(\"WARNING: No class/interface method found for %s. Assuming %s.\" % (iface, self._candidateInst))\n\t\telse:\n\t\t\treturn True\n\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\t# Check class\n\t\tif tentative and interfaceToSkip(iface, candidate, tentative):\n\t\t\treturn 1\n\t# If we get here, iface isn't trusted\n\tif ifaceToSkip(iface, candidate, tentative):\n\t\treturn 0\n\tif iface[\"type\"] == \"untrusted\":\n\t\t# If iface is trusted use it.\n\t\treturn 1\n\t# Check that iface is only known if *candidate* is trusted.\n\tif iface == candidate:\n\t\t# No need to check this in general.\n\t\traise WafError, \"This iface"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\ttoDel = False\n\t\tif tentative:\n\t\t\tassert(candidate.__class__ == tentative)\n\t\t\tassert(not toDel)\n\t\t\ttoDel = True\n\t\telif tentative is True:\n\t\t\tassert(candidate.__class__!= tentative)\n\t\t\tassert(not toDel)\n\n\tdef testClass(self, candidate):\n\t\tiface = candidate()\n\t\tassert(verifyClass(iface, candidate))\n\t\tassert(candidate.__class__ == tentative)\n\n\tdef testConstructorInvalid(self):\n\t\tiface = candidate(foo=42)"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif iface is IInjExample:\n\t\t\tverifyClass(candidate, tentative)\n\t\t\tassert len(iface.getDescendents(candidate)) == 1, iface.getDescendents(candidate)\n\t\telse:\n\t\t\tassert isTechniqueAvailable(candidate)\n\n\tdef verifyInterface(iface, candidate, tentative=False):\n\t\tif iface is IInjExample:\n\t\t\tverifyClass(candidate, tentative)\n\t\t\tassert len(iface.getDescendents(candidate)) == 1, iface.getDescendents(candidate)"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tifaceName = iface.friendly_name\n\t\tfor iface in candidates:\n\t\t\tif iface.path.basename(candidate.getFriendlyName()) == ifaceName:\n\t\t\t\tif not tentative:\n\t\t\t\t\traise SafeException(\"In trusted data (not a T-bit device).  Please ensure that %s is tainted.\"%(ifaceName))\n\t\t\t\telif tentative:\n\t\t\t\t\t# Tentative verify.  We test both cases too; we simply reject any\n\t\t\t\t\t# exceptions from the user in order to fix this further.\n\t\t\t\t\traise SafeException(\"In trusted data (not a"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif tentative:\n\t\t\tif iface.name.endswith('-example'):\n\t\t\t\tmod = iface.getProductClass()\n\t\t\telse:\n\t\t\t\tmod = iface.getModule()\n\t\t\tif not mod.isActive():\n\t\t\t\traise RuntimeError(\"No Active Window defined for %s\" % iface)\n\t\t\ttry:\n\t\t\t\tst = sys.modules[mod.__name__]\n\t\t\texcept AttributeError:\n\t\t\t\tst = None\n\t\t\tif st and not mod.isActive():\n\t\t\t\traise RuntimeError(\"Active Window for %s doesn't exist\" % iface)\n\t\ttry:\n\t\t\tst.query"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tcpt = get_classes[candidate]\n\t\tiface = cpt.__name__\n#FIXME: we test here only the types of interfaces are indeed\n#\t\tassert ((iface == \"dummy\" and cpt.isInterface) or (iface == \"ns\"))\n\t\tif iface == \"dummy\":\n\t\t\treturn\n\ndef getTypesNice(candidate, iface=\"dummy\"):\n\t\"\"\"\n\tReturns a list describing the types of given interface\n\t\"\"\"\n\tl = []\n\tifacen = {\"dummy\": iface}\n\tif ifacec in candidates:\n\t\tl."}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif tentative: return\n\tifaceName = None\n\tfor x in candidate.keys():\n\t\tifaceName = ifacePrefix + x\n\t\tif (candidate[x]['iface'] == ifaceName) or (candidate[x]['iface'] == \"*\"):\n\t\t\treturn True\n\treturn False\n\ndef findIface(candidate, candidates = None):\n\t\"\"\"\n\tFind the iface with the same arguments as *candidate*\n\tgiven in the list of candidates (or the\n\tcandidates argument)\n\t\"\"\"\n\tif (candidates == None):\n\t\tcandidates = candidate\n\tfoundOne"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif iface is None:\n\t\t\ttry:\n\t\t\t\tiface = candidate.getProtocols().get(\"iface\")\n\t\t\texcept:\n\t\t\t\tiface = candidate.Interface\n\t\tif iface is None:\n\t\t\traise NotImplementedError(\"Interface %s was not found.\" % (candidate))\n\tif not tentative:\n\t\tif not iface.isEqualTo(iface):\t\t\t\t# assume class is present.\n\t\t\treturn 0\n\telif iface.getParent().isEqualTo(iface.getParent()): \t# FIXME?\n\t\tif not isinstance(iface, tentative):\n\t\t\t#"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tn = None\n\t\tif explicit_mc:\n\t\t\tif explicit_mc == 1:\n\t\t\t\tif classes.has_key(baseclass):\n\t\t\t\t\tn = classes[explicit_mc][0]\n\t\t\t\telse:\n\t\t\t\t\traise MissingBaseClassException('Singleton name of %s found' % baseclass)\n\t\t\telif explicit_mc > 1:\n\t\t\t\traise DuplicateMcException('Multimetaclassing for %s found' % baseclass)\n\t\telif len(bases) == 1:\n\t\t\tn = bases[0]\n\t\telse:\n\t\t\tmro = bases[1:]\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t# this is an OpaqueType, meaning this is an Object\n\t\tmetadict = dict(base.__dict__)\n\t\t# check if \"metaclass\" attribute (since it could be \"Meta\", but we assume it exists)\n\t\tif not metadict.has_key(\"metaclass\") and explicit_mc:\n\t\t\traise AssertionError(\"metaclass not given for first positional argument: %r\" % metadict)\n\t\telif explicit_mc:\n\t\t\tmetadict[\"metaclass\"] = getSubclassFromTypedMCFlags(mcaInternedTypes)\n\t\telse:\n\t\t\t# this shouldn't be reached"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif not explicit_mc:\n\t\t\texplicit_mc = []\n\t\tmro = dict(implicit_mc)\n\t\twhile len(bases) > 0:\n\t\t\tif isinstance(bases[0], Exception):\n\t\t\t\tbreak\n\t\t\tmro.update(getattr(bases[0], '__metaclass__', ()))\n\t\tif explicit_mc:\n\t\t\ttry: inherited_mcs = mro['__metaclass__']\n\t\t\texcept KeyError: inherited_mcs = []\n\t\t\tmro[inherited_mcs[0]]=inherited_mcs\n\tmro.update(explicit_mc)\n\treturn mro\n\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = bases[0]\n\t\timplicit_mc = bases[1]\n\t\tbases.append(explicit_mc)\n\t\treturn explicit_mc,implicit_mc\n\n\t# Determine metaclass from 2nd bases\n\tmro = getmro(bases[1]) # 2nd should be last to first\n\tfor base in bases[2:]:\n\t\t# if bases is None, there is no metaclass\n\t\tif not bases:\n\t\t\treturn base,\"\"\t# nothing special\n\telse:\n\t\t# no metaclass found\n\t\treturn base,"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tnew = None\n\t\texplicit_bases = bases.get(explicit_mc or (set(bases) if explicit_mc else None))\n\t\t\n\t\tif explicit_bases:\n\t\t\t# If we're explicitly defining the virtual abstract base class\n\t\t\t# inheritance, use that. We know that it is a direct base class\n\t\t\t# of the highest class we are checking deep inheritance.\n\t\t\tif explicit_mc: explicit_bases = explicit_bases - explicit_mc\n\n\t\t\tlastBases = bases[:]\n\t\t\tbases = explicit_bases\n\n\t\treturn lastBases\n\telse:\n\t\t# We have"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t    explicit_mc = {}\n\t\tif isinstance(bases, CTypes):\n\t\t    if bases._name.endswith(\"ABCMeta\"):\n\t\t        if metaclass(bases._name).find('Magic') == -1:\n\t\t            return bases._name\n\t\t    elif not metaclass(bases._name).find('MTAType'):\n\t\t        if metaclass(bases._name).find('__metaclass__') == -1 and explicit_mc.has_key('metaclass'):\n\t\t            if explicit_mc['metaclass'] is not None:\n\t\t\t    return explicit_mc['metaclass']\n\t\t    else:\n\t\t        if metaclass"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is not None:    return explicit_mc\n\t\treturn bases[0] \n\n\ndef isAnyBuiltinType(typeobj):\n\t\"\"\"\n\tCheck if given type or class has a PyTypeObject that is not the same class as\nthe corresponding type.\n\ttypeobj must be either of type, class or reference.\n\t\"\"\"\n\ttt = isInternalType(typeobj)\n\t# print 'isType(tt): typeobj is type:', tt, typeobj\n\tif not tt or type(tt)!= types.ClassType:\n\t    return 0\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tklass = 2\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = findClass(bases, 'explicit_mc')\n\t\tif explicit_mc is None:\n\t\t\t# special case: \"explicit_mc = get_explicit_mc({})\".format(bases)\n\t\t\tif len(bases) > 1 and any(a in bases for a in ['__metaclass__', '__module__']):\n\t\t\t\tklass = 1\n\t\treturn klass\n\n_type_list = []\n\ndef _type_tostr(type_):\n\tif isinstance(type_, list):\n\t\treturn _list_"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc:\n\t\t\tbases = resolve_mro(bases)\n\t\tmro = []\n\t\tfor base in bases:\n\t\t\tfor name, obj in inspect.getmembers(base):\n\t\t\t\tif name not in (\n\t\t\t\t\t\t'__metaclass__', '__class__', '__new__', '__prepare__',\n\t\t\t\t\t\t'__new__', '__prepare_ex__', '_prepare',\n\t\t\t\t\t\t'__reduce__', '__reduce_ex__', '_reduce', '__reduce_ex__',\n\t\t\t\t\t\t'__get__', '__getitem__', '__gt__', '__hash__', '__ge__',\n\t\t\t\t\t\t'__le__', '__lt__', '__ne__', '_le','send', '_"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t# We don't want to reassign these two lines, because then we can't\n\t\t# distinguish from one class having two inherited __init__ methods in the\n\t\t# C++ code.\n\n\t\t# For custom base classes we don't want __metaclass__ to be the same\n\t\t# as C++ to class.\n\n\t\t# Anything derived from that class is now \"used\" in cdef class\n\t\tseen = {}\n\t\talready_seen = {}\n\t\tdef scan(cdefs, lines):\n\t\t\tfor line in lines:\n\t\t\t\tif line.startswith('#define'):\n\t\t\t\t\tfound ="}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self.pop(key)\n\t\telse:\n\t\t\traise KeyError(key)\n\n\tdef popitem(self):\n\t    \"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\n\t\tUse this in conjunction with:\n\t\t\tfor k, v in D.items(): print k, v\n\t\t\"\"\"\n\t\tif len(self) == 0:\n\t\t\traise KeyError('popitem(): dictionary is empty')\n\t\tk = self.head"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\tv = default\n\t\telse:\n\t\t\tdel self[key]\n\t\treturn v\n\n\tdef popitem(self):\n\t\t\"\"\"\n        'D.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\tThe list is iterated though.\n\t\tUse pop() to remove values from D.\n\t\tThe tuple returned also is a key, value pair with"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv=self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise KeyError(key)\n\t\t\telse:\n\t\t\t\traise \n\t\telse:\n\t\t\treturn v\n\t\n\tdef setdefault(self, key, default=None):\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\t\n\tdef update(self, E, **F):\n\t\t\"\"\"\n\t\tupdate() replaces the existing dict.\n\t\tE is updated by the internal dictionary L{update} function, but \n\t\tin addition to the"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self.__getitem__(key)\n\t\treturn default\n\tpop.__doc__ = __str__\n\n\nclass DeepCopy(object):\n\t\"\"\"An object decorating a shallow copy of dict (or other dict-like)\n\n\tSimilar to Dict but provides a faster __deepcopy__, maintaining\n\tdeep copies of dict keys and their subvalues, this makes it possible to\n\treduce the overhead of __getstate__ and __setstate__ calls to __init__.\n\tThis way, d['key'] always returns a copy of dict (even for the value\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\t\treturn default\n\t\telif default == self.DefaultString:\n\t\t\treturn ''\n\t\telse:\n\t\t\traise KeyError('Key not found: %s' % key)\n\n\tdef Peek(self, k = None):\n\t\t\"\"\"D.peek(k = ['A','B','C','D'], default = 'A') -> v, remove and return the corresponding value.\tIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n\t\tif k in self:\n\t\t\treturn self[k]\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t    \n\t\ti = self.i\n\t\td = i[key] if key in i else default\n\t\tdel self.i[key]\n\t\treturn d\n\t\t\n\tdef setdefault(self, key, default=None):\n\t\tif key not in self.i:\t\n\t\t\treturn default\n\t\telse:\n\t\t\treturn self[key]\n\t\t\n\tdef popitem(self):\n\t\t# d = C.__getitem__(self, key[,d,default[,key]])\n\t\ti=self.i\n\t\tdel self.i[key]\n\t\treturn self.i,i\n\n\tdef"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tx = self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\telif default is self.__marker or default is KeyError:\n\t\t\t\treturn d.pop(key, None)\n\t\t\telse:\n\t\t\t\traise\n\t\telse:\n\t\t\tdel self[key]\n\t\t\treturn x\n\tpop.__doc__ = pop.__doc__\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tv = default\n\t\telse:\n\t\t\tdel self[key]\n\t\treturn v\n\tdef setdefault(self, key, x=None):\n\t\t\"\"\"\n\tE.setdefault(k[,d]) -> v, set D[k]=v if k not in D, else v.\tIf x is specified,\n\t\tsetdefault(k,d) is called on each key-value pair.\n\t\teg, x='a', raise KeyError if D.get(k,'') else KeyError.\t\"\"\"\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\treturn default\n\t\telse:\n\t\t\tdel self[key]\n\t\t\treturn value\n\t\t\n\n\nif __name__ == '__main__':\n\tv1 = Vocab()\n\tfor k in range(10):\n\t\tv1[0,k] = 'a'\n\t\n\tv2 = Vocab()\n\tfor k in range(10):\n\t\tv2[1,k] = 'A'\n\t\n\tfor k in range(len(v"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tv = default\n\t\telse:\n\t\t\tdel self[key]\n\t\treturn v\n\t\n\tdef popitem(self):\t\n\t\tv, remove = self.pop(0)\n\t\tdel self[remove]\n\t\treturn v\n\t\n\tdef get(self, k, d=None):\n\t\t\"\"\"\n\t\tE.get(k[,d]) -> v, remove and return some configuration\n\t\tvalues as a list. Returns None if d is specified, otherwise KeyError is raised.\n\t\t\n\t\t>>> G = ConfigObj"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\t(keyslot, val) = (self._keyslot, self._val)\n\t\tif kupfer_common.is_null(keyslot) or kupfer_common.is_null(val):\n\t\t\tself._keyslot, self._val = None, None\n\t\t\tself.changed()\n\t\telse:\n\t\t\treturn self._keyslot, val\n\n\tdef save(self):\n\t\t\"\"\"\n\t\tStore the Kupfer database from the database\n\t\t\"\"\"\n\t\tself.kupfer = Kupfer(self._filename, mode='w')\n\n\t\tif self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\ttry:\n\t\t\t(key, value) = self.pop()\n\t\t\treturn (key, value)\n\t\texcept KeyError:\n\t\t\treturn None, False\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\t\tCheck whether a key exists in `self`.\n\n\t\tUsage:\n\t\t\tobj.has_key(\"foo\") ==> 1\n\t\t\"\"\"\n\t\ttry:\n\t\t\tprev = None\n\t\t\tfor item in self:\n\t\t\t\tif item.has_key(key)!= 0:\n\t\t\t\t\tprev = (prev, item)\n\t\t\treturn prev\n\t\texcept KeyError:\n\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\twhile self.contents:\n\t\t\t(key, value) = os.path.split(self.contents.pop())\n\t\t\tif key.endswith(\"/\"):\n\t\t\t\ttry:\n\t\t\t\t\tdel self.contents[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpath = os.path.join(os.path.dirname(self.name), key)\n\t\t\t\tself.contents[key] = [os.path.join(path, x)\n\t\t\t\t\t\t   for x in os.listdir(path)\n\t\t\t\t\t\t   if x.endswith(self.extensions)]\n\n\tdef _check"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tk, v = [x.strip() for x in self.__dict__.items() if not x.startswith('_')]\n\t\tresult = self.pop(k)\n\t\tif not result:\n\t\t\tdel self[k]\n\t\t\treturn k, []\n\t\tresult = [(unicode(k), unicode(v)) for v in result]\n\t\tdel self[k]\n\t\treturn result\n\n\tdef keys(self):\t\treturn self.__dict__.keys()\n\n\tdef values(self):\treturn [x[1] for x in self.keys()]\n\n\tdef values_dict("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\titem = self.popitem()\n\t\treturn item[1], item[0]\n\n\tdef setdefault(self, key, default=None):\n\t\t'''\n\t\tReturn the value of `key` from the dict. If the key is not present, \n\t\tadd it and return it as the value. \n\t\t'''\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef update(self, other=None, **newdict):\n\t\t'''\n\t\tUpdate all attributes with the supplied kwargs.\n\t\tIf an option is passed in the constructor"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\twith self._lock:\n\t\t\ttry:\n\t\t\t\twhile 1:\n\t\t\t\t\tk, v = self._queue[0]\n\t\t\t\t\tself._queue.popleft()\n\t\t\t\t\tif k == key:\n\t\t\t\t\t\tdel self[v]\n\t\t\t\t\t\treturn k, v\n\t\t\t\t\t\tbreak\n\t\t\texcept IndexError:\n\t\t\t\treturn None, None\n\n\tdef __str__(self):\n\t\twith self._lock:\n\t\t\tret = ''\n\t\t\tfor k, v in self._queue:\n\t\t\t\tret += str(k) + ':'+ str(v) + '\\n'\n\t\t\treturn ret.rstrip()\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif len(self.data) < 2:\n\t\t\tprint \"This operation requires 2 items: '%s'\"%self.data[0]\n\t\t\treturn\n\n\t\tk, v = self.pop(0)\n\t\tif k == v:\n\t\t\treturn\n\n\t\tself.data.pop(0)\n\t\tif k!= v:\n\t\t\tself.k += 1\n\t\t\tprint \"Popping '%s'\"%k\n\n#---------------------------------\n# Private methods\n#---------------------------------\n\n    def __str__(self):\n\treturn \"SortedList([%s])\" %\", \".join(map"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tself.redis.zrem(\"zrem\", self.name)\n\t\titem = self.redis.hgetall(\"zrem\")\n\t\tself.redis.zrem(\"zadd\", item[0])\n\t\treturn item\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tfor key_value in reversed(self.lookupdict.keys()):\n\t\t\tif self.lookupdict.has_key(key_value):\n\t\t\t\tvalue = self.lookupdict[key_value]\n\t\t\t\tself.lookupdict[key_value] = None\n\t\t\t\tself.lookupdict.pop(key_value)\n\t\t\telse:\n\t\t\t\traise KeyError, \"There is no item %s in the dict, popping it and removing it from the dictionary\" % key_value\n\t\tfor key, value in self.lookupdict.items():\n\t\t\tif value == self.lookupdict["}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn (self.pop(key, {}), ())[1:2]\n\n\t#=============================================================================\n\tdef update(self, other):\n\t\t\"\"\"\n\t\tConvenience method that applies the update functionality to the items of the\n\t\tlist or dictionary. Returns (key, value) pairs which matched:\n\n\t\t* If key is not present, this raises an exception.\n\t\t* If the list contains keys that are not in the list, that the items are removed,\n\t\tand the items removed are not allowed.\n\t\t* Else if key is present in more than one entry, there are no keys left to"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif len(self) <= 1:\n\t\t\traise KeyError, 'popitem(): dictionary is empty'\n\t\tmissing, value = self.popitem()\n\t\tif missing is EXCEPTION_DICT:\n\t\t\traise KeyError, \\\n\t\t\t\t'popitem(): dictionary is missing an item: %s' % value\n\t\treturn value\n\n\tdef pop(self, key, *default):\n\t\t\"\"\"\n\t\tRemove and return the most recent value for the `key` attribute.\tIf no\n\t\tvalue was found, raise a `KeyError`. If `default` is a list, return the\n\t\tdefault values instead"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\twhile self.get() is not self.END:\n\t\t\tm = self.get()\n\t\t\tkey, value = m\n\t\t\tdel m\n\t\t\tvalue = self._itemgetter(key)\n\t\treturn key, value\n\n\tdef _iterators(self):\t\t# Needed for testability\n\t\twhile 1:\n\t\t\tkey = self.get()\n\t\t\tif key is StopIteration:\n\t\t\t\traise StopIteration\n\t\t\tfor k, v in self._mapping.iteritems():\n\t\t\t\tyield k, v\n\n\nclass ConfigBase:\n\t\"\"\"\n\tA mixin that implements common config handling with the core"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\t(key, value) = popitem(self)\n\t\treturn key, value\n\n\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\tReturn a list of keys in the set.\n\n\t>>> d1 = {'x': 1, 'y': 2}\n\t>>> len(d1)\n\t1\n\n\t x in d1\n\t y\n\n\t x in 1\n\n\t y in 2\n\t\"\"\"\n\t\treturn self.get(key) or []\n\n\n\tdef iterkeys(self, *args, **kwargs):\n\t\t\"\"\"\n\tReturn an iterator over the keys of the set as a"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\ttry:\n\t\t\t(key, value) = self[self.current()]\n\t\texcept KeyError:\n\t\t\traise KeyError(\"No key available\")\n\n\t\tif self.sorted_keys is None:\n\t\t\tsorted_keys = range(len(self.recent_keys))\n\t\t\tself._sort()\n\t\telse:\n\t\t\tsorted_values = [entry[1] for entry in self.sorted_entries]\n\n\t\t\tfor key in sorted_keys:\n\t\t\t\t# Note that we can use sorted_values (since the values are\n\t\t\t\t# kept sorted in the order they were recorded, which is why"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.data.popitem(last=False)\n\n\tdef setdefault(self, item, *args, **kwargs):\n\t\tself.data.__setitem__(self, item, *args, **kwargs)\n\t\treturn self.__getitem__(item)\n\n\tdef setvalue(self, item, value):\n\t\tif self.getdefault(item) == value:\n\t\t\treturn\n\t\telse:\n\t\t\traise SettingAlreadySetException(\"The item %r is already set to value %r\" % (item, value))\n\n\tdef delitem(self, item):\n\t\tif self.get"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif self._k == 0:\n\t\t\traise KeyError, \"popitem(): dictionary is empty\"\n\t\tkey = self._i.pop()\n\t\tdel self._i[key]\n\t\treturn key, self._i\n\n\tdef add(self, key, value):\n\t\tkey = self.insert(key, value)\n\t\tself._cache.append(key)\n\n\tdef extend(self, other):\n\t\ti = other\n\t\twhile i is not self._i:\n\t\t\tself.put(i[0], i[1])\n\t\t\ti = i[1]\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self._itempopleft()[2]\n\n\tdef pop(self):\t\"\"\"\n\tPop the next topmost item off the stack (not the topmost).\n\t\"\"\"\n\t\treturn self._itempop()\n\n\tdef peek(self):\n\t\t\"\"\"\n\t\tReturn the last item on the stack of items (not the topmost).\n\t\t\"\"\"\n\t\treturn self._itempop(1)[0]\n\n\tdef move(self, after=True):\n\t\t\"\"\"\n\t\tMove the current frame one level up one level above the topmost item. \n\t\tIf `after` is set"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tfor _, value in self._items:\n\t\t\tyield value\n\n\tdef _keys(self):\t\t\t\t# pragma: no cover\n\t\tkeys = list(self._keys_queue)\n\t\tk = next(k for k, v in self._items)\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tval = self._popitem()\n\t\texcept KeyError as e:\n\t\t\tif e.args[0] == EAGAIN:\n\t\t\t\t# this occurs when an expired key is passed to popitem()\n\t\t\t\t# and the queue is empty\n\t\t\t\tself.clear()\n\t\t\t\traise KeyError"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.datamap.pop()\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return the result (`value`, `remaining`) pair least recently used.\n\t\t\"\"\"\n\t\tvalue = self.datamap.popitem(last=False)\n\t\tself._value = value\n\t\treturn value\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemove item from `self.datamap`. `key` is expected to be a tuple:\n\t\t * The key corresponding to deleted value. If deleted, ``KeyError`` is raised\n\t\t * if no value with that key is found."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.dict.popitem(last=False)[0]\n\n\tdef close(self):\treturn self.dict = dict()\n\t\t\t\ndef get(path):\n\treturn PeekingKey(path)\n\nclass PeekingKeyException(Exception):\n\tdef __init__(self, key):\n\t\tself.args = (key,)\n\t\tself.key = str(key)\n\t\tself.args = '.'.join(repr(arg) for arg in self.args)\n\t\t\n\tdef __str__(self):\n\t\treturn repr(self.args)\n\t\n\t@property\n"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tkey, value = self.pop(0)\n\t\tself.cache[key] = value\n\t\treturn key, value\n\t\t\n\t# Get an item by key.\n\tdef getitem(self, key):\n\t\t\"\"\"\n\t\tReturn a (key, value) pair from the cache, or None\n\t\tif it doesn't exist.\n\t\t\"\"\"\n\t\tvalue = self.cache[key]\n\t\tif value is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn tuple(value)\n\t\n\t# Get all of a (key, value) pairs.\n\tdef values("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif len(self) > 0:\n\t\t\treturn self[0]\n\t\telse:\n\t\t\traise IndexError('pop from an empty list')\n\n\tdef pop(self, key):\n\t\t\"\"\"\n\tRemove and return a representation of the form::\n\n\t{'key':(k, v)}\n\t\n\tKeys must be strings.\n\t\"\"\"\n\t\tfor (i, v) in enumerate(self):\n\t\t\tif v[0] == key:\n\t\t\t\treturn ((key, v[1]))\n\t\traise IndexError('pop from an empty list')\n\n\tdef iteritems(self):\n\t\tfor"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tassert not self.isclosed()\n\t\titem = self.items.pop(-1)\n\t\tself.closeall()\n\t\treturn (item[0], item[1][0])\n\n\t##\n\tdef isclosed(self):\treturn self.closed\n\n##\n# Return a partial ordered (mapping) of all items in the map,\n# sorted by key\n\ndef sorted(map):\n\tif map:\n\t\treturn list(map.items())\n\telse:\treturn []\n\n# List of pairs with the most recently used keys (the last element\n# is"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.cache[self._fkey(self._k)]\n\t\texcept KeyError:\n\t\t\treturn (None, None)\n\n\nclass LRUCache:\n\t\"\"\"A cache which is limited to only LRU entries (in RAM).\n\t\n\t:param capacity: The maximum number of entries in the cache.\n\t:type capacity: int\n\t:param minvalue: The minimum value.\n\t:type minvalue: int\n\t\"\"\"\n\t__slots__ = ('capacity','minvalue', '_capacity')\n\n\tdef __init__(self, capacity, minvalue"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\t# See if we've recently popped the key/value pair\n\t\td = self.data\n\t\tkey, val = d.popitem()\n\t\tdel d[key]\n\t\td['@'] = val\n\t\treturn key, d\n\n\tdef setdefault(self, key, d=None):\n\t\tif key not in self:\n\t\t\tself[key] = d\n\t\treturn self[key]\n\n\tdef update(self, other):\n\t\t\"\"\"\n\t\tSet this configuration from a dictionary, mapping keys to values.\n\t\tSome wrinkles may use this if they wish. ("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tassert self._last_item is not _empty\n\t\tdata = self._list_to_data(self._last_item)\n\t\tif not data:\n\t\t\treturn (\"\", None)\n\t\tlast_item = data[-1][-1]\n\t\tself._shift_out_of_list(data, last_item)\n\t\tlast_item = data[0][-1]\n\t\treturn (\"\", last_item)\n\tpopitem = popitem\n\n\tdef pop(self, last=True):\n\t\tkey = self._last_item + 1\n\t\treturn tuple(self[key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self.cache[self.first()]\n\t\texcept KeyError:\n\t\t\t# no more items to pop\n\t\t\traise KeyError()\n\t\tdel self.first(), self.key(), self.value\n\t\treturn key, value\n\tpopitem = popitem\n\n\tdef __contains__(self, key):\n\t\treturn key in self.cache\n\n\t# let an item be popped off the end of the list\n\tdef __delitem__(self, key):\n\t\tself.cache.remove(self.first())\n\t\twhile len(self.cache) > key:\n\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\tk, v = self.cache[0]\n\t\texcept KeyError:\n\t\t\traise KeyError(\"The cache is empty\")\n\t\tdel self.cache[0]\n\t\tdel self.cache[0]\n\t\tlst = self.cache[1]\n\t\tdel self.cache[1]\n\t\tif lst is key:\n\t\t\tlst = self.cache[0]\n\t\treturn k, lst\n\t# ----------------------------------------------------------------------\n\tdef setdefault(self, key, failobj = None):\n\t\t\"\"\"\n\t\tReimplement setdefault to replace the given `(key, value)` pair from"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\ttry:\n\t\t\tkey, value = self.idict.popitem(last=False)\n\t\texcept KeyError:\n\t\t\traise KeyError(repr(key))\n\t\tself.idict.pop((key, value))\n\t\treturn key, value\n\tpopitem.__safe_for_unpickling__ = True\n\tdef update(self, o):\treturn self.idict.update(o)\n\tupdate.__safe_for_unpickling__ = True\n\ndef get_app():\n\tglobal app\n\tif not app:\n\t\timport app\n\treturn app\n\n\napp ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\titems = self.data.pop(0, END+1)\n\t\tif len(items) == 0:\n\t\t\t# only return the last element\n\t\t\treturn None\n\t\tval, data = items[0]\n\t\tdel data[KEY_TERMINATOR]\n\t\tdel items[0]\n\t\treturn (val, data)\n\n# A little test suite to see whether repeating works and whether two iterators \n# are the same sequence even if a loop really takes a long time.\n\nclass TestBasicIterators(unittest.TestCase):\n\n\tdef test_basic(self):\n\t"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\titem = self.random\n\t\tself[item[0]] = (key for key, value in self.items() if item[0] <> item[1])\n\t\treturn item\n\t\t\n\tdef random(self, key):\n\t\t\"\"\"\n\t\tRandomly select a item from the store.\t\t\n\t\t\"\"\"\n\t\tif not(self.items() == []):\n\t\t\treturn (random(), key)\n\t\telse:\n\t\t\titems = (k for k, v in self.items() if v == key)\n\t\t\treturn random(), items[-1][1]\n\t\t\n# -----------------------------\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn self._data[self._pos]\n\t# -------------------------------------------------------------------------\n\t# Iterator protocol\n\t# -------------------------------------------------------------------------\n\tdef __next__(self):\n\t\twhile 1:\n\t\t\ttry:\n\t\t\t\tkey, value = self._data.popitem()\n\t\t\t\tself._pos += 1\n\t\t\texcept IndexError:\n\t\t\t\traise StopIteration()\n\t\t\tif isinstance(key, int):\n\t\t\t\treturn (key, value)\n\t\t\telse:\n\t\t\t\traise TypeError('invalid index or slice')\n\n\tdef next(self):\n\t\tif self._pos >= len(self._data):\n\t\t\traise StopIteration()\n\t\ti"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tassert self.item is not None and self.item_count is not None\n\t\tx, i = random.sample(self.item, 1)\n\t\tself.counter -= 1\n\t\tself.item = None\n\t\tself.item_count -= 1\n\t\tself._random = x\n\t\treturn (x, i)\n\n\tdef __str__(self):\n\t\tx, i = self.random\n\t\tp = self.counter - 1\n\t\tif self.item_count:\n\t\t\tp += 1\n\t\treturn \"Element \" + str(item) + \": (\" + str("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\twhile self.is_empty():\n\t\t\trand = randrange(0, 0xff)\n\t\t\tkey = rand.next()\n\t\t\tkey_part = key.random_string(16)\n\t\t\tvalue = rand.next()\n\t\t\tif self.is_empty():\n\t\t\t\treturn ((key_part + key_part).encode(\"utf-8\"), value.encode(\"utf-8\"))\n\t\t\tself.redis.lpush(key_part, value)\n\t\t\treturn (key_part, value)\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\tSet and"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\titem = random.randint(0, len(self))\n\t\tself = self[item]\n\t\tself[self.index] = item\n\n\tdef shuffle(self):\n\t\trandom.shuffle(self)\n\t\tself.index = random.choice(range(len(self)))\n\n\tdef splitlines(self, lines, split_on=1):\n\t\t\"\"\"\n\t\tSplit string into substrings using lines separated with spaces\n\t\taround the words contained in the text.\n\t\t\"\"\"\n\t\tnew_lines = []\n\t\tif split_on:\n\t\t\tfor i, l in enumerate"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tpair = self.left.random.pop(self.key, self.key.next())\n\t\tt, f, s = pair\n\t\tself.set_result(t, f, s)\n\t\treturn s, pair\n\t\t\n\tdef shuffle(self):\n\t\t\"\"\"Shuffle the set.\"\"\"\n\t\twhile 1:\n\t\t\ttry: self.left.shuffle_left()\n\t\t\texcept IndexError: pass\n\t\t\texcept AttributeError: break\n\t\t\telse: break\n\t\t\n\n\t\nclass RandomQueue(Queue):\n\tdef __init__(self, *args, **kwargs):\n\t\tQueue.__init__(self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\twith self.lock:\n\t\t\tvalue = self.random.pop(0)\n\t\t\tself.data[key].clear()\n\t\t\tself.data[key] = value\n\t\t\treturn value\n\n\tdef random_choice(self, choices=None, n=1):\n\t\t\"\"\"\n\t\tReturn a random `(key, value)` and put that value in `choices`.\n\t\t\"\"\"\n\t\tif choices is None:\n\t\t\tchoices = self.data.keys()\n\n\t\tself.data = {}\n\t\tself.data_size = sum(len(v) for v in choices)\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn (self.random.pop(), self.random.pop())\n\n\t#\n\t# Randomize\n\t#\n\n\tdef shuffle (self, seed=None, size=None):\t\t\t# __doc__\n\t\t\"\"\"\n\t\tRandomize the item list; by calling the appropriate method\n\t\tto produce the random number of each item.\t\n\t\t\"\"\"\n\t\tif seed is not None:\n\t\t\trandom.seed (seed)\n\n\t\tif size is not None:\n\t\t\tassert (size > 0 and size <= len(self))\n\t\t\tself.random.shuffle (size, random."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn pop(self.items(), 1)\n\n\tdef clear(self):\t\t\"\"\"\n\tReturn items currently in the set.\n\t:return: a set of {key, value} items.\n\t\"\"\"\n\treturn self[0]\n\n\nimport random\nimport os, tempfile\nimport shutil\n\n# TODO: add support for Ramdisk stuff here.\nclass RamdiskFile(file):\n\t\"\"\"Implements the Ramdisk file object.\n\t:param fname: name of file\n\t:param chunk: size of read chunk\n\t:raise IOError: open error\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\t# TODO: use bisect method.\n\t\tfor i in range(self._rand):\n\t\t\tkey = random.choice(self._rnd)\n\t\t\tself._rnd[key] = random.choice(self._items)\n\n\tdef removeitem(self, key):\n\t\t\"\"\"\n\t\tRemoves and returns an `(key, value)` pair.\t\"\"\"\n\t\tassert len(key) > 0\n\t\tassert self._sorted is not None\n\n\t\tfor key in self._rng[key]:\n\t\t\tself._items.remove(key)\n\t\tself._rng.pop(key, None)\n\n\t"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self._in_regex)\n\n\tdef _create_is_regex(self) -> Pattern:\t\"\"\"\n\tCreate the is-style parameter regular expression.\n\nReturns the is-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self._is_regex)\n\n\tdef _create_as_regex(self) -> Pattern:\t\"\"\"\n\tCreate the as-style parameter regular expression.\n\nReturns the as-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self._"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.regex\n\t@classmethod\n\tdef _create_regex_rexp (cls) -> Pattern:\treturn re.compile(':(?:\\\\s*)(\\w+)')\n\t@classmethod\n\tdef _create_reg_ex_rexp (cls) -> Pattern:\treturn re.compile('.*:(.*)::(.*)')\n\t@classmethod\n\tdef _create_sub_rexp (cls) -> Pattern:\treturn re.compile(r'\\b(?:[\\w\\-])({(?:.*?)|(?:\\\\s*}(?:\\\\s*)?)?\\)$')\n\t@"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t return self._regex_in_definition\n\n\t@property\n\tdef template_path(_self) -> str:\n\t\t\"\"\"\n\t\tThe path containing default template file for the input parameter.\n\n\t\t:rtype: str\n\t\t\"\"\"\n\t\treturn self._template_path\n\n\tdef __init_docstring__(self) -> Pattern:\n\n\t\tdefault_template_args = {\n\t\t\t\"default\": { \"required\": True },\n\t\t}\n\t\tdefault_template_kwargs = {\n\t\t\t\"required\": False,\n\t\t}\n\n\t\treturn super().__init_docstring__(default_template_args, default"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern(type=\"in-style\")\n\n\tdef __repr__(self):\n\t\treturn \"<re.Pattern instance>\"\n\n\tdef _create_re_string(self) -> Pattern:\t\"\"\"\n\tCreate the re-style parameter regular expression that matches *exactly* to any character.\n\n\tParameters\n\t----------\n\tpattern : a re.Pattern instance or None\n\t\t\tPattern in which to create an appropriate *exactly* re pattern instance.\n\t\"\"\"\n\t\tif pattern is not None:\n\t\t\tself.pattern = pattern\n\n\tdef __str__(self):\n\t\tif self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self.pattern, self.re_in_string)\n\tre_in_string = _create_in_regex\n\nclass _Pattern:\t\"\"\"\n\tPattern class that searches the same pattern to find in the string.\n\t\"\"\"\n\tdef __init__(self, pattern, flags=0):\n\t\t\"\"\"\n\t\tCreate an instance.\n\n\t\t:param pattern:\tThe pattern regex to search for in the parameter.\n\t\t:type  pattern:\tobject or Pattern\n\t\t:param flags:\tThe replacement flags for this part of the pattern.\n\t\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self._pattern\n\n\t# private:\n\n\tdef _escape_regex(self, regex: str) -> str:\n\t\t\"\"\"\n\t\tEscape the given regular expression.\n\n\t\tArguments:\n\t\t    regex (str): A regular expression to escape.\n\n\t\tReturns:\n\t\t    str: Escaped Regular expression.\n\t\t\"\"\"\n\t\t# We want to escape the backslashes as \\, or \\\\.\n\t\tif not regex.startswith(\"\\\\\"):\n\t\t\tregex = \"/\" + regex\n\t\tregex = re.escape(regex)\n\t\treturn regex\n\n\t# -------------------------------------------------------------------------\n\n\tdef _pattern_to_reg(self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn  \"(?:(%s)|(.)??)\" % \"|\".join(self._escaped_strings)\n\nclass _In_PatternParser(object):\n\t\"\"\"\n\tThis class represents the PatternParser. It handles the in- and\n-regex patterns.\n\n>>> f = Pattern()\n>>> _, patterns, _ = _Create_pattern_parser('x')\n>>> y = Pattern(f.compile(r'(\\|)')).parse('The\\s*a')\n>>> print(y)\n'The the.x'\n\n>>> _, patterns, _ = _Create_pattern"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(_IN_NAMED)\n\n\tdef _create_in_var_name(self, vname, n_to_name):\n\t\t\"\"\"\n\t\tCreate regular expression using given normal variable names with given vname prefix.\n\t\tIf the parameter name is a regex, parameter string pattern is compiled to a regular expression pattern.\n\t\tIf vname is a glob-string containing named patterns, parameter string pattern is compiled to a regular expression pattern.\n\t\tIf vname is a glob-string containing named groups, parameter string pattern is compiled to a regular expression pattern.\n\t\tIf a"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn _InStyleRegex(self.pattern, r\"\\#\\d+(?=:)\")\n\nclass _InStyleRegex(Pattern):\n\t\"\"\"\n\tRegular expression pattern.\n\n\tUsed internally by\n\t:class:`Pattern <_InStyleRegex>`.\n\n\t:param compiled: The re compiled regular expression pattern (if any).\t\n\t:param compiled_re: The re compiled regular expression pattern (if any).\t\n\t:param **kwargs: Keyword parameters to pass to :class:`re.compile()`.\n\n\t\"\"\"\n\n\tdef __init__(self, compiled=False, compiled_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self._string, re.IGNORECASE | re.VERBOSE)\n\n\tdef _validate(self, value: str) -> None:\n\t\t\"\"\"\n\t\tValidates and returns a pattern matching the given value (see :func:`re.match`) or None of return value :class:`re.MatchObject`.\n\t\tRaises a :class:`~re.error.ParseFatalException` on parsing error.\n\n\t\t.. note::\n\t\t    This will compile a regular expression directly into\n\t\t    a Python regular expression by re.sub(). If an incorrect\n\t\t    expression is given, then a :class:`re."}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tconverter =  _converting._Converter()\n\t\t# FIXME: This doesn't support custom parameter names.\n\t\tconverter.add_parameter_style( \"parameter_style\",  \"css-color\", self._parameter_style, 1 )\n\t\tconverter.add_format_parameter( \"format_expression\",\t\tself._format_expression,\tself._parameter_style, 1 )\n\t\tconverter.add_format_parameter( \"fill_background\",\t\tself._fill_background,\tself._parameter_style, 1 )\n\t\treturn converter\n\n\tdef _set_parameter_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef convert(self, value: str, params: _converting._Converter) -> _converting._Converter:\n\t\t\"\"\"\n\t\tConverts a parameter style value to the appropriate kind of\n\t\tconverter/conversions.\n\n\t\tSome of the parameters can be either :class:`str` (case-sensitive\n\t\tstrings, where the key indicates the format of a single parameter value\n\t\t(and therefore a list of values separated by spaces) or :class:`list` or\n\t\t:class:`tuple`, whither it is a list containing values which must be strings.\n\t\tThis is"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn self._conversion\n\n# =============================================================================\n#\n#                           Default converter class.\n# =============================================================================\n#\nclass ScalingConverter(object):\n\t\"\"\"\n\tThe default converter.\n\n\tThis object maps to an element in the input source using the\n\t:class:`~skrf.data.DataSource` to convert the values in the source into the\n\ttarget using the :class:`~skrf.data.DataSink`.\n\n\tThis converter also allows for converting values\n\tdirectly into numerical values.\n\n\t\"\"\"\n\t\n\tdef __init__(self, source) -> None:\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\ttry: return self._converting\n\t\texcept AttributeError:\n\t\t\tpass # no converter registered\n\n\tdef __repr__(self):\n\t\treturn \"%s()\" % self.__class__.__name__ # create the representation\n\n\t@staticmethod\n\tdef _format_expr(expr):\n\t\tformat_str = \"%s\"\n\n\t\tif expr == MathExpression.TRUE: format_str += \" && <!\"\n\t\telif expr == MathExpression.FALSE: format_str += \"=\"\n\t\telif expr is None:\n\t\t\tformat_str += \"NULL\"\n\t\telif isinstance(expr, bool):\n\t\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter()\n\t\t\n\tdef _get_converter(self) -> _converting._Converter:\t\"\"\"\n\tGet the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _conversion._Converter()\n\t\t\n\tdef _check_input(self, parameter, name):\n\t\tif parameter.name == name: return parameter\n\t\t\n\t\t# If the parameter is not in the current instance, it can't be used\n\t\t# for further parameters.\n\t\tif parameter.get_type()!= self._parameter_type:\n\t\t\traise ValueError(\"The parameter %s"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter\n\"\"\"\nTest for the Smart Verification Rules.\n\"\"\"\nimport json\nimport re\nfrom ddt import ddt, unpack\nfrom django.test import TestCase\n\nfrom lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationCheckpoint\nfrom student.tests.factories import UserFactory\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\nfrom xmodule.modulestore.tests.factories import CourseFactory\n\nfrom verify_student.tests.test_models import FormFieldDataCache\n\nMOCK_TOTAL_NUM_PASSES = 14\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\n\t\n\ndef _update_converter_helper(converter, converter_id, add_new_parameter_converter, remove_new_parameter_converter):\n\t\n\t\"\"\"\n\tUpdate the formatter for a converter.\n\t\n\tArguments:\n\t    converter : Converter.\n\t    converter_id : str\n\t    add_new_parameter_converter : bool\n\t    remove_new_parameter_converter : bool\n\t\t\t \n\t\"\"\"\n\t\n\tconverter.set_converter(converter_id, converter)\n\t\n\tif add_new_parameter_converter:\n\t\tconverter.add_converter(model_parameter_converter)\n\t\tconverter.set"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tconverter = self._get_converter()\n\t\treturn converter\n\n\tdef _get_converter(self) -> _converting._Converter:\n\t\t\"\"\"\n\t\t:return: `Converting` instance.\n\t\t:rtype: :class:`._converting._Converter`\n\t\t\"\"\"\n\t\tconverter = None\n\t\tfor attr, value in self.attrib.items():\n\t\t\t# Check if value is not None and converts to correct type\n\t\t\tif type(value) is _converting.Converting:\n\t\t\t\tvalue.update(convert(converter, attr))\n\t\t\telif type(value) in"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tpass\n\t\traise NotImplementedError()\n\n@typecheck(int, (int, int), (float, float))\ndef _convert(self, value: _converting._Converter, value: _converting._Converting) -> _converting._Result:\n\t\"\"\"\n\tConvenience function for ``self._convert``.\n\n\tReturns the converter (:class:`._converting._Converter`) instance. This method\n\tcan be called to wrap parameters: `value`, `unit`, `value_base`, `unit_base`.\n\t\"\"\"\n\tif isinstance(value, (int, float)):"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter(self._value)\n\n\tdef _validate(self, value, name, value_type, value_format):\n\t\tif value_format and not self.validate(value, value_format):\n\t\t\tself.error(name, \"value '%s' is not in %s unit, '%s'\", value, self._value, value_format)\n\n\tdef info(self, name, description, valid=None):\n\t\t\"\"\"\n\tReturn information about the format and its definition. Details can be set\n\tto a value of the `name`, the `"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 22.16 GiB total capacity; 20.56 GiB already allocated; 1.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\tif default is not None:\n\t\t\traise ValueError(\"default and tzinfo are mutually exclusive\")\n\n\t\ttry:\n\t\t\tresult = _parse(timestr, **kwargs)\n\t\texcept (TypeError, ValueError) as e:\n\t\t\traise ParserError(e)\n\n\t\tresult_cstime = _result_datetime(result, default)\n\t\tresult_explicit = _result_datetime(result.effective_earliest, result.tzinfo)\n\t\tresult_microsecond = _result_datetime(result.microsecond, result.tzinfo)\n\n\t\t# we know the base date is in seconds (and a tzinfo object"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# (begin, end) = _parse_time(timestr)\n\t\t# return datetime.datetime(\n\t\t#\tyear=today.year, month=today.month, day=today.day,\n\t\t#\thour=today.hour, minute=today.minute, second=today.second,\n\t\t#\tmicrosecond=today.microsecond, tzinfo=tzinfos)\n\n\t\t# Use keyword_groups rather than this, as it is case sensitive.\n\t\tlocale.setlocale(locale.LC_ALL, DEFAULT_LOCALE)\n\n\t\t# if fuzzy_with_tokens is False"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        if isinstance(timestr, (int, float)):\n\t\t\tif datetime:\n\t\t\t\tresult, t = _parse_datetime(timestr, *self._parse_time_zone(timestr))\n\t\t\t\tif result is None:\n\t\t\t\t\t\traise ValueError(timestr)\n\t\t\telse:\n\t\t\t\tresult = self._parse_time_zone(timestr)\n\t\telif isinstance(timestr, (str, unicode)):\n#\t\t\traise TypeError(timestr)\n\t\t\tresult = default\n\t\telif hasattr(timestr, '__call__'):\n\t\t\tresult = self._parse_time_zone("}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    if fuzzy_with_tokens:\n        if \"fuzzy\" in kwargs:\n\t\t\tkwargs[\"fuzzy\"] = True\n\t\telse:\n\t\t\tkwargs[\"fuzzy\"] = False\n\n\t\tif \"tzinfo\" in kwargs and kwargs[\"tzinfo\"]:\n\t\t\tkwargs[\"tzinfo\"] = kwargs[\"tzinfo\"]\n\t\ttzinfo_class = kwargs.get(\"tzinfo_class\", tzinfo)\n\n\t\tif tzinfo_class is not None:\n\t\t\tif tzinfo_class not in _tzinfo_cache:\n\t\t\t\ttzinfo_class = tzinfo_class\n\t\t\telif \"tzoffset\" in kwargs or \"tzfileloc\" in kwargs:\n\t\t\t\t"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\traise ParserError('%r is not a valid date' % (timestr,))\n\n# classes\n\n\nclass _resultbase(object):\n\t\"\"\"Internal helper object for the parse function.\"\"\"\n\t__slots__ = ()\n\n\tdef __init__(self, value):\n\t\tself.value = value\n\tdef encode(self, *args, **kwargs):\n\t\treturn self.value\n\tdef result(self):\n\t\treturn self.parse()\n\tdef escape(self, *args, **kwargs):\n\t\treturn self.escape_from_string(*args, **kwargs)\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\nif __name__ == '__main__':\n    import doctest, calendar\n    if SKIP_TESTS:\n        doctest.testmod()\n        raise SystemExit(0)\n\n    # $Id: test_parse.py,v 1.4 2005/03/28 18:13:02 agrim Exp $\n    import sys\n    if (sys.version_info >= (2,3) or\n        (not hasattr(sys.stdout, 'buffer') or sys.stdout.isatty())):\n        print >> sys.stderr, \\\n            \"Using Python 2.3 or later is recommended.\\n\"\n        sys.exit(1)\n\n    UTC"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# Use the explicit UTC class in dateutil.parser.\n\treturn _parse(self, timestr, **kwargs)\n\n\nif __name__ == \"__main__\":\n\timport doctest, random, datetime\n\tfrom dateutil import parser\n\n\t_doctests = doctest.testmod(sys.modules[__name__])\n\n\t_test_classes = (parse, datetime, RegexpTest, UTCTimeZoneDiffTest)\n\t_test_classes_for_parse_text = dict( (_cls.__name__, cls)\n\t\tfor _cls in _doctests[_test_classes]\n\t"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    timestr, tzinfos = self._normalized(timestr, tzinfos)\n    return _parse(timestr, default, ignoretz, tzinfos)\n\n\ndef parsefstring(timestr, default=_unparse_tzinfos):\n    r\"\"\"\n    Same as :func:`parse`, but constructs a :class:`datetime.datetime`\n    object from the given string formatted using the rules defined by\n    the class attributes, e.g., month names. Optionally it can take\n    either a string or a sequence of strings and return a tuple of\n    strings. Use the module-level constant ``strptime`` for more"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    if isinstance(timestr, datetime.datetime):\n        dt = datetime.date.fromordinal(1 + int(timestr.strftime(\"%Y%m%d%H%M%S\")))\n        if dt.tzinfo is None:\n            dt = datetime.datetime.fromtimestamp(timestr)\n    else:\n        dt = parse(timestr, fuzzy_with_tokens=False, tzinfos=tzinfos)\n    return dt.replace(tzinfo=tzinfo)\n\ndef parsedate(datestr, fuzzy=True, tzinfos=None, \n             ignoretz=False, tzinfos_set=None"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n\n    if not isinstance(timestr, _compat_str):\n        raise TypeError(\"timestr must be a string\")\n\n    m = _PARSE_DATE_RE.match(timestr)\n    if not m:\n        raise ValueError(\"timestr does not match format: %s\" % repr(timestr))\n\n    try:\n        year = int(m.group('year'))\n        month = int(m.group('month'))\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        minute = int(m.group('minute'))\n        second = int(m"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._fromutc(dt).replace(tzinfo=None)\n\t\texcept self._tsutil.tsutil.IncompatibleDatetime:\n\t\t\treturn None\n\n\tdef _fromutc(self, dt):\n\t\traise NotImplementedError('fromutc must be implemented in subclass')\n\n\tdef gmtinfo(self):\n\t\t\"Return this time in the Gmt Format\"\n\t\treturn self.utctzinfo()\n\n\tdef utcoffset(self):\n\t\t\"Return this timezone's offset from UTC\"\n\t\treturn self._fromutcoffset(0)\n\n\tdef dst(self"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tresult = dt.replace(tzinfo=naive)\n\n\treturn result.astimezone(local_tz)\n\n\n\t# -------------\n\t# Convert to a string.\n\t# -------------\n\tdef __str__(self):\n\t\treturn self.isoformat() + \" \" + \\\n\t\t\tself.iso8601()\n\n\n# -------------\n# A Date object used to represent an ISO 8601 string.\n# ------------------\n\nclass Date (object):\n\t\"\"\"\n\tRepresents a :class:`datetime.datetime`.\n\t\n\tThis object behaves as a ``datetime.date``, but with additional attributes"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if not isinstance(dt, datetime.tzinfo):\n\traise ValueError('argument is not of a known timezone')\n\t\n\tnow = dt.replace(tzinfo=timezone.utc)\n\tdston = dt - datetime.timedelta(\n        hours   = tzname_info.hours,\n        minutes = tzname_info.minutes,\n        seconds = tzname_info.seconds\n        )\n\tdstoff = dt.replace(tzinfo=timezone.utc).astimezone( \\\n\t\tutc).replace(tzinfo=None)\n\n\tif dstoff <= now:\n\t    return now\n\telse:\n\t    return dston\n\ndef"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\tif not isinstance(dt, datetime.datetime):\n\t\t\traise TypeError, \"%r is not a datetime\" % (dt,)\n\n\t\treturn self.__fromutc(dt).replace(tzinfo=self.tzinfo)\n\n\n\tdef __fromutc(self, dt):\n\n\t\treturn fromutc(dt).astimezone(self.tzinfo)\n\n\n\tdef _get_time(self):\n\n\t\treturn datetime.datetime.fromtimestamp(\n\t\t\tself._get_time_string()).replace(tzinfo=self.tzinfo\n\t\t\t\t).astimezone(self.tzinfo)\n\n\n\tdef _set_time(self, time):"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    dt = self.normalize(dt)\n\n    if dt.tzinfo is not None:\n        while dt.tzinfo.utcoffset(dt) is not None:\n            dt = dt.replace(tzinfo = dt.tzinfo)\n\n    return dt\n\n\ndef now(string, tzinfo=None, locale=False):\n\t\"\"\"\n\tTurns a string into a :term:`Time <tzinfo>` object of the\n\tgiven *string* and timezone. If *string* has no timezone information,\n\tit defaults to the string local timezone object.\n\n\t.. note::\n\n\t    If both `locale` and the string"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    current_local = _local_to_utc(dt.timestamp())\n    delta = current_local - _local_to_utc(current_local.tzinfo())\n    if delta.seconds % 60 > 1:\t    # pragma: no cover\n        raise OverflowError('Time zone ambiguous')\n    elif delta.seconds < -1:    # pragma: no cover\n        # this is a case of a CQL error, since we know this has left the\n        # clock (as defined in iso_8601).\n        # Note: this is also a single second; it is OK to assume we are\n        # looking for"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\treturn dt + timedelta(days = 1)\n\telif isinstance(dt, str):\n\t\treturn dt\n\t\t# Assume it's a string: decode it\n\t\ttry:\n\t\t\treturn parse_timezone(dt)\n\t\texcept TimeError:\n\t\t\treturn datetime.datetime.fromordinal(dt.split()[0])\n\traise TimeError(f'Unknown datetime type {dt.tz!r}')\n\ndef parse_timezone(tz):\n\t\"\"\"\n\tConvert a timezone string (e.g. ``Europe/Sofia`` or\n\t``Japan`` as returned by"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tif not isinstance(dt, datetime.datetime):\n\t    return None\n\n\tunambiguous = dt.replace(tzinfo=None)\n\tif not unambiguous.tzinfo:\n\t    return unambiguous\n\n\t# Try to use a naive datetime with tzinfo but fall back\n\t# to naive datetime instances.\n\tif dt.tzinfo is None:\n\t    dt = dt.replace(tzinfo=dt.tzinfo)\n\n\tdt = dt.replace(tzinfo=None)\n\n\ttzutc = gettz(dt)\n\n\tif not hasattr(tzutc, 'normalize'):\n\t    return None\n\n\treturn tzutc\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\treturn dt.replace(microsecond=0).astimezone(self._tzinfo)\n\n\telif isinstance(dt, datetime.datetime): return dt - self._offset\n\telif isinstance(dt, timedelta): return dt - self._offset\n\telif isinstance(dt, date): return dt.date()\n\telif isinstance(dt, (str, unicode)): return dt\n\telse: raise TypeError\n\ndef delta_in_seconds(td):\n\t\"\"\"\n\tGiven a timedelta in seconds, calculates the difference in seconds.\n\tOther calculated values are zero (might be negative).\n\t\"\"\"\n"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if is_ambiguous_datetime(dt.tzinfo):\t\n\treturn dt\n\tin_fold    = in_ambiguous_dates(dt)\n\tif in_fold and not dt.fold:\t\n#\t\tprint \"\"\"\n# --------------------------------------------------------------\n#      ERROR: Cannot determine if a given datetime is not ambiguous.\n#      --------------------------------------------------------------\n#      \"\"\"\n#\n#\t\t\n    # In the case of ambiguous datetimes, the datetime will always be\n    # ambiguous, so there is nothing to look.\n\treturn\n\telif in_fold and dt.fold.date() == dt."}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n\t#!/usr/bin/env python\n#\n# Copyright 2007 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\treturn dt.replace(tzinfo=default_tzinfo)\n\ndef _cmpkey(a, b):\n\treturn (a > b) - (a < b)\n\ndef _to_localtime(dt, default_timezone=None, locale_getter=_cmpkey):\n\t\"\"\"\n\tConverts a datetime object to the local timezone\n\tused by the datetime._parse_time function\n\n\t:param dt:\n\t\tThe datetime to convert (just like datetime.now())\n\t:param default_timezone:\n\t\tOptional :py:class:`tzinfo` to apply unless\n\t\tapplicable to"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\treturn dt.replace(tzinfo = tz.tzutc())\n\n#!/usr/bin/env python\n\n\"\"\"\nPython code used to train and evaluate a model on pretrained CNN.\n\"\"\"\n\n\n\nimport argparse\nimport sys\nimport datetime\nfrom functools import reduce\nimport time\nfrom utils.reader import get_readers, get_model_reader, get_valid_reader, prepare_reader\nimport numpy as np\nimport tensorflow as tf\n\nfrom..utils import logging\n\n\n\ndef build_model(input_height, input_width, depth, dropout=0.0,\n                  use"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=default_tzinfo())\n    return dt.astimezone(tzinfo)\n\ndef localtime_to_utc_offset(offset, tz = None):\n    \"\"\"\n    Converts local time to UTC.\n    \n    >>> import dateutil\n    >>> from datetime import datetime\n    >>> localtime_to_utc_offset(datetime(2005, 3, 1))\n    datetime.datetime(2005, 3, 1, 1, tzinfo=<UTC>)\n    >>> localtime_to_utc_offset(datetime(2005, 3, 1, 23, 59, 59),\n   ..."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n\nfrom datetime import datetime\nfrom typing import Dict, Union\n\nimport numpy as np\nfrom pandas import DataFrame, date_range\nimport pandas._testing as tm\nimport pandas.index as _idx\n\nfrom.base import BaseSlicingTests\n\n\nclass SharedWithSparseTests:\n    def test_iloc_getitem(self, multiindex_year_month_day_dataframe_random_data):\n        idx = MultiIndex.from_product([\n            date_range(\"1/1/2000\", periods=2),\n            range(5),\n        ], names=(\"Date\", \"Year\"))\n\n        rng ="}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    def set_tz(self, dt, tzinfo):\n        \"\"\"\n        Set the ``tzinfo`` parameter on naive datetimes only\n\n:param dt:\n    The datetime on which to replace the time zone\n\n\tA naive datetime object is constructed with the parameters\n\t``tzinfo`` parameter. Use with care. \n\t\"\"\"\n\t\n        try:\n            if dt.tzinfo is None:\n#                 dt = self.localize(dt)\n#             else:\n#                 dt = self.localize(dt, is_dst=False)\n#         except:\n# #             raise DateError(\"Failed to convert naive"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    return dt.replace(tzinfo=tzinfo).astimezone(default_tzinfo())\"\"\"\nTests for wptserve.py\n\"\"\"\nimport os\nimport os.path\nimport shutil\nimport socket\nimport subprocess\nimport tempfile\nimport unittest\n\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom mock import patch, Mock\n\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.settings import Settings\n\nsettings = Settings()\nsettings.setstr('MAX_REQUESTS', '500')\n\n\nclass CommandLineTests(unittest.TestCase):\n\n   "}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n        \n\n\n# -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\n\nfrom django.db import models\nfrom ckeditor.fields import RichTextField\nfrom django.core.validators import MaxValueValidator, MinValueValidator\n\n\nclass JSONField(models.TextField):\n    description = \"a JSON field that has a maximum length\"\n\n    __metaclass__ = models.SubfieldBase\n\n    def formfield(self, **kwargs):\n        if self.max_length == None:\n            return super(JSONField, self).formfield(**kwargs)\n\n        if self.max_length is None:"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\timport datetime\n\t\tdef _naive(dt):\n\t\t\t# Convert to naive but always return naiveness\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\n\t\treturn _naive\n\n\tdef tz_factory(name):\t\t# For backwards compat\n\t\treturn default_tzinfo\n\n\treturn tz_factory\n\n\ndef time_zone(dzone):\t# for backward compat\n\t\"\"\"\n\nfrom base.views import cache\nfrom base.views import (\n    CachedViewFactory, UserCreateView, UserUpdateView,\n    TeamCreateView, TeamUpdateView,\n    PostCreateView, PostUpdate"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    import datetime\n    return datetime.datetime.replace(tzinfo=dt,\n                                  time=datetime.timedelta(seconds=tzoffset(tzinfo).total_seconds()),\n                                  tzinfo=tzinfo)\n\ndef parse(datestring):\n    \"\"\"\n    Converts the input string to datetime.datetime instance, as a\n    datetime.datetime instance\n\n    :param datestring:\n    :return:\n    :raise ValueError: if the input is malformed\n\n    Examples:\n        * 2014-01-01 12:30:00Z\n\n   .. doctest::\n\n        >>> parse(\"Today is 06:30:00 PM | 2014-01-01 12:30:"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself.cut_chars = ''\n\t\tself.after = bytes\n\t\t\n\tdef get_cut_chars(self) -> bytes:\n\t\treturn self.cut_chars\n\tdef set_beyond_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself.beyond_cut_chars = ''\n\t\tself.cut_chars = ''\n\t\tself.beyond_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    self._reset_cut_chars = True\n    before = set_cut_chars(before,'')\n    after = set_cut_chars(after,'')\n    if before == '' or after == '':\n        before = ''\n        after = ''\n        offset = 0\n    else:\n        offset = len(before) + len(after)\n\n    if after!= '':\n        before = set_cut_chars(before,'', after)\n        after = set_cut_chars(after,'', before)\n\n    self._trim_past = list(filter(None, before + after))\n    self._trim"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    # Split string:\n    before = before.split('\\n')\n    self.file_content = before[1:]\n\n    # Split file after:\n    after = after.split('\\n')\n    self.file_content_after = after[1:]\n\n    # Split file not at end:\n    self.file_content_not_ended = '\\n'.join(\n        [self.file_content, self.file_content_after]).strip()\n    self.file_content_already_ended = '\\n'.join(\n        [self.file_content_not_ended, self.file_content"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    if not before or not after:\n        return\n    to_write = len(before) + len(after)\n    while to_write > 0:\n        to_write = min(to_write, len(before) + len(after))\n        if to_write < len(before):\n            to_write *= 2\n        if to_write > 0 and of.get_position()!= before[0].get_position():\n            raise ValueError(\"Split files are not aligned\")\n\n        \n    if (after.size and not after.size > 0) and self.get_buffer()[0].get_modified"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    # FIXME: What if the end of the line was not found?\n    new_end = re.sub(r'^\\s*(?:\\n\\s*\\Z)', '\\r\\n', after).strip()\n\n    cut_start = re.compile(r'^{}$'.format(self.read_range).strip(), re.M).match\n    cut_end = re.compile(r'^{}$'.format(self.read_range).strip(), re.M).match\n\n    new_text = self.write_string\n    new_end_text = new_end.replace(raw=new"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    self._find_shift = {'': '', '<': '', '+': '', '<=': '', '>': '', '<': '', '\"': '', '|': '', 'x': '', 'y': '', 'n': '', 'o': '', '*','' : '', '~': '', '*': '','s': '', 'p': '', 'z': ''}\n\n    def __getitem__(self, key: str, *args: bool) -> Union[int, slice]:\n        \"\"\"Check, if the data exists in the dataset.\n\nArgs:\n    key: Key to check.\nReturns:\n    True if the data exists and is a slice"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tprint(\"Set of chars from :\", before, \"-->\", \"...\")\n\t\tpre, suf = self._check_cut(''.join(before))\n\t\tself.cut_chars = before + suf + after\n\n\t\tif suf:\n\t\t\tself.cut_chars += before + suf + suf\n\t\tif pre:\n\t\t\tself.cut_chars += self._re.sub('\\\\1', '#'+pre)\n\t\t\tself.cut_chars += self._re.sub('\\\\1', pre)\n\n\tdef update_after(self):\n\t\t\"\"\"\n\t\tUpdate the slice."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    new_file = BytesIO(byts.decode('utf8')).read()\n    \n    lines = [str(o) for o in self.lines]\n    if after!= self.previous_line:\n        lines.append(str(self.line_separator.join(lines)))\n\n    if self.prev_line!= self.line_separator.join(lines):\n        lines = lines[:-1]\n        lines.append(self.line_separator.join(lines))\n        new_file.write(''.join(self.prev_line.split(self.line_separator) + lines))\n        new_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    for i in range(len(before) - len(after)):\n        before_file = before[i:]\n        after_file = after[i:]\n\tb = os.path.basename(before_file)\n\tif not os.path.isabs(b):\n\t    before_path = os.path.join(os.path.dirname(before_file), b)\n        else:\n\t    before_path = os.path.join(os.path.dirname(before_file), b + self.pathsep)\n        after_path = before_path + self.pathsep\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    with open(self._filename, 'r') as infile:\n            for char in infile:\n                temp = int(char, 16)\n                self._file.seek(temp)\n                self._file.write(before + temp + after)\n\n    def get_cut_chars(self, begin: bytes) -> bytes:\n        \"\"\"\n        Get the cutchars which were used by seeking.\n        \"\"\"\n        chars = []\n        after = \"\"\n\n        for i in range(begin, 0, -1):\n            char = self._file.read(1)\n            if char == \"\":\n                break\n            chars.append("}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\ttry:\n\t\t\trequest.get_body()\n\t\t\treq_body = request.get_body()\n\t\t\treq_id = req_body.get('RequestId', None)\n\t\t\tif req_body.get('Content-Type') is None:\n\t\t\t\treturn 'bad request'\n\n\t\t\t# find the right MIME type\n\t\t\tctype = req_body.get('Content-Type')\n\n\t\t\tif ctype == 'text/plain':\n\t\t\t\tif len(req_body) > 1:\n\t\t\t\t\tbody = req_body.get_payload(decode=True)\n\t\t\t\t\tif len("}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\treturn _identify_message(request)\n\n\ndef get_messages(request: RequestType, client: DiasporaBaseClient) -> List[Message]:\n\t\"\"\"\n\tTry to find the message from all clients and check each message. For each client, try to send out the message out the same way, and add the relevant payload and create corresponding messages. If no such message, show a message. \n\t\"\"\"\n\tmessages = []\n\tfor client_client in client.clients:\n\t\tmessages_in = []\n\t\tfor message_client in client_client.clients:\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tpayload_type = identify_payload(request.data)\n\n\t\tif payload_type is None:\n\t\t\traise Exception('No payload found')\n\t\t# this shouldnt be here as a private message. The other end are insignificantly different from some other places.\n\t\tprint(request.user, payload_type[0])\n\n\t\tcheck_for_existing_payload(payload_type[0], payload_type[1])\n\t\treturn payload_type[1]\n\texcept DiasporaException as err:\n\t\tprint(err)\n\t\traise err\n\ndef identify"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif not request.__auth__:\n\t\t\traise ImproperlyConfigured('You must be authenticated to receive this message')\n\n\t\tif request.__id__:\n\t\t\ttry:\n\t\t\t\tprivate_msg = PrivateMessage.objects.get(pk=request.__id__)\n\t\t\t\ttry:\n\t\t\t\t\tmsg = PrivateMessage.objects.get(pk=private_msg.pk)\n\t\t\t\t\tassert private_msg.user == msg.user\n\t\t\t\texcept PrivateMessage.MultipleObjectsReturned as e:\n\t\t\t\t\traise ValidationError({\n\t\t\t\t\t\tself.__name__: _('The message is duplicated')}\n\t\t\t\t\t\t)\n\t\t\texcept PrivateMessage.DoesNotExist:\n\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tsender = get_sender(request)\n\t\tsender_nick = sender.lower()\n\t\trecipient = get_recipient(request)\n\t\tis_dm_message = str('dm' in request.GET) and request.GET['dm']\n\t\tsender_msg_type = sender_nick.replace(sender, \"!\").lower()\n\t\tif is_dm_message:\n\t\t\treturn True, sender_msg_type, recipient\n\t\telse:\n\t\t\treturn False, request.GET['recipient']\n\texcept Exception as err:\n\t\ttraceback.print_exc()\n\ndef"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# We're using a simple payload.\n\t\t# This payload has no extra attributes.\n\t\tif PayloadIsFull(request) and (not PayloadIsEmpty(request)):\n\t\t\tfor prop in PayloadGetProperties(request):\n\t\t\t\tif 'Data' in prop and not prop['Data'] in [ 'address', 'country', 'city','region' ]:\n\t\t\t\t\treturn True\n\t\treturn False\n\n\ttry:\n\t\tmsg_id = parse_msg_id(request)\n\t\tif msg_id == 'INV':\n\t\t\tmsg_id = request['INV']['"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# TODO: Make this configurable.\n\t\treturn False\n\telif request.HasField('payload'):\n\t\t# TODO: Handle generic payloads, e.g. messages?\n\t\treturn False\n\telif request.HasField('type'):\n\t\treturn True\n\telif request.HasField('name'):\n\t\treturn True\n\telif request.HasField('token'):\n\t\treturn True\n\telif request.HasField('body'):\n\t\treturn True\n\telif request.HasField('timestamp'):\n\t\treturn True\n\n\ttime = request.time\n\n\tif'request_id' not in request.message:\n\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# Let them know of this message and store those requests\n\t\tresponse_dict = {\"status\": 1, \"data\": \"I'm running on Diaspora\"}\n\t\trequest.response = response_dict # Save response\n\t\treturn request\n\n\tdef on_public_stream(self, request: DiasporaRequest) -> None:  # The first pubsub message.\n\t\t\"\"\"\n\t\tThis message was received by the Diaspora daemon, and we should send this to the other daemon.\n\t\t\"\"\"\n\t\t# Let them know of this message and store those requests\n\t\tresponse"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor message in request.messages:\n\t\t\tif message.command == \"PRIVMSG\" and request.sender == message.sender and request.target == message.target:\n\t\t\t\treturn message\n\n\n\n@asyncio.coroutine\ndef check_dvi(request: ClientRequest):\n\t'''Checks if a request from a Diaspora daemon is connected. If not, a Diaspora daemon is assumed.\n\nArgs:\n\trequest: The Diaspora request to check\n\t'''\n\tasyncio.coroutine(identify_request(request))\n\treturn client.is_alive()"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# Check if the request is marked valid, otherwise raise an exception.\n\t\t# If the request was valid, do nothing\n\n\t\tif not is_valid_legacy_payload(str(request.params[\"message\"][0])):\n\t\t\treturn False\n\n\t\tif \"sender_ts\" not in request.params[\"message\"][0]:\n\t\t\trequest.add_header(\"sender_ts\", \"ts\")\n\n\t\treturn True\n\n\tdef check_signature(msg_type: int, request: RequestType):\t\"\"\"Checks signature of message.\n\t\tCheck if signature is correct.\n\t\t\"\"\"\n\t\tfor"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\t# The Matrix request is represented with a list of values (row,col)\n\t\trow, col = identify_request_info(request)\n\n\t\t# Check row dimension\n\t\tif len(row) == 2 and type(row[0]) is not Matrix:\n\t\t\treturn True\n\n\t\t# Check column dimension\n\t\tif len(col) == 2 and type(col[0]) is not Matrix:\n\t\t\treturn True\n\n\t\t# Check number of matrix row\n\t\teq_rows = len(row) == 3 and type(row[0][0]) is Matrix and len(row) =="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif isinstance(request, MatrixRequest) and request.request_type!= MatrixRequestType.TYPE.VALUE:\n\t\t\traise RuntimeError('Input was of type %s but returned type not list' % type(request))\n\t\tif isinstance(request, MultiInputMatrixRequest) and request.request_type!= MultiInputMatrixRequestType.TYPE.VALUE:\n\t\t\tif ishashable(request[1]):\n\t\t\t\traise RuntimeError('Input should be hashable.')\n\t\t\treturn all(not isinstance(a, int) or isint(a) for a in request)\n\t\telse:\n\t\t\treturn request == Matrix"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\ttype_ = request.get(\"type\", \"\")\n\t\tif type_ == \"matrix\":\n\t\t\treturn True\n\t\telif type_ in [\"search\", \"project\"]:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\t@staticmethod\n\tdef process_request(request, input_objects, model_type, model_properties: dict) -> HttpResponse:\n\t\t\"\"\"\n\t\tProcesses the form request and processes the search results,\n\t\tif necessary.  The output of this function is then rendered\n\t\tin the response.\n\t\t\"\"\"\n\n\t\tsearch_term = request.POST.get(\""}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.is_matrix:\n\t\t\treturn True\n\n\t\ttry:\n\t\t\tmatrix = Matrix.objects.get(data=request.data)\n\t\t\treturn True\n\t\texcept Matrix.DoesNotExist:\n\t\t\treturn False\n\n\telse:\n\t\treturn False\n\n\ndef get_type_by_name(type: str) -> bool:\n\treturn type in request_type_names.keys()\n\n\ndef create_matrix(data: dict, rows: Sequence[Dict[str, Any]]) -> None:\n\t\"\"\"\n\tCreate a matrix from a python dict of dictionaries (the keys should"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.get('user_id') == -1:\n\t\t\treturn False\n\t\tuser = self.store.user(request['user_id'])\n\t\tmatrix_request = MatrixRequest.all().filter(request = int(request['query']) | int(request['id']))\n\t\tif matrix_request.count() == 0:\n\t\t\treturn False\n\n\t\treturn user.has_pair(matrix_request)\n\n\tdef get_matrix_matrix_data(self, **kwargs) -> Tuple[bool, Optional[bytes]]:\n\t\tif 'user_id' not in kwargs or"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tresult: bool = bool(request.get(\"result\"))\n\n\t\tif type(request.get(\"matrix_request_type\")) == MatrixRequestType:\n\t\t\tmatrix_request_type = request.get(\"matrix_request_type\")\n\t\t\tmatrix_requests: list = await MatrixService().request_matrix_request_types.check(matrix_request_type=matrix_request_type)\n\t\telse:\n\t\t\tmatrix_requests = list()\n\t\t\tmatrix_requests_raw = await MatrixService().request_matrix_request_types.check()\n\t\t\tfor row in matrix_requests_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\t# First, try to recognize this request type\n\t\tname_of_request = \"matrix_\" + requestType\n\t\tif requestType == MatrixRequestType.SOA:\n\t\t\t# We are a perfect match against the whole body\n\t\t\tif name_of_request == \"matrix_id\":\n\t\t\t\treturn True\n\n\t\t# Maybe we don't recognize this request type, but it's a special case\n\t\t# where we don't have exactly 1 or more request names\n\t\telse:\n\t\t\tfor item in request.all_list_items():\n\t\t\t\tif name_of_request in item:\n\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tlogging.info(\"Checking for Matrix request\")\n\t\treturn request.isMatrixRequest()\n\n\tdef list_requests(self) -> Iterable[HTTPRequest]:\n\t\t\"\"\"\n\t\tLists all requests.\n\n\t\tReturn: An iterable of HTTPRequest objects.\n\t\t\"\"\"\n\t\tlogging.info(\"Listing all requests\")\n\t\tfor request in self.requests:\n\t\t\tlogging.info(request.prettyPrint() + \": \" + request)\n\n\t\treturn (i for i in self.requests)\n\n\tdef list_prerequisites(self) -> Iterable[PredicateRequest]:\n\t\t\"\"\"\n\t\tLists"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.get_type() == MatrixRequestType.MATRIX\n# Copyright 2012,  Nachi Ueno,  NTT MCL,  Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.form.get_bool(\"identify_request\")\n\t# endregion\n\n\t# region FILLREQUEST\n\n\tdef can_cancel_request(self, request: RequestType) -> bool:\t\"\"\"\n\tFor all others request type for deleting.  \n\tFor all others, it is fine to change this state\n\tfor the request_type, because the input_token is checked in request.form\n\t\"\"\"\n\treturn request.form.get_bool(\"can_cancel\")\n\t# endregion\n\n\t# region CHANREQUEST\n\n\tdef can_create_request"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tfmt ='(%d/%m/%Y) '\n\t\tif dt.tzinfo:\n\t\t\tfmt += '%s%s'% (\n\t\t\t\ttz.next(),\n\t\t\t\tdt.tzinfo.tzname()\n\t\t\t)\n\t\telse:\n\t\t\tfmt +='' + str(dt.strftime('%Y-%m-%dT%H'))\n\t\treturn fmt\n\n\tdef generate(self, node):\n\t\t# Generate the node's dependencies\n\t\tdep_graph = DependencyGraph() + node.dependencies\n\t\tdeps = self.get_node_depends(dep_graph) + [node]\n\t\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif not isinstance(dt, datetime):\n\t\t\treturn dt\n\t\t\n\t\treturn datetime.strftime(dt,'%b %d, %Y')\t\n\t\n\n### This code is borrowed from the Emacs-Gui:\n\n# ------------------------------------------------------------------------\n\n## class node\n\nclass Node(object):\n\t\n    \"\"\"\n    A node in the graph. It can be anything that has a set of attributes.\n    We use it like any Python list, except the node type.\n\n    \"\"\"\n    \n    ### SPECIAL METHODS ###\n    \n    def __init__(self,attr,value):\n        # If value is a list, then we"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt == datetime.datetime:\n\t\t\t# no need to output an empty one.\n\t\t\treturn 'T00:00:00' # no timestamp in output\n\t\tif type(dt) == datetime.date:\n\t\t\treturn dt.strftime('T00:00:00')\n\t\telse:\n\t\t\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\treturn format_dt()\n\ndef format_dt_hmsdms(dt, epoch=0, lat=0, lon=0, alt_km=0):\n\t\"\"\"\n\tFormat"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn time.strftime(\"%Y-%m-%dT%H:%M:%S\", dt)\n\nclass DroneMeld():\n\t\"\"\"\n\tGenerate the milds for visualizing the Drone Nodes.\n\t\"\"\"\n\n\tdef __init__(self, node):\n\t\tsuper(DroneMeld, self).__init__()\n\t\tself.node = node\n\t\tself.drs = dict()\n\t\tself.dt = 2*PI\n\t\tself.color = '#000000'\n\n\tdef build(self):\n\t\t\"\"\"\n\t\tBuild the nodes to walk through the d"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tfor i in dt:\n\t\t\tif i.utcoffset(dt) is not None:\n\t\t\t\tdt = dt + i.utcoffset(dt)\n\treturn dt\n\n\nclass TZOffset(tzinfo):\n\t\"\"\"\n\tA tzinfo class for TZ aware date offsets\n\t\"\"\"\n\tdef utcoffset(self, dt):\n\t\treturn timedelta(0)\n\n\tdef dst(self, dt):\n\t\treturn timedelta(0)\n\n\nclass FixedOffset(tzinfo):\n\t\"\"\"\n\tA tzinfo class for fixed-offset datetimes\n\t\"\"\"\n\t# A fixed offset timezone, as no"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t \n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\t\ndef format_date(string):\t\t\t\t\t\t\t\t\t\n\t\"\"\"\n\tFormat a string in the way that D* nodes expect.\t\"\"\"\n\t\n\treturn string.strftime(\"%Y-%m%d\")\n\n\n# Helper functions to take arbitrary arguments\ndef do_test(method):\n    \"\"\"\n    Called when the test method is invoked.   \n    Calls the method with whatever attributes were passed in using \n    this decorator.\n    \"\"\"\n    test_method = '_test_' + method.lower()\n    if hasattr(test_method"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tassert isinstance(dt, datetime), 'dt must be a datetime'\n\t\tdtformat = None\n\t\tif dt.tzinfo is None:\n\t\t\tdtformat = '%d %b %Y %H.%M.%S'\n\t\telif isinstance(dt.tzinfo, dateutil.tz.tzutc):\n\t\t\tdtformat = '%d %b %Y %H:%M:%S'\n\t\telif isinstance(dt, datetime.datetime):\n\t\t\tdtformat = '%Y-%m-%d %H:%M:%S'\n\t\telse:\n\t\t\treturn None # nocoverage\n\t\tif hasattr(dt"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tn = dt.tzinfo.tzname(dt)\n\t\tdt = dt.timetuple(n)\n\t\treturn dt[:10] + \"-\" + dt[11:]\n\n\"\"\"\nFor the sake of the translation of UTC and local time stamps,\ncreate a function to use as a filter of datetime fields.  If none of these fields\nare None, None is returned from the datetime filter.\nThe function will accept any datetime objects that are of this format:\n\t   date/time/datetime_string\n\"\"\"\ndef utc_to_local(dt):\n    \"\"\"\n    Convert the given datetime to an"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn format_value_str('dt', dt)\t# TODO: Check for the format\n\t\n\n\n\n##### Photon Energy ###\ndef format_psi(psi):\n        \"\"\"\n        Format a Psi4Node from the way that the root Psi4 has been called.\n        If the Psi4Node has been called before, the previous calls to Phi3 is ignored.\n        \"\"\"\n        return format_value_str(\"psi\", psi)\nfrom django.utils import timezone\nfrom django.utils.safestring import mark_safe\n\nfrom olympia import amo\nfrom o"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime('%d %B %Y %H:%M:%S')\n\n\ndef format_dms(dd):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dms.strftime('%d %B %Y %H:%M:%S')\n\n\n##\n## Casper JSON decoder.\n##\n\nclass Casper:\n\t\"\"\"\n\tEncode JSON as JSON.\n\t\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = json.loads(data)\n\n\tdef parse(self):"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n# TODO: Remove this, when I use new models\n# TODO: Use set type instead of set, just return a set.\n\n# TODO: Improve how these work on different platforms? That would require refactoring\n# See https://github.com/open-craft/open-craft/blob/master/main/fixers/fix_code/fix_code.py  for more info, etc.\n# But this would require refactoring too.\n\ndef parse_file():\n\t\"\"\"\n\tRead in the list of files supplied by the user and process\n\tthe files using"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tret = None\n\n\t\tif hasattr(text, \"find_tags\") and callable(replacer):\n\t\t\tret = text.find_tags(text, replacer)\n\n\t\treturn ret, text.replace_with_replacer\n\telse:\n\t\tlines = text.splitlines()\n\t\tret, text = [], None\n\n\t\tfor line in lines:\n\t\t\tm = _re.fullmatch(line)\n\t\t\tif m is not None:\n\t\t\t\tret.extend(find_tags(m, replacer))\n\t\t\t# no tag found at all\n\t\t\telif m"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\t# type: (Text, Callable[[Any], str]) -> Set\n\t\tif isinstance(text, (str, bytes)):\n\t\t\t# Python 3\n\t\t\ttags = set(re.findall(r'\\w', text))\n\t\telse:\n\t\t\ttags = set(re.findall(r'\\b{8,}', text))\n\t\tif (isinstance(text, int) or isinstance(text, float)) and text!= '':\n\t\t\ttags.discard('')\n\t\tif len(tags) > 0:\n\t\t\treturn tags, ''\n\t\telse:\n\t\t\treturn None, ''\n\telse:"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttags = set()\n\t\tfound_repl = False\n\t\treplacer_txt = ''\n\t\tmatch = False\n\t\tfor line in text.split('\\n'):\n\t\t\tif line.strip() == '':\n\t\t\t\tline = ''\n\t\t\t\tcontinue\n\t\t\tif line[-1] == '\\\\':\n\t\t\t\tline = line[:-1].rstrip('\\n')\n\t\t\tmatched = line or line.startswith('#')\n\t\t\tif matched or not found_repl:\n\t\t\t\tif match:\n\t\t\t\t\t# Ignore matches inside comment. Ignore for #TAG\n\t\t\t\t\t# TODO(cmaloney): handle inline comments\n\t\t\t\t\t\n\t\t\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttagged_words = tags(text)\n\t\tif replacer:\n\t\t\treplacer(tagged_words)\n\t\tset_names = set(tagged_words)\n\t\ttags = set_names - set(re.findall('[a-z]+', text))\n\t\treturn set(tags), replaced(tagged_words)\n\treturn tagged_words, text\n\n\n@command('untag',\n\tshort_help = 'Remove tag(s) from word tags.')\ndef untag(text: str) -> None:\n\t\"\"\"\n\tRemove tag(s) from word tags"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tret = set()\n\t\ttext = strip_tags(text)\n\t\tfor line in text.splitlines(True):\n\t\t\tline = line.strip()\n\t\t\tif replacer:\n\t\t\t\tmatch = replacer(line)\n\t\t\telse:\n\t\t\t\tmatch = tag_matches_regexp.search(line)\n\t\t\tif match:\n\t\t\t\tif match.group(2):\n\t\t\t\t\tmatch = tag_matches_regexp.sub(\"\", match.group(2))\n\t\t\t\tmatch = replace_func(match)\n\t\t\t\tif match.group(1):\n\t\t\t\t\tret.add(\"{0}"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tres = set()\n\t\ttags = set()\n\t\tresult = None\n\n\t\t# Strip HTML tags within <td> tags\n\t\ttags = strip_tags(re.split(r'<[^<]*>', text, flags=re.DOTALL))\n\t\ttags = [tag.lower() for tag in tags if tag!= '']      # strip html tags\n\n\t\tfor tag in tags:\n\t\t\tif tag in result:\n\t\t\t\tcontinue\n\t\t\telif replacer:\n\t\t\t\tresult = tag +'' + text[len(tag) :].strip()\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif not replacer:\n\t\t\treplacer = get_replacer()\n\t\ttext = clean_spaces(text)\n\n\t\t# Strip the start and end quotes: if not a single string, then\n\t\t# simply give it a single None, to avoid the same result\n\t\t# depending on how it was called\n\t\tdef repl(m):\n\t\t\treturn m.group(1) or ''\n\n\t\ttag_replacer = lambda a: replacer(a[0] if len(a) == 1 else None)\n\t\tre_search = re.compile(r"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttag_pattern = regex.compile(r'<(?P<tag>' + id_start + r')\\s*\\>' + text + r'\\s*=\\s*(?P=tag) + r'\\s*</(?P=tag)>\\s*$', re.DOTALL).scan\n\n    # TODO: Can use a regular expression for tags?\n    return search_tags(text, replacer)\n\n\ndef remove_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\t\t\n    \"\"\"Remove tags from text.\n\nTries to ignore tags outside code blocks.\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tresult: Set = set([])\n\n\t\t# This is going to make the replacement only succeed on a string containing a comma\n\t\ttagre = re.compile('\\s')\n\n\t\t# Now use re.split since we need to check if there are whitespace at the start of the string.\n\t\twhile True:\n\t\t\tmatched = tagre.search(text)\n\t\t\tif matched:\n\t\t\t\tresult.add(matched.group(0))\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\tresult |= set(text)\n\n\t\treturn list(result)\n\n\tdef get_tag_list(text: str)"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor link in LINKS:\n\t\t\tif link['src'] == text:\n\t\t\t\tlink['href'] = link['href'].replace(link['href'] + '/', '')\n\t\t\telse:\n\t\t\t\tlink['href'] = link['href'].replace('../@', '')\n\n\n\ndef build_links_from_text(file_path, out_path):\n    \"Build links from a file based on the path and name of a file_path\"\n    file_name = path.splitext(file_path)[0]\n    link_list = load_path_and_name(file_path)\n\n   "}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext = text.strip().replace(\"\\n\", \" \")\t\t\t\t# Cleanup HTML from text\n\t\ttry:\n\t\t\tparser = Parser(trim_blocks=False)\n\t\t\tlinks = parser.parse(text)\n\t\t\tfor link in links:\n\t\t\t\thref, title, text = link\n\n\t\t\t\tif href and not len(href) and not title:\n\t\t\t\t\tcontinue\n\n\t\t\t\ttry:\n\t\t\t\t\turl = urlparse(href)\n\t\t\t\t\tif not url.netloc:\n\t\t\t\t\t\turl = \"http://\" + url\n\t\t\t\t\t\n\t\t\t\t\tif not url.path:\n\t\t\t\t\t\t# Append if not already linked\n\t\t\t\t\t\tif"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext = text.encode('ascii', 'xmlcharrefreplace') # not sure why this happens.\n\t\tfor (regex, link) in rex.findall(r'(?im)<a\\s+href=\"(?!\\1).+\">', text):\n\t\t\ttext = text.replace(regex, link)\n\t\t\ttext = text.replace(regex_unicodereplace, link) \n\treturn text\n\t\n\t\n\t\n\t\t\n\t\t\n\n\t\n\n#===========================================================\n#\n#  CLASS DEFINITION OF CLASSES\n#\n#===========================================================\n\n\nclass Article(list):\n\t\"\"\"\n\tArticle is a"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttry:\n\t\t\turls = HR.process_text_links(text)\t\t# links to text elements (all elements)\n\t\t\ttext = urlize(text)\t\t# convert links to unicode, strip urls\n\t\texcept UnboundLocalError:\n\t\t\ttext = linkify(text) \t# convert unicode links to plain text, do no processing\n\t\texcept UnicodeDecodeError:\n\t\t\tpass\t\t# ignore invalid characters\n\n\tif not urls:\n\t\treturn text\n\t#print url_regex.match(text).groups() if url_regex else None\n\n\treturn process_url"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t#print (\"## LINKS\")\n\t\t#links = text.findAll('a')\n\t\tlinkify = False\n\t\tif \"http://\" in text:\n\t\t\turl =  text\n\t\telif \"i.am/\" in text:\n\t\t\tlinkify = True\n\t\telse:\n\t\t\tprint(text)\n\t\t\tsys.exit(1)\n\t\t#print (\"%s links, %s linkified\" % (links, linkify))\n\t\tfor link in links:\n\t\t\ta = link.get('href')\n\t\t\ta = a.split(\"/\")[-1].split(\".\")\n\t\t\tif"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n# Parse XML and save into lists\np = etree.parse(source_file)\ntext_list = []\nid_list = []\np_list = etree.parse(processed_file)\ndoc_list = []\nurl_list = []\nfor p in p_list.iter():\n\ti=0\n\tfor o in p:\n\t\tif o.tag == \"a\":\n\t\t\tfor attr in o.iter('a:attr[@href]'):\n\t\t\t\tif attr.get('href') and attr.text:\n\t\t\t\t\tif attr.get('href') not"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext = unicode(text).strip()\n\t\tif settings.INLINE_LINK_REGEX.match(text):\n\t\t\t# Special case new lines which start with a punctuation or newline,\n\t\t\t# which were introduced in the wikis so no valid text will be added\n\t\t\treturn text\n\n\t\t# Linkify all links with the text we found\n\t\t# Do not add a blank line after end of list\n\t\tif settings.INLINE_LINK_REGEX.match(text) and settings.INLINE_LINK_APPEND_REGEX.match(text):\n\t\t\treturn ''\n\n\t\t# Linkify"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext = text.replace('&#039;', \"'\")\n\t\ttext = text.replace('&#8217;', '\"')\n\t\ttext = text.replace('&#8220;', '\"')\n\t\ttext = text.replace('&#8221;', '`')\n\t\ttext = text.replace('&#8222;', '`')\n\t\ttext = text.replace('&#8224;', '`\")\n\t\ttext = text.replace('&#8225;', \"'\")\n\t\ttext = text.replace('&#8211;', \"'\")\n\t\ttext = text.replace('&#8212;', '`')\n\t\t"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n\n\t# A more efficient method for identifying which URLs have\n    # been resolved relative links in the document, so we can\n    # determine which tags are currently known, and remove them from the text.\n    # TODO: We have to change this code so that URLs can use references\n    # to nodes, elements, and links to titles etc. are processed correctly.\t\n    link_resolver = _LinkResolver(attr_map.copy(), links)\n\n    # Create the list of all HTML elements that don't have a URL;\n    # for URLs that can't parse html in them yet.\n    # The purpose of this method"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\n\t\n\tdef repl(match):\t\t\t# {{{\n\t\t\"\"\"Replaces links with more tags (e.g. <title>)\"\"\"\n\t\ttext = handle_image(text)\n\n\t\t# The replacement operator can be used to replace links in\n\t\t# brackets or are replaced by hyphens. Only \"python-Markdown\"\n\t\t# is allowed in the link text.  Many link texts do not contain\n\t\t# leading whitespace, and certain tags are not allowed inside links.\n\t\t# We convert those to <a> and <img> to\n       \t\t<a href=\"something\" rel=\""}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tcontent_type = None\n\n\t\tdef resp_read(chunk_size):\n\t\t\ttry:\n\t\t\t\tchunk = resp_file.read(chunk_size)\n\t\t\t\tif not chunk:\n\t\t\t\t\tsock.close()  # close server connection!\n\t\t\t\t\treturn None\n\t\t\t\tlength = struct.unpack(\">i\", chunk)[0]\n\t\t\t\tcontent_type = await resp_read_with_retry(chunk_size, sock, length)\n\t\t\texcept (FileNotFoundError, ProtocolError) as e:\n\t\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\t\traise\n\t\t\t\traise FileNotFoundError(f\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\trequest_start = time()\n\t\twith requests.Session() as session:\n\t\t\turl = session.head(url, allow_redirects=True, timeout=5)\n\t\t\tr = session.status_code\n\t\t\tif r == 304:\n\t\t\t\t# Sometimes a HEAD request always ends in a redirect\n\t\t\t\tsession._logger.debug('HEAD request took longer than 5 minutes')\n\t\t\t\treturn None\n\t\t\telif r < 499:\n\t\t\t\t# HEADs should be HTML decoded so we assume the first few bytes are of the\n\t\t\t\t# same type\n\t\t\t\tsession._logger.debug('HEAD request failed"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif not have_curl:\n\t\t\treturn url\n\n\t\twith tempfile.NamedTemporaryFile() as tmp:\n\t\t\ttmp.write(download_content(url))\n\t\t\treturn tmp.name\n\n# -------------------------------------------------------------------------------------------------- #\n#                                                                #\n#   TESTING MODULE                         #\n# -------------------------------------------------------------------------------------------------- #\n\ndef test_module(name: str,\n              verbose: Union[bool, str],\n              force_update: bool = False,\n              install: Optional[bool] = None,\n              use_cache: bool = True,\n              url: str = None,\n              ) -> Tuple[bool, str"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tf = fetch_curl_handle(url)\n\t\tif f.response:\n\t\t\t# The data url seems to contain a charset value of \"Unknown\" which\n\t\t\t# results in an unrecognised charset. As such, \"charset\" must be either\n\t\t\t# a constant or a value like \"US-ASCII\", and in this case a UnicodeEncodeError\n\t\t\t# will be thrown. This is fine, as 'charset' is not used for binary data.\n\t\t\t# We want to return it to be a binary type too:\n\t\t\treturn f.response.headers[\"content-type\"]\n\t\treturn"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif url is None:\n\t\t\traise MissingSchema(\"No content type specified.\")\n\t\telse:\n\t\t\treturn None\n\n\tdef fetch_headers(url: str, headers: Any) -> dict:\n\t\t\"\"\"\n\t\tFetch the HEAD of a remote url to determine the HTTP response headers.\n\n\t\t:param url: The remote url string to query headers for.\n\t\t:param headers: The HTTP headers to get.\n\n\t\t:return: The fetched object.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn requests.head(url, headers=headers).text\n\t\texcept HTTPError as e:\n\t\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tresponse = requests.head(url, allow_redirects=False)\n\t\tif response.ok:\n\t\t\treturn response.headers[\"content-type\"]\n\t\telse:\n\t\t\treturn None\n\n\tdef fetch_path(host: str, path: str) -> Optional[str]:\t\"\"\"\n\tFetch a remote URL of the form /user/repo, the path to the user/repo.\t\"\"\"\n\t\turl = parse.urlparse(host)\n\t\tif url.netloc not in PATHS:\n\t\t\treturn None\n\n\t\trepo_name = path.replace(\"/\", \".\").replace(\":\","}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif not os.path.exists(local_dir: os.curdir) or local_key!= '':\n\t\t\tif os.path.isdir(local_dir) and local_key!= '':\n\t\t\t\tlocal_path = local_dir\n\t\t\telif '..' in local_dir and local_dir!= '':\n\t\t\t\t# In case of \"..\" and \"\" only the root dir is supposedly\n\t\t\t\t# to be unpacked.\n\t\t\t\tlocal_path = local_dir + '/'\n\t\t\telif not local_dir:\n\t\t\t\t raise EnvironmentError('The root path defined by'+ local_dir +'' + '"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif not url:\n\t\t\treturn None\n\n\t\tfor line in urlopen(url).readlines():\n\t\t\tline = line.strip()\n\n\t\t\tif line.lower().startswith('content-type: '):\n\t\t\t\tif line.startswith('text/plain; charset=utf-8'):\n\t\t\t\t\tcontent_type = line.split('charset=')[-1].strip('\"')\n\n\t\t\t\telif line.startswith('text/html; charset=utf-8'):\n\t\t\t\t\tcontent_type = line.split('charset=')[-1].strip('\"')\n\n\t\t\t\tbreak\n\n\t\treturn content_type\n\n\tdef save_file(self,"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tparsed = urllib.parse.urlparse(url)\n\n\t\tfor prefix in ['X-HTTP-Method-Override', 'Keep-Alive',\n\t\t                'Range', 'upgrade', \"User-Agent\"]:\n\t\t\tif parsed.netloc.endswith(prefix):\n\t\t\t\ttry:\n\t\t\t\t\tparsed = parsed._replace(netloc=parsed.netloc + prefix)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\treturn parsed.scheme + '://' + parsed.netloc\n#!/usr/bin/env python\n\n\n# import modules\nimport os\nimport os.path\nimport sys\nimport re\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\ttry:\n\t\t\treturn subprocess.check_output(['git','show-ref', '--quiet', '--verify', url], cwd=BASE)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tif 'fatal:'in str(e):\n\t\t\t\traise subprocess.CalledProcessError(e.output)\n\t\t\telif 'git: error'in str(e):\n\t\t\t\traise subprocess.CalledProcessError(e.output)\n\t\t\telse:\n\t\t\t\treturn None\n\n\tdef fetch_repo_name(url: str) -> Optional[str]:\t\t\t\t\"\"\"\n\tFetch the name of the remote repo, i."}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag is None or tag == \"\":\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\t\t\n\t\treturn _test_tag\n\ndef _update_db(engine:str, db:str, user:str):\n\t\"\"\"\n\tUpdates the DB for a given user.\t\n\t\"\"\"\n\tupdate_data(engine,db,user)\n\tengine_map[engine][user] = engine\n\ndef update_data(engine:str,db:str,user:str,data:str) -> None:\n\t\"\"\"\n\tUpdates the data"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in [\"ADV\", \"PRV\", \"ADJ\"]import copy\n\ndef apply_merge(f, x, y):\n    \"\"\"\n    If (x + y) = f(x) and it is not satified, merge it with\n    (x - y). If (x > y), do nothing. If this is done, mark (x < y)\n    and continue.\n    If x > y, break. If x == y, return 0. If y == x.\n    \"\"\"\n    if not x and y:\n        return x, 0\n    elif y:\n        return"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn (tag in tag_dict)\n\n\ttags: List[str] = []\n\ttags_in_set: List[str] = []\n\n\tfor t in tags_in_set:\n\t\tif t in set(tag_dict[t[0]]):\n\t\t\ttags_in_set[t[0]].append(t)\n\n\treturn len(tags) == len(tags_in_set)\n\n\ndef check_conditions(check: str) -> List[Dict[str, List[str]]]:\n\tresult = []\n\tfor f in check.split"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag.lower() in tag_list\n# Copyright 2016 Red Hat, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn word2index.has_tag(tag)\n\n\n\n# Copyright 2019 - Nokia\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND,"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in ['PMC', 'V', 'W']\t\n\n\tdef is_in_list(self, list_str: list) -> bool:\n\t\t\"\"\"\n\t\tReturn whether an argument is a list or tuple as input\n\t\tfor a word.\n\t\t\"\"\"\n\t\tif tag in ['PMC', 'V', 'W']:\n\t\t\treturn bool(list_str)\n\t\treturn isinstance(list_str, str)\n\n\tdef is_in_string(self, string_str: str) -> bool:\n\t\t\"\"\"\n\t\tReturn whether an argument is a string or"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tvalid_tags = [\"A\", \"b\", \"C\", \"d\", \"E\", \"f\", \"g\", \"H\"]\n\t\t# Valid tag\n\t\tif (tag in valid_tags and tag in TAGGING_FEATURES): return True\n\t\t# Indicate a tag\n\t\treturn False\n\n\ttags = list(filter(None, ttag.split(\" \")) + list(map(str.strip, tag.split(\":\"))))\n\tpossible_tags = []\n\tpossible_tags += [\"this\", \"test\", \"a\", \"sentence\", \"of\", \"the\", \"list"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn re.search(\":[^a-zA-Z0-9_]$\", tag) or re.search(\":[^A-Za-z0-9_ ]\", tag)\n\n\ndef test_number(number: str) -> bool:\t\t\"\"\"\n\tTest a word whether it could be accepted as a number.\t\"\"\"\n\treturn re.search(\"[\\s]+\", number)\n\n\ndef test_string(string: str) -> bool:\t\t\"\"\"\n\tTest a word whether it could be accepted as a string.\t\"\"\"\n\treturn re.search(r\"[\\n\\\\"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tprint(type(tag))\n\t\tif tag not in SIM_VERBS:\n\t\t\traise ValueError(\"Invalid tag\")\n\t\treturn type(tag) in {str, int}\n\telse:\n\t\traise ValueError(\"Invalid tag\")\n\ndef tag_from_list(lst: List[str]) -> tuple:\n\t\"\"\"\n\tConvert a list of words into a tag.\n\n\tReturns:\n\t\tTuple with (int) tag: The list of words.\n\t\"\"\"\n\treturn (lst[1], lst[0])\n\t\t\nclass Trainer(object):\n\n\t\"\"\""}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\t# For some reason, the only difference was with a word:\n\t\t# 1. The tag must not be present but not a lemma of the specified words.\n\t\t# 2. The word is not a lempl or the word is a number that starts with a numbered number 1.\n\t\traise NotImplementedError\n\n\tdef count_words(self, tag: str) -> int:\n\t\t\"\"\"\n\t\tReturn the number of words, or the total words in the word case of the tag. If there are no words, return zero.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef parse(self"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tif node.nodeType == xml.dom.Node.ELEMENT_NODE:\n\t\t\tdict = {}\n\t\t\tfor each in node:\n\t\t\t\tif each.nodeName == \"node\":\n\t\t\t\t\tdict[each.attributes[\"key\"]] = \"\"\n\t\t\t\telif each.nodeName == \"string\":\n\t\t\t\t\tdict[each.attributes[\"key\"]] = \"\"\n\t\t\t\telse:\n\t\t\t\t\traise TypeError(\"Unexpected non-tag value, expected boolean, string, element or text node.\")\n\t\t\treturn dict\n\t\telif node.nodeType == xml.dom.Node.TEXT_NODE:\n\t\t\treturn \"\".join('\\n'.join("}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tres = {}\n\t\tfor k, v in node.items():\n\t\t\tvalue = v\n\t\t\tif type(k) in (str, unicode):\n\t\t\t\tvalue = xml_to_dict(v)\n\t\t\tres[k] = value\n\t\treturn res\n\ndef xml_to_dict(e):\n\tret = {}\n\tfor n, v in e.iteritems(): ret[n] = xml_children_as_dict(v)\n\treturn ret\n\ndef xml_tree_from_xml(xml_string):\n\t\"\"\"\n\tParse a xml string and"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tchildren = node.getchildren()\n\t\tif children and isinstance(children[0], str):\n\t\t\tnode['xml'] = {}\n\t\t\tfor child in children:\n\t\t\t\ttag = child[0]\n\t\t\t\tif tag in ['children', 'comment', 'tag']:\n\t\t\t\t\tcontinue\n\t\t\t\ttag_name = ''\n\t\t\t\tif isinstance(tag, str):\n\t\t\t\t\ttag_name = tag\n\t\t\t\telif tag == 'dict':\n\t\t\t\t\ttag_dict = {}\n\t\t\t\t\tkey = tag_name[-1]\n\t\t\t\t\tval = ''\n\t\t\t\t\tif isinstance(val, str):\n\t\t\t\t\t\ttag_dict"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tif xml_children_as_dict!= None:\n\t\t\tfor xml_child in node.findall(\"./xmlns:*\"):\n\t\t\t\tif xml_child.tag not in xml_children_as_dict:\n\t\t\t\t\t# We are only interested in leaf nodes\n\t\t\t\t\tchild = xml_child.find(\"xmlns:{tag}\".format(tag=xml_child.attrib[\"tag\"]))\n\t\t\t\t\tif child.tag not in xml_children_as_dict:\n\t\t\t\t\t\t# We are interested in leaf nodes\n\t\t\t\t\t\tchild = xml_child.find(\"xmlns:{tag}meta\".format(tag=child.tag"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tif isinstance(node, basestring):\t# is a single child node\n\t\t\ttag = node\n\t\t\tchildren = []\n\t\telif isinstance(node, xml.etree.ElementTree.Element):\n\t\t\ttag = node\n\t\t\tchildren = [child for child in node]\n\t\telse:\n\t\t\treturn node, {}\n\t\tif isinstance(children, basestring):\t# recursively convert node's children\n\t\t\ttag = children\n\t\t\tchildren = [child for child in children]\n\t\tif isinstance(children, list):\t# recursively recursivly\n\t\t\ttag = children[0]\n\t\t\tchildren = [child"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t return Dict(\n            name = node.get_name(),\n            attrs = dict(\n                name = str(node.tag), # This is the xml child tag name\n                text = str(node.text) if node.text!= \"\" else \"\", # The xml text\n                attrs = node.get_attrs(),\n                node = node,\n            )\n        )\n\ndef xml_child_elements_to_string(dom_element):\n    \"\"\"\n\tThis function parses the xml child elements out of the given xml element. \nFor example <a><b>Foo</b></a> gets converted to a string"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\n\tdef is_parent(self, node): #pylint: disable=W0102\n\t\t\"\"\"\n\t\tReturn True if <node/> node would be returned in \"node.parentNode\" element\n\n\t\t@param node: DOM Element node\n\n\t\t@return: True if <node> node would be returned in \"node.childNodes\" element\n\n\t\t@raise KeyError: if node does not have a parent of None element\n\t\"\"\"\n\t\tif not self.xml:\n\t\t\treturn True\n\n\t\tkids = self.xml.kids()\n\n\t\tfor tagname in kids:\n\t\t\tif self.xml."}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\t\n\tchildren = dict()\n\tfor child in node:\n\t\tassert(type(child) is ET._Element)\n\t\n\t\tif type(child.tag) in children:\n\t\t\tchildren[child.tag].append(child)\n\t\telse:  \n\t\t\tchildren[child.tag] = [child]\t\n\t\t# If 'node' is an Element - create child's subtree in order to be able to find it's tag in its childnodes\n\t\tnodeidmap[str(child.tag).strip()] = child\n\n\ndef xml_nodes_as_dict(nodes):\t\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tchildren = {}\n\t\tfor child in node:\n\t\t\tchildren.setdefault(child.name, []).append(child)\n\t\treturn children\n\treturn xml.children_as_dict(node)\n\t\t\t\ndef xml_child_name(node, namespace):\n\tname = xml_child(node, namespace)\n\tif name is None:\n\t\treturn '#define %s_NAME (x) (x) : %s\\n\"%s\"%(namespace, name, node.text)\n\t\t\n\treturn '#define %s_NAME'%(namespace.replace('-','_').replace('@','_'),"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tresult = {}\n\t\tname = node.get(\"name\")\n\t\tif name:\n\t\t\tresult[name] = []\n\t\t\tfor child_node in node:\n\t\t\t\tif child_node.tag == \"member\":\n\t\t\t\t\tfor item in xml_children_as_dict(child_node):\n\t\t\t\t\t\tif item.text:\n\t\t\t\t\t\t\tresult[str(item.text).lower()].append(item.text)\n\t\t\t\telse:\n\t\t\t\t\tif not is_valid_node(child_node.tag):\n\t\t\t\t\t\tprint(\"ERROR: Failed to parse: \", child_node.tag)"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n# Diaspora 1.6.4 and earlier have it's own.\ndef check_recipient_match_and_sender_is_same(recipient_handle, sender_handle):\n\t\"\"\"\n\tEnsure that an entity has a match between both sender and recipient (as in SenderHandle), in the same way.\n\nBasically we've already verified the recipient is the same from both sender and receiver (same object is\nboth one of the recipient's's, i.e., if the sender's SenderHandle's recipient already matches\n(or was the recipient's own object), the"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_matches = sender_handle.get_from()\n\t\tif not sender_matches:\n\t\t\tsender_matches = \"\"\t# default value\n\t\tif sender_matches.strip() == entity_handle.get_from():\n\t\t\tsender_handle.add_recipient(entity_handle)\n\ndef check_owner_and_sender_match(owner_handle, sender_handle):\n\t\"\"\"\n\tEnsure that owner and sender handles match.\n\nBasically we've already verified the owner is who they say before. However, the sender might\nbe trying to create a match, since"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender = entity_handle.sender\n\t\tif sender!= sender_handle:\n\t\t\traise Exception(\"Sender and entity handle are differents: sender (%s) & (%s)\" % (sender_handle, entity_handle))\n\n\t\tself.log.info(\"VERIFYING MESSAGE %s\" % sender.profile_id)\n\n\t\t# This is done like in the first time so that we have received a message that the sender wouldn't\n\t\t# handle (which wouldn't have a matching sender address).\n\t\t# We create this version of the msg_type field\n\t\tmsg_type"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\t\n\tsender_metadata={}\n\tsender=g_user\n\t\n\tif g_configuration_is_server_info_value_in_config_object(sender_handle) and len(sender_metadata) > 0:\n\t\tsender_metadata=json.loads(open(sender_info_value_in_config_object()).read())\n\t\n\tentity_metadata=json.loads(open(entity_handle).read())\n\t\n\t\n\t##\n\t\t## If email recipient does not match, return 0 \n\t\t##\n\t\t#if ((sender_metadata[\"sender\"])==(entity_metadata[\""}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\ndef check_receiver_and_entity_handle_match(receiver_handle, entity_handle):\t\"\"\"\n\tEnsure that receiver and entity handles match.\n\nBasically we've already verified the receiver is who they say when receiving the payload. However, the receiver might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\ndef check_sender_id_and_recipient_handle_match(sender_id, recipient_id):\n\t\"\"\"\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_actor_metadata_matches = \\\n\t  settings.RELATIVE_SOURCE_FRAME + \"METADATA_\" + SAME_VERSION.replace(\".\", \"_\") + \".DATA\"\n\t\treceiver_actor_metadata_matches = \\\n\t  settings.RELATIVE_TARGET_FRAME + \"METADATA_\" + SAME_VERSION.replace(\".\", \"_\") + \".DATA\"\n\n\t\tmatch_target_actor_metadata = regexUtils.get_actor(S_METADATA_ACTOR, sender_actor_metadata_matches)\n\t\tmatch_sender_actor_metadata = regexUtils.get_actor(S_METADATA_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\n\tsender_email = get_sender_email_from_header(sender_handle)\n\tsender_domain = sender_email.split('@')[-1]\n\n\n\tif sender_domain == sender_handle:\n\t\treturn \"sender\"\n\n\tif entity_domain == entity_handle:\n\t\treturn \"entity\"\n\n\n\treturn \"unknown sender domain on which we should send {0}\".format(sender_domain)\n\ndef is_recipient(recipient):\n    # This is just a convenience wrapper to check the type of recipient\n    return recipient.__class__.__name__ == \"Recipient\"\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t \n\tmatching_sender = find_matching_sender_and_entity_using_entity(sender_handle, entity_handle)\t#Get all matches\n\tfor k,v in matching_sender.items():\t#For all entities that are in the payload then see if they match\n\n\t\t#if there are no other matches, we can exit a regex match if the first is blank\n\t\t\n\t\tif (k == \"\" and v == \"\"):\n\t\t\treturn False\n\n\t\t#We only want to know the sender's entity, its not a user who started the matching sender (user who sent the first author)\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\t# The payload does not exist because we just check for any user who's sender exists.\n\t\tif sender_handle not in entity_handles_dict[sender_handle]:\n\t\t\t# This sender already matches our sender\n\t\t\treturn\n\t# If the user didn't select this sender in the payload then it is not the same to our sender\n\tif sender_handle not in entity_handles_dict[sender_handle]:\n\t\t# The sender does not match\n\t\treturn False\n\tsender_id = entity_handles_dict[sender_handle][sender_handle.strip()]\n\tfor person"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\n\tif (sender_handle.content_type == CONTENT_TYPE_TEXT and\n\t\tentity_handle.content_type == CONTENT_TYPE_TEXT):\n\t\t\tlog_entity_failure(\"Text/text mismatch. Ignoring.\")\n\t\t\treturn False, (False, None)\n\n\telif (sender_handle.content_type == CONTENT_TYPE_MIMETYPE or\n\t\tentity_handle.content_type == CONTENT_TYPE_MIMETYPE):\n\t\t\tlog_entity_failure(\"MimeType mismatch. Ignoring.\")\n\t\t\treturn False, (False, None)\n\telse:"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path:\n\t\t\tprint(\"Using node info for %s (%s)\" % (document_path, url))\n\t\telse:\n\t\t\tprint(\"Using node info for %s\" % url)\n\t\tif not os.path.exists(document_path) and os.path.isfile(url):\n\t\t\traise IOError(\"Can't read node_info.well-known from %s\" % url)\n\n\t\timport urllib\n\t\timport hashlib\n\n\t\twhereami = urllib.urlopen(url)\n\t\ttry:\n\t\t\tif hashlib.md5(whereami."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\t# This will call parse_remote() with a document path, but there\n\t\t# shouldn't be an error\n\t\tif document_path is None:\n\t\t\tdocument_path = 'index.html'\n\t\ttry:\n\t\t\t# It is more forgiving than what is returned, but since the webservice\n\t\t\t# currently does require the protocol, it is usually a 404, which might\n\t\t\t# return statuscode 500 in this case\n\t\t\tresponse = urlopen(url, timeout=timeout)\n\t\texcept HTTPError as e:\n\t\t\tLOGGER.warning(\"Got a %s response from the webservice"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\timport config\n\t\t\tdocument_path = config.get('nodeinfo', 'document_path')\n\t\t\t\n\t\treturn _get_nodeinfo(url=url, document_path=document_path)\n\ndef get_nodeinfo_version(url, token=None, **kw):\n\t\"\"\"\n\tGenerate a NodeINFO version (including revision) for the given document.\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg token: The token (optional). Should be None (for HEAD requests)\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = self.config[\"DOCUMENT_PATH\"]\n\n\t\treturn {\"protocol\": \"NodeInfo\",\n\t\t\t\t\"path\": document_path}\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013 Ole Krause-Sparmann\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish,"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tnodeinfo_request = \"url=%s&mime=TEXT%s&filename=FILE%s\" % \\\n\t\t\t\t\t  (url, str(mimetypes.guess_type(url)), document_path)\n\t\tres = _request(nodeinfo_request)\n\t\treturn res['Value']\n\tdef get_nodeinfo_well_known_document_list(document_path):\n\t\t\"\"\"\n\tGenerate a NodeInfo.well-known document list.\n\n\tSee spec: http://nodeinfo.diaspora.software\n\n\t:arg url: The full base url with protocol"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = get_document_path()\n\t\t\t\n\t\twith NamedTemporaryFile(suffix='.xml', dir='.') as f:\n\t\t\ttry:\n\t\t\t\tdesc_url_to_file(url=url, \n\t\t\t\t\t\tfilename = os.path.basename(f.name), \n\t\t\t\t\t\tdocument_root = document_path, \n\t\t\t\t\t\tfile_name = f.name, \n\t\t\t\t\t\tcontent_type=None, \n\t\t\t\t\t\tformat='xml',\n\t\t\t\t\t\tcontent_subtype='xml',\n\t\t\t\t\t\tdownload_as=url)\n\t\t\t\treturn dict("}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t # Default return uri\n\t # and document path is relative to url\n\t assert '/' not in url\n\t uri = url\n\t# If url ends with a /, remove trailing /\n\tif document_path == '/':\n\t\tdocument_path = ''\n\t# Convert relative path from url to relative path\n\t# and ignore the localization of a path.\n\tfor i in range(len(uri)):\n\t\tif uri[i] == '/':\n\t\t\turi = uri[0:-1] + '/' + uri[i + 1:]\n\t# Convert base uri to relative path\n\tbase_uri ="}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif not document_path:\n\t\t\tpath = '{http}://{interface}{path}'.format(\n\t\t\t\thttp='http://www.diaspora.org', \n\t\t\t\tinterface='WellKnown-Document', \n\t\t\t\tpath=document_path\n\t\t\t)\n\t\telse:\n\t\t\tpath = document_path\n\n\t\tnodeinfo = {}\n\t\tfor link in get_file_tree(url).findall('a'):\n\t\t\tif link.get('href') and not link.text:\n\t\t\t\tnodeinfo[link.get('href')] = link.get('href')\n\t\t\t\tbreak\n\n\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\t# make sure we are connected to the internet\n\t\timport socket\n\t\ttry:\n\t\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsock.connect(url.hostname)\n\t\t\treturn json.load(sock)\n\t\texcept Exception, err:\n\t\t\tself.log.fatal(\"Error when talking to \" + url + \" : \" + str(err))\n\t\t\treturn None\n\n\n\tdef get_nodeinfo_well_known_version(self, url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo.well-"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tassert False\n\ttry:\n\t\tnodeinfo = nodeinfo.NodeInfo(url)\n\texcept Exception:\n\t\traise NodeInfoError(url)\n\n\tif document_path is None:\n\t\treturn get_nodeinfo_node_path(nodeinfo)\n\telse:\n\t\tdocument_path = os.path.abspath(document_path)\n\n\t\tcache = nodeinfo.Cache(nodeinfo.node_info_base_url, 1, 2)\n\t\treturn get_nodeinfo_well_known(cache, document_path)\n\n\ndef get_nodeinfo_"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Get the public certificate info and signature from the X.509 certificate and use that to sign the message\n\t\tcertObj, _ = X509.load_cert(crypto_util.get_certificate_filename(doc['x509cert'], 'pem'))\n\t\tcert = X509.load_cert(crypto_util.get_certificate_filename(doc['x509cert'], 'pem'), password=_privateKey)\n\t\tsignature = cert.get_signature(signature)\n\t\tif not signature.get_subject().get_components():\n\t\t\tlog(\"X.509 certificate has no subject,"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\ndef do_compute_digest(doc, signature, nonce, expiration_date):\t\n\t\"\"\"\n\tCompute the digest for a message and return it as a string, or\n\tNone if it was not parsable as a public key.\t\n\t\"\"\"\n\n\tdigest_elements = []\n\n\tif public_key.algorithm == \"SHA1\":\n\t\tdigest_elements.append(b(\"RSA-SHA1\"))\n\telse:\n\t\tdigest_elements.append(b(\"RSA\"))\n\t\t\n\tdigest = b(digest_elements.pop() + b(\" \") + seconds_to_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n#\t\t# If a reference signature was missing, then an exception is raised.\n#\tif signature == None:\n#\t\t\tsig = \"signed:\" + str(public_key)\n#\t\telse:\n#\t\t\tsig = \"signed:\" + str(signature)\n#\t\tmsg = \"Error occurred while verifying relayable signature\"\n\t#\t\tif \"error\" in doc and doc[\"error\"] == False and doc[\"publicKey\"]!= \"\":\n#\t\t\tif \"pubkey\" in doc and doc[\"pubkey\"]!= \"\":\n#\t\t\t\tmsg = \"Error"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tpubkey_prefix = doc.documentElement.getAttributeNS(None, 'ds:tag')[1]\n\n\t\t# Extract the signatures.\n\t\tif len(signature)!= 20:\n\t\t\tprint 'WARNING: Invalid xml for signer: %s' % pubkey_prefix\n\t\t\tprint 'Signature in document:', doc.documentElement.toxml()\n\t\t\traise ValueError('\"signature\" element must contain 20-byte public keys, but is %s' % len(signature))\n\n\t\t# Extract the pubkeys.\n\t\tkeys = filter(None, [ doc.documentElement.getAttributeNS(x, 'ks:key')"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t#Verify that signatures match\n\t\tsignatures = signatures_match(signature)\n\t\tif signatures is None:\n\t\t\treturn False\n\t\t\t\n\t\t#Verify signing tags, which means we have an unknown tag\n\t\tif not signatures[\"pubkey_type\"] in [\"pubkey\", \"xpubkey\"] and not signatures[\"pubkeys_type\"] in [\"xpubkey\", \"xprv\"]:\n\t\t\t#The pubkey and xpubkey are only required if they do not exist in the signature\n\t\t\t#We allow the 'pubkey_type_error'', so we will skip all other errors.\n\t\t\treturn True\n\t\t\t\n\t\t#Verify that"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tsigner = SigningKey.load(public_key) \n\n\t\tif signature!= signer.signatures.get('not_before', None) and not self._verified_message_found(signature.get_verifying_key(), signature.get_signature()):\n\t\t\tsignature.remove_signature('not_before')\n\n\t\t\treturn False\n\n\t\tresult = self._verified_message_found(signature.get_verifying_key(), signature.get_signature())\n\t\treturn result\n\n\t@staticmethod\n\tdef _get_signed_message_hash(signed_msg):\n\n\t\t'''\n\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tdef parse(elem):\t\t\t\t# This function takes a string and returns the decoded version:\n\t\t\treturn tuple(int(x) for x in elem.split('.'))\n\n\t\t# This function takes a string and returns the decoded version:\n\t\tasn1_msg_digest = parse(signature)\n\t\tsigned_msg_digest = asn1_msg_digest.get('msg-digest')\n\t\texpect_msg_digest = asn1_msg_digest.get('expectmsg-digest')\n\t\t\n\t\tif not (signed_msg_digest and expect_msg_digest):\n\t\t\t\tsys.stderr"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tif not isinstance(signature, str):\n\t\t\treturn False\n\t\tif public_key.verify_key(signature, doc): return True\n\t\tif not signature.verify(doc): return False\n\t\treturn True\n\n\tdef verify(doc):\n\t\treturn verify_relayable_signature(\n\t\t\tpublic_key = doc['publicKey'],\n\t\t\tdoc = doc['body'],\n\t\t)\n# -*- coding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\nfrom odoo import fields, models, tools\n\n\nclass ResConfigSettings("}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n\n\n\n\t# Get public key object.\n\tprint signature\n\n\t# Retrieve XML elements from XEP-0096.\n\txml1 = parse_xep0096(signature)\n\tif not pub_key_xml_elements(xml1, public_key, doc):\n\t\traise ValueError(\"Error: Found invalid XML for PGP public key: \" + public_key)\n\n\n\t# Verify that the XEP-0096 is not too old.\n\tnew_pub_key_elements(xml1, public_keys[pub_key_xml_elements(xml1, doc)],"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tassert signature == public_key.sign(signature)\n\ndef print_xml(xml, indent=0):\t#pragma NO COVER\n\t\"\"\"\n\tPrint xml to STDOUT\n\t\"\"\"\n\tsys.stdout.write(indent)\n\nclass XMLGenerator(xml.sax.handler.ContentHandler, DTDHandler):\n\t\"\"\"\n\tGenerate XML for a DTD.\n\t\"\"\"\n\tdef __init__(self, outfp):\n\t\txml.sax.handler.ContentHandler.__init__(self)\n\t\tself.outfp = outfp\n\tdef comment("}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdoc = document\n\t\tcomponents = []\n\t\t# Check all lines for the last line\n\t\tfor line in doc.split(\"\\n\"):\n\t\t\tif not line:\n\t\t\t\tcontinue\n\t\t\tcomponents.append(json.loads(line))\n\t\tfor component in components:\n\t\t\tfor entry in get_file(\"/webfinger.js/%s\" % component, only_if_exists=True).values():\n\t\t\t\tif not entry:\n\t\t\t\t\tcontinue\n\t\t\t\tlabel, values = entry[\"label\"].split(\"#\", 1)\n\t\t\t\thtml_values = []\n\t\t\t\tfor value in values.split("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tjson = document.replace(\"```\", \"{}\")\t# JSON output will be processed once a string literal is found\n\t\tlines = json.split(\"\\n\")\t\t\t# Split into sentences\n\t\tsentences = []\n\t\tfor i in lines:\n\t\t\tsentence: str = i.replace(\"u'\", \"n'\") # Replace'u\\' with'n'\n\t\t\t\tsentences.append(sentence)\n\tsentences = np.array(sentences) \t# Flattening list of sentences\n\tsentences = sentences.T\t\t\t# Convert to matrix multiplication\n\tsentences = sentences.astype(\"str\")\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tassert document.endswith(\".json\"), \"Only XRD and OpenDNS documents can be used once\"\n\n\t\tdiaspora_data = json.loads(document)\n\t\treturn DiasporaWebfinger(diaspora_data[\"base_url\"])\n\n\tdef get_domain_name(domain_name: str, *, default_value: Any) -> str:\n\t\treturn domain_name\n\n\tdef format_error_message(domain_name: str, errors: List[Dict]) -> str:\n\t\tif errors:\n\t\t\treturn \"Errors: \" + \", \"."}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn {\n\t\t\t'address': document,\n\t\t\t'client_ip': '0.0.0.0',\n\t\t\t'created_at': str(document.created_at),\n\t\t\t'name': 'Web Finger',\n\t\t\t'signature': 'default_statement',\n\t\t\t'message': 'Your WebFinger is connected with Weasyl-Fred House (or other)!'\n\t\t}\n\ndef parse_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse a single webfinger.\n\t\n\tDoes not parse the WebFinger"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdocument = json.loads(document)\n\n\t\ttitle = document[\"title\"]\n\t\tsubtitle = document[\"subtitle\"]\n\n\t\ttitle_, subtitle_ = tuple([x.lower() for x in title.split(\" \") if(x)])\n\t\tsubtitle_ = str.join(subtitle_, '') # Remove leading and trailing whitespace\n\n\t\ttry:\n\t\t\tdus = document[\"dus\"]\n\t\texcept KeyError:\n\t\t\traise ValueError(f\"{title}: Invalid Dus.\")\n\n\t\t# If no author, return default\n\t\tif dus is None:\n\t\t\treturn None\n\t\telse:\n\t\t\tassert"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tjson = loads(document)\n\n\t\tif \"type\" in list(json.keys()):\t\n\t\t\ttype = \"JSON\"\n\t\telse:\n\t\t\t# TODO: use the type field or default here\n\t\t\ttype = None\n\n\t\treturn {\n\t\t\t\"name\": json[\"name\"],\n\t\t\t\"category\": json[\"category\"],\n\t\t\t\"description\": json[\"description\"],\n\t\t\t\"icon\": json.get(\"icon\", \"https://diaspora.github.io/diaspora_federation/static/img/icons/\" + json[\"icon\"]),\n\t\t\t\"icon_url"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\n\n\tdocument = json.loads(document)\n\n\tdocument = json.loads(document)\n\tdocuments = document[\"results\"]\n\n\tfor index, document_file in enumerate(documents):\t\n\t\tfor file_file in document_file[\"files\"]:\n\t\t\t\n\t\t\tdoc = {}\n\t\t\ttry:\n\t\t\t\tdoc[\"name\"] = \"{}.json\".format(file_file[\"title\"])\n\t\t\t\tdoc[\"id\"] = str(file_file[\"id\"])\n\t\t\t\tdoc[\"type\"] = \"application/json\"\n\t\t\t\tdoc[\"description\"] = file_file[\"description\"]\n\t\t\t\tdoc[\"start"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\t# JSON is a bit tricky. We don't use {{\"a\": \"b\"}, \"d\": {\"e\": \"f\"}, \"x\": {\"t\": \"e\"}}, and {\"a\": {\"b\": 1}} does not. \n\t\tparsed = json.loads(document, object_pairs_hook=OrderedDict)\n\t\tif any(['_from' in x for x in parsed.keys()]):\n\t\t\treturn {str(k): float(parsed[_from]) for k in parsed}\n\t\telse:\n\t\t\treturn parsed\n\telse:\n\t\treturn document\n\n\n# TODO: \n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\twith open(document) as input_file:\n\t\t\treturn {key: parse_diaspora_webfinger(value) for key, value in yaml.load(input_file).items()}\n\traise AttributeError(\"Webfinger not found\")\n\ndef set_time_zone(document: str) -> Dict:\n    \"\"\"Set time zone.\n\n    The time zone data is in the following form:\n\n        TIME_ZONE=\"Zacate\".\n    \"\"\"\n    raise AttributeError(\"webfinger not found\")\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tif document.strip() == \"\":\n\t\t\tpass\n\n\t\tdocument_json = json.loads(document)\n\n\t\treturn document_json['d2i']\n\n\ndef parse_fronta_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Fronta webfinger which is either in JSON format (old) or XRD (old).\n\nhttps://fronta.github.io/fronta_federation/discovery/webfinger.html\t\"\"\"\n\t\tif document.strip() == \"\":\n\t\t\tpass\n\n\t\tdocument_json = json."}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tresponse = _fetch_webfinger_document_or_local(handle)\n\t\ttry:\n\t\t\treturn response['document']\n\t\texcept KeyError:\n\t\t\treturn None\n\traise Error(\"get_webfinger_document() did not detect a document in O365 format.\")from django.core.validators import MinValueValidator\nfrom django.db import models\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom cms.models import CMSPlugin\n\n\nclass PageAdmin(CMSPlugin):\n    \"\"\"\n    Permission plugin to add/edit/delete items to a Page's admin."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\t# https://docs.google.com/document/d/1W0I4R-Xx-J-Hg9_rZ5-H0-BwKoE-jDhxF-Z/edit#gid=5\n\t\t\twebfinger_url = \"http://www.google.com/webfinger/annotations/\" + handle + \"/\" + DOC_IDENTIFIER\n\t\t\t#print(\"Attempting to retrieve an existing Webfinger ID for user %s\" % (handle))\n\t\t\t# http://stackoverflow.com/questions/3789"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\trequest = Request(query={\"view\": \"static/views/webfinger.html\"})\n\t\trequest.add_header(\"Content-Type\", \"text/xml\")\n\t\tresponse = urlopen(request)\n\n\t\tif response.getcode() < 200 or response.getcode() > 206:\n\t\t\tlogging.info(\"The page was not properly rendered.\")\n\t\t\treturn None\n\n\t\treturn response.read().decode('utf-8')\n\n\t@staticmethod\n\tdef verify_doc_structure(doc: str) -> Type[str]:\t# type: ignore\n\t\tif doc is None:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif is_remote_server():\n\t\t\tlogger.warning(\"[!] We are behind a webfinger proxy, not releasing webfinger.\")\n\t\telse:\n\t\t\treturn get_document_data(handle, \"_webfinger_document\")\n\treturn None\n\n\nasync def get_document_data(handle: str, force_rebuild: bool=False) -> Optional[Dict[str, Any]]:\n\t\"\"\"\n\tTry to retrieve a RFC7033 document. This can return a json document if\n\tthe document content cannot be loaded e.g. because it is not a"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn handle[:3] == 'GIF:' and ouiui.get_webfinger_handle(handle.encode('ascii'))!= \"\"\n\t\texcept:\n\t\t\treturn \"\"\n\n\tdef try_retrieve_google_drive_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7032 Google Drive document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn ouiui.get_google_drive_document_handle(handle.encode('ascii'))!= \"\"\n\t\texcept:\n\t\t\treturn \"\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn file_to_string(handle, errors='replace').replace('http://', 'https://')\n\t\texcept ValueError:\n\t\t\terror_message = \"Invalid webfinger document\"\n\t\t\tif handle!= 'webfinger':\n\t\t\t\terror_message += \".\"\n\n\t\t\tlogger.warning(error_message)\n\t\t\traise Exception(error_message)\n\n\tdef get_webfinger_page_content(handle: str) -> Element:\t\t\"\"\"\n\tTry to retrieve the content of the webfinger page. Returns None if it has\n\tno content yet"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tdoc = fsm.open(handle)\n\t\t\tdocument = doc.read()\n\t\texcept (OSError, IOError):\n\t\t\treturn None\n\treturn document\n\ndef try_retrieve_webfinger(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document from the file location on the\n    filesystem. This function is a shortcut to try_retrieve_webfinger_document(),\n    but it is not intended for general use (i.e. only for WebFinger documents).\n    \"\"\"\n    try:\n        doc = fetch_"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn handle.read().decode('utf-8')\n\t\texcept UnicodeDecodeError:\n\t\t\treturn None\n\treturn None\n\ndef get_page_source_from_link(link: str) -> Optional[str]:\t\t\"\"\"\n\tGiven the link of a page, retrieves the source code of the page if it exists, and if not\n\tfound returns None.\n\t\"\"\"\n\ttry:\n\t\treturn urlopen(link)\n\texcept (IOError, Exception) as err:\n\t\tlogging.exception(\"Unable to retrieve page source code: {0}\".format(link"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tcontent = get_content(handle)\n\tif content is not None:\n\t\tcontent = content.encode()\n\treturn content\n\ndef get_content_type_from_mimetype(mimetype: str) -> str:\n\t\"\"\"\n\tTry to determine content type by parsing the first few bytes of the mimetype.\t\t\"\"\"\n\tmimetype_content_types = _parse_mimetype_content_types(mimetype)\n\tif len(mimetype_content_types) == 0:\n\t\traise UnknownContentTypeException('The content type could not be parsed as valid.')\n\n\t# If no content"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif handle == 'RFC7033':\n\t\t\thandle = 'docwiki'\n\t\t\n\t\ttry:\n\t\t\traw_text = wikipedia_webfinger.GetPage(handle)\n\t\texcept KeyError:\n\t\t\treturn None\n\t\texcept wikipedia.IsRedirectPage:\n\t\t\treturn raw_text\n\t\texcept wikipedia.IsNotRedirectPage:\n\t\t\treturn raw_text\n\n\t\treturn raw_text\n\n\ndef try_post_message(handle: str, msg: str) -> None:\n\t\"\"\"\n\tTry to post message to Google's Tumblr API. Does not send"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t return {d['text'] : d for d in handle.data.getchildren() if d.name == (NS.DADIAC_HANDLE, None)}\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCopyright (C) 2013 Dariusz Suchojad <dsuch at zato.io>\n\nLicensed under LGPLv3, see LICENSE.txt for terms and conditions.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\n# stdlib\nimport logging\n\n# anyjson\nfrom celery import shared_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n\t\n\t#TODO: add check to find out what fields I have\n    fields = {}\n\tfor field, schema, description in handle.fields:\n\t    # Ignore the field if it's just a dot or a comma\n\tif '.' in field:\n\t    field_name, schema = field.split('.')\n\t    field_type, table_name, field_desc = field_desc.split(' ', 1)\n\t    if (schema!= 'public' and schema == 'pg_catalog' and table_name == 'pg_catalog'):\n\t\tfields[field_name] = (field_type, table_name"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\ttry:\n\t\t\thandle.open()\n\t\texcept pycurl.error:\n\t\t\t# FIXME?\n\t\t\traise\n\t\tfile = handle.extractfile('diaspora_webfinger.pdf')\n\t\treturn _parse_diaspora_webfinger_pdf(file.read())\n\telse:\n\t\treturn None\n\ndef retrieve_and_parse_diaspora_webfinger_pdf(handle_or_str):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document using a urllib2 module and return a file"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n\timport subprocess\n\treturn_code = subprocess.call([get_webfinger_wrapper(), '-v'])\n\treturn dict(handle = handle, return_code = return_code)\t\n\n\ndef retrieve_webfinger(handle)\t:\t\n\t\"\"\"\n\tRetrieve a document from the specified remote Pandora Webfinger server.\n\n:arg handle: Remote handle to retrieve\n:returns: Document\t\"\"\"\n\timport requests\n\ttry:\n\t\treturn requests.get('http://'+server+'/pandora/ws/webfinger', verify=False,"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\t# TODO: fix these\n\tdvi_id = handle.get_id()\n\tdvi_handle_handle = handle\n\tdvi_doc_type = handle.get_record_type()\n\tsession_o = session.SharedObject(dvi_doc_type, dvi_handle_handle)\n\tsession_o.ReadDocument(handle)\n\n\tresult = handle.read_dvi_document()\n\n\tif True: # FIXME: test when using local db?\n\t\tdocument_id = result['document_id']\n\t\tdocument_handle = session"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n        response = urllib2.urlopen(URI+\"/WebFinger/\"+handle)\n        data = response.read()\n        return_dianewebfront_data(data)\t\t\n               \n\ndef retrieve_dianewebfront_data(data):\n        \"\"\"\n\tParse a remote Diaspora webfront document into a dictionary.\n    \n\t:arg data: the webfront document to parse\n:returns: parsed document \n\t:returns: None\n\t:raises: ParseError\"\"\"\n\ndef delete_dianewebfront_data(handle):\n        \"\"\"Remove a remote Diaspora webfront file"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\ttry:\t\t\timport webfinger, urllib2\n\t\t\timport rdflib\n\t\texcept ImportError: \t\timport rdflib\n\t\t\n\t\tr = rdflib.URIRef(handle)\n\t\trdflib_version = '1.0'\n\t\tr.parse('http://purl.org/dc/terms/', 'graph', format='xml')\n\t\t\n\t\tif r.info().get('version')!= rdflib_version:\n\t\t\t\t\t\t\t\traise Exception(\"Could not retrieve Diaspora webfinger document (version = %s). Original WebFinger server is version %s.\" % (r."}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\turl = handle.parsed[0]\n\t\tdoi = handle.parsed[3]\n\t\treturn [handle, url, str(doi)]\n\"\"\"\n\"\"\"\n\nimport re\nfrom malcolm.expression.expression import Expression\nfrom malcolm.core.symbexec import *\n\ndef is_expression_call(sym):\n    return not isinstance(sym, UnaryOp) and not isinstance(sym, Variable)\n\nclass Function(Expr):\n    \"\"\"\n    Expression as the basic unit of a function call.\n\n    A Function can not be a function, and could be"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tdiaspora_handle = DiasporaHandle()\n\t\tdiaspora_handle.handle = handle\n\t\tresponse = diaspora_handle.retrieve_and_parse(\"https://www.daspora.com/webfinger/documents/webfinger_{doc_name}.htm\")\t\t\t\n\t\t# if the document does not exist it is created\n\t\tif response is None:\n\t\t\tresponse = create_document(handle)\t\n\n\texcept ValueError, error:\n\t\twith open('errors.txt', 'a') as f:\n\t\t\tf.write("}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn _search_dia() \\\n\t\t      .map(lambda d: DiasporaWebfinger(handle, int(d.attrib['id']))).\\\n\t\t       fetch_document(count=1000)\n# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\thost_url = HOSTS_HOST_URL % host\n\n\t\tresponse = _fetch_json(host_url, {}, True)\n\n\t\t# build a list of all the hosts with the latest Diaspora host-meta, and the\n\t\t# diaspora meta xml file for this host and the latest version.\n\t\t# we have to take care to have all the hosts with the latest version.\n\t\tparser = xml.etree.ElementTree.XMLParser()\n\t\tresponse.extend(parser.fromstring(response_body))\n\n\t\t# only return the first one\n\t\treturn response["}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t # TODO: make sure this function doesn't throw error, no API call\n\t '''\n\t\tReturns a remote Diaspora host-meta document (if any)\n\t\t'''\n\t\ttry:\n\n\t\t\twfd = get_wfd_json(host)\n\t\t\tfor dia in wfd['wd']:\n\t\t\t\tif dia['host'] == host:\n\t\t\t\t\treturn dia\n\t\t\t\telif dia['diaspora_host'] == host:\n\t\t\t\t\treturn dia['content']\n\t\t\treturn ''\n\t\texcept:\n\t\t\treturn ''\n\n\ndef retrieve_all_hosts(limit"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\trequest = DiasporaUtils.createRequest(host)\n\t\tresponse = DiasporaUtils.sendRequest(request)\n\n\t\tlogger.debug(\"Retrieved Diaspora host-meta document \")\n\t\treturn DiasporaUtils.loadDiasporaObjectFromFile(response, host, \"XRD\")\nfrom datetime import datetime, timedelta\n\nfrom django.db import models\n\n# Create your models here.\n\nclass TimeFrame(models.Model):\n    pub_date = models.DateTimeField(max_length = 2)\n    foo_attr = models."}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turl = host[\"host_meta_url_string\"].format(host[\"host_id\"])\n\t\treturn self._http_request(method=\"GET\", url=url).parsed_content\n\n\tdef retrieve_site_settings(site_id):\t\"\"\"\n\tRetrieve the list of sites for the site in the host-meta document.\n\n:arg site_id: Site ID for the site to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turls = (\n\t\t\t\"{server}/host_meta/{site_id}/sites?auth={token}\".format(\n\t\t\t\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\ttry:\n\t\t\tresp = requests.get(__db_url__(host)).json()\n\t\texcept Exception:\n\t\t\treturn None\n\t\treturn resp\n\n\t@classmethod\n\tdef insert_host(cls, host_name, port_id, name, description=None, \n\t\t\t\t\tdescription_html=None):\n\t\t\"\"\"Insert the data to the datatree.\"\"\"\n\t\ttry:\n\t\t\treq_params = \\\n\t\t\t\t{'name': name, 'node_id': port_id, 'description': description, \n\t\t\t\t\t'title': description_html}\n\t\t\tdata = {\"server"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn get_xrds_session().get_data(url=daum_url + 'host/' + host,\n                                                 username=daum_username,\n                                                 password=daum_password,\n                                                 allow_redirects=False).text\n\ndef retrieve_duxiliary_host_meta(host):\t\"\"\"\n\tRetrieve a remote Duxiliary host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn get_xrds_session().get_data(url=daum_url + 'host/' + host,"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn DiasporaHost.from_json_body(get_request('get',\n\t\t\t'//host-meta/host-meta:host/' + host))\n\n\n@app.route(\"/hosts/diaspora/<string:host>\", methods=[\"GET\"])\ndef retrieve_hosts_diaspora(host):\n\t\"\"\"\n\tRetrieve data from Diaspora.\n\n\t:param host: Host to retrieve from\n\t:returns:  JSON result\n\n\t:raises DiasporaRequestError: If the data could not be located.\n\t:raises Diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tresp = Diaspora.connection.request('/Diaspora/host/%s' % host)\n\t\tif 'error' in resp and resp['error'] is not None:\n\t\t\tres = json.loads(resp['error'])\n\t\telse:\n\t\t\treturn res\n\telse:\n\t\treturn None\n\ndef retrieve_diaspora_xml(xml):\t\"\"\"\n\tRetrieve a remote Dia configuration document \n\tstream provided by a Diaspora XML document.\n\n\t:arg xml: Dia configuration document as received from Diaspora\n\t\"\"\"\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\nclass DataStore:\n\t\"\"\" A class for reading and saving data. \"\"\"\n\t\n\tdef __init__(self, host='localhost', port=5984, user='admin', passwd=''):\n\t\t\"\"\" Constructor.\n\n\t\t:arg host:\n\t\t\tHostname to connect to.\n\n\t\t:arg port:\n\t\t\tPort to connect to.\n\n\t\t:arg user:\n\t\t\tUser to connect as.\n\n\t\t:arg passwd:\n\t\t\tPassword to connect as.\n\n\t\t\"\"\"\n\t\t\n\t\tself.host = host\n\t\tself.port = port\n\t\tself.username = user\n\t\tself.passwd = passwd"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tlogging.debug(\"Checking if Diaspora host: {0}\".format(host))\n\t\txml_tree = retrieve_xml(host)\n\t\tif xml_tree is None:\treturn\n\t\troot = et.fromstring(xml_tree).find('{{root}}database-system/{%s}database-model' % DRIVERA_NS).text\n\t\ttry:\n\t\t\treturn root.find(NAMESPACE+\"{%s}database-model/{%s}host-meta\" % (NAMESPACE, DRIVERA_NS))['attributes']\n\t\texcept:\n\t\t\t#raise"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not method in HTTP_METHODS:\n\t\t\tif data and isinstance(data, basestring) and hasattr(data, \"read\"):\n\t\t\t\tdata = cgi.parse_qsl(data)\n\t\t\telif data and hasattr(data, \"form\"):\n\t\t\t\tdata = cgi.parse_qs(data)\n\t\t# TODO: Add exception checking and add more information to res.response\n\t\tlogger.debug('Sending %s %s', method, url)\n\t\tif isinstance(data, basestring) and hasattr(data, \"read\"):\n\t\t\tdata = cgi.parse_qsl(data)\n\t\t\tdata"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tfull_method = \"POST\" \n\t\ttry:\n\t\t\treturn_value = requests.post(url, data=data, headers=get_headers(timeout))  # if error (and if timeout)\n\t\t\tif return_value.status_code in [requests.codes.ok, requests.codes.created, requests.codes.accepted]:\n\t\t\t\treturn return_value, full_method\n\t\t\telse:\n\t\t\t\treturn (return_value, None)\n\t\texcept requests.exceptions.RequestException as error:\n\t\t\tprint(error)\n\t\t\ttraceback.print_exc()\n\t\t\treturn ("}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif data is not None and method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout)\n\t\telif data and method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout)\n\t\t\tif r.status_code == 200:\n\t\t\t\t#print \"[DEBUG] status code is success\"\n\t\t\t\tl = r.text.encode(\"utf8\")\n\t\t\t\tif l.startswith(\"4\"):\n\t\t\t\t\treturn 10001\n\t\t\t\telif l.startswith(\"5\"):\n\t\t\t\t\t#print \"[WARNING] server returned error code"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif data!=None:\n\t\t\theaders = {\"Content-type\": \"application/x-www-form-urlencoded\"}\n                        http.request(method, url, timeout=timeout, headers=headers)\n                        return parse_response(http, data, method)\n\n\t\treturn None, None, None, None\n\n\ttry:\n\t\tresponse = http.post(url, data, timeout=timeout, **kwargs)\n\t\treturn response\n\texcept Exception as e:\n\t\tif timeout > 0:\n\t\t\ttime.sleep(timeout)\n\t\traise e\n\ndef encode_document(document, to"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\t\n\t# Try the raw data first.\n\trequest = requests.post(url, data=data, verify=False, **kwargs)\n\n\t# If we get a timeout, it might be that it has been reached.\n\tif request.status_code >= 400:\n\t\t# If the status code isn't 2xx, we need to retry 3 times\n\t\tretries = 3\n\t\twhile (retries > 0):\n\t\t\ttry:\n\t\t\t\t# Send the request.\n\t\t\t\tr = request.status_code\n\t\t\t\n\t\t\texcept requests.exceptions.Timeout as e:\n\t\t\t\tif e.response."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\trequest_kwargs = {\n\t\t\t\"timeout\" : timeout,\n\t\t\t\"data\" : data,\n\t\t\t\"verify\" : False\n\t\t}\n\t\trequest_kwargs.update(kwargs)\n\t\turl = \"%s%s?%s\" % (\n\t\t\tself.build_url_protocol(),\n\t\t\turl,\n\t\t\turlencode(request_kwargs)\n\t\t\t#urlencode(request_kwargs, True)\n\t\t\t#urlencode(request_kwargs, True)\n\t\t)\n\n\t\trequest_kwargs[\"timeout\"] = timeout\n\t\trequest_kwargs[\"verify\"] = False\n\n\t\tresponse = self."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\ttry:\n\t\t\tif method == \"get\":\n\t\t\t\treturn self.http.request(\"POST\", url, urllib.urlencode(data), timeout=int(timeout), **kwargs)\n\t\t\telif method == \"post\":\n\t\t\t\treturn self.http.request(\"POST\", url, urllib.urlencode(data), timeout=int(timeout), **kwargs)\n\t\texcept Exception, ex:\n\t\t\tif ex.args[0] == \"timed out waiting for response\":\n\t\t\t\tif \"timed out\" in ex.args[1]:\n\t\t\t\t\traise\n\t\t\t\telse:\n\t\t\t\t\traise ex\n\t\t\telse:"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\t\n\t# Check for a timeout\n\tif timeout >= 400:\n\t\ttimeout = 400\n\t\n\t# If this request is a POST request, wait for a bit\n\tis_post_request = (method == 'post')\n\t\n\t# Connect to server\n\ts = socket.socket()\n\ts.connect((url, int(port)))\n\n\t# Use a custom error handler\n\ttry:\n\t\tresp = s.getresponse()\n\t\terror = resp.reason\n\texcept (socket.error) as e:\n\t\terrors['Error'] = str(e)\n\t\treturn ({},"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif \"data\" not in data:\n\t\t\traise ValueError(\"Data was required.\")\n\t\tconnection = httplib.HTTPConnection(url)\n\t\tif timeout is None:\n\t\t\tconnection.connect()\n\t\t\tconnection.request(\"POST\", url)\n\t\t\t\n\t\telse:\n\t\t\tconnection.request(\"POST\", url, urlencode(data))\n\t\t\n\t\ttry:\n\t\t\tconnection.getresponse()\n\t\texcept (httplib.HTTPException, socket.error) as e:\n\t\t\tconnection.close()\n\t\t\tlogger.error(\"Error sending document %r to %r error: %r\" % (url, url, e))"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n\n\t# build the request\n\tr = Request(method, url)\n\n\t# send an HTTP request and return response\n\tr.prepare(timeout=timeout)\n\tr.send(data, timeout)\n\n\t# return the result\n\treturn (r.getresponse(), r.getresponse())\n\nclass ApiException(Exception):\n\t# no doc found\n\tdef __init__(self, message):\n\t\tself.message = messagefrom __future__ import unicode_literals\nimport logging\nimport subprocess\nimport sys\n\nfrom rbtools.api.errors import APIError\nfrom rb"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif isinstance(val, dict):\n\t\t\tindex = {}\n\t\t\tfor k in val:\n\t\t\t\tindex[k] = dict_insert(val[k], k, *keys)\n\t\t\treturn index\n\t\telse:\n\t\t\treturn set([k for k in val if k not in dict_get(val, d)])\n\n\n# -----------------------\n# file saving\n# -----------------------\ndef save(data, filename):\n\tif os.path.isfile(filename):\n\t\tos.unlink(filename)\n\tfp = open(filename, 'w')\n\tif not isinstance(data"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic[key] = _list_append_value(dic[key], val, keys)\n\n\tnested_dict.update({key : dic[key]})\n\treturn nested_dict\n\ndef list_remove(dic, lst):\n\tkeys = dic.keys()\n\tkeys.remove(lst[0])\n\ndef _list_pop_value(dic, key, value):\n\treturn value\n\n\ndef dict_pop(dic, key, *keys):\n\t\"\"\"\n\tpop and return a key value \n\nto take the value of a nested key, all keys"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic[key] = {}\n\t\tfor a in keys:\n\t\t\tif isinstance(val, dict):\n\t\t\t\tdic[key][a] = val.get(a, {})\n\t\t\telse:\n\t\t\t\tdic[key][a] = val\n\n\ndef get_nested_keys_from_dict(dic):\n\t\"\"\"\n\tget the values of nested keys from a dictionary\n\n\t:param dic: a dictionary object to search for nested keys and return values\n\t:return: a list with the nested keys\n\t\"\"\"\n\treturn [_ for _ in dic if _ not in keys]\n\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tnew_dic={}\n\tfor key in keys[:-1]:\n\t\tnew_dic[key]=dic[keys[-1]]\n\t\tnew_dic[keys[-1]].insert(dic[keys[-1]], value=val)\n\treturn new_dic\n\n\ndef format_index(index, n_entries, label=None, prefix=\"\"):\n\t\"\"\"\n\tmake a string of indices, each an integer in a given list of entries of\n\tdifferent length.\n\n\t:param index: the first index of an integer in a list of entries\n\t:param n_entries: the total"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n\n\tif isinstance(dic, dict):\n\t\tfor key, val in dic.items():\n\t\t\tif not isinstance(val, str):\n\t\t\t\traise YeditException(\"error: key (%s) must be a string\" % str(key))\n\t\t\tif key not in keys:\n\t\t\t\tkeys.append(key)\n\t\t\tif val not in dic:\n\t\t\t\tdic[val] = []\n\t\t\tdic[val].append(key)\n\n\telif isinstance(dic, list):\n\t\tfor val in dic:\n\t\t\tif not isinstance(val, str):\n\t\t\t\traise YeditException(\""}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n\n\"\"\"Utilities for reading and writing FLIGHT archive tar.gz files.\n\nXXX references to utf-8 need further investigation.\nXXX more info at http://flatiter.org/flatiter/format-spec.html\n\"\"\"\n\n# Copyright (C) 2009-2012 Darcy Mason\n# This file is part of pyculqual\n#\n# pyculqual is free software; you can redistribute it and/or modify it under the\n# terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation; either version 2.1 of the"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\td_key = key\n\t\tfor k_key, v_key in keys:\n\t\t\tif k_key is not None:\n\t\t\t\td_key += '.' + k_key\n\t\t\td_key += '.' + v_key\n\t\tif d_key is not None:\n\t\t\tdic[d_key] = val\n\t\telif key in dic:\n\t\t\tdic[key] = dic[key]\n\n\telif key == 'dict':\n\t\treturn [dict_insert(dic[x], x, key, *keys) for x in xrange(0, len(keys))]\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif key in dic:\n\t\t\tdic[key].insert(val)\n\t\telse:\n\t\t\tdic[key] = { val: {} }\n\treturn dic\n\n#\n#   Splitter\n#\nclass Splitter:\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, key, minSize):\n\t\t\"\"\"\n\t\tkey can be int - or str\n\t\t\"\"\"\n\t\tself.key = key\n\t\tself.minSize = minSize\n\t\tself.items = []\n\n\tdef maxSplit(self):\n\t\t\"\"\"\n\t\tmax split value into two halves"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif key in dic:\n\t\t\tdic[key].append(val)\n\t\telse:\n\t\t\tdic[key] = [val]\n\ndef dict_get(dic, key):\n\tfor key, val in dic.iteritems():\n\t\tif dict_get(val, key) == {}:\n\t\t\tdel dic[key]\n\nimport os\nimport re\nimport sys\nimport time\nimport argparse\nimport random\nfrom sets import Set\nimport os.path\n\n#TODO try to get the time\ntry:\n    my_time = time.time  # py2\nexcept AttributeError"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif key in dic: return\n\tdic[key] = val(*(keys + tuple(keys)))\n\treturn len(dic)\n\ndef list_pop(dic, field):\n\t\"\"\"\n\tpop the first (key, value) from a list in a dictionnary\n\t\"\"\"\n\tlast = dic.pop(field)\n\tdel dic[field]\n\tkey, value = dic.pop(field)[0]\n\treturn last, value\n\ndef dict_popitem(dic):\n\t\"\"\"\n\tremove and return some (key, value) pair from"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tif isinstance(settings_dirs, list):\n\t\t\tsettings_dirs = settings_dirs # make a mutable list\n\t\tbase_dir = settings_dirs[0]\n\n\t\tif spec_option is not None:\n\t\t\tspec_option = settings_dirs[spec_option]\n\t\tif self.verbose >= 2:\n\t\t\tprint \"\\tParsing file directory {0}\".format(base_dir)\n\n\t\tdata_file0 = os.path.join(base_dir, \"%08i.data\" % self.out_prefix)\n\t\tdata_file_1 = os.path."}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfiles = []\n\t\tfor settings in settings_dirs:\n\t\t\tif settings[\"file_name\"] in spec_option:\n\t\t\t\tfiles.append(settings[\"file_name\"])\n\n\t\treturn files\n\n##~~ create custom file names   ######\ndef create_custom_file_names(settings_dirs, settings):\n\tfiles = settings_dirs[int(settings_dirs[0])].get(\"custom_file_names\")\n\tif files:\n\t\tif settings.has_key(\"custom_file_names\"):\n\t\t\tdata = dict(zip(files, settings[\"custom"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tsettings_dir = os.path.join(settings_dirs[0], spec_option)\n\t\tif not os.path.exists(settings_dir):\n\t\t\tos.makedirs(settings_dir)\n\t\treturn os.path.join(settings_dir, 'complex-types.txt')\n\telse:\n\t\treturn 'INI_TYPES'\n\ndef _init():\n\titype_map = {}\n\tfor module_dir in settings_dirs:\n\t\tsys.path.append(module_dir)\n\t\titypes = ItypeMap.read_itypes"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfiletype_dirs = [os.path.join(settings_dirs[0], f) for f in os.listdir(settings_dirs[0]) if '.i' not in f]\n\t\tfiletype_name = 'i' + filetype_dirs[0]\n\t\treturn load_standard_types(filetype_name, spec_option, **settings_dirs[0])\n\telse:\n\t\treturn load_std_list(opts.name, spec_option)\n\t\t\t\ndef load_std_list(filename, spec_option, **settings_dirs):\n\tglobal arraytypes\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\t# Make file names of all of the input files\n\t\tfilenames = list()\n\t\tfor setting_dir in settings_dirs:\t\n\t\t\tif spec_option!= 'default':\n\t\t\t\tsetting_name = os.path.basename(setting_dir)\n\t\t\t\tsettings_fname = os.path.join(setting_dir,'settings.py') \n\t\t\t\tif not os.path.exists(settings_fname):\n\t\t\t\t\tmsg = \"Could not find file {}\".format(setting_name)\n\t\t\t\t\traise IOError(msg)\n\t\t\t\tfilenames.append(settings_fname)\n\n\t\tfname = ''"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tassert(isinstance(settings_dirs, dict))\n\t\tsettings_dirs = normalize_dirs(settings_dirs)\n\n\t\tcomplex_type_name = 'OutiType'\n\t\tfor dirname in settings_dirs:\n\t\t\tcomplex_type_name += '-'+dirname\n\n\t\ttry:\n\t\t\tnew_complex_type = ComplexType(complex_type_name, settings_dirs, spec_option)\n\t\texcept TypeError:\n\t\t\tpass\n\t\telse:\n\t\t\tassert(isinstance(new_complex_type, BaseComplexType))\n\t\t\tassert(new_complex_type.is_valid"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\t\n\ttype = None\n\t\n\tif settings_dirs == None:\n\t\tsettings_dirs = [os.path.join(current_dir, 'indext').replace(\"\\\\\",\"/\") + \"/\"]\n\t\tsettings_dirs.insert(0, os.path.join(\"..\", \"indext\"))\n\tif spec_option == \"spec\":\n\t\ttype = \"spec\" # use default\n\t\tdefault_settings = {\"spec_type\": \"custom\", \"spec_title\": None, \"spec_description\": None, \"spec_license\": None}\n\telif spec_option =="}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\t\n\tos.chdir(os.path.join(settings_dirs.source_dir, 'inispec'))\n\tos.system('mkdir *.so')\n\tos.system('cp *.so.')\n\tos.chdir('src/spec')\n\t\n\t# Create the spec file \n\tos.system('cp *.so *.src')\n\tos.system('cp *.so *.b.src')\n\tos.chdir('src/spec')\n\tos.system('cp../libs/spec.v0/*.cs')\n\tos.system('cp../libs/spec."}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfor settings_dir in settings_dirs:\n\t\t\tcreate_directory(settings_dir, settings_option)\n\t\t\tcreate_file_handler(os.path.join(settings_dir, spec_option+'_'+settings_dir.lower()), os.path.join(settings_dir, spec_option + '_'+settings_dir.lower()), 'inif.in')\n\nclass IniType:\n\tdef __init__(self, input_file_names, output_file_name, spec_option):\n\t\tself.inputs = []\n\t\tself.outputs = []\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tnew_file_names = []\n\t\tnew_file_names.append('.ini')\n\t\tnew_file_names.append('.ini2')\n\t\tprint '#'*len(new_file_names)\n\t\tfor dir in settings_dirs:\n\t\t\tsearch = False\n\t\t\tfor file in glob.glob(os.path.join(dir, spec_option + '*.ini')):\n\t\t\t\tif file.endswith('.ini'):\n\t\t\t\t\tnew_file_names.append('.ini')\n\t\t\t\t\tsearch = True\n\t\t\t\t\tbreak\n\t\t\tif search == True:\n\t\t\t\tprint '#'*len(file"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tmodule = cls._conf.find_by_name(\"module\")\n\t\tif module:\n\t\t\treturn module()\n\t\telse:\n\t\t\tlogSys.warning(\"configmanager [%s] is broken\" % cls._conf.get_section_name(\"module\"))\n\t\t\traise Exception(\"configmanager [%s] is broken\" % cls._conf.get_section_name(\"module\"))\n\t\nglobal_config_mgr = None\ndef get_global_config_mgr():\n\tglobal global_config_mgr\n\treturn global_config_mgr\n\nclass ConfigManagerException(Exception):\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tapp_modules = find_app_apps(True)\n\t\treturn modules.get(config_manager_name(app_modules, 'production'), None)\n\n\t@classmethod\n\tdef register(cls, config_manager_name, config_manager_classes):\n\t\t\"\"\"\n\t\tRegisters a new config manager.\n\t\t\"\"\"\n\t\tapplication = AppBuilder().make()\n\t\t# Add the application to the global config manager.\n\t\tregistry = cls.config_manager_builders.register(application)\n\t\tconfig_manager = registry.instantiate(config_manager_name, **configuration"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.plugin_manager.get_plugin_configuration('ansible')\n\n\t@classmethod\n\tdef _plugin_manager(cls):\n\t\t\"\"\"\n\t\tReturns the plugin manager, to which all\n\t\tplugins are subtracted to create.\n\t\t\"\"\"\n\t\treturn PluginManager(plugin_manager=PluginManager())\n\n\t@classmethod\n\tdef register_plugin(cls, plugin=None, plugin_name=None, plugin_args_callback=None):\n\t\t\"\"\"\n\t\tFactory method for registering an new plugin. The plugin will\n\t\tbe registered via the `plugin_manager`"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_config_manager\n# -*- cpy-indent-level: 4; indent-tabs-mode: nil -*-\n# ex: set expandtab softtabstop=4 shiftwidth=4:\n#\n# Copyright (C) 2008,2009,2010,2011,2013  Contributor\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\n\t\n\treturn ConfigManager()\n\n\n\ndef cli_main():\n\t\"\"\"\n\tMain\n\t\"\"\"\t\n\t\n\t# Get command line arguments\n\targ_parser = argparse.ArgumentParser()\n\t\"\"\"\n\tparser = arg_parser.add_argument_group('global arguments')\n\t\"\"\"\n\tgroup1: CLI variable (optional)\n\t\"\"\"\n\t\n\t# Get command line arguments\n\targ_parser.add_argument('--host', \n\thelp='Server IP/Hostname', type=str, required=True)\n\t\n\t\"\"\"\n\tgroup2: CLI variable (required, for the CLI"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tglobal ansible_config_manager\n\t\treturn ansible_config_manager\n\n\tdef get_inventory(self, **kwargs):\n\t\treturn self.inventory\n\n\tdef get_config_data(self, **kwargs):\n\t\treturn self.cloud_config_manager.get_config_data(**kwargs)\n\n\n#\n# Authors: Travis Oliphant, Ed Schofield, Robin Dunn, Thomas Sargent,\n#          Andreas Mueller\n#\n# License: BSD\n\nimport numpy as np\n\nfrom tick.base import OneHotMixin"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tconfig_types  = {'mysql': MySQLConfigManager,  \\\n\t\t\t\t'oracle': OracleConfigManager,  \\\n\t\t\t\t'postgres': PostgresConfigManager,  \\\n\t\t\t\t'db2': DatabaseConfigManager,  \\\n\t\t\t\t'firebird': FirebirdConfigManager,  \\\n\t\t\t\t'mssql':MSSQLConfigManager,  \\\n\t\t\t\t'smtpd':SMTConfigManager,  \\\n\t\t\t\t'db':DB2ConfigManager,  \\\n\t\t\t\t'postgres_ext': PostgreConfManager}\n\t\tm = sys.modules[cls.__module__]\n\t\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tif not hasattr(cls,'ansible_config_manager'):\n\t\t\tmodule_name = cls.__module__\n\t\t\tclass_name = cls.__name__.split(\".\")[-1]\n\n\t\t\tmodule = sys.modules[module_name]\n\n\t\t\tcls.ansible_config_manager = importlib.import_module(\"{module}.{class_name}\".format(module=module_name, class_name=class_name))\n\n\t\t\t# reload the config manager to apply the parameters to the class object\n\t\t\tcls.ansible_config_manager.module_types = dict(cls.ansible_config_manager.get"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.get('ansible_config_manager', cls.inventory)\n\n\t@classmethod\n\tdef get_config_paths(cls):\n\t\t\"\"\"\n\t\tGets all config paths for the local machine.\n\n\t\tReturns:\n\t\t\tThe paths to config files.\n\n\t\t:return:\n\t\t\tThe paths.\n\t\t\"\"\"\n\t\treturn cls.get('local_conf_path', get_config_root())\n\n\t@classmethod\n\tdef get_service_conf_paths(cls):\n\t\t\"\"\"\n\t\tGets all config paths configuration for the local machine.\n\n\t\t:return:\n\t\t\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_config_manager\n\n\t@classmethod\n\tdef get_config_manager(cls):\n\t\t\"\"\"\n\t\tReturns the Configuration Manager.\n\t\t\"\"\"\n\t\treturn cls.__get_instance('config_manager')\n\n\t@classmethod\n\tdef load(cls, yaml, **kwargs):\n\t\t\"\"\"\n\t\tLoads a ConfigurationManager from a YAML file.\n\t\t:param yaml: An optional YAML document to execute\n\t\t:param kwargs: Optional keyword arguments, will be passed directly to the YAML file\n\t\t:return: ConfigurationManager\n\t\t\"\"\"\n\t\tconfiguration = cls._get_"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn __workspace_manager\n\n\n# ------------------------------\n\nclass __workspace_manager:\n\t\"\"\"\n\tHandles the working directory.\n\t\"\"\"\n\tdef __init__(self, path):\n\t\tself._path = path\n\n\t\tself._repo_name = None\n\t\tself._owner = None\n\t\tself._root = None\n\t\tself._name = None\n\n\tdef _get_owner(self):\n\t\tif len(self._path) > 1:\n\t\t\treturn self._repo_name\n\t\telse:\n\t\t\traise NoWorkingDirectory(self._path[0])\n\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\t# The managers cache the workspace managers, so it needs to be unusable.\n\t\tif not cls._workspace_manager:\n\t\t\tcls._workspace_manager = workspace_manager_template.get_workspaces()\n\t\t\n\t\treturn cls._workspace_manager\n\n\t@classmethod\n\tdef create_workspace_manager(cls, workspace_name):\n\t\t\"\"\"\n\t\tCreates a workspace manager factory that stores all of the workspace\n\t\tmanagers in a workspace.\n\n\t\t:param workspace_name: The workspace name.\n\t\t:type workspace_name: str, str\n\t\t\"\"\"\n\t\tworkspace_manager ="}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls.workspace_manager()\n\"\"\"SCons.Tool.suncc\n\nTool-specific initialization for Sun Solaris (Forte) CC and cc.\n\nThere normally shouldn't be any need to import this module directly.\nIt will usually be imported through the generic SCons.Tool.Tool()\nselection method.\n\n\"\"\"\n\n#\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._wsman\n\n# Global: The workspace manager used, a workspace instance, a reference to a workspace instance\nWorkspaceManager=WorkspaceManager()\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn WORKSPACE_MANAGER\n\n\tdef get_workspace(self, workspace_id):\t\t#pylint: disable=W0613\n\t\t\"\"\"\n\t\tGets the workspace by its id.  Uses workspace_id.\n\t\tReturns None in case a workspace was found but a reference to invalid workspace is\n\t\tspecified.\n\t\t\"\"\"\n\t\tlogging.debug(\"WMS.get_workspace\" + str(workspace_id) + \": workspace_id \" + str(workspace_id))\n\t\tworkspace_mgr = self.get_workspace_manager()\n\t\t# check if workspace exists\n\t\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tworkspace_manager = get_workspace_manager()\n\t\tif not issubclass(workspace_manager, cls):\n\t\t\traise TypeError(\"{0} must be a workspace manager!\".format(cls.__name__))\n\t\treturn workspace_manager\n\n\t@classmethod\n\tdef import_all(cls):\n\t\t\"\"\"\n\t\tImports all of the plugins in the manager. \n\t\t\"\"\"\n\t\tif len(sys.argv) > 1:\n\t\t\tpath = sys.argv[1]\n\t\telse:\n\t\t\tpath = os.path.abspath(os.path.dirname(__file__)).split(os."}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn _workspace_manager_map[workspace]\n\n##========================================================================\n\ndef _workspace_manager(identifier, project_name, workspace, manager):\n\t\"\"\"\n\tGets the workspace manager object for the current workspace.\n\n\tArguments:\n\t\tidentifier (str): The identifier for an already active workspace.\n\t\t\t\tAny other identifier will be replaced in the user-defined workspace manager.\n\t\tproject_name (str): The name of the current workspace.\n\t\tworkspace (Workspace): A workspace object.\n\t\tmanager (WorkspaceManager): An optional workspace manager to add to.\n\tReturns:\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif not cls.workspace_manager:\n\t\t\tcls.workspace_manager = WorkspaceManager()\n\t\t\tworkspace_manager_map = {\n\t\t\t\t'default': 'dwarf', 'workspace': 'WorkspaceManager'\n\t\t\t}\n\t\t\tmap(workspace_manager_map.get, cls.workspace_manager.get_workspace_ids())\n\t\t\tcls.workspace_manager.workspace_ids = workspace_manager_map\n\t\treturn cls.workspace_manager\n\n\t@property\n\tdef workspace_manager_manager_dict(self):\n\t\tif not self._workspace_manager_manager"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls.workspace_manager\n\n\t@property\t\n\tdef workspace_manager_name(self):\n\t\treturn self.__workspace_manager_name\n\n\t@property\n\tdef workspace_manager_config(self):\n\t\treturn self.__workspace_manager_config\n\n\t@property\n\tdef workspace_manager_name_prefix(self):\n\t\tif not self.__workspace_manager_name_prefix:\n\t\t\traise ValueError('No workspace manager name specified.')\n\t\toutput = 'WANDER {0}'.format(self.__workspace_manager_name)\n\t\treturn output\n\n\tdef"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tworkspace_manager = WsManager.instance()\n\n\t\t# get the workspace manager\n\t\tworkspace_manager.load_from_project(workspace_manager.active_project())\n\n\t\treturn workspace_manager\n\n\t@classmethod\n\tdef has_workspace(cls, name):\n\t\treturn workspace_manager_by_name(name) is not None\n\n\tdef __getattribute__(self, name):\n\t\treturn lambda *args, **kwargs: self._workspace_manager.workspace_manager.get_workspace(name)\n\n\t@property\n\tdef active_workspace(self):\n\t\t\"\"\"\n\t"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.manager\n\n\n\n\n##################################################################################################################################\n# AbstractPluginManager class:\n\nclass AbstractPluginManager(object):\n\n\t\"\"\"\n\tThe AbstractPluginManager class is the base class for all plugins that\n\tinherit from this abstract plugin manager. The methods defined in such an\n\tinstance of this class are called from the factory functions.\n\t\"\"\"\n\n\tdef init(self):\n\t\tlog('AbstractPluginManager is initialized.')\n\t\tself._manager.loadPlugins()\n\t\tlog('Plugin Manager loaded.')\n\t\tself.defaultPluginClasses = self.getPluginClasses()\n\t\tself.default"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\traise NotImplementedError()\n\n\nclass DashboardLayout(DashboardLayout):\n\n\tdef __init__(self, dashboard, children):\n\t\tDashboardLayout.__init__(self)\n\t\tself.children = children\n\t\tself.index = 0\n\n\nclass Dashboard(Dashboard):\n\n\tdef __init__(self, dashboard, children):\n\t\tDashboard.__init__(self, dashboard, children)\n\t\tself.is_shown = True\n\n\tdef init_with_context(self, context):\n\t\t# Init.\n\t\tself.children = [Dashboard(dashboard, [obj]) for obj"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn getattr(cls, \"_plugins\", None)\n\nclass SplunklibHandler(handler.PluginHandler):\n\t\"\"\"\n\tContains the PluginHandler class to get the PLUGIN_PATH for the plug-in.\n\tThis class is created by loading a PluginHandler's `__plugin_name__` to\n\ta SplunkdPlugin instance from a configuration directory.\n\t\"\"\"\n\t\n\tdef __init__(self, logger=None):\n\t\t\"\"\"\n\t\tInitializes the plugin, setting its logger.\n\t\t\"\"\"\n\t\tif logger is None:\n\t\t\tlogger = logging.getLogger(__"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.module.__plugins\n\tdef import_plugins(mod, importable=True):\n\t\t\"\"\"\n\t\tImport and return plugins.\tThe given module must exist in the plugin search path.\n\n\t\tThis allows users to use plugin manager directly, with'self.get_available_plugins().plugins_dict' for plugin\n\t\twith a dict of {'module', 'class'} (to load the class with get_available_plugins for example).\n\n\t\t@type mod: C{mod.PluginManager}\n\t\t@param mod: The :class:`PluginManagerBase` to import\n\t\t@"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn PluginsManager\n\n\t@property\n\tdef active_plugins(self):\n\t\t\"\"\"Returns a list of available plugins.\"\"\"\n\t\treturn Plugins.instance() if Plugins.instance is not None else []\n\n\tdef activate(self, instance):\n\t\t\"\"\"\n\t\tSets up the plugin for the given instance.\n\t\t\"\"\"\n\t\tplugins.active.add(instance.plugin_object.plugin_object)\n\t\t# set the active class\n\t\tplugins.active.classes[instance.plugin_object.plugin_type] = instance\n\n\tdef deactivate(self, instance):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugins\n\n\tdef get_plugin(self, name):\n\t\t\"\"\"\n\t\tRetrieves a named plugin.\n\t\t\"\"\"\n\t\treturn self.__get_plugin__(name)\n#    Gedit snippets plugin\n#    Copyright (C) 2005-2006  Jesse van den Kieboom <jessevdk@gnome.org>\n#\n#    This program is free software; you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation; either version 2 of the"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tmodule_list = get_modules(cls.__module__)\n\n\t\treturn {k:cls.__view__[k](cls) for k in module_list if k.startswith('_') and issubclass(cls, k)}\n\n\t@classmethod\n\tdef view_plugins(cls):\n\t\t\"\"\"\n\t\tGets a list of view plugins.\n\t\t\"\"\"\n\t\t# TODO do something smarter.\n\t\tview_list = get_plugins(cls.__view__)\n\t\treturn {k:cls.__view__[k](cls) for k in view_list if not hasattr(cls, k)}\n\n\t@classmethod"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.plugins\n\t\t\nplugins_manager = PluginManager()\nfrom __future__ import unicode_literals\nimport datetime\nimport logging\nfrom collections import defaultdict\n\nfrom django.core import mail\nfrom django.core.mail.message import sanitize_address\nfrom django.template import Context, loader\nfrom django.utils.translation import ugettext as _\n\nfrom kitsune.forums.models import TopicReadTracker\nfrom kitsune.questions.events import QuestionReplyEvent\nfrom kitsune.questions.events import QuestionReplyVote\nfrom kitsune.questions.models import"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tfor plugin in cls.plugins.values():\n\t\t\tyield plugin_manager(plugin)\n\n\ndef plugin_path(plugin_type):\n\t\"\"\"\n\tReturns the path of a plugin by its type.\n\t\"\"\"\n\tfor plugin in cls.plugins.values():\n\t\tif plugin_type in plugin.types:\n\t\t\treturn plugin.plugin_path_for_type(plugin_type)\n\ndef get_plugin_data(plugin_name, plugin_path):\n\t\"\"\"\n\tReturns list of data for a given plugin.\n\t\"\"\"\n\tif plugin_path:"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tif cls.__plugin_manager is None:\n\t\t\tcls.__plugin_manager = PluginManager()\n\t\treturn cls.__plugin_manager\n\n\t@classmethod\n\tdef get_plugin(cls, plugin_slug, parent_slug = None):\n\t\t\"\"\"\n\t\tGets a plugin by its class name and optionally the parent's parent slug.\n\t\tThis is because the parent class does not have all the fields necessary to extract the plugin\n\t\tfield (parent=None).\n\t\t\n\t\t:param plugin_slug: The plugin's slug (without ending '.')\n\t\t:type plugin_slug: str.\n\t\t:"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\trequired_fields = {\n\t\t\t'content': ['content','spec'],\t# content of spec (JSON)\n\t\t\t'spec': ['content','spec'],\t\t# content of spec (JSON)\n\t\t}\n\n\t\tif spec_content:\n\t\t\trequired_fields['spec'] = spec_content.keys()\n\n\t\trequired_fields['spec'] = [k for k in required_fields.keys() if not k in spec_content]\n\t\trequired_fields.sort()\n\n\t\tfor key in required_fields:\n\t\t\tif not hasattr(cls, key):\n\t\t\t\traise"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\traise IRValidatorException('spec file content is missing and the required '\n\t\t\t  'fields are missing')\n\t# validate yaml file contents to make sure spec has all required fields\n\ttry:\n\t\tyaml.utils.yaml.safe_load(spec_content)\n\texcept Exception, msg:\n\t\traise IRValidationError, msg\n\telse:\n\t\treturn spec_content\n\t\n# return a dict of dict of { key: values }\ndef validate_from_dict(cls, data={}):\n\t\"\"\"\n\tvalidates that data (a dictionary) has all required fields\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tret = {}\n\t\tif not spec_content:\t# if no spec content, try default\n\t\t\ttry:\n\t\t\t\tcontent_dict = json.load(pkg_resources.resource_string(spec_content))\n\t\t\t\tret = cls(content_dict[\"spec_text\"] if \"__desc__\" not in content_dict else content_dict[\"__desc__\"])\n\t\t\texcept IOError:\n\t\t\t\traise IRValidatorException(\"Spec file \" + str(spec_content) + \" is not present \" + str(spec_file))\n\t\tif \"spec_text\" not in ret:\n\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\t# check whether we have data that matches the spec file\n\t\t# (this is very simple and ugly, because I've used it as a class \n\t\t# to hold attributes for now and I haven't tested with this yet)\n\t\tfor i in range(len(spec_content) - 1, -1, -1):\n\t\t    if spec_content[i]!= spec_content[i + 1]:\n\t\t\t    raise IRValidatorException (\"spec file content does not match\",\n\t\t\t\t\t\t\t\t   \"inspec file content: \" + str(spec_content)+\"\\nvalidating at \" + str(i +"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tpass\n\t# validate spec file\n\tfile_info = cls.read(spec_content)\n\tvalidator = check_spec(file_info)\n\t# validate data\n\tcontent = validator.dict_repr(spec_content)\n\t\t# check valid\n\treturn content\n\n#\n#  file.h.\nclass File(object):\n\t__slots__ = ('mode','mode_name','spec_content')\n\n\tdef __init__(self,\n                 parent_package,\n                 name,\n                 filetype,\n                 content):\n\t\t# parent package path\n\t\tself"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is not None:\n\t\t\tspec_content = yaml.dump(spec_content)\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"spec file empty\")\n\t\tif not spec_content.find('id') == -1:\n\t\t\traise IRValidatorException(\"spec file empty\")\n\t\treturn spec_content.strip('$')\n\nclass RHConference(db.Base):\n\t\"\"\"\n\tRHRConference class\n\tThis is the basic model to hold a single conference\n\ton a conference page. It holds a"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tspec_content_schema = yaml.load(spec_content)\n\n\t\t# check mandatory fields\n\t\tmandatory_fields = set(spec_content_schema)\n\t\tassert'required' in mandatory_fields, \"'required' field not found in spec file\"\n\n\t\tfor field in mandatory_fields:\n\t\t\tname = field.strip().lower()\n\t\t\tval = spec_content_schema.get(name)\n\t\t\tif val is None:\n\t\t\t\traise IRValidatorException(\"'%s' field cannot be blank if present\" % field)\n\t\t\tif not isinstance(val, types.StringTypes"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"Spec content required\")\n\n\t\tfor entry in yaml_parse_schema(spec_content):\n\t\t\tif not isinstance(entry, dict):\n\t\t\t\traise IRValidatorException(\"Malformed spec file\")\n\n\t\t\tentry, data = yaml_load(entry)\n\n\t\t\tfor k in entry:\n\t\t\t\tif not isinstance(k, six.string_types):\n\t\t\t\t\traise IRValidatorException(\"Malformed spec file\")\n\t\t\t\tif k not in cls.REQUIRED_FIELDS:\n\t\t\t\t\traise IRValidatorException(\"Missing mandatory field %s in spec"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\n\t# validate spec\n\ttry:\n\t\tif spec_content.strip() == '':\n\t\t\treturn {}\n\n\texcept AttributeError:\n\t\traise IRValidatorException('spec file required')\n\n\t# get information by reading spec content\n\ttry:\n\t\t# try reading JSON\n\t\twith open(spec_content, 'r', encoding='utf-8') as f:\n\t\t\tspec = yaml.load(f)\n\n\t\t# check spec is valid\n\t\ttry:\n\t\t\t# check if it contains an ip with ip_address:port\n\t\t\tif 'ip_address' in spec:"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\t\n\tspec = {'name' : '', 'type' : 'String',\n\t\t\t'modified_type': 'String', 'optional': False}\n\t\n\t\n\tif spec_content:\n\t\tfor line in spec_content:\n\t\t\tmatch = re.match('^\\{(\\w+)\\}(\\{[\\w\\.\\.\\.,-]*)?}\\[\\^(?P<name>.*)\\]$', line, flags=re.MULTILINE)\n\t\t\tif match:\n\t\t\t\tspec['name'] = match.group('name')\n\t\t\telse:\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t'Spec"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif not yaml_file:\n\t\t\traise IRValidatorException(\"Input YAML file has NO 'yaml_file' attribute\")\n\t\tif not is_yaml(yaml_file):\n\t\t\traise IRValidatorException(\"YAML file does not contain valid YAML data\")\n\t\t\n\t\ttry:\n\t\t\twith open(yaml_file) as yaml_file_reader:\n\t\t\t\tyaml_data = yaml.load(yaml_file_reader)\n\t\texcept:\n\t\t\traise IRValidatorException(\"YAML file is not valid yaml data\")\n\t\t\n\t\tif \"$schema\" not in yaml_data:\n\t\t\traise I"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file is None: yaml_file = cls.file\n\t\telif not isinstance(yaml_file, Path):\n\t\t\traise IRValidatorException(\"yaml_file was not a file\")\n\t\tvalid, msg = cls._load_from_file(file_=yaml_file, yaml_file=lambda obj:obj['yaml'])\n\t\tif not valid: raise IRValidatorException(\"YAML schema does not match the expected schema\")\n\t\treturn cls._parse_schema_file(cls.validate(yaml_file)['yaml'], valid, error_message=msg)\n\n\t@classmethod"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif not yaml_file:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"yaml_file or yaml_loader must not exist at \"\n\t\t\t\t\"the time of instantiation\"\n\t\t\t)\n\n\t\tif not os.path.isfile(yaml_file):\n\t\t\traise IRValidatorException(\n\t\t\t\t\"yaml_file '{0}' not found\".format(yaml_file)\n\t\t\t)\n\n\t\ttry:\n\t\t\tdata = yaml.load(open(yaml_file, 'r'))\n\n\t\texcept yaml.parser.ParserError as error:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"failed"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif cls.required_data_validation:\n\t\t\ttry:\n\t\t\t\twith open(yaml_file) as stream:\n\t\t\t\t\tdata_dict = yaml.safe_load(stream)\n\t\t\t\t\tif yaml_file.endswith(\".yaml\"):\n\t\t\t\t\t\treturn cls.init_yaml(data_dict, cls.required_data_validation_schema)\n\t\t\t\texcept Exception as error:\n\t\t\t\t\tlogging.error(\"Error during json validation: \" + str(error))\n\t\t\texcept Exception as error:\n\t\t\t\tlogging.error(\"Error during parsing file: \" + str(error))\n\t\t\t\traise IRValidator"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\ttry:\n\t\t\tyaml_file = open(yaml_file, 'r') if yaml_file is not None else \\\n\t\t\t\topen(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'yaml-schema.yaml'), 'r') if yaml_file is not None else None\n\t\t\tdata = yaml.load_all(yaml_file) if yaml_file is not None else None\n\t\t\terrors = validate_data(data, schema_data.get('validator', {}))\n\t\t\tif len(errors)!= 0:\n\t\t\t\traise IRValidator"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\tyaml_file = os.path.join(cls.get_current_folder(), yaml)\n\n\t\twith open(yaml_file, 'rb') as f:\n\t\t\tyaml_string = f.read()\n\n\t\treturn yaml_string\n\n\t@classmethod\n\tdef from_string(cls, text):\n\n\t\tyaml_string = cls.parse(text)\n\n\t\tif not cls.validate_from_string(yaml_string):\n\t\t\traise IRValidatorException(\"YAML file isn't valid.\")\n\t\treturn yaml_string.get('data"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tlogger.debug(\"Loading YAML for %s\", yaml_file.name)\n\t\t# Use the yaml parser to create a base dictionary\n\t\t# (note - need to be separate function before)\n\t\tyamlparser = SafeYamlValidator()\n\t\t# Check if YAML file exists, and initialize the data\n\t\tif yaml_file is None:\n\t\t\tlogger.debug(\"No YAML file in: %s,\" % yaml_file_name)\n\t\t\traise IRConverterException(\"No YAML file in: %s\" % yaml_file_name)\n\t\t\t# Initialize the base dictionary in case none"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tdata = None\n\t\tif yaml_file is None:\n\t\t\tif hasattr(yaml, 'RoundTripLoader'):\n\t\t\t\tyaml_file = yaml.RoundTripLoader(yaml_file)\n\t\t\telse:\n\t\t\t\traise IRValidatorException(\"File %s does not exist. Please, provide a YAML first.\"%yaml_file)\n\t\ttry:\n\t\t\tdata = yaml.load(yaml_file)\n\t\texcept yaml.YAMLError, exc:\n\t\t\traise IRValidatorException(\"Invalid YAML specification in file. %s\"%(exc))\n\n\t\ttry:\n\t\t\tdata['file'] #"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\twith open(yaml_file, 'rb') as data:\n\t\t\tyaml_content = yaml.safe_load(data)\n\t\tconf = {}\n\t\tif not yaml_content:\n\t\t\traise IRValidatorException('missing or empty YAML')\n\t\tif not isinstance(yaml_content, dict):\n\t\t\tyaml_content = {}\n\t\tfor k, v in yaml_content.iteritems():\n\t\t\tif k =='version':\n\t\t\t\tconf['version'] = v\n\t\t\telif k == 'description':\n\t\t\t\tconf['description'] = v\n\t\t\telif isinstance(yaml_content[k"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tlogger.info(\"Loading YAML file %s...\", yaml_file)\n\t\tif yaml_file is None or not os.path.exists(template_path(yaml_file)):\n\t\t\traise IRValidatorException(\"YAML file %s was not found!\" % yaml_file)\n\t\tdata = None\n\t\tfile_path = template_path(yaml_file)\n\t\tlogger.info(\"Parsing file %s...\", file_path)\n\t\twith file(file_path, 'r') as f:\n\t\t\tdata = yaml.load(f)\t\n\n\t\tif data is None:\n\t\t"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\n\n\tdef parse(self):\n\t\t# TODO: Make this a parser.\n\t\t#  The parser takes care of parsing.\n\n\t\traise AssertionError\n\n\n\tdef output(self, msg):\n\t\t# TODO: Make this a parser.\n\n\t\tif len(msg) > 0:\n\t\t\tprint msg\n\n\n\n\n\nclass Configurable(BaseConfigurable):\n\t# Inherit everything.\n\n\t# Configurable attributes.\n\n\tdef get_config(self, obj, section, name):\n\t\t\"\"\"\n\t\tGet the config entry with the name \"name\".\n\t\tIf section is not None, parse section"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude = defaultdict(list)\t# {include_group_1 : [include_group_2, include_group_3]}\n\n\t\tfor group_name, group_node in parser_dict.iteritems():\n\t\t\tif group_name[0] == '!':\t# Not handled\n\t\t\t\tcontinue\n\n\t\t\tinclude[group_name[0]].append(group_node.get('include', []))\n\t\t\tinclude[group_name[0]].append(group_node.get('group', []))\n\n\t\tself.__expand(include)\n\n\t\treturn include\n\n\tdef __expand(self, include"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tself.include_groups = parser_dict.get('include_groups', \"\")\n\n\tdef _template_vars(self, include_params):\n\t\t\"\"\"\n\t\tFill in the template dict with variables specific to the spec.\n\t\tThe include_params are a list of (name,value) params for the include\n\t\t\"\"\"\n\t\tvar_names = []\n\n\t\tfor name, value in include_params.items():\n\t\t\tif name[0] is '$':\n\t\t\t\tname = '$' + name[1:] # strip leading $\n\n\t\treturn dict(var_names=var_names,"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\texclude_dirs = parser_dict.get('exclude_dirs', '').split(',')\n\t\tif list == 'filter':\n\t\t\tif '*' in exclude_dirs:\n\t\t\t\t\texclude_dirs = exclude_dirs.split('*')\n\t\t\tfor dir in exclude_dirs:\n\t\t\t\tfor ext in self._src_paths:\n\t\t\t\t\tif parser_dict.get(ext):\n\t\t\t\t\t\tfor gpath in parser_dict[ext]:\n\t\t\t\t\t\t\t_srcfiles.append(os.path.join(dir, gpath))\n\n\nclass ModuleDictExclude(ModuleDictParser):\n\t\"\"\"\n\tProcess the"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tsection_dict = {}\n\t\tfor section in parser_dict:\n\t\t\tfor include_type, name, condition in section:\n\t\t\t\tif include_type == \"type\":\n\t\t\t\t\tsection_dict[\"type\"] = conditions.todict(condition)\n\t\t\t\telif include_type == \"condition\":\n\t\t\t\t\tsection_dict[\"condition\"] = conditions.todict(condition)\n\t\t\t\telif include_type == \"enum\":\n\t\t\t\t\tsection_dict[\"enum\"] = tuple(sorted(condition.keys()))\n\t\t\t\telif include_type == \"section\":\n\t\t\t\t\tsection_dict[\"section\"] = [section"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tsection_dict = parser_dict.copy()\n\n\t\tdef handle_include(data):\n\t\t\tif 'include' in data:\n\t\t\t\t# Split into (src_type, src_key, src_start, src_end).\n\t\t\t\tinclude_key = ('include', data.pop('include'))\n\t\t\t\tsource_list = data.pop('source')\n\t\t\t\tignore_list = data.pop('ignore')\n\n\t\t\t\t# Handle include group\n\t\t\t\tif not (isinstance(source_list, list) and isinstance(source_list[-1], dict)):\n\t\t\t\t\traise Exception('Add include source"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tself.logger.debug(\"parsing include groups...\")\n\t\t\n\t\tinclude_groups = {}\n\t\t\n\t\tfor include_group_name in parser_dict.keys():\n\t\t\tparser_dict[include_group_name] = parser_dict.pop(include_group_name)\t\t\n\t\t\ttry:\n\t\t\t\tinclude_name = parser_dict[include_group_name]\n\t\t\t\tif (not parser_dict[include_name]['name']) or parser_dict[include_name]['name'] == '':\n\t\t\t\t\tself.logger.warning(\"Include group name '%s' is not defined.\", include_group"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tparser_dict['section_defaults']['include_groups'].update(parser_dict['defaults']['include_groups'])\n\t\treturn\n\n\tdef add_file(self, filename, dest):\n\t\t\"\"\"\n\t\tAdds the file to the dest, creating the real file if it didn't\n\t\talready exist. Also ensures that a directory named filename == dest\n\t\tusing a base name.\n\t\t\"\"\"\n\t\tif not os.path.basename(filename):\n\t\t\tfilename = os.path.basename(filename)\n\n\t\t# make sure file not too long\n\t\tsize = os.stat(filename)."}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif 'include' not in parser_dict.keys():\n\t\t\traise ConfigError(\"include key missing\")\t\n\t\t\t\n\t\tinclude = [s.lower() for s in parser_dict['include']]\n\t\t\n\t\tif 'include' in parser_dict and parser_dict['include']:\n\t\t\t# Check for the key\n\t\t\tif not parser_dict['include'].has_key(list(parser_dict['include'].keys())[0]):\n\t\t\t\traise ConfigError(\"include key key missing\")\n\t \n\t\t\tself.include = parser_dict['include']\n\n\t\t\tinclude_names = []\n\t\t\t\n\t\t\tfor s"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude_dict = parser_dict.copy()\n\t\tinclude_dict['includes'] = [\n\t\t\tself.__IncludeStatement(e) for e in self.include_path]\n\n\tdef _parse_include(self, include_node):\n\t\tinclude_path = include_node.getAttribute('filename')\n\t\tinclude_path += \".h\"\n\n\t\tif not include_path in self._include_paths:\n\t\t\traise InvalidIncludeException(include_path)\n\t\tinclude_node = self._include_paths[include_path]\n\t\tself.__AddInclude(include_node)"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn (self._dict, self._other_spec, self._other_set)\n\n\tdef get_data(self, *args, **kwargs):\n\t\treturn self.get(*args, **kwargs)\n\n\tdef set_data(self, *args, **kwargs):\n\t\tif self._dict is not self._other_spec:\n\t\t\traise TypeError(f'Cannot set data for {self._other_spec!r}')\n\t\tself._dict = args[0]\n\t\tself._other_spec = args[1]\n\n\tdef apply(self, fn):\n\t\t"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tpass\n\n\tdef expand(self):\n\t\t\"\"\"\n\t\tExpand source files that were resolved using the build command line as\n\t\ta directory tree.\n\t\t\n\t\t@return  A Source instance, populated by loading the source files\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\tdef __str__(self):\n\t\t\"\"\"Display the target for display (includes included files).\"\"\"\n\t\tret = [self.name]\n\t\tfor k,v in self._dep_keys.items():\n\t\t\tret.append(''+ k + '=' + (unicode(v),) or str(v))\n\t\treturn '\\n'."}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tspec, src = super(PkgConfig, self).get_source_defaults()\n\t\tspec.defaults.update(self.generator.default_source_defaults())\n\t\treturn AddrSpec.pkg_defaults\n\n\tdef check_required_settings(self):\n\t\tself.required_settings.update(self.generator.required_settings)\n\t\tself.required_settings['PKG_PATHS'] = self.generator.paths\n\t\tself.required_settings['PKG_FILES'] = self.generator.files\n\t\tself.required_settings['PKG_PREFIXES'] = self.generator"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\t# Get specs from Spec (from spec[1]).\n\t\t# Also update __getitem__ and __len__() for values.\n\t\tdef __getitem__(self, key):\n\t\t\tval = self.get(key, None)\n\t\t\tif val is None:\n\t\t\t\traise KeyError(key)\n\t\t\treturn val\n\t\tval = None\n\t\tfor name in self.__specs:\n\t\t\tval = getattr(self, name)\n\t\t\tif val is not None:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tif val is None:\n\t\t\t\traise KeyError(name)\n\n\t\t# Merge other values with other"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tspec = {\n\t\t\t'includedirs':'',\n\t\t\t'uselib':'',\n\t\t\t'subdirs':'',\n\t\t}\n\n\t\tfor x in self.__dict__.keys():\n\t\t\tx = x.replace(',','|')\n\t\t\tif x in spec and self.__dict__[x]!= None:\n\t\t\t\tspec[x] = self.__dict__[x]\n\t\t\t\tbreak\n\n\t\treturn spec\n\n\tdef set_define_macros(self,macros):\n\t\t\"\"\"\n\t\tSet the given macros to a list of strings containing the target\n\t\tdefinitions.\n\n\t\t:params macros: A dictionary"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tspec = dict()\n\t\tspec.update(self._args_spec_defaults)\n\t\tif self._command_line:\n\t\t\tspec[\"cmdline\"] = self._command_line\n\t\tif self._env_args:\n\t\t\tspec[\"env\"] = self._env_args\n\t\tif self._env:\n\t\t\tspec[\"env\"] = self._env\n\t\treturn dict(spec=spec)            \n\t\t\n\tdef _merge_dict(self, dict1, dict2, set_props=None,\n\t\t\t\t\tignore_keys=None, force=True):\n\t\t\"\"\"\n\t\tMerge"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\t#print(self.args)\n\t\tself.return_type = None\n\t\tself.return_arg = None\n\t\tself.arg_count = 0\n\t\tself.args_count = 0;\n\t\tself.arg_names = None\n\t\tself.return_type = dict()\n\t\tself.ret_type = dict()\n\t\tif len(self.args) == 0:\n\t\t\tself.return_type = dict\t# No return.\n\t\telif len(self.args) == 1:\n\t\t\tself.return_type = {str:dict}\n\t\telse"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn Spec.parse(self, self.spec, self.local_dict)\n\n\t@staticmethod\n\tdef parse(text, spec, local_dict):\n\t\t\"\"\"Parse specs and other sources.\n\n\t\tArgs:\n\t\t\ttext:\n\t\t\t\tText to parse, as text string.\n\t\t\tspec:\n\t\t\t\tSpec to parse against\n\t\t\tlocal_dict:\n\t\t\t\tLocalDict to be substituted for substitutions, or None\n\n\t\tReturns:\n\t\t\tParseResults if specs are invalid, or a None otherwise.\n\t\t\"\"\"\n\t\tebuilds = global_dict(local_dict=local_dict"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn {\n\t\t\t'debug': self.missing_value,\n\t\t\t'debugpath': self.missing_value,\n\t\t\t'glslc': self.undefined_value,\n\t\t\t'glsc': self.undefined_value,\n\t\t\t'glsvc': self.undefined_value,\n\t\t\t'gitrev': self.invalid_value,\n\t\t\t'gitrevshort': self.invalid_value,\n\t\t\t'gitrevrev': self.invalid_value,\n\t\t\t'glstag': self.invalid_value,\n\t\t\t'glsrev': self.invalid"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.check_values(self.options, [\"<name>\"] + \\\n\t\t\t[\"<flag>\", \"<value>\"])\n\t\tself.parse_options()\n\t\tself.setup_output_data()\n\t\treturn self.options\n\n\tdef _set_opt_values(self, opts):\n\t\tself.options = opts\n\n\tdef check_values(self, opts, values_dict = None):\n\t\t\"\"\" Check values and optionally set attributes in the options object\n\t\t\tfor --help, for example.\n\t\t\n\t\t@param opt_values_dict: dictionary of values for option-"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor param, value in self.options.__dict__.items():\n\t\t\tif DeprecatedVariable.alias_is_valid(param):\n\t\t\t\tdeprecated_args[param] = value\n\t\treturn deprecated_args\n\n\tdef validate(self):\n\t\t\"\"\"\n\t\tValidate this section\n\t\t\"\"\"\n\t\tif not hasattr(self, 'validate_max_length'):\n\t\t\treturn\n\t\tfor param, value in six.iteritems(self.validate_max_length):\n\t\t\tif val is None and param not in self.options:\n\t\t\t\tcontinue\n\t\t\tif not isinstance"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = frappe._dict({\n\t\t\t\"issingle\": 0,\n\t\t\t\"ispagesize\": 0,\n\t\t\t\"maxlength\": 0,\n\t\t\t\"width\": None,\n\t\t\t\"print_preview\": 0,\n\t\t\t\"hidden\": 0,\n\t\t\t\"colwidth\": None,\n\t\t\t\"maxwidth\": None,\n\t\t\t\"print_hide\": 0,\n\t\t\t\"allow_on_submit\": 0,\n\t\t\t\"in_create\": 1,\n\t\t\t\"print_hide_if_duplicate\": 0,\n\t\t\t\"in_print\": 0,\n\t\t\t\"is"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tfor option, deprecate_this in self.DEPRECATED:\t\n\t\t\ttry:\n\t\t\t\tyield deprecate_this.name\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\n\t## Returns a sorted of the methods in a list\n\t#\tTODO: remove this, it's too slow!\n\tdef get_all_methods(self, plugin):\n\t\t\"\"\"\n\t\tReturning list of all method entries, ordered by entry name\n\t\t\"\"\"\n\t\tmethods = []\n\t\tif hasattr(self, \"implementations\"):\n\t\t\tfor implementation in self.implementations:\n\t\t\t\timplementations = getattr(implementation"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {k: v for k, v in self.__dict__.items() if k not in self.__reserved_keys}\n\t\treturn deprecated_args\n\n\tdef deprecated_args_warning(self):\n\t\tmessage = []\n\t\tfor a in self.deprecated_args:\n\t\t\tmessage.append(\"Deprecated argument '%s'\" % to_unicode(a))\n\t\tmessage = '\\n'.join(message)\n\t\tfor line in message.splitlines():\n\t\t\tself.module.warn(line)\n\n\tdef find_file(self, path=None):\n\t\t\"\"\"\n\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\targs = {}\n\t\t# args has type and name\n\t\t# special case: action, config, default, config_path\n\t\t#     we also look for this in action, config, default and config_path\n\t\t#     if no such key exists\n\t\targs_deprecated = {\n\t\t\t'action'\t:\t\taction_only_info,\n\t\t\t'action_name'\t\t:\t\taction,\n\t\t\t'cmd'\t\t\t:\t\taction_help,\n\t\t\t'args'\t\t\t:\t\targs_action_path_usage,\n\t\t\t'cmd_"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tresult = {}\n\t\tresult[u\"name\"] = self.__class__.command.__class__.command_name\n\t\treturn result\n\n\tdef get_description(self):\n\t\ttext = u\"Runs a command in the user namespace.\"\n\t\ttext += u\"\\n\\n\\tCommands are:\\n\"\n\t\tfor name in self.get_deprecated_args():\n\t\t\ttext += u\"\\t- \" + name + \"\\n\"\n\t\treturn text\nimport unittest, random, sys, time\nsys.path.extend(['.','..','../..','py'])\nimport h"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated = {\"old\": {}, \"new\": {}}\n\t\tfor old_arg in [\"debug\", \"install_deps\", \"ignore_deps\"]:\n\t\t\tdeprecated[\"old\"][old_arg] = self.config['args'][old_arg]\n\t\t\t\tif old_arg in self.deprecated:\n\t\t\t\t\tdeprecated[\"new\"][old_arg] = self.deprecated[old_arg]\n\t\treturn deprecated\n\n\tdef update_progress(self, task_progress):\n\t    \"\"\"Updates progress information in task progress\n\t\t@param task_progress: a number of tasks done\n\t\t\"\"\"\n\t    total = float("}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = \"\"\n\t\tif self.deprecated_for is not None:\n\t\t\tfor deprecated_field in self.deprecated_for:\n\t\t\t\tdeprecated_args += \"<br>\" + deprecated_field + \"</br>\"\n\t\treturn deprecated_args\n\n\tdef on_update(self):\n\t\t\"\"\"\n\t \tClear `tabAuthorization` and `modified` values and log the current doctype if\n\t\tit has changed\n\t\t\"\"\"\n\t\tfrappe.cache().hdel(\"mail\", self.name)\n\t\tself.modified = now()\n\t\tif not self.name:\n\t\t\tself.name"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {}\n\ndef get_deprecated_dict(self):\n\t\"\"\"\n\tReturning dict (optionally overwritten) with deprecation information for a class instance (class instance).\n\t\"\"\"\n\t# get all options (class, instance) - deprecated - options from class\n\tdeprecated = get_deprecated_list_for_table(\"deprecated\")\n\tfor k, v in deprecated:\n\t\to = self.options.get(k, None)\n\t\tif not o: continue\n\t\t# only deprecate options from class\n\t\tif o.deprecated: continue\n\t\t# options have higher priority"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tret = {\n\t\t\t\"is_private\" : True,\n\t\t\t\"type\":   None,\n\t\t\t\"default\": \"none\"\n\t\t}\n\t\tret.update(self.deprecated_args)\n\t\treturn ret\n\n\tdef set_deprecated_args(self, deprecation_options_dict=None):\n\t\t\"\"\"\n\t\tSet options which deprecate others, and saves them into the dict specified in\n\t\t:py:attr:`.args`\n\n\t\tThis may be used on a per-object basis, for example in a class hierarchy hierarchy like this::\n\n\t\t\tmy_object.set_"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\t# If the key is missing \"usage\" in cli_args, it's present that way.\n\t\tif \"usage\" in cli_args:\n\t\t\tsys.stderr.write(\"cli_args contains \" + cli_args[\"--usage\"] + \"\\n\")\n\t\t\tsys.exit(2)\n\t\t# Otherwise warn that it was removed.\n\t\telse:\n\t\t\tsys.stderr.write(\"Didn't find a key in cli_args\\n\")\n\t\t\tsys.exit(2)\n\ndef parse_args(cli_args):\n\t\"\"\"\n\tRetrieves arguments in cli"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif \"files\" not in cli_args.keys():\n\t\t\tshow_help(self.usage)\n\t\t\tsys.exit(2)\n\t\tmain(cli_args, answer_file_args)\n\t\t\n\t@staticmethod\n\tdef validate_args(cli_args: dict):\n\t\t\"\"\"\n\t\tValidates and prints args.\n\n:param cli_args: cli parameters from argparse\n\n:return: None\n\t\"\"\"\n\t\trequired_args = {\n\t\t\t\"output_path\",\n\t\t\t\"verbose\",\n\t\t\t\"overwrite_output\",\n\t\t\t\"quiet\",\n\t\t\t\""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args is None or cli_args == \"\":\n\t\t\tprint(\"No cli args provided.\")\n\t\t\treturn False\n\n\t\tdeprecation = []\n\t\tfor k, v in zip(cli_args, answer_file_args):\n\t\t\tif not cli_args[k]:\n\t\t\t\tcontinue\n\t\t\ttype = \"deprecated\" if v else \"removed\"\n\t\t\tif self._is_valid_arg(x, type) and not self._is_valid_arg(y, type):\n\t\t\t\tdeprecation.append(\"{0}={1}\".format(k, type))\n\n\t\tif cli_args"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tcli_args[self.FILE_FLAG] = answer_file_args\n\n\t\tfor k in cli_args.keys():\n\t\t\tcli_args[self.FILE_FLAG] = cli_args.get(k)\n\n\t\treturn cli_args\n\n\tdef run(self, *args, **kwargs):\n\t\targuments = self.validate_args(args, kwargs)\n\n\t\tcli_args = self.validate_cli_args(arguments, kwargs)\n\n\t\tif 'file' not in kwargs:\n\t\t\tkwargs['file'] = '-'\n\n\t\targs = parse_cli_arguments("}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif set(cli_args.keys())!= set(answer_file_args.keys()):\n\t\t\targ_lines = set(tostr(item, errors=\"surrogate_or_strict\").splitlines() if item.startswith(\"!\")\n\t\t\tmissing = arg_lines - cli_args\n\t\t\tfor arg_line in sorted(missing):\n\t\t\t\tmsg = \"Argument {0} was deprecated\".format(arg_line)\n\t\t\t\tif arg_line in CLIArgNames:\n\t\t\t\t\tfor line in CLIArgNames[arg_line]:\n\t\t\t\t\t\tif line in answer_file_args and arg_line"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args.deprecate:\n\t\t\treturn\n\t\tfor name in cli_args.deprecated:\n\t\t\tif answer_file_args.get(name):\n\t\t\t\tcli_args[name] = answer_file_args[name]\n\n\tdef run_with_debug(self, cli_args):\n\t\t\"\"\"\n\t\tRuns the application with debug flag\n\t\t@param cli_args: the dict of arguments from the cli\n\t\t\"\"\"\n\t\tlogger.debug(\"--> Running with Debug flag \" + str(cli_args))\n\t\tif \"debug\" in cli_args:\n\t\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args['help'] or cli_args['init']:\n\t\t\t# If a deprecated key was supplied for help, skip the help or the\n\t\t\t# init arguments in order to have a description.\n\t\t\treturn\n\t\tcli_args['help'] = False\n\t\tcli_args['init'] = False\n\t\tif'mandatory' not in cli_args:\n\t\t\tfrom.argument_parser import CLIArgumentDocumenter\n\t\t\tcli_args['mandatory'] = True\n\t\tif 'choices' in cli_args:\n\t\t\t# Make sure the value is a list of values.\n\t\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args.use_cli and not answer_file_args:\n\t\t\tcli_args.append(\"--use_cli=y --use_cli_for_dev=y\")\n\t\tfor arg, path in answer_file_args.items():\n\t\t\tif not arg in cli_args or not cli_args[arg]:\n\t\t\t\tpath = None\n\t\t\tmsg = \"--{0} is deprecated. Please use {1} instead.\\n\".format(arg, path)\n\t\t\tif cli_args[arg]:\n\t\t\t\tmsg += \"{0}\\n\".format(cli_args"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor item in cli_args:\n\t\t\tfor item_key in item.keys():\n\t\t\t\tif item_key not in answer_file_args and item[item_key]!= None:\n\t\t\t\t\tprint item.get(item_key, item[\"default\"])\n\t\t\tprint()\n\ndef cli_filter_usage(filters, filtershow=None, ignore_flags=None):\n\t\"\"\"\n\tFilters command line for listing of available arguments.\n\tFilters are case-insensitive, i.e. ``?``, ``-``, ``*`` and so on.\n\n\t:param filters:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\ttry:\n\t\t\targs = copy.copy(cli_args)\n\t\t\ttry:\n\t\t\t\targs['ANSIBLE_ARGS'] = copy.copy(answer_file_args)\n\t\t\t\tret = self.validation_errors(args)\n\t\t\texcept ValueError as e:\n\t\t\t\tfor arg in answer_file_args:\n\t\t\t\t\tif ('no such option: --' not in arg and 'no such file' not in arg):\n\t\t\t\t\t\tmsg = (\"missing value for option '%s' in arg for command '%s'\"\n\t\t\t\t\t\t\t  % (arg, self.cli_cmd))\n\t\t\t\t\t\tself.print_"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tif self.__class__.commands.has_key(command_name):\n\t\t\treturn self.commands[command_name]\n\t\telse:\n\t\t\traise KeyError(\"Don't know how to get this command %s\" % command_name)\n\t#end get_parser_option_specs\n\t# A few special cases...\n\tdef get_help_string(self):\n\t\treturn self.help_string\n\t#END get_help_string\n\n\tdef get_short_help_string(self):\n\t\t\"\"\"\n\tGet the single-line help string, with main="}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tcmd = self.virsh.help(command_name)\n\t\tresults = cmd.stdout.strip().split('\\n')\n\t\tcmd.close()\n\t\tret = []\n\t\tfor line in results:\n\t\t\t# only add the line if it's not in the command line set\n\t\t\tif line.find(':') > 0:\n\t\t\t\tif line.find(self.cmd_prefix) > -1:\n\t\t\t\t\tret.append(line)\n\t\treturn ret\n\n\tdef get_cmd_help_str(self, cmd_name):\n\t\t\"\"\"\n\tGets the help string of"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\toptions = [\n\t\t\tshlex.split(argument) for argument in sys.argv[1:]\n\t\t\tif argument.find('-') >= 0\n\t\t\tand argument.split('-cf')[-1]!= 'op'\n\t\t]\n\t\tretval = []\n\n\t\tfor option in options:\n\t\t\tif option not in self.command_line_options:\n\t\t\t\tretval.append(option.replace('-', '_'))\n\t\treturn retval# -*- encoding: utf-8 -*-\n\"\"\"Test class for Foreman Discovery Rules\n\n.. note:: This program is free software; you can redistribute it and/or"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self.command_option_specs.get(command_name, [])\n\n\tdef validate_options(self):\n\t\t\"\"\"\n\t\tIf the command can be executed in the session that the mandatory options are correct\n\t\tReturns a list of the all the error messages\n\t\t\"\"\"\n\t\t#Check command line options\n\t\tself._check_commands(self.commands, self.command_option_specs.keys(), \"Check a valid options.\",\n\t\t                    \"Command not found.\")\n\n\t\t#Check required options without any subcommands (use with run_check_required).\n\t\trequired_options = list"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tself.commands = commands.commandParser.get_parser(command_name)\n\treturn self.commands['global_option_cfg'].commands\nfrom datetime import datetime\nfrom pathlib import Path\nimport random\nimport string\nimport urllib\nimport re\nimport random\nfrom time import mktime, localtime, time, timezone\n\nfrom collections import namedtuple\n\nfrom dateutil.parser import parse\nfrom django.conf import settings\nfrom django.db import models\nfrom django.contrib.auth.models import User, Group\nfrom django.db.models import Q\nfrom django.conf import settings"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\t#self.parser = optparse.OptionParser(usage=(\"%prog [get|set|reset] [COMMAND] [ARGS]\"),\n\t    # version=__version__)\n        cmd = self.parsers[\"set\"]\n        self.parser.add_option(cmd, \"-n\", type=\"int\", help=\"set how many commands to change on the given cmd\")\n        cmd.add_option(\"-s\", \"--sudo\", dest = \"sudo\", action = \"store_true\", \n\t\t\thelp = \"whether to check sudo access, set only with -r\")\n        cmd.add_option(\"-r\", \"--raw\", type"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tcmds = self._commands_dict['list'][command_name]\n\n\t\t#get all option names\n\t\topts = []\n\t\tfor command_name in cmds:\n\t\t\topts.extend(cmds[command_name]['cli_opts'])\n\n\t\treturn  opts\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self.options[command_name].get_opt_list()\n\ndef check_command_options(options, command):\n\t\"\"\"Checks the input to see if it uses the right command\n\t\n\t:param options: the options the user wants the command to take\n\t:param command: a valid command name from the command line (e.g. \"%macro:echo %foo\")\n\t:return: True or False\n\t\"\"\"\n\tfor option in options:\n\t\toption_value = getattr(command, option)\n\t\t# Get the option's value as a string\n\t\targ"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\topthal = {}\n\t\topthal['opt'] = \"all\"\n\t\topthal['opt'] += \"opthal \"\n\t\topthal['opt'] += \"opthal_timeout=\" + str(self.timeout) + \",job_state=\" + str(self.job_state)\n\t\topthal['opt'] += \"opthal_error=\" + str(self.error)\n\t\topthal['opt'] += \"opterr=\" + str(self.error_error)\n\t\topthal['opt'] += \"cltime=\""}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\toptions = []\n\t\tresult = self.xmlfile.run(\"%s -xml --machine --node %s\" % (command_name, self.domain_xmlfile))\n\t\tfor line in result.stdout_text.splitlines():\n\t\t\tif 'xml' in line:\n\t\t\t\tresult = self.xmlfile.run(\"%s -xmlout-xml --machine %s %s\" % (command_name, self.domain_xmlfile, line))\n\t\t\t\tfor i in result.stdout_text.split('\\n'):\n\t\t\t\t\tlinesplit = re.compile('[\\t ]*([^<"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tcommand_name = self.command_name_to_command(command_name)\n\t\tret_type = None\n\t\targ_type = None\n\t\targuments = []\n\t\tfor argument_name in self.positional_arguments():\n\t\t\targ = self.parse_argument(argument_name)\n\t\t\tif arg is not None:\n\t\t\t\targuments.append(arg)\n\t\t\telif arg is not None:\n\t\t\t\tret_type = arg_type\n\t\tif ret_type is None and arg_type is None:# default to command parameters\n\t\t\tret_type = self.command_name"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif not argument_name in self.__options.keys(): return None\n\t\treturn self.__options[argument_name]\n\t# -----------------------------------------------------------------------------------\n\n\tdef get_option(self, command_name, argument_name):\t\t\t\t\t\n\t\t\"\"\"\n\t\tReturns the specified Option value.\n\t\t\"\"\"\n\t\treturn self.__options[command_name][argument_name]\n\t\n\t# -----------------------------------------------------------------------------------\n\n\tdef get_option_args(self, command_name):\t\t\t\t\n\t\t\"\"\"\n\t\tReturns the Command-line arguments used when selecting options.\n\t\t\"\"\"\n\t\treturn self.__options[command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.config.get(command_name, {}).get(argument_name, None)\n\tdef get_configuration(self, command_name, command_args_name=None,\n\t\tcommand_key_name_names=None):\n\t\t\"\"\"\n\t\tGets the defined configuration file associated with the <command> node.\n\t\tUsage: configuration [command] [argument]\n        \"\"\"\n\t\tcommand = self.config.get(command_name)\n\t\targuments = parse_commandline(command)\n\t\tif command_args_name in arguments:\t\t# If name is specified"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self._command_spec[command_name]\n\n\tdef set_option_spec(self, command_name, argument_name, spec):\t\t# Set a new specification (and possibly a new\n\t\t# one). Arguments are passed as strings directly to the\n\t\t# :meth:`.configure` call.\n\t\tself._command_spec[command_name] = spec\n\n\tdef get_options(self, command_name):\t\t# (may be overridden)\n\t\t\"\"\"\n\t\t:return: List consisting of all options set from the given\n\t\tcommand type. For example,"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif self.has_commands:\n\t\t\tself.print_commands = True\n\t\tself.__commands.sort()\n\t\treturn get_option_spec_for_name(command_name, argument_name)\n\t\n\n\tdef set_option_spec_from_file(self, command_name, option_name):\n\t\t\"\"\"\n\t\tLoads the specified configuration file and set one or more\n\t\tvalues in as the command line arguments.\n\t\tIf a single argument name isn't passed to command line, that will\n\t\tbe treated as a single option from configuration files.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tcommand_descriptor = self._get_commands().get(command_name, None)\n\t\toption_spec = {}\n\t\tfor name, spec in command_descriptor:\n\t\t\tif spec.get('label') == argument_name:\n\t\t\t\toption_spec['label'] = name\n\t\t\t\targument_spec.update(spec)\t\t\t\t\n\t\treturn option_spec\n\t\t\n\tdef get_command_desc(self, command_name):\n\t\t\"\"\"\n\t\tReturns description of a previously-loaded command for the specified \n\t\tcommand name.\n\t\t\"\"\"\t\n\t\tname = self._commands.get("}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.cmd.get_option(command_name,argument_name)\n\t\n\tdef get_options_object(self, command_name):\t\"\"\"\n\tGets Options Object from the Command.\n\t\"\"\"\n\toptions = self.cmd.get_options(command_name)\n\t\n\tfor option in options:\n\t\tif option[0] == command_name:\n\t\t\treturn option\n\t\t# default has not been set correctly\n\t\telse:\n\t\t\treturn None\n\t\t\n\treturn None\n\t\n#################\n# Server Commands #\n#################\n\ndef build_help(session"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self._command_line_configuration.get(command_name, {}).get(argument_name)\n\t\n\tdef get_options(self):\n\t\t\"\"\"\n\t\tGets a list of all the available options.\t\t\n\t\t\"\"\"\n\t\treturn self._options.keys()\n\t\n\t@property\n\tdef options(self):\n\t\t\"\"\"\n\t\tGet a list of all the available options.\n\t\t\"\"\"\n\t\treturn self._options.values()\n\t\n\tdef format_option_spec(self, command_name, argument_name, optional=True):\n\t\t\"\"\"\n\t\tFormats a command line"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif (issubclass(command_name, BaseCommand)):\n\t\t\tkwargs = {}\n\t\telse:\n\t\t\tkwargs = dict(\n\t\t\t\tdest = argument_name,\n\t\t\t\taction = command_name.name,\n\t\t\t\trequired = True\n\t\t\t)\n\t\tmodule_path, handler = dbutil.find_module(command_name)\n\t\tif not module_path: return (command_name, arguments)\n\t\telse:\tmodule = handler(module_path)\n\t\treturn (command_name, arguments, module())\n\tget_command_spec = get_option_spec\n\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tresult = {}\n\t\tif not self.has_option(command_name):\n\t\t\traise RuntimeError('Command does not exist: %s.' % (command_name))\n\t\tif not self.has_argument(argument_name):\n\t\t\traise RuntimeError('Argument does not exist:'+ argument_name)\n\t\ttry:\n\t\t\tvalue = self.get_argument(argument_name)\n\t\texcept ValueError as e:\n\t\t\traise RuntimeError('Error getting argument: %s: %s.' % (argument_name, value))\n\t\telif value.isdigit() or value == '_':\n\t\t"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tret = []\n\t\tif self.args is not None:\n\t\t\tfor arg in self.args.split(','):\n\t\t\t\ttry:\n\t\t\t\t\tif len(arg.strip()) > 0 and arg[0] == '-':\t# if the argument starts with a '+'\n\t\t\t\t\t\tret += arg[1:]\n\t\t\t\t\telif len(arg) == 0:\t# remove the option\n\t\t\t\t\t\tpass\t# this should not happen\n\t\t\tif'' in ret and ')' in ret:\t# if the argument is a ')'  with no parameters\n\t\t\t\tret = ['{0} '.format(arg) for arg in"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn super(SilentArgs, self).get_silent_args(args) + ['silent_args']\n# -*- encoding: utf-8 -*-\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tresult = []\n\t\tfor e in self._silent:\n\t\t\tif e in self._silent_args:\n\t\t\t\tresult.append(repr(e))\n\t\t\telif e[0:2] == [\" \", \" \", \"\\t\", \"'\", \"''\", \"(\", \")\"]:\n\t\t\t\tif str(e[3]) == \"True\":\n\t\t\t\t\tresult.append(\"\\n% \" + repr(e[4]))\n\t\t\t\telse:\n\t\t\t\t\tresult.append(str(e[3]))\n\t\treturn result\n\nclass Processer():\n\tdef __init__(self, silent=False):\n\t\t"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tif len(args) > 1:\n\t\t\tres = [s.lower() for s in silenced_args[1].split(\", \")]\n\t\t\treturn args + res[1:]\n\t\treturn []\n\n\tdef get_output_from(self):\n\t\treturn self.input_path[\"input\"], self.output_path[\"output\"]\n\n\tdef load_file(self):\n\t\t\"\"\"docstring for load file\"\"\"\n\t\tself.input_path = self.load_file_by_extension(self.input_extension) \n\t\tself.output_path = self.load"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn [\"--silent\"] + args\n\n\tdef get_silent_args_dict(self):\n\t\t\"\"\"\n\tdictionary with silenced argument names in key=value format\n\n:param args: The received arguments.\n:return: None\n\t\"\"\"\n\t\treturn {\"-silent\" : [\"--silent\", \"-x\"]} # we just ignore the argnames\n\ndef parse_args(args):\n\t\"\"\"\n\tParse the passed list of arguments.\n\nThe dict of arguments contains all of the possible arguments that may be\nextracted by this method:\n\n\"\"\"\n\targs = make_"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent_args = []\n\t\tfor arg in  args:\n\t\t\tsilent_args += [ x.lower() for x in self.sels[arg].keys() ]\n\t\treturn silent_args\n\n\tdef get_silent_command(self, id):\t\n\t\tsilent_cmd = self.sels[id]\n\t\treturn silent_cmd\n\n\tdef get_args(self, args, silent=False):\t\n\t\tsilent_args = []\n\t\tfor idx, arg in enumerate(args):\n\t\t\tif arg in silent_args:\n\t\t\t\tsilent_args.remove("}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent = []\t\t# [True,...]\n\t\tsilent_args = []\t\t# [string]\n\n\t\ti=0;\n\t\twhile i < len(args):\n\t\t\tline_args_lenght = len(args[i])\n\n\t\t\tif line_args_lenght < 11 or line_args_lenght > 18 :\n\t\t\t\tbreak\n\n\t\t\targ = args[i].split(' ')\n\n\t\t\tif len(arg)!= 0:\n\t\t\t\tif arg[0] =='silent' :\t\n\t\t\t\t\tsilent.append(arg[1])\n\t\t\t\telif arg[0"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent_args = []\n\t\tfor i,arg in enumerate(args):\n\t\t\tif arg[1] is None:\n\t\t\t\tsilent_args.append(str(self.get_arg(i+1)))\n\t\t\telse:\n\t\t\t\tsilent_args.append(str(arg[1]))\n\t\treturn silent_args\n\n\tdef set_silent_args(self, silent_args):\t\t\n\t\t\"\"\"\n\t\tset the silent arguments\n\t\t\"\"\"\n\t\tself.silent_args = silent_args\n\n\tdef get_silent_options(self, options):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent = ['silent']\n\t\tp, l, n = 0, len(self.args), 0\n\n\t\twhile p < l and self.args[p] == '--':\n\t\t\tp += 1\n\t\tfor arg in args:\n\t\t\tn += 1\n\t\t\tsilent.append(self.args[p])\n\t\treturn silent, p + 1\n\n\t@cached\n\tdef get_silent_list(self, args):\n\t\t\"\"\"\n\tlist of silent argument\n\n:param args: The received arguments.\n:return: list, slienced option names\t\"\"\"\n\t\tsilent"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent = []\n\t\tif args['v']: silent.append(' '.join([self.get_ansi('B'),\n\t\t\t\t\t self.get_ansi('V')]))\n\t\tif args['l']: silent.append(' '.join([self.get_ansi('L'),\n\t\t\t\t\t self.get_ansi('L', False)]))\n\t\tif args['d']: silent.append(' '.join([self.get_ansi('D'),\n\t\t\t\t\t\tself.get_ansi('D', False)]))\n\t\treturn silent\n\n\tdef get_args(self, args):\n\t\t\"\"\"\n\tReturn the received arguments\n"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg.isPrimitive():\n\t\t\t\traise ValidationError(\"The optional arguments '%s' not provided.\" % self.name)\n\n\tdef validate_dependencies(self):\n\t\t\"\"\"\n\t\tPerform a sanity check for the dependencies of each value of the\n\t\tparameter. The check is performed only once for each parameter.\n\t\t\n\t\tValidates self.inputs\n\t\t\"\"\"\n\t\tself._validate_requires_type(\"input\", self.inputs)\n\t\tfor arg in self.inputs:\n\t\t\tvar = arg.paramType()\n\t\t\tif var in self._variables:\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor req in args:\n\t\t\tif not req.checkParamValid():\n\t\t\t\traise MKException(\"Argument '%s %s' is not required (parameter '%s').\"%(args[req.getName()], req.paramName, req.paramValue))\n\tdef checkParamValid(self):\t# Check if required parameters are provided.\n\t\tfor param in self.params:\t# Check if specified values are provided.\n\t\t\tif not param.isRequired():\n\t\t\t\traise MKException(\"Parameter '%s %s' is required (parameter '%s').\"%(param.getName(), param.paramName"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor p in self.get_all(args.get(\"module_name\"), args.get(\"module_number\")):\n\t\t\tif p.get(\"requires\").get(args.get(\"required\")) and args.get(\"default\"):\n\t\t\t\t# TODO\n\t\t\t\t\t# Add it to the mandatory options.\n\t\t\t\t\tself.add_error(p.get(\"name\"), args[\"module\"], \"requires\")\n\n\tdef validate_version(self):\n\t\t\"\"\"Check if the version of the module satisfies the requirements.\n\n\t\tIf any are missing, throw an exception.\n\t\t\"\"\"\n\t\tversions = P"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\targ_names = ('dbms', 'doctypes', 'permissions', 'ignore_mandatory', 'no_value_passed')\n\n\t\tfor name in arg_names:\n\t\t\tif not args.get(name):\n\t\t\t\tmsgprint(_(\"Please specify %s.\") % name,\n\t\t\t\t\traise_exception=IncorrectLdapError)\n\n\t@abstractmethod\n\tdef _call_sql(self, sql):\n\t\tif sql:\n\t\t\tself.cursor.execute(sql)\n\n\t#\n\t# Database Methods\n\t#\n\tdef connect(self, **conn_data):\n\t\ttry"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif self.require_args:\n\t\t\tfor arg in self.require_args:\n\t\t\t\tif None in arg or not arg[0] : # Only warn if None is provided\n\t\t\t\t\treturn False\n\t\t\t\tif not arg[0] in self.param_map[arg] : # No error message\n\t\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\t\treturn True\n\n\tdef get_version(self):\n\t\t\"\"\"\n\t\tReturn the current version of Cargo, used to generate the tarball\n\t\tof the project. Takes into account the version number at\n\t\tthe start of this project"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tdef is_required(key):\n\t\t\tif not key in args and not args[key]:\n\t\t\t\tthrow(_(\"You must specify {0}\").format(key))\n\n\t\treturn args\n\n\tdef add_arguments(self, parser):\n\t\tparser.add_argument('args', nargs='*')\n\n\tdef parse(self, user_arguments=None):\n\t\treturn self.parser.parse_args(user_arguments).parse_args(self.args)\n\n\tdef handle(self, *args, **options):\n\t\targs = self.parse(user_arguments=self.args)"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor req_arg in self._get_req_list():\n\t\t\tif not is_missing(req_arg):\n\t\t\t\treturn False\n\t\treturn True\n\tdef _get_req_list(self):\n\t\t\"\"\"\n\t\tRetrieves the parsed list of arguments from all the required\n\t\targuments (one name, one value) found in\n\t\tthe request.\n\t\tIf no argument was specified, it uses the default \n\t\tvalue. Otherwise it checks if all the required \n        arguments exist.\t\"\"\"\n\t\ttry:\n\t\t\t# Parse arguments\n\t\t\targ_list = [req_arg.strip"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif not args:\n\t\t\targs = {k.lstrip('-') for k in self.required_args_names}\n\n\t\tfor required_arg_name in sorted(self.required_args):\n\t\t\tif not getattr(args, required_arg_name) and \\\n\t\t\t\tnot getattr(args, required_arg_name):\n\t\t\t\tmsg = \"{0} requires a value. \\\n\t\t\t\t\t    Should be one of {1}\".format(args.name, ', '.join(self.required_args))\n\t\t\t\traise ArgumentException(msg)\n\n\tdef get_dest_for_task(self"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\trequired_args = any([x.strip(\"'\") for x in self.args.as_list() if args.get(x, '').strip(\"'\")])\n\t\tif required_args:\n\t\t\tif len(args.split()) > 1:\n\t\t\t\treturn False, \"If you provide a variable name, include the variable name and value.\"\n\t\t\tif len(args.split()) == 1:\n\t\t\t\treturn True, \"All of the provided command line arguments must be provided.\"\n\t\treturn True, \"\"\n\t\n\tdef set_args(self):\n\t\t\"Set the parsed arguments to be saved"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor x in self.required_args:\n\t\t\tif x not in args:\n\t\t\t\treturn \"Missing argument: \" + x\n\t\treturn True\n\n\tdef validate_mandatory_fields(self):\n\t\t\"\"\"\n\t\tValidate additionnal fields.\n\t\tValidate that all required fields were provided.\n\t\tIf there is not all required attributes present, raise an exception.\n\t\t\"\"\"\n\t\tfor x in self.req_data:\n\t\t\tif x not in self.field_map:\n\t\t\t\tself.add_error(line_number(x), 'Required field:'+ x)\n\t"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t    \n\targuments = \"\"\n\tfor arg in options_spec.keys():\n\t\telements = option_mapping.get(arg)\n\t\tif elements:\n\t\t\targuments += \"|\"\n\t\n\t\t# if the item is a simple regex it is treated as a\n\t\t# regular-expression and matched against the regular\n\t\t# expression (if any)\n\t\t# remove the item and its children\n\t\telements = re.sub(r\"\\W\", \"\", arg)\n\t\n\t\tif elements:\n\t\t\targuments += \"|\"\n\t\targuments += elements\n\t\t\n\treturn arguments\n\ndef get_user_command(options"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n    mandatory_args = []\n    for arg_name in args:\n        if arg_name not in mandatory_args:\n            mandatory_args.append(arg_name)\n            \n    return mandatory_args\n        \n\n\ndef get_option_names(options_spec, options):\n    \"\"\"\n    Get options in all configurations of the specified\n    command from the configuration file.\n    \n    :param options_spec:  command-line options in the config file\n    :param options:  configuration options\n    :return: command line options \n    \"\"\"\n    \n    command_name = options_spec.command\n    input_options = options_spec.options"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\toptions_spec = options_spec[:]\n\t\toptions = self.options_to_args(options_spec)\n\t\tfor option in options:\n\n\t\t\t# Check whether the argument has been provided\n\t\t\trequired_when = command_type()\n\n\t\t\t# Get the argument passed. If it isn't available in the\n\t\t\t# command line options, then it is not valid\n\t\t\t# FIXME: This is too complicated...\n\t\t\tfor option_name, option_type in self.option_types_to_args.iteritems():\n\t\t\t\tif option.required_when is None:\n\t\t\t\t\t# The missing"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n        # Validate options\n        if options_spec is None:\n            raise RuntimeError(\"Options spec is not specified\")\n        options_spec = list(options_spec)\n        if not hasattr(options_spec, '__iter__'):\n            raise ValueError('options_spec is not specified as a list (option_spec format: %s)'\n                                 % options_spec)\n\n        # Validate command\n        if not command_name in options_spec:\n            raise ValueError('command_name {0} not in options_spec'.format(command_name))\n\n        # Check that provided all arguments are of correct type\n        for arg in options_spec"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tif len(args) == 0:\n\t\t\targs = []\n\n\t\tif (not isinstance(options_spec, list)) or \\\n\t\t\t\t(''in options_spec and 'optional'in options_spec):\n\t\t\t\targs += options_spec\n\n\tdef get_match_args(self, option_name, options_spec):\n\t\targs = []\n\t\trequired_whens = {option_name: []}\n\n\t\tif options_spec:\n\t\t\trequired_whens = options_spec\n\n\t\tfor flag in options_spec:\n\t\t\t# Check against required when/required and not specified"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n        if options_spec not in ('required_when',) and command_name == 'default':\n            options_spec = {}\n\telif command_name in ( \"delete\" or \"all\" ) and command_name not in options_spec:\n\t    if len(options_spec) == 1:\n\t\toptions_spec.append(options_spec.)\n\t    else:\n\t\toptions_spec = ('required_when',) + options_spec\n\telif command_name in ( \"append\" or \"all\" ) and command_name!= 'append':\n\t    if len(options_spec) == 1:"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\targs = self.args\n\t\tfirst = True\n\t\tfor arg in args:\n\t\t\tif not first:\n\t\t\t\tself._raise_syntax_error(\"%s requires a first argument, and a second one, got %s\" % \\\n\t\t\t\t\t(command_name, first))\n\t\t\tfirst = False\n\t\t\targs.pop(0)\n\n\t\treturn args\n\t\n\tdef _raise_syntax_error(self, command_name, reason):\n\t\traise IsyValueError(\"Cannot specify both '%s' and '%s'\" % (command_name, reason))\n\t\n\tdef get_command_args"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\targs_matched = (args is None or not args[command_name:]!= options_spec[command_name:])\n\t\t# TODO: do some special processing for missing optional args?\n\n\t\tif not args_matched:\n\t\t\tif command_name == 'optional':\n\t\t\t\targs = options_spec[command_name : ][0]\n\t\t\telse:\n\t\t\t\tmsg = 'Option %s requires argument but no additional args after subcommand' % command_name\n\t\t\t\traise ValueError(msg)\n\n\t\treturn args_matched\n\n\tdef _parse_command_line(self, command_line, options"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tcommand_args = []\n\t\tif'required_when' in options_spec:\n\t\t\tcommand_args.append('--%s' % (options_spec.required_when))\n\t\tif 'condition' in options_spec:\n\t\t\tcommand_args.append('--%s' % (options_spec.condition))\n\n\t\treturn command_args\n\n\tdef _convert_command(self, action, argument, options_spec, command_name):\n\t\tparsed_argument = parse_argument(argument)\n\t\tparsed_args = self._get_arguments(command_name, options_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\ttry:\n\t\tif cmd_module.has_command(command_name):\n\t\t\tcmd_module.check_commands(command_name, \"--help\")\n\t\tall_args = cmd_module.find_args(command_name, options_spec)\t\n\t\treturn all_args\n\texcept Exception:\n\t\ttry:\n\t\t\tname, value = get_arg_with_default(command_name, default=\"no-\"+command_name)\n\t\t\tcommand = cmd_module.find_commands(command_name)\n\t\t\toption = cmd_module.get_option("}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlength = min(len(args), len(self.arguments))\n\t\tif length > self.arguments.maxlen:\n\t\t\traise ValueError(\"Argument cannot be longer than {} characters: {}\".format(length, repr(self.arguments)))\n\n\t@property\n\tdef is_positional(self):\n\t\treturn self.positional\n\n\tdef get_parameter_type(self):\n\t\treturn (self.positional or '').lower() or os.path.splitext(self.positional)[1]\n\n\nclass EnumArgument(BaseArgument):\n\t\"\"\"\n\tAn Argument based on Enum type.\n\t\"\"\""}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.maxlen:\n\t\t\traise Exception('too many parameters %u > %u' % (len(args), self.maxlen))\n\nclass IntVar(float):\n\tdef __new__(cls, *args, **kwargs):\n\t\treturn float.__new__(cls, *args, **kwargs)\n\nclass CharVar(str):\n\tdef __new__(cls, *args, **kwargs):\n\t\treturn str.__new__(cls, *args, **kwargs)\n\nclass DateVar(str):\n\tdef __new__(cls, *args,"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.minimum_length or len(args) < self.maximum_length:\n\t\t\traise ValueError('Arguments provided are longer than {} parameters'.format(\n\t\t\t\t\t\t\t\t\t\t\t\tself.minimum_length))\n\n\tdef _get_validator(self, name):\n\t\t\"\"\"Get validator for given name, return the validator itself.\"\"\"\n\t\tvalidators = self._registry.get(name, {}).get('validators', [])\n\t\tif validators:\n\t\t\treturn validators[0]\n\n\t\tif name.startswith('http://ois-x.org/') or name.startswith('https://'):"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > len(self.get_args()):\n\t\t\tfrappe.throw(_('Argument length of {0} cannot be longer than {1}\\\n\t\t\t\t\t\tmessage')\n\t\t\t\t\t\t\t.format(length_message(args[0]), self.get_doctype_label()))\n\n\tdef get_length(self, arg):\n\t\t\"\"\"\n\t\tGet length of given value based on given type and length\n\t\t:param arg: Argument to check.\n\t\t\"\"\"\n\t\treturn len(str(arg)) + 1\n\n\tdef format(self, doc):\n\t\t\"\"\"\n\t\tFormat formatted doc."}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif self._length is not None and len(args)!= self._length:\n\t\t\tlogging.error(\"length value of args from field '%s' \" \\\n\t\t\t\t\"is %d (expecting %d)\\n\", args, self._length, self._name)\n\n\tdef _check_mandatory(self, args):\n\t\t\"\"\"Validate that mandatory argument is present, raise exception if not.\"\"\"\n\t\tfor param in self._mandatory:\n\t\t\tif not param in args:\n\t\t\t\traise ValueError(\"%s is mandatory.\" % param)\n\n\tdef _check_unique(self, args):\n\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlength_params = {}\n\t\tfor key in args.keys():\n\t\t\tif length_params.has_key(key):\n\t\t\t\tl = len(args[key])\n\t\t\t\tif l >= length_params[key][0]:\n\t\t\t\t\tlength_params[key] = (l, length_params[key][1] + 1)\n\t\t\t\t\tdel args[key]\n\n\tdef add_length_param(self, length_param):\n\t\t\"\"\"Add the length param\n\n\t\t:param length_param: The length param\n\t\t\"\"\"\n\t\tlength_params = self.length_params"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\t# For now just use this list of arguments as this could be\n\t\t# wrong, though, like the following code:\n\t\t#if len(args) < 2:\n\t\t\t#raise TypeError(\"Wrong number of arguments.\")\n\t\t\n\t\t# For now just check length\n\t\tif len(args) >= self.minlength:\n\t\t\traise ValueError(\"Invalid arguments.\")\n\t\n\tdef get_value(self, key):\n\t\t\"\"\"Get value from raw data\"\"\"\n\t\t# For now just check key \n\t\t#Check if value is a string and its in ascii,\n\t\t#if it is, try to"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlen_invalid = []\n\t\tfor arg in args:\n\t\t\ttry:\n\t\t\t\tlen_invalid.append(int(arg))\n\t\t\texcept ValueError:\n\t\t\t\tlogger.warn(\"The length must be expressed exactly 0-255 characters. Length cannot be zero.\")\n\t\t\t\tsys.exit(1)\n\t\t\n\t\tif len_invalid:\n\t\t\tlogger.warn(\"The length specified on the command line contains invalid (must not exceed length of %d)\" % len_invalid)\n\t\t\n\n\n\t\t\ndef main():\n\tp = argparse.ArgumentParser()\n\tp.add_argument(\"-v\", \"--verbose\","}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif len(arg) < 1:\n\t\t\t\traise ValueError, \"Argument to 'validate_length_args' must be at least 1 character\"\n\n\tdef convert_str(self, type, value):\n\t\t\"\"\"\n\t\tCast str type to python type and return casted pointer.\n\n\t:param type: Name of the type. This string will be returned as such\n\tindicated by the type's parameter type. Possible values are specified\n\tin type.\n\t:param value: Pointer to the cast value.\n\t:return: Pointer to the casted"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor num, arg in enumerate(args):\n\t\t\tif len(args[num]) < self._max_arguments:\n\t\t\t\traise ValueError(\"Arguments must be longer than {} parameters ({}).\"\n\t\t\t\t\t\t\t.format(self._min_length, self._max_arguments))\n\n\tdef __call__(self):\n\t\treturn self._arguments()\n\nclass Type(ABC):\n\tdef __init__(self, name, value):\n\t\tABC.__init__(self)\n\t\tself.name = name\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\treturn self.name +"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, (list, tuple)):\n\t\t\tfor choice in args:\n\t\t\t\tif isinstance(choice, tuple):\n\t\t\t\t\tif choice[0] in self.choices:\n\t\t\t\t\t\tself.choices.remove(choice[0])\n\t\t\t\t\t\tprint \"choices are removed from list\"\n\t\t\t\t\t\tself.value = args[0]\n\t\t\t\t\t\tdel args[0]\n\t\t\t\t\t\tself._validate_choices()\n\n\tdef validate_choices(self):\n\t\t\"\"\"\n\t\tCheck if value of choices is a combination of allowed choices.\n\t\tvalidates integers and strings.\n\t\t\"\"\"\n\n\t\timport re\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not self.valid_options_list(args):\n\t\t\tself.print_help()\n\t\t\texit()\n\n\tdef validate_options_list(self, choices_list):\n\t\t\"\"\"\n\t\tEnsure that the specified value is within the range acceptable for\n\t\ta list of options.\n\n        :param choices_list: The values supplied to choices_list\n\t\twill be checked.\n\t\t\"\"\"\n\n\t\tif not isinstance(choices_list, list):\n\t\t\tself.print_help()\n\t\t\texit()\n\n\tdef parse_args(self, args_str, require_all_tokens"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tvalid_choices = None\n\t\ttry:\n\t\t\tvalid_choices = parse_choice_list(args)\n\t\texcept:\n\t\t\tpass\n\t\tif valid_choices:\n\t\t\tconfig.available_choices = valid_choices\n\t\t\tif \"unicable\" in config.available_choices:\n\t\t\t\tconfig.unicable.unicable = config.unicable.unicable list.getSelectionList()\n\t\t\tif len(known_networks)!= 3:\n\t\t\t\tself.session.open(\n\t\t\t\t\tMessageBox,\n\t\t\t\t\t_(\"The '%s' argument is not complete\", \"unicable.py\"),"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not self.validators:\n\t\t\tself.validators = []\n\t\tif isinstance(self.validators, (list, tuple)):\n\t\t\tfor v in args:\n\t\t\t\tif not isinstance(v, (list, tuple)) or len(v)!= 2:\n\t\t\t\t\tself.validators.append(str)\n\t\telse:\n\t\t\tself.validators.append(str)\n\n\tdef __call__(self, val):\n\t\ttry:\n\t\t\tv = self.validators[val]\n\t\t\tif not isinstance(v, tuple):\n\t\t\t\tself.validators[val] = self.defaults.get"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor i in args:\n\t\t\tif str(i).strip() == \"\" or str(i).strip() == \"0\":\n\t\t\t\tif i.find(\"flags\")!= -1 or i[-3:] == \":\":\n\t\t\t\t\traise ValueError(\"%s: bad options value\" % arg)\n\t\t\t\tflags = [i]\n\t\t\t\tdel i[-3:]\n\t\t\t\tif int(flags[0], 8)!= 0:\n\t\t\t\t\traise ValueError(\"%s: bad value\" % arg)\n\t\t\t\tif int(flags[1], 16)!= 0:\n\t\t\t\t\traise ValueError(\"%s: bad value\" %"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn True\n\n\tdef validate_length(self, args):\n\t\tif self._length is None:\n\t\t\treturn False\n\n\t\tif len(args)!= len(self._choices):\n\t\t\tself._errors.append(_(\"Incorrect syntax of length arguments \"\n\t\t\t\t\t\"to number of choices\"))\n\t\t\treturn False\n\n\t\tif args[0] not in self._choices:\n\t\t\tself._errors.append(_(\"Invalid argument to number of choices\"))\n\t\t\treturn False\n\n\t\t# Now check the length; if it doesn't take any\n\t# length criteria and we're out of possible range,\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 1 or args[0][0] not in ('help', 'h', 'list','store']:\n\t\t\traise ArgumentError(\"Please enter two integers\")\n\n\t\tfor index in range(len(args)):\n\t\t\tif not args[index].strip():\n\t\t\t\tcontinue\n\n\t\t\tif self._options[args[index]] is None:\n\t\t\t\traise ArgumentError(\"Choice {} needs an argument without this is not set\".format(args[index]))\n\t\t\telse:\n\t\t\t\tself._options[args[index]] = args[index]\n\n\tdef _get_valid_choices"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) == 1 and args[0] in self.choices:\n\t\t\tfor n, k, v in self.choices:\n\t\t\t\tif n == args[0]:\n\t\t\t\t\tself.choices[n] = str(v)\n\t\t\t\t\tdel self.choices[n]\n# -*- coding: utf-8 -*-\n#\n# This file is part of EUDAT B2Share.\n# Copyright (C) 2016 University of Tuebingen, CERN.\n#\n# B2Share is free software; you can redistribute it and/or\n# modify it under the"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\t# Check this is a valid argument\n\t\tif not args:\n\t\t\treturn InvalidArguments(_(\"You need to supply an argument to use this command.\"))\n\t\t# Check argument validity\n\t\tfor opt in self.options.values():\n\t\t\tassert not isinstance(opt[1], (int, six.string_types))\n\t\t# Check argument type\n\t\tfor typ in self.options.types():\n\t\t\tif not isinstance(typ, (int, str, bool)):\n\t\t\t\treturn InvalidArguments(_(\"Argument '%s' of type '%s' must be of type int, string or bool type.\" % (opt"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tvalid_values = get_valid_values(args)\n\t\titem_supplied_items, item_supplied_quants = get_itemised_items(args)\n\n\t\tif valid_values:\n\t\t\tfor item_code, value in valid_values.items():\n\t\t\t\tif not item_supplied_items.has_key(item_code):\n\t\t\t\t\tmsgprint(_(\"Row #{0}: \").format(item_code), raise_exception=1)\n\n\t\t\t\tif item_supplied_quants.has_key(item_code):\n\t\t\t\t\tmsgprint(_(\"Quant #{"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif arg_lower_bound(args.lower, args.upper):\n\t\t\tif args.value < args.min_value or args.max_value > args.min_value or args.value > args.max_value:\n\t\t\t\traise ValueError(\"arguments are not within the interval ({0} and {1})\".format(args.min_value, args.max_value))\n\t\t\telse:\n\t\t\t\treturn True\n\t\telse:\n\t\t\traise ValueError(\"Arguments start with a value in [{0}, {1}]\".format(args.lower, args.upper))\n\n## \t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tstart_time = self._timer.isActive()\n\t\tdelta = (time.time() - start_time) / 60.0\n\t\tactual_min = self._args['min'] if'min' in self._args else None\n\t\tactual_max = self._args['max'] if'max' in self._args else None\n\n\t\t# if min/max values are specified:\n\t\tif'min' in args:\n\t\t\tif actual_min == args['min']:\n\t\t\t\t# a range check was done, return it:\n\t\t\t\t# (True, None)... (None"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in sorted(args):\n\t\t\ttry:\n\t\t\t\tv = int(arg)\n\t\t\texcept ValueError:\n\t\t\t\t# Invalid integer\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tv = max(min(v, self.message.get(\"max\")), 0)\n\t\t\texcept ValueError:\n\t\t\t\t# Invalid max length (must be <= max_val)\n\t\t\t\tcontinue\n\t\t\tself.message[\"min_val\"][arg] = v\n\n\t\t# check for minimum and maximum values\n\t\tif self.message.get(\"min\") is not None and \\\n\t\t\tself.message[\"min_val\"]["}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif type(args) is tuple and len(args) > 0:\n\t\t\tmin_value = args[0]\n\t\t\tif type(min_value) is tuple and len(min_value) > 0:\n\t\t\t\tmax_value = args[1]\n\t\t\t\tif type(max_value) is tuple and len(max_value) > 0:\n\t\t\t\t\tif min_value > max_value:\n\t\t\t\t\t\traise ValueError(\"{0} cannot be greater than max_value\".format(args[0]))\n\"\"\"\nSupport for interface with a Broadlink RM device.\n\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\targs_len = len(args)\n\n\t\tif args_len < 1:\n\t\t\treturn False\n\n\t\tfor idx in range(1, args_len):\n\t\t\tminimum,maximum = args[idx-1], args[idx]\n\t\t\tif not 0 < minimum <= maximum:\n\t\t\t\treturn False\n\n\t\treturn True\n\nclass CardSet:\n\t\"\"\"A card set.\"\"\"\n\n\t_hand = None\n\t_deck = None\n\t_index = None\n\n\tdef __init__(self, cardset_name, cardset, name):\n\t\tself._name = name\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor opt in self.params:\n\t\t\tvalid = False\n\t\t\tv = None\n\t\t\tif opt.is_fixed_min():\n\t\t\t\tv = self.params[opt][1]\n\t\t\telif opt.is_fixed_max():\n\t\t\t\tv = self.params[opt][0]\n\t\t\tif v is not None and v > args[opt]:\n\t\t\t\targs[opt] = v\n\t\treturn args\n\n\tdef get_param_values(self, param):\n\t\t\"\"\"\n\t\tGet allowed (non-negative) parameters for this parameter.\n\n\t\t:param param: The parameter"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tmin_value, max_value = None, None\n\t\tfor i in range(len(args)):\n\t\t\targ = args[i]\n\t\t\tif MIN_ARGUMENT <= arg <= MAX_ARGUMENT:\n\t\t\t\tmin_value = arg\n\t\t\telif arg < MIN_ARGUMENT:\n\t\t\t\tmax_value = arg\n\n\t\tif min_value is not None:\n\t\t\tif max_value == MIN_ARGUMENT:\n\t\t\t\tmin_value += 1\n\t\t\telif max_value == MAX_ARGUMENT:\n\t\t\t\tmax_value -= 1\n\t\t\telse:\n\t\t\t\traise error.InputError('Expecting value:"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\targuments = [ x.get_min(), x.get_max(), x.get_min_max(), x.get_max_min() ]\n                for k,v in args.iteritems():\n                        if v<self.min_value: \n                                arguments.append(k)\n\t\t\t\n\t\tif len(self.validators) > 0:\n\t\t\t\targuments.append('(optional arguments)' )\n\t\telse:\n\t\t\t\targuments.append('(optional arguments not found)' )\n\t\tif self.max_value:\n\t\t\targuments.append('(maximum possible values)' )\n\t\t\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor key, value in args.iteritems():\n\t\t\targ_min = cstr(key)\n\t\t\targ_max = cstr(key) +'is not within'+ cstr(value)\n\t\t\tif arg_min < arg_max:\n\t\t\t\traise frappe.ValidationError(\"Min value required\")\n\n\tdef validate_required_if_missing(self, doc):\n\t\t\"\"\"Validate that document is empty\"\"\"\n\t\trequired_fields = [\"fiscal_year\", \"from_fiscal_year\", \"to_fiscal_year\"]\n\n\n\t\tfor field in required_fields:\n\t\t\tfor"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tmin_value = float(args[0]) if args[0] else None\n\t\tmax_value = float(args[1]) if args[1] else None\n\t\tif min_value and max_value:\n\t\t\tif max_value < min_value:\n\t\t\t\tfrappe.throw(_(\"{0} must be greater than or equal to {1}\").format(\"Minimum Value\",\n\t\t\t\t\tfrappe.bold(min_value)), UnobValidationCondition)\n\t\t\tif min_value > max_value:\n\t\t\t\tfrappe.throw(_(\"{0} must be less than or equal to {1"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tprint \"Create a complex argument type\"\n\t\tret = create_complex_type(subcommand, type_name, option_name)\n\t\tif ret is None:\n\t\t\tsys.stderr.write(\"Error creating the complex type '%s'.\\n\" % type_name)\n\t\t\tsys.stderr.write(param_error[1])\n\t\treturn ret\n\n\tdef format_simple_arg(self, arga, argb, type_name, options, spec_option):\n\t\t\"\"\"Build the complex argument\"\"\"\n\t\ttry:\n\t\t\tres = \"%s%s%s"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tif type_name not in self.complex_types:\n\t\t\tsys.exit(\"[ERROR] Invalid complex argument type '%s'\\n\" % type_name)\n\t\t\n\t\ttype_inst = self.complex_types[type_name]\n\t\toption = option_inst.option\n\t\tif type_inst.parse_command_args:\n\t\t\targ_value = get_value(subcommand.options, option)\n\t\t\tif arg_value is not None:\n\t\t\t\ttype_inst.value = arg_value\n\t\telse:\n\t\t\ttype_inst.value = get_value(option"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tif not spec_option:\n\t\t\treturn option_name\n\t\telse:\n\t\t\treturn self.option_types[option_name]['complex_types'].get(subcommand, None)\n\n\t# ------------------------------------------------------------------------------\n\n\tdef get_complex_argumet_type(self, command, type_name, option_name, spec_option,\n\t\t\t\t\t\t\t   default=None):\n\t\t\"\"\"\n\t\tBuild the complex argument type\n\n\t\t:param command: the command name\n\t\t:param type_name: the complex type name\n\t\t:param option_name: the option name\n\t\t:param spec"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\ttypes = [opt for opt in self.spec_opts if opt.name == type_name]\n\t\tif not types:\n\t\t\traise InvalidCommand(\"No such type for %s.\" % type_name)\n\t\ttype = types[0]\n\n\t\t# build the type instance\n\t\treal_type = type(self, option_name, None, option_name)\n\t\toption_index = option_index or types.index(type)\n\t\twhile real_type._short_opts and option_index:\n\t\t\toption = real_type._short_opts[option_index]"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tcomplex_type_class = ComplexArgumet_by_name[type_name][0]\n\t\t\n\t\tcomplex_type = self._get_complex_argument_type(complex_type_class, subcommand, option_name)\n\t\t\n\t\tself._make_output_type(complex_type.name, options.command_spec, type_name, complex_type)\n\t\t\n\t\treturn complex_type\n\t\t\n\tdef _get_complex_argument_type(self, complex_type, subcommand, option_name):\n\t\tif subcommand == 'add_argument':\n\t\t\treturn ComplexArgument(description='"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tcomplex_type=ComplexArgumenter(type_name, option_name, [])\n\t\tif complex_type:\n\t\t\tself.parser.add_argument('-s', '--spec', '--spec', nargs='+', action=spec_argumenter.check_argument, help='The parameter names and values of the complex argument type.')\n\t\t\tself.parser.add_argument('-v', '--val', '-v', action=spec_argumenter.check_argument, help='The value of the complex argument.')\n\t\t\tself.parser.add_argument('-a', '--default', action="}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\targ = '-D%s' % option_name\n\t\ttype_instance = self.__convert_argument_to_type(type_name,arg,option_name,arg)\n\t\treturn type_instance\n\n\t'''\n\tAdd the option arguments\n\n:param parser: the parser being parsed\n\t:type parser: :class:`argparse.ArgumentParser`\n\t:return: the argument parser instance\n\t:rtype: :class:`argparse.ArgumentParser`\n\t'''\n\toptions = {}\n\toptions[\"name\"]  += \" -C %s\" % self.short_name\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn create_type(self.parse_template(\"_build_complex_type\", subcommand=subcommand,\n\t\t\t                                           command=make_option_tuple(type_name, option_name, \"complex\", \"type\", type_name),\n\t\t\t                                                     spec_option=spec_option, options_spec=repr(spec_option), help=help_string), \"complex_argument\")\n\n\tdef _convert_arg_line(self, arg_line_option, help_string):\n\t\targ_line_option_value = arg_line_option[2:]\n\t\targ_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\toption_def = {}\n\t\tspec_option_def = {}\n\t\tfor k in self.opt_list: options.append((k, self.opt_dict[k]))\n\t\toption_def = self.opt_def\n\t\tspec_option_def = self.spec_option_def\n\n\t\tif len(options)!= 0: return option_def, spec_option_def\n\t\t#end if\n\n\t\treturn option_def,[k for k in options if k not in spec_option]\n\t\t#end if\n\t\treturn option_def, spec_option_def"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tdef print_single_arg_type(s, line, indent):\n\t\t\t\"\"\"Print single argument type\"\"\"\n\t\t\tif not s:\n\t\t\t\treturn\n\n\t\t\t# If a \"real\" parameter is requested, create a complex\n\t\t\t# type with the real value and set its value as the arg\n\t\t\tif s['real']:\n\t\t\t\tcomplex_type_type = s['type']\n\t\t\t\tcomplex_type_name = s['real_type_name']\n\t\t\telse:\n\t\t\t\tcomplex_type_type = ComplexType(complex_type_name, option_name, [])\n\t\t\t# Set"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\treturn dict(), dict(options=args,\n\t\t\tnested_args=NestedArgumentHolder.get_options_for(args)\n\t\t\t)\n\n\n# vim: fileencoding=utf-8 et sw=4 sts=4\nimport sys\nfrom services.spawn import MobileTemplate\nfrom services.spawn import WeaponTemplate\nfrom resources.datatables import WeaponType\nfrom resources.datatables import Difficulty\nfrom resources.datatables import Options\nfrom java.util import Vector\n\n\ndef addTemplate(core):\n\tmobileTemplate = MobileTemplate()\n\t\n\tmobileTemplate.set"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tdict = {}\n\t\tfor arg in filter(lambda x: x not in self.control_opts,\n\t\t\tfilter(lambda x: hasattr(self, x), args)):\n\t\t\tif arg not in self.control_args:\n\t\t\t\tdict[arg] = []\n\t\t\tdict[arg].append(self.control_args[arg])\t\t\t\n\t\treturn dict\n\t\n\tdef update_nested_custom(self, dict, custom):\t\t\n\t\t\"\"\"\n\t\tUpdate a nested dictionary with custom entries, \n\t\t\n\t\tArguments:\n\t\tdict: The dict for update.\n\t\tcustom: The custom"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tcontrol_args = dict()\n\t\tnested_args = dict()\n\t\t# Get parameters.\n\t\tcontrol_args = self.extract_arg(args)\n\t\tnested_args = self.extract_arg(args)\n\n\n\t\tdef _add_opt_arg_if_empty(args, inner_args):\n\t\t\t# If the argument is blank then return None.\n\t\t\tif len(args) == 0:\n\t\t\t\targ_key = None\n\t\t\t\tfor inner_arg in inner_args:\n\t\t\t\t\targ_key = inner_arg\n\t\t\t\t\tif inner_arg[0:"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targuments = dict()\n\t\targuments.update(self.parse_args(args))\n\t\tnested_custom = dict(args[1])\n\t\tif \"name\" in args[0]:\n\t\t\t# Use nested_args to add arguments.\n\t\t\tkwargs = dict(args[0].pop(\"args\"))\n\t\t\targuments.update(\"{0}_{1}_{2}\".format(args[0][\"name\"], args[0][\"label\"],\n\t\t\t\targs[1][\"label\"]))\n\t\t\targuments.update(kwargs)\n\t\t\tif args[0].get(\"type\")==\"int\" and kwargs.get(\""}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = self.process_options(args)\n\t\tif args is None:\n\t\t\treturn None, None\n\t\telse:\n\t\t\targs_flat = flatten_dict(self._get_nested_args(args))\n\t\t\tif args_flat:\n\t\t\t\treturn args_flat, args\n\t\t\telse:\n\t\t\t\treturn None, None\n\n\tdef get_nested_custom_from_cli_args(self, cli_args):\n\t\t\"\"\"\n\t\tFind all options from the cli arguments and return all combinations\n\t\tfrom the opt args. This is usefull if there is no conflict.\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\treturn {}, {}\n# -*- coding: utf-8 -*-\n#\n# nbd2json documentation build configuration file, created by\n# sphinx-quickstart on Tue Sep 15 10:20:44 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tdef flatten(input):\t\treturn list(input[0]) + [flatten(item) for item in input[1:]]\n\n\t\tnested_args = nest(flatten(args), [])\n\t\t\n\t\tresult = dict(\n\t\t\tcontrol_args=dict(flat_inputs=nested_args, variables=self._vars)\n\t\t)\n\n\t\t# Note: this is not a full implementation of nest, use the built-in\n\t\t# logic.\n\t\treturn result\n\n\tdef _generate_custom_arg_tuple(self, args, prepend=False, prepend_name=False):\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tnested_args = []\n\t\t# check if any of the arguments needs to be split into a compound\n\t\tif not isinstance(args, list):\n\t\t\treturn (args, None)\n\n\t\tfor i in range(0,len(args)):\n\t\t\tif args[i] == '|':\n\t\t\t\t# get the value of the tag that is being parsed\n\t\t\t\tvalue = self.get_tag(args[i+1:-1])\n\n\t\t\t\t# get the value of the 'extended' attribute if it was\n\t\t\t\t# found in the current definition\n\t\t\t\textended_arg  = self.get_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t# Get arg's name, and remove the.json suffix:\n\t\tif self.arg == '--merge-json':\n\t\t\tdel args['--merge-json']\n\t\tif self.arg == '--merge-xml':\n\t\t\tdel args['--merge-xml']\n\t\tif self.arg == '--delete-old-backup':\n\t\t\tdel args['--delete-old-backup']\n\n\t\t# Convert input args (if any) into list (if any) and put into arg\n\t\t# and to args.\n\t\tinput_args = self.arg.rsplit(' ') if self.arg"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\n\t_control_args = {'control': [],\n\t\t\t  'command': [],\n\t\t\t  'description': [],\n\t\t\t  'command_args': []}\n\t\n\t_nested_args = {'command': [], 'description': [],\n\t\t'extended': [],\n\t\t'args': [], 'configurable': True, 'type': None}\n\t\n\t\t# This list is the result of the command line parsing for any \n\t\t# command which calls it with this information.\n\t\tcommand_list = []\n\t\tfor arg in args:\n\t\t\tif arg.split('.')[-1] in ['cmd_', 'cmd_plus_"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\treturn dict(zip(extra_vars, vars_dict.get(extra_vars, [])))\n\t\treturn None\n\n\tkeys = list(extra_vars.keys())\n\tif extra_vars:\n\t\tkeys += extra_vars\n\n\tdef _mergedict(orig, repl, val):\n\t\t\"\"\"\n\t\tInternal recursive build_dict implementation.\n\t\t\"\"\"\n\t\tif isinstance(val, dict):\n\t\t\torig.update(val)\n\t\t\tif isinstance(repl, dict):\n\t\t\t\trepl.update(val)\n\t\t\ttry:\n\t\t\t\treturn"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars is None:\n\t\t\tif isinstance(extra_vars, (list, tuple)):\n\t\t\t\tconfig = ConfigObj()\n\t\t\t\tconfig.update(extra_vars)\n\t\t\t\textra_vars = config.get_extra_vars()\n\t\t\telse:\n\t\t\t\textra_vars = []\n\n\t\tdef _merge(dic, old_val, new_val):\n\t\t\told_entries = _merged_dict(old_val.items(), new_val.items())\n\t\t\tnew_entries = _merged_dict(new_val.items(), old_entries)\n\t\t\tif isinstance"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\treturn tuple(extra_vars + list(vars_dict.values()))\n\n\t\tresult = dict()\n\n\t\tvarg_re = re.compile('\\$(\\w+|\\{[^}]*\\})')\n\t\tfor extra_vars_re in extra_vars:\n\t\t\tfor varname, val in iteritems(vars_re.groupdict(False)):\n\t\t\t\tresult[varname] = val\n\n\t\tif extra_vars:\n\t\t\t# Pull out variables in the extra-vars pattern\n\t\t\ttmp_variables = set()\n\t\t\tfor varname, val in iteritems(extra_vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\n\tif __merge_dicts:\n\t\tmerged_dict = __merge_dicts(vars_dict, extra_vars)\n\telse:\n\t\tmerged_dict = {}\n\n\tif merge_lists:\n\t\tfor key in vars_dict:\n\t\t\tmerged_dict.update(vars_dict[key])\n\n\treturn merged_dict  \n######################################################################\n# Run\n######################################################################\n\n\ndef run_command(cmd, shell = True, print_output = True):\n\t\"\"\"executes the cmd in a shell, if return code!= 0, raise errors if it exits\"\"\"\n\n\tif shell"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\tfor var in vars_dict:\n\t\t\t\tif var not in extra_vars:\n\t\t\t\t\textra_vars.append(var)\n\t\treturn extra_vars\n\n# Copyright 2013 Canonical Ltd.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\tif not isinstance(extra_vars, list):\n\t\t\t\textra_vars = [extra_vars]\n\n\tfor k in extra_vars:\n\t\tif k in vars_dict:\n\t\t\tcontinue\n\t\tif isinstance(vars_dict[k], dict):\n\t\t\tmerge_extra_vars(vars_dict[k], extra_vars)\n\t\telse:\n\t\t\tvars_dict[k] = dict(extra_vars + vars_dict[k])\n\ndef load_config_dict(app):\n\t\"\"\" Load module_name and config_file"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tdef merge_dict_with_extra(d1, d2):\n\t\t\td = d1.copy()\n\t\t\tc = _replace_vars(d, extra_vars)\n\t\t\td2.update(c)\n\t\t\treturn str(d)\n\n\t\tif isinstance(extra_vars, dict):\n\t\t\treturn merge_dict_with_extra(vars_dict, extra_vars)\n\t\t# Merge new dict and old dict\n\t\telse:\n\t\t\treturn dict(vars_dict.items() + extra_vars.items())\n\treturn merge_dict_with_extra("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif isinstance(vars_dict, dict):\n\t\t\textra_vars = [vars_dict[var] for var in extra_vars \\\n\t\t\t\tif var in vars_dict]\n\tret = {}\n\tfor (key, val) in iteritems(extra_vars):\n\t\tif key!= 'name' and val is not None:\n\t\t\tret[key] = val\n\treturn ret\n\n@conf\ndef _safe_eval(regex, val):\n\t\"\"\"\n\tSafe eval, that evaluates to a string that could be\n\tused as a python string in some situations, but"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tnew_vars = []\n\t\tfor k,v in vars_dict.iteritems():\n\t\t\tif isinstance(v, list):\n\t\t\t\tnew_vars.extend(v)\n\t\t\telse:\n\t\t\t\tnew_vars.append(v)\n\t\treturn extra_vars + new_vars\n\"\"\"\nA script for generating the reports for the different kinds of events, and\nanalysis graphs.\n\n:copyright: 2015 Red Hat Inc.\n\"\"\"\n\nimport fnmatch\nimport json\nimport os\nimport re\nimport string\nimport subprocess\nimport sys\nimport glob\nimport time\nimport shutil\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor key, val in iteritems(vars_dict):\n\t\t\tif not extra_vars.get(key):\n\t\t\t\tvars_dict[key] = val\n\t\t\telse:\n\t\t\t\t# there are places like new variables\n\t\t\t\tif isinstance(val, dict):\n\t\t\t\t\textra_vars.update(val)\n\t\t\t\t\tcontinue\n\n\t\t\t\tif hasattr(val, '__call__'):\n\t\t\t\t\tif val.__name__ == 'Task':\n\t\t\t\t\t\ttask = val\n\t\t\t\t\telif val.__name__ == 'PlayContext':\n\t\t\t\t\t\tplaycontext = val\n\t\t\t\t\telse:\n\t\t\t\t\t\ttask = task.copy(exclude_parent"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t# TODO: refactor this (or elsewhere) to take the vars/vars_per_host somehow.\n\t\t#  Will potentially use the \"extra-vars\" dict because a special parameter.\n\t\t#  If that ever changes, make a temporary copy before continuing.\n\t\t#  This may result in errors if ansible variables change and also\n\t\t#  not re-run ansible or this will be problematic if another variable is not\n\t\t#  defined at all. As long as you modify this, then this will not affect\n\t\t#  your playbook.\n\t\t#\n\t\t#  This copy"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\n\t# make sure we have a plugin argument for the current workspace\n\tif ir_workspace:\n\t\tir_plugins = ir_plugin.get_enabled_plugins()\n\telse:\n\t\tir_plugins = []\n\n\t# if we're not running any of the requested plugins, stop here\n\tif not ir_workspace:\n\t\tir_plugin_names = ir_plugins\n\telse:\n\t\tir_plugin_names = [ ir_plugin.name ]\n\n\tif verbose is False:\n\t\t# print some info, or verbose to the console\n\n\t\tprint '# ansible: starting"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n        # Create a playbook instance\n        self.create_ansible_playbook(ir_workspace)\n\n        # Create a directory for the playbook\n        if verbose is None:\n            verbose = os.path.join(ir_workspace.base_directory, \"ansible-playbook\")\n            if not os.path.exists(verbose):\n                os.makedirs(verbose)\n            \n        # Add any extra variables to the cli command line\n        extra_vars = ansible_args.copy()\n        extra_vars['ANSIBLE_TEST'] = \"True\"\n        extra_vars['ANSIBLE_INVENTORY_FILE'] = \"playbooks/%s/%"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n    ansible_args = {'oc': 'yaml'}\n    \n    # we'll just need to get the hostname and\n    # port of the agent - use ansible-plugin -p <hostname>\n    # to find the default v2 api\n    with api.lcd( ansible_path = ansible_path ) as lcd_api:\n\t    lcd = LCDConnection( verbose )\n\tif( not lcd.check_version() ):\n\t\t# if not, then we're down.  Exit with an error\n\t\tlcd.disconnect(lcd_api, cli_path, verbose)\n\t\tif verbose"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\n\t\tcli_wrapper = _get_cli_wrapper(ir_plugin, verbose)\n\t\tcli_wrapper.prompt = lambda msg: None\n\t\t# TODO: remove from cli_wrapper.prompt before Ansible 1.2 can support\n\t\tcli_wrapper.max_width = 120  # TODO\n\n\t\t# NOTE: I think this is very likely to raise \"Bad value received\"\n\t\t#       when no user is prompted to enter something in it\n\t\tcli_wrapper.token = None\n\t\tcli_wrapper.become_pass = None\n\t\tcli_wrapper.become_prompt = None"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ndef playbook(ir_workspace, ir_plugin, verbose=None, extra_vars={}):\n\t\"\"\"Calls AnsiblePlaybook to inject an ansible playbook script which can later be run\n\twithout waiting.\n\n\t:param ir_workspace: The workspace to look for the playbook scripts\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t\"\"\"\n\t_run_command(\n\t\t_get_playbook_cmd(ir_workspace, ir"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n    # Make sure we are on the python version we're executing.\n    script_args = [arg for arg in \"python open_yaml_config.py\".split()\n                 if \"--python-version\" in arg]\n    if script_args and not script_args[0] in [\"import-error\", \"import\", \"exit\"]:\n   \t\n        parser.error(\"Unable to invoke 'import-error', see above for \"\n                   \"usage. Try a shell with 'bash'.\" + \"\\n\\n\"\n                   \"Example: ansible-playbook open_yaml_config.py -c...\")\n    \n    # We"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\n\t# TODO: Refactor to use ansible config parameter parsing (ie, host, # FIXME: do we?)\n\n\t#TODO: refactor so we can add options for the plugin and for later injection\n\n\t_plugin = PluginManager(ir_workspace, plugin_path)\n\tplugin = _plugin.Plugin(inventory_path, ir_plugin)\n\n\t# We need to figure out what to invoke the plugin and have the options.\n\t# Unfortunately the CLI does not properly assign options for our plugin\n\t# to make sure the right values are being set, so we are more strict about this"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t)\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2017-2020  The Project X-Ray Authors.\n#\n# Use of this source code is governed by a ISC-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/ISC\n#\n# SPDX-License-Identifier: ISC\nfrom prjxray import util\n\n# Test for xraysinpaint rendering\n# We have to disable this, since we will need the following import\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\tansible_args.update(extra_vars)\n\tansible_args.update(ansible_args_from_playbook(ir_workspace, verbose, extra_vars))\n\t\n\twith AnsibleExitStack() as exitstack:\n\t\trobbundle.util.call_stack(exitstack, dry_run=True)\n\"\"\"\nUnit tests for pyuavcan.transport.connection\n\n\"\"\"\nfrom pyuavcan.transport.exceptions import ProtocolError, \\\n        InvalidCredentialsError, ResponseError\n\n\nclass ProtocolErrorTest(object):\n    def test_message(self):\n        \"\"\"Transport"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\ttry:\n\t\t\treturn cli_args\n\t\texcept Exception as ex:\n\t\t\tcli.parser.print_help()\n\t\t\tprint_error(ex, \"This is not a valid context plugin, use one!\")\n\n\n\tif isinstance(ir_plugin, IARPlugin):\n\t\tir_plugin.get_plugin_result()\n\n\t# Update dict\n\t# This method can be used to pass 'template vars dict\n\t# into'results' variable\n\t_update_vars.__call__(*vars_dict)\n\ndef _update_vars(update_dict, cli_args):\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t       # TODO(cmaloney): Add debug messages, etc.\n\t       cli_ret = None\n\t       try:\n\t\t        ansible_args = cli_args[2:]\t\t       # remove the first \" \"\n\t\t        for j in range(1, len(vars_dict) + 1):\n\t\t           cli_ret = cli_ret + str(vars_dict[j]) + \" \"\n\t\t           cli_args = cli_args[:j] + vars_dict[j] + ansible_args\n\t\t        # print(vars_dict)\n\t        # Log some stuff\n\t        logging.debug(\""}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# add to base params. If not already added\n\t\tif vars_dict is None:\n\t\t\tvars_dict = {}\n\n\t\t# add to the command line args\n\t\tcli_args_extra = []\n\t\tcli_args_list = []\n\t\tcli_args_dict = {}\n\n\t\tmodule_args = vars_dict.get(\"module_args\", {})\n\t\tcli_args_list = vars_dict.get(\"cli_args\", {})\n\t\tif isinstance(cli_args_list, str):\n\t\t\tcli_args.append(list(map(str, cli_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tcurrent = dict()\n\t\tir_plugin.plugin_parameters = ''  # Will be passed as Ansible extra-vars\n\t\tir_runner = AnsibleRunner(ir_plugin._task.args)\n\t\tir_runner.playbook = ir_plugin.playbook\n\t\tir_runner.vars_dict = vars_dict\n\t\tir_runner.remote_user = ir_plugin.username\n\t\tir_runner.remote_pass = ir_plugin.password\n\t\tir_runner.remote_port = ir_plugin.port\n\t\tresults = ir_runner.run()\n\t\tcurrent['"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\trun_ansible(cli_args, var_bind, debug=True)\n\n##########################################\n# UTILITY FUNCTIONS :\n# - Main entry point for ansible run/install\n# - Specific groups of commands for ansible run/install\n#\n# - Infrared plugins can register their own file structure to this\n#   to ensure they all have their own file structure set up\n# - They can also directly be called as an Ansible inventory script.\n#   This is somewhat dubious and destructive to the output of the\n#   ansible inventory script (and will only contain)"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t # TODO: Use a better API instead...\n\t# logger.info('{0}{1}'.format('run_playbook', vars_dict))\n\t\n\t\tretvalue, resvalue, output = RunPlugin(cli_args, vars_dict, ir_plugin, ir_workspace)\n\t\tif retvalue!= 0:\n\t\t\tprint >> sys.stderr, \"ERROR. See 'run_plugin' method log for more info\"\n\t\t\tprint >> sys.stderr, \"ERROR: {0}\".format(retvalue[-1])\n\n\t\tif resvalue:\n\t\t\t# if there are any output, save stdout"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif not vars_dict:\n\t\t\tcommon.warning('-' + _('\\tNo vars_dict passed to ansible runner, skipping this section'))\n\t\t\treturn ir_plugin.exit_code\n\n\t\t# copy variables\n\t\tcmd = list([x for x in vars_dict.keys() if vars_dict[x]])\n\t\t# filter on common arguments\n\t\tcommon.filter_results(dict((key, vars_dict[key]))\n\t\t\tfor key in cmd)\n\t\t\n\t\t# build result\n\t\tstatus, output = _return_output(cli_args, out=output, rose_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\trunner_result = runner_base(vars_dict, ir_workspace, ir_plugin)\n\t\tsys.exit(runner_result)\t\t\t\t\t\t\n\ndef main(cli_args=None):\n\t\"\"\"\n\tRuns a given Ansible script on an appliance.\n\t\"\"\"\n\tinit_global_variables(cli_args)\t\n\t# Setup the workspace object\n\t_initialize_workspace()\t\n\t# Execute the ansible cli\n\tif __name__ == '__main__':\n\t\texec_cli(cli_args)\n#!/usr/bin/python\n#\n#"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# Get the playbook and command line arguments for it to execute\n\t\tansible_conf_file = '/etc/ansible/ci/plugins/{0}'.format(ir_plugin.name)\n\t\t# Read in the file we are about to execute, remove any existing items (key=value pairs; remove those)\n\t\tansible_conf_contents = None\n\t\twith open(ansible_conf_file, \"r\") as f:\n\t\t\tansible_conf_contents = f.read()\n\n\t\t# Add the extra-vars to ansible_conf_contents\n\t\tansible_conf_contents += cli_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n       for cli_arg in cli_args:\n\t\tcli_vars = vars_dict[cli_arg]\n\t\tmodule_path = os.path.dirname(ir_plugin.module_path)\n\t\tif not rose.variable.array_exists(cli_vars[cli_arg]):\n\t\t\traise ValueError(\n\t\t\t\t\"%s option in ansible.ini is missing\" % cli_arg\n\t\t\t)\t\t\n\t\t\n\t\t# check if the CLI has a config file\n\t\twith open(ir_plugin.config_file, 'r') as f:\n\t\t\ttry:\n\t\t\t\tconfig ="}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues = values_dict\n\t\tif parser_name not in ['image_type_vm', 'image_type_p2p', 'image_type_qcow2', 'image_type_qed']:\n\t\t\tvalues[\"command_line\"] = \"<argument name=\\\"%s\\\" value=\\\"%s\\\"/>\" % (parser_name, str(values[\"value\"]))\n\t\tvalues[\"values\"] = {}\n\t\tvalues[\"values\"][parser_name] = []\n\t\tfor name_value_pair in values.get(\"values_list\", list()):\n\t\t\tvalues[\"values\"][parser_name"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tcommand_dict = values_dict\n\t\tfor key, value in command_dict.items():\n\t\t\tcommand_dict[command.command].extend(value.split())\n\t\tdel command_dict[\"DEFAULT_VALUE\"]\n\n\tdef _run_command(self, command, **options):\n\t\t\"\"\"Run a CLI command and returns the appropriate result tuple including a\n\t\tlist of lines of CLI commands.\n\n\t\t:param command: Name of CLI command, e.g. commmand or vmkcreate\n\t\t:param options: Command specified options\n\t\t:type command: String\n\t\t:type"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif not hasattr(values_dict, \"__iter__\"):\n\t\t\tvalues_dict = []\n\t\tif parser_name == \"list_opts\":\n\t\t\tvalues_dict = parser_values_list(values_dict)\n\t\tif parser_name == \"list_keys\":\n\t\t\tvalues_dict = parser_values_list(values_dict)\n\t\tif parser_name == \"dict_key_val_split\":\n\t\t\tvalues_dict = parser_values_keyval(values_dict)\n\t\t\tvalues_dict = dict(zip(parser_values_keyval(values_dict),"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor key, value in values_dict.iteritems():\n\t\t\tif type(value) == str:\n\t\t\t\tsetattr(self, key, value.split())\n\t\t\telif type(value) == list:\n\t\t\t\tnew_list = []\n\t\t\t\tfor value_item in value:\n\t\t\t\t\tnew_list.append(value_item.split())\n\t\t\t\tsetattr(self, key, new_list)\n\n\tdef _get_args_str(self):\n\t\t\"\"\"\n\t\tGet the first arg (self.argv[0]) and transform it into a string.\n\t\t\"\"\"\n\t\tcmd_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor key, value in values_dict.items():\n\t\t\tif not isinstance(value, basestring):\n\t\t\t\traise ValueError()\n\t\t\ttry:\n\t\t\t\tvalues_dict[key] = _convert_non_cli_arg(value, self._values)\n\t\t\texcept ValueError:\n\t\t\t\tvalues_dict[key] = _convert_cli_arg(value, self._values)\n\n\tdef _check_args(self, values_dict):\n\t\tno_args_args = [k for k, v in values_dict.items() if not str(k).startswith('no_')]"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\targs_dict = vars(self.get_argparse_args(parser_name, values_dict))\n\t\tfor kw_name in args_dict:\n\t\t\tif kw_name not in self.kwargs:\n\t\t\t\tdel args_dict[kw_name]\t\t\t\t# delete keyword argument\n\t\treturn super(self.__class__, self)._convert_non_cli_args(parser_name, values_dict)\n\n\tdef _run(self):\n\t\tif self.logger is None:\n\t\t\traise RuntimeError('Must specify a logger')\n\t\t\"\"\"\n\t\tRun command, e.g"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = {'action_type': None, 'access_type': None, 'access_control_list': None, 'ip_address': None, \\\n\t\t\t'ipv6_address': None,'mac_address': None, 'net_mask': None, 'auto_delete': None}\n\n\t\tparser = self.commands[parser_name]\n\t\tfor arg_name in values_dict:\n\t\t\tparsed_value = parser._get(_to_str(values_dict[arg_name]), values_dict)\n\t\t\tsetattr(self, arg_name, parsed_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor key in values_dict.keys():\n\t\t\tassert values_dict[key] is not None, \"Key {} was not found\".format(key)\n\n\tdef _set_from_cli(self, options, values_dict):\n\t\t\"\"\"\n\t\tHelper function to set the values from a cli option.\n\t\tValid values are in either of the type(s) above\n\n\t\t:param options: The options to operate on.\n\t\t:param values_dict: The dict of values\n\t\t\"\"\"\n\t\t# If value is a list convert to list\n\t\tif isinstance(values_dict"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif values_dict.get(self.command_name) is not None:\n\t\t\tvalues_dict[self.command_name] = self.command_name\n\nclass VirshCommand(Command):\n\t\"\"\"\n\tThe class of the generic Virsh cmd.\n\tThis class is used for executing commands on remote host. It reads\n\ta single line and communicates with with_exit_code as a callback function.\n\n\tExample:\n\n\t.. code-block:: bash\n\n\t   virsh cmds = VirshCommand('virt-ls', raise_if_fail=False"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = {}\n\t\tfor key in values_dict:\n\t\t\tvalues_dict[key] = values_dict[key].split()\n\t\tparser = getattr(self, parser_name)\n\t\tfor key in values_dict:\n\t\t\tvalue = values_dict[key]\n\t\t\tparser.values.ensure_value(key, value)\n\t\treturn parser\n\n\tdef __setattr__(self, key, value):\n\t\t\"\"\"\n\tSets a value from the given dictionary.\n\t:param key: The name of the parameter.\n\t:param value: The value to set"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_data = {}\n\t\tif plugin_dir.endswith(\"plugins.xul\"):\t# XUL XUL\n\t\t\tplugin_name = \"xul.plugins.version.%s.%s.zip\" % (plugin_dir, plugin_dir)\n\t\telif plugin_dir.endswith(\"plugins.wixlib\"):\t\t# WixLib XB\n\t\t\tplugin_name = \"wixlib.plugins.%s.%s.zip\" % (plugin_dir, plugin_dir)\n\t\telse: # XPC\n\t\t\tplugin_name = plugin_dir\n\t\tplugin_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tresult = {}\n\t\tresult['name'] = os.path.basename(plugin_dir)\n\t\tresult['dir'] = os.path.dirname(plugin_dir)\n\t\tresult['properties'] = {}\n\t\tresult['properties']['filetypes'] = {}\n\t\tresult['properties']['filetypes']['.md5sum'] = plugin_dir\n\t\t# result['folder']  = os.path.basename(plugin_dir)\n\t\tresult['folder']  = os.path.basename(fileserver.dir() + \"/\" + plugin_dir.replace('/', os.sep))\n\t\t"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn_dict = {}\n\t\tfor item in os.listdir(plugin_dir):\n\t\t\ttry:\n\t\t\t\treturn_dict[item_name(item)] = os.path.getsize(os.path.join(plugin_dir, item))\n\t\t\texcept Exception:\n\t\t\t\tif isplugindir(item):\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\traise\n\n\treturn return_dict# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n   "}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tproperties = {}\n\t\tproperties[\"PLUGINPATH\"] = plugin_dir\n\t\tproperties[\"PLUGINNAME\"] = \"Plugin Plugin %s\" % PLUGIN_NAME\n\t\tif not plugin_dir.endswith(PLUGIN_EXT):\n\t\t\tproperties[\"PLUGINPATH\"] += \"/\"\n\t\tproperties[\"PLUGINJSONPATH\"] = os.path.join(plugin_dir, PLUGIN_JSON_FILE) if not os.path.isfile(PLUGIN_JSON_FILE) else PLUGIN_JSON_FILE\n\t\tproperties[\"PLUGINPROPERTIES\"] = get_pluginproperties(PLUGININFO_PLUGIN_PATH"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn get_plugin_spec_from_dir(plugin_dir,\n\t\t\t\t\t\t\t\t\t   ['command','files_to_lint','files_to_run'],\n\t\t\t\t\t\t\t\t\t   plugin_dir=plugin_dir)['properties']\n\ndef get_list_from_dir(dir_path,plugin_dir,filter_list=[],ignore_list=[]):\n\t\"\"\"\n\tReturns a list of all plugins defined in the directory provided.\n\n\t:param dir_path: A directory path where all the plugins are stored\n\t                e.g. __dir__/\n\t:param plugin_dir: A path"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tflat_dict = {}\n\t\tif plugin_dir.endswith('/'):\n\t\t\t# we need to recursively get the build/framework path\n\t\t\tplugin_dir = plugin_dir[0:-1]\n\t\t\tplugin_file = listdir(plugin_dir)\n\t\t\t# we can get the.plugin files in the plugin directory for the plugin\n\t\t\tplugin_files = []\n\t\t\tfor f in plugin_file:\n\t\t\t\tif f.endswith('.plugin'):\n\t\t\t\t\tplugin_files.append(path + '/' + f)\n\t\t\tfor f in plugin_files + glob(f'{plugin_dir}/__"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_spec_dict = {}\n\t\tfor name, value in list(PLUGIN_ATTRIBUTES.items()):\n\t\t\tif name not in plugin_dir:\n\t\t\t\tcontinue\n\t\t\tvalue = resolve_template(plugin_dir[name], value)\n\t\t\tif isinstance(value, basestring):\n\t\t\t\tplugin_spec_dict[name] = value.split(\"|\")\n\t\t\telse: # assume string or unicode - only strings allowed\n\t\t\t\tif isinstance(value, list):\n\t\t\t\t\tplugin_spec_dict[name] = value\n\t\t\t\telse:\n\t\t\t\t\tplugin_spec_dict[name] = [value"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\ttry:\n\t\t\tdict_out = {}\n\t\t\tdict_out[PLUGIN_DIR] = get_plugin_info_plist(plugin_dir, \"*.ini\")\n\t\t\tdict_out[PLUGIN_NAME] = {}\n\t\texcept:\n\t\t\tpass\n\t\treturn dict_out\n\treturn {}\n\ndef get_plugin_info_plist(path, list=False):\n\t\"\"\"\n\tReads the specified path to the plugin.plist\n\tFile \"path\" to the plugin.plist is parsed as a python data structure holding everything about\n\tthe parsed xml file\n\t\n\t\"\"\"\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tproperties = {}\n\t\tplugin_info = plugin_dir + '/plugins.yaml'\n\t\ttry:\n\t\t\tif not os.path.isfile(plugin_info):\n\t\t\t\treturn properties\n\t\texcept Exception as e:\n\t\t\traise ValueError('plugin directory %s, %s' % (plugin_dir, str(e)))\n\n\t\tplugin_properties = flatten_dict(parse_file_to_dict(plugin_info), self.yaml_format_version)\n\t\tplugin_spec_properties.update(plugin_properties)\n\t\treturn plugin_properties\n\nclass YamlConfig("}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn __flatten_dict(filter_config_to_dict(\n\t\t\t_load_from_dir(plugin_dir, '.plugin_spec')\n\t\t)\n\t)\n\ndef _load_from_dir(plugin_dir, filename, validate):\n\t\"\"\"\n\tLoad a file\n\n:param plugin_dir: A path to the plugin's dir\n:param filename: The file that the plugin reads\n\t:param validate: Flag to validate the configuration\n\t\"\"\"\n\timport yaml\n\twith open(filename, 'r') as fh:\n\t\tcontent = yaml."}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif self.config_variable is not None:\n\t\t\tself.config_variable[:] = self.config_variable\t# update env variable\n\t\tself.writeConfig()\t\t\t\t\t\t# write new configuration file\n\t\tself.config = MyConfigParser()\n\tdef get(self,section,parameter):\n\t\t\"\"\"\n\t\tGet a configuration setting.\n\t\tsection: either section_name or property\n\t\tparameter: (name,value) string, like 'test', for a configuration setting\n\t\t\"\"\"\t\t\t\n\t\tsection = \"%s.%s\" % (section,parameter)\n\t\treturn self.config"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tfrom..environment_config import Config\n\t\tif self.value is None:\n\t\t\treturn\n\t\tself._path = self.value\n\n\tdef expand_user_path(self):\n\t\t\"\"\"\n\t\tExpand ~ and environment variables and ~user if necessary.\n\n\t\t@note: this will try to optimize things, but will implicitly expand ~ or\n\t\tenvironment variables found into it in every folder and will always add \"~user\".\n\n\t\t@return: an expanded ~ or user path\n\t\t\"\"\"\n\t\tif self.value is not None:\n\t\t\tif not (self._path and os.path."}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.config = ConfigObj(False, False)\n\t\tself.init()\n\n\n\tdef create_dirs(self):\n\t\t\"\"\"\n\t\tCreate the directories and initialize them as needed.\n\t\t\"\"\"\n\t\tif not os.path.exists(self.base_dir):\n\t\t\tos.mkdir(self.base_dir)\n\t\t\tprint \"created %s\" % (self.base_dir)\n\nclass ConfigObj:\n\t\"\"\"\n\tHolds the config object informations and provides methods and functions for parsing them.\n\t\"\"\"\n\n\n\tdef __init__(self, init_"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tconf_fp = os.environ.get('OAI2_INI_CONF_FP', None)\n\t\tif conf_fp is None:\n\t\t\tconf_fp = './data/inject_config.json'\n\t\tself._config = util.get_config(conf_fp)\n\t\tself._config['token'] = util.get_config(self._token)\t\n\tdef write_to_file(self,file_data):\n\t\t\"\"\"Write to a specified file with json format data\"\"\"\n\t\tfile_data = util.remove_empty_keys(file_"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not self._env['PATH']:\n\t\t\tself._env['PATH'] = []\n\t\tif self._env['PATH']:\n\t\t\tself._env.load(self._env['PATH'])\n\t\tself.env = self._env\n\n\tdef get_config_vars(self, paths = ''):\n\t\t\"\"\"\n\t\t@param paths: The environment variable or list of paths.\n\t\t@type paths: List\n\t\t@return: The paths to the configuration files.\n\t\t@rtype: List\n\t\t\"\"\"\n\t\tif isinstance(paths, basestring):\n\t\t\tpaths = path(paths, type"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not os.environ.has_key('PATH'):\n\t\t\tself.environ['PATH']=''\n\t\tif not self.environ.has_key('DJANGO_SETTINGS_MODULE'):\n\t\t\tfrom django.conf import settings\n\t\t\tself.environ['DJANGO_SETTINGS_MODULE']=settings.SETTINGS_MODULE\n\t\tif not self.environ.has_key('DJANGO_CONFIGURATION'):\n\t\t\tself.environ['DJANGO_CONFIGURATION']=''\n\t\tif not self.environ.has_key('DJANGO_SETTINGS_MODULE'):\n\t\t\tfrom django.conf import settings\n\t\t\tself.environ['DJANGO_SETTINGS_"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not os.path.isfile(self.configPath):\n\t\t\tprint(\"[Config] No configuration file found at '{0}'\".format(self.configPath))\n\t\t\treturn\n\t\t\n\t\twith open (self.configPath) as f:\n\t\t\tconfStr = f.read()\n\t\t\n\t\tself._config = json.loads(confStr)\n\t\tself._config.setdefault('environment', {})\n\n\tdef get(self, section, option):\n\t\t\"\"\"\n\t\tReturn the setting of the given option.\n\t\t\"\"\"\n\t\tif not self._config:\n\t\t\treturn self._config    \n\n\t\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not self.env_for_all:\n\t\t\tself.set('ENV_CONFIG_PATH',\n\t\t\tos.path.join(self.bld.bldnode.abspath(), '..', '..', '..', 'extras'))\n\n\t#\n\t# FOR RPATH\n\t#\n\n\tpath = (os.path.join(self.bld.srcnode.abspath(), 'config')\n\t\tif not os.path.exists(path)):\n\t\t\tos.makedirs(path)\n\n\t#\n\t# TARGET\n\t#\n\n\tif not Options.options.upload:\n\n\t\t# make a target for"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\ttry:\n\t\t\tf = open(_os.path.join(self.config_base, \"config.py\"), \"w\")\n\t\texcept IOError, e:\n\t\t\tprint \"Could not read config.py. Continuing.\"\n\t\t\treturn False\n\t\t\n\t\tf.write(\"#\n\t# Environment #\n\t#\n\n\ttry:\n\t\tself.config.environ = self.conf_env.env['env']\n\t\tself.config.environ['CFG_TMPDIR'] = cstr(normpath(os.getcwd().replace('\\\\', '\\\\\\\\')))\n\texcept:\n\t\tself.config"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif hasattr(self, 'config'):\n\t\t\treturn\n\t\tself.env = {}\n\t\tself.uselib = self.to_list(getattr(self.config, 'uselib', []))\n\t\tself.includes = self.to_list(getattr(self.config, 'includes', []))\n\t\tif self.uselib:\n\t\t\tself.flags.extend(self.to_list(self.uselib))\n\n\tdef run(self):\n\t\t\"\"\"\n\tParse the command line and read the command line. This method\n\tshould be called only once in the main process."}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tcli.extend_parser(self, root_subparsers, \"spec-sub-parser\")\n\t\tcli.description = \"Specify a spec file and set which options are \" + \\\n\t\t\t\"applied to the specification.  See the man pages for \" + \\\n\t\t\t\"more information.\"\n\t\tcli.epilog = \"Use the'spec-file' sub-command to define options in the \" + \\\n\t\t\t\"specification file and set options as needed.\"\n\nfrom __future__ import with_statement\nimport os.path\nimport re\nfrom functools import partial\nfrom fabric.contrib."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tparser   = self._parser\n\t\tparser.add_argument('-r',\n\t\t\t\t\t\t\t'--run-root',\n\t\t\t\t\t\t\tdefault=self._run_root,\n\t\t\t\t\t\t\taction='store',\n\t\t\t\t\t\t\tdest='run_root'\t\t\n\t\t\t\t\t\t)\n\n\t@staticmethod\n\tdef _set_defaults():\n\t\t\"\"\"\n\t\tset the default arguments for a cli, and return the option dict.\n\t\t\"\"\"\n\t\topt = {}\n\t\topt['run_root'] = None\n\t\topt['run_flags'] = None\n\t\treturn opt\n\n\tdef _process_cmd(self, cmd, argv"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\t# The command_table() class will be instantiated if we\n\t\t# have the parser defined options.\n\t\tif self._subparsers is None:\n\t\t\tself._subparsers = {}\n\t\tif self._parser is None:\n\t\t\tself._parser = argparse.ArgumentParser(\n\t\t\t\tusage=\"usage: %prog [filename1 [filename2...]]\")\n\t\t\t# Need a formatter so add in some of the options...\n\t\t\tformatter = self._parser.formatter_class(\n\t\t\t\tprog=(self.prog if self._parser.prog is not None else 'convert'))\n\t\t\t# We want the"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\n\n\tcopy_opts = copy.deepcopy(self.cli_options)\n\tcopy_opts[\"args\"] = argparse.Namespace()\n\tcopy_opts.name_override = argparse.Namespace(help=\"The name to use to create the new subcommand.\")\n\tcopy_opts.title_override = argparse.Namespace(help=\"The title to use to show the subcommand.\")\n\tcopy_opts.description_override = argparse.Namespace(help=\"The description to use to describe the subcommand.\")\n\tcopy_opts.aliases = argparse.Namespace(help=\"Aliases to add to the new subcommand.\")"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\t# Now that a single parser is given, set up the parent parser and\n\t\t# return it.\n\t\tparent_parser = self._parent_parser()\n\t\tparent_parser._parents.append(self._parent_parser())\n\n\t\t# Update the top-level namespace of every subparser and its\n\t\t# sub-parsers. The key from the namespace is the name of the subparser.\n\t\tif subparser.name is not None:\n\t\t\tparent_parser._name = subparser.name\n\n\t\t# Update the scope context.\n\t\tparent_parser.set_usage(cli."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tfor opt in root_subparsers:\n\t\t\topt.add_to_parser(opt.handler)\n\n\n\tdef add_common_args(self, parser, parent):\n\t\tCommonParser.add_common_args(self, parser, parent)\n\n\t\tcommon_args = parser.add_argument_group(\n\t\t\t\"Common options\", \"Common options\")\n\t\tcommon_args.add_argument(\n\t\t\t\"--debug\", \"-D\",\n\t\t\tdefault=False, action=\"store_true\",\n\t\t\tdest=\"loglevel_debug\",\n\t\t\thelp=\"enable logging to debug for diag"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=\"Tool for the Mindatra Command Line Processing Tool.\")\n\t\tparser.add_argument(\"app_conf_file\", type=argparse_filestring,\n\t\t\tdefault=None, nargs=\"+\")\n\t\tparser.add_argument(\"app_conf_opts\", type=ArgumentParser, default=None, nargs=\"+\")\n\t\tparser.set_defaults(main=self, namespace_args=False,\n\t\t\t\tmain_with_dash=False, help=False)\n\t\tself.app_conf_opts = parser.parse_args(namespace="}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself.configure_subparser(root_subparsers)\n\t\tself.options.extend(root_subparsers)\n\n\tdef finalize_options(self):\n\t\t\"\"\"\n\t\tFinalizes the commandline-wide CLI options.\n\t\t\"\"\"\n\t\tself.command_options = self.parser.parsed[0].opts\n\t\tself.cli_options = self.parser.parsed[1].opts\n\n\tdef call_subparser(self, cli, opts, args):\n\t\t\"\"\"\n\t\tCalls a sub-parser with the options selected.\n\n\t\t:param cli:  the CLI options object"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tsubparser.add_argument(\"name\",\n\t\t                  help=\"The name of the section's section.\")\n\t\tsubparser.add_argument(\"description\",\n\t\t                  default=\"An automated tool for visualizing\")\n\t\tsubparser.add_argument(\"arguments\",\n\t\t                  nargs=\"*\",\n\t\t                  help=\"The argument passed to the section.\")\n\t\tsubparser.add_argument(\"--only\",\n\t\t                  action=\"store_true\",\n\t\t                  help=\"Only show the section description.\")\n\t\treturn\n\n\tdef main(self, parser, namespace):\n\t\twith open(namespace.__file__, \"r\", \""}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\t# TODO: Add support for other parsers.\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself.main_parser = self.main_subparsers[\"spec\"]\n\t\tself.main_parser.choices[\"package_types\"].choices[\"installed\"] = (\"set\", \"set as current pkg type\")\n\t\tself.main_parser.defaults[\"remove_prefix_dist\"] = False\n\n\tdef run(self):\n\t\tif self.main_parser is not None:\n\t\t\targs = self.parse_args()\n\t\t\tif args is None:\n\t\t\t\treturn False"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tresult = os.path.abspath(os.path.join(root, path))\n\t\tif not os.access(os.path.dirname(path), os.X_OK):\n\t\t\tif debug:\n\t\t\t\tprint(\"Remove root file %s\" %path)\n\t\t\tos.unlink(path)\n\t\telse:\n\t\t\tlogging.debug(str(result)+path+\": no such file or directory\")\n\n\nclass SimpleConfig(object):\n\tdef __init__(self, basedir, configfile):\n\t\tself.basedir = basedir\n\t\tself.configfile = configfile\n\t\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor name in map(removepath, path.split(\"/\")):\n\t\t\tparts = name.split(\"/\")\n\t\t\tif parts[:-1] in root:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tif path in map(removepath, path.split(\"/\")):\n\t\t\t\troot.remove(path)\n\treturn path\n\ndef removepath(directory, top):\n\t\"Remove path, but only if the directory is blacklisted.\"\n\tif top == \"/\":\n\t\tprint_error(\"Path is not blacklisted.\")\n\t\tsys.exit(1)\n\n\ttop = os.path.normpath(top"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tif not path.startswith('/') and not root.startswith('/'):\n\t\t\t\troot = os.path.dirname(root)\n\t\texcept UnicodeError:\n\t\t\tpath = path.rstrip('/')\n\t\t# remove leading / from path (if it begins or ends with a slash)\n\t\twhile len(path) > 0 and path[: -1] == '/':\n\t\t\tpath = path[1:]\n\t\t# remove trailing slash and not including it\n\t\treturn os.path.join(path, root)\n\t# return root if root is absolute, return \"\" on failure\n\telse:\n\t\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tparts = path.split(root)\n\t\tif parts[0] == root:\n\t\t\tprint \"Removed %s from list\" %root\n\t\t\treturn parts[1:]\n\t\telse:\n\t\t\t# path is a directory we need to remove\n\t\t\tif len(parts) == 1: print \"Removing %s\" %splittedpath\n\t\t\t\tparts.pop(0)\n\n\t\t\t# path and root are identical\n\t\t\tpath = \"/\".join(parts)\n\t# now, remove the root path, no matter where we left it\n\tif len(path) <len(root):\n\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor p in path:\n\t\t\tif not os.path.isabs(p):\n\t\t\t\tp = os.path.join(config.usage.timeshift_path.value, p)\n\t\t\tif os.path.isdir(p):\n\t\t\t\tdel_files = listdir(p)\n\t\t\t\tfor f in del_files:\n\t\t\t\t\tremove(f)\n\t\t\telif os.path.isfile(p):\n\t\t\t\ttry:\n\t\t\t\t\tos.unlink(p)\n\t\t\t\texcept Exception as err:\n\t\t\t\t\tprint \"[TimeShift] Failed to remove path '%s':\" % p, err\n\t\timport Timer"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tos.rmdir(root)\n\t\texcept OSError as exc:\n\t\t\tlogger.debug(\"error: Cannot remove '%s' (error='%s')\", root, exc)\n\ndef get_installed_apps(bench='.'):\n\tapps = [app for app in frappe.get_installed_apps() if app not in ('frappe', 'erpnext')]\n\n\tif bench:\n\t\tapps = [app for app in apps if app in frappe.get_installed_apps(bench)]\n\n\treturn apps\n\ndef get_bench_dir(bench='.'):\n\treturn get"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\t\t\t\t\t\tos.remove(os.path.realpath(path)) # delete file\n\t\texcept:pass\n\t\ttry:\n\t\t\tos.rmdir(os.path.realpath(root))\t# remove root recursively\n\t\texcept OSError, e:\n\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\traise\n\n\tdef oslink(self,src,dst):\n\t\tsys.stdout.write('Linking %s %s -> %s\\n' %(src,dst,path2str(self.path)))\n\n\tdef selinux(self):\n\t\tos.system\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tif path.startswith(root):\n\t\t\t\tout = open(path, 'r').read()\n\t\t\t\tif out:\n\t\t\t\t\tout = out.replace('\\x00','').strip('\\n')\n\t\t\t\t\tl = set(out.splitlines() + [n for n in \\\n\t\t\t\t\t\tout.splitlines() if n.strip()!= ''])\n\t\t\t\t\tfor i in l:\n\t\t\t\t\t\tout = out.replace(i, '')\t\t\t\t\t\t# remove \\n\n\t\t\t\t\t\t# and append new line at the end of file\n\t\t\t\t\t\tout += i + '\\n'\n\t\t\t\t\t\tprint 'Removing %"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\timport os\n\t\tret = os.system('rm -rf %s' % root)\n\t\tif ret!= 0:\n\t\t\tos._exit(ret)\n\n\t#\n\t# Get current branch\n\t#\n\tdef get_current_branch(self):\n\t\ttry:\n\t\t\tbranch = shell.call('cd %s ; git branch 2>/dev/null; git rev-parse --symbolic-full-name' % self.location)\n\t\texcept:\n\t\t\ttry:\n\t\t\t\tinfo, err = Shell.run(['git', 'branch'], quiet = True)\n\t\t\t\tif"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tif os.path.isdir(path):\n\t\t\t\tos.rmdir(path)\n\t\t\telif os.path.isfile(path):\n\t\t\t\timport shutil as copy\n\t\t\t\tret = copy.deepcopy(open(path).read())\n\t\t\t\tpath = os.path.dirname(path)\n\t\t\t\tif os.path.exists(path) and os.path.isfile(os.path.join(path, \".backup\")):\n\t\t\t\t\tos.remove(os.path.join(path, \".backup\"))\n\t\t\t\t\tos.rmdir(path)\n\t\t\t\tos.rmdir(path)\n\t"}]}
