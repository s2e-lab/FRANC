{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\t\n\ttime = Time(nanoseconds, tz)\n\treturn time\n\t\n\t# time = time.replace(tzinfo=tz)\n# def hydrate_time(timezone, tz=None, utc_offset=False):\n# \t\"\"\"\n#      Get time between `utc_offset` and `tz`.\n#      * ``tz`` must be either ``None`` or a pytz timezone. See `pytz` module.\n#      * ``datetime`` must be timezone aware. See `localtz` module.\n#      * ``utc_offset`` must be ``False`` or"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\t\n\tif isinstance(natime, float):\n\t\tnanoseconds = int(nanoseconds + nanoseconds)\n\t\thours, rem = divmod(nanoseconds, 3600)\n\t\tdays, rem = divmod(rem, 30)\n\t\tif nanoseconds > 99900000000000000:\n\t\t\tnanoseconds = 0\n\t\t\thours += 1\n\t\thour = int(hour)+(60*60)\n\t\tmins = abs(tz.utcoffset(int(hour)))\n\t\treturn datetime.time(hours,mins+tz.dst())\n\telse:\n\t\treturn datetime.time("}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\t# Calculate time in the past\n\ttry:\n\t\tts =  DateTime(nanoseconds)\n\t\ttime_in = Time(ts)\n\texcept ValueError:\n\t\tif tz is not None:\n\t\t\ttz.replace(tzinfo=None)\n\t\ttime_in = Time(nanoseconds)\n\n\t# Calculate time in minutes and seconds\n\tminute = nanoseconds // 60\n\tseconds = nanoseconds % 60\n\n\treturn time_in.to_primitive_number(), minute.to_primitive_number(), seconds\n\ndef hydrate_localtime(nan"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\tif tz is None:\n\t\tif not hasattr(time, '_local_time'):\n\t\t\ttime._local_time = time._now()\n\n\t\t\n\treturn Time(nanoseconds.get(\"Time\") or nanoseconds.get(\"Local Time\"))\n\n# Copyright Iris contributors\n#\n# This file is part of Iris and is released under the LGPL license.\n# See COPYING and COPYING.LESSER in the root of the repository for full\n# licensing details.\n\"\"\"\nUnit tests for the :func:`iris.analysis.MEAN` aggregator.\n\n\"\"\"\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\n\t\n\tif np.issubdtype(time, np.datetime64):\n\t\tms = True\n\n\telif np.issubdtype(time, Time):\n\t\tms = False\n\telse:\n\t\traise TypeError(\"Time input must be an Astropy Time object \"\n\t\t\t   \"(got %d characters)\" % (len(time.data),))\n\t\n\tif tz is None:\n\t\tif ms:\n\t\t\ttz = pytz.timezone('UTC')\n\t\telse:\n\t\t\ttz = pytz.timezone('UTC')\n\t\t\t\n\tif  np.size(time[0])!= np.size"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\tif not tz:\n\t\ttz = pytz.timezone('UTC')\n\t\t\t\n\tnow = Time(tz.tzname())\n\tnow_utc = Time(now.timezone)\n\t\n\tnow_local = timezone(tz.tzname())\n\t\n\tnow_timezone = tz.tzname()\n\t\n\tif tz.utcoffset(now) < now_local:\n\t\t# Time of start of UTC time, reset in local time. If it is in that\n\t\t# timezone, just skip it and turn on the local time.\n\t\tutc_start_time = now.replace(tzinfo=None)\n\t\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\tif tz is None:\n\t\ttz = localtime()\n\ttz = tz.utcoffset(nanoseconds)\n\t\n\tnaive = datetime.datetime(1970, 1, 1, tzinfo=tz)\n\n\t# convert to UTC time without timezone.\n\tnaive_localtime = datetime.datetime.utcfromtimestamp(naive)\n\tprint naive.utctimetuple()\n\t\n\t# convert to local time\n\tnaive_loc = naive.astimezone(tz)\n\t\n\t# convert to local time string\n\tnaive_time = naive_loc.strftime(\"%Y/%m/%d %H:%M:%S.%"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\tif not tz:\n\t\ttz = get_localzone()\n\ttimestamp = now()\n\t\n\t# Check if the system has been configured using pytz\n\t# TODO: Add some other common timezone support.\n\n\tif not _is_system_configured():\n\t\treturn datetime.datetime.combine(timestamp, ZERO, microsecond=UTC_ZERO_TICK)\n\t\n\t# Get the local timezone\n\tlocaltime = get_localzone()\n\t\n\t# If we're running in UTC and in DST, let's take that into account:\n\tif _utc_offset(time=timestamp) >"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n# def interval_time(start, end, periods=1, tz=None, **kwargs):\n\t\n\t# #TODO implement interval-like time-object\n\timport pandas.tseries.frequencies as frequencies\n\t# return Time.from_intervals(\n\t\n\t\n\tfrom datetime import datetime as dt\n\tfrom calendar import monthrange\n\n    \tstart_ = dt(2014, 1, 1)\n\tend_ = dt(2014, 12, 31)\n\tfirst_date_ = dt(2014, 3, 28)\n\tlast_date_ = dt(2014, 12, 31)"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\t\n\tif tz is None:\n\t\ttz = LOCALTZ\n\t\n\tt1 = 0\n\tt2 = 0\n\tt4 = 0\n\tt8 = 0\n\tt16 = 0\n\tt32 = 0\n\tt64 = 0\t\n\n\tif len(nanoseconds) == 1 and nanoseconds[0] == nanoseconds[-1]:\n\t\tt1 = nanoseconds[0]\n\t\tt2 = nanoseconds[1]\n\t\tt4 = nanoseconds[2]\n\t\tt8 = time.mktime(time.strptime(t"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\t# time_from_hms : seconds value in hms units\n\tnow_hms  = datetime.now()\n\t\n\tdelta_time  = (now_hms - value)\n\t\n\tif delta_time < timedelta(0):\n\t\treturn \"P\"\n\telif delta_time < timedelta(-1):\n\t\treturn \"M\"\n\telif delta_time < timedelta(microseconds=1):\n\t\treturn str(int(abs(now_hms - value)))\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License,"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\n\tdef dehydrate_date(value):\n\t\t\"\"\"\n\t\tDehydrator for `date` values.\n\t\t\n\t\t:param value:\n\t\t:type value: str\n\t\t:return:\n\t\t\"\"\"\n\t\t\n\n\tdef dehydrate_year(value):\n\t\t\"\"\"\n\t\tDehydrator for `year` values.\n\t\t\n\t\t:param value:\n\t\t:type value: str\n\t\t:return:\n\t\t\"\"\"\n\t\t\n\n\tdef dehydrate_month(value):\n\t\t\"\"\"\n\t\tDehydrator for `month` values.\n\t\t\n\t\t:"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\tif isinstance(value, timedelta):\n\t\t\treturn value\n\t\telse:\n\t\t\treturn timedelta(seconds=int(value))\n\t\n\t\n\tdef dehydrate_timedelta_seconds(value):\n\t\"\"\"\n\tDehydrator for `timedelta` seconds.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\tif value == -2:\n\t\treturn timedelta(0)\n\telif value == -1:\n\t\treturn timedelta(days=1)\n\telif value < 0:\n\t\treturn timedelta(days=(value // 3600"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\t# NOTE: There is a potential bug:\n\t\t#     - the implementation below uses __cmp__, don't use it in this code\n\t#     - to do more advanced comparisons between two datetimes?\n\t\t#     - we keep the ability to compare and compare dates based on dates?\n\t\t#     - we can keep a list of datetimes?\n\t# NOTE:\n\t#   - The following is the way to compute this.\n\t#   - The code below does not use __cmp__, which works in Python 2.x where the\n\t#     Python 2.x version hasn't been"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\ndef increment_timedelta(value):\n\t\"\"\"\n\tIncrementing the given value until the specified timestamp \n\tis reached.\n\t\n\t:param value: The value to increment\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t# We always increment the value.\n\tvalue += datetime.timedelta(seconds=1)\n\t\n\t# We increment over the number of seconds until the given value \n\t# is reached.\n\ti = 2 if value.seconds == 1 else value.seconds - 1\n\t# If the increment failed, the counter increased.\n\tif i!="}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\ndef to_seconds(value, time_unit):\n\t\"\"\"\n\tReturns the number of seconds in a given time unit\n\t:param value:\n\t:type value: int\n\t:param time_unit:\n\t:type time_unit: str\n\t:return:\n\t\"\"\"\n\treturn value\n\t\ndef datetime_to_seconds(value):\n\t\"\"\"\n\tReturns the number of seconds in a given datetime object\n\t:param value:\n\t:type value: datetime\n\t:return:\n\t\"\"\"\n\treturn value\n\t\t\ndef date(value):\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\t# We use timedelta so that there are no gaps\n\tif 2 * value:\n\t\treturn value - 2\n\t\n\telif value == 1:\n\t\treturn 2\n\telse:\n\t\t# Gap is now one\n\t\treturn pow(value, 5)\n\n\ndef string_types():\n\t\"\"\"\n\tCreate the available Types. Use :ref:`types <types_for_enumerables>`\n\tinstead.\n\t\n\t:return: Types\n\t:rtype: `tuple`\n\t:Example:\n\t\n\t   >>> Types.StringTypes()\n\t   ('number',)\n\t\n\t   >>> Types"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\tparts = tuple(float(x) for x in value.split(\":\"))\n\t\n\tseconds = 0\t\t\t\t\t\t\t\t\t\t# seconds\n\tfloat_seconds = 0.0\n\n\tif len(parts) == 3:\n\t\tseconds = float(parts[0]) + float(parts[1]) / 60\t\t\t\t\t\t\t\t\t # seconds\n\telif len(parts) == 4:\n\t\tsecs = float(parts[0]) * float(parts[1]) + float(parts[2]) / 3600\t\t\t\t\t# seconds\n\n\treturn round(seconds * 3600, 5), round(float_seconds, 5"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\tif not isinstance(value, timedelta):\n\t\traise TypeError('value must be instance of timedelta')\n\treturn  value.days * 86400\n\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: datetime\n\t:return:\n\t\"\"\"\n\t\n\n\tif isinstance(value, datetime):\n\t\treturn value.date()#!/usr/bin/env python\n\n\"\"\"\nCreate a bunch of sample.gff which can then be used to generate a.fasta file\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tvalue = timedelta(seconds=value)\n\texcept (ValueError, TypeError):\n\t\tpass\n\t\t\n\tif value < timedelta(0):\n\t\treturn \"less than or equal to 0\"\n\t\n\treturn value\n\ndef dehydrate_numeric(value):\n\t\"\"\"\n\tDehydrator for `numeric` values.\n\t\n\t:param value:\n\t:type value: numeric\n\t:return:\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tvalue = int(value)\n\t\tnumeric = True\n\texcept (ValueError, TypeError):\n\t\tpass\n\t\t\n\t"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\treturn datetime.timedelta(seconds=value)\n\t\n@register.filter(\"timesince\")\ndef ago(value, arg=None):\n\t# type: (int, int) -> Date\n\t\"\"\"\n\tDate diff to human readable string.\n\t\n\t:param value: datetime.timedelta object\n\t:type value: Any\n\t:param arg: timespan\n\t:type arg: str|datetime.timedelta|int\n\t:return:\n\t:rtype: int|float|str\n\t\"\"\"\n\t\n\tif not value:\n\t\tvalue = Date.current()\n\tvalue ="}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\t# check required value\n\tif not value:\n\t\treturn \"\"\n\t\n\t# parse the value\n\tvalue_parts = value.split(\" \")\n\tif len(value_parts) == 2: \n\t\ttime_min = datetime.strptime(value_parts[0], \"%H:%M:%S\")\n\t\ttime_sec = float(value_parts[1])\n\telse: \n\t\treturn \"\"\n\t\n\t# convert\n\ttimestamp = datetime.strptime(\n\t\tvalue_parts[0], \n\t\t\"%Y-%m-%d %H:%M:%S\"\n\t)."}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\treturn time.mktime(value.timetuple())\n\ndef default_time():\n\ttry:\n\t\treturn datetime.now()\n\texcept:\n\t\treturn datetime.now()\n\ndef timestamp_str():\n\treturn str(randint(0, 999999))\n\ndef date_str():\n\treturn datetime_now().strftime('%Y/%m/%d')####################################################################################################\n#\n# Gnuradio Python Flow Graph\n# Title: UHD Proto Visor Decoder\n# Author: Martin Paces <martin.paces@eox.at>\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\treturn time() - value\n\n\n# =============================================================================\n## @class Interval\n#  Base Interval implementation.\n#  @see OptimizedInterval\n#  @author Vanya BELYAEV Ivan.Belyaev@itep.ru\n#  @since 2016-01-07\nclass OptimizedInterval(object):\n\t\"\"\"\n\tSimple interval defined by a finite interval, containing a start, end & points of the interval.\n\t\n\tInterval is based in the Haversine formula\n\t\"\"\"\n\t\n\tdef __init__(self, start, end, number = 1"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\treturn str(value)\n\ndef _from_timestamp(value):\n\t\"\"\"\n\tConvert :py:class:`datetime.datetime` value to\n\t:py:class:`int` values representing `time` value\n\t:return: :ref:`python:time`\n\t:rtype: int\n\t\n\t:Example:\n\n\t>>> from datetime import date, time, timedelta\n\t>>> \tfrom util.timestamp import Time\n\t>>> \tassert datetime(2015,3,1,5,15,1) - timedelta(1) == Time(2015,3,1,"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\n\tif value is None: \n\t\tvalue = time.timezone\n\tif isinstance(value, Time): \n\t\treturn str(value)\n\t\t#return str(value)\n\telif isinstance(value, (int, float)): \n\t\treturn str(value)\n\telif isinstance(value,str):\n\t\t#return strftime(TIME_FORMAT[1], gmtime(value))\n\telse:\n\t\treturn strftime(TIME_FORMAT)\n\ndef increment_logging_counter(start_time = None, final_time = None):\n\t\"\"\"\n\tIncrement logging counter.\n\t\n\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tv = str(round(value/60))\n\t\tif int(v) > 0:\n\t\t\tv = int(v) * float(60)\n\t\treturn time(int(v), int(v)/60).strftime(\"%H:%M:%S\")\n\texcept OverflowError:\n\t\treturn 0\n\t\t# -*- coding: iso-8859-1 -*-\n\"\"\" crypto.keyedHash.py\n    ==============\n\n    MD160 based key, based on the RSA encryption key\n    Copyright (C) 2015  Dieter Adriaanse and Christian Rother\n   "}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\ttime = value\n\tif isinstance(time, Date):\n\t\ttime = datetime.fromtimestamp(time)\n\t\n\tif isinstance(time, datetime.time):\n\t\treturn time.time()\n\t\n\ttry:\n\t\treturn datetime.strptime(str(time),\n\t\t\t\t\t\t  \"%d/%m/%Y %H:%M\")\n\texcept ValueError as ve:\n\t\traise ParseError(ve.message)\n\t\n\ndef dehydrate_datetime(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: Datetime\n\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\n\nclass Time2Seconds(Value):\n\n\tdef __init__(self, *args):\n\t\t\"\"\"\n\t\tCreate a value based on a list of times. When multiple values are\n\t\tgiven, *args must be of length\n\t\te.g. (time1, time2, etc), or\n\t\t:python: tuple(int, int)\n\t\t:param args:\n\n\t\t:param default:\n\t\t:type default: None\n\t\t:return:\n\t\t\"\"\"\n\t\tValue.__init__(self, *args)\n\t\tfor name in args:\n\t\t\tself.add_value"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n    time.time()\n    return value\n\n\nimport datetime\n\ndef dehydrate_date(value):\n    \"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: datetime.datetime\n\t:return:\n\t\"\"\"\n\t\n    return datetime.date(value.year, value.month, value.day)\n\n\n\n\n\n\ndef get_date_time():\n    \"\"\"\n\tReturns datetime.time object representing the current day and time.\n\t\n\t:return:\n\t\"\"\"\n\treturn datetime.datetime.now().time()\n\n\ndef"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\n\n\n# ---------------------- CELL\n\ndef cel_polygon(polygon):\n\t\"\"\"\n\tReturn a polygon as an exterior ring.\n\t\n\t:param polygon:\n\t:type polygon: Polygon\n\t:return:\n\t:rtype: Polygon\n\t\"\"\"\n\tc1 = polygon_to_c1(polygon)\n\twhile c1.coords_near((0,0)):\n\t\tc1 = c1.union_with_coordinates(geom.box(c1))\n\tc1 = c1.difference(geom.box(c1).center()) \n\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\treturn value/3\n\t##\n\t# value_as_datetime(\"2017-12-10T00:00:00Z\")\n\t# returns (datetime.datetime(2010, 1, 1, 12, 10, 0),value)\n\t# return (datetime.datetime(2010, 1, 1, 12, 10, 0),float(value_as_datetime(\"2016-11-10T00:00:00Z\")))\n\t#\n\t# input_from_string(\"2017-12-10T00:00:00Z\")\n\t# input_from_string"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\tvalue.append(float(value[1]))\n\t\tvalue.append(float(-value[0]))\n\t\tvalue.append(float(- value[2]))\n\t\treturn\n\ndef dehydrate_polygon(value):\n\t\"\"\"\n\tDehydrator for Polygon data.\n\t\n\t:param value:\n\t:type value: Polygon\n\t:return:\n\t\"\"\"\n\t\tvalue.append(float(value[1]))\n\t\tvalue.append(float(value[0]))\n\t\tvalue.append(float(value[3]))\n\t\treturn\n\ndef dehy"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\t\n\ndef get_path(*path):\n\t\"\"\"\n\tGets a list of paths matching a given path.\n\t\n\t:param path:\n\t:type path: list(str)\n\t\n\t:return:\n\t:rtype: list(str)\n\t\"\"\"\n\t\n\tp = str(path[0])\n\ti = 1\n\twhile i < len(path):\n\t\tif p == path[i]:\n\t\t\tp = p.replace('\\\\', '/')\n\t\t\ti += 1\n\t\tp = p.replace(path[i], '')\n\t\ti += 1\n\t\n\tif len(path"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\n\ndef add_point_field(field_name, data):\n\t\"\"\"\n\tAdds a new field to data.\n\t\n\t:param field_name: \n\t:type field_name: str\n\t:param data:\n\t:type data:\n\t:return:\n\t\"\"\"\n\t# field_name is already a field or has no description\n\ttry:\n\t\t# if type(data) is not dataType:\n\t\tdata[field_name] = dict(data)\n\texcept:\n\t\tdata = {field_name: data}\n\treturn data\n\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t \n\t\n\t\n\t\n\tvalue = json.loads(value)\n\ttry:\n\t\tpoints_info = create_point_info(value[\"geometry\"])[0]\n\texcept:\n\t\traise Exception(\"Cannot create Point\")\n\t\t\n\tdata_points.points.add(points_info)\n\tpoint = get_point(points_info)\n\tdata_points.point.srid = 4326 if point.srid == 4326 else point.srid\n\tpoint.save()\n\treturn pointfrom pybindgen import Module, FileCodeSink, param, retval, cppclass, typehandlers\n\n\nimport py"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\treturn value.x, value.y\n\ndef _get_int_number_from_string(string, key=None):\n\t\"\"\"\n\tExtract integer value from string. \n\t\n\t:param string:\n\t:type string: str\n\t:param key:\n\t:type key: int\n\t:return:\n\t\"\"\"\n\tif string and \".\" in string:\n\t\ttry:\n\t\t\tinteger = int(float(string))\n\t\t\treturn integer\n\t\texcept ValueError as err:\n\t\t\tprint(err)\n\t\t\texit()\n\telif key and \".\" in string["}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\t\t\n\nclass Drift():\n\t\"\"\"\n\tDrift class.\n\t\n\tDehydrator for a point based on a constant density function (point-to-point matrix).\n\t\n\tEvidently, these points are considered a \"point\" by default. This is necessary for the DRI model.  The DRI is a sphere with respect to the centre of the sphere and with respect to the X, Y and Z directions.\n\t\"\"\"\n\n\n\tdef __init__(self, mu, sigma, alpha=0.5, delta = 0.0, sigma0=1.0, alpha"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t    \n\tif isinstance(value, Point):\n\t\tvalue = dehydrate_value(value)\n\tif isinstance(value, list) and all(is_integer_list(v) for v in value):\n\t\treturn value\n\treturn dehydrate_value(str(value))\n\t\ndef dehydrate_value(value):\n\t\"\"\"\n\tDehydrate value.\n\t\n\t:param value: A value of any type\n\t\n\t:return: A value converted to a string\n\t\n\t:example:\n\t\n\t>>> dehydrate_value(32)"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\n\nclass Vector:\n\t\"\"\"\n\tThe Vector Class provides a function for performing distance computations between Point.\n\t\"\"\"\n\tdef __init__(self, x = None, y = None, z = None):\n\t\t\"\"\"\n\t\tInitializes the Vector object.\n\t\t\n\t\t:param x: X coordinate used to determine the distance\n\t\t:type x: float\n\t\t:param y: Y coordinate used to determine the angle\n\t\t:type y: float\n\t\t:param z: Z coordinate used to determine the direction\n\t\t:type z: float\n\t\t:return:\n\t\t\"\"\"\n\t\tself"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\t\n\treturn self.keys\n\n##\n## Interface to the FDTD record\n##\n\nclass FDTD(CRecord):\n\t\n\t@staticmethod\t\n\tdef find(name, attrs={}, ctx=None):\n\t\tif FDTD._find(name, attrs) is None:\n\t\t\treturn None\n\t\td = MetaRecord()\n\t\td.name = name\n\t\td.attrs = attrs\n\t\t\n\t\treturn d\n\t\t\n##\n## Interface to the FIDA record\n##\n\nclass FIDA(CRecord):\n\t\n\t@staticmethod\t\n\tdef find(name, attrs={},"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\t\n\treturn self.records.keys()\n\ndef _make_key(id):\n\t\"\"\"\n\tCreate a key from a record (from the record)\n\t\n\t:param id: the record id\n\t:return: the key\n\t\"\"\"\n\t\n\tkey = u\"%s%d%s%d%s\" % (id, \\\n\t\t      id[0], id[1], base64.urlsafe_b64encode(id[2]), id[3] )\n\t# now we add the key and uid information.\n\t\n\tkey = base64.urlsafe_b64encode("}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n    def is_blank_line(self):\n\tif self.has_newlines == 1:\n\treturn False \n\n\tself.keys_list[self.values_list_id_index(LINE_BLANK)] = \\\n            list_entry_string(self.values_string[self.keys_id_index(LINE_BLANK)])\n\ndef read_data_record(values_str, keys_data_str):\n    \"\"\"\n    read one line of data from a specified string.\n\n    :param values_str: values delimited by whitespace\n    :type values_str: string\n    :"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\n\toutput = self.query(self.QUERY_KEY_KEY_NAME, (key,))[0]\n\treturn output\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\n/***************************************************************************\n SpatialJoinWithinSubsets\n                                 A QGIS plugin\n This plugin adds a table join (virtual)\n                             to the table used in multiple to create the key comboboxes.\n                             -------------------\n        begin                : 2015-08-06\n        git sha              : $Format:%H$\n        copyright            : (C) 2015 by Luiz Andrade - Cartographic Engineer @ Brazilian Army\n       "}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\t\n\treturn self.__keys\n\t\n\t###############\n #\n # DB \n #\n #\n def __get_dbs( self ):\n\treturn self.__dbs\n\t# END DB \n  \n  def __get_dbs_by_file( self, db_file ):\n\tfile_content = ''\n\tif db_file:\n\t    file_content = db_file.readlines()\n\treturn self.__get_dbs_by_lines( file_content )\n  \n  def __get_dbs_by_lines( self, lines   ):\n\tlines_list = []\n\tfor i,"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n# vim: ts=4 sw=4 noet\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\nclass Record(BaseRecord):\n\t\"\"\"\n\tStore a record in a file.\n\t\n\t:Note: If a record doesn't have the fields described\n  \t\tbelow, one will be implicitly created. \n\t\"\"\"\n\n\t# the record type, e.g. \"A\" \"N\"\n\trecordType = Record.getField(\"TYPE\", str)\n\t\n\t# the primary key fields\n\tprimaryKey = Record.getField(\"PRIMARY\", str)\n\tprimaryRecord = Record.getField(\"RECORDTYPE\", str)\n\tprimaryFlags = Record.getField(\"PRIMARYFLAGS\", int"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\n\tkeys = []\n\tfor field in self.fields:\n\t\tif field in self.__dict__: \n\t\t\tkeys.append(self.__dict__[field])\n\treturn keys\t\t\t\n\n\n\n@python_2_unicode_compatible\nclass TLV(TLVBase):\n\n\tdef __init__(self, *args, **kwargs):\n\t\tTLVBase.__init__(self)\n\t\tkw = dict(*args, **kwargs)   # py2: no long line (e.g. repr(self))\n\t\tself.set(**kw)\n\t\tself._fields = []\n\t\t\n\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\treturn self.keys\n\t@staticmethod\n\tdef keys_equal(list1, list2, ignore_case_diffs = False):\n\t\"\"\"\n\tCheck if two records have the same keys.\n\t\n\t:param list1: list of key names.\n\t:param ignore_case_diffs: ignore case differences.\n\t:return: True if lista == listb\n\t\telse False\n\t\"\"\"\n\t# if a list has same keys, do a simple case-insensitive comparison.\n\tstr1 = str(list1)\n\tstr2 = str(list2)\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\nclass KeyRecord(object):\n\t\"\"\"\n\tThis is the class for the keys stored in the record.\n\t\n\tIt can be considered to be subclassed for specific key-specific stuff like\n\tattributes, and they can be accessed through class methods.\n\t\n\tThe methods provide the \"key_type\" property.\n\t\"\"\"\n\t\n\tdef __init__(self, key_type, key, type_options=None, default=None):\n\t\t\"\"\"\n\t\tConstructor for KeyRecord Object\n\t\t\n\t\t:param key_type: optional key type\n\t\t:param key: identifier for this record from which we"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t    \n\t# find all available versions for the protocol\n\tversions = {}\n\tallprotos = {}\n\tfor version_info in cls._protocol_versions.values():\n\t\tproto = version_info[0]\n\t\tif not isinstance(proto, (tuple, list)):\n\t\t\tcontinue\n\t\tallprotos[proto] = version_info[-1]\n\t\tversions[proto] = version_info\n\n\tif protocol_version is None:\n\t\treturn versions\n\n\t# find the latest known protocols\n\tversions = [versions[x] for x in allprotos.keys() if allprotos"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n    # Parse protocols info and dict.\n    result = {}\n    ret = {}\n    for protocol in protocol_info:\n        protocol_tuple = protocol_info[protocol]\n        version = protocol_tuple[protocol_tuple.keys()[0]]\n        (ret_tuple, version_tuple) = cls.read_version(version)\n        if ret_tuple:\n            result[protocol_tuple[protocol]] = ret_tuple\n        elif version_tuple:\n            result[version_tuple[0]] = version_tuple\n\n    # Filter out empty tuple.\n    if len(result) == 0:\n        return {}\n    else"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\thandler_dict = {}\n\t\n\t# determine protocol version\n\tversion = int(''.join(str(v) for v in protocol_version))\n\t\n\t\n\t# determine supported protocols and version\n\tfor protocol_version in protocol_versions(ProtocolVersion.HOLDING_UPGRADE):\n\t\thandler_dict[protocol_version] = {}\n\t\tversion_info = protocol_version.split('.')\n\t\tfor version in version_info[:-1]:\n\t\t\ttry:\n\t\t\t\thandler_dict[protocol_version][version] = import_class(base=protocol_version, **version)\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\t# TODO should be made generic\n\t\n\textensions = ''\n\tfor extension in list(cls.__dict__.get('extensions', [])):\n\t\tif extension and extension not in extensions:\n\t\t\textensions = list(extensions) + [extension]\n\t\n\tif protocol_version is None:\n\t\traise TypeError('protocol_version can only be None')\n\telif type(protocol_version) is not int:\n\t\traise TypeError('protocol_version must be int or %s' % protocol_version)\n\telse:\n\t\tversion = str(protocol_version)\n\t\treturn {'protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\n\thandlers = {}\n\tfor version in protocol_version:\n\t\thandlers[version] = cls\n\t\t\n\treturn handlers\n\t\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n################################################################################\n# (c) 2015, Markus Wildi, markus.wildi@bluewin.ch\n################################################################################\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n# \n#  General classes\n# \nclass Connection (object):\n\t\"\"\"\n\tConnection class - connects and manages a connection to the graph.\n\tThe graph may only be used once.\n\t\"\"\"\n\tdef __init__(self, host, port, version='0.1'):\n\t\t\"\"\"\n\t\t:param host: ip address for connecting to the remote graph\n\t\t:param port: port number for connecting to the remote graph\n\t\t:param version: version of connected/discovered connection\n\t\t\"\"\"\n\t\tself.host = host\n\t\tself.port = port\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t #TODO: refactor this code\n        if isinstance(protocol_version, int):\n            protocol_version = tuple(protocol_version)\n        elif not isinstance(protocol_version, (tuple, list)):\n            protocol_version = (protocol_version,)\n        if protocol_version == (3,):\n            return {'all': {'version': protocol_version}}\n        elif protocol_version == (5,):\n            return {'client': {'version': protocol_version}}\n        elif (4, 0) <= protocol_version <= (3, 5):\n            return {'client': {'version': protocol_version}}\n        elif (2, 5"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\thandler_versions = {}\n\tfor version in protocol_version:\n\t\ttry:\n\t\t\tif not version:\n\t\t\t\tversion = (0, 0)\n\t\t\thandler_versions[(version[0], version[1])] = cls.handler_version(version[2])\n\t\texcept Exception as e:\n\t\t\tlogging.warn((\"Handler version tuple definition for version %s is \"\n\t\t\t\t\t\t\t\"invalid (%s). Using default\"), version)\n\t\n\thandler_handlers = {}\n\tfor handler in Handler.handlers:\n\t\ttry:\n\t\t\tname = handler.__class__.__name__\n\t\texcept"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t    \n\n\tif not protocol_version:\n\t    protocol_version = (3, 5)\n\thandlers = {}\n\tfor version in protocol_version:\n\t    try:\n\t\thandler = cls._handlers[version]\n\t    except KeyError:\n\t\traise TypeError('Unsupported Bolt protocol version \"%s\"\\n' \\\n\t\t   'supported versions are %s' \\\n\t\t    % (version, ', '.join(sorted(_list_available_versions.keys()))))\n\n\t    # XXX: this logic allows us to get the version tuple we need?\n\t    version_handler = handler(version)\n\t    version_handlers = VersionHandler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\treturn dict(\n\t\thandlerclass(cls, protocol_version),)\n\t\n\n@_injector_method(\"version\")\ndef version_handler(stream, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of version handler factories,\n\tkeyed by version tuple. If an explicit version is provided,\n\tthe dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no version\n\tversion is provided, all available versions will be returned.\n\t\n\t:param stream: stream to take version from\n\t:param protocol_version: tuple identifying"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \n\tdef transaction_transaction(func):\n\t\t@wraps(func)\n\t\tdef run_query(self, query, *args, **kw):\n\t\t\tcur = self._connection._connection.cursor()\n\t\t\ttry:\n\t\t\t\t# Check that the transaction doesn't need to be terminated if not using timeout.\n\t\t\t\tquery_res = cur.execute(query, self._connection._session, timeout=timeout)\n\t\t\texcept Exception as e:\n\t\t\t\tself.error('An exception occurred while running %s:\\n%s\\n%s'%(func.__name__, e, sys.exc"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    ## @file\n# This file is used to define common functions for various C programs.\n#\n# \\author T. Lukaczyk, F. Palacios\n# \\version 6.0.0 \"Falcon\"\n#\n# \\endcode\n# Last modified: 28/02/2015\n#\n\nclass GdbException(Exception):\n    \"\"\"\n    Exception class for all gdb related error messages.\n    (Not used)\n    \"\"\"\n    def __init__(self, *args):\n        self.__source__ = args[0]\n        self.__message__ ="}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t\n\tdef func(**data):\n\t\tfrom transaction.base import Transaction\n\t\tglobal _transaction\n\t\ttry:\n\t\t\t_transaction = Transaction.asTransaction(metadata=metadata, timeout=timeout)\n\t\texcept Exception as e:\n\t\t\traise e\n\t\treturn _transaction.execute(*data)\n\t\n\treturn decorate(func, func.__name__, wrapper='transaction')\n\nclass Cursor(tuple):\n\tdef execute(self, *args, **context):\n\t\treturn self[0].__class__(self.cursor.execute(*args, **values(context, **self.args)))\n\t\n\tdef"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t    assert isinstance(timeout, (int,float))\n\n\t    if metadata is None:\n\t\tassert \"timeout\" in metadata, \"You must pass a \\\n'timeout' parameter to specify the transaction timeout\"\n\t\tmetadata[\"timeout\"] = timeout\n\n\n\t    def filter_callback(f):\n\t\tdef newf(tx):\n\t\t    return tx.run(\"MATCH (a)<-[a]->(b:Person)\")\n\t    tx = update_transaction_callback(newf, tx, filter_callback)\n\n\t    def update_transaction_callback(tx, result):\n\t\tassert tx is not None,"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \n\t    \n\t    \n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tif not kwargs.has_key(\"transaction\"):\n\t\t\t\t\tkwargs[\"transaction\"] = dbms.transaction.get()[config.database_file_name]\n\t\t\t\t\n\t\t\t\t# TODO : implement a connection pool for better performance\n\t\t\t\n\t\t\t\treturn f(transaction=args[0], timeout=args[1], **kwargs)\n\t\t\texcept KeyError:\n\t\t\t\tprint \"WARNING: You need to provide a transaction function without a connection pool!\"\n\t\t\t\n\t\t\treturn wrapper\n\t\t\n\t\treturn wrapper\n\t\n\treturn"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\ndef setup_database_connection(url):\n\t\"\"\"\n\tThis function sets up a database connection if needed and returns its connection.\n\t:param str url:\n\t    The URL where the database connection will be attempted. When this function is called with no URL, no URL will be attempted. \n\t:return: bool\n\t\"\"\"\n\tassert(url.endswith(\"/\")), \"URL must end in '/'\" + url.title()\n\n\tif \"://\" not in url:\n\t\turl += \"/\"\n\n\ttry:\n\t\tlogging.debug(\"creating connection with url (%s)\" % (url))\n\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t\t\n\t    \n\t\n\t@decorator\n\tdef atomic_block_size(f):\n\t\t@wraps(f)\n\t\tdef count_statements(*f_args, **f_kwargs):\n\t\t\tif transaction.db is None:\n\t\t\t\treturn 0\n\t\t\tif timeout:\n\t\t\t\ttransaction.db.transaction.start()\n\t\t\t\treturn transaction.db.countStatement(transaction=None, **f_args, **f_kwargs)\n\t\t\treturn f_kwargs['cursor'].fetchone()[0]\n\t\treturn count_statements\n\t\n\tfrom neo4j.exceptions import TransactionFailed, TransactionNotFound, Transaction"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t    def transaction(func):\n\t\t@wraps(func)\n\t\tdef wrapped(*args, **kwargs):\n\t\t    start_timestamp = util.as_datetime(datetime.datetime.now())\n\t\t    ret = func(*args, **kwargs)\n\t\t    duration = None\n\t\t    while not end_timestamp:\n\t\t\tif duration is not None:\n\t\t\t    duration = util.as_timedelta(duration)\n\t\t\t    \n\t\t    txn_execute(func, start_timestamp, duration=duration)\n\t\t    end_timestamp = util.as_datetime(datetime.datetime.now())\n\t\t    txn_commit(func)\n\t\t    deferLater(reactor, 0"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \n\tdbms.setTXMetaData(metadata, timeout=timeout)\n\treturn dbms.listQueries(timeout=timeout)\n\n####\n\ndef list_transactions(dbms, timeout=50, **kwargs):\n\t\"\"\"\n\tList all transactions.\n\t@param timeout:\n\t\tthe transaction timeout in seconds.\n\tIf it is ``None`` it will use the default timeout.\n\tIf time limit is not passed, it will be set to an infinite wait time in the current time.\n\t:type timeout: int or :const:`None`\n\t:returns:"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \n\tdef wrap(func):\n\t\t\"\"\"\n\t\tDecorator that makes sure ``func`` is actually decorated with ``@Transaction.beginTransaction`` \n\t\tand ``@Transaction.savepoint`` before executing a function.\n\t\t\n\t\tExamples::\n\n\t\t    @transaction.beginTransaction()\n\t    \"\"\"\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tfunc(*args, **kwargs)\n\t\t\texcept:\n\t\t\t\timport traceback\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsys.exit(1)\n\t\treturn inner\n\t\nclass Cursor(object):\n\tdef __"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tvalue = self.getitem(key).index\n\t\tif value is None:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn value\n\t\n\tdef pop(self, key):\n\t\t\"\"\"\n\t\tRemoves and returns an item from the array.\n\t\t\n\t\t:param key: the key\n\t\t\"\"\"\n\t\tvalue = self[key]\n\t\tif value is None:\n\t\t\traise KeyError('Cannot remove item {} from array'.format(key))\n\t\telse:\n\t\t\tdel self[key]\n\t\t\treturn value\n\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\tRemoves from the list of"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tif key not in self.contents:\n\t\t\traise KeyError(repr(str(key)))\n\n\t\treturn self.contents.index(key)\n\t\t\n\n\tdef __repr__(self):\n\t\treturn \"<{}>\".format(self.name)\n\t\t\t\n\tdef __setitem__(self, key, element):\n\t\tif key not in self.contents:\n\t\t\traise KeyError(repr(str(key)))\n\t\t\n\t\tself.contents[key] = element\n\t\tself.index += 1\n\t\n\n\nclass LinkList:\n\t\"\"\"A linked list that includes information about links on the index.\"\"\"\n\n\tdef"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\n#\tdef __getitem__(self, item):\n#\t\twhile self:\n#\t\t\treturn self.items[item]\n#\t\traise IndexError# --------------------------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for license information.\n# --------------------------------------------------------------------------------------------\n# pylint: skip-file\n# coding=utf-8\n\nfrom azure.cli.core.commands.parameters import (\n    azure_files_identity_provider_resource_id,\n    azure_files_identity_provider_"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tif key < 0:\n\t\t\treturn\n\t\tif key >= len(self.__items):\n\t\t\traise IndexError('Item number out of range')\n\t\t\n\t\tif key < 0 or key >= len(self.__items):\n\t\t\traise IndexError('Item number out of range')\n\t\t\n\t\treturn self.__items[key]\n\t\n\tdef __setitem__(self, key, value):\n\t\tif key < 0:\n\t\t\tself.__items[key] = []\n\t\telif key >= len(self.__items):\n\t\t\traise IndexError('Item number out of range')\n\t\t\n\t\tself.__items[key].append(value"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\treturn int(self[key])\nimport unittest\nimport sys\nimport os\nimport tempfile\n\n# find. -name '*.c' -delete\ntestdirname = os.path.abspath(os.path.dirname(__file__))\n\nif __name__ == '__main__':\n    current_path = os.path.dirname(os.path.abspath(__file__))\n    os.chdir(os.path.join(current_path, '../../'))\n\n    current_path = os.getcwd()\n\n    print('\\n')\n    print('%s testing %s' % (current_path, __name"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\treturn self.keys().index(key)\n\n\t# ----------------------------------------------------------------------\n\tdef indexByColumn(self, value):\n\t\"\"\"\n\tReturn the index of the given item, or None if no columns are associated\n\twith it.\n\t:param value: an index\n\t:return: index\n\t:rtype: int | None\n\t \"\"\"\n\t\treturn self.index(value)\n\n\t# ----------------------------------------------------------------------\n\t# Item management (indexes)\n\t# ----------------------------------------------------------------------\n\tdef firstItemIdx(self, key):\n\t\t\"\"\"\n\tReturn the index of the first item which in a given key"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\nclass TreeNode:\n\t\"\"\"\n\tThis is the class which implements a node in the trees in a simple string.\n\t\n\tThis is implemented using the \"bignum\" algorithm to get bilateral trees.\n\t\"\"\"\n\tdef __init__(self, key):\n\t\tself.value = 0\n\t\tself.key = key\n\n\tdef getNode(self, key):\n\t\treturn None\n\t\t\n\tdef setNode(self, key, value):\n\t\tself.value = value\n\n\tdef getValue(self):\n\t\treturn self.value\n\n\tdef __str__(self"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\n\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016-2019, Ekevoo.com.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\treturn key & 0xFFFF\n\tdef values(self, key):\n\t\"\"\"\n\tReturn an iterator for the items that follow key\n\t:param key: lower then\n\t:return: items\n\t:rtype: generator\n\t\"\"\"\n\twhile not self.is_empty(key):\n\t\tyield key\n\nclass ItemGenerator(object):\n\t\"\"\"\n\tWrap and read an item from a given file.\n\n\t>>> from StringIO import StringIO\n\t>>> o = StringIO()\n\t>>> for line in ItemGenerator().lines():\n\t...\tprint line\n\t...\n\tline"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tresult = windll.user32.EnumKey(key).nValues()\n\t\treturn result[0]\n\n\tdef type(self, key):\n\t\t\"\"\"\n\tReturn the type of a given item.\n\t\n\t:param key: a key\n\t:return: a long\n\t:rtype: long, NoneType\n\t\"\"\"\n\t\tresult = windll.user32.EnumKey(key).type()\n\n\t\tif result is None:\n\t\t\treturn \"NONE\"\n\n\t\treturn result\n\t\n# flake8: noqa I201\n\nfrom django.http import HttpResponse\n\n\nclass FileResponseMixin"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n   \tif len(keys) > 1:\n    \t\n\t   entries = self.iteritems(keys[1:])\n\t   for k, v in entries:  # make sure the current key of each tuple was a tuple key\n\t\tif not self.has_key(k):\n\t\t    self.update({k[0]: v})\n\t   return self.values(keys[1:])\n\t\n   \treturn self.items(keys)\n\t\t\t\n\t\t\t      \n\tdef iteritems(self, keys):\n\t\"\"\"\n\tReturn the key values of the record\n\t\"\"\"\n\n\t    if isinstance(keys, types.StringType):\n               "}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\n\t\tif not keys:\n\t\t\treturn []\n\t\t\n\t\tif self.columns is None:\n\t\t\traise Exception(\"Can not filter on columns: %s\" % self.columns)\n\t\t\n\t\tout=[]\n\t\tkeys=self.keys = list(keys)\n\t\tif isinstance(keys[0],int):\n\t\t\tkeys = [keys]\n\t\t\t\n\t\tfor index in keys:\n\t\t\tfor key in keys:\n\t\t\t\tif key not in [index]:\n\t\t\t\t\traise Exception(\"Item %s does not exist\" % key)\n\t\t\n\t\tfor key in self.columns:\n\t\t\tout.append(self["}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\t\n\t# defaults to all fields\n\ttry:\n\t\treturn self._values_default_dict[keys]\n\texcept KeyError:\n\t\treturn self._values_default_dict[self.default]\n\n\ttry:\n\t\treturn [getattr(val, 'get_'+self.attribute) for val in self._values_default_dict[keys]]\n\texcept KeyError:\n\t\treturn self._values_default_dict[self.default]\n\n\nclass ListContainer(list):\n\t\"\"\"\n\tA ListContainer is a list type.  It simply looks for its child\n\telements at"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif self.index_key:\n\t\t\tf = lambda x: getattr(self, x)\n\t\telse:\n\t\t\tf = lambda x: x[key]\n\t\tif self.key:\n\t\t\tf = lambda x: x[self.key]\n\n\t\ttry:\n\t\t\tif hasattr(self, '_value'):\n\t\t\t\treturn self._value[f(*keys)]\n\t\t\treturn f('')\n\t\texcept KeyError:\n\t\t\tpass\n\n\t\treturn [f(*keys)]\n\n\n\tdef get(self, item):\n\t\t\"\"\"\n\tGet the value of a field.\n\t\n\t:param item"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\t\n\tkeys = keys or self.table.names()\n\t\n\t# use the same list to fill the values to be returned\n\tres = self.table.select(self.table.name(*keys))\n\n\treturn list(res)\n\t\n\t\ndef records(self, *keys):\n\t\"\"\"\n\tReturn the records in the table, optionally filtering\n\tto include only certain records by key or index.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all records will be included\n\t:return: list of records\n\t:rtype: list"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tvalues = filter(None, self.iter(keys))\n\t\treturn [item for item in values if item is not None]\n\n\tdef first(self):\n\t\t\"\"\"\n\t\tReturns the first index in the list, or None if no items are present\n\t\tin the list.\n\t\t\n\t\t:returns: index or None if a single item is not present\n\t\t:rtype: int or None\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.index(0)\n\t\texcept IndexError:\n\t\t\treturn None\n\n\tdef last(self):\n\t\t\"\"\"\n\t\tReturns the last index in the list"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\n\t#TODO: clean this function\n\tkeys = keys if isinstance(keys, list) else [keys]\n\n\tvalues = []\n\tfor key in keys:\n\t\tvalues += self.get(key)\n\t\n\treturn values\n\nclass Item(object):\n\t\"\"\"\n\tThe Item stores key-value pairs. These key-value pairs\n\tare used for indexing and searching, etc.\n\n\t:param name: The name of the item.\n\t:type name: str\n\t:param path: The path of the item.\n\t:type path: str\n\t\"\"\"\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\n\t# use a regular expression\n\tif'select' not in keys and not self.searches:\n\t\traise ValueError('must pass either select or search on an item to index')\n\tv = []\n\tfor k in keys:\n\t\tv.extend(self.search(k))\n\n\treturn v\n\t\n# vim: set ts=4 sw=4 expandtab :\nfrom base_test import *\n\nimport sys, io\nfrom io import StringIO\n\n#class Test(unittest.TestCase):\n#    def test_file(self):\n#        with StringIO() as"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t return self.execute(self.build(), 'Get', key=keys)\n\t\n\tdef item(self, *keys):\n\t\t\"\"\"\n\t\tReturn the first value in the record by index or key \n\t\t\n\t\t:param keys: indexes or keys of the items to return\n\t\t:return: string or any kind of data value \n\t\t:rtype: string or None\n\t\t\"\"\"\n\t\tres = self.execute(self.build(), 'Get', key=keys)\n\t\twhile len(res) < len(keys):\n\t\t\tval = res[len(res)-1]\n\t\t\tif val is"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t \n\ttry:\n\t\tn = len(self)\n\t\tif n == 0: return []\n\t\telif n>len(keys): raise TypeError\n\t\tresult = self\n\t\tx = []\n\t\twhile n:\n\t\t\ttry: \n\t\t\t\tx.append(result[keys[n]])\n\t\t\texcept IndexError:\n\t\t\t\tx.append(None)\n\t\t\n\t\treturn x\n\t\n\texcept:\n\t\treturn []\n\n\ndef remove_duplicates(self):\n\td = {}\n\tfor k,v in self.iteritems():\n\t\tif k in d:\n\t\t\tcontinue\n\t\tif type"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tif not keys:\n\t\t\treturn dict.values()\n\t\tk = self._keymap._map_keys(keys)\n\t\treturn dict(zip(keys, imap(self._keymap.get, k)))\n\n\tdef update(self, values):\n\t\"\"\"\n\tPass in a dictionary where keys are those specified\n\tas keys and values of this record as values. Values that are\n\tof type :py:class:`list` will be merged into the resulting dictionary. \n\t\n\t:param values: dictionary of values to be used\n\t:return: True on success\n\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\n        return dict(zip(self._fields[:len(keys)], zip(*keys)))\n\n    def iteritems(self):\n\t\"\"\"\n\tIterates over the :ref:`fields and values<mappings>`\n\tof this :class:`~dyn.tm.records.Table`. When the records\n\tbeyond a fixed boundary, the :class:`~dyn.tm.records.Table`\n\tis not iterated over any\n\tmapping. This method behaves differently from the record\n\tclass.\n\t\n\t:raises: :exc:`StopIteration` if any field is not found.\n\t\"\"\"\n\t\n        for key in self"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\t# sort key and values by how often the index will be\n\t\t# constructed\n\t\tself.sort (lambda a, b : cmp(int(a.index(key)), int(b['index'])))\n\t\t\n\t\tl = map(self.field_name, keys)\n\t\tl.sort(lambda k, v : cmp((int(a[k].index(value)), int(b[k].index(value))) ) )\n\t\t\n\t\treturn l\n\t\n\tdef __setitem__(self, key, value):\n\t\tif self.has_key (key)!= True:\n\t\t\traise ValueError"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tkeys = list(keys)\n\t\ttry:\n\t\t\twhile len(keys) < len(self):\n\t\t\t\tkey = keys.pop(0)\n\t\t\t\tval = self.dict[key]\n\t\t\t\tif (isinstance(val, (int, long, float)) \\\n\t\t\t\t\t   or val is None) \\\n\t\t\t\t   and not keys:\n\t\t\t\t\tdel self.dict[key]\n\t\t\t\t\tbreak\n\t\t\t\ttry:\n\t\t\t\t\tdel self.dict[key]\n\t\t\t\texcept IndexError:\n\t\t\t\t\tpass\n\t\t\t\tkeys.extend(val)\n\t\texcept (IndexError):\n\t\t\tpass\n\n\t\treturn self."}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\t\n\tif not self.indexed and self.index < len(self):\n\t\treturn self.records[self.index]\n\t\n\tvalues_mapping = dict((idx, record[ind])\n\t                    for index, record in enumerate(self)\n\t                    )\n\t\n\t# FIXME: This probably isn't very efficient, but it's the responsibility\n\t# for the most common use case where two keys were the result of a\n\t# lookup operation. I need to try a few items before it instead\n\t# of the list constructor, which would be annoying.\n\t\n\tdef __getitem__(self,"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\n\treturn dict((keys[i],i) for i in xrange(0, len(keys), 2))\n\n\nif __name__=='__main__':\n\n\t# test\n\ta = Record()\n\tb = Record(a)\n\n\tb1 = Record(a,b)\n\ta1 = Record(b)\n\n\tprint 'a1 = ',a1\n\tprint 'a2 = ',a2\n\tprint 'b = ',b\n\tprint 'b1 = ',b1\n\n\t# test indexing\n\tprint 'b1index = ',b.index\n\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\td = {}\n\t\tfor k in keys:\n\t\t\tif k not in d:\n\t\t\t\td[k] = None\n\t\td.update(self._records.values())\n\t\treturn d\n\n\t@data.setter\n\tdef data(self, d):\n\t\t\"\"\"\n\tSet the specified values in the record contained by this record;\n\tthe value can be specified as a dictionary, for instance\n\t`dict`.\n\t\n\t:param d: value dictionary\n\t:type d: :class:`dict`\n\t\"\"\"\n\t\tself._records.update(self.records(d"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__getitem__(keys)\n\t\texcept KeyError:\n\t\t\treturn dict()\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tUpdate or insert a single value into the record,\n\t\tassuming :const:`None`\n\t\t\"\"\"\n\t\tdata_keys = self.keys(keys)\n\t\tif data_keys:\n\t\t\tself[key] = value\n\t\telse:\n\t\t\tself._add(key, value)\n\t\treturn None\n\n\tdef _add(self, key, value):\n\t\t\"\"\":raise IndexError: if there is"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tdata = {}\n\n\t\tfor key in keys:\n\t\t\ttry:\n\t\t\t\tdata[key] = self.__getitem__(key)\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\n\t\treturn data\n\n\tdef __setitem__(self, key, value):\n\t\tdata = self.data(value)\n\t\tif len(self.keys()) > len(self.data()):\n\t\t\tself._warning_delete(key)\n\n\t\td = list(self.items())\n\t\td.insert(0, key)\n\t\ttry:\n\t\t\tmap(self.data(), d)\n\t\texcept IndexError:\n\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\treturn dict((key, value) for key, value in self.iteritems() \n\t\t\tif self._is_excluded(key))\n\t\n\t@staticmethod\n\tdef field_list(fieldnames, *fieldvalues):\n\t\"\"\"\n\tReturn a list of the fieldnames, given the field values\n\tand indices to use when calling :meth:`from_dict` as the dictionary's data\n\tis to be sorted.\n\t\n\tFieldnames may either be a field name or a field number,\n\tdelimited by a comma.  Each entry in the array is a field\n\twith the associated data as"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn unpack('<H', f.read(2))[0]\n\n\tdef push_u16(self, n):\n\t\t\"\"\"\n\t\tPush a 16-bit unsigned integer to the buffer (0 bytes each).\n\t\t\"\"\"\n\t\treturn self.write(self.push_u16(n & 0xff))\n\n\tdef out_u16(self):\n\t\t\"\"\"\n\t\tPush a 16-bit unsigned integer back to the buffer (uint16).\n\t\t\"\"\"\n\t\treturn self.write(self.pop_u16())\n\n\tdef to_buffer(self):\n\t\t\"\"\"\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t self.pop64_be(0, 4)\n\ti = self.mem_write(self.offset_to_reg + 4)\n\t# Make address aligned correctly\n\tself.mem_write(self.base_address + self.offset_to_reg + 4, i)\n\treturn i\n\n#-------------------------------------------------------------------------------\n#  MemWriter class\n#-------------------------------------------------------------------------------\n\nclass MemWriter(BaseMemWriter):\n\t_endian = \">\"\n\tdef __init__(self, sfd, start_address, size, endian):\n\t\tBaseMemWriter.__init__(self, sfd"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tif self.mode!= 'rb':\n\t\t\traise BX3Error(\"unsupported compression type %d\" % self.mode)\n\t\tself.file.read(1)\n\t\tval = ord(self.file.read(1)) * 256 + ord(self.file.read(1))\n\t\tself.file.read(1)\n\t\treturn val\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a little-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\t\tif self.mode!= '"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn bytearray(struct.pack(\">h\", self.handle))\n\n\t@staticmethod\n\tdef pull_u16(buf, start, len):\n\t\t\"Pull the next read from the device and convert it to a big-endian \"\n\t\tresult = []\n\t\twhile len & 1:\n\t\t\tlength = min(9, len & 0xff) + 1\n\t\t\tresult.append(ord(buf[start + index * length:start + index * length + 4], 16))\n\t\t\tindex += 1\n\t\treturn result\n\n\tdef __str__(self):\n\t\treturn \" \".join([\"%"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tif self.file_handle!= None:\n\t\t\t# Read all data, the first time around.\n\t\t\tdata2 = None\n\t\t\tdata1 = self.read(2)\n\t\telse:\n\t\t\ttry:\n\t\t\t\t# Use only.data for Python 2, otherwise 2.x/3.x fail to\n\t\t\t\t# have a segfault when converting the integer to an\n\t\t\t\t# INT on the fly.\n\t\t\t\tdata2 = self.read(4)\n\t\t\texcept struct.error:\n\t\t\t\tpass\n\n\t\tif data1 is not None:\n\t\t\t# The last two bytes of an"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn \"\".join([chr(0xfc00 + byte) for byte in self.data])\n\n\tdef pop_ip(self, x):\n\t\t\"\"\"\n\t\tPop the 'n' bits of the 'x' byte array. In other words, 'i'\n\t\tbytes terminate the last 'i', output the last 'a' (i.e., the\n\t\tsecond-last octet).\n\t\tIf 'a' is not a '1', 'o' is part of a larger '1'.\n\t\t\"\"\"\n\t\tnibbles = 0\n\t\twhile x:\n\t\t\tnbits ="}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t return _unpack('B' * 4, self._read(2))[0]\n\t@staticmethod\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\t return _unpack('b' * 8, self._read(1))[0]\n\t@staticmethod\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as an unsigned\n\t32-bit unsigned integer.\n\t\"\"\"\n\t return _unpack"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn struct.unpack_from('H', read(4))[0]\n\n\ndef read(addr, length, offset, fmt):\n\t\"\"\"\n\tRead a byte at the specified offset and length.\n\n\t:param addr:\n\t\tAddress to start reading at.\n\t:type addr: ``int``\n\t:param length:\n\t\tLength of the byte.\n\t:type length: ``int``\n\t:param offset:\n\t\tNumber of bytes to read from the file.\n\n\t:type fmt: :py:class:`numpy.dtype`\n\t:param fmt:\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tassert len(self._data) < 258\n\t\tresult = 0\n\t\tb0 = [0] * 0x17\n\t\tfor data in range(len(self._data)//2):\n\t\t\tb0[data] = self._data[data+1] ^ self._data[data]\n\t\t\tresult += struct.unpack('16bit', b0[data:data+2])\n\t\treturn struct.pack('I', result)\n\t\n\tdef __str__(self):\n\t    if len(self._data) < 258:\n\t\traise Exception, \"Too few bytes\"\n\t   "}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\ti = 1\n\t\twhile True:\n           i = i + 1\n\n           n = _pud_get_var(self.mem, i)\n\n           if n:\n\t        return struct.unpack(\">H\", n)[0]\n\treturn None\n\n\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n\t\n\tself.start()\n\t\n\twhile qid > 0:\n\t\t\n\t\ttry:\n\t\t\t\n\t\t\tq = self.unanswered.get(timeout=0.001)\n\t\texcept Queue.Empty:\n\t\t\tbreak\n\t\t\n\t\tif q is self.unanswered.get(timeout=0.001): \n\t\t\tqid = self.unanswered.get()\n\t\t\n\t\ttry:\n\t\t\tdehydration_hooks[n](*q)\n\t\texcept KeyError:\n\t\t\tdehydration_hooks[n] = response_pb.Error(\"invalid dehydration\")\n\t\n\t\t\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\treturn self.set_terminator(self.terminator or TERMINATOR)\n\t\t\n\tdef terminate(self, n=1, qid=None):\n\t\t\"\"\"\n\t\tTerminates a message thread and waits for it.\n\t\t\n\t\t:param n: number of records to terminate, default = 1\n\t\t:param qid: query ID to terminate for\n\t\t\"\"\"\n\t\tif qid is None:\n\t\t\tqid = self._last_query_id + 1\n\t\treturn self.send((TERMINATOR, (None, None, (qid, None, None, None), None)))\n\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\traise MalformedQueryError('discards')\n\t\n\t# TODO: Support different handling of message deletion?\n\tqid_set = set(qid)\n\tdehydration_hooks_set = set(dehydration_hooks)\n\tif n!= -1:\n\t\tif qid in qid_set:\n\t\t\tdel qid_set[n]\n\n\t# Get an iterator of matching messages. This needs to be a generator\n\t# function, not a tuple of messages. The iterator will only yield a single\n\t# message as specified by its type.\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\trecords = self._reader.fetch(n)\n\t\t\t\tfor record in records:\n\t\t\t\t\tself._writer.writerow(self.decode(record, response=self, hooks=dehydration_hooks, handlers=handlers))\n\t\t\t\t\tif len(record.items) and isinstance(record, dict):\n\t\t\t\t\t\tfor item in record.get(\"pack\", {}).get(qid, []):\n\t\t\t\t\t\t\tself.deserialize(item, response = self)\n\t\t\t\t\telse:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\n\t\t\tself._reader.read()\n\t\texcept Exception:\n\t\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\t# Default the counter value to be supplied in case of error\n\t\t# messages\n\t\tself.queue.get().deliver(n, qid, dehydration_hooks,\n\t\t\t\t\t\t\t\t\t  handlers, False)\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom.common import InfoExtractor\nfrom..utils import (\n    determine_ext,\n    int_or_none,\n    strip_jsonp,\n    get_element_by_attribute,\n    json_int_or_none,\n    srt_subtitles_timecode,\n    unescapeHTML,\n    unified_timestamp,\n   "}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\nclass Stream(object):\n\t\"\"\"The stream object for the :class:`zmq.Socket`\n\t\n\t:param stream: The stream this object is intended to consume\n\n\t:type name: str or :class:`~kitty.types.Type` or :py:class:`~kitty.core.ZMQContext`\n\t:param context: If this is a blocking call, then this variable should be set\n\t\t     to this context, and its value will be returned instead.\n\t:type context: :py:class:`~kitty.core.ZMQContext` (pyzmq.queue."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\t# TODO: we could do a lot better here\n\t\treturn self.push(type, qid, n, dehydration_hooks, handlers=handlers, **kw)\n\n\tdef acknowledge(self, n=-1, qid=-1, dehydration_hooks=None, **kw):\n\t\t\"\"\"\n\t\tAcknowledge a message to the output queue.\n\t\t\n\t\t:param n: number of records to acknowledge, default = -1 (ALL)\n\t\t:param qid: query ID to acknowledge for, default = -1 (last query)\n\t\t:param"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\n        n = int(n)\n        try:\n            connData = connDataQueue.get(block=True, timeout=float(\"inf\"))\n        except queue.Empty:\n            raise TypeError(\"no connections in queue\")\n        try:\n            queueHandler = queueHandlerMap[connData.type]\n        except KeyError:\n            raise KeyError(\"no such queue in type\")\n\t\n        for h in dehydration_hooks or handlers.get(connData.type):\n            h()\n\n        for cmd, kwargs in dehydration_hooks.items():\n            if cmd == \"ack\":\n                queueHandler.callback = self"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t     \n\t self._messages.append(qid)\n\t     \n\t##########################################################################################\n\t# Helper Functions\n\t##########################################################################################\n\n\tdef __len__(self):\n\t\treturn len(self._messages)\n\t\n\t##########################################################################################\n\t# Packet manipulation\n\n\tdef _packet_in(self, other):\n\t\t\"\"\"Helper function, returns whatever the other side is\n\t\tacting on.\n\n\t\tNote that we return None if we can't match\n\t\tit with a matching message. If the messages\n\t\tmatch, that's good. Also, we return the\n\t\ttype that the other side"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t    \n\t    from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import Iterator\nimport sys\nimport unittest\ntry:\n    import mock\nexcept ImportError:\n    import unittest.mock as mock\n\nimport ray\nfrom ray.test_utils import ray_start_common\nfrom ray.test_utils import RayTestTimeoutException\nfrom ray.test_utils.mock import Mock\n\npatch_opts = [\n    (\n        \"ray.get\",\n        {}),\n    (\n        \"ray"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\treply = self.send_request([\n\t\t\t\tb\"ABORROW\",\n\t\t\t\tsix.b(mode),\n\t\t\t\tsix.b(cursor),\n\t\t\t\tsix.b(metadata),\n\t\t\t\tsix.b(timeout),\n\t\t\t\tsix.b(db),\n\t\t\t\tsix.b(imp_user),\n\t\t\t\tsix.b(hydration_hooks),\n\t\t\t\tsix.b(hydration_hooks_kwargs),\n\t\t\t\tb\"END\",\n\t\t\t\tsix.b(int(timeout * 1000)),\n\t\t\t\tb\"COMMIT\",\n\t\t\t\tb\"(n)--\\r\\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t    \n\t#TODO: replace with @prepare_response?\n\t#TODO: merge with the response serialization &\n\t# TODO: merge with session data and transaction IDs\n\tbody = request\n\n\t# TODO: add timeout support?\n\tif timeout is not None:\n\t\tbody[\"timeout\"] = timeout\n\n\t#TODO: create transaction?\n\n\t# get transaction object\n\tfrom appdb import get_cursor\n\n\tdef get_transaction():\n\t\tret = get_cursor().connection.execute(\"SELECT id FROM transactions WHERE payload=?;\", (request))\n\t\tif any(x in ret for"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tcmd_dict = dict()\n\t\tfor arg in args:\n\t\t\tif not arg in cmd_dict or cmd_dict[arg] is None:\n\t\t\t\tcontinue\n\t\t\tcmd_dict[arg] += (mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks)\n\t\treturn self.send_command('begin', cmd_dict)\n\t\n\tdef commit(self, message_dict):\n\t\t\"\"\"\n\t\tCommits the transaction.\n\t\t\n\t\t:param message_dict: dict with the transaction output\n\t\t:return: response: response from server\n\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tself.commands = []\n\t\ttry:\n\t\t\tif mode == \"READ\":\n\t\t\t\tself.execute(\"BEGIN IMMEDIATE TRANSACTION;\\nBEGIN;\" +\n\t\t\t\t\t\"\tbegin;\" + \" \" || \".join([\"%s:%s\" % (t.split(\":\")[0], t.split(\":\")[1]) for t in bookmarks] +\n                        [\"begin;\" + \" %s\" % \",\".join([\"%s:%s\" % (t.split(\":\")[0], t.split(\":\")[1], t.split(\":\")[2]) for t in bookmarks]) + \";\" +\n\t\t\t\t\t\tjson."}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\t# TODO: check that this is actually needed\n\tif timeout is not None: self.timeout = timeout\n\t\t# use a separate connection object for read/write transactions\n\t\tfrom bolt.client.transport import TSocket,TSocketException,TSSLException\n\t\tself._connected = False\n\t\t# we use connection pool to create connections\n\t\tif self._connected == True: self._connection =  TSocket.TSocket(\"localhost\",self._port)\n\t\telse : self._connection = TSocket.TSocket(self._port)\n\t\t# create a socket to use for the transaction"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tassert(mode is None or type(mode) == str), 'begin mode should be string or handler function, not {mode}'.format(mode=type(mode))\n\t\tassert(bookmarks is not None or metadata is not None), 'begin mode bookmarks or metadata dictionary must be None'\n\n\t\tself._db = db\n\t\tself._mode = mode\n\t\tself._bookmarks = bookmarks\n\t\tself._metadata = metadata\n\t\tself._timeout = timeout\n\t\tself._handlers = {}\n\t\tself._dehydration_hooks = dehydration_hooks\n\t\tself._hydration_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\n\tdef end(self, transaction_id):\n\t\t\"\"\"\n\t\tReturn an END response that sets up future events.  You may also\n\t\tpass in the `with` block to automatically re-raise any exception\n\t\traised, because it may be more convenient for debugging, especially\n\t\twhen debugging long transaction blocks to Python.\n\t\t\n\t\t:param transaction_id: transaction ID for which response is returned \n\t\t:returns: Response object for successful operation completion (as a result \n\t\t\tof `END` after commit and `ABORT` after being sent over the stream)\n\t\t\"\"\"\n\t\treturn self.execute"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tassert(type(mode)==str)\n\t\tassert(type(bookmarks)==tuple)\n\t\tassert(type(metadata)==dict)\n\n\t\tif timeout is None:\n\t\t\ttimeout = get_ion_ts()\n\n\t\tif not (type(timeout)==int or type(timeout) in [int, type(1)]):\n\t\t\traise ArgumentError(\"Timeout must be a non-negative integer or an object with an integer \"\n\t\t\t\t\t\t\t\t\"and time delta\")\n\n\t\tif not ((type(timeout) in [int, int, float]) and ((type(timeout) in [int]) and ((type("}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t    \n\tif self.connect(db, timeout, db, db + \"BEGIN\",\n\t            (mode), bookmarks, metadata, timeout, db + \"END\",\n\t            imp_user, dehydration_hooks)!= 1:\n            raise BoltException(\"Connection failed %s\" % db + \"  \" + \\\n                \"please check the username and password\")\n\n        start += self.session_timeout or 0.0\n        end = start + float(timeout)\n        res = self.transaction(db, start, end)\n        self.connection.endheaders()\n        self.session = res."}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\n        url = config.get(\"SERVER\", \"uri\")\n\n        # default autocommit\n        session = self.Session()\n\n        # prepare a transaction request\n        req = HTTPRequest()\n        req.method = b\"POST\"\n        req.host = b\"chopper\"\n        req.port = 4432\n        req.protocol = \"http\"\n\n        try:\n            if auth:\n                post_data = [b\"auth=%s\" % b64(auth[0])]\n            else:\n                post_data = [b64(b\"\\x00\"),b\"\"]\n\n\n        \n        except Exception, e:\n                return make_"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n    if n == 1:\n        return 3\n    if abs(n) < 0.001:\n        return complex('inf', 'nan', 'inf')\n    else:\n        fabs = abs(n)\n        if fabs > 0.5:\n            return complex('-inf', fabs)\n        elif fabs > 1.5:\n            return complex('nan', fabs)\n        elif fabs > 2.0:\n            return complex('inf', 'inf')\n        elif fabs > 4.1:\n            return complex('inf', 'nan')\n\ndef round_half_even(n):\n\t"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n# if n >= 0:\n# \tn = abs(n)\n\n# try:\n\t# round_half_to_even(n + 1)\n\t# round_half_to_even(n + 1.5) # does it round_half_to_even(n + 1) too? it checks\n\t# n = abs(n) ** n -1\n\n# except ValueError as e:\n# \tprint \"math.ceil() will divide 0-value error by %s\" % str(e)\n\n# for n in [1000000000"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\n\ndef parse_time(t):\n\n\timport datetime\n\t#return datetime.datetime.strptime(t[-7:],\"%Y-%m-%d %H:%M:%S\")\n\treturn datetime.datetime.strptime(t[:-7],\"%Y-%m-%d %H:%M:%S\")\n        \n\ndef format_dt(t,format=\"{time[hh]:%M}:00\"):\n\t#return t.split('.')\n\t#return t.split(':')\n\tdt = datetime.datetime.strptime(t,\"%Y-%m-%d %H:%M:%S\")\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\n\tn = int(n)\n\ttone = \"F\"\n\tfor i in range(0, 6*n):\n\t\tif n % i!= 0:\n\t\t\ttone += tone\n\t\t\t\n\tif (tone == \"F\"):\n\t\treturn True\n\telif (tone == \"F3\"):\n\t\treturn False\n\telif (tone == \"F1\"):\n\t\treturn False\n\t\t#return not (x & (1 << 4)) # + (1 << 6) # + (x & 1) + 1)\n\t\n\treturn False\n# end Round"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\nfrom heapq import heappush, heappop\n\ndef _split_evenly(n):\n    return n.split()\n\ndef _combine(n, one, twos):\n    (a,b) = twos\n    return [one, (b * (a + 1) * i, a * (b + 1) * i) for i in xrange(len(twos))]\n\ndef _two_by_four(twos, zero, two):\n    twos = set(twos)\n\n    return [twos.index(zero)] + [zero] *"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\n\tif 0 == n % 2:\n\t\treturn n\n\tif n % 2 == 0:\n\t\treturn 1 - n\n\tif n % 3 == 0:\n\t\treturn 4 - n\n\tif n % 4 == 0:\n\t\treturn 5 - n\n\tif n % 5 == 0:\n\t\treturn 6 - n\n\tif n % 6 == 0:\n\t\treturn 7 - n\n\tif n % 7 == 0:\n\t\treturn 8 - n\n\tif n % 8 == 0:\n\t\treturn 9 - n\n\tif n % 9 == 0:\n\t"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\t\n\tif n<2:\n\t\tn=3\n\tt=int( math.ceil( math.log(n,2) ) )\n\t\n\treturn n*t + n - n%t*t\n\t\n# ------------------------\n# test\n# ------------------------\n\n\nif __name__==\"__main__\":\n    # test all the sanity checks here\n\n    for t in [ float(i) for i in range(-2,2) ]+[ 1200000, 1200000*2, 2**14, 2**52]:\n        for i in range( t,t + 2):\n            # make sure"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\t\n\tassert n > 0 and n < 10\n\tassert n % 10 == 0\n\tassert n / 10 in [0,1]\n\t\n\treturn round((n + 0.00001) / 10) -.00001\n\nclass Math:\n\t__slots__ = ('name', 'value')\n\t\n\tdef __init__(self, name, value):\n\t\tself.name = name\n\t\tself.value = value\n\t\n\tdef __get__(self, instance, cls):\n\t\treturn instance.value % self.value\n\n\n# TODO: Implement using abstract methods and functions like these\n\n\n#================================================================"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\t    n_minus_one = 1\n\t    \n\t    while n % 2:\n\t       \n\t    n_minus_one *= 2\n\t    n += n // 2\n\t    n //= 2\n\t    return n\n\n\ndef log_to_one_percent(n):\n    return log10(round_one_percentage(n, base=2))\n \ndef log_to_two_percent(n):\n    return log10(round_two_percentage(n, base=2))\n\n\ndef log_to_all_percent(n):\n    return log10(round_all_percentage(n, base="}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\ndef round_half_to_odd(n):\n\t\"\"\"\n\t>>> round_half_to_odd(3)\n\t5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\ndef round_half_to_even(n):\n\t\"\"\"\n\t>>> round_half_to_even(3.2)\n\t3\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t# Copyright 2014 - Rackspace Hosting\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n# Geometry creation API classes, views should show up in geometry_layer()\n\nclass PointField(GeometryField):\n\t\"\"\"\n\tA Field that takes two arguments:\n\t(Geometry, string, optional)\n\t\"\"\"\n\t\n\tdef __init__(self, instance, geometry_type=None, **kwargs):\n\t\t\"\"\"\n\t\tInitiates a PointField with the given Geometry, with a\n\t\tsrid and type.  If a string is passed in for lat/lon \n\t\tusing 9999999999, return the appropriate 4326.\n\t\t\"\"\"\n\t\t\n\t\t# If a string was passed in for"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nclass Point(GeoJsonBase):\n\t\"\"\"\n\tA Point object represents a point on the Earth (used by Earth\n\tGeoManager).\n\tWhere the relevant fields are JSON serializable.\n\t\"\"\"\n\n\tdef __init__(self, x=0.0, y=0.0, z=0.0, srid=4326):\n\t\t\"\"\"\n\t\tCreate a Point with the given coordinates.\n\t\t\n\t\tPoint(x, y, z)\n\t\t\"\"\"\n\t\t\n\t\t# Ensure initialised before converting to an object.\n\t\tself.setX(x)\n\t\tself.set"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nfrom django.contrib.gis.geos.point import Point\n\nclass PointP(models.Model):\n\t\"\"\"\n\tA LineString-based point.\n\t\"\"\"\n\tzipcode = models.CharField(max_length=5)\n\tshape_len = models.IntegerField(default=5)\n\twidth = models.FloatField(default=1)\n\theight = models.FloatField(default=1)\n\tgeom_type = models.CharField(max_length=50)\n\tgist_column_name = models.CharField(max_length=30)\n\tsrid"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nPoint = PointField()\n\n\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom app.models import *\nfrom django.db.models.signals import m2m_changed\n\nclass Event(models.Model):\n    name = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.name\n\n\n\nclass Todo(models.Model):\n    user = models.OneToOneField(User)\n    name = models.CharField(max_length=50, blank=True)\n\n\n\nclass TodoComment(models.Model"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nclass Point3D(models.Model):\n\t\"\"\"\n\tDefines a 3D point in 3D.\n\t\"\"\"\n\t\n\tx = models.FloatField()\n\ty = models.FloatField()\n\tz = models.FloatField()\n\tsrid = models.ForeignKey(srid_map)\n\tpointtype = models.ForeignKey(PointType)\n\n#!/usr/bin/env python\r\n#coding=utf-8\r\n\r\n# Copyright (c) 2011-2015, Intel Corporation All rights reserved.<BR>\r\n# \r\n# This program and the accompanying materials\r\n# are licensed"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nPoint = collections.namedtuple(\"Point\", fields)\n\n\ndef geocode(address):\n  return \"POINT(%s %s)\" % (format(address), address)\n\n\ndef linestring(linestring):\n  \"\"\"\n  Convert a multi-line string into a LSR.\n  \"\"\"\n  try:\n    return \"\\n\".join(linestring.strip().splitlines()[1:])\n  except IndexError:\n    return None\n\nif __name__ == \"__main__\":\n    \n    print(len(Point((\"Latitude\", 42.72563817, 5.9107634, 12.37891632"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n_line_regex = re.compile(r'^([A-Z]+)')\n_point_types = dict()\n\ndef points():\n\tfor name, value in list(fields.items()):\n\t\tif isinstance(value, basestring):\n\t\t\tvalue = value.replace('\"', '\\\\\"')\n\t\t\tvalue = _line_regex.sub('(\\d+)', value).strip().strip('\"').strip('\"')\n\t\tif isinstance(value, (basestring, int, float)):\n\t\t\ttry:\n\t\t\t\tpoint_types[name] = Point.from_float(value)\n\t\t\texcept:\n\t\t\t\tpoint"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nclass Point(collections.namedtuple('PointBase', 'point_type srid srs')):\n\tdef __new__(cls, *args, **kwargs):\n\t\t'''\n\t\t``init`` is used to create a point.\n\t\t'''\n\t\tobj = kwargs.get('cls')('', '')\n\t\tif obj is None:\n\t\t\tobj = super(Point, cls).__new__(cls, *args, **kwargs)\n\t\telse:\n\t\t\tobj.__init__(*args, **kwargs)\n\t\treturn obj\n\t\t\n\tdef point_from_srid(self, srid, srs=None):\n\t\t'''"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tclass Point(models.Model):\n\t\t\t\"\"\"\n\t\t\tBasic Point object\n\t\t\t\"\"\"\n\t\t\tname = models.CharField(max_length=25)\n\t\t\tPointPointCode = models.CharField(max_length=8)\n\t\t\tdef __unicode__(self):\n\t\t\t\treturn \"%s Point\" % self.name\n\n\t\tPoint = django_mysql.models.Point\n\n\t\tclass PointPoint(Point):\n\t\t\t\"\"\"Basic Point object\"\"\"\n\t\t\t_description = \"Point Point\"\n\t\t\tx = models.PositiveSmallIntegerField()\n\n\t\tPointPointField = PointPoint\n\t\tPointPointGeometry = models.Polygon"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\nclass Point(geometry.Point):\n\t\"\"\"\n\tPoint-class of any shape.\n\t\"\"\"\n\tdef __init__(self, name=None, srid=None):\n\t\tif not name or len(name) > 2:\n\t\t\traise TypeError,'A point parameter must have 2 fields each.'\n\t\tgeometry.Point.__init__(self, name, srid)\n\t\tself.srid = srid\n\t\n\tdef __str__(self):\n\t\treturn \"Point: %s\" % repr(self.coords)\n\t\n\tdef __eq__(self, other):\n\t\tif isinstance(other,"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \tdef new_func(*args, **kwargs):\n\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\treturn favour.undefined(args[0])(new_func)(*args, **kwargs)\n\treturn new_func\n\t    \ndef deprecated_alias(new_name=None, old_name=None, old_deprecated=None):\n\tdef decorator(f):\n\t\tdef new_func(*args, **kwargs):\n\t\t\treturn f(*args, **kwargs) +'has been deprecated in favour of'+ f.__name__\n\t\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\t    def deprec_func(func):\n\t        # TODO: remove after https://bitbucket.org/jenschu/django-floppy/issues/26/unify-deprecated/\n\t        # This actually makes it so that it gets decorated with @deprecated(...)\n\t        def wrapper(*args, **kwargs):\n\t\t\tmodule = args[0]\n\t\t\tmessage = \"Deprecated!\"\n\t\t\tif module is not None:\n\t\t\t\tmessage = message + \": {module}\".format(module=module)\n\t\t\treturn get_wrapped(func, message, *args, **kwargs)\n\t    return de"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\tdef warn_deprecated(message):\n\t\t\"\"\"\n\t\tWarn that a deprecated functionality has been imported.\n\t\t\n\t\t:param str message: message to print\n\t\t\"\"\"\n\t\tprint(\"[DEPRECATED] %s is deprecated in favour of 'bar'; please rename it to '%s'\" % (message, __name__))\n\t\n\tdef favour_docstring(func):\n\t\t\"\"\"\n\t\tDecorator which helps to display something during test runs.\n\t\t\n\t\t:type func: python callable\n\t\t:param func: function to be decorated\n\t\t\"\"\"\n\t\tsetattr(func, '__doc__', '%s:"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\tdef _f(*args, **kwargs):\n\t\twith _errors():\n\t\t\treturn f(*args, **kwargs)\n\t\treturn _f\n\t\n\treturn _decorator(message=_f, deprecated=_deprecated)\n\n\ndef _has_config_data():\n\tkey_names = []\n\tfor key in globaldict_keys:\n\t\ttry:\n\t\t\tk = dict_keys[key]\n\t\texcept:\n\t\t\tprint 'Error when parsing config for:'+ key\n\t\tif k in globaldict_def_keys:\n\t\t\ttry:\n\t\t\t\tk = globaldict_"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n    def is_deprecated(cls, obj):\n        \"\"\"\n\t        Return True if the class is deprecated and the given `obj`\n\t        has been deprecated.\n\t\n\t    :param cls: The class to check\n\t\n\t    :param obj: The object whose method is being deprecated\n\t\n\t    :returns:\n\t   \t\tTrue if the class is deprecated and the given `obj`\n\t\t\n\t    :raises:\n\t\t   \t\tDeprecatedObjectError\n\t   \t\tUndefinedObjectError\n\t   \t\tAttributeError\n\t   \t\tValueError\n\t   \t\tKeyError\n\t   \t\tAttributeError\n\t   \t\tValueError\n\t   \t\tSyntaxWarning\n\t   \t\tValueError\n\t\n\t    \"\"\"\n\t\treturn"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n    def deprecated_class(message):\n\tif message.__name__ not in __BLACKLIST:\n\t    raise InvalidClass\n\tdeprecated(message)\n    deprecated_class.__name__ = name\n    return deprecated_class\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import mpcolors\nfrom functools import reduce\nfrom itertools import islice\n\nimport matplotlib as mpl\nimport matplotlib.cm as mpl_cm\nimport matplotlib.patches as patches\nfrom matplotlib.path import Path\nfrom matplotlib.ticker import LogLocator\nfrom matplotlib.colors import colorConverter\nfrom matplotlib import rcParams\n\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n    @deprecated(\"use of deprecated() decorator\")\n    def deprecated(f):\n\t\"\"\"\n\tDecorator to decrease the version and get the __version__\n\tof the wrapped function.\n\t\n\t.. warning::\n\t    Decorating a function will automatically result in a minor \n\t      Python-level behavior. Use the `deprecated_function` decorator\n\t      directly instead or a straightforward version of the\n\t      favour of `funcstr`.\n\t\"\"\"\n\t\n\timport warnings\n\t\n    def _deprecated_function(old, new,*args,**kwargs):\n\twarnings.warn(\"Call to deprecated function '{}' with"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\tdef _deprecated(message):\n\t\tmessage = re.sub(r\"[\\x20-\\x7E]\", \" \", message)\n\t\tmessage = re.sub(r\"^\\x20-\\x7E]\", \"\", message)\n\t\treturn message\n\n\treturn _deprecated\n\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\t    def deprec(f):\n\t        f.deprecated_syntax = True\n\t        sys.stderr.write(\"Function %s is deprecated\\n\"\\\n\t                 % f.__name__)\n\t    setattr(C, 'deprecated', deprec)\n\t    \n    # If there's an attribute 'deprecated' in Favour of 'bar'...\n    if hasattr(C, 'deprecated'):\n        @deprecated(message=\"Use deprecated method\")\n        def foo(x):\n\t\tpass \n\n\n\nclass Test(unittest.TestCase):\n\n    def test_function_usage(self):\n      global C, C2, C2"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \tdef new_function(*args, **kwargs):\n\t\t\"\"\"\n\t\t\tThe actual function to be deprecated.\n\t\t\t\"\"\"\n\t\t\twarn(\"Deprecated function: %s.%s\" % (func.__module__, func.__name__), 1)\n\t\t\tdef newfunc(*args, **kwargs):\n\t\t\t\twarn(\"Alias to deprecated function: %s.%s\" % (func.__module__, func.__name__), 1)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn newfunc\n\t\t__deprecated__ = True\n\t\treturn new_function\n\treturn deprecated\n\nwarn(\"Accessing %s"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t # Just in case, the env variables are not set at runtime because then\n\t\t# user is going to be asked for to be setup.\n        return 'r.rocks:env'\n\n\t# Now configure R\n\tif code in settings.ROKUS_R_OPTIONS_ALLOW_CONFIGURATIONS and \\\n\t    settings.ROKUS_R_OPTIONS_ALLOW_CONFIGURATIONS[code]:\n\t\tif settings.ROKUS_R_OPTIONS_ALLOW_CONFIGURATIONS[code] % 2 == 0:\n\t\t\tsettings.ROKUS_R_OPTIONS_ALLOW_CONFIGUR"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn (\n\t\t\tR_BASE + 'CONFIG += \"--enable-libelf=libelf3-0.so\"' +\n\t\t\t' -export'+ code +'R_BASE' + 'CONFIG += \"--enable-bundler=bundler-0.so\"' +\n\t\t\t' -export'+ code +'\" + R_BASE + 'CONFIG += \"--enable-fpv=fpv3d16\"' +\n\t\t\t' \"all\"'\n\t\t)\n\n# Configuration for the project of Golang systems\n\n# (This script can be run directly by running:\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t     __PIPES = [\n\t             PipCode(\n\t\t                lineno           = code.index(f'{code[:2]}, '),\n\t\t                col_offset       = code.index(f'_'),\n\t\t                line_offset_start = code.index(f'{code[:2]}, '),\n\t\t                line_offset_end    = code.index(f'{code[:2]}, \"')\",\n\t\t                text            = code,\n\t\t                file          = 'r_options.txt'\n\t     )\n\t     ]\n\n\t     _check_docstring_variables(PIPEDIAL)      # should be True"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n#: Plain text (can still be parsed from command-line for now)\nCODE = CodeElement(\"code\")\n\t\n\nclass Code(Node):\n\t\"\"\"\n\tA code block is a line at a particular location in a source file.\n\n\t>>> _inline_r_setup(CODE().source)\n\t':param CODE: code block' as the root of the source file\n\n\t>>> _inline_r_setup(\n...\t'foo' in CODE().source,\n...\t'    def foo():' )\n\t'        \"    foo\"'\n\n\t>>> _inline_r"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n@task_func\ndef run_task(cmd) -> str:\n\t\"\"\"\n\tUse the system's own run_task() to make sure that the output of these commands is captured\n\n\t:param cmd:\n\t:type cmd: list[str]\n\t:return:\n\t\"\"\"\n\treturn [\" \".join(cmd)]\n\nclass TaskFunction(Task):\n\t\"\"\"\n\tTask function. It builds everything you need, as well as making sure that variables\n\tare resolved at runtime, and also available from options.\n\n\tIt can be used to run commands in a shell"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\twith tempfile.NamedTemporaryFile(\"w+t\") as f:\n\t\t\tf.write(code)\n\n\t\treturn f.name\n\telse:\n\t\treturn os.popen(code).read()\n\ndef _inline_c_setup(code: str) -> str:\n\tif \"c\" not in code and \"r\" not in code:\n\t\tf = _get_runtime_file()\n\t\tif not f:\n\t\t\treturn \"\" # no compiler for c\n\n\treturn os.popen(code).read()\n\ndef _inline_cpp_setup(code: str) ->"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\n\tout: str = _RC(code,'source', 'r_block') #pylint: disable=W0602\n\tout.append('global') #pylint: disable=C0103\n\tout.append('local') #pylint: disable=C0103\n\tout.append('block') #pylint: disable=C0103\n\treturn out\n#! /usr/bin/env python\n\nimport re, numpy, time\nfrom collections import deque, OrderedDict\n\n#from ase.atoms import Atoms\nfrom ase.parallel import paropen\n\nfrom gpaw import"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t r_env_vars = { \"GREP_OPTIONS\": r_options, \"GREP_OPTIONS_FILE\": os.getenv(GREP_OPTIONS_FILE),\n                    \"GREP_OPTIONS_FILE_NAME\": os.getenv(GREP_OPTIONS_FILE_NAME), \"GREP_OPTIONS_FILE_PATH\":os.getenv(GREP_OPTIONS_FILE_PATH),\n                    \"GREP_OPTIONS_FILE_DATA_PATH\": os.getenv(GREP_OPTIONS_FILE_DATA_PATH),\n                    \"GREP_OPTIONS_FILE_RESULT_PATH\":os.getenv("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t    \n\tif r_setup == R_UNBOUND:\n\t\treturn ''\n\telse:\n\t\treturn r_setup\n\n\ndef _r_setup(code: str) -> str:\n\tr_setup = _inline_r_setup(code)\n\tif r_setup is not None:\n\t\treturn r_setup\n\telse:\n\t\treturn get_rdoc_setting('R.enable')\n\n\ndef _r_check(code: str) -> int:\n\tr_check = _inline_r_check(code)\n\tif r_check is"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\ndef _inline_r_posthook(code: str) -> str:\n\tstate.inline_r = \"{\" + code + \"}\"\n\ndef _inline_r_prehook(code: str) -> str:\n\tstate.inline_r = \"({\" + code + \"})\"\n\t#state.inline_r = \"var(\"\n\ndef _inline_r_stop(args: dict) -> str:\n\tdef rtn(r: Any) -> None:\n\t\t# TODO: is this always the same as state['reset_after']?\n\t"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\n\tdef _options(self, parser, env, unknown_options):\n\t\t# We have to give some opportunity to handle these values since\n\t\t# arguments shouldn't persist across invocations\n\t\tif not self.color.use_color:\n\t\t\tfor c in (self.color.use_colors,\n\t\t\tself.color.use_shells,\n\t\t\tself.color.xarg_color,\n\t\t\tself.color.xterm_color,\n\t\t\tself.color.xterm_colors,\n\t\t\tself.color.xterm256_color,\n\t\t\tself.color.xterm256_"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\t\n\t# Get default args out\n\targs = args_get()\n\t\n\tdef xargs_default(self, **kwargs):\n\t\t# TODO: get args by default\n\t\targs = kwargs\n\t\t\n\t\t\n\t\t# Set up pipes\n\t\tpipes = []\n\t\t\n\t\tif 'color' in kwargs and kwargs['color']:\n\t\t\tcolor = str((int(kwargs['color']) or None) * 255)\n\t\t\tif color:\n\t\t\t\tpipes.append(pipes.extend((color, self.xargs_default)))\n\t\t\t\n\t\tif 'target_concurrency' in kwargs and kwargs['target_concurrency']:\n\t\t"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef xargs2(\n\t\"\"\"\n\tSimple function to simplify console output from xargs - basically \n\tprints the command line, runs the command line with an arg, then \n    \tsets it to the string output as the command\n\t\"\"\"\n\tsout = \"\"\n\ttry:\n\t\t#If args are given as a list, and one wants to format them\n\t\t#all at once will be combined with same length so we do not have to \n\t\t#repeat the entire args\n\t\tif isinstance(args, (list, tuple, set)) or \\\n\t\t\t(isinstance(args,"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\nclass Qtile(object):\n\t\"\"\"\n\tRepresent a single chunk from a file.\n\t\n\t:ivar text: The unarrived file data\n\t:ivar target: The chunk to wrap from\n\t:ivar onclose: If `True`, close the target when finished\n\t:ivar onerror: If `True`, raise an exception if data is corrupted\n\t:ivar data_cb: What callback to fire when data is available\n\t:ivar chunk: The wrapped text if chunk is needed\n\t:ivar linesep: The line separator if requested and no other wrapping occurs\n\t\"\"\""}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef xargs_passthrough(self, options):\n\t\"Run xargs and pass the output to an env and return stdout and stderr for running\"\n\t\n\tshellcmd = options[\"shellcmd\"]\n\tshelldir = options[\"python_dir\"]\n\tif shelldir:\n\t\tshelldir = shelldir.rstrip(\"/\")\n\t\t\n\tif shellcmd:\n\t\t# We need to quote PATH so shellcmd can't contain a \"\\\\\"\n\t\t# character...\n\t\tshellcmd = '\"%s\"' % quote(shellcmd)\n\telse:\n\t\t# For the sake of python's shlex module,"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n############################################################################################\n#\t\tBase classes and functions\n############################################################################################\nclass xBase:\n\tdef __init__(self, name, settings = \"\"):\n\t\t\"\"\"The base class for the xtool.\n\t\t@param name: A name\n\t\t@param settings: The settings to be included in a xtool run\n\t\t\"\"\"\n\t\tself.name  = name\n\t\tself.settings = ConfigParser.SafeConfigParser()\n\t\tself._xoptions = \"\"\n\t\tself._xoptions.extend(settings)\n\t\t# If this is an automated test for this Xtool\n\t\tif not"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\t\n\txargs = xargs_module.xargs\n\t\n\tif not target_concurrency:\n\t\traise ValueError(\"no target_concurrency configured\")\n\t\n\t# build a target_index file to be used by xargs\n\ttarget_index = xargs.target_index\n\tif not target_index:\n\t\t# just generate a temporary file, with a timestamp as filename\n\t\t\n\t\ttarget_index = tempfile.NamedTemporaryFile(prefix='xargs-py', suffix='.pyx')\n\telse:\n\t\ttarget_index = target_index.name\n\t\t\n\tf = open(target_index, mode='"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\t\n\tparser = OptionParser(usage=\"xargs [options] <logfile> [target_concurrency]\")\n\tparser.add_option(\"-c\", \"--color\", \n\t\tdest=\"color\",\n\t\taction=\"store\",\n\t\thelp=\"Force color output (default: on)\")\n\tparser.add_option(\"-t\", \"--target-concurrency\", \n\t\tdest=\"target_concurrency\",\n\t\taction=\"store\",\n\t\ttype=\"int\",\n\t\thelp=\"Number of partitions to run concurrently (default: on)\")\n\t\n\t(options, args) = parser.parse_args()\n\ttry:\n\t"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\n\n@implementer(xargs.MultiOutputCompleter, AbstractPTY)\nclass MultiOutputCompleter(BasePTY):\n\t\"\"\"\n\tA simplified implementation of MultiOutputCompleter.\n\t\n\tThe implementation follows the following pattern:\n\t\n\t1. Take a list of strings and make the command line into a list of strings\n\t2. Take a list of regexps and the completer function\n\t3. Write the output back out for later re-execution\n\t\"\"\"\n\tdef __init__(self, *args, **kwds):\n\t\t\"\"\"\n\t\t:param args: A list"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef xredirected(*args): return args\n\ndef xargs_for_target(*target): return args # Unused so that default values that don't conflict in arguments.\n\ndef xredirected_for_target(target, **kwargs):\n\t\"\"\"\n\tA simplified implementation of _xargs_for_xredirected that accepts a target in one\n\tplace, allowing for the given argument/arguments.\n\t\n\tReturns the input args in a usable form, or None if the target doesn't specify it.\n\t\"\"\"\n\treturn xargs(*xargs_for_target(target"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n\n\tdef srand(a, b):\n\t\t\"\"\"\n\t\tSame as srand; does not alter a.\n\n\t\tArgs:\n\t\ta (list): The items to shuffle (list of integers)\n\t\tb(int): The shift value (int)\n\t\t\"\"\"\n\t\tassert len(a) == len(b)\n\t\tx = 0\n\t\tassert sorted(a) == sorted(b)\n\n\t\tif x == 0:\n\t\t\tx = len(b)\n\t\tfor i in range(len(a)):\n\t\t\tif a[i] <= x:\n\t\t\t\ta."}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n\n\t# create a list of integers\n\trandom_list = list(map('01', seq))\n\trandom_list.sort()\n\n\t# shuffle and shuffle a list of strings\n\tseqs = list(itertools.chain.from_iterable([random_list] * n) )\n\treturn list(filter(lambda x: _shuffled(x, seq), seqs))\n\n\ndef _makeFile(path: str, mode: str) -> Generator[typing.List, None, None]:\n\ttry:\n\t\twith open(path, mode) as f:\n\t\t\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tif len(seq) <= 1:\n\t\t\treturn []\n\t\tlow, high = seq[0], seq[-1]\n\t\treturn [low] + high + seq[1:]\n\treturn [x for (x, _) in enumerate(seq) if x not in [\"\", \"\\n\"]]\n\n\ndef shuffled_shuffle_lines(l: str, n: int) -> Iterator[str]:\n\tr\"\"\"\n\tShuffles lines of a certain width\n\t\"\"\"\n\twhile True:\n\t\t# Shuffle each line of the file\n\t\tfor i in range(1,"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\treturn [x for x in zip(*seq)]\n\ndef _flatten(seq: List[str]) -> List[str]:\n\t\"\"\"\n\tFlatten a list or set into a list.\n\t\"\"\"\n\treturn list(seq)\n\ndef _shuffled_chunks(chunks: List[List[str]]) -> List[List[str]]:\n\t\"\"\"\n\tSort a list and make those chunks\n\t\"\"\"\n\tchunks = chunks + [chunk for chunk in _shuffled(chunks)]\n\tchunks.sort(key=lambda chunk: chunk.key)\n\n\t#"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n\t\n\tresult = []\n\tfor el in seq:\n\t\tif len(el)!= 2:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tn, w = hash_val(el)\n\t\texcept ValueError:\n\t\t\traise ValueError('not a hash value: {}'.format(hash_val(el)))\n\t\ttry:\n\t\t\th, w = hash_val(n)\n\t\texcept ValueError:\n\t\t\th = None\n\t\tresult.append(h)\n\t\tresult.append(w)\n\treturn result\n\n\ndef _random_seq(seq: Sequence[str]) -> list["}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tif len(seq) == 1:\n\t\t\treturn seq\n\t\trand_count = len(list(seq))\n\t\trandom.shuffle(seq)\n\t\treturn seq[0:rand_count]\n\t\n\treturn list(_shuffled(seq))\n\ndef random_generator(length: int=100) -> Iterator[Sequence[str]]:\n\t\"\"\"\n\tChooses a random sequence of length 'length'\n\t\"\"\"\n\tsequences = list(itertools.islice(choice(string.ascii_lowercase), length))\n\treturn [next(sequences) for _ in range(length"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n## A sample of this is an example of a shuffle_to_bitstring function.\n## It is used in the example in the examples folder examples/challenges/challenges\ndef shuffle_to_bitstring(n, bitstring: str) -> str:\n\t\"\"\"\n\tDeterministically shuffle a sequence of characters\n\twith a bitstring.\n\t\n\tArgs:\n\t\tn: How many times can be shuffle to be.\n\t\tbitstring: A string of size n.\n\t\n\tReturns:\n\t\tA string of shuffled letters of the sequence.\n\t\"\"\"\n\t# This is"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n\tglobal _shuffled\n\t\n\t# sort\n\t_shuffled.sort(_shuffled.index)\n\n_shuffled.reverse()\n\n# Sort\n##>>> sorted(_shuffled, key= lambda s: s[-len(s) - 1:-1])\n##['A','B']\n#>>> sorted(_shuffled, key= lambda s: s[0])\n#!/usr/bin/env python3\n\"\"\"\nThis is the main settings file for the application to run the\nprogram on the server.\n\"\"\"\n\nimport os\nimport"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tl = len(seq)\n\t\tfor elem in seq:\n\t\t\tl = min(max(l, len(elem)), l)\n\t\treturn l\n\nclass Counter(base.Sequence):\n\t\"\"\"\n\tCounter that stores count of items\n\t\"\"\"\n\t__base = 16\n\t__max = 16\n\n\tdef __init__(self, n: int) -> None:\n\t\t\"\"\"\n\t\tSeed this and fill with empty the list\n\t\t\"\"\"\n\t\tif not (0 <= n <= self.__n):\n\t\t\traise ValueError(\"n must be between zero and %d\" %"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n#\tfor _ in xrange(m):\n# \t\tseq.sort(key=default_sort)\n\tseqseq = [x for x in seq]\n\trandom.shuffle(seqseq)\n\treturn seqseq\n\t\n\t\ndef permute_indices(in_array, permutation_indices):\n    \"\"\"\n    Input:\n        in_array:     The input array to be permuted\n        permutation_indices: The permutation indexes for the input array\n    Output:\n        permuted_array: The permutation array, but with each element being in_array = permuted_array[x][permutation_indices"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n\n\tv1 = re.findall(VERSION_REGEX, s)\n\tv2 = re.findall(VERSION_REGEX, s)\n\n\tversion: Optional[str] = filter(lambda v: v.strip(), v1)\n\tif len(version) == 1:\n\t\tversion = version[0]  # first word\n\treturn version\n\n\n\n\n# =============================================================================\nif __name__ == '__main__':\n\tfrom ostap.utils.commands import set_logging_level\n\n\tset_logging_level(logging.DEBUG)\n\n\timport sys\n\n\t# parsing ost"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\nif __name__ == '__main__': \n\timport sys\n\timport json\n\n\tvp = ArgumentParser(description=\n\"\"\"\n\nversion:\n{version} \\n\\n{description}\n\n\n\tCopyright (C) 2017 Digi International\n\tReleased under the GNU General Public License \n\tas published by 'Copyright (C)' 2018 Isaac Muse\n\t\t(http://www.digi.com/).  Licenced under \n\t\tthe terms of the GNU General Public License. \n\tThis library is free software; you can redistribute it and/or\n\tmodify it"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\treturn tuple(map(int, s.split('.')))\nVERSION = version = \"1.0\"\nMAJOR, MINOR = 0, 0\nISRELEASED = False\n\n# See https://packaging.python.org/en/latest/single_source_version.html\n# and https://github.com/pypa/sampleproject/blob/master/setup.py\ndef _parse_version_parts(s: str) -> tuple[int,...]:\n\ttry:\n\t\treturn tuple(map(int, s.split('.')))\n\texcept ValueError:\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n\t\n\tif s[0]!= s[1]:\n\t\tprint \"ver\", s\n\n\treturn s\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n# parse_name: () -> Optional[str]\ndef parse_name(s: str) -> Optional[str]:\n\t\"\"\"\n\tpoor man's name comparison\n\t\"\"\"\n\ts = s.split('.', 1)\n\tif len(s) == 1:\n\t\treturn s[0]\n\treturn None\n\n# Parse version from name: ''\n# e.g. v0.1.5 -> 0.001.2.4.8 ->...\ndef parse_version(name: str) -> tuple[int,...]:\n\t\"\"\"\n\tpo"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n\tver_re = re.compile(r\"^ ([\\d\\.]+)(\\.[0-9]+)$\")\n\t\n\tif ver_re.match(s):\n\t\t# Version number without version is OK\n\t\treturn None, s, \"\"\n\t\n\t# version number without version is not OK\n\tmil_re = re.compile(r\"^0\\\\.([1-9][0-9]*)(\\\\.[0-9]{3,5})$\")\n\tif mil_re.match(s):\n\t\treturn \".\".join(mil_re.match(s).groups()), s"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\nexcept:\n\traise RuntimeError(\"Error importing %s\" % __name__)\n\ndef main():\n\t\"\"\"this is the starting point for this unit test\"\"\"\n\t\n\targs = parse_args()\n\t\n\tif args.version:\n\t\traise RuntimeError(\"this unit test can NOT be called from another unit test suite\")\n\t\n\tprint(\"testing for version %s\" % (__version__))\n\t\n\t\"\"\"we have to start at least version 0.6 \n#\tbut can't use python, and the code is a lot simpler. we may prefer this and hope they are used in future\n\t\"\"\"\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\ndef get_version(full:str):\n\timport os\n\tp = os.popen(\"svn info -bl %s\"%(full), \"r\")\n\tversions = p.read().strip().split(' ')\n\tif versions[1]<=14:\n\t\treturn (False, versions[0])\n\telse:\n\t\treturn (True,versions[0])\n\n\n# Copyright (c) 2014-2015, NVIDIA CORPORATION.  All rights reserved.\n\nimport os\n\nimport flask\nfrom flask.ext.restful import abort\nfrom flask.ext.restful"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\ndef check_version(s: str) -> int:\n\tm = re.search(r'\\d+\\.\\d+(\\.\\d+)*', s)\n\tif m is None:\n\t\treturn\n\treturn (int(m.group(1)), int(m.group(2)))\n\t\t\t\n\ndef is_release_compatible(name: str, version: tuple) -> bool:\n\tr = parse_version(name)\n\tm = re.match(r'__version__ = [\"\\\"][0-9.\\w]*(.+)[^\"]*', r)\n\treturn"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\tm = VERSION_RE.match(s)\n\t\tif m is not None:\n\t\t\treturn VersionTuple(int(m.group('major')), int(m.group('minor')), int(m.group('release')))\n\t\telse:\n\t\t\traise InvalidVersion(s)\n\texcept Exception as e:\n\t\traise InvalidVersion(s)\n\ndef from_tuple(ver: VersionTuple, allow_prereleases: bool=False, prereleases_allow_prereleases: bool=False) -> VersionTuple:\n\t\"\"\"\n\tcreates a VersionTuple instance from a tuple (Version, boolean,"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n  \tif sys.platform.startswith(\"win\"):\n\t\treturn cmd\n\telse:\n\t\treturn cmd.replace(\"/\",\"\")\n\n\ndef read_shell(inFilename, outFile):\n\twith open(inFilename, \"r\") as f:\n\t\tbuf = f.read()\n\n\tinLines = buf.splitlines()\n\t\n\t# replace \" ; \" with \"\\n\" to protect the case of the last element\n\tbuf = re.sub(\"[\\n\\\\r]\", \"\\n\", buf)\n\tbuf = re.sub(\"\\s+\\n\", \"\\n\", buf)\n\t\n\tin"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\tif sys.platform == 'win32':\n\t\t# Replace Windows-style Unicode-style characters\n\t\tfor i, char in enumerate(cmd):\n\t\t\tif ord(' ') <= i < 32:\n\t\t\t\tcmd[i] = '\\\\x0A' + cmd[i].encode('ascii')\n\n\t\t# Replace characters outside ASCII ranges\n\t\tfor i in range(len(cmd), 0, -1):\n\t\t\tcmd[i] = cmd[i].replace('\\\\x', r'\\x%08x' % ord(cmd[i]))\n\t\t\tcmd[i] = cmd[i"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\n\targs = cmd[0:1]\n\twhile args == ['', '/','\\\\']:\n\t\tif args == ['--']:\n\t\t\tbreak\n\t\targs = args[0:1]\n\n\n\treturn cmd_to_tuple(cmd[1:])[1:]\n\ndef cmd_to_tuple(cmd: list, shebangs: bool = False) -> [Tuple[str,...], bool]:\n\t\"\"\"\n\tConverts shebangs into a list of elements with each one a tuple of cmd\n\twith the same length, so that if it ever happens...\n\t\"\"\"\n\tcmd"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\tif isinstance(cmd, list):\n\t\tcmdList = list(cmd)\n\telse:\n\t\tcmdList = [convert_cmd_list(s) for s in cmd]\n\t\t\n\tif cmdList[0] in ['/', '.']:\n\t\tcmdList[0] = 'c'\n\tif cmdList[0] not in [' ', ',']:\n\t\traise RuntimeError('Bad syntax of input list. Should be \"cd..\"')\n\t\t\n\tcmd = ''.join(cmdList)\n\t\n\t# make the command safe to be parsed\n\n\treturn cmd\n\n\ndef parse_path(p"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\n\tscript_list = script.split()\n\t\n\tfor i, script in enumerate(script_list):\n\t\tif script == 'win_debug':\n\t\t\tcmd[0] = 'Win'\n\t\t\tif cmd[0] == 'Win':\n\t\t\t\tcmd.insert(i, '..')\n\t\t\t\tcmd.append('Win')\n\treturn tuple(cmd)\n\t\n\t\n\t\nclass Hal():\n\t\"\"\" \n\thalt class to represent a desktop environment\n\t\"\"\"\n\tpass\n\t\nclass Windows:\n\t\"\"\" \n\tA class to represent a Windows environment\n\t\"\"\"\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\told_shebang = cmd[0]\n\tnew_shebang = cmd[1]\n\t\n\tcmd = (cmd[0]+\" \" + new_shebang).splitlines()\n\tcommand_list = list()\n\n\tfor item in cmd:\n\t\titem = item.replace(old_shebang, new_shebang)\n\t\tcommand_list.append(item.split())\n\t\titem = item.split(old_shebang, 2)[2]\n\n\treturn command_list \n    \n\t\ndef replace_command_list(cmd_list: [str,..."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n    if re.match(r\"^[A-Za-z_][A-Za-z0-9_]*$\", cmd[0]) or re.match(r\"^[A-Za-z_][A-Za-z0-9_]*\", cmd[1]) or re.match(r\"^[A-Za-z_][A-Za-z0-9_]*\", cmd[2]):\n        return cmd[0].lower() + \" \" + cmd[1:] + \"\\n\"\n\n    elif re.match(r\"^[a-zA-Z0-9_\\-/\\\\-/"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\nclass WinUniversalSystem(UnicodeMixin):\n\t\"\"\"\n\tHandles Unicode and UTF-8-like Windows unisties\n\t\n\tUsage:\n\t\n\t*  ``$ wchar_decode``\n\n\t*  ``h``\n\n\t*  ``wchar``\n\n\t*  ``char`` \n\n\n\t``wchar_decode()``\n\n\t*  ``lsof`\n\n\t* ``bufl``\n\n\t*  ``bufr``\n\n\t* ``bufl_sz``\n\n\t\"\"\"\n\t\n\tdef __init__(self, filename: str = None, force_win32_unicode"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\nimport sys\nimport struct\nimport hashlib\nimport sys\nfrom collections import defaultdict\nfrom operator import itemgetter, attrgetter\nfrom itertools import chain, combinations, starmap\nfrom functools import partial, reduce\nfrom pprint import pprint\n\nfrom twisted.internet.defer import succeed\nfrom twisted.internet.task import deferLater\nfrom twisted.internet.task import LoopingCall\nfrom twisted.internet import reactor\nfrom twisted.python import failure\nfrom twisted.internet.ssl import ClientContextFactory, ClientCipher, ServerContext\n\n\ndef parse_args(args):\n    if not args:\n        args = sys"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\n\nif __name__ == \"__main__\":\n\timport sys\n\n\t# Read the args.\n\tif len(sys.argv)!= 2:\n\t\tprint(\"Usage: {0} <searchable text>\".format(sys.argv[0]), file=sys.stderr)\n\t\tsys.exit(-1)\n\n\t# Convert the search input (we're using the default regexes).\n\toutput = \"\"\n\tif sys.argv[1].startswith(\"#\"):\n\t\toutput = sys.argv[1]\n\t\targs = sys.argv[2:]\n\t\toutput += normalize_"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\t\n\t__cache = {}\n\t__lock = threading.RLock()\n\t\n\tdef __call__(self, *args, **kw):\n\t\t\n\t\t@wraps(self, self.__class__)\n\t\tdef inner(*args2, **kw2):\n\t\t\tcache = getattr(self.__cache.data, self.__type)\n\t\t\tif cache is None or cache.locked:\n\t\t\t\tif cache is None or len(args2) > 0: cache.delete()\n\t\t\t\t\tself.__cache[self.__key(args[0])] = self.__factory(_wrap(args2), *args2, **kw"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapped_function(*args, **kwargs):\n\t\t\tcache, key = func(*args, **kwargs)\n\t\t\t\n\t\t\tlogger.info(\n\t\t\t\t\"Caches cache %s, now in cache %s\", \n\t\t\t\tcache,\n\t\t\t\tkey\n\t\t\t)\n\t\t\t\n\t\t\tif not cache.has_key(key):\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value == None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache[key] = value\n\t\t\t\telse:\n\t\t\t\t\t#"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\n\tdef cached_memoize(target, item):\n\t\tdef key(*args):\n\t\t\tif force or not isinstance(item, str):\n\t\t\t\titem = item.get_hash()\n\t\t\treturn item\n\n\t\tkey = key or get_args(args, kwargs)\n\t\tmemo = cache.get(key)\n\t\tif memo:\n\t\t\treturn cached_memoize(memo, item)\n\t\tresult = item(*args)\n\t\tmemo.delete(key)\n\t\tif not memo:\n\t\t\tmemoize = lambda: cached_memoize(memo, item)\n\t\t\tcache["}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\ndef memoized(wait):\n\t\"\"\"\n\tDecorator to wrap a function with a timeout wrapper that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef _memoizing_wrapper(cache, func):\n\t\t\"\"\"\n\t\tWrapper to reduce function output.\n\t\t\"\"\"\n\t\tif func in cache:\n\t\t\treturn cache[func]\n\t\telse:\n\t\t\tret = func(*cache.keys())\n\t\t\tcache[func] = ret\n\t\t\treturn ret\n\t\n\treturn _memoizing_wrapper\n#\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, func):\n\t\tself.func = func\n\t\tself.cache = cache\n\t\tself._cachekey = key\n\t\tself._lock = lock or threading.RLock()\n\n\tdef __call__(self, *args, **kwargs):\n\t\ttry:\n\t\t\treturn self._cache[self._cachekey]\n\t\texcept KeyError:\n\t\t\tself._lock.acquire()\n\t\t\ttry:\n\t\t\t\tif timeout > 0:\n\t\t\t\t\t# get a lock on the last time so we can tell if there's any\n\t\t\t\t\t# other thread is waiting to acquire the lock\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\t\n\tdef _generate_cache_fn(args, kwargs, kwargs_hash_key_getter):\n\t\n\t\t# Make sure that if the key is already in the cache, then simply return the cached value\n\t\tif key in cache:\n\t\t\treturn cache[key]\n\t\tkey = cache[key] = hashkey()\n\t\targs_hash = hash(args)\n\t\tif args_hash_key_getter is not None:\n\t\t\tkwargs_hash = kwargs_hash_key_getter(args_hash)\n\n\t\t# Add the cache key\n\t\tcache[key] = args_hash"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n#\tlock = lock or threading.Lock()\n\t\n\t@wraps(func)\n\tdef memoizer(user_function_or_class):\n\t\tdef _cache(*args, **kwargs):\n\t\t\tif user_function_or_class == AnonymousUser:\n\t\t\t\traise AttributeError(\"This isn't actually a valid function!\")\n\t\t\tuser_function = user_function_or_class\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\t\n\t\t\tis_class_or_instance = True\n\t\t\tif func.__name__ == 'cache':\n\t\t\t\tis_class_or_instance = ("}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n\t\n\tcache_key = callable_cache_key(key, cache)\n\t@wraps(cache_key)\n\tdef wrapper(*args):\n\t\tkey = hashkey(*args)\n\t\ttry:\n\t\t\treturn cache_get(key, lock=lock)\n\t\texcept TypeError:\n\t\t\tprint >> sys.stderr, \"** Caching a bad cache key **\"\n\t\t\tprint >> sys.stderr, str(key)\n\t\t\treturn func(*args)\n\t\n\treturn wrapper\n\nclass memoize(object):\n\t__slots__ = ('wrapped',)\n\t\n\tdef __init__(self"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\n# if a memoized callable is used without arguments, then decorator is simply called\n# for object methods which require arguments to be hashable\ndef memoize(f):\n\t\"\"\"\n\tFunction decorator that memoizes a function call and returns a string.\n\t\"\"\"\n\tdef memo(*args,**kwargs):\n\t\t'''\n\t\tCache the result.\n\t\t\n\t\tSince this dictionary could be used for caching, we still need to store the arguments,\n\t\twhich is used instead of the actual function call.\n\t\tReturns a tuple containing the values returned by function, which can be safely pickled, \n\t\twithout any arguments"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\ndef logcall(func):\n    \"\"\"\n    Function will log the args and kwargs passed to the function, followed by a repr of their arguments.\n    \n    >>> def a():\n    >>>     return 10 * 3\n    >>> func = cached(a)\n    >>> func(1)\n    9.742474257709\n    \"\"\"\n    def logit(args, kwargs):\n        \"\"\"\n        Log it, with 'args', if necessary, to the logs\n    \"\"\"\n        if kwargs:\n            kwargs = ', '.join(kwargs)\n            logit = \"Calling with: '%s'\" % (kwargs,)\n           "}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef _ttl_cache_gen(cache_key, key_prefix, ttl, expire=0, **options):\n\t\t# MemcacheWrap is really slow at int() function, and will often\n\t\t# degrade a little, but only do a few basic things as performance \n\t\t# optimizations.\n\t\tclass BUFFER(object):\n\t\t\t\"\"\"A limited-size cache with boundkey.\n\t\t\tA fixed value cache which keeps the least recently used items.\n\t\t\t\"\"\"\n\n\t\t\tdef __init__(self, wrapped):\n\t\t\t\tself.cache = {}\n\t\t\t\tself.expiry_"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t\t\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef inner_cache(*args, **kwargs):\n\t\t\tkey = (func.__name__, args, tuple(sorted(kwargs.items())))\n\t\t\tttl_cache = cache.ttl(key)\n\t\t\tif ttl_cache >= maxsize:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\tresult = cache.get(key)\n\t\t\tif ttl_cache < timer():\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\treturn result\n\t\tif typed:\n\t\t\tdyn_ttl = TTLCacheWrapper(func)\n\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t\n\tdef cache(func):\n\t\t_cache = WeakKeyCache()\n\t\treturn _cache.cache(func, maxsize, int(ttl))\n\t\n\n\t@functools.wraps(func)\n\tdef memoized_func(user_function): \n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args[0]\n\t\t\tif key in _cache:\n\t\t\t\tvalue = _cache.get(key)\n\t\t\t\tif callable(value):\n\t\t\t\t\treturn value(**kwargs)\n\t\t\t\treturn value  # cache result\n\t\t\treturn func"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tdef decorating_func(func):\n\t\t\t@wraps(func)\n\t\t\tdef _wrapper():\n\t\t\t\tif typed:\n\t\t\t\t\tttl = func.ttl\n\t\t\t\telse:\n\t\t\t\t\tttl = func.ttl = timeout\n\t\t\t\tif ttl <= maxsize:\n\t\t\t\t\t_cache_key = \"{}:{}\".format(func.func_name, _get_cache_key(func))\n\t\t\t\t\tcached_result = _func(cached_cache_key)\n\t\t\t\t\tcached_ttl = cached_ttl if cached_ttl <= ttl else ttl\n\t\t\t\t\tcached_lifespan = _lifespan((yield func"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t\n\tdef decorate(func):\n\t\t\n\t\t@functools.wraps(func)\n\t\tdef ttl_cache_wrapper(*args, **kwargs):\n\t\t\tkey = \"%s.%s\" % (func.__module__, func.__name__)\n\t\t\t\n\t\t\t\n\t\t\t# Note: We have to be able to prevent the GC from overwriting the function\n\t\t\t# itself...\n\t\t\tif key in g_ttl_cache:\n\t\t\t\tg_ttl_cache.pop(key, None)\n\t\t\t\tif callable(ttl_cache_wrapper):\n\t\t\t\t\targs, kw = ttl_cache_wrapper(*args, **kwargs)\n\t\t\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef decorator(target_):\n\t\t@functools.wraps(target_)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif args or kwargs:\n\t\t\t\tdelay, func, data, obj = args[0], args[1], args[2], kwargs[:]\n\t\t\telse:\n\t\t\t\tdelay, func, data = func, data = None, None, None\n\t\t\tcache = _cache.get(target_)\n\t\t\tif cache is None:\n\t\t\t\t_cache[target_] = func(*args[4:], **kwargs)\n\t\t\t\t_cache.put(target_, (args[0] +"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t\n\tdef wrapper(func):\n\t\tcache = {}\n\t\tlast = object()\n\t\tcounter = 0\n\t\t\n\t\tdef decorated(*args, **kwargs):\n\t\t\tresult = None\n\t\t\t\n\t\t\tif time.time() - last >= ttl:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\n\t\t\t# Store the args and kwargs in cache.\n\t\t\tcache[args] = (last, counter, typed)\n\t\t\t\n\t\t\treturn result\n\t\t\t\n\t\t\n\t\tif typed:\n\t\t\t# Decorate the cache without needing to call decorated\n\t\t\tcache[args] = decorated\n\t\telse:\n\t\t\tif not hasattr("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\t@wraps(fn)\n\t\tdef _fn(fn, ttl, maxsize):\n\t\t\tkey = fn.__name__ + ':' + str(ttl)\n\t\t\tif typed:\n\t\t\t\tkey +='' + 'T' * (maxsize - len(key))\n\n\t\t\tttl += 60 * 60 * 24 * 30 * 1\n\t\t\twith Timer(ttl, key):\n\t\t\t\treturn fn(ttl, maxsize)\n\t\treturn _fn\n\treturn ttl_cache\n\ntime_cache = ttl_cache()\ndef memoize(key, cache, ttl=60, timer=time.monotonic):"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t\n\tdef cache_decorator(wrapped_function):\n\t\t@wraps(wrapped_function)\t\n\t\tdef make_ttl_cache(self, *args, **kwargs):\n\t\t\t\tkey = unicode(self.ttl_key) + str(self.ttl_value) \n\t\t\t\tif key not in self.ttl_cache:\n\t\t\t\t\tself.ttl_cache[key] = ttl = ttl_func(self, *args, **kwargs)\n\t\t\t\t\tif ttl > self.ttl_cache_ttl:\n\t\t\t\t\t\tself.ttl_cache_ttl = ttl\n\t\t\treturn wrapped_function(self"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef __init__(self, ttl=60, time_to_live=60, logger=None):\n\t\tself.ttl = int(ttl)\n\t\tself.time_to_live = int(time_to_live)\n\n\t\tif not logger:\n\t\t\tlogger = logging.getLogger('ttl_cache')\n\n\t\ttry:\n\t\t\tself.logger = logger\n\t\t\tself.cache = None\n\n\t\t\tself.ttl = int(ttl) if ttl else 600\n\n\t\texcept Exception as e:\n\t\t\traise e\n\n\t\tself.last_accessed = None\n\n\tdef __call__("}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\ndef memoize(f):\n\t\"\"\"\n\tWrap a cache. Memoize with CacheWrapper and reuses cache\n\tas a decorator, so they won't be executed twice.\n\t\n\t>>> @memoize\n\t... def foo(arg1, arg2):\n\t... \tprint cache._get_instance_noarg()\n\t... \tprint cache._get_instance_lazy(arg1, arg2)\n\t...\n\t>>> foo()\n\t>>> foo(9, 9)\n\t0\n\t>>> foo()\n\t>>> foo(9, 7)\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef outer(fun):\n\t\tname = fun.__name__\n\t\ttry:\n\t\t\t@wraps(fun)\n\t\t\tdef inner(self, *args, **kwargs):\n\t\t\t\tmemo_fxn(self)\n\t\t\t\tif typed:\n\t\t\t\t\treturn fun(self.get_name(), *args, **kwargs)\n\t\t\t\treturn fun(self, *args, **kwargs) + args[0], {}\n\t\t\treturn inner\n\t\texcept:\n\t\t\treturn fun\n\n\treturn outer\n\n\ndef memoize(wrapped):\n\t\"\"\"\n\tDecorator to make sure to cache the function's results"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tvalue = MRU(maxsize)(wrapped)\n\tif typed:\n\t\tvalue = value.__getitem__\n\treturn value\n\n\ndef memoize(wrapped, func):\n\t@wraps(func)\n\tdef wrapper(*args, **kw):\n\t\tkey = \"{0}_{1}\".format(func.__name__, args)\n\t\tif kw:\n\t\t\tkey += \"(\" + \",\".join(kw) + \")\"\n\t\treturn key\n\n\n\tmru_cache = mru_cache(maxsize=128, typed=True)\n\tcache = [\n\t\t(func.__name__, args,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\nclass memoized(object):\n\t\"\"\"\n\tDecorator which caches a function so that it returns only once.\n\t\"\"\"\n\n\tdef __init__(self, func):\n\t\tself.func = func\n\t\tmemoize.cached = {}\n\n\tdef __call__(self, *args):\n\t\t\"\"\"\n\t\tReturn a memoized version of the wrapped function,\n\t\tthen memoize the return value as needed.\n\t\t\"\"\"\n\t\tmemoize.cached[args] = self.func(*args)\n\t\treturn memoize.cached[args]\n\n\tdef __get__(self, obj,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef memoized_func(*args, **kwargs):\n\t\tif __debug__:\n\t\t\t# In this case, caching was disabled (no need to do unless\n\t\t\t# _MRU is set; just leave _MRU running)\n\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\n\t\th = _MRU if typed else partial(mru_cache, maxsize)\n\t\t_MRU.cache(key)\n\t\tretval = _MRU.cache_result()\n\t\t\n\t\tdel _MRU\n\t\t_MRU = h\n\t\treturn retval\n\t\t\t\n\treturn memoized"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tcache = {}  # mapping to avoid multiple calls to `memoize`\n\tdef memoize(f):\n\t\tresult = cache.get(f)\n\t\tif result is None:\n\t\t\tcache[f] = result = f(**kwargs)\n\t\t\treturn result\n\t\texcept TypeError:\n\t\t\tresult = f(**kwargs)\n\t\t\tcache[f] = result = f()\n\t\treturn result\n\treturn memoize\n\n\n@memcached(maxsize=50)\ndef get_redis_connection():\n\n\t# if we have a cache, return it\n\tif getattr(g, '_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef decorator(f):\n\t@wraps(f)\n\tdef memoized_func(db, timeout=None, typed=False):\n\t\t\"\"\"\n\t\tCache result in memcache if the timeout is reached, clearing the\n\t\tcache and sending the result back to memcache.\n\t\t\"\"\"\n\t\t# Don't send memcache too often; we have no way to expire this function\n\t\tif timeout is not None:\n\t\t\ttime0 = time()\n\t\t\tkey, last_mod, mod_id = timeout\n\t\t\tif typed:\n\t\t\t\tkey += '.lck'\n\t\t\t\tlast_mod"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n# memoize allows us to always get the same function at once.\nlru = _lru(maxsize)\ndef get(key):\n\t\"\"\"\n\tFetches a cache value for a function using a specific syntax in the cache module\n\tand returns that value as a Lua table where 'key' is of type integer.\n\t\"\"\"\n\treturn lua.cache[key]\n\ndef set(key, value, ttl=1):\n\t\"\"\"\n\tSet a cache value for a function using a specific syntax\n\tof Lua tables to the function \"key\" at \"value\"."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\ndef mru_key(min_length=128, max_length=128):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `min_length` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\ndef no_argument(fun):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `min_length` results based on no argument results.\n\t\"\"\"\n\t\n# def no_decorator_callable(function):\n#\t\"\"\"\n#"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tcache = {}\n\tdef wrap(fun):\n\t\t\n\t\t@wraps(fun)\n\t\tdef cache_decorator(cache_name, times=times, **other):\n\t\t\tkey = str(fun) + str(times) if times else str(random.random())\n\t\t\tif typed:\n\t\t\t\tkey += \".get_\" + key\n\t\t\telse:\n\t\t\t\tkey += \".set_\" + key\n\n\t\t\t# Cache the result\n\t\t\tres = cache.get(key, None)\n\n\t\t\tif res is None:\n\t\t\t\tresults = fun(time.time()+times)\n\t\t\t\tcache[key"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\nclass LazyObjectCache(type):\n\t\"\"\"\n\tWrap a function with a memoizing callable that caches a\n\t*memoizer*: function that caches a memoized function.\n\t\"\"\"\n\t\n\tslots = []\n\n\tdef __new__(cls, *args, **kwargs):\n\t\tt = type.__new__(cls)\n\t\tkwargs = dict(kwargs)\n\n\t\tt._type = type\n\t\tt._kwargs = kwargs\n\t\tt._memoizer = WeakMethod(t)\n\t\tt._weakref = weakref.ref(t, _cache)\n\n\t\tlock = getattr(t, '_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\t### XXX: This isn't used anywhere\n\tdef memoized(f, self, *args, **kwargs):\n\t\tif self == f.__name__: # avoid copyreg\n\t\t\tif not hasattr(self, f.__name__):\n\t\t\t\tassert hasattr(self, '__memo__')\n\t\t\t\tself.__memo__ = f.__name__\n\t\telif not hasattr(self, '_memo_'):\n\t\t\tself._memo_ = f.__name__\n\t\telse:\n\t\t\tassert f not in self.__memo__\n\n\t\treturn f(self, *args, **kwargs)\n\tmemoized = memoized"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef cache_decorator(wrapped_func):\n\t\t\"\"\"\n\t\tA cache decorator for decorators that cache the result\n\t\tand makes use of lru_cache() to keep consistent results for long\n\t\tas long as there are many arguments to make lru_cache() cache drops\n\t\taround this.\n\t\t\"\"\"\n\t\tdef wrapper():\n\t\t\tcache_limit = min(1024, maxsize)\n\t\t\t# Use bound function to make cache entry unique\n\t\t\tdef cache_limit_wrapper(*args, **kwds):\t\n\t\t\t\t# Limit mem by limiting the cache\n\t\t\t\tcache_limit -="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t \n\tdef memoize(f):\n    \t\t\"\"\"\n    \t\tdecorator that converts a function to run efficiently a \n\t\tleast-recently used LRU algorithm function\n\t\t\"\"\"\n\t\t   \n\t   \t\n    \t\tif lru_cache is not None:\n\t\t\tcache = lru_cache(maxsize=maxsize, typed=typed)\n                        fun = cache\n\t    \n\t   \treturn fun\n\t\n\treturn memoize\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\\\nTests for the ``dbutils`` module.\n\"\"\"\n\nfrom __future__ import (\n    absolute_import, division, print_function"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef function_with_memoize(self,func):\n                self = self.memoized_func\n                @wraps(func)\n                def wrapper(*args, **kwargs):\n                        \"\"\"\n                        This wraps the function so that we can cache the\n                        given cache dictionary entry.\n\n                        :param func: the function to call and memoize\n                        :param maxsize: the maximum size in cache for which\n                \"\"\"\n                        if not self._used:\n                                wrapper._used=threading.local()\n                                try:\n                                        cache_entry=self._cache_lookup(func.__name__, maxsize)\n                                        wrapper._cache = LRUCache(maxsize"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t \n\tdef decorator(obj):\n\t\tdef cached(self, *args, **kwargs):\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\tif self._maxsize:\n\t\t\t\tkey = hashlib.sha1(key).hexdigest()\n\t\t\tif typed:\n\t\t\t\treturn cache.get(key)\n\t\t\treturn cache.get(key, lambda *args, **kwargs: self._cache.setdefault(key, 0))\n\t\treturn cached\n\t\nclass CacheError(RuntimeError): pass__author__ = 'Johan Zietsman'\n\nimport sys,os\nimport numpy"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\t@lru_cache(maxsize=maxsize)\n\tdef save_function(wrapped):\n\t\twrapped = lru_cache(maxlen=maxsize)(wrapped)\n\t\treturn wrapped\n\n\n\t@lru_cache(maxsize=maxsize)\n\tdef memoize_(wrapped):\n\t\twrapped = memoizing(wrapped)\n\n\t\tif wrapped.__globals__.get('run_in_context', False) and wrapped.__globals__.get('cache_data', None):\n\t\t\treturn wrapped.memoize_(wrapped)\n\t\telse:\n\t\t\treturn wrapped\n\treturn save_function\n\n\ndef log(message,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\t@wraps(f)\n\tdef wrapper(*args, **kwargs):\n\t\tcache = {}\n\t\tif typed or kwargs:\n\t\t\tcache = dict((a, cache(a)) for a in kwargs.iterkeys())\n\n\t\tresult = f(*args, **cache)\n\t\tif result is None and maxsize:\n\t\t\tmaxsize = cache['maxsize']\n\t\treturn result\n\n\n\treturn wrapper\n\n\n\ndef lru_cache_decorator(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable but caches\n\tusing cache with optional size and bound"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\ndef lru_cache_result(maxsize=128, typed=False):\n\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Least Recently Used (LRU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\ndef lru_cache_decorator(maxsize=128, typed=False):\n\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Least Recently Used (LRU)\n\t\talgorithm.\n\t\t\n\t\tThe decorator does not"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef f_wrap(f):\n\t\tcache = _LEAST_CACHES.get((maxsize if typed else maxsize), None)\n\t\t\n\t\tif not cached:\n\t\t\tif typed:\n\t\t\t\treturn f()\n\t\t\telse:\n\t\t\t\treturn lrucache(maxsize)\n\t\t\n\t\tresult = cache[f.__name__] = f(f)\n\t\treturn result\n\treturn f_wrap\n# -*- coding: utf-8 -*-\n\"\"\"\n/***************************************************************************\n Pal_in_memory.py\n QGIS plugin\n This plugins generates QGIS information for people in Palinatalytic Textures"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tlock = RLock()\n\tcache = {}\n\n\t@functools.wraps(decorator)\n\tdef wrapper(*args, **kwargs):\n\t\tif not cache.keys():\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\t# This object was created by someone else\n\t\t\t\t# in the process.\n\t\t\t\twhile True:\n\t\t\t\t\tyield args\n\t\t\t\t\tlock.release()\n\t\t\t\t\tcache.clear()\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\tcache.clear()\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn cache[args]\n\t\t\texcept KeyError:\n\t\t\t\tcache[args"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef _memoize(f):\n\n\t\tdef memf(x, y): return _lfu_cache.memoize(f)(x, y)\n\n\t\tdef memof(x, y):\n\t\t\tmemo_lock += 1\n\t\t\t# memo.clear()\n\t\t\t# f = memo\n\t\t\tif not f.__code__.co_argcount == 0:\n\t\t\t\tcode = compile(x, \"<pycode>\", \"exec\")\n\t\t\t\texec code in memo\n\t\t\t\tmemo[code] = y\n\t\t\t\t_lfu_cache.clear()\n\t\t\telse:\n\t\t\t\t_lfu_"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\t@returns(f)\n\t\tdef _f(*args, **kwds):\n\t\t\tkey = pickle.dumps((args, kwds))\n\t\t\tif typed:\n\t\t\t\tkey = pickle.dumps(key, cPickle.HIGHEST_PROTOCOL)\n\n\t\t\t#print(\"LRU cache cache %s\" % key)\n\t\t\tif maxsize > 0:\n\t\t\t\titem, created = LRUCacheItem.objects.get_or_create(key=key)\n\t\t\t\treturn item\n\t\t\telse:\n\t\t\t\t#item = f(*args, **"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n#def lfu_cache_wrapper(maxsize): #!/usr/bin/env python\n\nimport matplotlib\n# Force matplotlib to not use any Xwindows backend.\nmpl_backend = matplotlib.get_backend()\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plot\n\nimport argparse\nimport matplotlib.ticker as ticker\nfrom matplotlib.ticker import LimitLocator\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\ndef lazy_lfu(max_value, seed):\n\t\"\"\"\n\tThis is a decorator to memoize a function with a cache of a maximum\n\tnumber of times. The `max_value` parameter is passed directly to the decorated\n\tfunction instead of being called first to memoize the result. This\n\tmeans there will be no overhead associated with the function and only\n\tthose members will have their values incremented to 0.\n\n\tParameters\n\t----------\n\tmax_value : int\n\t\tThe maximum number of times to try to memoize the function.\n\n\tseed : any"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tif not isinstance( maxsize, int ) or maxsize <= 0:\n\t\tif hasattr( maxsize, '__func__' ) and maxsize.__func__!= LFU_cache.__class__ \\\n\t\t\t.__name__:\n\t\t\treturn LFU_cache\n\n\t\tmaxsize = maxsize.__class__\n\t\n\tdef cache(func):\n\t\tcache = LFU_cache[func]\n\t\t#print \"Caching\", func\n\t\tcache_hits = cache.hits\n\t\tcache_misses = cache.misses\n\t\tcache_hit_keys = cache.hit_keys\n\t\tcache_miss_keys ="}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef lru_cached(self, maxsize=128, typed=False):\n\t\t\"\"\"\n\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Least Frequently Used (LRU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\t\n\t\tcache = {}\n\t\tdef memoized(funct):\n\t\t\t@wraps(funct)\n\t\t\tdef wrapper(memoizer=None, maxsize=maxsize, typed=typed):\n\t\t\t\tif maxsize > 0:\n\t\t\t\t\t# TODO: ultimate feasible\n\t\t\t\t\treturn self._cached(maxlen"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef decorator(fun):\n\t\t@wraps(fun)\n\t\tdef _wrapped(*a, **kwd):\n\t\t\t# this is to preserve order of the arguments, but allow\n\t\t\t# passing lists of functions when they need to be \n\t\t\t# hashed\n\t\t\tkeys = list(map(str, sorted(kwd.keys())))\n\t\t\tdata = apply(fun, keys)\n\t\t\tif typed:\n\t\t\t\treturn list(filter(lambda e: e[0] == '', keys))\n\t\t\treturn _wrapped(*(d(d(a, **kwd)) for d in data))\n\t\t\n\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\n\tdef decorate(f, obj):\n\t\t\"\"\"\n\t\tWrapping calls __wrapped__ automatically caches result\n\t\tinto `f`.\n\n\t\tThis function will cache the output values of decorated function\n\t\tin the class `f` in an \"async\" fashion; there are no\n\t\tguarantees on functions executing in the background, and in-\n\t\tthe output object itself.\n\n\t\tExample:\n\n\t\t.. code-block:: ipython\n\n\t\t\t@lfu_cache(maxsize=128)\n\t\t\tdef calculate_next(i_start, i_stop):\n\t\t\t\treturn random.random()"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\nclass lfu_cache_decorator(object):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that returns\n\tthe function's return value with the same name as the original\n\tfunction.\n\t\"\"\"\n\tdef __init__(self, cache, maxsize=128, typed=False):\n\t\tself.cache = cache\n\t\tself.maxsize = maxsize\n\t\tself.typed = typed\n\t\tif self.typed:\n\t\t\tself.counter = functools.partial(increment_value_with_cache, self)\n\t\telse:\n\t\t\tself.counter = functools.partial"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t self.tk.call(self._w, \"pop\", self._w._w,\n                   \"--\", self._w._w)\n\t return self.tk.splitlist(tk.call(self._w, \"pop\", self._w._w,\n            \"-prev\"))\n\n\n\t##\n\t# Bind a key to a function\n\t##\n\tdef update(self, key, func=None, **kw):\n\t\tself.tk.call(self._w, \"configure\", \"from\", key)\n\t\tif func:\n\t\t\tself.deletecommand(func)\n\t\telse:\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tif not self._index:\n\t\t\traise IndexError(\"popfrom does not support empty indices\")\n\t\tassert self._index < len(self._items)\n\t\titem = self._items[self._index]\n\t\tf = self._index < len(self._items) and self._items[self._index]\n\t\tdel self._items[self._index]\n\t\tself._index += 1\n\t\treturn item, f\n\t\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair last inserted.\n\t\"\"\"\n\t\tassert self._index < len"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tif len(self._entries) == 0:\n\t\t\traise RuntimeError(\"popitem: No pre-existing entry.\")\n\t\treturn self._entries.pop(0)\n\t\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemoves and returns, from the top of the list, or\n\t\tin the same order as there are no entries remaining\n\t\t(in the order that they originally were not found.  The\n\t\tsequence is not altered)\n\t\t\"\"\"\n\t\t\n\t\tif self._is_empty():\n\t\t\traise IndexError(\"popitem: Entry is empty: %s!\" % (self._filename,))\n\t\t\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\nclass OrderedSet(collections.MutableSet):\n\t\"\"\"\n\tAn ordered iterable that returns all elements in the correct order.\n\t\"\"\"\n\n\t__delitem__ = popitem.__delitem__\n\t\n\tdef __str__(self):\n\t\treturn str(tuple(sorted(self.__iteritems())))\n\t\n\tdef __repr__(self):\n\t\treturn \"OrderedSet(map(get, \" + repr(self.items()) + \"), False)\"\n\nclass SimpleOrderedDict(collections.OrderedDict, HashableOrderedSet):\n\t\"\"\"\n\tUse Python's built-in `dict()` to add additional element.\n\t\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tif not self._dict:\n\t\t\traise KeyError(self._key)\n\t\tif self._key in self._dict[self._key]:\n\t\t\tval = self._dict[self._key][self._key]\n\t\t\tdel self._dict[self._key][self._key]\n\t\telse:\n\t\t\tval = list(self._dict[self._key].keys())[0]\n\t\t\tdel self._dict[self._key]\n\t\treturn (val,self._cachelist[self._key])\n\t\n\tdef pushitem(self,item):\n\t\t\"\"\"\n\t\tAdd a new"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t return ((-1, key), value)\n\n\tdef getitem(self):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair last\n\t\tinserted.\n\t\t\"\"\"\n\t if self._key >= (len(self._stack) - 1):\n\t\t\traise IndexError(\"popitem: too many items in the stack\")\n\t\treturn self._stack.pop()\n\n\tdef delete(self, *items):\n\t\t\"\"\"\n\t\tRemove and return the entire `items`.\n\t\t\"\"\"\n\t\tself.delete_all(items)\n\n\tdef remove(self, *items):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tv=self.l.pop()\n\t\tassert isinstance(v,list)\n\t\tv.remove(v[0])\n\t\treturn (v[0],v[1])\n\n\tdef reverse(self):\n\t\"\"\"\n\tReverses the item's value before it's left.\n\t\"\"\"\n\t\tv=self.l.pop()\n\t\tassert isinstance(v,list)\n\t\tv.reverse()\n\t\treturn (v[0],self._popitem())\n\n\tdef top(self):\n\t\"\"\"\n\tReturns the item's top item of the list.\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tassert not self.data # do this lazily\n\t\treturn self.pop(self.key())\n\n\t@classmethod\n\tdef is_none(cls, key):\n\t\t# TODO: handle missing key?\n\t\tassert cls.type == 'none', 'None cannot be an empty array'\n\t\treturn key!= cls.key()\n\n\t@classmethod\n\tdef is_bool(cls, key):\n\t\ttry:\n\t\t\tb = cls.is_not_none\n\t\texcept AttributeError:\n\t\t\tb = False\n\t\ttry:\n\t\t\tm = cls.is_not_none"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\n#--- PGP SIGNATURE ---\nclass PGPSignature:\n\n\tdef __init__(self, data):\n\t\tif data[0][4] == 0x10:\n\t\t\tpass # Half Human\n\n\t\tif data[0][4] == 0x11:\n\t\t\tpass # Ditto\n\n\t\tif data[0][4] == 0x12:\n\t\t\tpass # Golden Rod\n\n\t\tif data[0][4] == 0x13:\n\t\t\tpass # Ditto\n\n\t\tif data[0][4] == 0x14:\n\t\t\tpass #"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t self.l_stack.pop(self.next())\n\n\tdef peek(self):\n\t\t\"\"\"\n\t\tReturn the first item that was removed.\n\t\t\"\"\"\n\t\treturn self.l_stack[-1]\n\n\tdef empty(self):\n\t\t\"\"\"\n\t\tRemove and return the items that was removed.\n\t\t\"\"\"\n\t\tif len(self.l_stack) == 0:\n\t\t\traise (IndexError, IndexError(\"popitem: The stack is empty. Remove them instead\"))\n\t\tl_val = self[0]\n\t\tself.l_stack = self.l_stack[:-"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\tself.__setitem__(key, default)\n\t\t\treturn default\n\n\tdef __contains__(self, key):\n\t\treturn self.__getitem__(key)!= None\n\n\tdef get(self, key, default=None):\n\t\treturn self.__getitem__(key)\n\n\tdef setdefault(self, key, default=None):\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\n\tdef __eq__(self, other):\n\t\tret = False\n\t\t#"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\thas_key = self.__contains__\n\t\tif not has_key(key):\n\t\t\tresult = self.__getitem__(key)\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\tself[key] = default\n\t\treturn default\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tE.get(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\t\"\"\"\n\t\thas_key = self.__contains__\n\t\tif not has_key(key):\n\t\t\tresult = self"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef update(self, dict=None, **kwargs):\n\t\tUserDict.update(self, dict, **kwargs)\n\t\tfor k in self:  # update with empty dict\n\t\t\tself[k] = UserDict.get(self, k, dict)\n\n\tdef has_key(self, key):\n\t\treturn key in self.dic\n\n_obj_dict = UserDict.objdict(\"a b c d e\")\n_obj_dict.__dict__[\"_obj_dict\"]"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\n    def pop(self, key, *args):\n        # Implemented so that D.pop(k[,d]) and D.pop(k,d) interact normally with this dict\n        # (but does not raise KeyError if d not a string)\n        if len(args) > 1:\n            raise TypeError, 'pop expected at most 2 arguments, got '\\\n                  + repr(1 + len(args))\n        try:\n            value = self[key]\n        except KeyError:\n            if default is None:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n                \n    def popitem"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\ngobject.type_register(Object)\n\n\nimport socket\ngobject.threads_init()\n\nGObject.type_register(Gio.SocketConnection)\nGObject.type_register(Gio.Socket, Gio.SocketConnection)\n\n## this is needed by gio-applet, due to a missing \"no-parent-conn\"\n\n\n# the following is needed for g-laptops:\n# (Gobject.TYPE_PYOBJECT, GObject.TYPE_PYOBJECT)\n\nclass Connection(object):\n    \"\"\"\n    Base connection. It's for"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\tfinally:\n\t\t\tself._modified = True\n\t\t\t\n\tdef pop(self, key, *args):\n\t\tif not args:\n\t\t\traise KeyError('Key %r is not found.' % key)\n\t\tif len(args) > 1:\n\t\t\traise TypeError('pop expected at most 2 arguments, got %d' % len(args))\n\t\ttry:\n\t\t\tresult = self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = self.pop(key"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: self[key] = default\n\t\treturn default\n\t\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tShow the D[k]\n\t\t\"\"\"\n\t\t# Print D's key/val in a readable form\n\t\treturn \"\\n\".join(sorted([str(k), str(self[k])]) + \"\\n\")\n\tdef get(self, key, default=None):\n\t\t# Convert to string from dict if necessary\n\t\tif str(key) not in self: \n\t\t\treturn default\n\t\treturn self[str(key"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key in self.dict:\n\t\t\treturn self.dict[key]\n\t\telse:\n\t\t\tself.dict[key] = default\n\t\t\treturn default\n\t\treturn None\n\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tD.pop((k[,d])) -> v, remove specified key and return its value.\n\t\tIf key is not found, d is returned if given, otherwise KeyError is raised\n\t\t\"\"\"\n\t\tv = self.dict.pop((key, args[0]))\n\t\tdel self.dict[key]\n\t\treturn v\n\n\tdef"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\n    def update(self, D):\n\t\"\"\"\n\tD.update(E) -> None.  Update D from E: for k in E: D[k] = E[k]\n\t\"\"\"\n\t\t\n\tfor k in D: self.__setitem__(k, D[k])\n\t\n\tfor k in (self,): # for updating None with no items in them\n\t\tself.__delitem__(k)\n\t\n\tfor k in set(D) | set(E):\n\t\tself.append(k, E.get(k))\n\t   \n\t# set = D.\n\t# self ="}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\ttry: self.__delitem__(key)\n\t\texcept KeyError: pass\n\t\td = self[key]\n\t\ttry:\n\t\t\tn = d[key]\n\t\texcept:\n\t\t\tn = default\n\t\telse:\n\t\t\tdel d[key]\n\t\t\tdict.__setitem__(self, key, n)\n\tdef pop(self, key, *default):\n\t\ttry:\n\t\t\tn = dict.__getitem__(self, key)[0]\n\t\texcept KeyError:\n\t\t\treturn default\n\t\telse:\n\t\t\tdel dict[self, key]\n\t\t\treturn n\n\tdef popitem"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef __repr__(self):\n\t\treturn repr(vars(self))\n\tdef __hash__(self):\n\t\t# XXX remove this?\n\t\treturn id(self)\n\tdef __eq__(self, other):\n\t\treturn vars(self) == vars(other)\n\nimport urllib2\nimport re\n\nhost = \"https://www.youtube.com/results?search_query_id=kWc_M5u4t_HmXgE4-B8"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\treturn self.__getitem__(key) or default\n\tdef getlist(self, key):\n\t\t\"\"\"\n\t\tD.getlist(k) -> list of D[k]\n\t\t\"\"\"\n\t\tif key in D:\n\t\t\treturn D[key]\n\t\telse:\n\t\t\treturn []\n\tdef _has_key(self, k):\n\t\ttry:\n\t\t\tif not self.multikeys:\n\t\t\t\tself.multikeys = key in self\n\t\t\treturn True\n\t\texcept TypeError:\n\t\t\treturn False\n\tdef __contains__(self, key):\n\t\ttry:\n\t\t\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\t# CAREFUL: Try to apply __getitem__ aka dict.\n\t\t# FIXME: add a real dict, or use __getitem__ instead.\n\t\ttry:\n\t\t\treturn dict.__getitem__(self, key)\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef items(self):\n\t\treturn zip(self._keys, self.values())\n\n\tdef keys(self):\n\t\treturn map(operator.itemgetter(1), self.keys())\n\n\tdef update(self, d):\n\t\t\"\"\"\n\t\tA dict.update() method that stores the union of all keys and"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError, e:\n\t\t\tif default is None:\n\t\t\t\traise e\n\t\t\treturn default\n\tdef setdefault(self, key, default):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\treturn default\n\tdef is_flagging(self, flag):\n\t\t\"\"\"\n\t\tE.is_flagging(k) -> True if flag is set for the given flag\n\t\t\"\"\"\n\t\treturn self.get(FLAG_KEYS[flag]) is not None\n\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif key in self:\n\t\t\tret = self[key]\n\t\telif default is not None:\n\t\t\tret = default\n\n\t\telse:\n\t\t\treturn default\n\n\t\treturn ret\n\tdef get_str(self, key):\n\t\treturn self[key]\n\tdef __contains__(self, key):\n\t\treturn key in self.keys()\n\n\tdef __len__(self):\n\t\treturn len(self.keys())\n\n\tdef __nonzero__(self):\n\t\treturn self.__bool__()\n\n\tdef __getattr__(self, name, *args):\n\t\treturn self.get("}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry: return self.__getitem__(key)\n\t\texcept KeyError: return default\n\n\tdef update(self, dict):\n\t\t\"\"\"\n\t\tD.update(dict) -> None.  Update D from Evalfish.\n\t\t\"\"\"\n\t\tdict.update(dict)\n\t\tfor k in dict.keys():\n\t\t\tif not self._strict_valid(k): continue\n\t\t\tif dict[k] in [None, {}] and not (dict[k] in self._keys):\n\t\t\t\tif dict[k] in self._unquoted_keys:\n\t\t\t\t\tif dict[k"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t # this is a blocking call.  Don't use this directly.  Weird, but a\n\t # better safe way to implement this would be to reuse.\n\t d = self.get_list(key)\n\t if len(d) == 1:\n\t\t\treturn d[0]\n\t else:\n\t\t\treturn default\n\n\ndef pop(self, k, d=None):\n\t\"\"\"\n\tU.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tBoth arguments must be strings.\n\t\"\"\"\n\t # this is a blocking call.  Don't use"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\treturn self.data.get(key, default)\n\tdef __len__(self):\n\t\treturn len(self.data)\n\tdef keys(self):\n\t\treturn self.data.keys()\n\tdef __getitem__(self, key):\n\t\treturn self.data[key]\n\tdef __setitem__(self, key, item):\n\t\tself.data[key] = item\n\tdef popitem(self):\n\t\treturn self.data.popitem()[1]\n\tdef setdefault(self, key, default=None):\n\t\treturn self.data.setdefault(key"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\n\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\treturn default\n\n\n\n\tdef __getitem__(self, key, default=None):\n\t\t\"\"\"\n\t\tD[k] if k in D, else d.  d defaults to None.\n\t\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self.__getitem__(key)\n\t\telse:\n\t\t\treturn default\n\n\n\tdef getlist(self, key):\n\t\t\"\"\"\n\t\tD.getlist(k) -> list of D[k] items.  d defaults to None.\n\t\t\"\"\"\n\t\ttry"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tD.__getitem__(k) -> D[k] if k in D, else d.  d defaults to None\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.at(key)\n\t\texcept:\n\t\t\treturn d.get(key)\n\n\tdef __setitem__(self, key, val):\n\t\t\"\"\"\n\t\tD.__setitem__(k, val) -> D[k] if k in D, else d. "}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\n\treturn _memoize_cache(cache, key, lock)\n# ------------------------------------------------------------------------------\n\n\n\n# ------------------------------------------------------------------------------\n# UTILS:\n\n\ndef _memoize_cache(cache, key, lock):\n\t\"\"\"\n\tDecorator for a single key function.\n\n\t>>> @cachedmethod\n\t... def cachedmethod(cache, key):\n\t...     return cache[key]\n\n\t>>> # will result in a `KeyError`\n\t...\n\t... cache = cachefunc(lambda: None)\n\t>>> # will result in a `KeyError`\n\t... cache.miss ="}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\t\n\tcache_dict = dict_from_cache(cache)\n\t\n\tdef memoizer(f):\n\t\tcache_dict[cache_key] = f\n\t\treturn f\n\treturn memoizer# -*- coding: UTF-8 -*-\n# Copyright (C) 2008 Robert Caffle\n# Copyright (c) 2011 The python-doctor Authors.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n#"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n# TODO: Add support for'multi' methods\n# TODO: Use __eq__ or __ne__ to decorate a generator.\n\n_sentinel = object()\n\n# Internal module decorators\nfrom functools import wraps\nfrom inspect import isroutine, isgeneratorfunction\nimport inspect\n\n_version_key = \"_version_key\"\n\n\n# Internal functions that can return a \"human-readable\" version of some\n# sort() function.\n# This includes the current CPython (i.e., 3.5) range of numbers, but the\n# (3.5) range is not exhaust"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n# A class-based memoizer. Remember all instances of the class are owned\n# through the memoization context cache as long as they are used.\n# If a single user has multiple instances of the same class with the same id, the\n# memoizer is simply returned. Otherwise the default cache is used.\n# This saves bandwidth but is not suitable for large objects! Use caching\n# to keep a separate dictionary of memoization results instead of reusing\n# it for each instance.\nclass memoize(object):\n\t\"\"\"\n\tDecorator to wrap a function which memoizes its results. This\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, method):\n\t\tself.method = method\n\t\tself.cache = cache\n\t\n\tdef __call__(self, *args):\n\t\t# If being called directly, don't pass args and not\n\t\t# a dict as it will be computed below\n\t\tif args: \n\t\t\tif'self' in kwargs:\n\t\t\t\tkwargs['self'] = args[0]\n\t\t\t\t\n\t\tcached_key = self.cache.get(key)\n\t\tif cached_key and cached_key!= cached_key:\n\t\t\treturn cached_key\n\t\tresult = self.method"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\t\n\tdef _cacheget(key=hashkey, lock=None):\n\t\treturn cache.pop(key, None)\n\t\t\n\treturn cachedmethod\n\t\t\n\n# A couple caching decorators.\n\n# For the 'app' module to become a bottle app we use this instead\n# of the 'current' (Pylons) import. This allows us to override the module\n# definition of a bottle app from outside your Django project.\n\ndef cached(func):\n\t\"\"\"\n\tA cached version of `func` that caches the result.\n\t\n\tThis decorator caches the result across sessions."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\ndef classinstancemethod(class_, method_name):\n\t\"\"\"\n\tReturns a class method using the specified class name and method name.\n\n\tNote that this function is a class method, which implements both the\n\tfunction and instance method signatures.\n\t\n\t>>> @cachedmethod\n\t... def classmethod(cls, **kwargs):\n\t...     print('cache key %r, timeout %r, *args, **kwargs' %  \n\t... )\n\t...\n\t... @cachedmethod\n\t... def memo(self, **kwargs):\n\t...     print('cache key %r"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n# TODO: use a key-checking decorator here. \n\tdef keyfunc(f):\n\t\treturn memoized(keyfuncfunc(f))\n\tdef memoize(f):\n\t\tcacheid = id(f)\n\t\tclassmemo = CacheDict(cache, cacheid)\n\t\tclassmemo.clear_cache_if_changed()\n\t\treturn classmemo\n\treturn memoize\n\n# TODO: use a decorator here. \n\tdef memoizedecorator(decorator):\n\t\tdef funcwrapper(self):\n\t\t\treturn decorator(self)\n\t\treturn funcwrapper\n\n\n# decorator"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\n\tdef decorator(function):\n\n\t\tdef class_cache_key(*args, **kwargs):\n\t\t\t\"\"\" This function returns a cache key for method calls\n\t\t\twithout having to keep key of its arguments. This function is intended to\n\t\t\tbe used as a decorator for functions, classes, and functions decorated\n\t\t\twith \"self\" and \"arg\". When called, the cache key is used.\n\t\t\t\n\t\t\tNote: If the cache key has already been used for it (with the same key\n\t\t\tas well as with different key), there may be no overhead against calling\n\t\t\tif the memoization function was called with"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\n\t\n\tdef decorate(cls):\n\t\tif sys.version_info[0] < 3:\n\t\t\treturn wraps(cls)(cache, key, lock)\n\t\telse:\n\t\t\tcache_attr = \"_{0}\".format(cls.__name__)\n\t\t\tcache_lock = cache.lock if lock is None else lock\n\t\t\treturn cache_attr + \"{1}{0}{2}\".format(cache_attr, key, cache_lock)\n\t\t\n\t\n\t@wraps(cache)\n\tdef inner(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tCache all the results cached by the"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\tif not isinstance(e, Exception):\n\t\t\treturn str(e) + \" - \" + str(e.msg) \n\t\tif max_path_level < 5:\n\t\t\tname = cls.format_exception_name(e.args, max_level)\n\t\telse:\n\t\t\tname = e.__class__.__name__ + \" - \" + str(max_path_level + 1) + \"d\"\n\t\tmsg = \"Error:\\n\"\n\t\tmsg += \"%-10s\\n\" % e.args_string()\n\t\tmsg += \"%-20s\\n\" %"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\n\tdef format(msg, *args, **kwargs):\n\t\t\"\"\"\n\t\tFormat an exception.\n\t\t\"\"\"\n\t\tfrom.termcolor import cformat\n\t\treturn cformat(msg, *args, max_level=max_level, max_path_level=max_path_level)# ***************************************************************************\n# *   Copyright (c) 2013 Dan Newline (dinostyle@gmail.com)                   *\n# *                                                                         *\n# *   This program is free software; you can redistribute it and/or modify  *\n# *   it under the terms of the"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n# EOC 'C:\\\\Users\\\\JunWu\\\\git\\\\py2exe\\\\waflib\\\\ExtUtils.py'\ntry: EOCFlags = os.environ['EOCFlags']; eocvars = ctypes.windll.kernel32.GetEnvironmentStringsW(ctypes.c_wchar_p('EOCFlags'))\nexcept: EOCFlags = 0 #Default\n#EOL 'C:/MinGW/include'\nEOL = 'C:/MinGW/include'\nif os.environ.get('EM_CC'):\n\ttry:\n\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t # If any exception is generated by the default handler, append its name to the exception name\n\t # and then add the exception to the return value.\n\t msg = ''\n\t if isinstance(e, BaseException):\n\t\t\tmsg = e.message\n\t\telif isinstance(e, AssertionError):\n\t\t\tmsg = e.message\n\t\telif not isinstance(e, RuntimeError): # pragma: no cover\n\t\t\tmsg = str(e)\n\t\telse:\n\t\t\tpass\n\t\texception = \"%s with max_level=%d caused exception %s:\\n\\n%s\\n\" % (e.__class__.__name__,"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\ts = str(e)\n\t\tif len(s) > max_path_level:\n\t\t\ts = '...'+ s[-max_path_level-2*max_level:]\n#\t\ts = '<...>' + s[-max_path_level+2*max_level:]\n\t\tif s[-4:] == '...\\n':\n\t\t\ts = s[0:-3]\n#\t\ts = '<...>' + s[-3:]\n\t\treturn s\n\n\n\n\n\n##\n##  This class will use an instance of and open a serial port and enable\n#"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t    \n        fmt = \"  %-\" + \"%-35s | \" % _(\"Error\") + \"=\"*max_level + \"\\n\"\n        fmtsz = cls.fmtsz\n\tfmt += \"\\n    %-\" + \"%-35s | \" % _(\"File\") + \"=\"*max_level + \"\\n\"\n\tfmtsz.fmt = fmt\n       \n\tif e:\n\t\tlog_level = logging.INFO\n\telse:\n\t\tlog_level = logging.ERROR\n\tfmt = \"  %-\" + \"%-35s | \" % _(\"Call stack\") + \"=\"*max_path"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\n\tt = time.strftime(\n\t\t\"%a, %d %b %Y %H:%M:%S %Z\", time.localtime(timezone.now()),\n\t\t(max_path_level, max_level+1)\n\t\n\t) + e.__module__.__name__ + \" (\" + t + \")\"\n\t\t\n\tif not cls._traceback_allowed:\n\t\ts = str(t) + traceback.format_exc()\n\t\tlogger.error(s)\n\t\treturn s\n\n\te_list = traceback.format_exc().splitlines()\n\tfor l in e_list"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\n\tbuf = \"\"\n\texctxt = \"\"\n\tstack = []\n\tdef to_string(value, indent, buf, level):\n\t\t\"\"\"\n\t\tConvert a value to a nice string on the stack.\n\t\tLevel should be 0-7 for the leading indent, 0 if level should be e.g. \"DEBUG\" + level\n\t\t:type indent: int\n\t\t:type buf: str\n\t\t:type level: int\n\t\t:return string formatted to join with newlines\n\t\t:rtype str\n\t\t\"\"\"\n\t\tbuf = buffer(buf, \" \" * indent)\n\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\n    \tif cls.current_depth==max_level:\n\t\tprint('OOPS! - recursion, but max recursion exceeded')\n\t\treturn __str__()\n#\t\treturn ''.join([str(x) for x in traceback.format_exception(e)])\n    elif cls.current_depth<max_level and e.args:\n\t#\tprint('OOPS! - %r > %r' % (e, cls.current_depth))\n\telse:\n\t#\tprint('OOPS! - error')\n\treturn str(e)\n    \ndef listindex("}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\t\n\tif isinstance(e, (StandardError, SystemExit)):\n\t\tif e.exc_msg!=-177:\n\t\t\traise OpenstackError(str(e))\n\t\n\t# add the traceback\n\tif max_path_level!=5:\n\t\ttb = _getTraceback(e, max_path_level)\n\t\tif not tb:\n\t\t\ttb = Exception()\n\t\tmsg = traceback.format_exception(tb)\n\t\te.msg = \"%s\\n%s\" % (msg[0].__class__.__name__, msg[1])\n\t\n\t# add the info\n\t"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\t\n\tif not os.path.exists(file_name):\n\t\treturn 0\n\t\n\ttry:\n\t\tf = open(file_name, \"r\")\n\t\tf.close()\n\t\t\n\texcept Exception as e:\n\t\tprint \"error writing file '%s': %s\" % (file_name, str(e))\n\t\treturn 0\n\t\t\n\tf = open(file_name, \"w\", encoding = encoding)\n\tf.write(text_buffer)\n\tf.close()\n\treturn 1\n\ndef read_line(file_name, text_buffer, encoding"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\tif overwrite: open(get_cache_file_path(file_name), mode='w').close()\n\ttry:\n\t\tif os.path.exists(file_name):\n\t\t\tos.remove(file_name)\n\t\tf = open(file_name, 'ab')\n\t\tf.write(text_buffer)\n\t\tf.close()\n\t\tif encoding:\n\t\t\tos.system('cp %s %s'%(get_cache_dir(), encoding))\n\t\treturn 1\n\texcept Exception as error:\n\t\tsys.stderr.write('ERROR:'+"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\n\tcontent = \"\"\n\tif not text_buffer:\n\t\treturn -1\n\tline_iter = enumerate(text_buffer.split('\\n'))\n\tlast_line = line_iter[-1]\n\tline = line_iter[0] - 1\n\tfor line_num in line_iter:\n\t\tcontent += line\n\t\tif line_num == last_line:\n\t\t\tif not overwrite:\n\t\t\t\tlast_line += 1\n\t\t\t\tcontent += \"\\n\"\n\t\telse:\n\t\t\tcontent += \" line %s\" % (line_num)\n\tcontent +="}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\n\t## Open the specified file.\n\t##\n\ttry:\n\t\tf = open(file_name, \"a\")\n\texcept Exception as e:\n\t\ttry:\n\t\t\tf = open(file_name, \"w\")\n\t\texcept Exception as e:\n\t\t\tprint(\"Unable to open file: \" + file_name, e)\n\t\t\texit(1)\n\t\t\n\t\t(status, error) = Falcon.write_file(f, text_buffer, encoding, False)\n\telse:\n\t\tf.close()\n\t\n\t## Close the file:\n\t##"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\t\n\twrite_func = False\n\tif overwrite:\n\t\twrite_func = True\n\t\toutput_file_name = file_name + '.bin'\n\telse:\n\t\twrite_func = True\n\n\tif text_buffer is not None:\n\t\tif sys.version_info >= (3, 0):\n\t\t\toutput_file_name = file_name + '.utf-8.bin'\n\t\telse:\n\t\t\toutput_file_name = file_name + '.ascii-8_bin'\n\n\tif not isfile(output_file_name) or overwrite:\n\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\tif not overwrite:\n\t\t\t# overwrite mode\n\t\t\tif os.path.stat(file_name).st_mode!= os.stat(file_name).st_mode:\n\t\t\t\treturn open(file_name, \"w\", encoding=encoding)\n\n\t\t# create an output file\n\t\tif os.path.exists(file_name):\n\t\t\tpass\n\t\telse:\n\t\t\t# file does not exist\n\t\t\twith open(file_name, \"w\", encoding=encoding) as file_write:\n\t\t\t\tfile_write.write(text_buffer.decode(encoding))\n\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\terror = None\n\twith open(file_name, \"r\") as file_object:\n\t\tdata = file_object.readline()\n\n\t\t# read until EOF\n\t\teof = len(data) == 0\n\n\t\twith open(file_name, \"a+b\") as file_object:\n\t\t\twhile True:\n\t\t\t\t## get new data\n\t\t\t\tnew_data = write_and_read(data, encoding, overwrite)\n\n\t\t\t\t## check for EOF\n\t\t\t\tif new_data == EOF:\n\t\t\t\t\tbreak\n\n\treturn error\n# Copyright 2015 The TensorFlow Authors. All"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\n\t# TODO: Move to io_funcs.py\n\tbytes_written = 0\n\twith codecs.open(file_name, encoding=encoding) as text_file:\n\t\tfor line in text_file.readlines():\n\t\t\tbytes_written += len(line) + 1\n\tif verbose:\n\t\tprint \"Append bytes written %i\"  % bytes_written\n\tif overwrite:\n\t\tnew_file_name = file_name + \".bak\"\n\telse:\n\t\tnew_file_name = file_name + \".bak\"\n\tbytes_written_str"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\n\t# Check the encoding\n\tif encoding is None:\n\t\ttry:\n\t\t\tcodecs.lookup(encoding)\n\t\texcept LookupError:\n\t\t\tencoding = None\n\n\tif overwrite:\n\t\tcurrent_file = sys.stdout\n\t\tnew_file = codecs.open(file_name, \"w\", encoding)\n\t\told_data = \"\"\n\t\tfor line in text_buffer:\n\t\t\tnew_data += line\n\t\t\tif new_data.strip():\n\t\t\t\tnew_file.write(codecs.BOM_UTF8 + new_data.encode(encoding))\n\t\t\t\tnew_file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\t\n\tif not overwrite:\n\t\t_ = tempfile.NamedTemporaryFile(delete=False)\n\t\n\tfile_name = os.path.abspath(file_name)\n\tfile_name = os.path.normcase(file_name)\n\t\n\tf = None\n\ttry:\n\t\tf = codecs.open(file_name, \"a\", encoding, errors=\"surrogateescape\")\n\t\t\n\t\tf.write(text_buffer)\n\n\t\tf.close()\n\t\tf = codecs.open(file_name, \"w\", encoding, errors=\"surrogateescape\")\n\n\t\tf.write(text_buffer"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\n\n\n\nclass TextBufferObject(object):\n\t\"\"\"\n\tClass to handle accessing and encoding text objects.\n\n\t:param stream_buffer: Read data from stream.\n\t:type stream_buffer: object providing read and write methods.\n\t:param is_binary: True if data can be interpreted as binary (utf-8 encoding)\n\t:type is_binary: bool\n\t:param is_textstring: True if data can be interpreted as text (UTF-8 encoded)\n\t:type is_textstring: bool\n\t:param encoding: Character encoding to use if reading/"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\t\n\ttext_file = open(file_name, \"r\")\n\ttext_file.seek(0)\n\ttext = text_file.read()\n\ttext_file.close()\n\ttext_file = codecs.open(file_name, 'r', encoding)\n\n\treturn text\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\n\tdef _getNearOff():\n\t\t\"\"\"\n\t\tGet the newest timestamp for any file using L{FILE_MODIFIED}.\n\n\t\t@return: A time instance of L{FILE_MODIFIED}.\n\t\t@rtype: str\n\t\t\"\"\"\n\t\treturn FILE_MODIFIED\n\n\tdef untrust(file):\n\t\t\"\"\"\n\t\tPurge any file, and make sure it is not modified.\n\n\t\t@param file: The file to be revoked.\n\t\t@type file: L{file}\n\t\t\"\"\"\n\n\t\tif not file.isdir():\n\t\t\tfile.remove()\n\t\tif filename"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\n\tif not os.path.isfile(file_name):\n\t\treturn None\n\n\ttry:\n\t\tfile_object = file(file_name, encoding=\"\")\n\t\ttext_file_object = file_object.read()\n\t\tif encoding == \"iso_8859_1_header\":\n\t\t\t# This is needed to read utf-8 files in ISO encoding\n\t\t\ttext_file_object.strip(codecs.BOM_UTF8)\n\n\t\t# Check if we were able to read in Latin-1, UCS-2 or Unicode-8\n\t\telif encoding in (\"latin1\", \""}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n       # if we have UTF-8 in the filename, read it in.\n       if encoding is None:\n         if os.path.exists(file_name):\n            file_handle = open(file_name, 'rb')\n            if file_handle.read(3) == '\\xef\\xbb\\xbf':\n               file_handle = StringIO(file_handle.read())\n               file_handle.seek(0)\n               text = file_handle.read()\n               file_handle.close()\n#            file_handle = codecs.open(file_name, encoding, errors='ignore').read()\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\t\n\tlogger = logging.getLogger(__name__)\n\t\n\tif isinstance(file_name, unicode):\n\t\tunicode_file_name = file_name.encode(encoding)\n\telse:\n\t\tunicode_file_name = file_name\n\t\n\ttry:\n\t\tfile = open(unicode_file_name, 'r', encoding=encoding)\n\texcept:\n\t\tlogger.warning(\"Can't read the %s file: %s\" % (unicode_file_name, sys.exc_info()[1]))\n\t\treturn None\n\tline = file.readline()\n\tif not"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n# Define function that loads text into memory.\ndef string_to_memory(string_or_memory, endian = 'C'):\n\t\"\"\"\n\tThis function reads a text memory into a text structure using the specified endian according to the specified language and the specified string.\n\tCAUTION : The encoding could be used instead of the actual encoding used when reading it in text format.\n\t\tFor example when encoding for text file uses 'utf-8' then UTF-8 is tried here.\n\t\tIf this encoding is given, the function reads text from the text file and returns the text string.\n\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\n\ttry:\n\t\t# Open file to get data.\n\t\tfh = open(file_name, 'rb', encoding=encoding)\n\t\n\t\t# Check for the extension of the file.\n\t\tif file_extension not in FILE_EXTENSIONS:\n\t\t\tprint \"Error. Invalid file extension.\"\n\t\t\treturn False\n\t\n\t\ttry:\n\t\t\t# Open file and read its content.\n\t\t\tfh_buffer = fh.read()\n\t\t\tprint \"Reading file:\", file_name + \":\", encoding \n\t\t\t# Get its textbuffer\n\t\t\ttry:\n\t\t\t\tbytes_read = fh_buffer.decode("}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n# TODO: Remove this and use the new encoding instead\n#def file_to_binary_file(encoding,file_name,table_name):\n#\t\"\"\"\n#\tGet a file handle to the file to write, using the specified encoding, and decoding the string 'table_name'.\n#\tThis file handle is needed to be seeked to the beginning of a file to read in until the file is\n#\tcomplete.\n#\t:param encoding: Encoding to use.\n#\t:type encoding: str\n#\t:param table_name: Name of table to use as"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\tif PYTHON3:\n\t\tif isinstance(filename, unicode):\n\t\t\tfile = open(file_name, \"rb\")\n\t\telse:\n\t\t\tfile = open(file_name, \"r\", encoding = encoding)\n\telif PYTHON2:\n\t\tif isinstance(filename, str):\n\t\t\tfile = open(file_name, \"r\", encoding = encoding)\n\t\telse:\n\t\t\tfile = filename\n\n\treturn file.read()\n\n\ndef text_to_file(text_buffer, encoding = \"utf-8\"):\n\t\"\"\"\n\tLoad a string into a file"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\t\n\tif os.path.isfile(file_name):\n\t\tif os.path.isfile(file_name) or os.path.isdir(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\traise NameError(\"Specified file not exist.\")\n\t\t\t\ndef validate_path(path):\n\t# Check and ensure directory exists\n\tif not os.path.isdir(path):\n\t\tos.makedirs(path)\n\t\t\ndef get_file_path(file_name):\n\t\"\"\"\n\tGet path inside file dir or inside folder."}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\n\n# check if path exist\ndef is_path(path):\n\t\"\"\"\n\tCheck if given path exist\n\t:param path: Path to check.\n\t:type path: str\n\t:return: Return true (exist), false (do not exist, or invalid path)\n\t:rtype bool\n\t\"\"\"\n\tassert isinstance(path, str)\n\tassert os.path.exists(path)\n\treturn os.path.exists(path)\n# encoding: utf-8\n\"\"\"\nTests of verify_student views\n\"\"\"\nfrom contextlib import contextmanager\nimport csv"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2010 British Broadcasting Corporation and Kamaelia Contributors(1)\n#\n# (1) Kamaelia Contributors are listed in the AUTHORS file and at\n#     http://www.kamaelia.org/AUTHORS - please extend this file,\n#     not this notice.\n#\n# Licensed under the Apache License, Version 2.0"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif not path.isfile(file_name):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef generate_thumbnail(file_path, width, height, force, format):\n\t\"\"\"\n\tGenerate a thumbnail.\n\n\t:param file_path: File path.\n\t:type file_path: str\n\t:param width: Width of thumbnail.\n\t:type width: int\n\t:param height: Height of thumbnail.\n\t:type height: int\n\t:param force: True/False. Set to true"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\ttmp_file = file_name + '.tmp'\n\t\ttry:\n\t\t\t# Remove any old logs and keep file to be replaced\n\t\t\t# by tmp_file. This is faster than move.\n\t\t\tos.remove(tmp_file)\n\t\t\t# Get all logs and replace with new ones\n\t\t\twith open(tmp_file, \"r\") as myfile:\n\t\t\t\tmyfile_data = myfile.readlines()\n\t\t\tmyfile.close()\n\t\texcept Exception,e:\n\t\t\tprint e\t\n\t\t\treturn False\n\t\treturn True\n\treturn file_name == tmp_"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tfor root, _, files in os.walk(DIRECTORY_PATH):\n\t\t\tfor f in files:\n\t\t\t\tif not f.endswith(\".py\") and not f.startswith('__'):\n\t\t\t\t\treturn True\n\n\treturn False\n\ndef get_file_extension(file_type):\n\t\"\"\"\n\tGet file extension for given file extension name and content type.\n\t\"\"\"\n\text_name = None\n\tif file_type is None:\n\t\treturn ''\n\telif file_type not in IMAGE_EXTENSION:\n\t\tfor ext in EXTENSIONS_LIST:\n\t\t\tif file_extension"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif(file_name is None) or(file_name == \"\"):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn file_name\n\ndef is_dir_path(file_name, dir_name, exclude_non_existing=True):\n\t\"\"\"\n\tCheck if dir path exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param dir_name: Name of directory to check (including parent)\n\t:type dir_name: str\n\t:param exclude_non_existing: True to exclude non existing directory"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\ndef create_json_file(file_name):\n\t\"\"\"\n\tCreate a file if it does not exists.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: None\n\t\"\"\"\n\tif not is_file_exist(file_name):\n\t\tfile_name = file_name[0:3] + '-' + file_name[-4:-1] +"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\n\treturn os.path.isfile(file_name)\n\ndef read_file(file_name):\n\t\"\"\"\n\tRead file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t\"\"\"\n\ttry:\n\t\thandle=open(file_name,'r')\n\texcept IOError:\n\t\treturn 0\t\t\t\n\tlines=handle.readlines()\n\thandle.close()\n\tfor line in lines:\n\t\tline=line.strip()\n\t\tif line:\n\t\t\treturn line.strip('\\r\\n').split('\\"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\treturn os.path.isfile(file_name)\n\t# TODO: Error handling - Not yet implemented and not possible.\n\t#else: return os.path.isfile(file_name + \".txt\")\n\t\ndef get_file_list(directory):\n\t\"\"\"\n\tGet the list of files in the directory provided and return them as a list.\n\t:param directory: The directory where the file is downloaded.\n\t:type directory: str\n\t\"\"\"\n\t# print \"Retrieving list of files in directory '%s'\"%directory\n\tlog(\"Retrieving list"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself.__logger = logging.getLogger(__name__)\n\n\t\tself.__logger.setLevel(logging.DEBUG) # Default logslevel\n\t\tself.__logger.debug(\"Logger initialized\")\n\n\n\t@property\n\tdef logger(self):\n\t\t\"\"\"\n\t\tGet the logger\n\t\t\"\"\"\n\t\treturn self.__logger\n\n\t@logger.setter\n\tdef logger(self, logger):\n\t\t\"\"\"\n\t\tInitialize logger\n\t\t\"\"\"\n\t\tself.__logger = logger\n\n\n\t@property\n\tdef logger_name(self"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\t\n\t# Reset logging to stdout.\n\t# Note: This will delete all existing log files.\n\tlogging.basicConfig(level=logging.INFO, filename=LOGFILE_NAME, format='%(asctime)s [%(levelname)-8s] - %(name)s - %(funcName)s :: %(message)s')\n\t\n\t# Reset logging to a file.\n\tfilename = os.path.join(LOGFILE_DIR, LOGFILE_NAME)\n\tif os.path.isfile(filename):\n\t\tos.remove(filename)\n\t\n\t# Reset logging to stderr.\n\t#"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\ndef start():\n\n\t\"\"\"\n\tStart the main application\n\t\"\"\"\n\t# start the app\n\tdebug = bool(get_debug())\n\n\tif debug:\n\n\t\tdef __loggerName():\n\t\t\treturn \"{}-{}\".format(\"SteamLogger\", \"pyTutu-{}.py\".format(os.getpid()))\n\n\t\t# register a new logger\n\t\tclass SmartDashboard(object):\n\n\t\t\t_logger = __loggerName()\n\n\t\t\t@staticmethod\n\t\t\tdef get_logger():\n\t\t\t\tif not hasattr(SmartDashboard, \"_logger\"):\n\t\t\t\t\tSmartDashboard._logger = logging\n\t\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t # Resets logging after each test.\n\n\t\tos.environ['PYTHONUNBUFFERED'] = '1'\n\t\tfor key, handler in logging._handlers.items():\n\t\t\thandler.clear()\n\n\tdef _restore_logging():\n\t\timport logging\n\t\tlogger = logging.getLogger()\n\n\t\tif not isinstance(logger, logging.NullHandler):\n\t\t\treturn\n\n\t\tfor handler in logger.handlers:\n\t\t\tlogger.removeHandler(handler)\n\n\t\tlogger.setLevel(logging.INFO)\n\t\tlogger.addHandler(logging.NullHandler())\n\t\tlogger.propagate = False\n\n\tdef _"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\tlogging.basicConfig(\n\t\tlevel=getattr(logging, getattr(__name__, \"LOGGER_LEVEL\", \"INFO\")),\n\t\tformat=\"%(asctime)s : %(levelname)s : %(message)s\",\n\t\tdatefmt=\"%Y-%m-%d %H:%M:%S\"\n\t)\n\n# ================================\n# Functions\n\ndef _log(message):\n\t\"\"\"\n\tLog the message to the application logger\n\t\"\"\"\n\t\n\n\tif not isinstance(message, str):\n\t\tmessage = message.decode()\n\n\tlog_file = cls.LOG_FILE\n\n\tif"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\nclass _logger(object):\n\t\"\"\"\n\tSimple logger (abstract). If you create and set up your logger and\n\tinherit from the parent logger, remember to not call the instance's\n\tlogger() method.\n\t\"\"\"\n\n\tdef __init__(self, name=\"Logger Instance\", output=None, level=DEBUG):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tself.level = level\n\t\tself.file_log = None\n\t\tself.logger = None\n\t\tself.name = name\n\t\tself.debuglog = output\n\n\n\tdef __call__(self, msg, *args"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\tcls._log_level = 1\n\tcls.reset()\n\tif not cls.isAvailable():\n\t\treturn\n\tcls.setupLogging()\n\treturn\n\ndef _init_log(cls, logfile=None):\n\t\"\"\"\n\tInit\n\t:param logfile:\n\t:return:\n\t\"\"\"\n\tlog_name = _LOG_PREFIX + log_prefix.format(cls.name)\n\tif logfile is not None:\n\t\tfilename = os.path.join(logfile, log_name)\n\telse:\n\t\tfilename = log_name\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\nfrom __future__ import absolute_import\n# Copyright (c) 2010-2015 openpyxl\n\n\"\"\"\nWrite a set of XLSX-Compatible and friends to xml which can be used to populate the\ndatabase as an XLSX graph, when we make changes to the current version\nthe application will not be able to display this data in XML format\n\n\"\"\"\nfrom openpyxl.xmlns.reader import H2OReader, H2OElement, XMLReader\nfrom openpyxl.reader.workbook import open_workbook\nfrom openpyxl.reader.strings import StringReader\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\ndef _reset_handlers(module):\n\t\"\"\"\n\tReset all handlers, in order to be able to reset them later\n\t\"\"\"\n\t\n\tfor handler in module._handlers.values():\n\t\tif handler is not None:\n#\t\t\tlogging.error('Trying to reset handler: %s', type(handler))\n\t\t\thandler._reset()\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport markupfield.fields\nfrom django.conf import settings\nimport markupfield.fields.autoslugfield\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\ndef _getOptions():\n\toptParser = optparse.OptionParser()\n\n\toptParser.add_option('-v', '--verbosity', \n\t\taction=\"store\", type='int', dest='verbose', default=1)\n\n\toptParser.add_option('-d', '--debug',\n\t\taction=\"store_true\", dest='verbose', default=False)\n\n\toptParser.add_option('-o', '--output',\n\t\taction=\"store\", type='string', dest='outFile')\n\n\treturn optParser.parse_args()\n\ndef main():   \n\tprint _get"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n\n\n\n\t\n\t# The \"Base\" type in use for documentation only.\n\t# \n\tdef initMethod(self, target_class, *args, **kwparams):\n\t\t\"\"\"\n\t\tIn order to get further functionality in CamelCase,\n\t\twe need to do some additional checking and caching.\n\t\t\"\"\"\n\t\t# Get the base class and the name of the class to build\n\t\ttarget_class_init = getBaseClass(target_class)\n\t\tlogger.info(\"Building function '%s'\" % target_class)\n\t\ttarget_class = __import__(target_class"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\ndef _getTargetClassDerivation(superClass, method):\n\tbaseClass = superClass\n\tif hasattr(baseClass, 'className'):\n\t\tbaseClass = getattr(baseClass, 'className')\n\tif hasattr(method, 'im_func'):\n\t\tmethod = getattr(method, 'im_func')\n\tif hasattr(method, 'im_class'):\n\t\tcls = method.im_class\n\telif hasattr(method, 'im_self'):\n\t\t#cls = method.im_self.__class__ or method.__class__\n\t\t#if"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\nclass PythonObject(PyObject):\n\tdef __init__(self, name, type, doc, argtypes, docid):\n\t\tPyObject.__init__(self, name, doc)\n\t\tself.name = name\n\t\tself.type = type\n\t\tself.argtypes = argtypes\n\t\tself.doc = doc\n\t\tself.docid = docid\n\n\tdef __repr__(self):\n\t\treturn '%s( %s )' % (self.type, self.name)\n\n\tdef _writeXML(self, printer, x, use, name):\n\t\tif use == 'py':"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t    \n\t    \n\tdef _getTargetClass_pythran(self):\n\t\"\"\"\n\tDefine to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t    \n\tdef _getTargetClass_pyothran(self):\n\t\"\"\"\n\tDefine to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t    \n\tdef _getTargetClass_pythran_forced(self):\n\t\"\"\"\n\tSpecial case for targets whos forced their results\n\t\"\"\"\n\t        "}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n\n\treturn Configuration\n\nclass Configuration(object):\n\t\"\"\"\n\tBase class for different configuration\n\t\"\"\"\n\n\tdef __init__(self, defaults = {}):\n\t\t\"\"\"\n\t\t@param defaults: overrides default values\n\t\t\"\"\"\n\t\tself.__defaults = defaults\n\t\tif self.__defaults is not None:\n\t\t\tself.__dict__.update(self.__defaults)\n\n\tdef getDefaults(self):\n\t\t\"\"\"\n\t\t@return: dict of defaults\n\t\t\"\"\"\n\t\treturn self.__defaults\n\n\tdef __getattr__(self, attr):\n\t\t\"\"\"\n\t\tGet key value from the"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n       #return self.getTargetClass()      \n\n    def add_arguments(self, function):\n        \"\"\"\n\tCall the function with any arguments.\n\tThe function should accept a dictionary of arguments.\n\n\tNote that values which are dictionaries can be\n\tconverted to strings, but they can either be strings\n\twhich will be converted to objects where applicable\n\tto the arguments.\n       \n\tEspecially useful in situations where the value\n\tis meant to be lists or dictionaries.\n\n        Arguments\n        -----------\n\tfunction\n\t\t\tA dictionary of key,value pairs.\n\n\t\t- if the function is a Dictionary"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n\n\nclass ClassLoader(object):\n\t\"\"\"\n\tBase class to handle all import plugins.\n\n\t@ivar plugin: The plugin's class object.\n\t@ivar _parentPath: The parent of this instance of the plugin's\n\tattribute.\n\t\"\"\"\n\n\n\tdef __init__(self, plugin, parentPath):\n\t\t\"\"\"\n\t\tInitializes this class.\n\n\t\t@param plugin: The plugin's class object.\n\t\t@type plugin: L{Plugin}\n\t\t@param parentPath: The parent of this instance of the plugin's\n\t\tattribute.\n\t\t@type parentPath:"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n\tdef getCurrentVersion(self):\n\t\t\"\"\"\n\t\tReturn the version of python that the extension's\n\t\timplementation is using.\n\t\t\"\"\"\n\t\treturn VERSION\n\tdef getModuleName(self):\n\t\t\"\"\"\n\t\tReturn the module name of the extension module.\n\t\t\"\"\"\n\t\treturn self.configSection.split()[0]\n\t\t\n\tdef getModuleDisplayName(self):\n\t\t\"\"\"\n\t\tReturn the module display name of the extension module.\n\t\t\"\"\"\n\t\treturn self.getInfoElement(\"DisplayName\")[0]\n\nif __name__ == \"__main__\":\n\timport sys"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\nclass TargetClass(Target):\n\t\"\"\"\n\tThe 'TargetClass' is a base class of all compiler types.\n\t\"\"\"\n\t\n\tdef __init__(self, engine, addr):\n\t\t\"\"\"\n\t\t@param engine: The engine the implementation shall use, this \n\t\tclass implements.\n\t\t\n\t\t@param addr: A native address/port\n\t\t\"\"\"\n\t\tobject.__init__(self, engine.getAddress())\n\t\tself.engine = engine\n\t\tself.addr = addr\n\t\t\n\t\tglobal DEFAULT_TARGETS\n\t\tDEFAULT_TARGETS = []\n\t\t\n\t\tself.defaultTargets = engine.getDefault"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n# \t\treturn _self.parent.parent().id() + \".\" + self.__name__\n# \t\n#\tdef isAvailable(self):\n#\t\t\"\"\"\n#\t\tDecide whether the class can be called\n#\n#\t\t@return True if it can be passed to the interpreter\n#\t\t\"\"\"\n#\t\treturn self.__class__.__name__ in self.names()\n#\n#\t\n#\tdef isAvailableIn(self, name):\n#\t\t\"\"\"\n#\t\tDoes this class exist in this class's dependencies.\n#\n"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\t# the 'x' suffix is used in the 'q' suffix\n\tsuffix = ''\n\t# for instance -- the first element is 'q'\n\tfor orderNum, orderValue in enumerate(orderings):\n\t    if orderValue!= 1:\n\t\tsuffix += orderValue\n\t    else:\n\t\tsuffix += '-'\n\t\t\n\treturn suffix\n\ndef mergeByPrefix(sourceAndSuffix):\n\t\"\"\"\n\tMerge all source files which start with the string specified in its\n\tsuffix with a given prefix.\n\t\n\tIf a source file has the same prefix that the merge will be done to\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\n\nfrom.utils import (\n\tget\n)\n\nclass Chain(list):\n\t\"\"\"\n\tA chain is an immutable list.\n\t\n\tA chain has two keys:\n\t\n\t* The common prefix of the key in front and the common suffix in the\n\t   common key in back.\n\t\n\tThe values are tuples sorted by the key (if they are unique), and each \n\ttuple indicates the new chain.\n\t\n\tNote that the chain may have more arguments than the number of\n\tchain items (numbered 0, 1, 1, 2,...) which indicate the number of\n\tnew items in"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\ts = ['']\n\t\n\tfor order in orderings:\n\t\tfor key,value in order.items():\n\t\t\tif type(value)==tuple and len(value)>1:\n\t\t\t\ts.append(\"%02X\" % value[0])\n\t\t\ts.append(\"%02X\" % value)\n\t\n\treturn s\n\ndef _addPrefix(prefixStr, obj):\n\t\"\"\"\n\tAdd a prefix to an object.\n\t\n\tGiven a multi-leading list of prefixes, add that prefix to the\n\tobject.\n\t\n\tBy default, an empty prefix is used.\n\t\n\tParameters:"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tnew_orderings = []\n\tfor ordered_order in orderings:\n\t\tnew_orderings += ordered_order.split('/')\n\t\n\treturn new_orderings\n\n\ndef _mergeNested(obj, other_objects):\n\t\"\"\"\n\tMerge the given Nested object as if they have been nested into one\n\tnested and merging with the values from the first object.\n\t\n\t\"\"\"\n\t\n\tfor obj_id in obj.id_list():\n\t\tobj_data = obj.get(obj_id, {})\n\t\t\n\t\tif '_index' in obj_data:\n\t\t\tobject_"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tnew_orderings = []\n\tfor order in orderings:\n\t\tif not order:\n\t\t\tcontinue\n\t\t\n\t\t# If current entry's suffix matches anything, then it\n\t\t# has to re-join it in front of it.\n\t\tprefixMatch = False\n\t\twhile True:\n\t\t\tprefixMatch = True\n\t\t\ttry:\n\t\t\t\torder[order.index(alias)]\n\t\t\texcept ValueError:\n\t\t\t\torder.append(order.index(alias))\n\t\t\t\n\t\t\tif prefixMatch:\n\t\t\t\t# Lastly, if prefix matches any string, reverse\n\t\t\t\torder.reverse()\n\t\t\t\tnew"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\n\tresult = list()\n\tif orderings:\n\t\tfor order in orderings.sort(lambda x, y: cmp(x['name'], y['name'])):\n\t\t\tfor xnew in order['orders']:\n\t\t\t\tif xnew['name'] == order['name']:\n\t\t\t\t\tresult.append(xnew)\n\t\t\t\t\tbreak\n\t\t\tresult.append(xnew)\n\treturn result\n#!/usr/bin/env python\n\nimport re\nfrom collections import deque\n\n\"\"\"\nA basic example of a thread-state machine:\n\nA state machine consists of a stack of states"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\n# -*- coding: utf-8 -*-\n\nimport os\nimport json\nfrom math import *\nimport cv2\nimport numpy as np\nimport argparse\n\n\"\"\"\n\"\"\"\ndef usage():\n\n    print(\"\\n\\n   <Image3D|Load images from a file>\")\n    sys.exit(1)\n\ndef detect_image_files(input_dir):\n\n    \"\"\"\n    \"\"\"\n\n    file_list = []\n\n    #load all images from input dir\n    for path, subdirs, files in os.walk('./inputs/'):\n        dir_list = [os.path.join(path,i"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\t# First for the base case.\n\treverse = {obj.order: 1 + obj.order for obj in orderings}\n\t\n\tneworderings = {}\n\tneworderings.update(orderings)\n\t\n\t# Now for the remainder case. This part could be made much more\n\t# elegantly.\n\t\n\treturn tuple([reverse.get(o, 0) for o in neworderings])\n\nfrom unittest import TestCase\nfrom mock import Mock\n\nfrom bogoogle.handlers import WebHandler\nfrom bogoogle.handlers import OAuth2Handler\nfrom"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\t\n\t# XXX this code was taken from epydoc:\n\t# a few ideas were already merged into this helper module,\n\t# but now it's safe to remove the last item completely,\n\t# so that is can be used if we need to.\n\tmergedOrderings = []\n\tfor ordering in orderings:\n\t\tif ordering:\n\t\t\tmergedOrderings.append(ordering)\n\torderedMergedOrderings = []\n\tfor item in mergedOrderings:\n\t\tif _is_reversed(item):\n\t\t\torderedMergedOrderings.append(reversed"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tresultOrderings = []\n\t\n\torderIndex = 0\n\tcurrentNode = orderings[orderIndex]\n\t\n\tskipSpaces = False\n\tif currentNode.childNodes:\n\t\tskipSpaces = True\n\t\tresultOrderings.append([])\n\telse:\n\t\tfor childNode in currentNode.childNodes:\n\t\t\tif skipSpaces and skipSpaces == True:\n\t\t\t\tcontinue\n\t\t\tif childNode.nodeType == childNode.ELEMENT_NODE:\n\t\t\t\tif childNode.namespaceURI == currentNode.namespaceURI and childNode.localName == 'order':\n\t\t\t\t\torderIndex += 1\n\t\t\t\telif childNode.nodeType == childNode.ATTRIBUTE"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\nclass IDeclaration(Interface):\n\t\"\"\"\n\t\"\"\"\n\t\n\tdef __init__(self, interface, declarations=None):\n\t\t\"\"\"\n\t\tCreate an object.\n\t\t\n\t\t:param interface: The object interface to make a copy from.\n\t\t:param declarations: A list of interface names that should be generated.\n\t\t\"\"\"\n\t\tsuper(IDeclaration, self).__init__()\n\t\t\n\t\tif declarations is None and len(declaration.declarations)==0:\n\t\t\treturn\n\t\t\n\t\tfor declaration in declarations:\n\t\t\tif isinstance(declaration.declarations[0], basestring):\n\t\t\t\tdeclaration = IDeclaration(interface,"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t\t\nclass IObject:\n\t\"\"\" An object that knows about our interfaces. An IObject is\n\tused to represent the definition of the interface and provides\n\tinteractively, in order to provide input/output of interfaces.\n\t\n\tI.e., implementing an `IInterface` might look like this::\n\t\t\n\t\tclass Prop(IObject):\n\t\t\tpass\n\t\n\t... But this can be implemented with classes which provide\n\ttheir attributes exposed through this interface, but also can\n\toverride these if they need to. When the class is instantiated, it\n\twill be called *by*"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ndef isProvidedByReference(obj): # pylint:disable=redefined-builtin\t\n\t\"\"\"\n\tReturn ``True`` if the given value is a proving instance reference\n\t\n\tThe value returned is an `~zope.interface.declarations.Declaration`.\n\t\"\"\"\n\treturn isinstance(obj, declarations.ProvidedBy)\n\ndef implements(theClass, interfaces): # pylint:disable=redefined-variable-type\n\t\"\"\"\n\tReturn a list of the interfaces defined in some objects\n\t\n\tThe given interfaces are returned as an :class:`zope.interface.declarations.Declaration`\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\n\nclass MethodDescriptor(object):\n\t\"\"\"\n\tThis descriptor provides information on an IDL method which can be\n\tdone at C level as far as I can tell. The interface definition is\n\teasiest for instance, as ``implements`` in the\n\tIRendezvous class hierarchy.\n\t\"\"\"\n\n\t__implements__ = (\n\t\tinterfaces.IDeclaration,\n\t\tinterfaces.INamespace,\n\t)\n\n\tdef __init__(self, name, parent, position):\n\t\t\"\"\"\n\t\tCreate a new IDefinition for the given name of the parent\n\t\tInterface"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\treturn _lookup(object)\n\ndef isProvidedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn `True` if an object was produced by its containing\n\tdeclarations defined in a `ProvidedBy` interface (or a regular `Interface`\n\tinterface).\n\t\"\"\"\n\tif not interface.providedBy(object):\n\t\treturn True\n\telse:\n\t\treturn _lookup(interface.providedBy(object))\n\ndef providedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn `True` if an object"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ngetInterfaces = providedBy\n\ndef findProvidedBy(object, interface=None): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\n\tif interface is None:\n\t\tinterface = getInterface()\n\t\t\n\tprovides = set()\n\n\tdef get(object):\n\t\tif not interface:\n\t\t\treturn provides\n\t\tif interfaces.providedBy(object, interface):\n\t\t\treturn provides\n\t\treturn set()\n\t\n\treturn get(object)\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t\t\n\t# this will become a constant\n\tif interface is None: \n\t\traise TypeError(\"no interface in %s\" % \\\n\t\t\t\t\t\t\t\trepr(interface))\n\t\t\t\n\tif callable(function):\n\t\tinterface = interface()\n\telse:\n\t\tinterface = apply(interface, function)\n\t\n\t\n\t# return the interface\n\treturn IDeclaration(interface)\n\n\n#=======================================================================\n#\timplements the object\n#=======================================================================\ndef implements(object, interface):\n\tassert inspect.ismethod(object) or inspect.isfunction(object)\n\tif type(interface) is int or type("}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t\n\t# TODO: make interfaces optional?\n\t# (easier to return just an interface if it has no dependencies, the \"optional\"\n\t# field will default to True).\n\timport zope.interface\n\treturn zope.interface.providedBy(object)\n\n\ndef implementedBy(object):\n\t\"\"\"\n\tReturn the interfaces provided by the given object\n\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`::\n\t\n\t   # The interfaces directly provided by this method.\n\t\"\"\"\n\tif zope.interface.providedBy(object):\n\t\treturn object"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ndef classImplements(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces implemented by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ndef implementedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces implemented by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ndef exportedTypes():\n\t\"\"\"\n\tReturn the exported object types\n\t\n\tThe type returned is an `~zope.interface."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\tpass\n\n\ndef exposedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces of a declared interface which have been marked as\n\texposed\n\t\n\tAccessing a named argument on an Interface's class is not allowed.\n\t\"\"\"\n\t\tpass\n\n\nclass IElementDeclaration(ZopeInterfaceClass): # pylint:disable=invalid-name\n\t\"\"\"\n\tInterface declaration in Zope interface descriptions.\n\t\n\tThe specified interface is exposed for all classes it is decorated (but the\n\tresult is cached and is not invalidated).\n\t\"\"\"\n\t\tpass\n\n\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\tdef _baseMinus(baseClasses):\n\t\t\t# Impl: I wanted to be able to compute all base classes, and not just\n\t\t\t# to have a base class for them, so we do it in order and return\n\t\t\treturn [(bcls(), c + 1) for bcls, c in baseClasses for c in classes]\n\t\treturn _baseMinus\n\n\t# return all the base classes sorted by priority\n\tbaseClasses = sorted(classes, key = lambda ci : ci[1])\n\tminPriority = [[(bcls(), c), c + 1] for bcls"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\t\t\n\t        # the list of base classes sorted starting with 'baseClasses'\n\t        baseClasses = sorted(classes, key=lambda c: len(c.name))\n\n\t        # now the list of base classes sorted from lowest to highest\n\t        # (so we don't need to check in reverse)\n        minorityClass = baseClasses[0]\n\t \n\t        # sort base classes list of base classes in increasing order\n\t        minorityClassSort = list(minorityClass.ordered_by.sort())\n\n\t        # compare the list of base classes in reverse order\n\t        best = 0\n\t        bestBaseClass"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n#\tmin_n_m_i = []\n#\tmin_m_i = []\n#\tfor a,b,c in classes:\n#\t\tif a == \"__main__\": continue\n \t\tif not a == \"__init__\": continue\n \t\tif not b == \"__init__\": continue\n\tfor index in range(min(len(classes), 10), 10):\n\t\tfor itr in range(min(lens(classes)), 10):\n\t\t\tmin_m = max(min_m, float(itr-index)/(len(classes)-i))\n#\t"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\n\tdef getMinusUniqueBases (classes):\n\t\tminus  = None\n\n\t\tfor klass in classes:\n\t\t\tbaseClasses = klass.mro()\n\t\t\t\n\t\t\tif minus is None:\n\t\t\t\tminus = baseClasses\n\t\t\telse:\n\t\t\t\tminus = minus.mro()\n\t\n\t\t\tfor baseClass in minus:\n\t\t\t\tyield baseClass\n\t\t\t\t\n\tdef uniqueBases(classes):\n\t\t\"\"\"\n\t\tReduce a list of base classes to its ordered unique equivalent\n\n\t\t:param classes: The classes to reduce.\n\t\t:type  classes: set\n\t\t:"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n##\n# Function: getConsolidated(graph) that returns nonempty\n# of the same type if both have been defined in the function\n#\n\n\ndef getConsolidated(graph):\n\tmerged = collections.defaultdict(set)\n\tfor n in graph.nodes:\n\t\tfor c in graph.neighbors(n):\n\t\t\tmerged[c].add(n)\n\tmerged = [(n, c) \n\t\t\t \t  for n, c in merged.iteritems()\t\t  if n in merged]\n\tmerged_with_nodes = collections.defaultdict(list)\n\tfor"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\t# sort base classes and make sure there is a maximum length\n\t\tmaxBases = []\n\t\tfor baseClass in classes:\n\t\t\tif len(baseClass)  < baseIdent:\n\t\t\t\t\tbaseClass = baseClass.__bases__[0]\n\t\t\t\t\tmaxBases.append(baseClass)\n\t\tminBases = []\n\t\tfor baseID in range(len(bases)):\n\t\t\tmaxBases.append(minBases[baseID] + 1)\n\t\treturn maxBases[:len(maxBases)-1]\n\ndef extractMinCover(listOfClasses,"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n#\treduce = lambda x, y: x + y\n\n\tfor aClass in classes:\n\t\tmaxBases = aClass.minSupport\n\n\treturn max(maxBases, key=reduce)\n\ndef minimalSets(sets):\n\t\"\"\"\n\tReduce a list of set classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n#\treduce = lambda x, y: x + y\n\n\tfor aSet in sets:\n\t\tminSupport = aSet[0]\n\t\tmaxSupport = aSet[1]\n\t\tif(reduce(maxSupport, minSupport)):\n\t\t\t"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\ndef uniqueBases(bases):\n\t\"\"\"\n\treduce a list of base classes to a single class\n\n\tThis may be overkill because we have two distinct bases, and one of the\n\tbase classes is probably not identical. In case of any of this, duplicates,\n\tthe more general approach is repeated and the general thing is that the\n\tlowest one can just be tried.\n\n\tFor example, we are in the following baseclasses:\n\t\t1. If the first base is an instance of something that has the most\n\t\tclass in common, its parent should be an instance of something"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\n\ti  = []\n\tbases = {}\n\tfor c in classes:\n\t\tc_i = bases[c] = []\n\t\ti.append(c_i)\n\t\tbases[c] = c_i\n\n\tfor c in classes:\n\t\tif len(c_i) > 1:\n\t\t\tfor x in c_i:\n\t\t\t\t# first check that no base classes have the same length\n\t\t\t\tif x in bases[c]:\n\t\t\t\t\tbases[c].append(x)\n\n\n\tb_i  = {}\n\tbases = {}\n\tB = {}"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\n\tnewBases = list()\n\tfor base in classes:\n\t\tif base not in newBases:\n\t\t\tnewBases.append(base)\n\tassert len(newBases) <= 2\n\tfinalMinimizer = None\n\tfor base in newBases:\n\t\tif base not in finalMinimizer:\n\t\t\tfinalMinimizer = base\n\tassert len(finalMinimizer) == 2\n\n\tif finalMinimizer[1]!= 1:\n\t\traise ValueError('%d is not a singleton of %s' % (finalMinimizer["}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t #  Do not consider self-describing interfaces like Interface.\n\t if not self._xmlreader.IsInterface(Namespaces.XMLNS_IFACE) or \\\n\t\t   not self._xmlreader.IsInterface(Namespaces.XMLNS_IFACE + \"complexType\"): # pylint:disable=protected-access\n\t\t\treturn None\n\n\t xmlNsNsL = []\n\t if all:\n\t\t\tfor xsd in self._xmlreader.GetAllAttributeNames():\n\t\t\t\txmlNsNsL.append(xsd)\n\t else:\n\t\t\txmlNsNsL.append(XSNamespaces._namesDict["}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t # We first need to figure this out, so self.fieldNames and self.fieldDescs\n\t\t# are synced between instances\n\t\tinfoDict = self\n\t\t# Now go through the interfaces\n\t\twhile infoDict.has_key('fieldDesc') and infoDict['fieldDesc']:\n\t\t\tname = infoDict['fieldDesc']\n\t\t\tinfo = infoDict['desc']\n\t\t\tif all or name not in self.fieldNamesAndDescriptions:\n\t\t\t\tself.fieldNamesAndDescriptions[name] = []\n\t\t\tself.fieldNamesAndDescriptions[name].append(info)\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t # If 'all', only return names and descriptions, even if 'names' is\n\t # requested, or only return names and descriptions specified\n\t # in 'all' setting.\n\t if all:\n            return self._namesAndDescriptionsAll()\n\t\t# Else, return all the attributes of all interfaces\n\t\treturn [attr for attr in self._attributes(self._interface)]\n\n\tdef namesAndDescriptionsAll(\n\t\tself, all=False, showDevices=False, showDevicesInfo=False):\n\t\tinterfaceNames = self._namesAndDescriptions(\n\t\t\tall=all and self._isAt"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tret = {}\n\t\tfor obj in self:\n\t\t\tret[obj.name] = obj\n\t\t\tif all: ret[obj.name.upper()] = _all_attrs(obj)\n\t\treturn ret\n\n\tdef isNonCallbackInterface(self):\n\t\t\"\"\"\n\t\tThe return value does not matter.\n\t\t\"\"\"\n\t\t# This attribute is either False or None, when it is not in\n\t\t# a callback interface.\n\t\treturn self.isInterface() and self.getExtendedAttribute(\"NonCallbackInterface\")\n\n\tdef isCallbackInterface(self):\n\t\t\"\"\"\n\t\tThe"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\n\t\t\n\t\tif all:\n\t\t\treturn self.interface.iface_cache.get(self.interface.iface_name, \"\").items()\n\n\t\treturn self.interface.iface_cache.get(self.interface.iface_name, {}).items()\n\t\n\tdef names(self):\n\t\t\"\"\"\n\t\tReturn attribute names.\n\t\t\"\"\"\n\t\t\n\t\tif self.interface.__name__ == \"InterfaceObject\":\n\t\t\treturn interfaceObjectNames(self, False)\n\t\telif self.interface.__name__ == \"InterfaceMethod\":\n\t\t\treturn interfaceMethodNames(self, False)\n\t\telif self.interface"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\n\n\tnamesAndDescriptions = []\n\tfor m in self.names():\n\t\tfor i in m().namesAndDescriptions:\n\t\t\tnamesAndDescriptions.append( self.mapName( i if not i.optional else None ) )\n\n\tparams = self\n\tparams.names = []\n\tparams.descriptions = {}\n\tfor n in namesAndDescriptions:\n\t\tparams.names.append( n.name )\n\t\tparams.descriptions[n.name] = _escape_HTML(n.description)\n\n\tif len(self): # pylint:disable=protected-"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tres = libxml2mod.xmlTextReaderReg(self._o, None, self._oType)\n\t\tif res is None:\n\t\t\tself._o = libxml2mod.xmlCreateTextReader(None, self._oType)\n\t\t\tself.doc = None\n\t\telse:\n\t\t\tself._o = res\n\t#\n\t# utility routines used in text constructors\n\t#\n\n\tdef checkNamespacesAvailable(self, namespace):\n\t\t\"\"\"\n\t\tCheck that the namespaces have not been registered with the\n\t\txmlTextReader object associated with the given namespace.  If\n\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\n#-------------------------------------------------------------------------------\n\n__pluginInfo__ = {\n      'name': 'NetCDF Reader',\n     'version': '1.0',\n      'description': 'An interface to read NetCDF files.',\n      'license': 'Apache-2',\n      'arguments': [],\n      'arguments/remarks': {\n          'type' : 'csv',\n         'metavar' : '<comma separated fields>',\n          'help'    : 'Determines the format the NetCDF file is in.',\n          'default' : 'NetCDF_r.nc',\n      },\n      'arguments/remarks/info': {\n          'nargs"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tnames = []\n\t\tdescriptions = _collect(self.config.getDescriptors(), self.descriptors,\n\t\t\tself.interface, self.extendedAttributes, descriptors=self.descriptors,\n\t\t\tdictionaries=self.dictionaries)\n\t\tfor key in names:\n\t\t\tval = _collect(self.get(key), self.descriptions,\n\t\t\t\t\tdictionaries=self.dictionaries)\n\t\t\tdescriptors = val.descriptors\n\t\t\tif all or descriptors is not None:\n\t\t\t\tnames.extend(descriptors[:])\n\t\t\t\tdescriptions.extend(descriptions[:])\n\t\treturn dict(list(zip("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\treturn [(name, getattr(interface, name, Null)) for name in self.keys()]\n\n\tdef _addKey(self, key, value):\n\t\tassert(0 <= key < (1<<32), \"Key must be in range [0,1<<32].\")\n\t\tif isinstance(value, bool):\n\t\t\tvalue = \"true\" if value else \"false\"\n\t\tself.keys().append((key, value))\n\n\t@staticmethod\n\tdef _sortKeys(mapping):\n\t\tr = []\n\t\tfor item in mapping:\n\t\t\tif(hasattr(item, \"__"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t return self.__attributes__\n\n\tdef attributes(self): # pylint:disable=redefined-builtin\n\t \"\"\"Return the attribute dictionary for the interface.\"\"\"\n\t return self.__attributes__\n\n\tdef add_interface(self, ifname):\n\t\t\"\"\"Append a new interface to the library.\"\"\"\n\t\tififname = ifname.lower()\n\t\tif self.__dict__.has_key(ifname):\n\t\t\treturn self.__dict__[ifname]\n\t\tself.interfaces.append(ifname)\n\t\tself.ifname = ifname\n\nclass _Interface:\n\t\"\"\"\n\tThe interface class.\n\t\"\"\"\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn list(self.__dict__.keys())\n\tdef getDict(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn a dictionary of all the attribute names defined in the interface.\n\t\t\"\"\"\n\t\treturn self.__dict__.keys()\n\tdef getParam(self, name): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif name not in self.__dict__:\n\t\t\traise NameError(\"no value is defined for this name.\")\n\t\treturn self.__dict__[name]\n\tdef getProperties(self): # pylint:disable=redefined-builtin"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\n\n\treturn self.attr._names\n\n @names.setter\n\tdef names(self, val):\n\t\tself.attr._names = val\n\t\n\tdef addAttr(self, name, value):\n\t\t\"\"\"\n\t\tAdd a attribute name, which is a string and the attribute value\n\t\tis a list of values.  Each value will be of form\n\t\tthe string `attr_name=value`\n\t\t\n\t\tIf we know that attributes may have been set in __init__, then\n\t\twe must also add a 'default' attribute\n\n\t\tA few attributes are just added that correspond to a name,"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\tret = []\n\t\tnames = dir(self)\n\t\tfor name in names:\n\t\t\tif not callable(getattr(self, name, None)):\n\t\t\t\tcontinue\n\t\t\tret.append(name)\n\t\treturn ret\n\n# ------------------------------------------------------------------------------\n# ------------------------------------------------------------------------------\n\nclass _FunctionProxy(FunctionProxy):\n\t\"\"\"\n\tProxy to the function registered in the\n\t:mod:`yapsy.PluginManager` singleton.\n\t\"\"\"\n\n\tdef __init__(self, name, signature, return_type=None, static_name=None, static_doc=None,\n\t\t\t\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t # Set up the attributes\n\t f = self._obj_.Proxy.AttributeCount\n\t l = self._obj_.Proxy.AttributeNames\n\t if all:\n\t\t r = _collect_list(self._obj_, self._root_, f, byref(l))\n\t\t return set(l)\n\t else:\n\t\t return f.value()\n\n\tdef show(self): # pylint: disable=redefined-variable\n\t \"\"\"\n\t\tDisplay the attribute names defined by the interface.\n\t\t\"\"\"\n\t return self._root_.Show()\n\n\tdef set(self, num, names):\n\t\t\"\"\"\n\t\tSet the attribute names defined"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\tattr_names = []\n\t\tif self.is_enum():\n\t\t\tattr_names = self.enums.keys()\n\t\telif self.is_interface():\n\t\t\tfor name in self._attributes:\n\t\t\t\tsub_names = name.rsplit('.', 1)\n\t\t\t\t# Ignore the'self', aka the 'interface', but\n\t\t\t\t# we treat'self' for the purpose that there isn't yet.\n\t\t\t\tif sub_names[0] =='self':\n\t\t\t\t\tsub_names = sub_names[1:][:-1]\n\t\t\t\t\tattr_names = sub_names + ['"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\n#\traise AttributeError, \\\n#\tTypeError, \\\n#\tAttributeError, \\\n#\tKeyError, \\\n#\tValueError, # See http://docs.python.org/reference/datamodel.html\n#\t...\n#\t>>> # This function uses these two names, but the ones you\n#\t... want to access are not just read from.\n\t#\t>>> a = 3\n#\t... \n#\t>>> print names(a)\n#\ta.name\n#\t>>> print names(a.getNames())\n#\t['a.Name']\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\tret = []\n\n\t\tfor i in self.interface:\n\t\t\tif i.is_attr_set(self):\n\t\t\t\tret.append(i.name)\n\t\treturn ret\n\n\tdef num_names(self):\n\t\t\"\"\"\n\t\tReturn the number of attributes defined by the interface.\n\t\t\"\"\"\n\t\treturn len(self.attributes)\n\n\tdef get_attr_names(self):\n\t\t\"\"\"\n\t\tReturn the attribute names defined by the interface.\n\n\t\tIf there isn't any attribute names, an empty list is returned.\n\t\t\"\"\"\n\t\treturn list(self.attributes"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn self._attrnames\n\n\tdef names_methods(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the attribute names and methods defined by the interface\n\t\"\"\"\n\t\treturn self._attrnames_methods\n\n\tdef has(self, name):\n\t\t\"\"\"\n\t\tReturn true if a method with 'name' attribute is defined. Else false\n\n\t\tIn case of basic attribute, always True:\n\n\t\t>>> s = M(\"foo\")\n\t\t>>> s.has(\"foo\")\n\t\tTrue\n\n\t\tIn case of compound attribute, always False if this interface"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn list(self.attrlist)\n\n\tdef __iter__(self):\n\t\treturn iter(self.attrlist)\n\n\tdef __missing__(self, what):\n\t\t\"\"\"\n\t\tRaise a KeyError as the attribute does not exist.\n\t\t\"\"\"\n\t\traise KeyError(\"You should provide an attribute name\")\n\n\tdef __getitem__(self, what):\n\t\titem = getattr(self.attrlist, what, None)\n\t\tif item is not None:\n\t\t\treturn item\n\t\traise KeyError(what)\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a more el"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif hasattr(sequence, \"_fields\"):\n\t\tsequence = sequence._fields\n\t\n\tif isinstance(sequence, declarations.Declarator):\n\t\treturn sequence.declarations\n\tif isinstance(sequence, int):\n\t\treturn sequence if output is None else output\n\t\n\tif hasattr(sequence, \"__iter__\"):\n\t\treturn list(map(_normalizeargs, sequence))\n\t#else:\n\t#\treturn list(sequence)\n\n\nclass StructDeclaration(object):\n\t\"\"\"Structured declaration definition\n\t\n\tStruct declaration is essentially a type declaration in Python.\n\t\n\tThe types define type declarations with a name,"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif input is None:\n\t\treturn _Sequence(sequence)\n\t\n\ttry:\n\t\tinput = input.__name__.strip(' \\t\\n\\r')\n\texcept Exception:\n\t\tpass\n\t\n\tif isinstance(input, _Sequence):\n\t\toutput = []\n\t\tfor base, ancestry in input:\n\t\t\tif not isinstance(ancestry, Declarions) or not ancestry.is_error:\n\t\t\t\tif base is None:\n\t\t\t\t\toutput.append((base, ancestry))\n\t\t\t\telse:\n\t\t\t\t\tif ancestry.parent == base: \n\t\t\t\t\t\toutput"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif output is None:\n\t\toutput = []\n\t\t\n\treturn _normalizeargs(sequence, output)\n\nclass Declarions:\n\t\"\"\"\n\tDeclarions used in the generation of decles.\n\t\n\tThe generator methods of ``Declarions`` will be applied to\n\tdeclarations and declarations of declarations, for these declarations\n\tuse ``del x`` or ``x += len(decls)`` - this declares a maximum\n\tnumber of declarations (which doesn't work when declares are added).\n\t\n\tThis generator is useful to be able to provide an iterable"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\t# normalize input to proper args format\n\tif output in ('list','tuple','listOf','dict','set','frozenset'):\n\t\treturn list\n\telif output is not 'dict':\n\t\treturn tuple\n\t\n\t# normalize output\n\tif output =='string':\n\t\treturn str\t\n\telif output =='string':\n\t\treturn unicode\t\n\telse:\n\t\traise Exception('Not implemented')\n\n\ndef _normalizedict(*args):\n\t\"\"\"\n\tNormalize dictionary.\n\t\n\tReturns a new dict object representing the input arguments.\n\t\"\"\"\n\t\n\tif hasattr(args, '"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif output == None: output = []\n\t\n\tfor interface in sequence:\n\t\tinterface, out = interface\n\t\tif out.find('operator(') > 0 or interface[0] == '(': # no parens\n\t\t\tout = out.strip()\n\t\telif out.find('[') > 0 or interface[0] == '[':\n\t\t\tout = _splitargs(out)\n\t\t\toutput.append(out)\n\t\t\tpass\t# strip interface name\n\t\telif interface.find(':') > 0:\n\t\t\tout = _stripargs(interface[1:])\n\t\t\toutput += list(out)"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif output is None:\n\t\toutput = ''\n\telif isinstance(output, str):\n\t\tif hasattr(sequence, '__iter__'):\n\t\t\toutput = ''.join(sequence)\n\t\telse:\n\t\t\toutput = [output.strip()]\n\t\n\t# Handle optional components.\n\tresult = ''\n\tfor arg in sequence:\n\t\tif isinstance(arg, tuple):\n\t\t\t# Handle the special case of list.\n\t\t\tif len(arg) == 2:\n\t\t\t\treturn result + arg[0] + ':'+ arg[1] \n\t\t\telse:\n\t\t\t\treturn result + arg"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif sequence == ():\n\t\treturn []\n\t\n\tif not isinstance(sequence[0], tuple):\n\t\traise TypeError, \"Sequence element (list of lists) got %r\" % sequence\n\t\n\tif isinstance(sequence[1], list):\n\t\t# TODO: This should be more efficient by using a list here.\n\t\tsequence = list(sequence)\n\tif output:\n\t\treturn sequence[0] + (output,)\n\telse:\n\t\treturn [sequence]\n\t\n\n#===================================================================================================\n# Functions\n#===================================================================================================\n\n# A constant of two parameters.\n#\n# A"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(sequence, list):\n\t\tresult = []\n\t\tfor seq in sequence:\n\t\t\tif type(seq) == str:\n\t\t\t\tresult.append(Seq(\n\t\t\t\t\tseq,\n\t\t\t\t\tself._parent,\n\t\t\t\t\tpriority=self._priority\n\t\t\t\t))\n\t\t\telif not isinstance(seq, Seq):\n\t\t\t\tlog.debug(u\"Invalid sequence type - ignored (%s)\", type(seq))\n\t\treturn result\n\n\tresult = None\n\tfor s in sequence:\n\t\ttry:\n\t\t\tresult=Seq(s)\n\t\texcept TypeError:\n\t\t\tlog.debug"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif output is None:\n\t\toutput = []\n\tif not (isinstance(sequence, list) or isinstance(sequence, tuple)):\n\t\twarnings.warn(\"sequence parameter must be a list, tuple or an iterable of interface specifications or interface names\",\n\t\t\tUnknownInterfaceWarning)\n\t\t\n\tif isinstance(sequence, basestring):\n\t\tsequence = [ sequence ]\n\t\n\tif isinstance(sequence, Interface):\n\t\tif len(sequence) == 0:\n\t\t\tsequence = InterfaceSequence()\n\t\tsequence = sequence.interfaces\n\n\tif isinstance(sequence, dict):\n\t\tif len(sequence) == 0:"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\n\tif output is None:\n\t\toutput = []\n\n\t# Make one copy of flags\n\tflags = list(flags)\n\tflags.extend(['%s' % flag for flag in flags])\n\n\tif len(sequence) > 1:\n\n\t\t# Replace the first in sequence\n\t\tsequence = sequence[1:]\n\t\tflags = ['%s' % flag for flag in flags]\n\n\t# Do the normal normalization\n\toutput = []\n\tflags = [flag.lower() for flag in flags]\n\tfor flag in flags: # Remove bad flag names\n\t\tflags.remove(flag"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tif not _optimizations_initialized():\n\t\twarn(\"could not load _c_optimizations; load is not available.\")\n\treturn _c_optimizations()[0]\n\ndef set_optimization(optimization):\n\t\"\"\"\n\tGiven a string representing a C function, make a callable.\n\t\n\tThe function should take the string that defines the function name\n\tor a C expression as its first and evaluate the code\n\tprovided in, and compute the global optimization result.\n\t\n\t@param optimization: The C function to use (without parameters)\n\t@rtype: void\n\t@returns"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\traise ImportError(\"cannot import the package `optimizations`. \")\n\t\n# __________________________________________________________________________\n# Set up a default logger for optional handling of debug messages.\n# Do this before importing the package.\n# Preferably there isn't a good way of doing this.\n# Preferably, that would be a good idea or the first place?\n# That way - import the package here and in particular is\n# also a good idea...? If so, the log message is not logged but the\n# package is not there.\n__DEFAULT_LOGGER = \\\n    _logging.getLogger('"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(sys, 'implementation'):\n\t\traise ImportError('can\\'t find an implementation module')\n\n\tif sys.implementation_version.startswith('3.3'):\n\t\t\n\t\tdef Cfun(n, fjac=None, x=None, ftol=None, xtol=None, gtol=None,\n\t\t\t\t   maxcor=None, norm=Inf, epsilon=_epsilon,\n\t\t\t\t   callback=None, options=None):\n\t\t\t\"\"\"Objective function.\"\"\"\n\t\t\txopt, fopt, niter, func_calls, warnflag = \\\n\t\t\t\tget_obj_opt"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\t# get the C optimization module from the config file\n\t\tcp = __import__('optimize', fromlist=['global_optimizations', 'all'])\n\t\t\n\t\t\n\t\t# retrieve the optimize module object\n\t\tc = cp.optimize\n\texcept ImportError:\n\t\t# assume no optimizations, but try loading the module\n\t\t# from it\n\t\ttry:\n\t\t\tglobals()['optimize']\n\t\texcept KeyError:\n\t\t\tpass\n\t\t\n\tif c:\n\t\tglobal_optimizations = {}\n\telse:\n\t\tfor optimizations in globals()['global_optimizations']:\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tfor module in available:\n\t\ttry:\n\t\t\timportlib.import_module(module)\n\t\t\treturn True\n\t\texcept ImportError:\n\t\t\traise ImportError, module # not found on all systems\n\t\n\treturn False\n\n\ndef find_module(name, filename=None):\n\t\n\t\"\"\"Find and load a module from its filename.\n\t\n\tIf it is not found immediately raise ImportError.\n\t\"\"\"\n\t\n\t# try to load from the given name\n\ttry:\n\t\treturn importlib.import_module(name)\n\texcept ImportError, ex:\n\t\tname = None\n\n\tif not name:"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\timport numpy as np\n\n\toptimizations = np.empty(np.shape(optimize_cache))\n\t\n\tif (None in optimizations.shape and optimize_cache.size):\n\t\t# optimize() called with None, but it will only be None if not in\n\t\t\t# optimize()\n\t\treturn Optimization(optimize_cache, True)\n\t\n\tif (optimizations.size > optimizations.max_optimization_evaluations and\n\t\toptimize_cache.size > optimizations.max_optimization_function_evaluations):\n\t\t# optimize() called for a maximum optimization function\n\t\t#"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\traise ImportError(\"could not import this file. Optimizations are probably not installed!\")\n\ndef _c_optimizations_set():\n\tglobal _optimizations, _optimizations_list\n\tif not _optimizations_list:\n\t\traise ImportError(\"optimizations not loaded yet\")\n\ndef _c_optimizations_load():\n\tglobal _optimizations, _optimizations_list\n\tif not _optimizations_list:\n\t\traise ImportError(\"optimizations not loaded yet\")\n\n_c_optimizations_available = _c_optimizations_available()\n_optimizations"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tif sys.flags.optimize!= 2:\n\t\traise ImportError(\"You may need the optimize module \"\n\t\t\t\"on this machine to get optimized version of this \"\n\t\t\t\"optimization method\")\n\t\t\n\t\ndef _find_best_optimizer(sys_argv):\n\t\"\"\"\n\tFind and return the best path to an optimizer. If available,\n\tit will return that for us.\"\"\"\n\t\n\topt = \"\"\n\twhile 1:\n\t\tc = sys_argv.pop(0)\n\t\tif c == \"--min_to_merge\":\n\t\t\toptimizer_name = sys_argv.pop"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tif not Optimization.opt_available():\n\t\traise ImportError, \"the optimizer %s cannot be used or not available\"%Optimization.version\n\treturn optimizations[Optimization.version]()\n\n\"\"\"\nSupport for Wink sensors. (for backwards compatibility; should not be used in production)\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.wink/\n\"\"\"\nimport logging\n\nREQUIREMENTS = ['python-wink==0.1.2']\n\nSENSOR_TYPES = {\n    'temperature': ['"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\tif not _current_optimizations_:\n\t\traise ImportError, \"No current optimizations.\"\n\treturn _current_optimizations_[0]\n\ndef _current_optimizations_ [ index, optimizations ] :\n\tif any( [ c == _c_optimization__(idx) for c in optimizations ]):\n\t\treturn [ _c_optimization_(idx) for idx in optimizations ][ 0 ]\n\telse:\n\t\traise IndexError, \"Optimization index out of range.\"\n \ndef _save_options_( index, optimizations, filename, last_optimizations, last_optim"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t#!/usr/bin/env python\r\n##  Gnome15 - Suite of tools for the Logitech G series keyboards and headsets\r\n##  Copyright (C) 2010 Brett Smith <tanktarta@blueyonder.co.uk>\r\n##\r\n##  This program is free software: you can redistribute it and/or modify\r\n##  it under the terms of the GNU General Public License as published by\r\n##  the Free Software Foundation, either version 3 of the License, or\r\n##  (at your option) any later version.\r"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tglobal USE_C_OPTIMIZATIONS\n\tUSE_C_OPTIMIZATIONS = USE_C_OPTIMIZATIONS\t\n\t\n\treturn USE_C_OPTIMIZATIONS\n\t\ndef _get_c_optimize(myopts):\n\t\"\"\"\n\tGiven a list of options, return the appropriate C optimization setting\n\t\n\t``myopts``.\n\t\"\"\"\n\tmyval = myopts.get('myval')\n\tif myval is None:\n\t\treturn 1\n\tglobal USE_C_OPTIMIZATIONS\n\tif USE_C_OPTIMIZATIONS"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\nUSE_GC = False\ndef use_gc():\n\treturn _use_c_impl(USE_GC)\n\ndef use_inline_c_optimizations():\n\treturn _use_c_impl(USE_GC)\nfrom django.db import models\nfrom django.db.models.signals import post_save\nfrom schools.models import (AbstractModel, Perturbation, Proposition,\n                            Interaction, TruckCategory, BudgetItem,\n                            Truck, Tag, TagGroup)\nfrom django.core.validators import MinValueValidator\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn bool(os.environ.get(\"PURE_PYTHON\") or _use_c_impl())\n\n##\n# Copyright (c) 2012-2015 Apple Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tif 'PURE_PYTHON' in os.environ:\n\t\treturn True\n\t\n\ttry:\n\t\tuse_python = os.environ['PURE_PYTHON']\n\texcept KeyError:\n\t\tuse_python = '1'\n\t\n\tif False: # pragma: no cover - we want to support the C optimizations\n\t\tdef _do_c_optimizations():\n\t\t\tif using_python == '1':\n\t\t\t\treturn True # pragma: no cover\n\t\t\t\n\t\t\tsys.warn(\"I'm assuming I want to use the C optimizations from a \"\n\t\t\t\t\t\"Python executable.\")\n\n\t\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\ndef _use_c_impl():\n\ttry:\n\t\treturn os.environ[\"_USE_C_IMPL\"]\n\texcept KeyError: #pragma: no cover\n\t\treturn None\n\tif _is_c_op_module(USE_C_IMPL):\n\t\t_check_deprecated_version(USE_C_IMPL)\n\t\treturn True\n\nUSE_C_IMPL = _use_c_impl()\n\nif USING_C_IMPL is not None:\n\tHAVE_USE_C_IMPL = True\nelif not __check_deprecated_version_warn(\"USE"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\t# We could also do more to get this information from `warnings`.\n\twarnings.warn(\"PURE_PYTHON\", stacklevel=2)\n\treturn _use_c_impl\n\n\ndef _use_c_impl(compiler):\n\tfor func in compiler._optimize:\n\t\ttry:\n\t\t\treturn func()\n\t\texcept AttributeError:\n\t\t\tpass\n\telse:\n\t\treturn False\n\n\n# On the python path:\nis_py2 = sys.version_info.major == 2\nis_py24 = sys.version_info.major == 2 and sys.version"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tif __version_info__ < (2, 7):\n\t\t# We need to work around a bug in the C extension with older Python versions\n\t\t#\n\t\t# http://bugs.python.org/issue12\n\t\treturn bool(os.environ.get('PURE_PYTHON') or _is_pypy())\n\telif __version_info__ < (3, 4):\n\t\treturn _is_pypy()\n\telse:  # Python version 3.4 does not support long integers for numbers.\n\treturn _is_pypy()\n\n\ndef _is_pypy():\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tif sys.platform == 'darwin':\n\t\treturn False\n\t\n\treturn True\n\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturns whether the 'C Optimization' environment variable is set,\n\twhich is up to a point when C was called (either inside or outside).\n\t\n\tIf not, we try to detect what version of Python we were compiled\n\twith.\n\tIf not, raise an exception.\n\t\n\t>>> from waflib.misc import c_optimizations\n\t>>> c_optimizations['GCC_VERSION']\n\t\t'4.7'\n\t\"\"\"\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tif sys.platform == \"win32\":\n\t\treturn _using_c_impl or 'no'\n\t\n\t# For a normal build, we require a C compiler to be used.\n\tif os.name == 'posix':\n\t\treturn False\n\t\n\t# On Windows, distutils.util.get_platform() may be 'novell' (Unix, Windows) or 'ansi' (Cygwin\n\t# or AIX) on Solaris.\n\telif sys.platform == 'darwin' and 'cyg' in sys.platform:\n\t\treturn _using_c_impl or"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\ndef _c_optimizations_changed(optimizations):\n\t\"\"\"\n\tThe opposite of `_c_optimizations_required`\n\t\"\"\"\n\t\n\tglobal c_optimizations_required\n\t\n\tif optimizations is not None:\n\t\tc_optimizations_required = None\n\t\n\t\tfor each in optimizations:\n\t\t\tif not each:\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif each[0]!= None:\n\t\t\t\tif each[0] == -1:\n\t\t\t\t\tif not isinstance(each[1], str):\n\t\t\t\t\t\teach[1] = str(each[1])\n\t\t\t\t\t\tcontinue\n\t\t\t\t"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n\t\n\t_c_optimizations_required()\n\t\n#\n# Private functions\n#\n\ndef set_optimizations():\n\t\"\"\"\n\tReset the global optimizations set to the default ones.\n\t\n\tNOTE: THIS FUNCTION WILL ONLY ADD A NEW FUNCTIONS HERE!!!\n\t\"\"\"\n\n\tglobal _g_optimizations_needed_in_file\n\n\t_g_optimizations_needed_in_file = {}\n\n\tfor o in __global_optimizations__:\n\t\tif o in _g_optimizations_needed_in_file.keys():\n\t\t\t_g_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n# =============================================================================\n#\n#                              C L A S E R S\n#\n# =============================================================================\n\ndef c_c_optimize(c, x0, sigma):\n\tglobal k\n\tglobal m\n\tglobal L, m\n\n\t# Check if the gradient is positive definite\n\tf = K(x0)\n\tfprime = -f(x0)\n\tif fprime == 0:\n\t\tfprime = K(x0+sigma)\n\n\t# Start the convergence criterion with the gradient\n\tg = -sigma*fprime * np.ones(K(x0"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\ndef make_solvers(\n\tglobal_optimistations_list,\n\tlocal_optimistations_list\n):\n\tresult_list = list()\n\n\tfor optimizer in global_optimistations_list:\n\t\tlocal_optimizer = optimizer.get_local_optimizations()\n\t\tresult_list.append((optimizer, local_optimizer))\n\t\n\treturn result_list\n\ndef _optimize_static(\n\tlocal_optimistations_list, \n\toptimizer_factory,\n\tlocal_optimizations_used,\n\toptimizer_used_dict, \n\tlocal"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\t# The first check is if a class with a single `__init__` method exists.\n\t\t# Otherwise, search for the `__init__` method, then run with the first\n\t\t# thing inside the class dict.\n\t\tfor class_ in _all_decorators.get(type)\n\t\tif not class_.__init__:\n\t\t\tcontinue\n\n\t\t# If the class has a `__call__`, then run the `__call__` function with the\n\t\t# appropriate context.\n\t\tif class_.__call__ is not None:\n\t\t\treturn __call__(class_, *args, **kwargs)\n\n\treturn"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n\n\nif not _c_optimizations_required():\n\n\t# check that all requirements are met\n\tfor module in mod_name.split('.'):\n\t\tif not any((module.startswith(x) for x in _required_modules)):\n\t\t\t# assume we have a module name\n\t\t\t# if it contains a forbidden module\n\t\t\tif not callable(module):\n\t\t\t\tcontinue\n\t\t\tif not module[2:] in _required_modules:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tcontinue\n\n\t# look for any class or module, and apply them all\n\t#\n\t# 1. load"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t \n\t__slots__ = (\"_optimizer\", \"_optimizations\", \"_optimized_optimization\", \"_optimizations_attempted\", \"_optimizations_removed\", \"run\", \"step\", \"state\", \"step_state\", \"global_optimizations_attempted\", \"global_optimizations_removed\")\n\n\tdef __init__(self, _optimizer, optimized_optimization):\n\t\t\n\t\tassert (type(optimized_optimization) is Optimization)\n\t\tassert (optimized_optimization._optimizations_attempted is False or optimized_optimization._optimized_optimization is None)\n\t\tassert (optimized_optimization._optimize is optimizer._optimized"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\ndef _c_optimizations_required():\n\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\ndef print_c_info_only():\n\t\"\"\"\n\tPrint the configuration and parameters, with information only.\n\t\"\"\"\n\tif os.path.isfile(c_const_file):\n\t\tif config.verbosity > 0:\n\t\t\tprint \"Reading information from\", c_const_file, \"...\"\n\t\tif os.path.isfile(c_params_file):\n\t\t\tif config.verbosity > 1:\n\t\t\t\tprint_header_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t \n\t# We've already handled some optimization loops in order to build it:\n\t# 1) Make a dictionary key -> tuple of optimized optimization functions\n\t# 2) Compute that function in the following order:\n\t#\t1a) Create a list of functions that have not yet been run, and we\n\t\t#\t2b) Update some arguments with (func, args, grad) where\n\t\t#\t\tthe function is evaluated at each optimization step.\n\t\t#\t3) Let a function of type 'f(x, *args)' that returns a list\n\t\t#\t\t\tof gradients"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t # Nothing to repeat.\n\t pass\n\ndef _c_optimizations_required(self):\n\t\"\"\"\n\tAll optimizations are needed.\n\t\"\"\"\n\t# Nothing to repeat.\n\tpass\n\n\n# TODO optimize for the following cases\n@add_standard_semantics\nclass Optimization(object):\n\t\"\"\"\n\tAn optimizations object provides various optimizations.\n\n\tThe following optimizations are supported:\n\n   .. sourcecode:: pycon\n\n        Optimization operators\n        ----------------- | + - | /=\n        ~ + - |                                           |\n\n\n   .. versionadded:: 0.4.0\n    \n   "}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\tdef _safe_call(f):\n\t\tif sys.version_info[0] == 2:\n\t\t\treturn f()\n\t\telse:\n\t\t\treturn f\n\t\n\t\n\tdef _default_c_options(self, asopt=False, with_statement=False):\n\t\tassert isinstance(asopt, bool)\n\t\tif asopt:\n\t\t\tdefault_options = {}\n\t\telse:\n\t\t\tassert asopt\n\t\tdefault_options = super(_c_optimizations_required, self)._default_c_options\n\t\t\n\t\tdef c_flags(f):\n\t\t\treturn f"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn _use_c_impl_required(\n\t\tplatform.python_implementation()!=platform.python_implementation(),\n\t\treason=(\"PyNico: %s is not supported by the current Python environment (%s) - ignoring this warning.\")\n\t\t\t% (platform.python_implementation(), platform.python_version()))\n\n# -*- coding: utf-8 -*-\n\n\"\"\"\n***************************************************************************\n    lascanopyPro.py\n    ---------------------\n    Date                 : August 2012\n    Copyright            : (C) 2012 by Victor Olaya\n    Email                : volayaf at gmail dot com\n***************************************************************************"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tfrom __future__ import print_function\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\nclass OptimizationState(object):\n\t\"\"\"\n\tRepresents the state of the optimized algorithm.\n\t\n\t.. seealso::\n\t    :py:mod:`paddle.optimizer`.\n\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the state for the optimized algorithm.\n\t\tShould be overridden by subclasses.\n\t\t\"\"\"\n\t\tpass\n\n\tdef set_optimize_state(self, optimized_state):\n\t\t\"\"\"\n\t\tSet the state for the"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\t__use_c_impl = True\n\t\ndef _use_c_opt_flags(x):\n\t\"\"\"\n\tReturn a string describing where the *c* compiler flags should be placed.\n\t\n\tThe return value will be a valid compiler flags to use if that compiler is\n\tavailable.  The return value will be :\n\t[[\"-O5\", \"optimization-level\", \"O\"], [\"-O3\", \"-opt-level\"], etc].\n\t\"\"\"\n\treturn _use_c_impl[x]\n\n# These must be set as DARPAst does not"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\t# XXX check for existence of required module\n\tc_op_requires = [op.lower() for op in ['IN_Q', 'IN_T', 'IN_R']]\n\t\n\t# XXX check for existence of required C-math operators\n\tc_special_ops =  [op+\"(\"+\"**\" + op+\"*\" if op in op_requires else op) \\\n\t\t\t\t\t\tfor op in ['MATCH_STRING', 'MIN_MATCH_STRING', 'MAX_MATCH_STRING']]\n\t\t\n\tc_not_in_math_ops = [op + op for op in ['"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\n\n#===------------------------------------------------------------------===\n# Compiles and evaluates the program\n#===------------------------------------------------------------------===\n\ndef _comp_str(_f):\n\t\"\"\"\n\tReturn the C source string equivalent of the C compiler.\n\t\"\"\"\n\tret = ''\n\tpos = f.tell()\n\ts = f.read()\n\n\t# print's: \"%s\"' % s.strip()\n\n\twhile pos * 4 < len(s):\n\t\tif s[pos * 4][0] == '%' and pos > 2:\n\t\t\tif len(s) == pos * 4 + 1"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn _use_c_impl(_core.C_OPTIMIZATION_REQUIRED)\n\n\ndef _gfc2py_fallback():\n\timport os\n\tcant_write_error(__file__,\n\t\t\t\t\t\tos.environ[\"PY_FORCE_CONFORMING_NEW\"],\n\t\t\t\t\t\t\"Error: Unable to generate C code, cannot override C code automatically\")\ndef get_error_message():\n\terr = _core.get_error_message()\n\tcant_write_error(__file__, err)\n\ndef _setup_c_api():\n\timport numpy\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn Configuration.get(\"allow_gc\")\n\ndef config_key_value_type():\n\t\"\"\"\n\tReturn a type for the configuration key to match the C implementation.\n\t\n\tThis is used as a key to return a value suitable for passing to the\n\tnative config manager.\"\"\"\n\t\n\treturn Configuration.get(\"type\")\n\t\ndef _config_value_type():\n\t\"\"\"\n\tReturn a type for the configuration value to match the C implementation.\n\n\tThis is used as a key to return a value suitable for passing to the\n\tnative config manager.\"\"\"\n\t\n\treturn Configuration."}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n_use_c_impl = _CImplementation()      # _CImplementation is used for this module.\n\ndef _use_c_impl_flag():\n\t\"\"\"\n\tReturn a true value if the C optimizations should be extended to C, for use with\n\tin-built optimizations.\n\t\n\tTo do\n\t- compute C_op(P)*P - P*C = H_op*C + 1\n\t- compute H_op * P - H_op(P)*C = 1 / P\n\tand do\n\t- compute the H_op(P"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn bool(os.getenv('PURE_USE_PYC_OPTIMIZATION','0') or\n\t\t\tget_configured_c_optimizations()!=-1)\n\ndef _use_c_impl(self):\n\t\"\"\"\n\tReturns a true value if the C implementation exists.\n\t\"\"\"\n\ttry:\n\t\timpl = compile_f90()\n\texcept CompileError:\t\t\n\t\n\t\tif not self.env.PX4:\n\t\t\timpl = 'px4'\n\t\telse:\n\t\t\timpl = 'px4-python'\n\t\n\tif not os."}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\t# initialize histogram\n\tself.__init__()\n\tself.make_bins()\n\tif self.initial_value:\n\t    self.do(fill=True)\n\t    if self.options.make_bins:\n\t\tself.bins.clear()\n\t    self.bins['0'] = default_bin()\n\t    if self.options.make_histogram:\n                self.bins.update(self.initial_value)\n\n#################################################################################\n#\n#   MAIN\n#\n#################################################################################\nif __name__ == '__main__':\n    opts, args = parser.parse_args()\n    main"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\t# The current histogram is simply the same one.\n\t\t# Nothing to reset yet.\n\t\tpass\n\n\tdef _do_hist_add_histogram(self, histo):\n\t\tif self._do_log_scale(): return\t\t\t\t\t\t\n\t\tassert len(histo) == 1\t\t\n\t\thistType = histo[0]\n\t\t# Find the correct histo which we already have.\n\t\thist1 = self._find_histogram(histo[0])\t\t\t\n\t\t# See whether or not the histogram is already the same.\n\t\ttry:\n\t\t\thisto.pop"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n    def add(self, name, value):\n        assert len(self.bins) == 0, \"Cannot add the histogram, no bins.\"\n        self.bins[name] = value\n    \n    def make_histogram(self):\n\t\"\"\"\n\t\tMake a histogram of values.\n\n\t\tParameters: \n\t\tNone\n\n\t\tReturns: 1D array\n\t\t\"\"\"\n\t#self.hist[name] = bins=len(self.bins)\n\t\tvalues = []\n\t\ttry:\n\t    for key, value in self.initial_values[self.index].items():\n\t\tvalues.append(value)"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\t# initialize the histograms \n\tfor key in self.hists:\n\t\tself[key] = self.histogram[key].reset()\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n###########################################################################\n###########################################################################\ndef main():\n\t\"\"\"\n\tCreate a sample of HIST and initialize the display and histogram.\n\n\tUseful when called from a command-line interface or from the command line.\n\n\t# build a histogram \n\thist = hist_dict()\n\thist.histogram['a'] = 1.\n\thist.histogram['b'] = 1\n\thist.histogram['c'] = 1\n\t\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\tdef make_bins(self):\n\t\"\"\"\n\tGenerate all bins.\n\t\n\tThis function has to be called if the histogram is not initialized\n\t\"\"\"\n\tif(self.initialize==False or \n\t\tself.initialize!= True):\n\t\tself.clear_stats()\n\t\tself.make_hist()\n\t\n\tbins = self.init_bins_values()\n\tmaxbin = max(bins.values())\n\treturn bins.copy()\n\t\n\t\n\t\n\t\n\t\n\nclass histogram_init(object):\n\tdef __init__(self, **kwargs):\n\t\tself.kwargs = kwargs"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\t##\n\t# @name\t\tstring\t\tThe string used in the histogram to be stored in \\\n\t#\t\tthe *bkg* attribute of :attr:`bkg_profile`.\n\t# @author\t\tstring\t\tThe name of the author of the :attr:`bkg_profile` \\\n\t#\t\tthat created the histogram.\n\t# @usage\t\ta\t\tThe histogram to be stored in this dictionary.\n\t# @param\t\tb\t\tAn object to be used as the *bkg* attribute of \\\n\t#\t\t\t\t"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\tself.bins = binspecs.get_bins()\n\tself.init_vals = binspecs.get_init_vals()\n\t\n\t\n\t# This is a bit of a mess.\n\tdef init_histogram(self, nbins = None, normed = None):\n\t\t\n\t\tif nbins is None: nbins = len(self.bins)\n\t\tif normed is None: normed = self.normed\n\t\tself.x_dim = np.size(self.bins[0])-1\n\t\tself.histx = np.logspace(-self.x"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\n\tdef __init__(self, name, initial_value=None, make_bins=False):\n\t\tif self._frozen:\n\t\t\traise RuntimeError(\"Can't initialize histogram!\")\n\n\t\tself.values = {}\n\t\tself.make_bins = make_bins\n\n\t\tself.hist = {}\n\t\tself.hist[name] = []\n\t\tif initial_value:\n\t\t\tself.initial = initial_value\n\t\t\tself.update()\n\n\tdef initialize(self):\n\t\tif self._frozen:\n\t\t\traise RuntimeError(\"Can't initialize histogram!\")\n\n\t\tif self.make_bins"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\n\tif self.initial_value == None:\n\t\tself.clear()\n\n\t##\n\t# Overloads\n\t##\n\t\n\tdef initByValue(self, value ):\n\t\t\"\"\"\n\t\tIn case no current context on the histogram is initialized,\n\t\tthe current histogram is just initialized.\n\t\t\n\t\t@param value: The value of the current context (float)\n\t\t@type value: int\n\t\t\"\"\"\n\t\t##print 'initByValue: '+str(value)\n\t\tself.__init__(value)\n\n\n\tdef getBinWidth(self):\n\t\t\"\"\"\n\t\t@raise ValueError"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\t# TODO: recompile this method so we don't need to do this here\n\timport logging\n\tglobal _default_histogram\n\t# TODO: add some more logging\n\tif _default_histogram is None:\n\t\tlogging.debug(\"Creating histogram.\")\n\t\tlogging.debug(\"Starting init.\")\n\t\tself._init_histogram(initial_value=False)\n\n\t# NOTE: initialize_histogram might be called already by the initial\n\t#   object.\n\tself.initialize_histogram()\n\n\t\n\tdef _init_histogram(self, initial_value=True):\n\t\t\n\t"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\t\n\twriter = csv.writer(sys.stdout)\n\tif header is None:\n\t\treader = csv.reader(self, dialect=csv.excel)\n\telse:\n\t\treader = csv.reader(open(header, \"rb\"), dialect=csv.excel)\n\tfor row in reader:\n\t\twriter.writerow(row)\n\t\twriter.writerows(row)\n\n\n# pytype: disable=wildcard-import,invalid-name\n\n\n# pytype: disable=wildcard-import,invalid-name\n\nimport sys\n\n# pytype: disable=wildcard-"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tfrom LatticeExt.format.latex import graph2csv, graph2latex\n\t\n\twriter = self.writer\n\n\tif header is not None:\n\t\twriter.writerow(header + separator*graph2csv(graph2latex(self)))\n\"\"\"\n===============================================================================\nSpectralClustering\n===============================================================================\n\nThis implementation corresponds to \"Spectral Clustering for Isomap.\nNature Methods in Clustering, 2nd edition, 2003,\nhttp://www-users.cs.umn.edu/swli/pdf/OASIS-2009-0011.pdf\n\nSee"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tg = self.graph\n\tif g.is_directed():\n\t\t# We assume that the undirected graph is a directed graph:\n\t\n\tnode_to_csv(self, separator, header, 'directed')\n\tfor e in self.vertices():\n\t\tnode_to_csv(e, separator, header, 'directed')\n\tfor f in self.edges():\n\t\tto_csv(f, separator, header, 'directed')\n\tfor n in self:\n\t\tpath.relsurf(path.line, os.path.join(g.source_"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tfrom LatLon import lena\n\texcept ImportError:\n\t\traise RuntimeError(\"Lena 0.5 (or later) is required. \"\n\t\t\t\"Try `pip install lena`.\")\n\t\n\tif len(self) == 0: return \"\"\n\t\t\t\n\treturn super(L_Graph, self).to_csv(separator, header)\n\n\ndef to_networkx(G):\n\t\"\"\"\n\tGenerate an undirected graph out of a :class:`Graph`\t\n\t(including the first column if none is given).\n\n\tThis is the *edge key"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\twith open(self, \"w\") as f:\n\t\theader = tuple(header)\n\t\tseparator = separator.split()\n\t\tprint(separator, end='')\n\t\tfor (p1, p2), v1, v2 in itertools.product(separator, header, separator):\n\t\t\tif p1 == v1 is None and v2 is None: continue # skip unmatched edges\n\t\t\t# first do x and v mapping in the order the first entries correspond\n\t\t\tself.insert(int(v1), int(v2))\n\t\t\tcontinue\n\t\n\tdef clear(self):\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tfrom.ToCSV import ToCSV\n\t\n\tif separator is None:\n\t\tseparator = \"\\t\"\n\t\n\tif header is None:\n\t\theader = None\n\t\n\tto_csv_func = self._to_csv\n\t\n\ttry:\n\t\tseparator = separator.encode()\n\texcept AttributeError:\n\t\tpass\n\t\n\twriter = csv.writer(self._fout,\n\t\tdelimiter=separator,\n\t\tquotechar='\"',\n#####################################################\n\t\t, dialect=csv.excel_tab,\n\t\tskipinitialspace=True)\n\t\n\tnames = [x[0] for x in writer."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n       # this needs to be set, see.lena documentation for more information.\n#      self.output = Output(output=\"graph_data.tsv\",\n#        format=NL)\n        \n       from lepton import Chem, LammpsError\n  \n        \n   #\n   def to_xml(self, outfile, level=WARN, header=None):\n\t\"\"\"\n\tExecute the command :py:meth:`llenatypes.utils.to_output` and parse the\n\toutputs into a :class:`xml.etree.ElementTree.Element`.\n\t\n\t*outfile* is the output"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tfrom nltools.utils.graphics import line_graph_to_latex, line_to_csv\n\tfrom utils import *\n\tfrom utils.text import to_str\n\tfrom utils.misc import open_file_write, open_file_read\n\n\tgraph = Graph(delimiter=separator, header=header)\n\tfor line in to_str(graph):\n\t\tyield line_graph_to_latex(line, file=open_file_write)\n\t\t\n\t\t\n\n# TODO: make this class be replaced with a `networkx.MultiDiGraph`\n# TODO: fix"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\traise DeprecationWarning(\"to_csv is not used in \"\n\t\t                              \"SequenceSet to_csv, so it can \"\n\t\t                              \"only work when the graph is multidimensional.\")\n\tif header is None:\n\t\theader = [\"\"]*self.__num_columns__\n\tif separator!= \",\":\n\t\tstr_separator='\\n'\n\t\tfor point in self.points:\n\t\t\tstr_separator=str_separator+\" \"*(self.__num_columns__-1)\n\t\t\tstr_separator=str_separator+\"\\n\"\n\t\t\tfor node in range(self.__num_columns__):\n\t\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n@implements_to_string\nclass Text(BaseGraph):\n\t\"\"\"\n\tAn abstract base class for representing graph that is a string\n\tin the graph of an experiment.\n\t\n\tEach node in the experiment defines an \"inputs\" node\n\tas its inputs.\n\n\tIn addition to its default outputs, a \"output\" node is created\n\tfor each node added to the last node, and a \"error\" node\n\tis created.\n\t\n\tThese node types are created dynamically, i.e., by\n\tdefing the inputs::\n\t\t\n\t\tx1 = Input(Input.Type."}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n#    errmsg=\"no-slicing for '%s'\"%coord_name\n#    print \"Getting error indices for '%s'\"%coord_name\n#    for idx in self._coord_index_list:\n#        errmsg+=str(idx)+\"=None\"\n#        if idx=='x':\n#            errmsg+=str(idx)+\"=None\"\n#        if idx=='y':\n#            errmsg+=str(idx)+\"=None\"\n#        if idx=='z':\n#            errmsg+=str(idx)+\"=None\"\n#        if idx=='err_idx"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n#\t# Obtain indices of error for given coord name\n#\terr_indices = []\n#\tfor i in range(self.__n_coords):\n#\t\tfor j in range(self.__n_errors):\n#\t\t\terr_indices.append(_get_err_indices(i, j))\n\t\t\n\t#\tif sys.version_info[0] < 3:\n#\t\tfor i in range(self.__n_errors):\n#\t\t\tfor j in range(self.__n_errors):\n#\t\t\t\terr_indices[i, j]"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n       dim_name = self._get_coord_dim(coord_name)[0]\n       \n       dim_val = self._get_dim_values(dim_name)\n       \n       if len(dim_val) == 0:\n                return [False]\n       \n       return [_get_err_indices(self, err_name) for err_name in dim_val]\n\n    def _slice_dim_values(self, dim_name):\n        \"\"\"\n\tSlice values to a dimension.  In the case of scalars (2D)\n\twe need to slice at this point to make a 1D"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n\n\t# Get the err data\n\tif coord_name not in self.data_retriever.metadata.err_metadata:\n\t    # Get errors from header\n        \tself.data_retriever.data.get_errors(coord_name, 'error')\n\n\tif len(self.data_retriever.metadata.err_metadata[coord_name][1])>0:\n\t    # Get errors from all frames in frame list\n\t    self.data_retriever.frame_list[coord_name] = self.data_retriever.data_retriever.get_list(coord_"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\ttry:\n\t\t\tcoords = self._coords_by_coord(coord_name)\n\t\texcept:\n\t\t\treturn list()\n\n\t\t# Get number of element in coord to use\n\t\telement_count = len(coords)\n\n\t\t# Make sure it's a real error\n\t\tif not len(coords[0]) == element_count:\n\t\t\treturn list()\n\n\t\t# Convert int indices to 1d numpy arrays\n\t\tcoords = [x[int(i)] for coord, i in enumerate(coords) if coord in coords]\n\n\t\t# If len(coords) == 1, find the"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t self._coords_by_name[coord_name] = (self._coords_by_id[coord_name][0], self._coords_by_id[coord_name][1])\n\t return self._coords_by_name[coord_name]\n\n\tdef get_coord_name_for_var(self, varname):\n\t\t\"\"\"\n\t\tGiven a variable name, returns its coord name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._coords_by_name[varname][0]\n\t\texcept:\n\t\t\treturn self._get_err_indices(varname)\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n\treturn err_locs[coord_name]\n\ndef get_grid_info(err_list):\n\t\"\"\"\n\tGet information about multiple grid points.\n\t\"\"\"\n\t\n\tgrid_info = [\n\t\t[(z, z2, z3) for z, z2 in err_list]\n\t\tfor z in err_list if isinstance(z, (LinearAxis, OffsetAxis))]\n\treturn grid_info\n\t\t\ndef get_grid_resolution(coord):\n\t\"\"\"\n\tGet the resolution of a single grid point\n\t\"\"\"\n\ttry:\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t return [self._err_indices[coord_name]]\n\n\tdef _get_coor_indices(self):\n\t\tcoor_indices = []\n\t\tfor coor in self.grid_coords:\n\t\t\tcoor_indices += [self._coord_indices[coord_i[0], coord_i[1]]]\n\t\treturn coor_indices\n\n\tdef _get_corridors(self, coord_names):\n\t\tcoords_ids = []\n\t\tfor cid, coord in enumerate(self.grid_coords):\n\t\t\ttry:\n\t\t\t\tcoor_index = self"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n\n\terr_indices=[]\n\tfor i,j,k in enumerate(self.key_dimension_names):\n\t\terr_indices.append(self._coordslist[i][j,k])+self._coordinateslist[i][k]\n\n\tif coord_name:\n\t    err_indices=self._get_err_indices(coord_name)\n\telse:\n\t    err_indices=None\n\n\treturn err_indices\n\n\n\n \tdef get_coord(self,coord_name):\n\n\t\"\"\"\n\tGet coordible coordinate.\n\t\"\"\"\n\n\tcoord=self._"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n#\t\tout = spgme.regionprops(self.coords_names[coord_name])\n#\t\tself.err_index_array = dict(zip(self.coords_names[coord_name].values, out))\n\t\n\tdef get_coord_err(self, coord):\n\t\t# Get standard error from coord by getting it from _coord.\n\t\ttry:\n\t\t\terr = self._coord.long_name\n\t\texcept AttributeError:\n\t\t\t# Error was not recorded at all, so get it from _coord.\n\t\t\t# NOTE: The error array only indicates"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\tfor key, value in context.value.items():\n\t\t\n\t\tif hasattr(self, '_d_fields'):\n\t\t\ttry:\n\t\t\t\tsetattr__(\"_\"+key, value)\n\t\t\texcept AttributeError:\n\t\t\t\tattr = getattr(self.__class__, key)\n\t\t\t\tgetattr(self, '_d_fields').append((attr,value))\n\t\t\t\tsetattr(self, key, value)\n\t\t\n\n\nfrom.. import Graph, PropertyMap, Edge\n\n#\n#.. autosummary::\n#   :toctree: generated/\n\n\nclass Node(Graph):\n\t\n\t#\n\t#.. add"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\tdef _get_values(self, context):\n\t\t\"\"\"Internal method that gets the values to show on the graph.\"\"\"\n\t\traise NotImplementedError\n\n\tdef _setup_nodes(self, nodes):\n\t\t\"\"\"Setup nodes for the given graph state.\"\"\"\n\t\t\n\t\tnode_handlers = self._callbacks.get(self, [])\n\t\tfor node_handler in nodes:\n\t\t\ttry:\n\t\t\t\tnode_handler()\n\t\t\texcept AssertionError:\n\t\t\t\tnode_handlers.append(self, node_handler)\n\nclass Graph(GraphBase):\n\tpass\n\nclass NodeHandler(GraphNodeHandlerBase"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\t# First, we convert the object to bytes and put it all back in\n\t# the graph. \n\tself.__graph__ = proto.Graph()\n\n\t# Now, we add the fields that are not present.\n\tfor field in self.__graph__.graph['FieldTypes']:\n\t\tif field[0] in PROV_FIELDS:\n\t\t\tif isinstance(self.__graph__, proto.Graph):\n\t\t\t\tprop = self.__graph__.node[field]\n\t\t\t\tfor p in prop.path():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif p.name().endswith(\":\"):\n\t\t\t\t\t\t\tself.__graph"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\n\n\n# =============================================================================\n# CFG Utility Functions\n# =============================================================================\n\n_CONTEXT_NAMES = [\"E\", \"T\", \"node\", \"edge\"]\n_DEFAULT_CONTEXT = {}\nfor _ctxname in _CONTEXT_NAMES:\n    _DEFAULT_CONTEXT[_ctxname] = _ctxname\n\n\n\ndef _is_context(ctxname):\n\tctx = _DEFAULT_CONTEXT[ctxname]\n\tif ctx == \"E\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n\ndef context_names():\n\tyield \"default\"\n\tfor name in context_names():\n\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\n\twith self._graph.as_default():\n\t\tself._context = context\n\n\t\t# set all subcontexts to their indices\n\t\tself._set_subcontexts()\n\t\tself.check_graph_dependencies()  # raise error\n#!/usr/bin/env python\n#\n# Copyright 2007 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\tif hasattr(context, 'value'):\n\t\tself.context = context\n\t\n\tfor name, value in self.properties.iteritems():\n\t\tif name in self.properties:\n\t\t\tself.properties[name].update(value)\n\t\n\tself.context.update(self.properties.itervalues())\n\ndef error_plot(value, name, error):\n\t\"\"\"\n\tCreate a plot of an error.\n\t\n\t@param value: Value to plot\n\t@type value: float or string\n\t@param name: Metric name\n\t@type (string, list(string))\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t    \n\tif context.error is None:\n\t    msg = \"Context error\" if context.is_empty() else \"Empty context error\"\n         else:\n            msg = \"Error occurred during destruction\" if context.is_empty() \\\n\t\t  else \"Error occurred during destruction\"     \n\telse:\n\t    msg = \"Error occurred during destruction\" if context.is_empty() \\\n\t\t  else \"Error occurred during destruction\"     \n\traise context as err\n# Copyright (c) 2012-2014 ARM Limited\n# All rights reserved.\n#\n# The license below extends only to copyright"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\n\n\t\n\n\n\n\n##########################################################################################\n\n\nclass Template(object):\n\t\"\"\"\n\tCreate a list of templates.\n\n\tTemplate instances are instantiated with a given *values*\n\t*at construction time*. The method :meth:`.Init` gets called\n\twhen the construction is asked to start. Additional values\n\tin the template constructor can be provided to\n\t`initialization` method.\n\n\t:param template: Template instance.\n\t:type template: list(:class:`.Template`)\n\t\"\"\"\n\n\t\t\n#################################################################################################\n\n\nclass Empty(object):\n\tpass\n\n\n\n########################################################################################"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\n\tif len(context.node_id)!= len(set(context.node_id)):\n\t\tself.error = \"Error: \" + \"Duplicate IDs of nodes: \" \n\t\tfor (node_id, err) in context.node_id.items():\n\t\t\tself.error = err\n\t\traise ValueError\n\t\n\tfor node_id in context.node_id:\n\t\t#node = self.edges[node_id[\"y\"]]\n\t\tself.add_node(node_id, node_id[\"x_low\"], node_id[\"y_low\"], node_"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n_default_name = \"Piggyback\"\n\ndef _set_name_defaults(self):\n\t\"\"\"\n\tSet the default global variables of Piggyback.\n\t\"\"\"\n\t# Set default name defaults to Gnuplot's name\n\tif self.name is not None and self.name!= \"\":\n\t\tself.name = _default_name\n\t# Set title defaults to Gnuplot's title\n\tif self.title is not None and self.title!= \"\":\n\t\tself.title = _default_name\n\treturn True\n\nclass PiggybackParser"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\ndef interpolate(X,Y, Z, *args, **kwargs):\n\t\"\"\"\n\tInterpolate a function to a mesh.\n\t\n\t*X* and *Y* are arrays with length nbins+1 (or nbins+1)+1 elements.\n\tData arrays must have at least a nbins and have at least 2 entries.\n\t*args* are passed to :func:`scipy.interpolate.interp1d() <interp1d.interpolate>`,\n\t**kwargs** are passed to :func:`scipy.interpolate.UnivariateSpline <UnivariateSpline.interpolate>`.\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\nclass histogram(object):\n\tdef __init__(self, x_min=None, x_max=None, y_min=None, y_max=None, n_bins=50):\n\t\t\"\"\"\n\t\tCreate a histogram.\n\t\t\n\t\t**PARAMETERS**\n\t\n\t\tx_min and x_max \u2013 *x*, *y*, *n*:\n\t\t\tx minimum and maximum for all x bins to take (default = all bins)\n\t\tx_min and x_max \u2013 *x*, *y*\n\t\t\tminimum and maximum for all x labels to take (default = all"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tif len(edges) == 0:\n\t\treturn 0\n\t\n\tif len(bins) > 1:\n\t\tx, y = numpy.histogram(bins, bins)\n\telse:\n\t\tx, y = bins[0]\n\t\n\tassert x.shape == y.shape\n\t\n\tif len(edges[0]) == 1:\n\t\tnum1 = x.shape[0] * edges[0][0]\n\telse:\n\t\tnum1 = edges[0][0] * edges[0][1]\n\tnum2 = edges[1][0] * edges["}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\n\tfrom numpy import linspace\n\tfrom. import histogram\t\t\t# interface to common routines\n\tfrom. import histogramming\n\tfrom. import tools\n\tfrom numpy import sum, exp\n\t\n\t# calculate integral over each bin\n\tminBin=max(min(edges), key=lambda e:(e[0]-edges[e[1]]) if e[0]>0 else 0)\n\tmaxBin=min(max(edges), key=lambda e:(e[0]+edges[-1]) if e[0]<0 else 0)\t\n\t\n\t# calculate the step\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tbins = np.asanyarray(bins, dtype=float, order='c')\n\tedges = np.asanyarray(edges, dtype=float, order='c')\n\t\n\tbincenters = np.bincount(bins, weights=edges)\n\t\n\tmean = bins.sum() / float(edges.sum())\n\tstd = edges.sum() / float(edges.sum())\n\t\n\treturn (bincenters.reshape(bins.shape+1) * edges).reshape(bincenters.shape+1) * mean/std\n#!/usr/bin/python3\n#"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n#\t\tn = len(bins)\n\t\n\t#\tprint \"\\nintegral:\", n, \"bins:\", type(bins)\n\t\n\tdx = [bins[i*n//2] - edges[i*n//2] for i in range(max(n//2, 2))]\n\t\n\t#\tprint \"density:\", float(sum(dx))\n\t\n#\t\tassert len(dx) == max(n//2, 2)\n\t\n\treturn numpy.sum([d*dx[i] for d in dx]) / len(dx)\n\ndef integral_of_linear("}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\nclass integral_curve(quadrature.IntegralCurve):\n\t\n\t\"\"\"\n\tCompute integral curve (slope/intercept).\n\t\n\tThe curve is assumed to be a function f or polynomial (with\n\tonly one coefficient in the interval). \n\"\"\"\n\nclass integral_curve2(quadrature.IntegralCurve):\n\t\n\t\"\"\"\n\tCompute integral curve (slope/intercept + 1)\n\t\n\tThe curve is assumed to be a function 1 for a higher degree\n\tin the interval. \n\t\n\tThe function is defined as in :class:`.integral_curve` definition.\n\tThe order of the curve is"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tbins = np.asarray(bins)\n\tedges = np.asarray(edges)\n\t\n\tscale = 1 # because edge length is a single number\n\t\n\t# we first need to compute the integral\n\t# for each bin\n\th = np.empty(len(bins))\n\tfor i,e in enumerate(edges):\n\t\th[i] = integral(bins[i], edges[i])\n\t\t# now see where e ends (and after) end, and set\n\t\t# how much we can add to h, given that\n\t\th[i] += scale\n\t\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\n\t# determine nbins and edges\n\tnbins = 1 + int(np.log2(nbins * 2)) + 1\n\tn_edges = len(edges)\n\t# set histogram edge width\n\twidth = np.log2(bins[1:] / bins[:- 1])\n\t# define a bin size and get the maximum length\n\tbin_size = int(np.min((bins[1] - bins[0]) / bin_edges)) + 1\n\t# compute integral\n\twidth /= (bin_size + 1)\n\tcounts_grid = np."}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n# ----------------------------------------------------------------------------\n# Copyright (c) 2013--, scikit-bio development team.\n#\n# Distributed under the terms of the Modified BSD License.\n#\n# The full license is in the file COPYING.txt, distributed with this software.\n# ----------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\n\nimport time\nimport logging\nimport threading\n\ntry:\n    from future.utils import enum\nexcept ImportError:\n    enum = None\n\nfrom. import _log\nfrom._version import __version__\n\ntry:\n    xrange  # python 3\n    from io import StringIO\nexcept ImportError"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\tidx = seq.get_idx_by_name(\"z\")\n\tsource = seq.get_inst_by_idx(idx)\n\tif source is None:\n\t\traise ValueError(\"Source sequence '%s'\" % seq)\n\treturn source.is_fill_request_seq()\n\n\n#!/usr/bin/env python\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at https://mozilla.org/MPL/2"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\tif not seq:\n\t\t# the string was not a CDATA\n\t\treturn False\n\t\n\t# if seq does not contain an empty sequence or all whitespace is\n\t# present in the string, all whitespace is tested before.\n\tif StringTools.starts_with_whitespace(seq):\n\t\treturn True\n\t\n\treturn False\n\n##\n# Check if a sequence is a Sequence in the list\n#\n# @param seq the The sequence to be checked\n# @return true if it is a sequence or not\ndef is_sequence_sequence(sequence):\n\t\"\"\"\n\tTest whether"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\ndef set_type_and_fill_request_seq(seq_type, seq_data):\n\tif type(seq_type)!= int or type(seq_type)!= long or type(seq_data)!= dict:\n\t\treturn\n\t\n\treturn seq_type, seq_data\n\ndef print_types(seq_types):\n\tfor seq_type in seq_types:\n\t\tprint seq_type\n\t\n\treturn seq_types\n\ndef print_fields(seq_fields, seq_data, seq_type):\n\tfor seq, fld in seq_fields"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\ndef get_fill_request_seq(name, description=[], sequence=[], required=True):\n\t# TODO: is this an in-place change required?\n\ndef get_fill_request_seq_description(name, description=[]):\n\t'''\n\tGet a description of a fill request sequence.\n\t'''\n\treturn \"fill_seq\" if name in description else \"fill_seq_seq\", name\n\nclass FillRequestSeq(object):\n\tdef process_app(self, app, **kw):\n\t\tfrom flaskext.bunch import Bunch\n\t\t\"\"\""}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\n\t# For a sequence with a list representing the fill request,\n\t# and a list representing the source's fill request sequence.\n\t# Note: This test case is not valid for the constructor.\n\t\n\tfill_seq = []\n\tif hasattr(seq, 'fill'):\n\t\tfill_seq = [ seq ]\n\t\n\treturn len(fill_seq) == 0 and len(seq) > 1 and seq[0].fill == None\n\n#\n# Elements of a tree node.  Elements are of type T node\n# objects.  There are two children:\n#   "}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\ttype = seq.type\n\t\n\tif isinstance(type, Element):\n\t\treturn True\n\t\n\tif isinstance(type, Sequence):\n\t\tif isinstance(seq, StringDataSpec):\n\t\t\tseq = seq.val\n\t\n\t# we're not at the end of the original sequence\n\treturn TypeDef(type, None, seq) == \"fill_request_seq\"\n\n\n\ndef build_fill_request_seq(fill_request):\n\ttry:\n\t\tfill_request_seq = _get_by_string_data(fill_request)\n\texcept XMLError as err:"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\nclass FillRequestNode(Clinic.TagFilterNode):\n\tdef __init__(self, *k, **kw):\n\t\tsuper(FillRequestNode, self).__init__(**kw)\n\t\tself.is_fill_request = 1\n\n\tdef should_process(self, context):\n\t\treturn context.source_context.source_context.fill_count >= 1\n\n\tdef will_process(self, queue):\n\t\treturn queue.fill_count >= 0\n\n\tdef process(self, queue, i, j):\n\t\tnode = BNode()\n\t\tif node."}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\tres = True\n\t\n\tif seq.SourceType == 'FillRequest':\n\t\treturn False\n\t\t\n\tseqs = FillSeq.objects.filter(\n\t\tsource_sequence=seq,\n\t\torder_by='position'\n\t)\n\t# Are any seqs a source sequence?\n\tif seqs:\n\t\tfor seq in seqs:\n\t\t\tif seq.IsSourceSequence:\n\t\t\t\tres = False\n\t\t\t\tbreak\n\t# Not a source sequence\n\telse:\n\t\treturn res\n\ndef get_fill_request_srcnames(seq_order, seq):\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\tif isinstance(seq, DocumentSequence) or isinstance(seq, TextDocumentSequence):\n#\t\tprint \"Skipping FillRequestSeq: \" + seq.get_as_xml()\n\t\tdoc_seq = seq\n\telse:\n#\t\tprint \"Skipping FillRequestSeq: \" + seq\n\n\tfor child in seq:\n\t\tif not isinstance(child, Element) or not child.get_as_xml():\n\t\t\treturn False\n\treturn True\n\n\n##########################################################\n## Test case class to handle the Element\n##\n##########################################################\t\nclass ElementTestCase(unittest.TestCase):"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n#\tThe following code implements the code that allows us to\n#\tcreate Fill requests but also to create a Source\n#\telement, or a string element, and a special list of fill\n#\trequests. By using the above code we create a\n#\tFillRequestSeq object. For more details and other information\n#\tabout the functionality see SimpleFillRequests.\n#\n#\tYou will need a function named'make_one_fill_request',\n#\twhich allows you to create and handle the fill requests.\n#\n# It also provides the following method(s) and returns"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\ndef get_fill_request_el(obj):\n\t\"\"\"\n\t'Fill' objects represent a fill. The objects can be any object that can be rendered by the fill element from the current page. \n\tWhen no fill element is available then the fill object will be the display for the document, regardless of a particular fill type.\n\t\"\"\"\n\treturn None\n\t\ndef get_fill_request_type(fill_type):\n\t\"\"\"\n\t'Fill' objects represent a fill. The objects can be any object that can be rendered by the fill element from the current page. \n\t\"\"\""}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\ndef is_fill_request_elm(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\t\ndef add_fill_request(fname, lname='', ln=''):\n\t\"\"\"\n\tReturn (fname, lname, ln) of the form\n\t'fname' (from the file name can be derived from file name or module name)\n\tand 'fname' is the name string\n\t'lname' (from the prefix 'ln')\n\tor empty string when the module name has no prefix\n\t\"\"\"\n\ts = fname"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\ndef fill_request(name, *args):\n\t\"\"\"\n\tFill function, which will replace the current element\n\t\"\"\"\n\tdef get_fill_request(self, cr, uid, args, context):\n\t\tcr.execute('select id from procurement_order  where id=%s', (args[0],))\n\t\tresult = cr.fetchone()\n\t\tif result[1]:\n\t\t\trequestlist = self.browse(cr, uid, result[0])\n\t\t\tfor line in requestlist:\n\t\t\t\tfill_request(name, *line)\n\t\t\treturn True\n\t"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\n\n\tif is_fill_request_el(obj):\n\t\tcmd_list, cmd_help = get_cmd_lst_(obj, is_command_show_el=False)\n\t\tif cmd_list is None or len(cmd_list) == 0:\n\t\t\tcmd_list = obj.get_full_cmd_list()\n\n\t\ttry:\n\t\t\t\n\t\t\tif len(cmd_list) >= 1 and cmd_list[0] == 'fill':\n\t\t\t\t\n\t\t\t\tcmd_list, cmd_help = get_cmd_lst_(obj, is_command_show_el="}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\tname = obj.__class__ if obj.__class__ else obj.__name__\n\t\treturn name in (b'fill', b'request')\n\tdef is_check_request(obj):\n\t\tname = obj.__class__ if obj.__class__ else obj.__name__\n\t\treturn name in (b'check', b'request')\n\tdef is_add_request(obj):\n\t\tname = obj.__class__ if obj.__class__ else obj.__name__\n\t\treturn name in (b'add_file', b'add_folder', b'add_module', b"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\tif is_python3_module(obj):\n\t\t\tname = 'fill_' + obj.__name__\n\t\telif is_java_data(obj):\n\t\t\tname ='request'\n\t\telse:\n\t\t\tname = 'fill_' + getattr(obj, '__name__', repr(obj))\n\n\t\treturn name\n\traise NotImplementedError(\"Cannot handle {type}\".format(type = type(obj)))\n\n\nclass ContextProcessor(object):\n\tdef __init__(self, context):\n\t\tself.context = context\n\n\tdef __setstate__(self, state):\n\t\tself.context = state"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\tc = obj.get(\"text\")\n\t\tc = c.strip().replace(\"$$\",'&')\n\t\treturn(c)\n\tdef is_draw_request_el(obj):\n\t\t\"\"\"\n\t\tObject contains executable methods 'draw'.\n\t\t\"\"\"\n\t\tc = obj.get(\"fill\")\n\t\tc = c.strip()\n\t\treturn(c)\n\tdef is_draw_rect_el(obj):\n\t\t\"\"\"\n\t\tObject contains executable methods 'draw'.\n\t\t\"\"\"\n\t\tc = obj.get(\"draw\")\n\t\tc = c.strip()\n\t\t"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn(obj.fill is not None or obj.request is not None)\n\n\ndef _compile_and_execute(code):\n\t\"\"\"Helper for Compilations.compile and CompiledObject.execute\n\t\n\tThis is copied from Python 3.6 version: Backport of compiler.py\n\t\"\"\"\n\timport compiler\n\treturn compiler.compile(code)\n\n\nif __name__ == '__main__':\n\tdef f(x):\n\t\treturn x\n\tg = f(4)(4)\n\tx = g()\n\tx()\n#    Copyright 2018 SAS Project"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\ndef get_fill_info(obj, obj2copy):\n\t\"\"\"\n\tIf the method name is defined in obj, then it also matches obj2copy['fillinfo'],\n\tand the obj2copy['fillinfo'] is returned.  Otherwise the object is copied\n\tto obj2copy and returns it.  Returns new object that has fillinfo appended to obj.\n\t\"\"\"\n\n\t\n# TODO: handle obj2copy\n\ttry:\n\t\tobj2copy = obj2copy.copy()\n\texcept AttributeError:\n\t\tobj2copy = obj2copy.copy(obj)"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn hasattr(obj, \"fill\") and callable(obj.fill)\n\n#\n# ------------------------------------------------------------------------\n#\n#        get_path_id:\n#\n# ------------------------------------------------------------------------\n\n#----------------------------------------------------------------------------\n#\n# class PathId (Command):\n#\n# ------------------------------------------------------------------------\n\nclass PathId (Command):\n\t\"\"\"\n\tClass to get path ids.\n\t\"\"\"\n\tdef __init__(self, id=-1):\n\t\tCommand.__init__(self)\n\t\tself.__id = id\n\n\t#----------------------------------------------------------------------------\n\tdef is_fill_request_el(self):\n\t\t"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\tiname = get_name(obj)\n\t\tif platform.system() == 'Windows':\n\t\t\treturn os.access(iname, os.X_OK)\n\t\telse:\n\t\t\treturn os.access(iname.split(':')[0], os.X_OK)\n\ndef get_name(_obj):\n\t\"\"\"\n\tGet object name.\n\n\t\"\"\"\n\ttry:\n\t\treturn _obj.__name__\n\texcept AttributeError:\n\t\tpass\n\n\treturn _obj.__class__.__name__\n\ndef get_path(obj, root):\n\t\"\"\"\n\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t return isinstance(obj.launch, basestring) and executable(obj.launch) == 0\n\ndef check_user_group_name(name):\n\t\"\"\"\n\tReturn user group's name or False if it's not a user group.\n\t\"\"\"\n\tif not name:\n\t\treturn False\n\ttry:\n\t\tu = list(get_system_users(only_names=[name], verbose=0))[-1]\n\texcept IndexError:\n\t\treturn False\n\telse:\n\t\tif len(u.users) >= 1:\n\t\t\treturn True\n\t\telse:\n\t\t\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\n\n\tassert(type(obj) is str)\n\n\treturn obj == 'run'\n\ndef get_run_el(func_call_name):\n\t\"\"\"\n\tRetrieve an execute function function node.\n\t\"\"\"\n\t\n\n\tel = et_tools.exec_command(func_call_name)\n\n\treturn el.split(':')\n\ndef run_el(obj):\n\t\"\"\"\n\tCreate a new module node.\n\t\"\"\"\n\t\n\tel = get_run_el(obj)\n\tel[2] = (obj, [])\n\n\ttree = et_tools"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\tif not os.access(obj, os.X_OK):\n\t\t\treturn False\n\t\ttry:\n\t\t\tos.unlink(obj)\n\t\texcept EnvironmentError, ex:\n\t\t\treturn False\n\t\treturn True\n\ndef is_in_path(dir):\n\tif os.path.isabs(dir):\n\t\treturn os.path.isfile(dir)\n\tif not os.path.isdir(dir):\n\t\treturn False\n\treturn True\n\ndef to_boolean(val):\n\tnew_val = val.lower()\n\tif str(new_val).upper"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn obj.__class__ == Run\n\nclass Virtualenv(Run):\n\t\"\"\"\n\tExecutables created by the Virtualenv environment\n\t\"\"\"\n\tname = 'virtualenv'\n\n\tdef __init__(self, *args, **kwargs):\n\t\tRun.__init__(self, *args, **kwargs)\n\t\tself.env = ''\n\t\t\"\"\"absolute path to VALKROOT\"\"\"\n\t\tself.env_root = ''\n\n\tdef set_envname(self, name):\n\t\t\"\"\":name if name is not an absolute path,\n\t\t:type name: string"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\ndef is_run_and_raise(obj):\n\t\"\"\"\n\tObject contains executable exception 'run'.\n\t\"\"\"\n\t\ndef is_use_run(obj):\n\t\t\"\"\"\n\tObject contains executable exception 'use'.\n\t\"\"\"\n\t\ndef is_use_or_use_run(obj):\n\t\"\"\"\n\tObject contains executable exception 'use', 'run'.\n\t\"\"\"\n\t\ndef is_use_except(obj):\n\t\"\"\"\n\tObject contains executable exception 'use', 'except'.\n\t\"\"\"\n\t\ndef is_use(obj):\n\t\"\"\"\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(obj, collections.Iterable):\n\t\n\t\traise TypeError, \"Can only collect objects that can be run together with is_exec_el_objects (not {}.\".format(type(obj)))\n\t\n\tif sys.version_info > (3,):\n\t\n\t\t# Python 3 does not have next...\n\t\tif not sys.version_info.major >= 3:\n\t\t\n\t\t\t# Python 3 accepts **kwargs\n\t\t\tkwargs = {\n\t\t\t\t'encoding': obj.__encoding__\n\t\t\t}\n\t\t\t\n\t\t\tif hasattr(obj, '___run_el_function__'):\n\t\t\t\tkwargs['executable']"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\n\t\n\treturn obj.get('run',{})\n\ndef get_compile_flags(program):\n\t\"\"\"\n\tGet compile flags which are not processed in the current executable\n\t@param[in] program dict\n\t@return: list of flags\n\t\"\"\"\n\t\n\t\n\td = {}\n\trun = get_run_attributes(program, ['run'])\n\t\n\t# flags\n\tflags = []\n\t\n\t# compile\n\tif 'cprogram' not in run:\n\t\tcompile_flags(run, d)\t\n\t\n\t# flags\n\tflags.extend(run.get('"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\tif hasattr(obj, 'run'):\n\t\t\tif obj.run._fname == '<stdin>':\n\t\t\t\treturn False\n\t\t\tif obj.run._fname == '<stdout>':\n\t\t\t\traise AssertionError('Run in process %s failed' % obj.run.__class__.__name__)\n\t\treturn True\n\tdef collect_outputs(self):\n\t\tseen_files = []\n\t\tfor x in self.outputs:\n\t\t\tif not x.is_run():\n\t\t\t\tif is_run_el(x):\n\t\t\t\t\tseen_files.append(x)\n\t\treturn seen_files\n\tdef get"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\n\t\n\tif getattr(obj, \"s\", None) is not None:\n\t\tobj.s.is_run = is_run_el\n\ndef merge_list(list1, list2):\n\t\"\"\"\n\tReturn the merge result of 'list1' with 'list2' merged into a new list.\n\t\"\"\"\n\tif not list1: return []\n\tr = []\n\tfor i in list2:\n\t\tr.append(i)\n\treturn r\n\ndef shellquote(s):\n\tif '\\'' not in s and \"'\" not in s:"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\nclass FP_fill_compute_el(Proc_el_base_init):\n\t\n\t#---------------------------------------------------------------------------\n\t# __init__\n\t#---------------------------------------------------------------------------\n\tdef __init__(self, name, args, callerName=\"unknown()\", typeId=-1):\n\t\t\"\"\"\n\t\t:type name: String\n\t\t:param name: The name of the component\n\t\t:type args: Array\n\t\t:param args: Array of arguments\n\t\t:type callerName: String\n\t\t:param callerName: The name of the calling entity\n\t\t:type typeId: Array\n\t\t:param typeId:"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\n#===============================================================================\n# Base class for all types of types we are working with,\n# specifically for the generic type.  A subclass of this is not\n# required.  The only difference is that the data (class,\n# struct, etc.) is actually an ordered class type.\n# It is sufficient for tools like numpy.array, numpy.ma.core, etc,\n# and that does not provide a mechanism for an out of band (and it is\n# easier to understand in the future :-)\n\n#===============================================================================\n# Utility functions for creating various vector and list types\n\n# TODO:"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\ndef is_fill_instance_el(obj):\n\t\"\"\"\n\tObject contains exe methods 'fill_instance' and 'fill_compute'.\n\t\"\"\"\n\t\ndef is_empty_element(obj):\n\t\"\"\"\n\tObject contains exe methods 'contains'.\n\t\"\"\"\n\t\n\t\n@conf.commands.register\ndef is_script(obj):\n\t\"\"\"\n\tObject contains code from 'is', but it could be anything\n\tlike a python expression.\n\t\"\"\"\n\treturn\n\n\n@conf.commands.register\ndef is_data(obj):\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\n\t\n\tdef __get_path(self):\n\t\t\"\"\"\n\t\tGetter for \"path\" property\n\t\t\n\t\t\"\"\"\n\t\treturn self._path\n\t\n\tdef __set_path(self, path):\n\t\t\"\"\"\n\t\tSetter for \"path\" property\n\t\t\n\t\t\"\"\"\n\t\tself._path = path\n\n\tpath = property(__get_path, __set_path)\n\t\n\t#def __get_width(self):\n\t\t#\"\"\"\n\t\t#Getter for \"width\" property\n\t\t\n\t\t#\"\"\"\n\t\t#return self._width\n\t\n\twidth = property(__get_width, __set"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\ndef is_fill_compute_el(obj):\n\t\"\"\"\n\tObject is a class with the following properties :\n\t\n\t* obj : a python object\n\t* value : a python object\n\t\"\"\"\n\treturn True\n\t\"fill\"\n\ndef is_in_group_of_group(group_of_name):\n\t\"\"\"\n\tReturn True if at least one element of group of given name is in group of group_of_name.\n\tIn that case return False.\n\t\"\"\"\n\treturn 1\n\ndef is_in_group_of_groups("}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\n\n\tobj_str = \"\"\n\tif ('object' in dir(obj)) and (obj.object == 'compute'):\n\t\tobj_str = obj.name+\".\"+obj.description.replace('\\n',' ').strip(\" \")\n\n\tif (obj_str.find(\"fill\")>0):\n\t\treturn obj_str.split('fill')[0].strip(\" \").strip(\"\\n\")\n\t\t\t\t\n\treturn obj_str\n\n\n# Function to run the analysis.\n\ndef run(data, n):\n\t# Calculate input.\n\n\tprint \"Beginning analysis.\"\n\tinput"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\n\n\t# initialize obj\n\tobj.fill_computed = False\n\tobj.compute_computed = False\n\t# initialize obj inputs\n\tobj.get_input_values = []\n\tif obj.fill_computed:\n\t\tobj.get_input_values.append(read_only=True)\n\telse:\n\t\tobj.get_input_values.append(name=obj.name_out[-1])\n\tobj.set_input_values()\n\tobj.get_output_values()\n\t# obj.write_code_before_set()\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\ndef is_copy(obj):\n    \"\"\"\n\tBoolean. Should be used to get whether or not 'copy' commands are\n\tcreated/copied.\n\t\"\"\"\n\n\ndef get_all_text_blocks_from_each_line():\n    \"\"\"\n\tReturns a list of text blocks forming a given line. \n\t\"\"\"\n\tblocks = []\n    lines  = file(file_name, 'r').readlines()\n    for i in range(len(lines)):\n        blocks.append(lines[i].rstrip()+'\\n')\n    return blocks\n\n\ndef is_empty_line_"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\ndef is_fill_array_type(obj):\n    \"\"\"\n\tObject contains type that all elements of the array in 'fill' or\n\t'compute' is element of the type 'array' class\n\t\"\"\"\n        \n        \ndef is_fill_vector_type(obj):\n    \"\"\"\n\tObject contains type that all elements of the array in 'fill' type\n\tand 'compute' is element of the type'vector' class\n\t\"\"\"\n\ndef is_fill_number_type(obj):\n    \"\"\"\n\tObject contains type that all elements of the array in 'fill' type"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\tcmd = obj.path\n\t\treturn (cmd[-1] == 'fill') and \\\n\t\t\t('-r' == obj.source) and \\\n\t\t\t(obj.flags & os_path_exists(cmd[-1]))\n\t\t\n\tdef is_fill_list(obj):\n\t\t\"\"\"\n\t\tBoolean expression: True if list is filled. \n\t\tReturn: True if list is filled. \n\t\t\"\"\"\n\t\tcmd_string = obj.cmdline\n\t\treturn (cmd_string[0] == 'fill') and \\\n\t\t\t('-r' == obj.source) and \\\n\t\t\t(obj"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\tif level == -1:\n\t\tlevel = len(d1)\n\t\n\tif level == -1:\n\t\tlevel = 1\n\t\t\n\tresult = {}\n\t\n\tfor k in d1[1:]:\n\t\tif k not in d2[1:]:\n\t\t\tcontinue\n\t\t\n\t\tif not k in d1[level+1:]:\n\t\t\tresult[k] = d1[level+1] + list(d1[level+1])\n\t\telse:\n\t\t\tif d1[level-1] in d2[level-1:]:\n\t\t\t\tresult[k] = d1["}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t    \n\td1 = d1.copy()\n\td2 = d2.copy()\n\t\n\tif level > 1 or level < -1:\n\t\treturn d2\n\t\n\td1.update(d2)\n\td2.update(d1)\n\t\n\tif level!= -1:\n\t\treturn d2\n\telse:\n\t\treturn d1\n\t\n\ndef symmetric_difference(a, b):\n\t\"\"\" \n\tReturn a dictionary with items from *a* and *b* not contained \n\tin *a* but the corresponding items are included in *b*.\n\t\n"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\tret = {}\n\n\tif level == 0:\n\t\tfor k, v in d1.items():\n\t\t\tret[k] = v[level]\n\t\treturn ret\n\t\n\tif level == 1:\n\t\tfor k, v in d2.items():\n\t\t\tret[k] = v[1:-1]\n\t\n\tif level == -1:\n\t\tret = copy.deepcopy(d1)\n\t\tl = list(ret.keys())\n\telse:\n\t\tfor k in d1.keys():\n\t\t\tif ret.has_key(k):\n\t\t\t\tl"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n     if level == -1:\n    \t\tlevel = 2\n     else:\n    \t\ttry:\n    \t\tlevel = int(level)\n    \t\texcept:\n    \t        level = 2\n     if (not __debug__):\n    \t   return {}\n\n\ttry:\n    \t\tif not d1:\n    \t\t\treturn d2[0]\n    \t\t\n    \texcept AttributeError:\n    \t\tif level == -1:\n    \t\t\t# first call, it is in val instead of d2 as if by_key\n\t   \t\tif not by_key:\n    \t\t\t\traise KeyError('key not present twice')\n\n    \tif not d2"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\treturn _diff(d1.get(level, default), d2)\n\t\n\ndef difference_update(d1, d2, new_d1, new_d2):\n\t\"\"\"\n\tReturn a dictionary that holds all *new* values in the *d2*\n\tdictionary with items from the *d1* changed in *new_d2*.\n\t\n\t*new_d1* and *new_d2* are recursively updated in *d2*,\n\tbut not the *old_d2* if it is present in both dictionaries.\n\t\n\t*old_"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\t# We cannot just iterate over a dictionary of keys, i.e. d1[k1][k2][k3]\n\tif level == 0:\n\t\t# we're at the top level\n\t\treturn {**difference(d1, d2, 0)}\n\t\n\t# Since a key may be missing, the returned result is more complex with\n\t# some extra attributes. To be specific to each key, we'll\n\t# remove these for each key and return its result.\n\tif key in d1:\n\t\t# we can safely remove them\n\t\tif not list("}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t return _copy_dict(dict_union(d1, d2), level)\n\tresult = [d1]\n\tif isinstance(d1, list) or isinstance(d2, list):\n\t\tresult += _flatten(d1)\n\telif isinstance(d1, dict):\n\t\tresult += _copy_dict(d1, d2)\n\telse:\n\t\traise TypeError('unsupported type for d1: {0!s}'.format(type(d1)))\n\treturn _flatten(result)\n\n\ndef intersection(*p):\n\t\"\"\"\n\tReturn a"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\t# recursion is not always allowed.\n\tif not isinstance(d1, dict) or not isinstance(d2, dict):\n\t\tif level == -1:\n\t\t\t# d1 and d2 are already equivalent.\n\t\t\treturn d2\n\t\tif level < 0:\n\t\t\t# do the most efficient way than recursion from the outside.\n\t\t\tif d1 == None:\n\t\t\t\treturn d1\n\t\t\telse:\n\t\t\t\treturn d2 \n\tresult = {}\n\tif d1 is None:\n\t\tresult[d2] = d1\n\telse:\n\t\tresult["}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\tif level < 0:\n\t\t# no point in traversing (max recursion depth)\n\t\treturn d2\n\t\n\tk1a = [d1]\n\tk2a = [d2]\n\t\n\tfor item in d2:\n\t\tif item in k1a:\n\t\t\t# there exists a dictionnary; just leave it\n\t\t\treturn d2\n\t\t\n\t\telif item in k2a:\n\t\t\t# there exists, but its not a dictionnary:\n\t\t\treturn d1\n\t\n\tif level == 0:\n\t\t# remove all items from the sub dictionaries"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\tif level == -1:\n\t\treturn d1\n\t\n\telif level < 0:\n\t\treturn d2\n\n\telif level % 2 == 0:\n\t\treturn d1\n\t\t\n\telse:\n\t\treturn dict((x, y) for x, y in izip(d1.items(), d2.items()))\n\n# Copyright 2012 Nebula, Inc.\n# Copyright 2013 IBM Corp.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\nclass UniformGrid(AbstractGraph):\n\t\"\"\"\n\tUniformly sample uniformly at *coord*.\n\t\"\"\"\n\tdef __init__(self, n=5, xlim=(0.5, 10.)):\n\t\t\"\"\"\n\t\t:param n: Number of samples to draw.\n\t\t:param xlim: Tuple containing minimum and maximum values.\n\t\t\"\"\"\n\t\tself._n = n\n\t\tself._xlim = xlim\n\t\t\n\tdef draw(self, coord):\n\t\t\"\"\"\n\t\tDraw samples from uniform distribution *coord*.\n\t\t:param coord: Locator along sample"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t # Note: this works by starting at the start of the\n\t # histogram. The start index is the zeroth (most common value)\n\t # which isn't the mean, as the first bin is assumed to be 0...\n\t #\n\t\t# NOTE: this assumes that any histogram points start at +-0.5,\n\t # or the bin is zero!\n\t #\n\t\t# Returns a copy of the histogram with the same bins, including\n\t\t# a new histogram of zeros on edge zero.\n\t\tif self.length < self.bounds[coord]:\n\t\t\treturn self\n\t\tbins = self.bins"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t \n\tself.tk.call(\"grid\", \"create\", \"array\", coord, weight)\n\tx = self.tk.call(\"node\", \"create\", \"weight\", self._weightId)\n\ty = self.tk.call(\"node\", \"create\", \"weight\", self._weightId)\n\tself.create_line(x, y, weight)\n\tself.create_line(x, y+2, weight)\n\tself.create_line(x, y+3, weight)\n\t    \n\tfor r in range(0,1):\n\t   y ="}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\n\tminX = self.min()\n\tmaxX = self.max()\n\t\n\t# Get the range\n\tminRange = maxX - minX\n\n\n\tif weight < minRange:\n\t\traise Exception, \"The weight is below minRange\"\n\t\n\t# Set the values\n\tval = (1 - weight) * coord\n\tcount = 0\n\tfor i in xrange(len(self) - 1, -1, -1):\n\t\tif self[i] < coord:\n\t\t\tval = val + weight\n\t\t\tcount += 1\n\t\n\t# Create the new histogram object\n"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t \n\tif isinstance(coord, string_types):\n\t\tif coord not in self.data:\n\t\t\tself.data[coord] = {}\n\t\tself.data[coord][weight] = 1\n\t\treturn True\n\telif isinstance(coord, int):\n\t\tif not self.exists(tick=self.tick):\n\t\t\tself.declare()\n\t\tself.data[coord] = {} # reset counter\n\t\t\n\telif isinstance(coord, (list, tuple, np.ndarray)):\n\t\tif type(coord[0])!= int or type(coord[1])!= int:\n\t\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\t\n\tif self.data!= None:\n\t\ty = int(self.data.shape[0]/2)\n\t\tfor i in range(y):\n\t\t\tself.data[:, i] = self.data[:, i]/weight\n\ndef make_hist(hist, fillval):\n\thist.shape = (hist.shape[1],  hist.shape[0])\n\treturn [ hist[:-1] ]*hist[1:]\n\ndef histogram_from_histogram(hist, hist2, bins=(5, 6, 10)):\n\tdata, count = binhist(hist"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tcoord = self.GetCoord(coord)\n\t\tif self.h_edges[coord] > 0 and weight!= -1.0:\n\t\t\tself.h_edges[coord] -= weight\n\t\t\tself.draw_histogram(coord, weight)\n\t\t\n\tdef fill_hist_of_edges(self, hist, n=0):\n\t\t\"\"\"\n\t\tFill a histogram into the given histogram.\n\t\t\n\t\thist: number of entries to fill\n\t\tn: number of entries to fill before creating hist\n\t\t\"\"\"\n\t\tif n == 0: return hist\n\t\tif n > len(self"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tassert coord.shape == self.dim, \\\n\t\t\t\"Coordinate dim should be same as shape ( {}, {})\".format(coordinate.shape, coord.ndim)\n\t\tif coord.shape[0] == self.dim:\n\t\t\tif self.max:\n\t\t\t\tfor i in range(self.size):\n\t\t\t\t\tself.hist[i, i] = weight\n\t\telse:\n\t\t\tself.hist[coord, coord] = weight\n\nclass VigraArray(ndarray):\n\t\"\"\"\n\tA ``ndarray`` subclass instance that stores the data in a *vgrid* "}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\nclass Histogram(object):\n\t\"\"\"\n\tClass representing a histogram.\n\t\n\tHistorically, you can divide a list of historic (like a histogram) into\n\thistograms.\n\t\n\tA histogram must have the properties:\n\t\n\t- All have the properties tagged'start' (to show each of multiple bins),\n \t\t- All have the properties tagged 'binsize' (to show the bin-sizes)\n\t- A 'count' attribute that counts all bins' as a histogram.\n\t\"\"\"\n\t\n\tdef __init__(self, historic, bins):\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\nclass Interpolation:\n\tdef __init__(self, coord):\n\t\tassert isinstance(coord, (tuple, list))\n\n\t\tassert len(coord) in [2, 3]\n\t\tassert len(coord[2]) == 3\n\n\t\tself.coord = coord\n\t\tself.size_x, self.size_y = self.coord\n\n\t\t# Interpolation:\n\t\tself.left = (self.coord[0][0], self.coord[0][1])\n\t\tself.right = (self.coord[1][0], self.coord[1][1])\n\t"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif type(labels) not in (type([]), type(())) or(len(labels)>2 or len(labels)==0):\n\t\t\traise ValidationError('labels must be a dictionary with at least \"key\" and \"value\" keys.')\n\t\tif len(labels)!=4:\n\t\t\traise ValidationError('labels must be a array of length \"4\".')\n\t\tif set(labels.keys())!=set(['*']):\n\t\t\traise ValidationError('value keys must be distinct.')\n\t\tif set(labels.values())!=set(['']):\n\t\t\traise ValidationError('value values must be distinct.')\n\t\t\n\n\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\n\tif not isinstance(labels, dict):\n\t\traise ValidationError(\"expected dict but got {}\".format(type(labels)))\n\t\n\tif labels:\n\t\tfor k, v in labels.items():\n\t\t\tk = k.lower()\n\t\t\tif not re.match('^' + k + '$' + '[_a-z]' + '[' + k + ']' + v, message=k +'is not a label.'):\n\t\t\t\traise ValidationError(\"{} is not a label.\".format(v))\n\telse:\n\t\traise ValidationError(\"expected empty labels dict or None.\")\n\n\"\"\"\nIn this module, we"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\ndef check_for_errors(config):\n\t\"\"\"\n\tMake sure that any of the following sections are present:\n\n\t      - data directory\n\t      - settings file.\n\t      - configuration file.\n\t      - logging directory\n\tThey do not apply to the validation of values for certain settings and the\n\terror handling from each setting is done here.\n\t\"\"\"\t\n\tfor required_key in ( \"DATA_DIR\", \"LOGGING_DIR\", \"LOG_DIR\", \"LOG_FILE\" ):\n\t\t\n\t\tif not os.path.isdir( required_key ):\n\t\t\tif required_"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t    \n\tdef wrapper(func):\n\t\tkeys, vals = labels.iteritems()\n\t\terrors = {}\n\t\t\n\t\tif args:\n\t\t\tassert len(args) == len(vals), 'at least two arguments expected'\n\t\t\tfor k, lv in zip(keys, vals):\n\t\t\t\texpected = re.search(regex, lv).groupdict()\n\t\t\t\tmatches = [k for k, v in expected.iteritems() if v == args.get(k, {})]\n\t\t\t\tassert all(matches), '{} did not match'.format(v)\n\t\t\t\tfor name, err in errs.iteritems():"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\ndef validate_ports(*ports):\n\t\"\"\"\n\tValidates that all of the provided ports conform to the specified rules\n\tas defined by an IP address, or range of ports.\n\t\n\tArgs:\n\t    ports (dict): the port to validate\n\n\tReturns:\n\t    list: a list of errors that occurred during validations.\n\t\n\t\"\"\"\n\tif isinstance(ports, list):\n\t\t# Make sure all the ports have the same length\n\t\tports = [{\n\t\t        \"from\": port[\"from\"],\n\t\t        \"to\": port[\"to\"]\n\t        } for port in ports]\n\t\n\tif"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\t\n\tdef validate(self, data, **kwargs):\n\t\t\"\"\"\n\t\tValidate a label instance.\n\t\t\n\t\tReturns a dictionary of the validated validation data.\n\t\t\n\t\tSee docs for each of the fields that could be present in the instance and its\n\t\tcontents.\n\t\t\"\"\"\n\t\t\n\t\tinstance = super(Field, self).validate(data, **kwargs)\n\t\t\n\t\ttry:\n\t\t\tif 'key' in self.errors and isinstance(self.errors['key'], dict):\n\t\t\t\t# We raise an error if `key` is a dict.\n\t\t\t\tfor key, validation in self.key.items():"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\t\n\tdef _validate_string(string, labels):\n\t\t\"\"\"\n\t\tValidate that a string is a dict valid from the given labels dictionary.\n\t\t\n\t\tArgs:\n\t\t    string (basestring): the label to validate.\n\t\t\t\n\t\t\tlabels (dict): the different labels to validate with.\n\t\t\"\"\"\n\t\tcheck_all_keys(labels)\n\t\t\n\t\t# Check that only one label key is included\n\t\tkey = labels.keys()[0]\n\t\tif not key.startswith('$$') or not is_string(labels[key]):\n\t\t\treturn True\n\t\t\n\t\t# Check that the key matches our regular expressions\n\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\t\n\tfor key, val in labels.items():\n\t\ttry:\n\t\t\tif not (key in _valid_label_regexes and val == ''):\n\t\t\t\traise Exception\n\t\t\t\tassert True\n\t\t\telse:\n\t\t\t\tif callable(val):\n\t\t\t\t\tval = val()\n\treturn _VALID_LABELS\nfrom random import randint\n\nfrom common.methods import set_progress\n\n# TODO:\n# Add support for different game layout\n\nclass Board:\n    def __init__(self):\n        self.state = \" \"\n        self.board = []\n        self.cur_player = 0"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\n\tif not 'key' in labels:\n\t\traise ValidationError(\"The labels parameter has a key\")\n\tif not 'value' in labels:\n\t\traise ValidationError(\"The labels parameter has a value\")\n\n#!/usr/bin/env python\n\n# Copyright (c) 2015,  BROCADE COMMUNICATIONS SYSTEMS, INC\n\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# 1. Redistributions of source code must retain the above copyright notice,\n# this list"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\n\t\n\tassert set(labels.keys()) <= set(VALID_LABEL_KEYS)\n\t\n\t# No need to check the keys\n\tif not labels.keys() == VALID_LABEL_KEYS + []:\n\t\traise ValidationError(\"Missing labels: {}\".format(labels.keys()))\n\t\n\t# Check the list of all errors\n\tfor key, label in labels.items():\n\t\tassert label.keys() == VALID_LABEL_KEYS + [key]\n\t\n\t# Check the error messages\n\tfor key in labels:\n\t\temsg = \"\"\n\t\tfor key_error in labels.keys():\n\t\t\t"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t    \n\treturn [\n\t        re.compile(r'^[a-zA-Z0-9_!@$%^&*_=+,-]*$'),\n\t        re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')\n\t]\n\t\ndef _get_resource_name_re():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (string): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t    \n\treturn ["}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\n\treturn (\n\t\t\t\t\t\tre.compile(\"\"\"^[a-zA-Z0-9]+$\"\"\")\n\t\t\t\t\t| \tre.compile(\"\"\"^[a-zA-Z0-9]+$\"\"\")\n\t\t\t)\n\n\ndef _validate_resource_name(name):\n\t\"\"\"\n\tValidate a resource name against a regular expression.\n\t\n\tArgs:\n\t    name (str): resource name to validate (a regular expression,\n\t\t    of the form ``name/resource``).\n\t\n\tReturns:\n\t    bool: ``True`` if resource name is valid and valid.\n\t\"\"\"\n\n\ttry:\n\t\t"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\t# the regular expressions are compiled to regular expressions\n\t# without the.py extension\n\ttry:\n\t\tunicode\n\texcept NameError:\n\t\t# python 3\n\t\tpass\n\t\n\t# search for all the files matching the name\n\tfile_path = os.path.join(os.path.dirname(__file__), \"static\", \"resources\")\n\tfiles = []\n\t\n\tfor line in open(file_path):\n\t\tm = regexp.match(line.strip())\n\t\tif m:\n\t\t\tfiles.append(os.path.normpath(m.group(1)))\n\t\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t        \n\treturn __regex_obj__.search\n\ndef _get_resource_id_regex():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe ID of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource ID.\n\t\"\"\"\n\t\ndef _get_resource_name_regex2():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\tglobal RENAME_REGEX\n\t# First compile the regex once per class.\n\tRE_CLASS = re.compile(\"([A-Za-z]+)=\\?\")\n\t\n\t# Find a class-name by name.\n\tRE_NAME = re.compile(NAME_REGEX, re.I)\n\t\n\t# Search through each class.\n\tfor m in os.listdir(NAME_REGEX):\n\t\t# Check if this is a class-name.\n\t\tif m.find('.') == 0:\n\t\t\tcontinue\n\t\t\n\t\t# Is this an actual class name?\n\t\tif re.match"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\t# First, pull the basic information out of this file.\n\t# This regular expression is a list with the key that it says to\n\t# use to search in the.properties file and parse the data.\n\tproperties_filename = os.path.join(constants.KRAKES_ROOT_PATH,\n\t\t'source/kraken_properties.properties')\n\ttry:\n\t    properties_file = open(properties_filename)\n\texcept IOError:\n\t    raise OSError('Could not find the %s file.' % properties_filename)\n\t\n\t# Now we tokenize the content of"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\t# Define the compiled regex for the resource name\n\t# (e.g. \"awslogs|my-awseu-logs|my-awseu-awseu-logs#\")\n\tpattern = re.compile(r'.*s$|^-s$', re.IGNORECASE)\n\t\t\n\t# Define the compiled regex for the alphanumeric resource name\n\t# that will be used to validate that a name is not in a \"alphanumerical characters\"\n\tregex = re.compile(r'\\w{2}_?(\\d)')\n\t\n\t# Define the compiled regex for the alphanumeric resource name\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t# The regular expression to match any known Krake components.\n\tname_re = re.compile('([^/]{1,10})\\.[^/]{1,10}\\Z')\n\t# The optional name which we'll use to detect when parsing the resource\n\t# name (if it hasn't been specified yet).\n\tkey_re = re.compile('([^\\0]+)=([a-z0-9+-]+)\\Z')\n\t# A regular expression for filtering name/regex components.\n\tname_attr_re = re.compile('([^\\0]+)=(.*)$')\n\t\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\treturn [\n\t\t# re.compile(\"Krake\"),\n\t\t# re.compile(\"Namibia\"),\n\t\t# re.compile(\"Konstorjin\"),\n\t\t# re.compile(\"Kwacha\"),\n\t\t\n\t\t\n\t\tre.compile(\"(.*) \\(Kamak)(.)(\\w+ )?(\\s*\\([0-9]*)?([a-zA-Z]*)+\"),\n\t\tre.compile(\"(?:.*?) (\\^|\\))\"),\n\t\tre.compile(\"([^:]+\\.)(.*) (\\..+).*\"),\n\t\t]\n\t\n\t\n\n# Define"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\n\t# We allow the regex to reference the name because it's\n\t# compiled from a config file. It is only allowed for the\n\t# \"main\" resource name, as the name for a particular\n\t# resource is not an attribute.\n\tpattern = _STRING_REGEX\n\n\tdef __init__(self, resource_name, resource_pattern, *extra_args, **kwargs):\n\t\t\"\"\"\n\t\tInitialize the :class:`KrakowskiResource` object.\n\t\t\n\t\t:param string resource_name: The name of a pattern, in this file\n\t\t    format.\n\t\t:param"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tif value == '' or not value:\n\t\traise ValidationError('empty string')\n\tif value.lower() in ('true','1'):\n\t\treturn True\n\telif value.lower() in ('false','0'):\n\t\treturn False\n\telse:\n\t\treturn value\n\t\ndef validate_integer(value):\n\t\"\"\"\n\tCheck if the value is an integer.\n\t\n\tArgs:\n\t    value: a string\n\t\n\tRaises:\n\t    ValidationError: if value is not a whole number.\n\t\"\"\"\n\t\n\tif '.' in value:\n\t\ttry:\n\t\t\tinteger = int(value)"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tdef validator(value):\n\t\tif not isinstance(value, (str, unicode)):\n\t\t\traise ValidationError, value\n\t\n\treturn validate_regex(r\"^[\\w\\.\\-/]+$\", value, maxlength=50, allow_dash=True)\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_regex(regex):\n\t\"\"\"\n\tVerifying the given regular expression against the content of a compiled regular expression.\n\t\n\tArgs:\n\t    regex: the regular expression for which to validate the value\n\tRaises:\n\t    ValidationError: if regex is not a compiled regular expression\n\t\"\"\"\n\t\ndef generate_regex():\n\t\"\"\"\n\tGenerates a compiled regular expression from a given string.\n\t\n\tReturns:\n\t    A compiled regular expression.\n\t\"\"\"\n\tcompiled_regex = re.compile(regex)\n\t\n\treturn compiled_regex\n\ndef generate_regex_text():\n\t\"\"\"\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n    assert isinstance(value, (str, unicode)), \"must be a string\"\n\n    value = value.strip()\n    \n    if not value:\n        raise ValidationError(\"empty\")\n    \n    if not REGEX_VALID_RE.match(value) is None:\n        raise ValidationError(\"invalid %s\" % value) \n    \n    value_obj = REGEX_VALID_RE.match(value)\n    if not value_obj:\n        raise ValidationError(\"invalid %s\" % value) \n    \n    try:   \n        return value_obj.stringify()\n    except Exception, e:\n        e.message = unicode(e.message"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n##############################################################################\n##############################################################################\n# Private routines used in the parsing framework                               #\n#############################################################################\ndef extract_fields(string):\n\t\"\"\"\n\tSeparate lines in the CSV file\n\t\"\"\"\n\t\n\tif len(string) > 8:\n\t\treturn string[:8], string[8:]\n\telse:\n\t\treturn None, None\n\ndef validate_csv(file_path, value_dict):\n\t\"\"\"\n\tValidate the given value of the given CSV file against each of the given dictionary.\n\t\n\tThe values of the fields are validated (for those fields listed in the file)\n\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\n\t_reals, _groups, _replications, _rep_strings = True, [], [], []\n\t\n\tif _reals:\n\t    # We've got a comma and have to look at them later\n\t    if \",\" in value:\n\t\t# The comma matches its arguments by value.\n\t\tparts = value.split(\",\")\n\t\tvalue = parts[0]\n\t\t# The actual group match was matched: first group\n\t\tfor entry in parts[1:]:\n\t\t    # The value must match our entry (if match)\n\t\t    # then the group name must match the name specified in the entry\n\t\t    # and then"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tcomp_expr = ''\n\t\n\tfor ch in value:\n\t\tif ch == '#' or not (2 <= ch <= 5 or ch ==''):\n\t\t\tcontinue\n\t\t\n\t\tcomp_expr += ch\t\n\t\tif ch == '*':\n\t\t\tcontinue\n\t\t\n\t\tcomp_expr += '.'\n\ndef is_in_scope(user, scope):\n\t\"\"\"\n\tCheck whether the given user matches the given scope\n\tby ensuring that it contains all the user attributes in the \n\treturn value.\n\t\n\tArgs:\n\t    user: the current user instance\n\t    scope: the scope instance\n\t\n\tReturns:\n\t   "}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tif not match_any_pattern(regex.compile('^\\w+$'), value, re.I):\n\t\traise ValidationError(_('Not a valid value: \"%s\"') % value)\n\t\n\t\n\t# Validate the length of the given value\n\tfor (key, regex) in list(settings.VALID_CHARS.items()):\n\t\tif len(key)!= len(regex):\n\t\t\traise ValidationError(_('Values should be all alphanumerics of \"A-Za-z0-9_\\\\.-_=#[]^_`\"'))\n\t\telse:\n\t\t\tvalidate_length(value, regex"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate(value):\n\t\"\"\"\n\tCheck if the given value matches the regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value does not match the regular expression.\n\t\"\"\"\t\n\t# Use the regular expression parser to get some values.\n\tif not re.match('^[a-zA-Z_][a-zA-Z0-9_]*$', value):\n\t\traise ValidationError(u\"The pattern's value should be set to match the regular expression of the pattern.\")\n# Copyright (c) 2015 Miranti"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\t\n\tif not RegexValidator.isValid(regex, value, None):\n\t\traise ValidationError, \\\n\t\t\tgettext(\"The given value %(value)r of variable %(name)r is not valid.\" % \\\n\t\t\t{ \"value\" : value, \"name\" : value } )\n\t\ndef validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tReturns:\n\t    A tuple, where the first item is the key, and the second item is the regular expression.\n\t\"\"\"\n\t\ndef validate_"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\t# validate key\n\tif is_regex.match(key):\n\t\traise ValidationError(\"Invalid regular expression: '%s' contains invalid characters.\" % key, None)\n\t\n\t# validate regex\n\tif not (VALID_REGEX.match(key) is None or VALID_REGEX.match(VALID_REGEX.search(key)) is None):\n\t\traise ValidationError(\"Invalid regular expression: '%s' contains invalid characters.\" % key, None)\n\ndef is_match(pattern, value):\n\t\"\"\"\n\tCheck whether a string matches a pattern.\n\t\n\tArguments:\n\t    pattern: the"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\n\t\ndef password_hash(password):\n\t\"\"\"\n\tCompute the hash and check for validity.\n\t\n\tArgs:\n\t    password: the password to hash\n\t\t\n\tRaises:\n\t    ValidationError: if the given password string is not valid.\n\t\"\"\"\n\tif len(password) < 32:\n\t\traise ValidationError(\"'password' must be at least 32 characters long\")\n\t\t\n\thash_str = generate_salt()\n\thash_result = hash_str[:4] + password\n\thexdigest_result = hash_result[:5] + password\n\tresult = int(hashlib.sha512(hash_"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tregex = re.compile(r'^[\\s0-9a-zA-Z][\\s\\-=]+)$', re.UNICODE)\n\t\n\tif not regex.match(key):\n\t\traise ValidationError('Invalid key')\n\n\nclass AccountForm(Form):\n\t\"\"\"\n\tAccount form.\n\t\n\t\"\"\"\n\t\n\tname = forms.CharField(max_length=255)\n\t\n\tdef clean_name(self):\n\t\t\n\t\tval = self.cleaned_data['name'].strip().lower()\n\t\t\n\t\tif not val:\n\t\t\traise ValidationError('A label cannot be blank')\n\t\t\n\t\tif Name."}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif key == None:\n\t\traise ValidationError(\"Required key cannot be None\")\n\tif len(key) == 0:\n\t\traise ValidationError(\"Required key cannot be an empty string\")\n\tif key[0]!= '.':\n\t\traise ValidationError('The key must be a string, not \"%s\"' % key)\n\t\n\tif valid_key(key):\n\t\traise ValidationError('Validation error: \"%s\" matches key \"%s\"' % (key, valid_key(key)))# coding=utf-8\n\"\"\"\nThe Base class for objects representing common database objects\n\"\"\"\nimport random"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t    \n    assert 0 < len(key), \"Key must have length >= 0\"\n\n    if not regex.match(\"^[a-zA-Z0-9][a-zA-Z0-9_]*$\", key): \n        raise ValidationError(\"invalid key: '{key}'\".format(key=key)) \n    \n    \n\"\"\"\n==============================\nForm validators\n==============================\n\nEach validator takes a validator callable argument that takes no arguments and returns a validator.\nThe validator is called in one of two ways. The first argument is a callable providing a message, and the second argument is the message. This validator accepts no arguments, but"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_user(user):\n\t\"\"\"\n\tCheck if the user with a matching user exists in the system.\n\t\n\tArgs:\n\t    user: the user to validate\n\t\n\tRaises:\n\t    ValidationError: if the given user already exists in the system.\n\t\n\t\"\"\"\n\t\t\t\ndef validate_group(group):\n\t\"\"\"\n\tCheck if the given group is a valid system group.\n\t\n\tArgs:\n\t    group: the system group name\n\t\t\n\tRaises:\n\t    ValidationError: if the given group is not a valid system group.\n\t\n\t\"\"\"\n\t\t\t\ndef validate_permission_name(permission_name):\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_all_fields(self):\n\t\"\"\"\n\t Checks that all provided fields are actually present.\n\t\t\n\t\tAll required fields are guaranteed to validate.\n\t\t\"\"\"\n\t\t\t\n\tdef validate_field_groups(self):\n\t\t\"\"\"\n\t\tEnsures proper structure of all field groups\n\t\t\"\"\"\n\t\t\n\tdef validate_field_types(self):\n\t\t\"\"\"\n\t\tReturns a dictionary of field groups.\n\t\t\n\t\tValid fields are required.\n\t\tThe keys of the list are the field type names.\n\t\t\"\"\"\n\t\t\n\tdef validate_field_groups_and_fields(self):\n\t\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t    \n\t    if not validate_required_patterns([r\"^\\s*:*$\",  r\"^\\s*:admin$\",  r\"^\\s*:useradmin$\",  r\"^\\s*:adminuser$\", key]):\n\t\traise ValidationError(\"Pattern [%s][0] is invalid\" % key)\n\t\n\tif not validate_regex_pattern(\"^\\s*:*$\", key, r\"^\\s*:$\", r\"^\\s*:admin$\", key):\n\t    raise ValidationError(\"Key is invalid\")\n\t\n\tif not validate_required_patterns([\"^\\s*:*$\", key], r\"^\\s*:$\", key, r"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\t# If this doesn't validate successfully, it will return False.\n\tif key is None:\n\t\treturn False\n\t\n\t# Convert the key to a string\n\tkey = to_string(key)\n\t\n\t# Strip the leading quote or a backslash (to protect against a literal backslash)\n\tkey = key.strip(\"'\")\n\t\n\treturn key\n\ndef get_value_from_fields(fields):\n\t\"\"\"\n\tGet the raw value from a list of field names to be used in validating the \n\tvalue.\n\t\n\tArgs:\n\t    fields: a list of field names,"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t \n\ttry:\n\t\tif not re.match(r'^[A-Za-z0-9.\\-/]*$', key):\n\t\t\traise ValidationError('Incorrect characters in the key')\n\t\treturn True\n\texcept TypeError:\n\t\treturn False\n\t\n#################\n# JSON Serializers\n#\n#################\n\nfrom decimal import Decimal\n\nclass JSONEncoder(json.JSONEncoder):\n\t\n\tdef default(self, obj):\n\t\tif isinstance(obj, decimal):\n\t\t\treturn float(obj)\n\t\telif isinstance(obj, datetime):\n\t\t\treturn obj.strftime('%Y-%m-%"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(manifest_dict, dict):\n\t\traise ValueError(\"Manifest dictionary is not a dictionary\")\n\t\n\t# Generate default observer_schema dictionary\n\tif first_level:\n\t\tobserver_schema = {}\n\telse:\n\t\tobserver_schema = dict([])\n\t\t\n\t# Create a mapping from resource type to observer-schema\n\t# (including service type).\n\tobserver_schema['service'] = ('http://metadata/schema/service/v1beta1/metadata/service',)\n\tobserver_schema['name'] = ('http://metadata/schema/name/v1beta"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\t# first level: process this dict\n\tkey_value_dict = {'key': None, 'type': None}\n\tfor key in manifest_dict:\n\t\tif key in ['key', 'type']:\n\t\t\tkey_value_dict[key] = manifest_dict[key]\n\t\telse:\n\t\t\tkey_value_dict[key] = extract_value_from_list(manifest_dict[key])\n\t\n\t# process whole dict of ob servers\n\tfor key in key_value_dict:\n\t\tif key == 'observed':\n\t\t\ttry:\n\t\t\t\tobserved"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tif first_level:\n\t\treturn (ManifestDict({ 'kind'     : 'observer_schema',\n\t\t     'apiVersion' : API_VERSION,\n\t\t    'metadata' : [{'name'     : 'api_version', 'type' :'str', \n\t\t\t\t\t\t  'mode'     :'str'     ,'required' : True}],\n\t\t     'definitions' : {'kind' : 'observer_schema',\n\t\t\t\t\t\t\t 'apiVersion' : API_VERSION,\n\t\t\t\t\t\t\t'metadata' : []},\n\t\t     'patternProperties' : {'kind' : 'observer_schema',\n\t\t\t\t\t\t\t 'apiVersion"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\ndef generate_observer_schema_list(manifest_dict, first_level=False):\n\t\"\"\"\n\tTogether with :func:``generate_observer_schema_dict``, this function is\n\tcalled recursively to generate part of a default ``observer_schema_list`` from part of\n\ta Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\n\tcurrent_level = list(manifest_dict.keys())[-1]\n\t# Don't include any level dictionaries: the first level we're going to\n\t# generate that is just the root entry.\n\tif current_level == None:\n\t\tcurrent_level = list(manifest_dict.keys())[-1]\n\t\tdel manifest_dict[current_level]\n\n\tdef generate_observer_schema(content):\n\t\tif current_level not in current_level:\n\t\t\t# The current level's root entry will be used.\n\t\t\treturn\n\t\n\t\tdef extract_metadata("}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n    # If the file doesn't exist, return an empty dict.\n    if not os.path.exists(manifest_file):\n        if first_level:\n                return {}\n        else:\n                raise SaltInvocationError(\"Manifest could not be created. Check that \"\n                        \"file exists and try again.\")\n\n\t# Convert the list to a dictionary where the fields are the original\n\t# and the keys are the'resource.metadata.name' and'resource.metadata.namespace'\n\t# for this function to work.\n\tmanifest_dict = dict((k, v) for (k, v"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tbundle = {MANIFEST_KEY: manifest_dict}\n\t\n\tif first_level:\n\t\tbundle[MANIFEST_KEYS_KEY] = {}\n\t\tfor k, v in manifest_dict.items():\n\t\t\tif k in manifest_list:\n\t\t\t\tbundle[MANIFEST_KEYS_KEY][k] = v\n\t\t\t\t\n\t\t\t\t# Copy all keys found in the other dict, otherwise empty dict will be added.\n\t\t\t\tbundle[MANIFEST_KEYS_KEY][k] = dict(v.get_field_keys())\n\t\t\telse:\n\t\t\t\tbundle[MANIFEST_KEYS_KEY][k] ="}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tdict = {'all': dict_representer, \n\t\t\n\t\t'type': list_representer,\n\t\t'properties': {\n\n\t\t\t'properties': {\n\t\t\t\t'vel': {\n\t\t\t\t\t'type': int,\n\t\t\t\t\t'immutable': True,\n\t\t\t\t\t'type': str,\n\t\t\t\t\t'immutable': True,\n\t\t\t\t\t'desc': {\n\t\t\t\t\t\t'type': str,\n\t\t\t\t\t\t'tributeName': dict,\n\t\t\t\t\t\t'drawDef': lambda i: i.name,\n\t\t\t\t\t\t'mutable': True,\n\t\t\t\t\t\t'type': dict,\n\t\t\t\t\t\t'no_default': False,\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\t# I am not sure if what if empty dict was not sent to the Kubernetes API?\n\tif not first_level:\n\t\treturn\n\t\n\tdefault_observer_schema = {'schema_version': 1}\n\tschema_version_map = {MANIFEST_SCHEMA_VER: 1}\n\t\n\t# A little helper, to generate a new dict with additional fields (e.g.'version'\n\t# for the first level version of a Kubernetes resource which contains the current\n\t# version).  This would result in an empty dictionary (i.e.'version') if the version\n\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tdef get_observer_schema(resource, name, action, **kwargs):\n\t\t\"\"\"\n\t\tGenerate a ``observer_schema`` dictionary for a single ``observer_schema``\n\t\tpart of a Kubernetes resource, defined by ``resource``.\n\t\t\n\t\tIf ``first_level`` is true, the ``observer_schema`` are appended to the\n\t\tmanifest's ``observer_schema`` as well. Otherwise, if ``first_level`` is\n\t\ttrue, the result of ``getattr(observer_schema, name)`` will be set to the \n\t\tactual namespace of the resource.\n\t\t\n\t\tArguments:\n\t\t   "}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\ttry:\n\t  last_applied_manifest\n\texcept NameError:\n\t  last_applied_manifest = [] \n\t\n\tfor resp in response:\n\t    try:\n\n\t    updated_list = []\n\t   # if this is a duplicate request, skip updating\n\t   if previous and previous.manifest == resp.manifest:\n\t      return updated_list\n\t   else:\n\t      resp.update_last_applied_manifest_dict_from_resp(resp.last_applied_json, \n\t\t    previous=(previous + updated_list))\n\t\n\tif 'additionalProperties' in last_applied_manifest:\n\t    for additional_resp"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\t\n\t# Note: This function is also possible for the following situations\n\t# 1. When the user is a superuser, we create two endpoints: the second endpoint and\n\t#      we add the final endpoint to the first one with \"last_applied_manifest\".\n\n\tlast_applied_manifest = get_last_applied_manifest(observer_schema, response[0])\n\tobserver_schema = observer_schema[0]\n\tresponse = response[1:]\n\n\tfirst_endpoint = None\n\tif response[0] and response[1]:\n\t\tif len(response) >"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t# This function is idempotent because the current partial result could be an\n\t# empty list, so we need to re-evaluate this in a for loop\n\tfor field in fields:\n\t\tif field in observer_schema:\n\t\t\tobserver_schema.remove(field)\n\t\telif isinstance(observer_schema, list):\n\t\t\tobserver_schema = field\n\t\t\n\t\tlast_applied_manifest = { field : {} }\n\t\t\n\t\ttry:\n\t\t\t# Update the field\n\t\t\tlast_applied_manifest = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tfield,\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n    last_applied_manifest = [ ]\n    for field in ( 'after_deployment_id', 'id', 'name' ):\n        if field not in response: continue\n        if field not in last_applied_manifest:\n            last_applied_manifest.append( { field: {} })\n\n    for observer_field in observer_schema:\n        if 'after-deployment_id' not in response[observer_field]:\n            continue        \n        if 'id' not in response[observer_field].keys(): continue\n        if 'name' not in response[observer_field]['name']: continue\n\n        for key in response["}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\tfor field in [u'startup_version', u'namespace',\n\t\t\t\t\tu'version', u'replicas_count', u'resource_port', u'protocol_port'\n\t\t\t\t\t]:\n\t\t\n\t\tif not hasattr(response, field):\n\t\t\tcontinue\n\n\t\tif getattr(response, field) == None:\n\t\t\tcontinue\n\n\t\tif not hasattr(observer_schema[0], field) or not callable(observer_schema[0]):\n\t\t\tcontinue\n\n\t\t# Update any of the provided fields in the observer schema for elements that define\n\t\tif attribute_name not in observer_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t    for i, field in enumerate(observer_schema):\n\t        if field.endswith(\"._etag\"):\n            #log.debug(\"Updating last_applied_manifest for %s\\n\" % observer_schema[i]['key'])\n\t\t        last_applied_manifest[i]['last_applied_manifest']['key'] = \\\n\t\t             observer_schema[i]['key']\n\t        elif observer_schema[i]['key'] not in last_applied_manifest.keys():\n                        last_applied_manifest[observer_schema[i]['key']] = []\n                        last_applied_manifest[observer_schema"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\t# initialize\n\tobserver_schema = observer_list_schema_dict.keys()\n\tobserver_list_schema_dict = dict()\n\tfor observer_dict in observer_schema:\n\t\tfor observer_entry in observer_dict.values():\n\t\t\tobserver_list_schema_entry_schema_entry = \\\n\t\t\t\t\"   \\\"metadata_status\\\" : \\\"\"+observer_entry['metadata_status']+\"\\\"\\n\"\n\t\t\tobserver_list_schema_entry = observer_list_schema_entry + \"\\\"\\n\"\n\t\t\tobserver_list_schema_entry = observer"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfrom collections import defaultdict\n\tfrom datetime import datetime\n\tfrom requests.exceptions import ConnectionError\n\tfrom kubernetes.client.rest import ApiException\n\tfrom kubernetes.client.models.v1alpha1_pod_list import V1 alpha1PodList\n\n\tlast_applied_manifest = get_last_applied_manifest(observer_schema, response[0])\n\tlast_applied_manifest_updated = set_last_applied_manifest(observer_schema, response[0], last_applied_manifest, response[1]) \n\n\tif last_applied_manifest_updated:\n\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tif not last_applied_manifest:\n\t\tprint(\"No latest value found in response. No need to update\")\n\t\tlast_applied_manifest = []\n\n\tlast_applied_manifest.extend(response)\n\n\tupdated_timestamp = int(time.time())\n\tlogging.debug(\"The version number with last_applied_manifest %d updated\"\n\t\t% updated_timestamp)\n\tlogging.debug(\"The data: %s\" % latest_manifest)\n\n\tresp = make_response(\n\t\tobj_type=\"Cluster\", name='Cluster', version=(\n\t\t\tlatest_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n    # NOTE: no longer needs validation, may be improved\n\t## FIXME: if we break things, please update/re-enable the following tests\n\t##       by setting validate_pre_deployment to False, in which case,\n\t##       this function will throw validation errors if the check fails\n\tif not _validate_deployment(response):\n        if _is_deployment_active():\n            # Check if we have a template for the first deployment\n            if 'predeploy' in response:\n                first_template = response['predeploy'][0]['template']\n\n                if _is_blueprint(first_"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n    def update_last_applied_manifest_list_from_resp(self, observer_id, response):\n        \"\"\"\n        Go through all observed fields, which must be in the ``observer_id`` and\n        ``response``. As a result, will update the ``last_applied_manifest`` dict\n        from the response dict\n        \n        Args:\n            observer_id (str): the id of the corresponding observer\n            response (dict): the partial response from Kubernetes API\n            \n        Raises:\n            KeyError: The observed field is not present in the response dict\n        \n        \"\"\"\n        self.last_applied_manifest[observer_id] = response"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\ttry:\n\t\tlast = json.loads(last_applied_manifest)\n\t\t\n\t\tkey = last.keys()[0]\n\texcept:\n\t\tkey = last.keys()[0]\n\n\tif last.get('lastDeployDateTime', '0') > 9999999:\n\t\traise KeyError('Last deployed for'+  last['name'])\n\t\t\n\ttry:\n\t\tlast['lastDeployType']\n\texcept:\n\t\tpass\n\t\t\n\ttry:\n\t\tlast['lastDeployUUID4']\n\texcept:\n\t\tpass\n\t\n\ttry:\n\t\tlast['lastDeployDate']"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\tupdated = False\n\tif last_applied_manifest is not None:\n\t\tif isinstance(last_applied_manifest, dict):\n\t\t\t# if the observed field is not a dict (e.g. the kubernetes API\n\t\t\t#  would send it to the REST API\n\t\t\tvalidated_json = validate_against_field_name_or_value(last_applied_manifest)\n\t\t\tupdated = validated_json!= last_applied_manifest\n\t\telse:\n\t\t\tlast_applied_manifest = {}\n\t\t\tvalidated_json = True\n\n\tif not updated:\n\t\tif observer_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t    if last_applied_manifest is not None:\n\t\tfor k in list(last_applied_manifest):\n\t\t    if k not in observed:\n\t\t\tobserved[k] = {}\n\t\tfor k in observed:\n\t\t    observed[k] = observed[k]._replace(body=last_applied_manifest)\n\n\t\t\t\t\n\t\n\n\ndef update_last_applied_manifest_dict_from_resp(\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\n\tfor observed in observer_schema['observations']:\n\t\tif observed =='status':\n\t\t\tresponse['status'] = observer_schema['observed-status']['status']\n\t\telif observed == 'comment':\n\t\t\tresponse['comment'] = observer_schema['observed-status']['comment']\n\t\telif observed =='status-info':\n\t\t\tresponse['status-info'] = observer_schema['observer_status_info']['status-info']\n\t\telif observed =='status-summary':\n\t\t\tresponse['status-summary'] = observer_schema['observer_status_summary']['status-summary']"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\tif not \"last_applied_manifest\" in response:\n\t    raise KeyError(\"Cannot find response with key `last_applied_manifest` to update\")\n\n\tupdated_last_applied_manifest = response[\"last_applied_manifest\"]\n\tif updated_last_applied_manifest:\n\t    updated_item = updated_last_applied_manifest.copy()\n\t    updated_item[\"last_applied_manifest\"] = updated_item[\"last_applied_item_list\"]\n\n\t    # make sure we set `last_applied_entry_list` to the full value\n\t# if it had"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\n################################################################################\n# Exceptions\n################################################################################\n\nclass KubernetesApiException(Exception):\n\tdef __init__(self, code, message, response):\n\t\tsuper(KubernetesApiException, self).__init__(message)\n\t\tself.response = response\n\t\tself.code = code\n\nclass KubernetesApiUnauthorizedError(Exception):\n    def __init__(self, response):\n\t    self.response = response\n    def __str__(self):\n\t    return self.response\n  \nclass KubernetesApiNotAuthenticatedError(Exception):\n    def __init__(self, response):\n\t    raise KubernetesApiUnauthorizedError(response)\n   "}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t    last_applied_manifest = \\\n\t\t     self._add_last_applied_manifest_to_resp(last_applied_manifest)\n\n\t    \n\t    for k, v in result_dict.items():\n\t\ttry:\n\t\t    if k in last_applied_manifest:\n\t\t\tif observer_schema and v!= observer_schema[k]:\n\t\t\t    new_resp = copy.deepcopy(v)\n\t\t\t    v.update(new_resp)\n\t\n\t\t\telif observer_schema:\n\t\t\t    new_resp = observer_json_to_resp(v)\n\t\t\t    v = self._add_observer_metadata("}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n    agent = current_app.config['DATASTORE_AGENT']\n    \n    # first update last applied manifest\n    if 'last_applied_manifest' in current_app.config.keys():\n        update_last_applied_manifest_dict_from_resp(\n            last_applied_manifest)\n    \n    else:\n        agent.log_error('Failed to update last applied manifest dict to agent')\n    \n    # Now perform update last applied manifests\n    if 'last_applied_manifest' in current_app.config.keys():\n        new_last_applied_manifest = current_app.config['last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n        \tfor field in _FIELD_TRANSLATION_LIST:\n        \t\tif _already_exists(last_applied_manifest, field, observer_schema):\n        \t\t\tcontinue\n\n\t\tif observer_schema.has_key(field):\n        \t\tcontinue\n\t\tif last_applied_manifest.has_key(field):\n\t\t\tcontinue\n\n\t\tvalue = observer_schema[field]\n\t\t\n\t\tif isinstance(value, dict):\n\t\t\t\n\t\t\tif observer_schema[field] == {} \\\n\t\t\tand isinstance(last_applied_manifest[field], dict):\n\n\t\t\t\tupdate_last_applied_manifest"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\tlogger = logging.getLogger(__name__)\n\tschema_file ='schema.yaml'\n\n\t# If it's a list, we convert it to a dict so we can use it in our custom observer schema\n\t# e.g. \n\t# {\"namespace\": \"default\"}\n\tif type(app) is list or type(app) is dict:\n\t\tapp = [app]\n\t\n\tfor app in app:\n\t\tspec = parse(app.__sentry__ )\n\t\tlogger.debug('Generating observer template for %s' % spec.name)\n\n\t\tlogger.debug('"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t       \n\tfor k, v in app.spec.manifest.objects.items():\n\t\tif app.spec.use_named_secrets:\n\t\t\tcreate_resource_schema_object(k, v.to_dict())\n\t\telse:\n\t\t\tcreate_resource_schema(k, v.to_dict())\n    \n\t_logger.debug('Initialized.')\n    \n    \nclass CreateResourceSerializer:\n\t\"\"\"\n\tCreate a resource schema for the Kubernetes Resource type represented by a\n\t'spec.json', ensuring that no Kubernetes resource type has been specified.\n\t\n\tThis class can be used to"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\n\n\tif 'KUBE_RESTRICTS_CONFIG' in app.yaml:\n\t\tconf = app.yaml['KUBE_RESTRICTS_CONFIG']\n\t\tfor schema_name, template_name in [\n\t\t\t(os.path.splitext(os.path.basename(spec_filename)[0])[0])]:\n\t\t\tfor template_file, template_spec in conf.get(template_name).items():\n\t\t\t\tif template_file:\n\t\t\t\t\tadd_observer_spec(template_spec.get_path(), template_name,\n\t\t\t\t\t\t                    schema_name, template"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t    \n\n\tdef gen_observer_schema(name, version, description, template, target):\n\t    return {\n\t        'name': name,\n\t       'version': version,\n\t        'description': description,\n\t        'template': template,\n\t        'target': target\n\t        }\n\n\tenv = Environment.create()\n\n\tfor event in env.filter(not_(event_service)):\n\t\ttarget = 'kube_api_v3'\n\n\t    if'metadata' in event:\n\t\t    t = event['metadata']\n\t\t    if 'namespace' in t:\n\t\t\ttarget = t['namespace']\n\n\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t        # get the deployment spec\n\t        nspec = app.get_resource('/spec.manifest')\n\t        root_folder = os.path.realpath(__file__).replace(os.path.realpath('/'), '').replace('kube/config/resources/kubernetes/', '').replace('/usr/', '/')\n\t        nspec = nspec[len(root_folder) + 1:]\n\t\t# iterate through root_folder until we find a match\n\t        for entry in nspec:\n\t\t\ttry:\n\t\t\t\tfor entrySpec in yaml.load_all(entry):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tlogger.debug('gener"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\ndef generate_observer_spec(spec):\n\t\"\"\"\n\tGenerate an empty observer schema for the KRAKE Kubernetes resources\n\tspecified by the configuration file.\n\t\"\"\"\n\tif spec.config_file == '' or not os.path.exists(spec.config_file):\n\t\treturn ''\n\t\t\n\twith open(spec.config_file) as stream:\n\t\tbody = json.load(stream)\n\t\n\tif'metadata' in body:\n\t\tif 'name' not in body['metadata']:\n\t\t\tbody['metadata']['name'] = '__no title__'\n\t\tif '"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\t\n\tfrom. import resource_model\n\t\n\tif app.status == 'created':\n\t\tif resource_model.Resource:\n\t\t\tfrom.resource import *\n\t\telse:\n\t\t\t# Just put it in one of these instead of this line\n\t\t\tlogger.warning(\"Cannot generate default observer with resources. Not creating default observer schema for Kubernetes resource\")\n\t\t\treturn\n\t\t# First get the set of observer fields that will be included for Kubernetes\n\t\tfields_by_namespace = {'apiVersion', 'kind', 'isRunnable', 'type', 'path', 'apiRoot'}\n\t\t\n\t\tfield_paths"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\tdef schema_from_resource(meta_cls):\n\t\t\"\"\"\n\t\tObtain a list of observer schemas based on a custom resource in\n\t\t``spec.manifest``.\n\t\t\n\t\tReturns a list of observer schema.\n\t\t\"\"\"\n\t\t\n\t\t# Note: This is a bit ugly, but I may want this to be configurable,\n\t\t# which doesn't support a new-style class.  To be sure we only have one\n\t\t# meta_cls at a time, we should refactor this code.\n\t\t\n\t\tresults = []\n\n\t\tfor meta in app.config['resource_configs']:\n\n\t\t\tif"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\tobserver_schema = []\n\tfor namespace, name, properties in app.config['resource_definitions']:\n\t\tlogger = logging.getLogger(namespace)\n\t\tlogger.level = 5\n\t\t\n\t\tobserver_spec = {}\n\t\t\n\t\t## get all resource names from namespace\n\t\t#try:\n\t\t#\tobserver_spec['resource'][namespace]['properties']\n\t\t#except KeyError:\n\t\t#\tlogger.warning('Warning: Resource namespace {0} does not have '\n\t\t#                     'properties defined!'.format(namespace))\n\t\t#            \n\t\tresource_names = sorted([properties['"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\tfor agent in app.model.nodes:\n\t\t\tagent_type = node_instance_or_literal_as(agent)\n\t\t\t\n\t\t\t# Construct the resource schema\n\t\t\tresource_def_schema = {\n\t\t\t\t\"pod\": {\n\t\t\t\t\t\"metadata\": {\n\t\t\t\t\t\t\"labels\": agent_type.labels()\n\t\t\t\t\t} \n\t\t\t\t},\n\t\t\t\t\"nodes\": [\n\t\t\t\t\tnode_instance_or_literal_as(t) for t in agent.spec.partitions()\n\t\t\t\t]\n\t\t\t}\n\n\t\t\t# Add the default observer schema\n\t\t\tif not agent.has_listener(self):"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tdef out(query: str) -> str:\n\t\t# Extract any parameters starting with the \"s\" to match the SQL\n\t\tout_params: typing.Dict[str, typing.List[typing.Dict[str, Tuple[str, int]]] = {}\n\t\tfor p in params:\n\t\t\tout_params[p.name] = p\n\t\t\tfor v in out_params[p.name]:\n\t\t\t\tout_params[p.name].remove(v)\n\n\t\treturn (\n\t\t\tquery.replace(\"?\", \"%%\"),\n\t\t\t\", \".join(out_params."}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tvalues = ()\n\t\tif not isinstance(params, Mapping):\n\t\t\tparams = (params,)\n\t\tvalues = tuple(params)\n\t\tif getattr(params, 'in_style', None):\n\t\t\tvalues = (values,)\n\t\t\n\t\ttry:\n\t\t\treturn format('({})'.format(', '.join(values)))\n\t\t\n\t\texcept Exception as e:\n\t\t\traise ArgumentError(str(e))\n\t\n\texcept Exception as e:\n\t\traise ArgumentError(str(e))# Copyright (c) 2015 Mirantis Inc.\n#\n# Licensed under the"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\timport copy\n\timport types\n\tfrom pprint import pformat\n\t\n\tif issubclass(type(sql), dict):\n\t\tif SQLParams.SQL_TYPE in sql:\n\t\t\treturn _format_params(sql[SQLParams.SQL_TYPE], sql,\n\t\t\t\t\t\t\t\t\t\t   sql.get('in_style', '*'))\n\t\telse:\n\t\t\treturn copy.deepcopy(_format_params(sql, sql.get('in_style', [])))\n\telse:\n\t\treturn repr(sql)\n\n\ndef _format_params(in_style, params):\n\t\"\"\" \n\tFormat values"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tif not isinstance(\n\t\tsql, (bytes, types.GeneratorType)) and isinstance(params, (collections.abc.Mapping)):\n\t\tparams = (params,)\n\t\n\treturn tuple(sql + tuple(params))\n\ndef sqlquery(\n\tsql: bytes,\n\tparams: Mapping[str, Any],\n\tpostdata: Sequence[Any],\n\tstyle: str,\n\t**kwargs: Any\n) -> 'Query':\n\t\"\"\"\n\tA *Query* object for use on a PostgreSQL `Query` instance.\n\n\t:func:`.sqlgenerator.sqlquery`"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, sql_, params=(), **named_params):\n\t\tself.__sql = sql_\n\t\tself.__params = params\t\t\n\t\tself.__named_params = named_params\n\t\tself.__qparams = []\t\n\t\t\n\t\tif isinstance(sql_, bytes):\n\t\t\tsql = bytes(sql_)\n\t\tself.__sql = sql\n\t\tself.__params = {}\n\t\t\n\t\tfor p in self.__params:\n\t\t\tself.__params[p] = params[p]\n\t\t\n\t\n\tdef append(self, *qparams):\n\t\t\"\"\"\n\t\tAppend one or more :"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tfrom..converters import escape\n\n\toutp = []\n\t\n\t# prepare the arguments\n\tparam_styles = {}\n\t\n\tfor style_param in cls.param_style:\n\t\tif style_param.default_value is None:\n\t\t\tparam_styles[style_param.key] = 'N'\n\t\telse:\n\t\t\tparam_styles[style_param.key] = escape(style_param.default_value)\n\t\n\tin_parameter_styles = 0\n\tout_parameter_styles = 0\n\n\tfor param_style in (v for k, v in param"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tret = []\n\tfor type, value in query:\n\t\tif type == _QUERY_OPTIONS.TYPE.INSERT:\n\t\t\tif len(ret) == 0:\n\t\t\t\tcontinue\n\t\t\tfor result in value:\n\t\t\t\tret.append(result)\n\t\t\tcontinue\n\t\t\n\t\tif type == _QUERY_OPTIONS.TYPE.REPLACE:\n\t\t\tfor result in value:\n\t\t\t\tret.extend(map(lambda r: r.replace(**value),\n\t\t\t\t\t\t\t\tres))\n\t\t\tcontinue\n\n\t\tif type == _QUERY_OPTIONS.TYPE.REMOVE:\n\t\t\tif len(ret) == 0:"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tif isinstance(params, Mapping):\n\t\treturn tuple(format(param, **kw) for param in params.items())\n\t\n\telif isinstance(params, Sequence):\n\t\treturn tuple(_format(param, **kw) for param in params)\n\telse:\n\t\traise AssertionError(\"invalid argument type: %s, expected: Mapping, iterable, Mapping or Sequence\" % (type(params).__name__,))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tsql = {}\n\t\n\t# Use the IN_STYLES key as its default if it was not defined there.\n\tstyle = kwargs.pop(\"in_style\", SqlInStyle())\n\t\n\t# Get parameters in separate lists (each list item being a\n\t# string, tuple, list, or string).\n\tformatted = fmt_query(sql,\n\t\t\t\t\t\t[k for k in sql.keys() if k in style[\"args\"]])\n\n\tfor k in formatted:\n\t\tparameters = tuple(formatted[k].split(\",\"))\n\t\tparameters = {}\n\t\tif len("}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tsql = dict(\n\t\tsql=\t\t{\n\t\t\t'out_table':\t{\n\t\t\t\t},\n\t\t\t'out_field':\t\t'id' if 'id' in params else 'q' if 'q' in params else 'r',\n\t\t\t'out_index':\t\t{\n\t\t\t\t#...but...?\n\n\t\t\t\t#...or...?\n\t\t\t\t'int_field':\t\t#...otherwise, make integer fields for integer fields in SQL\n\t\t\t\t#...and also to strings\n\t\t\t\t'float_field':\t\t#...otherwise, make float fields for float"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\n\tout_params = { }\n\tfor param, typ in six.iteritems(of._SQLParams):\n\t\tout_params[param] = typ if isinstance(typ, int) else typ[0:1]\n\n\n\t# If a param's name/types/in-style parameters were not explicitly encoded and\n\t# the user set a parameter for them before converting them to SQL with the same\n\t# length or parameter type name, then the length must be set, and no\n\t# conversion is performed. Otherwise, the values in the params are already\n\t# encoded with the length,"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n\t# check for type conversion and fix arguments types\n\treturn tuple(tconvert(table, c) for table, col, c in zip(fields, one_param_style, tuple_style)))\n\n\ndef tconvert(table, c):\n\t'''Convert a SQL string to use it in a specific database connection.\n\t\n\t:param str table: The SQL string to convert.\n\t:param tuple c: The C parameters to use.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       A :class:`list` containing each set of converted out-style parameters.\n\t\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n\tif not isinstance(sql, text_type):\n\t\traise TypeError('Expected {0}, got {1}'.format(sql.__class__.__name__, sql))\n\t\n\tfor params in (params for params in (params for params in params)):\n\t\tparams = (\n\t\t\t(k, v)\n\t\t\tfor k, v in list(params.items())\n\t\t\tif not isinstance(v, (bytes, str, int))\n\t\t)\n\t\t\n\t\tif not params:\n\t\t\tparams = ()\n\t\t\n\t\tsql = _reconvert(\n\t\t\tsql,\n\t\t\t(self.format_multi([v"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\n\tparamstyle = settable(paramstyle)\n\tif paramstyle:\n\t\tfor key in params.keys():\n\t\t\tif key in paramstyle:\n\t\t\t\tparams[key] = paramstyle.pop(key)\n\t\t# A copy of the input list, where each element of\n\t\t# the input list is a ParameterSet object. We don't want to\n\t\t# keep this in sync with the original list - it is not\n\t\t# removed from the list as the resulting ParameterSet would\n\t\t# have a 'id' field, or it could lead to a problem.\n\t\tparam"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n#######################################################\n# Copyright (c) 2015, ArrayFire\n# All rights reserved.\n#\n# This file is distributed under 3-clause BSD license.\n# The complete license agreement can be obtained at:\n# http://arrayfire.com/licenses/BSD-3-Clause\n########################################################\n\nimport collections as cx\nfrom..api.definitions import IVariable, DimExpression\nfrom..api.function import Function, Argument, FunctionDefinition\nfrom..api.errors import (\n    error_accumulator,\n    internal_error,\n)\nfrom.errors import Constant\nfrom.types import Constant"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\t# Note we have to keep a separate 'params' mapping here\n\t\t# because they're a little more convenient than passing them into the\n\t\t# function directly.\n\t\t#\n\t\t# We convert the query of the original query into a :class:`dict`\n\t\t# that can be passed to the SQL query.\n\t\t#\n\t\t# This is mostly a convenience method, but it can be, e.g., by\n\t\t# calling ``SQLParams.in_style(query, params)``.\n\t\toutput = {}\n\t\tfor name, param in in_params:\n\t\t\tif isinstance("}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n\t# Convert the parameters to a mapping and then into a sequence of\n\t# mapping values and a sequence of sequence of converted out-style\n\t# parameters.\n\tmany_params = OrderedDict()\n\tfor param_name, param_value in params_to_int().items():\n\t\tif isinstance(param_value, int): break # Python 3\n\n\t\tmany_params[param_name] = {\n\t\t\t'readonly': param_value in int_groups,\n\t\t}\n\t\t\n\t\tif isinstance(param_value, str): param_value = u(param_value)\n\t\t\n\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n\tparam_table = (\n\t\t{\n\t\t\t'in_style' : 'params',\n\t\t\t'in_columns' : (\n\t\t\t\t'id',\n\t\t\t\t'name',\n\t\t\t\t'type',\n\t\t\t\t'index_type',\n\t\t\t\t'model',\n\t\t\t\t'model_name',\n\t\t\t\t'ondelete',\n\t\t\t),\n\t\t},\n\t)\n\t# This is a workaround to circumvent the escaping of spaces to\n\t# avoid SQL-reparsing problems when formatting parameters or sequences\n\t# are escaped, which messes up query handling.\n\t# Reverted"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\n\t# Use the \"plain\" SQL syntax where one parameter style is replaced with\n\t# the parameter name. In this example, we have a table \"a\" with two parameters,\n\t# \"b\" and \"c\". To use \"plain\" SQL syntax in the example above, we would\n\t# use:\n\t#\n\t# `echo \"schema.py < schema.sql.Table 'a' as a``\n\t#\n\t#.. note:: `echo \"schema.py < \"schema.sql.Table 'b' as b``\n\t# \"\"\"\n\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\t\n\t\tresult = []\n\t\t\n\t\tfor x in (x for x in (x for x in zip(*[x for x in params if (not isinstance(x[0], (list, list, set, tuple)) and len(x) == 2)]))):\n\t\t\tresult += x\n\t\t\n\t\tif result == []:\n\t\t\t# In this case, the output from the previous query is all in one\n\t\t\tresult = (x for x in (x for x in zip(*[x for x in params if (not x[0] in SQLParams.in_style))],)))\n\t\t\n\t\tif len(result) == 1"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n# This is an ugly HACK -- if you want to be very strict about getting\n# this working, one can comment it out.\n# A better way for Python2.5.1 or better, will need to set module level\n# to whatever you like below.\n# But we're not interested in the rest of this file in the first place.\n# I've chosen to remove it but my best guess I've learned it.\n# See also the examples in pyftd.utils/test_data2.py for an example\n# on how to use these.\n\n_pyversion"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n    _obfuscate_object('OBJFL', path)\n    \n    _obfuscate_object('OBJFL', os.path.abspath(path))\n    \n    return True\n    \"\"\"Helper functions to make handling of Fisher information.\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nfrom plaso.engine import time_events\nfrom plaso.frontend import frontend_stats\nfrom plaso.lib import errors\nfrom plaso.lib import specification\nfrom plaso.lib"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\t#if not util.ispyfs_root(path):\n\t#\tif util.ispyfs_root(self.fs.root):\n\t#\t\treturn False\n\tif not self.fs: return False\n\tif not util.ispyfs_root(path): return False\n\t\n\ttry:\n\t\tself.fs.makedir(path)\n\t\t\n\t\tif util.ispyfs_root(self.fs.root):\n\t\t\tfsobj = self.fs.new_file(self.fs.root, None)\n\t\t\treturn util.ispyfs"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\tif not os.path.dirname(path):\n\t\t\traise ValueError(\"Invalid OCFL path or non existing path.\")\n\t\telif not os.path.exists(path):\n\t\t\traise ValueError(\"Invalid OCFL path.\")\n\t\telse:\n\t\t\treturn True\n\tdef isObject(self, objectId):\n\t\t\"\"\"\n\t\tCheck if object exists in OCFL database.\n\t\t\n\t\tArgs:\n\t\tId -- id or ocfl id (str)\n\t\t\n\t\tReturns True when object exists in OCFL database (True or False), False otherwise.\n\t\t\"\"\"\n\t\todf ="}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\timport os\n\t\n\timport re\n\t\n\tif self.fs:\n\t\tif not os.path.exists(path): \n\t\t\traise CFileNotFound(path)\n\t\t\n\t\tdata = codecs.open(path, \"rb\", \n\t\t\t\tencoding=\"utf-8\").read()\n\telse: \n\t\tself.fs = io.open(path, encoding=\"utf-8\")\n\t\tdata = self.fs.read()\n\t\n\tself.validator = Validate(data, encoding_errors='ignore')\n\t\n\tself.path = path\n\tself._validate = True\n\t\n\treturn True\n\t##"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\treturn True\n\nclass SkeletonItem(object):\n\t\"\"\"\n\tBase class for skeleton objects\n\t\"\"\"\n\t\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, SkeletonItem) and self.path == other.path and self.is_parent == other.is_parent\n\t\n\t#{ Interface\n\n\tdef __init__(self, name=\"\", parent=None, id=-1, data_type_options={}):\n\t\t\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself._name\t\t\t\t= \"Unknown\" if self.parent is None else self.parent"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n    def open(self, path, mode):\n        \"\"\"\n        Open file to read, and return its contents (as opposed to pyfs objects).\n        \n        path: str\n            File or directory path to test.\n        mode: str (optional)\n            File open mode.\n        \"\"\"\n        raise NotImplementedError('implement this method in subclass')\n\n    def create_index(self, data):\n        \"\"\"\n        Create the index in the filesystem.\n        \n        data: dict (pyfs object, index name)\n            Dictionary-like object to store the index to.\n        \"\"\"\n        raise NotImplementedError('Implement this method in subclass')\n\n    def create(self, path"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\t# Create paths, as they may not exist.\n\tif not os.path.isdir(path):\n\t\ttry:\n\t\t\tos.makedirs(path)\n\t\texcept:\n\t\t\tif not os.path.isdir(os.path.dirname(path)):\n\t\t\t\traise\n\t\n\t# Create CVS objects to validate file exists\n\ttry:\n\t\tos.makedirs(os.path.dirname(path))\n\texcept OSError:\n\t\tif not os.path.isdir(os.path.dirname(path)):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\t\n\ttry"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\tlogging.debug(\"Validate %s (fs %s)\" % (path, self.fs))\n\t\tlogging.debug(\"Validating OCFL %s in %s\" % (path, self._path))\n\t\t# we check the OS that this code is working on has the same version as the fs for this version\n\t\tproc = subprocess.Popen(\"which %s\" % self._path, shell=True, stdout=subprocess.PIPE)\n\t\tproc.communicate()[0]\n\t\t# get the version number. if the path had better precision then we can use it. It will be different"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\tif not path:\n\t\t\tif not self.ismount():\n\t\t\t\tdbg.log(\"warning: mount point is not mounted.\",highlight=1)\n\t\t\treturn False\n\t\tif not path or not op.isfile(path):\n\t\t\tif not self.ismount():\n\t\t\t\tdbg.log(\"warning: mount failed.\",highlight=1)\n\t\t\treturn False\n\t\ttry:\n\t\t\tif not op.exists(path):\n\t\t\t\treturn False\n\t\t\tdata = open(path, \"r\").read().strip()\n\t\texcept (IOError, OSError):\t# TODO: log error if open did not"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\tif callable(self._status):\n\t\t\treturn self._status()\n\t\telse:\n\t\t\treturn getattr(self._validator(), prefix + self._type)()\n\n\tdef get_status(self):\n\t\t\"\"\"\n\t\tCheck if there is a validator for this type, and return its function as the\n\t\tvalue in case it's wrapped by a validator\n\t\t\"\"\"\n\t\tself._raise('get_status', self)\n\t\treturn self._status\n\n\tdef _get_default(self):\n\t\tif callable(self._default):\n\t\t\treturn self._default()\n\t\telse:\n\t\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\nclass ValidationStatusTest(unittest.TestCase):\n\n\t# Tests for _get_validator() (and some tests)\n\tdef test_valid(self):\n\t\t# Checks if validator succeeds with and without errors\n\t\t# Checks for exceptions and invalid value raises exception\n\t\twith self.assertRaises(ValueError): pass\n\t\twith self.assertRaises(ValueError): pass\n\t\twith self.assertRaises(ValueError): pass\n\t\twith self.assertRaises(ValueError): pass\n\t\t\n\t\twith self.assertRaises(ValueError):\n\t\t\tvalidator._get_validator('1')\n\n\t\twith self.assertRaises(ValueError):\n\t\t\tvalidation_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn self.status + prefix + self.status_text(prefix +'   ')\n\n\tdef status_text(self, prefix):\n\treturn self.status + prefix + self.status_end()\n\n\tdef message(self, prefix=''):\n\treturn message(self.status) + prefix + self.status_text(prefix +'   ')\n\n\tdef test_case_insensitive(self):\n\t\tif self.message('CASE'):\n\t\t\treturn True\n\n\t\treturn self.message('LOWERCASE')\n\n\tdef test_case_insensitive_prefix(self):\n\t\tif"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\t# NOTE - using a non-capturing value in Python 2.6 that does not\n\t\t# support non-ASCII chars.\n#\treturn str(self._status)\n\n\tdef status_tuple(self):\n\t\t\"\"\"\n\t\tReturn 2-tuple of validator status:\n\t\t(valid, message, result)\n\t\t\"\"\"\n\t\t(valid, message, result) = self._status\n\t\treturn (valid, message, result)\n\n\t# NOTE - for now the internal validator module can only check\n\t#        \"none\" value and return the whole thing, not in the\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t # Check if name is string instead of a string\n\t\tif isinstance(name, str):\n\t\t\tif prefix == '':\n\t\t\t\tprefix = 'Validator'\n\t\t# Check status\n\t\telif (isinstance(name, int) or isinstance(name, float) or isinstance(name, long)) and prefix == 'Name' and len(name) > 21:\n\t\t\tprefix = 'Validator_'\n\t\treturn str(prefix + 'Status')\n\t\n#============================\n# Class:   Hold  - Hold type   |\n#============================\nclass HoldType(object):\n\t\"\"\"\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t if callable(self.validator):\n\t\t\tdef _check_status(status):\n\t\t\t\tif status:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif not status or len(status) == 1:\n\t\t\t\t\t\t\treturn \"valid\"\n\t\t\t\t\t\tif status[-1] in ['.', '!', '.']:\n\t\t\t\t\t\t\tstatus = status + \"invalid%04x\"%len(status)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\telif status.isdigit():\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tif not status or len(status) == 3:\n\t\t\t\t\t\t\t\t\treturn \"valid\"\n\t\t\t\t\t\t\t\tif int(status):\n\t\t\t\t\t\t\t\t\treturn \"valid\"\n\t\t\t\t\t\t\t\treturn \"\"\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\nclass validator_info(object):\n\t\"\"\"\n\tStore information about a validator.\n\t\"\"\"\n\t\n\tdef __init__(self, name, validator_id, validator_tag, error, value_error, error_text, value_text, source_file_name, line, pos, source_function_name, source_line, code_file_name, error_line, code_text):\n\t\tself._name = name\n\t\tself._id = validator_id\n\t\tself._tag = validator_tag\n\t\tself._tag_list = []\n\t\tself.name ="}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn \"\\n\".join([\"<status>{0}: {1}({2})\".format(prefix.rjust(15), str(self.__status), self.get_class_name()), \"</status>\"])\nimport datetime\nimport decimal\nimport json\nimport random\nimport uuid\nfrom collections import OrderedDict\nfrom functools import partial, reduce\nimport time\nfrom urllib import urlencode\nimport urlparse\n\nimport drift\nimport httpretty\nimport mock  # lint-amnesty, pylint: disable=wrong-import-order\nfrom django.core import mail\nfrom django.core.urlresolvers import reverse"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t # return '%s - %s' % (self.status, self.message)\n\t result = '<%s> %s' % (self.status, self.message)\n\t return result\n\n\tdef raiseError(self, error_string, err_string=None):\n\t\tif hasattr(self,'messages'):\n\t\t\tself.messages.append(error_string)\n\t\t\tif err_string:\n\t\t\t\tself.addFail('[error]', err_string)\n\t\t\terror_string = str(error_string)\n\t\t\tfor m in self.messages:\n\t\t\t\terror_string = error"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\trc_prefix=''\n\t\tfor l in dir(self):\n\t\t\tif l.startswith(prefix):\n\t\t\t\trc_prefix = l\n\t\treturn rc_prefix\n\n\n\t@staticmethod\n\tdef status_number(status):\n\t\timport string\n\t\tnumber=0\n\t\ttry:\n\t\t\twhile number < 100:\n\t\t\t\ti = '%X' % ord(status[number])\n\t\t\t\tnumber += 1\n\t\texcept:\n\t\t\tnumber=0\n\t\treturn '%s.00%s' % (status_str(status), number) # Tells the log happy\n\n\t@staticmethod"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t return f\"{prefix} : {self.status}\"\n\ndef status(log=None, prefix=''):\n\t\"\"\"\n\tDisplay validation results about validation error, if any.\n\t\n\t:param log: (optional) function used to generate additional output.\n\t:type log: function\n\t\n\t:param prefix: (optional) string used to indicate the generated errors.\n\t:type prefix: string\n\t\n\t\"\"\"\n\n\tif log:\n\t\tif callable(log):\n\t\t\t#log(self)\n\t\t\treturn log(prefix)\n\t\telse:\n\t\t\t#log(self, log=log)"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn self.validation_log + \" - \" + LogValidator.escape(self.name) + \"=\" + prefix\n\t\t\nclass LogValidator(object):\n\t\"\"\"\n\t:param string prefix:\n\t:return:\n\t\"\"\"\n\tdef __init__(self, name, valid, regex, log_level):\n\t\tself.name  = name\n\t\tself.valid  = valid\n\t\tself.regex = re.compile(regex)\n\t\tself.log_level = log_level\n\n\n\tdef validate(self, string):\n\t\t\"\"\"\n\t\tValidate given string and return validation"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\tif self.status:\n\t\t\treturn self.status_str + prefix + \" \"\n\t\telse:\n\t\t\treturn prefix + \" \"\n\ndef dump_result(result, status, is_error):\n\treturn {\n\t\"return\" : result.status and not is_error,\n\t\"error\" : status,\n\t\"is_error\" : is_error\n\t}\n\ndef parse_json(text):\n\timport json\n\tobj = json.loads(text)\n\ttry:\n\t\tif 'error' in obj and obj['error']:\n\t\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\n\t\n\tif self._errors is None:\n\t\treturn ''\n\telif len(self._errors) == 0: # return empty string if no errors.\n\t\treturn 'no errors'\n\telse:\n\t\ts = ''\n\t\tfirst = True\n\t\tfor i in range(len(self._errors)):\n\t\t\tif first:\n\t\t\t\ts += '|'\n\t\t\ts += '\\n' * (self._errors[i].__len__())\n\t\t\tfirst = False\n\t\t\ts += str(i+1) +''\n\t\ts += prefix + s.replace('\\n', '\\n    ')"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t # TODO use an ordered list to track the order in which they were passed:\n\t\terror_messages = defaultdict(list)\n\n\t\ttotal_errors = sum([len(v) for v in self.errors.values()])\n\t\tif total_errors == 0:\n\t\t\treturn '%d validation(s)!' % len(self.errors)\n\t\telse:\n\t\t\t# sort by index to get the indices of the errors in the order in which the error was\n\t\t\t# generated from the end of this list:\n\t\t\terror_messages = sorted(self.errors.keys())\n\t\t\ttotal_errors"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t # log level is always the same as that of the last level\n\t logging.INFO(\"{}: status_str\".format(self._tag_name))\n         # Prefix\n\t if not prefix: prefix ='status_'\n\t else: prefix ='status_' + prefix\n\t # format tag\n        if self._tag_name in self.TAGS or self._tag_name.startswith(prefix):\n\t\t return self.TAGS[self._tag_name][0] +'status: {}'.format(self.TAGS[self._tag_name][1])\n\t\t# print info\n\t\treturn self.TAGS[self.TAGS"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t return format_as_html('%s%s', self.message, prefix = prefix)\n\ndef html_body(code):\n\t\"\"\"\n\tShows formated error page with error code and/or status code.\n\t\"\"\"\n\treturn \"<h2>%s%s</h2><p>\" % (_(\"Error code\"), str(code))\n\ndef html_quote(msg, safe=None):\n\t\"\"\"\n\tProperly quote a string so that it can be embedded in a message.\n\tFor example:\n\t>>> msg = _('Look at the %(variable)s"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + 'Status ='+ str(self) + '\\n'\n\nclass log_line:\n\t\"\"\"\n\tClass of log line log\n\t\"\"\"\n\tdef __init__ (self, ln = False):\n\t\tself.data = ''\n\t\tstr_data = ln + '\\n'\n\tdef get_data(self):\n\t\tif self.data == '': self.data = log_lines\n\t\treturn self.data\n\n\tdef __str__(self):\n\t\t\t\treturn self.data\n\nclass error_log:\n\t\"\"\"\n\tClass of error log"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t     \n\tcdict = {\n\t  '_': 'Validated'\n\t\t  }\n\t\n\t# make sure validations are strings\n\tdef type_error(self, val):\n\t    raise TypeError(Error.type_error, 'Validated %s' % type(val))\n    \n    # validate parameters and check sanity of format\n\tdef validate_value(self, inval):\n\t\tfor name, val in inval.items():\n\t\t\n\t    # check required key\n\t    if not name in self._data.keys():\n\t        raise TypeError(\"%s parameter not supported\" % name)\n\t\n\t\n        # set parameter name\n        param"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + self.get_summary_str()\n\n\tdef format_message(self, msg, prefix=''):\n\t\t\"\"\"\n\t\tFormat message to be shown to the user.\n\t\t\"\"\"\n\t\tif prefix:\n\t\t\tprefix = prefix + \": \" + msg\n\n\t\treturn prefix + self.get_message_str(msg)\n\n\tdef get_message_str(self, msg):\n\t\t\"\"\"\n\t\tReturn log info message as a string.\n\t\t\"\"\"\n\t\treturn \"%s: %s\" % \\\n\t\t\t(self.get_summary_str(), msg)\n\nclass"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn True\n\n\nclass InvalidToken(Exception):\n\t\"\"\"\n\tInvalid token\n\t\"\"\"\n\tpass\n\n\nclass BaseBackend(object):\n\t\"\"\"\n\tAbstract backend base class\n\t\"\"\"\n\tname = \"\"\n\n\tdef __init__(self, identifier, secret):\n\t\tself.name = identifier\n\t\tself.secret = secret\n\n\tdef send(self, msg):\n\t\t\"\"\"\n\t\tSend a message to the given identity (provided by CredManager)\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef get_all(self):\n\t\t\"\"\"\n\t\tGet all messages from"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\n    def to_string(self, number):  # pylint: disable=too-many-return-statements\n    \n        _base_validate_native(number)\n        for symbol in number.symbols:\n            if not self.is_valid(symbol):\n                return False\n    \n        return True\n# This file is based on work done by Franz Kaaplan's work from the django-rest-framework tutorial\n\nimport os\nimport re\nimport sys\nimport importlib\nimport inspect\nimport traceback\n\n\ndef import_by_path(dotted_path, error_prefix=''):\n    \"\"\"\n    Import a"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn identifier in self.VALIDITY and self.VALIDITY[identifier]\n\n\tdef __hash__(self):\n\t\t\"\"\"\n\t\tReturn a unique identifier.\n\t\t\"\"\"\n\t\treturn hash(self._identifier)\n\n\t@staticmethod\n\tdef is_valid_id(identifier):  # pylint: disable=unused-argument\n\t\t\"\"\"\n\t\tReturn True if identifier is valid, always True in this base implementation. If False, return False.\n\t\t\"\"\"\n\t\treturn identifier in IdDict or IdListDict or IdTupleDict\n\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn bool(identifier) and hasattr(self, identifier)\n\nclass FunctionDescriptor(ExtendedAttributes, AbstractDescriptor):\n\tdef __init__(self, name, visibility):\n\t\tself.name = name\n\t\tself.extensible = visibility\n\t\tself.static = visibility == 'extern'\n\t\tExtendedAttributes.__init__(self, visibility)\n\tdef get_type(self):\n\t\treturn \"function\"\n\tdef get_name(self):\n\t\t\"\"\"\n\t\tReturn the name of the function.\n\t\tThis assumes a class name and returns a string.\n\t\t\"\"\"\n\t\ttry"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t return idlib.is_valid(self.ffi, identifier) and self.ffi._is_valid(\n\t\t\tself.ffi._get_ffi_from_id(\n\t\t\t\tself.tcb.from_param(self.params, identifier)),\n\t\t\tffi.cast(\"intptr_t\", self.tcb))\n\n\tdef is_defined(self, identifier, force_error = False):  # pylint: disable =arguments-differ\n\t\t\"\"\"\n\t\tReturn True if identifier is valid, otherwise false.\n\t\t\"\"\"\n\t\tif not force_error and idlib.is_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn hasattr(self._data, 'dtype')\n\n\ndef _to_complex(data):  # _to_complex is only a workaround for https://github.com/scipy/scipy/issues/2934\n\ttry:\n\t\treturn complex(*data)\n\texcept (ValueError, TypeError):\n\t\treturn complex(*tuple(data))\n\n\ndef _parse_dtype(data, force_str=False):\n\t\"\"\"Returns the numpy dtype from a string.\n\tThis allows us to use a different type for the array.\n\tThis string conversion logic is also used to infer the most"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn identifier.startswith('_') or self.VALID_IDENTIFIER_REGEX.match(identifier)\n\n\t@staticmethod\n\tdef is_ascii_identifier(identifier):\n\t\t\"\"\"\n\t\tReturn True if identifier consist of printable characters.\n\t\tThis looks for some ASCII identifiers, and the characters (with a maximum len)\n\t\tof each character are considered to be valid.\n\t\t@param identifier: A non-ascii string, such as ABC123 (a-zA-Z0-9_);\n\t\t@return: bool\n\t\t@raise ValueError: Invalid identifier\n\t\t\"\"\"\n\t\treturn bool(IDENTIFIER_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\nclass Enumeration(BaseEnumeration):\n\t\"\"\"\n\tEnumeration class for simple numeric values; supports comparison.\n\t\"\"\"\n\tdef __init__(self, values_list):\n\t\t\"\"\"\n\t\tConstruct an Enumeration for a list of values.\n\t\t\n\t\t@param values_list: List of possible values for this enumeration.\n\t\t\"\"\"\n\t\tself.__values_list = values_list\n\t\n\t@property\n\tdef values_list(self):\n\t\t\"\"\"\n\t\tRetrieve and return values list.\n\t\t\n\t\t@return: List of possible values.\n\t\t@rtype: list, set\n\t\t\"\"\""}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\traise NotImplementedError\n\t\nclass NullImplementation(AbstractImplementation):\n\t\"\"\"\n\tCreate an EmptyImplementation object with no parameters.\n\t\"\"\"\n\tdef __init__(self):\n\t\traise NotImplementedError\n\t\t\n\tdef add_implementation_name(self, implementation_name):\n\t\traise NotImplementedError\n\n\tdef get_methods_names(self):\n\t\traise NotImplementedError\n\t\t\n\tdef get_method_signature(self, signature):\n\t\traise NotImplementedError\n\n\tdef call_method(self, method_name, params):\n\t\traise NotImplementedError\n\t\t\nclass BaseFrontend(AbstractFrontend):\n\t\"\"\"\n\tBase"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn (identifier in (None, \"None\", \"True\", None))\n# -*- coding: utf-8 -*-\n# Copyright 2010 Google Inc. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\n\tself.spec_version_str = self.__get_spec_version(extract_spec_version)\n\t\n    def getVersion(self):\n\t\"\"\"\n\t\tReturn the spec version.\n\t\t:return:\n\t\t\"\"\"\n\treturn self.spec_version\n\n    def __get_spec_version(self, extract_spec_version=False):\n\tif extract_spec_version:\n\t    version = self.getVersion()\n\t    # In case the version is unknown, try to grab a generic version from the\n\t    # path.  In that case, try to use the default version and try"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\tif not isinstance(inventory, dict):\n\t\t\tif not inspect.isclass(inventory) and issubclass(inventory, AnsibleUnsafe):\n\t\t\t\traise unittest.SkipTest('invalid inventory type %s' % inventory)\n\t\tif not parse_version(inventory) < parse_version(self.spec_version):\n\t\t\traise unittest.SkipTest('parsing inventory: invalid version %s' % inventory)\n\t\n\tdef transform(self, raw):\n\t\t\"\"\"\n\t\tTry to do this to transform the inventory data to what the\n\t\tsystem expects us to expect. Format the returned data into something\n\t\tlike lists so"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tif self.is_valid(inventory):\n\t\treturn self.spec_version\n\n\tif self.spec_version is None:\n\t\treturn self.is_valid(inventory)\n\n\telse:\n\t\t# we are not on the spec version so check the spec version first\n\t\tcur_spec_dir = os.path.dirname(inventory.load())\n\t\tspec_version_directory = None\n\t\tversion_file = None\n\n\t\tif os.path.isfile(os.path.join(cur_spec_dir, self.source)) and os.path.isfile(os"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n#\tI expect we could really do the parsing at all here rather than in the\n#\tconstructor, but for now each case is less complicated. It currently just\n#\tworks to fix the parse/validation functionality into the __parse_error\n#\tmethod below.\n\t\n\tif self.missing is False and ((not self.optional or self.validator!= True) and not self.is_required):\n\t    return \"missing required parameter\"\n\n\tif self.optional_keys:\n\t    if len(self.optional_keys)!= len(set(self.optional_keys)):\n\t\treturn \""}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n        # make sure this spec version is not given as a spec version if we don't\n        # have this kind of information\n        if self.spec_version is None:\n            try:\n                self.spec_version = get_spec_version(self.base_dir, self.name)\n            except ValueError:\n                if self.name == \"unknown_file\":\n                    # unknown file, OK\n                    raise ValueError(\"Unknown inventory file: %s\" % self.path)\n                else:\n                    # ok, this is not an info version\n                    raise ValueError(\"unknown inventory file: %s\" % self.path)\n\n       "}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t    \n\t# TODO/FIXME:\n\t# - is it a version that can be used for updating?\n\t# \t- can the inventory be removed from inventory or just be pruned\n\t# \t- can the inventory be modified if it is dirty and saved somewhere else\n\t# \t- could it have been set manually to its current spec?\n\t# \t- could it be updated now or removed from the tree?\n\n\t# TODO/FIXME:\n\t# - this can probably use base class to be able to track the spec version?\n\t# \t- but when"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\nclass TestResolve(unittest.TestCase):\n\n\tdef testCreateInputFromConfigFile(self):\n\t\tconfig = _get_config(None)\n\t\tself.assertEqual(config.format(self.spec_version), '1.2')\n\t\t\n\tdef testCreateErrorInvalidVersion(self):\n\t\tconfig = _get_config('not a list')\n\t\tself.assertRaises(ValueError, config.format, 'hello')\n\t\tself.assertRaises(ValueError, config.format, '1.2')\n\t\n\tdef testCreateConflictingVersion(self):\n\t\tfrom zeroinstall.injector"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\tenv = Environment(loader=PackageLoader('zim', 'templates/site.zim'))\n\t\n\ttry:\n\t\tinventory = Inventory(env)\n\n\t\tfor machine in env.machines:\n\t\t\ttry:\n\t\t\t\tvalidate(inventory, machine)\n\t\t\t\tvalidate(inventory, extract_spec_version=True)\n\t\t\texcept AssertionError as e:\n\t\t\t\tprint e.message\t\t\t\t\n\t\t\t\traise e\n\t\tsys.exit(\"Problematic playbook, or inaccurate or invalid configuration\")\n\texcept:\n\t\tprint \"An exception occured during inventory evaluation"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n   \tlog.debug(\"Running test validation on %s\" % self.test_type)\n\n   \tspec_version = '9999'\n\t\n   \tif extract_spec_version:\n\t\tlog.debug(\"In this version v%s, we are expecting an unpacked version.\" % spec_version)\n\t    \n   \ttry:\n\t    self.spec_version = float(self.spec_version)\n\t    if spec_version == 'fail':\n\t\traise SystemExit('ERROR: Invalid spec_version %s' % self.spec_version)\n\t    self.type_version = float(self.type_version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n####################################################################################################\n\nclass InventoryContainer(object):\n\t\"\"\"\n\tInventory containers are a way of displaying a list of hosts to\n\trun on a host list.\n\t\n\tEach Instance of a class stores the list of services that are created\n\ton that host.\n\t\n\tExample:\n\t\tinv: InventoryContainer(spec=__name__)\n\t\tinv['inventories.service'] = Service(\"inventories\",\"inv\",...)\n\t\tinv['host_lists'] = []\n\t\tinv['alarms'] = []\n\t\tinv.read_config()\n\t\t\n\t\tinv['alarms'].append"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tfor digest in manifest_files:\n\t\t\tkey = digest[0]\n\t\t\tchecksum_hash = self.get_hash_for_manifest_file(manifest_files, digest)\n\t\t\tfound = 0\n\t\t\tif self.compare_manifest_hashes(digest, manifest_files[digest], key, keyhash=hash_value(key)):\n\t\t\t\tfound = 1\n\n\t\t\tfor checksum in digests_used:\n\t\t\t\tif checksum_hash == digest:\n\t\t\t\t\tself._result.append(key)\n\t\t\t\t\tbreak\n\t\t\tif found ==0:\n\t\t\t\t# All digests used successfully."}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n#\tdef main():\n#\t\tmanifest = ManifestFromFile(\"meh-demo.manifest\")\n#\t\tmanifest.read_manifest(\"meh-demo.ini\")\n#\t\tdigests_used = manifest.get_digests_used()\n#\t\tcheck_digests_present_and_used(manifest, digests_used)\n\n\ndef print_info(manifest_name, info):\n        print(\"\\t\".join([str(c) for c in info.split(\"\\n\")]))\n        print(\"\\tSummary:\", info)\n        print(\"\\t\".join"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tself.verify_manifest(manifest_files, [self.manifest_path], all_digests=digests_used)\n\n\t@staticmethod\n\tdef check_all_manifest_files(path, error_messages):\n\t\t\"\"\"\n\t\tChecks all manifest files in given path and raises MissingDigests or\n\t\tMalformedDigestsError if manifests are missing or malformed.\n\t\t\"\"\"\n\t\tfor manifest_file in Path(path).glob('*'):\n\t\t\tif manifest_file.name =='manifest.json':\n\t\t\t\tmanifest = manifest_file.read_text()\n\t\t\t\tif not"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\nclass HashCheck:\n\t\"\"\"\n\tCheck the md5 of a package.\n\t\n\tIt depends on various package-hash files, and the main files for that hash are:\n\t\n\t* ``base_hash/config/package.hash``\n\t* ``base_hash/config/package.hash.md5hash``\n\t\n\t\n\tParameters\n\t----------\n\tmanifest_files : list of str\n\t\tA list of the files of which to check the digest. For\n\t\texample::\n\t\t\n\t\t[\n\t\t\t\"/path/to/foo.c\", \n\t\t]\n\t\n\tdigests_used"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n#\t\tprint \"Checking all digests...\"\n\t\n\tfor file in manifest_files:\n\t\ttry:\n\t\t\twith open(file, 'rb') as afile:\n\t\t\t\tself._check_file(\n\t\t\t\t\tfile, afile.read() if not digests_used else None)\n\t\t\tself._file_checksum_ok(file)\n\t\texcept IOError:\n\t\t\tprint \"ERROR - unable to read %s\" % file\n\t\n\tif any((self._file_checksum_ok, self._file_checksum_failed)):\n\t\tprint \"ERROR - all hashes verified!\"\n\t\n\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n#\t\tprint()\n#\t\t\n\n\t# Check for manifests that are also in manifest\n\tfor item in manifest_files:\n\t\t\n\t\t(fop_name, fop_data) = item\n\t\tfor digest_file in digests_used:\n\t\t\tif fop_name == digest_file:\n\t\t\t\tprint(\"FILE '%s' is already used and will not\" % digest_file)\n\t\t\t\treturn True\n\t\tprint(\"UNKNOWN CHECKSUMS\")\n\t\t\n\t\t# Check for files to hash\n\t\tfor file_name in manifest_files:\n\t\t\tcheck_file = self"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\t\n\tmanifest_digest_list = []\n\t\n\tfor fileid in manifest_files.iterkeys():\n\t\t\n\t\t# check each file in archive, extract file name\n\t\tf_out = fileid.split(\":\")[0]\n\t\tprint \"Checking file %s\" % f_out\n\t\t\n\t\tf = open(f_out, 'rb')\n\t\t\n\t\t# read in all lines of a digest file, read it into a list\n\t\tdigest_list = [line for line in f]\n\t\t\t\n\t\tdigest_files_used_list = []\n\t\tfor x in digest_list:\n\t\t\t\n\t\t\tdigest_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\t\n\tif not len(manifest_files):\n\t    return\n\n\t# Verify that every item in the manifest has a digest associated with it\n\t# Add it. Lazily check everything in the manifest to generate digests\n\t# and also check that all digests are present.\n\n\tdigests_used_set = set(digests_used)\n\tif not digests_used_set.issubset(set(manifest_files)):\n\t    raise InvalidDigestsException(\n\t\t\"Manifest has a digest associated with it. It is needed for the test to work.\\n\"\n\t    + \"The"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\traise NotImplementedError\n\ttry:\n\t\tinfo = self.read_manifest(manifest_files[0])\n\texcept IOError:\n\t\treturn False\n\tif info.digests_used == 0:\n\t\treturn False\n\n\tdigest_sets = self._build_digest_set(info)\n\tif digests_used > 0:\n\t\tfor digest_sha in manifest_hashes:\n\t\t\tif info.digests_used < digest_sets[digest_sha]:\n\t\t\t\tprint(\"WARNING: digest {digest} used in manifest, is less than {seen} {digest_used_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n   \tmanifest_digests = []\n\tfor f in manifest_files:\n\t    if os.path.exists(f):\n\t\twith open(f, 'r') as file:\n\t\t    manifest_digests.append(file.read())\n\tif len(manifest_digests) == 0:\n\t    return True\n\telse:\n\t    for digest in manifest_digests:\n\t\tif digest in digests_used:\n\t\t    return True\n\t\telse:\n\t\t    return False\n\ndef filter_manifest(s):\n    \"\"\"\n    Check for manifest, if manifest with correct SHA1 is requested\n    \"\"\"\n"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tversion = prior.get(\"version\")\n\tif version!= None and version!= \"\":\n\t\tvalidation_result = prior.get(\"validation_result\")\n\telse:\n\t\tvalidation_result = None\n\t\n\t\n\tif validation_result!= None:\n\t\tif version is None:\n\t\t\tvalidator = validate_version(self.get_version(validation_result, \"version\"))\n\t\telse:\n\t\t\tvalidator = validate_version(self.get_version(validation_result))\n\t\n\t\t\n\t\t\n\t\tvalidated_version = validator.version.from_string(version)\n\t\tvalidation_results"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tif len(prior) > 3:\n\t\traise ValueError('prior must be a tuple of (inventory_version, track, required, item)')\n\t\n\tself.validate_versions_in_inventory(prior)\n\n\tfor version, track, required, item in prior:\n\t\tif version < _MIN_SUPPORTED_MAJOR_VERSION:\n\t\t\traise ValueError('must be less than %r, got %r' % (_MIN_SUPPORTED_MAJOR_VERSION, version))\n\t\tif version > _MIN_SUPPORTED_MINOR_VERSION:\n\t\t\traise ValueError('must be at least %r"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\n\tif isinstance(prior, InventoryValidator):\n\t\tif prior.get('version'):\n\t\t\tself.valid = False\n\t\t\tself.error_message = 'This is a valid version of the database, you must upgrade to a lower version of the current inventory version of the database. You can store at least version {} or newer, please upgrade this one.'.format(prior.get('version'))\n\n\n\telif isinstance(prior, dict):\n\n\t\t#TODO: Check type of \"inventories\" and \"inventories_data\"\n\t\t#The \"inventories\" and \"inventories_"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\nclass InventoryValidator(Validator):\n\t\"\"\"\n\tClass to check for valid inventory objects.\n\t\"\"\"\n\tdef __init__(self, message_required=False, message_does_not_exist=False, message_no_value=False):\n\t\tValidator.__init__(self, message_required, message_does_not_exist, message_no_value)\n\t\tself.min_count = 0\n\t\tself.max_count = 0\n\t\t\n\tdef on_message(self, message):\n\t\t\"\"\"\n\t\tHandle message validation.\n\t\t\n\t\t@param message: message to be"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tversion = self.format(priority=\"prior-version-1\")\n\tif not self.is_valid_version(version):\n\t    raise ValidationException('\"%s\" is not a valid version for prior \"%s\"' % (self.name, version))\n\nclass InventoryValidator(object):\n    def validate_source(self, source, filename):\n        pass\n\n    @abstractmethod\n    def validate_remote(self, target, source, file_path):\n        pass\n\n\nclass Inventory(object):\n    \"\"\"\n    The inventory schema is an object defining various things for managing inventory\n    instances. An inventory belongs to either"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\tfor i in self.get_valid_prior().iterkeys():\n\t\t\ttry:\n\t\t\t\tif prior[i] < self.get_default_prior()[i]:\n\t\t\t\t\treturn False\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\treturn True\n\t\n\tdef get_valid_prior(self):\n\t\treturn {\n\t\t\t\"type\": \"container\",\n\t\t\t\"version\": self.version,\n\t\t\t\"container_name\": self.getName(),\n\t\t\t\"inventory_validation_type\": \"null\",\n\t\t\t\"inventory_validation_version\": 1,\n\t\t\t\"inventory_timestamp\": datetime.datetime"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tif type(prior)!= type(InventoryValidator):\n\t\traise TypeError(\"The prior type must be a InventoryValidator object, got \"\n\t\t\t\"'{0}' instead.\".format(type(prior)))\n\t\n\tif not self.is_valid():\n\t\traise ValueError(\"The prior is not valid.\")\n\t\n\tif not isinstance(prior.version, self.VERSION):\n\t\traise TypeError(\"The prior version must be a '{0}' object, got '{1}' instead.\"\n\t\t\t.format(type(prior.version), type(prior.version)))\n\t\n\tif prior.version < self.VERSION"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n#\t\tprior needs to be a ConceptValidator\n#\n#\t\tcheck if given argument is a priority constraint of the current production\n#\t\t:type\tprior:\tobject\n#\t\t\n#\t\treturns True if given argument is a priority constraint of the specified\n#\t\t:type\tprior:\tbool\n#\t\t\n#\t\tExample::\n#\t\t\n#\t\t\n#\t\tfrom FormalDependencyParser import FormalDependencyParser\n#\t\t\n#\t\tparser = FormalDependencyParser(self.input_file, verbose=False)\n#\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tif isinstance(prior, InventoryValidator):\n\t\treturn prior\n\t\n\tif not isinstance(prior, Inventory):\n\t\t\n\t\traise Exception((\"Input data structure can not be of type \"\n\t\t\t\t\"InventoryValidator or Inventory object.\"))\n\t\n\t\n\tif len(prior.keys)!= 2:\n\t\traise Exception(\"Unexpected input: prior contains more than 2 items.\")\n\t\n\tif not isinstance(self, Inventory):\n\t\t\n\t\traise Exception((\"Input data structure can not be of type \"\n\t\t\t\t\"InventoryValidator or Inventory object.\"))\n\t\n\t\n\tif prior[0]!= \"AnsibleInventory\":\n\t\traise Exception((\"Input data"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\nclass Dependencies(Validator):\n\t\"\"\"\n\tDependency checker.\n\t\"\"\"\n\tdef __init__(self):\n\t\tValidator.__init__(self)\n\t\t\n\t\t# For instance, Dependency[1] has no name.\n\t\tself.name = None\n\t\t\n\tdef validate(self, attrs, source):\n\t\t# Check if it is valid.\n\t\tif not self.namecheck(attrs['target'], attrs['pkg']) and not source.enabled:\n\t\t\t# Invalid Dependency.\n\t\t\treturn None\n\t\t\n\t\t# Get the dependency tree.\n\t\tdep = self._getdep(attrs['target"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\t# first, see which files are affected\n\tphysical_state_files = [x for x in inventory.state_files if x.physical_path]\n\tphysical_files_with_state = set(logical_state_files)\n\tin_state = dict()\n\t\n\t# now, compute full set of logical_path_ids for each logical file...\n\tfor logical_path_id in set(physical_files_with_state):\n\t\tphysical_files_with_state.discard(logical_path_id)\n\t\t\n\t\t#...and merge them down for easy checking"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tresults = {}\n\t\n\tfor version in sorted(reversed(inventory.version_lookup.versions['stable'])):\n\t\t# this is actually a map of physical_path_in_state -> filenames (keys)\n\t\t\n\t\tresult = set()\n\t\tfor filename in inventory.versions['stable'][version]:\n\t\t\tcandidate_key = filename.replace(version, \"unknown_\")\n\t\t\tresult.add(candidate_key)\n\n\t\t\tresult.add(filename)\n\t\t\n\t\tresult_union = set(result)\n\t\tresults[version] = frozenset(itertools.chain(result_union, result))\n\t\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tlogical_map = {}\n\t\n\t# Only consider logical if the version is known.\n\tif version!= inventory.RESOURCE_VERSION:\n\t\tlogical_paths = inventory._parse_logical_path_map(inventory.RESOURCE_VERSION)\n\n\t\t# If an entry's path is too large, we only consider it to be a directory.\n\t\tif len(logical_paths) >= inventory.MAX_RECURSION_DEPTH:\n\n\t\t\tfiles_in_dir = {}\n\t\t\t\n\t\t\tfor path in logical_paths:\n\t\t\t\tparent = path.path.parent\n\t\t\t\tif parent == \".\": "}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\n\tfrom collections import set_union\n\t\n\tlogical_paths = set()\n\tfor rel_path in inventory['_meta']['hostvars'].keys():\n\t\tlogical_paths.update({\n\t\t\tget_file_contents(os.path.join(inventory['_meta']['hostvars'][rel_path]['hostvars_path'], version))).intersection(\n\t\t\tget_file_metadata(os.path.join(inventory['_meta']['hostvars'][rel_path]['hosts_path'], version))).intersection(\n\t\t\tget_file_files(os.path.join(inventory['_meta']['hostvars'][rel_path]['extra"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\t# Get version_file_path:\n\t# 1) Get version_file_path from VERSION_FILE_PATH_MAP/VERSION_FILE_PATH_MAP.txt\n\t# and read in one line at a time\n\n\t# Return:\n\tdata = get_version_file_content(file_content=VERSION_FILE_PATH, version=version)\n\t\n\t# Split:\n\t# Version:\t\tversion\n\t#     /VERSION_FILE_PATH_MAP, VERSION_FILE_PATH_MAP.txt\n\t#        VERSION_FILE_PATH_MAP\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\t# Convert inventory to dict\n\tmapping = Inventory._get_mapping_for_version(version)\n\t\n\t# Check which file is a leaf package and return list as dict, and file_content\n\tdef get_leaf_path_map(content_package):\n\n\t\tleaf_paths = []\n\t\tleaf_stack = [content_package]\n\t\t\n\t\tfor part in content_package.split('/'):   # Look for the leaf\n\t\t\tfilenode = part.rsplit('/',1)[-1]\n\t\t\t\n\t\t\tif filenode in leaf_stack:\n\t\t\t\tleaf_paths.append(filenode)\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\tlogical_dict = dict()\n\n\t\t# Find a new version of the inventory.\n\t\tfor version in inventory:\n\t\t\tif version in inventory:\n\t\t\t\tfor m in inventory[version]:\n\t\t\t\t\tm = m.replace('//', '/')\n\t\t\t\t\tif '/' in m:\n\t\t\t\t\t\tlogical_path_in_state = m.split('/')[0] # leaf node\n\n\t\t\t\t\tm_content_list_path   = \"/\".join(m.split('/')[1:])\n\n\t\t\t\t\t# Check to see if the path is still in the list.\n\t\t\t\t\tif logical_path_in_state not in m_content"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\t# Get mapping of version to file in inventory\n\tmap = get_mapping(inventory, 1, version, \"file_mapping\")\n\tif not map:\n\t\treturn {}\n\t\n\t# Get mapping of version to file in inventory\n\tmap_contents = get_mapping(inventory, 1, version, \"file_mapping_contents\")\n\tif not map_contents:\n\t\treturn {}\n\t\n\t# Generate set of files to be duplicated in an inventory\n\tall_files = set(map_contents.keys())\n\tfor path in map_contents.keys():\n\t\tfiles ="}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\t# Collect state keys in inventory\n\tlogical_paths = get_paths_in_sstate_dir(inventory, version)\n\tlogical_files = {}\n\t\t\n\tfor path in logical_paths + (\n\t\t(path + \"/_states\") if path.startswith(version)\n\t\telse (path + \"/_states\")\n\t):\n\t\t# Try to match files in state directory, but only if the file does not exist\n\t\tstate_file = inventory.get_state(path)\n\t\tif not state_file_exists(path):\n\t\t\treturn set([])\n\t\t\n\t\tf"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tlogical_path_map = {}\n\t\n\tfor filename in inventory.get_path_names():\n\t\tif filename.startswith(\"state/\"): continue\n\t\t\n\t\trelative_path = \"%s/state/block/\" % inventory.get_path(filename)\n\t\t\n\t\tif relative_path in logical_path_map:\n\t\t\tfile_contents = open_file(qualified_name(filename))\n\t\t\tlogical_path_map[relative_path] = file_contents.readlines()\n\t\t\n\t\telse:\n\t\t\tlogical_path_map[relative_path] = os.popen(\"file -b -"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n#\t\tif not type(fixity) is dict or not fixity.has_key('info') or not fixity['info'] or not fixity.has_key('data'):\n#\t\t\traise RtfException(\"Need a YAML dictionary\")\n#\n#\t\tdata_path_tokens = []\n#\t\t\n#\t\tfor key in fixity['info'].keys():\n#\t\t\tif not key == 'data':\n#\t\t\t\traise RtfException(\"Missing \\'info\\' key in compact fixity block\")\n#\n#\t\t\tif 'file' in"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\tvalid_types = set([])\n\tall_types = set([])\n\t\n\tfor f in manifest_files:\n\t\tfp = open(f, 'r')\n\t\tdata = fp.read(1024)\n\t\tif len(data) < 1:\n\t\t\tassert False\n\t\telif not self.valid_types.issuperset(set(all_types)):\n\t\t\tassert False\n\t\ttry:\n\t\t\troot = ET.fromstring(''.join(data))\n\t\texcept ET.ParseError:\n\t\t\traise SafeException(_(\"invalid package file %s\") % f)\n\t\t\t\n\t\tfile_name ="}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\tfixity[\"manifest\"] = manifest_files\n\t\n\treturn(fixity)\n\t\n\t\n\t\"\"\"\n\tRemove any empty entries.\n\t\n\tGiven all the elements in the manifest, remove any trailing whitespace.\"\"\"\n\tnewentries = []\n\twhile not newentries:\n\t\tnewentries.append(\"\")\n\tnewentries[0] = newentries[0][:-1]\n\tnewentries[-1] = newentries[-1][:-1]\n\t\n\treturn(newentries)\n\t\t\ndef validate_rerun_rules(fixity, rules_file=None):\n\t\"\"\"\n\tValidate"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n        for f in fixity[:-1]:\n            fullpath = os.path.join(os.path.abspath(self.path), f)\n            if os.path.isfile(fullpath):\n                with open(fullpath, 'r') as f:\n                    doc = yaml.safe_load(f.read())\n                for section in doc.get('parameters', []):\n                    param, value = section['parameters'][-1].split('=', 1)\n                    if param == 'fixity':\n                        self._mark_fixity(f)\n                    elif param in manifest_files:\n                        check_manifest(os.getcwd(), f,"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\n\t# Extract contents from manifest\n\t# TODO: Handle multiple files\n\tcontent = []\n\tfor manifest in manifest_files:\n\t\ttry:\n\t\t\twith open(manifest) as f:\n\t\t\t\tcontent.append(f.read())\n\t\texcept IOError as e:\n\t\t\tlog.error(\"Warning: Error reading.in file %s; %s\", \\\n\t\t\t\t\tmanifest, e.strerror)\n\n\t# Validate\n\tfor fix in fixity:\n\t\tif fix['md5'] == fix['tag']:\n\t\t\treturn (True, content)\n\n\t# Compute missing files\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\t# Collect the manifest files\n\tfor manifest_file in manifest_files:\n\t\ttry:\n\t\t\twith open(manifest_file,'r') as f:\n\t\t\t\tfixity_tree = etree.parse(f, etree.XMLParser(remove_blank_text=True))\n\t\t\t\tfixity_root = fixity_tree.getroot()\n\t\t\t# Validate the file\n\t\t\tfor node in fixity_root.find('files').findall('file'):\n\t\t\t\tif node.text is None or node.text in fixity_entries:\n\t\t\t\t\tmanifest_files.remove(os."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\n        config = ConfigParser.ConfigParser()\n        config_config_file = os.path.join(os.path.dirname(__file__),'manifest', 'defaults.cfg')\n        parser = SafeConfigParser()\n\n        try:\n            parser.read([config_config_file])\n        except:\n            print 'Failed to read default configuration'\n            try:\n                parser.remove_section('default')\n            except:\n                pass\n\n        try:\n            os.mkdir(os.path.join(os.path.dirname(__file__), 'inventory'))\n        except:\n            pass\n\n        # Verify the fixity config sections (except the main one"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n########\n#  REMOVE\n########\n\n\t#################################################################\n\t@staticmethod\n\tdef rehash(obj_list):\n\t\t\"\"\"\n\t\tPerform the rehashing of a list of files in an instance list.\n\t\t\"\"\"\n\t\tret = []\n\t\tfor obj in obj_list:\n\t\t\tret.append(self.rehashable(obj))\n\t\treturn ret\n\n\t#################################################################\n\t@classmethod\n\tdef from_file(cls, manifest_file):\n\t\t\"\"\"\n\t\tPrepare the fixity block from a file like object.\n\t\t\n\t\t@param manifest_file: The"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\n\t# Check file types\n\tif fixity == \"meta\" and not \"application/vnd.diff\" in fixity_block.keys():\n\t\traise Exception(\"Fixity block has type'meta' and no 'application/vnd.diff' in the inventory block.\")\n\tif fixity == \"meta\" and  \"application/json\" in fixity_block.keys():\n\t\traise Exception(\"Fixity block has type'meta' and 'application/json' in the inventory block.\")\n\tif fixity == \"data\" and not re.match('\\d+$', fixity_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\n\tdef validate_manifest(self, manifest_file):\n\t\"\"\"\n\tValidate manifest file in inventory.\n\t\n\tThis method will validate that the manifest file for the provided\n\tmanifest is correct, and if so will result in an empty list in the inventory.\n\t\"\"\"\n\n\t# Validate manifest file path\n\tfilepath = os.path.normpath(f'{self.base_dir}/{manifest_file}')\n\tassert os.path.isfile(filepath), u'File does not exist'\n\n\t# validate file extension\n\textension = os.path.splitext(manifest_file)[1]"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tif path == '' or path.isspace():\n\t\t\treturn []\n\telif os.path.isfile(path):\n\t\treturn [path]\n\telse:\n\t\treturn []\n\ndef add_files(path, files, ignore):\n\t\"\"\"\n\tAdd files to the working dir (if not there),\n\tthe files specified by `files`\n\t\"\"\"\n\tfiles_append = files\n\tif files == []:\n\t\tfiles = []\n\t\treturn files\n\tif type(files)!= types.NoneType:\n\t\tfiles = [files]\n\tfor f in files:"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\ndef files_recursive(path):\n\t\"\"\"\n\tReturn the files below `path` as a list, relative to root \n\tif possible, e.g. `[a, b, c] = [a, c]`.\n\n\tExample\n\t======\n\t>>> paths = [r'c:\\temp\\foo\\bar.txt', r'a.txt']\n\t>>> files = list(files_recursive(paths))\n\t>>> files_recursive(r'.')\n\t['a.txt']\n\t>>> files_recursive(r'c:\\temp\\foo\\bar.txt"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\ndef files_walk(path, pattern):\n    \"\"\"\n\tyield each dir with files found under `path`\n\t\n\t>>> pattern_list('dir/*.txt', '*.pyc') == files_list('dir/**/*.py')\n\tTrue\n\t>>> patterns(['*.txt','*.log','.swp']) == list(files_walk('dir/**/*.py'))\n\tTrue\n\t>>> patterns(['file_1','file_1/*.txt','file_1/*.swp', 'file_2','file_1/*.txt'])\n\tTrue\n\t>>> patterns('*.txt','*.pyc"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t if not path.endswith(\"/\"):\n\t\t\tpath = path + \"/\"\n\n\t\tfiles = os.listdir(path)\n\t\tfiles.sort()\n\n\t\tret = []\n\t\tfor x in files:\n\t\t\tif not x.endswith(\".csv\"):\n\t\t\t\tret.append(x)\n\n\t\treturn ret\n\n\treturn files\n\n\n\nclass CustomEncoder(json.JSONEncoder):\n\tdef default(self, obj):\n\t\tif isinstance(obj, decimal.Decimal) and isNaN(obj):\n\t\t\treturn float(\"NaN\")\n\t\telif isinstance(obj, datetime):\n\t\t\treturn obj.strftime(\"%Y"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\n\t\t\n\tfiles_list = []\n\tif path == \"\" or path == \".\":\n\t\treturn [(\"\",\"\",\"/\")]\n\tif path[-1]!= \"/\":\n\t\tpath = \".\" + path\n\tfiles = listdir(path)\n\tfor f in files:\t\t\t\t\n\t\tprint \"%s%s\" % (path,f)\n\t\tfiles_list.append(f)\n\t\t\n\treturn files_list\n\n\nif __name__ == \"__main__\":\n\tflist = files_list(sys.argv[1])\n\tfor path in flist:\n\t\tfiles_list("}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t return [path / fn for fn in os.listdir(path) if fn.endswith(\".bin\")]\n\n\ndef folders_list(path):\n\t\"\"\"\n\tReturn the folders in `path`\n\t\"\"\"\n\t return [path / fn for fn in os.listdir(path) if fn.endswith(\".json\")]\n\n\ndef files_by_pattern_id(path):\n\t\"\"\"\n\tGet the files of a specific pattern\n\n\t:param path: Path to `search` folder\n\t:return: list of paths containing the pattern\n\n\t\"\"\"\n\treturn [file.replace(\".bin\","}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\n\n\tfilenames = os.listdir(path)\n\n\toutfiles = []\n\tfor filename in filenames:\n\t\tif re.search(r\"(.*){}\".format(re.escape(filename)), path):\n\t\t\toutfiles.append(filename)\n\n\t\telif os.path.isdir(path):\n\t\t\toutfiles = files_list(path+\"/\"+filename)\n\n\treturn outfiles\n\n\nif __name__ == '__main__':\n\ttry:\n\t\tos.makedirs(\"/disk/home/wenhuang/Desktop/temp/\")\n\texcept OSError as exc: # Python >2."}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tif path[-1]!= '/':\n\t\t\tpath = path + '/'\n\n\t\tfile_list = []\n\t\tfiles = os.listdir(path)\n\n\t\tfor f in files:\n\t\t\tif os.path.isfile(path + \"/\" + f):\n\t\t\t\tfile_list.append(f)\n\n\t\t\tif os.path.isdir(path + \"/\" + f):\n\t\t\t\tdirectory = path + \"/\" + f[:-4]\n\n\t\t\t\tfor file in os.listdir(directory):\n\t\t\t\t\tpath_name = file[:-4]\n\t\t\t\t\tfile_list.append( directory + path_name )\n\n\t\t\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t return listdir(path)\n\n# return the files in `path`\ndef files_list_sorted(path):\n\t\"\"\"\n\tReturn the files in `path` sorted\n\t\"\"\"\n\t files_list = []\n\t files_list_sorted= []\n\t for current, dirs, files in walk(path):\n\t\t\tfiles_list_sorted.extend(files)\n\t\t\tfor f in files_list_sorted:\n\t\t\t\tfiles_list += files\n\t\t\tfiles_list.extend(files)\n\treturn files_list\n\n# return the subdirectories under the given path.\ndef subdirs("}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\ndef files_size(path):\n\t\"\"\"\n\tReturn the size of the `path`\n\t\"\"\"\n\tpass\n\ndef get_list_files(list):\n\t\"\"\"\n\tReturn the files in list\n\t\"\"\"\n\t\n\tlist_file_info = {}\n\ttry:\n\t\t# Iterate through the filename list\n\t\tfor item in os.listdir(path):\n\t\t\t# Check whether this is an image file\n\t\t\tif os.path.isfile(os.path.join(path, item)) is True:\n\t\t\t\t# Get the name of the file\n\t\t\t\tfile_"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tfile_data = dict()\n\t\n\tif xls_utils.path_exists(source):\n\t\tfor filename in files:\n\t\t\tif os.path.basename(filename) == xmls_utils.path_basename(source):\n\t\t\t\tfile_data[filename] = md5_for_file(filename)\n\telse:\n\t\tfor filename in files:\n\t\t\tif os.path.basename(filename) == source:\n\t\t\t\tcontinue\n\t\t\tfile_data[filename] = md5_for_file(filename)\n\t\n\treturn file_data\n\n\ndef _find_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\timport sys\n\t\n\t# parse the arguments and get the name of the input file or folder\n\tif len(sys.argv) < 2:\n\t\tsys.stderr.write(\"USAGE: %s input_file path\\n\" % sys.argv[0])\n\t\tsys.exit(1)\n\n\txmls = []\n\tfor dirname, dirnames, filenames in os.walk(sys.argv[1]):\n\t\tfor filename in filenames:\n\t\t\txmls.append(filename)\n\tfiles = [x for x in xmls if"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tfiles_by_basename_dict = defaultdict(list)\n\t_zipdir_func = lambda p: p\n\tfor f in files:\n\t\tfor i in _zipdir_func(f):\n\t\t\tfiles_by_basename_dict[i].append(str(f))\n\t\n\t# get the path for this archive\n\tarchive_path = os.path.join(source, *files_by_basename_dict.keys())\n\t\n\n\tdata = {\n\t\t'media_path': _zipdir_func(source),\n\t\t'tar_path': archive"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tgroup_xml_filename = None\n\t\n\tfor filename in files:\n\t\tif filename.endswith('.xml'):\n\t\t\txml_filename = os.path.splitext(filename)[0]\n\t\t\tgroup_xml_filename = _unique_group_xml_filename(source, filename, xml_filename, {})\n\t\t\tbreak\n\n\treturn group_xml_filename\n\t\n\n\ndef _load_sources(source):\n\t\"\"\"\n\tLoad sources from XML file\n\t\n\tParameters\n\t----------\n\tsource : ElementTree._Element\n\t\n\t  XML source (http://www.crafter.org/xml"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tif isinstance(files, list):\n\t\tfor i, f in enumerate(files):\n\t\t\tif \"xml/\" in f:\n\t\t\t\tname = xml_basename(i)\n\t\t\t\tif name.endswith(\".zip\"):\n\t\t\t\t\tname = name[:-4]\n\t\t\t\telse:\n\t\t\t\t\tname = name[:-4]\n\t\t\t\tf[\"name\"] = name\n\t\t\telse:\n\t\t\t\tcontinue\n\telse:\n\t\tname = files\n\t\n\t# Create a dict instance\n\td = {}\n\tfor f in files:\n\t\tif \"name\" not in f:\n\t\t\tcontinue\n\t\tname ="}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t    \n\troot = ET.Element('Files')\n\tfor filename in files:\n\t    \n\t    info = ET.Element('File')\n\t    \n\t    root.append(info)\n\t    info.text = os.path.basename(filename)\n\n        for element in xmls:\n\t    \n            element_name = element.get('Name')\n            extension = os.path.splitext(element_name)[1]\n            \n            if os.path.isfile(element_name):            \n\t        extension = extension.lower()             \n                if extension in XML_EXTENSION_DICT:\n\t        extension_path          = XML_EXTENSION"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t \n\tfiles = _get_matching_files(files, xml_filename) \n\tgroups = {}\n\tfor filename in files:\n\t\tfilename = _extract_xml_file_name(filename, xmls)\n\t\tname = filename.split(' ')[-1]\n\t\tgroups[name] = {}\n\t\tfor group in xmls:\n\t\t\tname = group[0]\n\t\t\tif name in groups:\n\t\t\t\tgroups[name][group] = groups[name].get(group, [])\n\t\t\t\tgroups[name][group].append(filename)\n\t\n\t\n\treturn groups\n\n\ndef"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\t# get list of files to convert to xml\n\tfiles = [xml_filename+x for x in files]\n\t\n\t# get list of files to include in the xml_filename\n\tfiles = [i for i in files if i.find(xml_filename)>0]\n\tfiles = [i for i in files if i.find(files[0].lstrip('/'))!=-1]\n\t\n\t# group into groups by name and data\n\tgroups = {}\n\tfor i in range(len(files)):\n\t\tif files[i].find('filename=\"')>0:\n\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\txml_names = [x.replace(os.sep, '.') for x in xmls]\n\t\n\tfile_dict = defaultdict(list)\n\t\n\tfor fn in files:\t\t\t\t\t \n\t\twith open(fn) as xml_file:\n\t\t\txml_names2 = [x.replace('.xml', '') for x in xml_file]\n\t\t\t\n\t\t\tfor xml_name in xml_names2:\t\n\t\t\t\ttry:\n\t\t\t\t\txml_file2 = xml_file.replace('.xml', '')\n\t\t\t\texcept KeyError:\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\txml_file2 = xml_file2."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t # Create list of files if needed\n\t\tif not files:\n\t\t\tfor xml_filename in xmls:\n\t\t\t\tif zipfile.is_zipfile(xml_filename):\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\treturn None\n\n\t\treturn_data = {}\n\t\tfor file in files:\n\t\t\treturn_data[file] = md5(file)\n\n\t\t# If successful, return list\n\t\tif True in return_data.values():\n\t\t\treturn return_data\n\t\telse:\n\t\t\treturn None\n\t\t\n\t# Else return single file\n\telse:\n\t\treturn None\n\t\n\t\ndef _load"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tmatchlist = []\n\tfor root, dirnames, filenames in os.walk(sys.path[0]):\n\t\tfor filename in fnmatch.filter(filenames, \"*.py\"):\n\t\t\tif os.path.splitext(filename)[1] == '.py' and os.path.splitext(filename)[1]!= \".py\":\n\t\t\t\tcontinue\n\t\t\tif os.path.join(root, filename) in matchlist:\n\t\t\t\tmatchlist.append(filename)\n\tif len(matchlist) > 1:\n\t\treturn True\n\telse:\n\t\treturn False# Copyright ("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\t# Use our regex to get patterns for matching\n\t# this will allow us \"filename\" as well as \"dir/filename\" as well as\n\t# \"dir/..\" -> \"a\" - asterisks with the word A_i that starts with \"file/\"\n\tbase = os.path.basename(file_path)\n\tpattern = r\"\\s*%s(?P<filename>%s)\\s*\\([^)]*$\" % (base, prefix)\n\t\n\treturn (os.path.isfile(file_path) and\n\t\t\tre.match(pattern, file_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\tif prefix[:len(prefix)] == prefix:\n\t\treturn True\n\telse:\n\t\treturn False\n\t\n\"\"\"\nclass FPGA:\n\n\tdef __init__(self, filename = None, encoding = None, verbose = True):\n\t\t# read encoding if specified\n\t\tif encoding:\n\t\t\tencoding = encoding.lower()\n\t\t\n\t\t# set the current encoding for read\n\t\tself.encoding = encoding\n\t\t\n\t\t# the name of the dataset\n\t\tself.data_name = ''\n\t\t\n\t\t\t\t\n\t\t# read dataset metadata\n\t\tself.dataset = None\n\t\tself.metadata = {}"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\tprefix = os.path.expanduser(prefix)\n\tif not os.path.exists(file_path):\n\t\treturn False\n\t\t\n\tif file_path.endswith(\".po\"):\n\t\tprint(\"Checking if file is a.po file...\")\n\t\tif not (re.match(\"^..\", prefix) and re.match(\"^..\", file_path)):\n\t\t\tprint(\"Error: file path does not follow expected pattern:\")\n\t\t\tprint(\"   %s: is a.po file, must end with.po or.po.gz\" % prefix)\n\t\t\tprint(\"   %s:"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\t\n\tif re.compile(r'(?!\\.).+', re.IGNORECASE).search(prefix)!= None:\n\t\t# Identifies the package to which this package lives\n\t\tmyprefix = prefix\n\t\tprint(\"A prefix with this name already exists: \\n\")\n\t\traise Exception(\"Prefix \\\"\" + myprefix + \"\\\" is already in use\")\n\t\n\treturn os.path.exists(file_path)\n\t\ndef list_dir_files(directory) :\n\t\"\"\"\n\tEnumerate all the files in all given directory\n\t\n\tParameters\n\t----------\n\tdirectory : str\n\t    Directory name"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\treturn os.path.isfile(file_path) \\\n\t\n\n##\n# Copyright 2012-2019 Ghent University\n#\n# This file is part of EasyBuild,\n# originally created by the HPC team of Ghent University (http://ugent.be/hpc/en),\n# with support of Ghent University (http://ugent.be/hpc),\n# the Flemish Supercomputer Centre (VSC) (https://vscentrum.be/nl/en),\n# the Hercules foundation (http://www"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t \n\t\n\timport fnmatch\n\timport subprocess\n\t\n\tfiles = glob.glob(file_path)\n\t\n\tfor f in files:\n\t\tif fnmatch.fnmatch(f, prefix):\n\t\t\treturn True\n\t\n\treturn False\n\ndef get_root_project_path(dir):\n\t\"\"\"\n\tGet the directory path of a project\n\t\n\tParameters\n\t----------\n\tdir : str\n\t    Path to the project\n\tReturns\n\t-------\n\tstr\n\t    Directory path\n\t\n\tRaises\n\t------\n\tRaises\n\t------\n\tRaises\n\t------\n\tRaises\n\t------\n\tRaises\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\n\tsuffixes = ('.g', '.c', '.h', '.cpp', '.c++', '.s', '.S')\n\tpattern = r'%s([0-9a-zA-Z/.-]+)\\.([a-zA-Z0-9_]+)?' % '|'.join(suffixes)\n\tpatt = re.compile(r'^.([a-z0-9_]+|\\.[a-z0-9_]+)$')\n\tif not filename_match(filename_to_unicode(file_path), prefix):\n#\t\tmatches_file_by_short_prefix("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\n\tif pathjoin(prefix, file_path) == None:\n\t\treturn False\n\t\n\ttry:\n\t\twith io.open(pathjoin(prefix, file_path), 'r', encoding='utf-8') as fin:\n\t\t\tfor i, line in enumerate(fin):\n\t\t\t\tword = line.split()[0]\n\t\t\t\tfor j, ch in enumerate(line.split()[1:]):\n\t\t\t\t\tch = ch.strip()\n\t\t\t\t\tif ch == prefix+word[j:]:\n\t\t\t\t\t\treturn True\n\t\treturn False\n\texcept IOError:\n\t\treturn False\n\t\nfrom sklearn import"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tname = file_path.split('.')\n \t\n\tpattern = re.compile('.*%s' % prefix)\n\tmatch = pattern.match(name)\n\treturn match\n\ndef match_file_by_prefix_name(prefix, file_path):\n\t\"\"\"\n\tIdentify if a file belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\t"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\tfiles_re = re.compile(prefix + \"_([0-9][0-9.]*).txt\")\n\t\tfiles = list(map(lambda x: os.path.basename(x).strip(), files))\n\t\treturn [os.path.join(prefix,x) for x in files if files_re.search(x)]\n\n\treturn files\n\n\ndef select_filenames_by_extension(files):\n\t\"\"\"\n\tGet files which matches to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes inic"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t \n\tfiles_list = []\n\tfor file1 in files:\n\t\twith io.openfile(file1, \"r\", encoding='utf-8') as f1:\n\t\t\tfor file2 in f1:\n\t\t\t\tif file2.startswith(prefix + \".\"):\n\t\t\t\t\tfiles_list.append(file2)\n\t\n\n\treturn files_list\n\t\ndef select_filenames_by_prefix_and_suffix(prefix_and_suffix, files):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\n\tif prefix[-1]!= os.path.sep:\n\t\traise ValueError(\"Only first occurence of prefix is '%s'\" % (\n\t\t\t\tprefix))\n\tcurrent = files\n\tfounds = glob.glob(prefix + '*/')\n\tif len(founds) == 0:\n\t\traise ValueError(\"No filename matches %s\" % (prefix))\n\telse:\n\t\tfound = []\n\t\tfor x in founds:\n\t\t\tfound.append(os.path.join(current + x))\n\n\t\t# find common prefix\n\t\tsuffixes = filter(lambda x:"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfiles_list = []\n\tfor fname in files:\n\t\tif not fname.startswith(prefix):\n\t\t\tcontinue\n\t\t\n\t\tfiles_list.append(fname)\n\t\n\treturn files_list\n\ndef is_document_directory(folder, files):\n\t\"\"\"\n\tCheck if a directory exists.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    File Path or directory in the project\n\tfiles: str list\n\t    Files paths\n\tReturns \n\t-------\n\tbool \n\t\twhether document directory exists \n\t\"\"\"\n\t\n\tif not os.path.exists(folder):\n\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\n\tr = {}\n\t\n\t\n\tfor f in files:\n\n\t\tf = os.path.abspath(f)\n\t\t# print f\n\t\t\n\t\tif prefix:\n\t\t\tf = prefix + '.' + f\n\t\t\n\t\tif os.path.exists(f) and f not in r:\n\t\t\t# print 'f %s not found' % f\n\t\t\tr[f] = find_file(f)\n\t\t\n\t\telif os.path.exists(f):\n\t\t\t\n\t\t\tif prefix:\n\t\t\t\tf = prefix + '.' + f\n\t\t\tif os.path.isfile(f) and not os.path"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tif len(prefix)==0:\n\t\treturn []\n\t\n\tdef select_files_by_prefix(prefix, files):\n\t\t\"\"\"\n\t\tGet files which belongs to a document package.\n\t\t\n\t\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\t\n\t\tParameters\n\t\t----------\n\t\tprefix : str\n\t   \t\tName prefix\n\t\tfiles : iterable list\n\t   \t\tFiles paths\n\t\tReturns\n\t\t-------\n\t\t\n\t\"\"\"\n\t\n\tall_files = list(files)\n\t\n\tfiles_filtered = [\n\t\tfilename\n\t\tfor"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tnewlist = []\n\tfor f in files:\n\t\tif not fnmatch.fnmatch(f, prefix + os.path.sep + \"*.*\"):\n\t\t\tnewlist.append(f)\n\treturn newlist\n\n\n\ndef get_filenametypes():\n\t\"\"\"\n\tEnumerate of file extensions to be handled.\n\t\n\tReturns\n\t-------\n\tset\n\t\"\"\"\n\t\n\treturn set(\n\t\tfnmatch.filter(\n\t\t\tget_extension_filenames(),\n\t\t\tget_resource_filetype_filenames(),\n\t\t\tget_resource_filetype_resource_type_filenames()\n\t\t)"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfiles = set(files)\n\t\n\t# Lista os listas encontrados nel file_extensions\n\tfiles_ext = []\n\t\n\tfor prefix_path in prefix:\n\t\t\n\t\tprefix_ext = os.path.splitext(prefix_path)[1]\n\t\tfiles_ext = fnmatch.filter(files_ext, prefix_ext)\n\t\t\t\n\t\tfiles = [f for f in files if os.path.splitext(f)[1] in files_ext]\n\t\n\treturn files\t\t\n\t\n\n\ndef select_filenames_by_extension(ext):\n\t\n\t# Filtra tod"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfor filename in files:\n\t#\tfiles have to be in sub directories\n\t\tfiles_filtered = files[:]\n\t\tfiles_filtered.append(filename)\n\t\t\n\t\tfor path, subdirs, files in os.walk(prefix + \"/\" + filename):\n\t\t\tfiles_filtered = [f for f in files_filtered if f.startswith(prefix + \"/\" + filename)]\n\t\t\t\n\t\t\tfor item in files_filtered:\n\t\t\t\tif not item.startswith(prefix + \"/\" + filename):\n\t\t\t\t\tcontinue\n\t\t\t\treturn item\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\n\t\n\traise Error(\"Unable to find prefix {0}.\".format"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfiles = [f[:-len(prefix) + 1] for f in files]\n\t\n\t\n\tfiles_list = []\n\tfor f in files:\n\t\ttry:\n\t\t\tfiles_list.append(f.replace(prefix, prefix[len(prefix) + 1:]))\n\t\texcept IndexError:\n\t\t\tfiles_list.append(f)\n\t\n\treturn files_list\n\t\t\ndef select_docx_by_suffix(suffix, suffix_list):\n\t\"\"\"\n\tGet filepaths for given suffix files.\n\t\n\tRetorna os arquivos da lista `suffix`"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# Check the root element\n\tif folder[-1]!= '/':\n\t\tfolder += '/'\n\t\n\t# Create a list of packages \n\tpkgs = []\n\tfor root, dirs, files in os.walk(folder):\n\t\t\n\t\t# Check each file\n\t\tif ('install.py' in files) and ('uninstall.py' in files):\n\t\t\tpkgs.append(root.replace(folder, '').replace('/', '.'))\n\t\t\n\t# Create the dictionary for the data we want to output\n\t# This makes the dict for easy\n\tpkgs = {}\n\tpkgs['name'] = files"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\nimport fnmatch\nfrom itertools import islice\nimport re\nfrom copy import copy\nfrom math import floor, sqrt\ntry:\n\timport npyscreen\n\tfrom npyscreen import get_screen_size\n\timport numpy as npy\n\tfrom glob import glob\nexcept ImportError:\n\t# no nbconvert module available\n\tfrom scikits.data_paths import find_data_path\n\tfrom scikits.setuputils import pkg_data_filename\n\timport numpy as np\n\tfrom npyscreen import get_screen_size\n\timport datetime\n\nfrom os."}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\tfolders = glob.glob(folder + '/*.xml*')\n\t\tdata = {}\n\t\tif not folders:\n\t\t\treturn data\n\n\t\tfor folder in folders:\n\t\t\troot, ext = os.path.splitext(folder)\n\t\t\tif ext == '.xml':\n\t\t\t\txml_files = glob.glob(folder + '/' + '*xml')\n\t\t\t\troot = xml_files[0]\n\t\t\t\tdata[root] = {}\n\t\t\t\tfor xml_file in xml_files[1:]:\n\t\t\t\t\txml = extract_data(root + '/' + xml_file, root)\n\t\t\t\t\tif xml"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\t_result = get_files(folder, '*.xml')\n\t\tfor filename in _result:\n\t\t\t_new_dict = {}\n\t\t\t_new_dict['filename'] = os.path.basename(filename)\n\t\t\t_new_dict['name'] = os.path.splitext(filename)[0]\n\t\t\t_new_dict['description'] = _new_dict['name']\n\t\t\t_new_dict['type'] = 'file'\n\t\t\t_new_dict['size'] = os.path.getsize(filename)\n\t\t\t_new_dict['author"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\troot_folder = folder.split(os.sep)\n\t\n\tpkg_list = []\n\t\n\tfor file in folders:\n\t\t\n\t\t#get name of current package\n\t\t\n\t\tfilename = os.path.split(filename)[1]\n\t\tif (basename.endswith(filename)) and (filename.find(\"_\") < 0 or filename.find(\"__init__.py\") < 0):\n\t\t\tcontinue\n\t\t\n\t\tfolder_name = os.path.join(root_folder, folder, file)\n\t\t\n\t\ttry:\n\t\t\timport pkgutil\n\t\t\tdata = pkgutil.walk_packages(file)\n\t\t\tpkg_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\t# Load XML\n\txml_list = _load_xml_list(folder)\n\t\n\t# Convert to dictionary\n\treturn _dict_convert(xml_list)\n\n\n\ndef _add_package_files(xml_list):\n    \"\"\"\n\tAdd files to packages' data list\n\t\n\tParams\n\t----------\n\txml_list :  list\n\t    List of XML files to add\n\t\"\"\"\n\n    # Initialize\n    packages_count, pkgs_count = 0, 0\n\n    if not isinstance(xml_list, (list,tuple,dict)):\n        _error_msg = \"Invalid data in"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\tr = os.path.join(folder, \"packages.list\")\n\t\twith open(r) as f:\n\t\t\treturn parse_packages(f)\n\t\t\n\n\ndef dump_tree(folder, f):\n\t\"\"\"\n\tDump packages into specified file\n\n\tTakes folder as input and dump it into.tree format\n\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tf : str\n\t    Output path to dumped file\n\n\tReturns\n\t-------\n\tstr\n\t\"\"\"\n\t   with open(f, \"w\") as f:\n\t\t   json.dump(parse"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tdata = {}\n\tresult = []\n\tfor dirname, dirnames, filenames in os.walk(folder):\n\t\tfor filename in fnmatch.filter(filenames, '*.xml') :\n\t\t\tif not filename.endswith(\".xml\"): continue\n\t\t\t\n\t\t\tfullname = os.path.join(dirname, filename)\n\t\t\tprint fullname\n\t\t\t\n\t\t\tif filename == \"packages.xml\":\n\t\t\t\tfor item in os.listdir(os.path.join(folder, filename)):\n\t\t\t\t\tif not item.endswith(\".xml\"): continue\n\n\t\t\t\t\tpkg_data = {}\n\t\t\t\t\tfor item in os.listdir(os.path"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# create folder name\n\tfolder = directory+os.sep+'packages'\n\n\t# load all xml files\n\tfor file in find_files(folder):\n\t\ttry:\n\t\t\t# make sure file.xml exists\n\t\t\tfile_xml = ET.parse(file)\n\t\t\tfile_xml.find('project')\n\t\t\t\n\t\t\t# check file.xml is XML file\n\t\t\tpackage_file = file_xml.findall('package')\n\t\t\tif not(package_file) == []:\n\t\t\t\tprint 'File'+ file +'is no XML file in folder'+ folder\n\t\t\t\tcontinue"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\tbasepath = os.path.split(folder)[0]\n\tfor filename in os.listdir(basepath):\n\t\t\n\t\tfullpath = os.path.join(basepath, filename)\n\t\tif fnmatch.fnmatch(filename, '*.xml'):\n\n\t\t\t# if filename does not start with an extension\n\t\t\tif filename.startswith('.'):\n\t\t\t\tfilename = filename[1:]\n\t\t\t\t\n\t\t\t# if filename has extension, split it and append the dict into the\n\t\t\t# dict \n\t\t\tdictstr = filename\n\t\t\t\n\t\t\t# split into list of data\n\t\t\tdata, meta = _explore_folder("}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\timport xml\n\tfrom xml.dom.minidom import parse\n\ttry:\n\t\ttree = parse(file_path)\n\t\troot = tree.getroot()\n\texcept:\n\t\traise Exception(\"No houte n\u00e3o encontrado xml indirco final do arquivo. Nome do tipo de arquivo.\")\n\t\n\tasset_tag = root.findall('asset')\n\tif len(asset_tag) < 1:\n\t\traise Exception(\"habilitado de arquivo n\u00e3o existe: %s.\" % root.findall('asset')[0])"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\tif not file_folder.exists():\n\t\traise IOError(\"Corrigir um arquivo XML de uma branco no file da folder.\")\n\n\n\tif prefix.split(\".\")[-1] == \"assets\":\n\t\tprefix = prefix.replace(\".xml\", \"\")\n\n\tfilename = \"{}.xml\".format(prefix)\n\n\tfile_path = file_path.replace(\"assets\", \"renditions\", filename)\n\n\twith open(file_path, 'r' ) as file:\n\t\treturn json.loads(file.read())\n\n\n\tprint( \"{}\".format(file"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tpath, filename = os.path.split(file_path)\n\tbase_path = '/'.join([path, str(filename)])\n\tprint 'Inscri\u00e7\u00e3o do file {0}'.format(file_path)\n\t\n\t# Cria armazena, busco todas as tags de arquivos do \u00e1lveis\n\t# pelo formar as voc\u00ea tenha arquivos as tags da chave privadas\n\ttags = {}\n\tlines = open(os.path.join(base_path, filename))\n\tsoup = BeautifulSoup("}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\tpass\n\traise ValueError('Ao pessoa de um arquivo.')\n\t\n\t\t\ndef _eval_file_list(file_path):\n\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo\n\te do endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t    nome do arquivo"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tdef _parse(directory):\n\t\ttag = directory.tag == 'assets' and directory.get('source_folder')\n\t\tdirectory_path = os.path.join(directory.get('source_folder'), 'assets/ ')\n\t\tif tag: group_path = os.path.join(directory_path, os.path.split(tag)[1]) # @UnusedVariable\n\t\telse: group_path = None\n\t\tret = {file_path: {}, group_path: []}\n\t\t\n\t\tfor directory in os.listdir(directory_path):\n\t\t\tpath = os."}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n#\tfile_name = prefix+str(time.time())\n#\tfile_path = os.path.join(file_folder, file_name)\n#\tfile_path = os.path.join(home_folder, file_name)\n#\tfile_path = os.path.join(os.path.join(file_path, \"time\"), 'data', file_name)\n#\tfile_path = os.path.join(home_folder, file_name)\n#\tfile_path = os.path.join(file_path,"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tlista_files = os.listdir(file_path)\n\tlista_lista = []\n\tfor k in lista_files:\n\t\tif prefix in k and k!= \"assets\" and k!= \"renditions\":\n\t\t\tif \"_xml\" in k:\n\t\t\t\txml = get_document(_xml_file(k))\n\t\t\t\twith open(file_path+k+\".json\", \"w\") as writer:\n\t\t\t\t\tjson.dump(xml, writer, indent=4, sort_keys=True)\t\n\t\t\t\t\tlista_lista.append(xml)"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n#\t\tprint(\"file:\" + file_path)\n#\t\txml_list = open(file_path)\n\t##\t\txml_doc = xmltodict.parse(file_path)\n#\t\tprint(xml_doc[\"root\"])\n#\t\txml_list = xmltodict.parse(file_path)\n#\t\tprint(\"xml_list.keys(): \"+str(xml_list.keys()))\n#\t\t#xml_doc = ast.literal_eval(file_path)\n\txml_root = make_root(prefix)"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# no existe, veja o conte\u00fado ser\u00e1 aceito\n\t\n\ttry:\n\t\t\n\t\twith open(file_path+prefix + file_folder + filename ) as f:\n\t\t\tprint( f.read() )\n\t\n\t\n\texcept Exception as e:\n\t\t\n\t\tprint( e.message )\n\t\t\n\t\n\n\ndef _exec(command):\n\n\tprint( \"exec \" + command )\n\n\ndef main():\n\n\t# lista todas as judiciais \n\ttry:\n\t\t\n\t\t# check the command line arguments\n\t\t\n\t\topts, file_path, prefix, nprocs"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tif \"radar\" in prefix:\n\t\tif not os.path.exists(file_path):\n\t\t\tlogger.info(\"Arquivo inexistente com file %s\" % file_path)\n\t\t\treturn None, None\n\t\telse:\n\t\t\treturn file_path, None\n\telif \"sagittal\" in prefix:\n\t\tif not os.path.exists(file_path):\n\t\t\tlogger.info(\"SAGITUAL inexistente com file %s\" % file_path)\n\t\t\treturn None, None\n\t\telse:\n\t\t\treturn"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n    \tlang_dir  = os.path.dirname(lang)\n    \tfiles = [file for file in os.listdir(lang_dir) if file.endswith(\".pdf\")]\n    \tfor file in files:\n    \t\tif os.path.isfile(os.path.join(lang_dir, file)):\n    \t\t\tcontent = lang_template.find({\"_id\":file})\n    \t\telse:\n    \t\t\tcontent = lang_template.find({\"_id\":file + \".pdf\"})\n    \t\t\tdata = lang_template.find({\"language\" : lang})\n\n        data[\"original\"] ="}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t \n\tif lang in config.LANGUAGES_LIST('artigo02.'):\n\t\tfrom artigo import Arabic2Eu\n\n\n\t\tlang = Arabic2Eu()\n\t\tfile_path = os.path.join(config.BASEDIR, '../data/artigo02.'+lang+'.txt')\n\n\t\ttext = os.path.join(os.path.dirname(__file__), file_path)\n\t\t#print lang, text\n\n\t\ttry:\n\t\t\tf = codecs.open(text, 'r', 'utf-8')\n\n\t\texcept IOError:\t\n\t\t\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\tif lang:\n\t\t\tself.add_language(lang)\n\t\telse:\n\t\t\tself.add_template(\"artigo02.pdf\")\n\t\n\t\toutput_file_path = (file_path).replace(\".pdf\", \".txt\")\n\t\toutput_file_path = output_file_path.replace(\"\\\\\", \"/\")\n\t\tself.write(output_file_path, u'PDF')\n\t\tself.write(output_file_path, u'\u0395\u03c0\u03b9\u03c3\u03b9\u03bc\u03ad\u03c1\u03bf\u03c2 %s' % lang)\n\n\tdef add_language(self"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\tif file_path not in self.__files_to_be_added:\n\t\t\tself.__files_to_be_added.append(file_path)\n\n\t\tif lang not in self.__languages:\n\t\t\tself.__languages[lang] = FileSettings(\"en\").find(file_path)\n\t\tself.__language += 1\n\n\tdef dump(self):\n\t\t\"\"\"\n\t{\n\t\t\"languages\": [\"artigo02\", \"artigo03\"],\n\t\t\"language_file\": \"artigo-language.txt\",\n\t\t\"original\": \"artigo03"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n\n\t\tif lang.startswith(\"artigo\") or lang.startswith(\"artiga\") or lang.startswith(\"artigo08\") or lang.startswith(\"artiga7\") or lang.startswith(\"artiga8\") or lang.startswith(\"artiga_16\") or lang.startswith(\"artigo_8\"):\n        \t\tif self.check_path(file_path) is False :\n        \t\t\tname = lang[6:]\n        \t\telse:\n        \t\t\tname = self.check_path(file_path)\n\n         \t    # add a new.md file"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n        if os.path.exists(file_path):\n\t        print \"\\t\\tAdding rendition for\\textigo02\"\n\t    os.remove(file_path)\n\n        self.textigo02 = Linguistics(lang, self.template_path)\n\n    def get_language(self):\n        return self.lang\n\n\n    def get_layout_name(self):\n        return \"linguistics\"\n\n    def get_language_code(self):\n        return self.language\n\n\n    def get_layout_data(self):\n\n\tlanguage = self.get_language()\n\tlayout"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\nclass TipFileUploader(Tutorial):\n\t\"Docstring for TipFileUploader\"\n\n\tdef __init__(self, extra, shared):\n\t\tTutorial.__init__(self, extra, shared)\n\t\tself.current_translator = None\n\t\tself.file_path = None\n\n\tdef handle_page(self):\n\t\t\n\t\t# self.file.add_page(\"\") \t\t\n\t\t# self.file.add_page(\"\") \t\t\n\t\tself.file.save(self.file_path)\n\nif __name__ == \"__main__\":\n\tT"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n        path = file_path.replace(lang, \"\")\n        path = path.replace(\".pdf\", \"\")\n        path = path.replace(\".docx\", \"\")\n        path = path.replace(\".doc\", \"\")\n        path = path.replace(\".docx\", \"\")\n        path = \"http://\" + path  \n        path = path.replace(\"/web/\", \"/web.txt\")\n        path = path.replace(\"/web/\", \"/web.zip\")\n        \n        if not path.startswith(\"/\"):\n            path = \"/\"+path\n            \n\n        # Check if the user is a valid user.\n        if not self.userExists(path):\n            return"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n        f = open(file_path, 'rb')\n        data= f.read()\n\tf.close()\n\n        text = data[:200] + u'\\n\\n' + data[200:]\n        self.response.out.write(markdown(text))\n\n    def add_word(self, lang, file_path):\n\t\"\"\"\n        {\n \t\t\"original\": \"Word01.pdf\",\n \t\t\"en\": \"Word01-en-fr.pdf\",\n\t}\n        \"\"\"\n\n        f = open(file_path, 'rb')\n        data= f.read()"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\treturn self.add(lang, file_path)\n\n\t\"\"\"\n\t[TODO]\n\t{\n\n\t    \"en\": \"artigo2017.pdf\"\n\t   ,\n}\n\n\n# Copyright 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tim = Image.open(basename)\n\t\tsize = im.size\n\t\theight = size[1]\n\t\twidth = size[0]\n\n\t\timage = im.convert('L')\n\t\tpix = image.load()\n\n\t\t# convert to JPG\n\t\timage.save('JPEG', optimize=True)\n\n\t\t# convert to PNG image\n\t\tpix = image.load()\n\n\t\tnew_width = width / float(im.size[0])\n\t\tnew_height = height / float(im.size[1])\n\n\t\tim = im.crop((0,0,"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\treturn self.add_asset_from_path(file_path)\n\n\tdef add_asset_from_path(file_path):\n\t\tasset_file = open(file_path, \"r\")\n\t\tasset_file_contents = asset_file.read()\n\t\tasset_file.close()\n\t\treturn asset_content(basename, file_path, asset_file_contents)\n\n\tdef has_asset(self, basename):\n\t\tfor root, subdirs, files in os.walk(self.path):\n\t\t\tfor f in files:\n\t\t\t\trpath = join"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n    \tfilename, e=self.info(\"getting file info\")\n\t\t\n\t\t(path, filename) = os.path.split(filename)\n\t\tbasename+=\"_\"+filename\n\t\te=self.info(\"getting file info\")\n\t\t    \n\t\tos.system(self.cmd+\" \"+str(filename)+\" \"+str(e))\n\t\te=self.info(\"getting file info\")\n\t\treturn (path, filename, e)\n\t\t\t  \n\t\n\tdef get_file_sizes(self):\n\t\tfile_paths=[]\n\t\ttry:        \n\t\t    file_lists=os.listdir"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n\t\t\n\t\treturn True\n\t\"\"\"\n\tif file_path[0] == '~':\n\t\tfullpath = os.path.join(file_path[1:], self.filename)\n\telse:\n\t\tfullpath = os.path.join(file_path, file_path)\n\t\t\n\tfullpath = fullpath+os.path.extsep+basename\n\t\t\n\treturn fullpath\n\t\ndef write_asset(self, new_asset, new_filename):\n\twith cd(self.path):\n\t\t#print 'write_asset() -->> ', new_filename\n\t\t\n\t\tos.makedirs(self."}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n\n\t# load image and convert it to a png:\n\traw_image = pyvips.Image.from_file(file_path)\n\timage = pyvips.Image.init_from_file(basename, True)\n\timage.pixeltype = self.pixeltype\n\timage.pixelsize = self.pixelsize\n\timage.type = 'png'\n\timage.write()\n\t\n\t# get the new image\n\ttry: \n\t\tnew_image = pyvips.Image.from_file(basename, True)\n\texcept: \n\t\tlog."}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t    \n\tself.assets[basename] = path.splitext(basename)[0]\n\n\tfor a_file_path in sorted(self.assets):\n\t\tif a_file_path.startswith(basename):\n\t\t\tfilename = a_file_path[len(basename) + 1 :].split('_')[-1]\n\t\t\tpath = self.assets[a_file_path].replace(self.assets[basename], path)\n\t\t\tself.assets_paths.append(path)\n\n\t\t\t#print('adding', path)\n\t\t\ttry:\n\t\t\t\tf = open(filename, 'r"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n        if os.path.exists(file_path):\n            self.image_paths.append(file_path)\n            self.assets.append(self.image_paths[-1])\n            self.files_names.append(\"{0}_artigo02_{1}\".format(basename,self.datetime))\n        else:\n           self.images.append(filename_format.format(basename))\n           self.files.append(file_path)\n\n        \n        self.write_assets.append(filename_format.format(basename))\n        \n        if len(self.images) > 1:\n            log(\"Found"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tself.current_project.filename = \"assets/{}\".format(slugify(basename))\n\t\tself.current_project.title = \"T\u00edtulo : {}\".format(basename)\n\t\tself.current_project.set_icon(\"img/artigo.png\")\n\t\tself.current_project.save()\n\t        self.current_task.filename = \"assets/{}\".format(slugify(basename))\n\t        self.current_task.title = \"T\u00edtulo : {}\".format(basename)\n\t        self.current_task.set_icon(\"img/ar"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tfile_path = os.path.abspath(os.path.join(self.asset_path, 'images', os.path.basename(file_path)))\n\t\tbasename = os.path.basename(basename)\n\t\tbase, ext = os.path.splitext(basename)\n\n\t\tname = file_path.replace(file_path, base)\n\n\t\tif 'artigo' not in self.__assets and os.path.exists(name):\n\t\t\tself.__assets['artigo'] = os.path.join(self.assets_path, name)\n\n\t\t\tasset"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n    \n\t    self._load_asset(basename, file_path)\n\n\tdef add_text(self, path):\n\t\t\"\"\"\n        path:   path of the texture file (relative to the build directory)\n\t\n        \"\"\"\n        path = os.path.normpath(path)\n\tdir, basename = os.path.split(path)\n\tpath = os.path.normpath(os.path.join(dir, basename))\n        print path\n        sys.exit()\n\n    def add_texture(self, texture_path):\n\t\"\"\"\n\tTexture image source. Use full path if source is empty"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t#TODO: check if the path is a file or directory\n\t\n\t#extract package groups\n\ttry:\n\t\t(root,ext)=os.path.splitext(zip_path)\n\texcept: \n\t\t\n\t\tprint \"Could not extract folder path, probably not a zip file, please check the zipfile path\"\n\t\n\t\n\t#get extension of zipfile\n\text=ext.lower()\n\tif ext not in [\".zip\",\"\"]:\n\t\tprint \"Could not extract file extension in zip file\"\n\t\n\t\n\tdata_dict={}\n\t#walk folder\n\tfor root, subFolder"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tzf = zipfile.ZipFile(zip_path)\n\treturn json.loads(zf.read('packages.json'))\n\n# encoding: utf-8\n# Copyright (c) 2001, Neville-Neil Consulting\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# Redistributions of source code must retain the above copyright notice,\n# this list of conditions and the following disclaimer.\n#\n# Redistributions in binary form must reproduce the"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\ttry:\n\t\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\texcept Exception, err:\n\t\tsys.stderr.write(err + \"\\n\")\n\t\tprint \"Unable to open the zipfile %s, Please verify it's in $PYTHONPATH!!\" % zip_path\n\t\tsys.exit(1)\n\t\t\n\troot = zip_file.namelist()[0]\n\tdata = {}\n\t# Find root folder\n\ttry:\n\t\tfor child in root.getChildNodes():\n\t\t\t#print child.Name\n\t\t\tif child.Name == 'data':"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zipf:\n\t\tfor xml_name in zipf.namelist():\n\t\t\n\t\t\txml_folder = os.path.join(xml_name[:-4], xml_name[:-4] + '_package')\n\t\t\tf = zipfile.ZipFile(xml_name, 'r')\n\t\t\ttree = f.read(xml_folder)\n\t\t\troot_elem = tree.decode('utf-8')\n\t\t\n\t\t\troot, _, files = next(os.walk(xml_folder))[0]\n\t\t\n\t\t\tdata = {}\n\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tzip = ZipFile(zip_path)\n\tzip.extractall()\n\n\tpackages = dict()\n\twith zipfile.ZipFile(zip_path, \"r\") as zip_info:\n\t\troot = zip.read(\"packages\")\n\t\tcurrent = root.format(zip_info.filename)\n\t\tpackages[current] = dict()\n\t\tfor root2 in current:\n\t\t\tpath = os.path.join(root, root2)\n\t\t\tpath = path[-6:]   # remove '.xml'\n\t\t\tpackages[current][path] = load_file(path)\n\n\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\tzip_file = zipfile.ZipFile('{}.{}'.format(zip_path, 'Packages.xml'))\n\tf = zipfile.ZipFile('{}.{}'.format(zip_path, 'Scripts.xml'))\n\txml_data = f.read('Packetizer-Version: 0x{:X}.xml'.format(zip_file.readline()))\n\n\tresult = {}\n\ttree = ET.parse(zip_path)\n\troot = tree.getroot()\n\tpackages = []\n\tfor child in root:\n\t\tif child.tag.endswith('Packages'):\n\t\t\tpackages."}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t return parse_xso.parse_archive(gzip.open(zip_path), 'PK')\n\n\ndef _generate_zipfile(zip_path):\n\t\"\"\"\n\tGenerate archive package\n\tParameters\n\t----------\n\tzip_path : pathlib.PurePath\n\t    zip package path\n\tReturns\n\t-------\n\tstr\n\t\"\"\"\t\n\treturn _explore_zipfile(zip_path)\n\n\ndef unpack_archive(content, destination_directory):\n\t\"\"\"\n\tUnpack archive as raw bytes.\n\t\n\tUnpack a raw bytes into a raw bytes\n\n\tParameters\n\t----------\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tpackage_names = _package_names(_PACKAGE_NAME_PREFIXES)\n\t\n\tpackages = dict()\n\tfor fname in package_names:\n\t\ttry:\n\t\t\tinfo = _zipfile.ZipFile(zip_path + fname)\n\n\t\t\tfor info_file in info.infolist():\n\t\t\t\tname, ext = os.path.splitext(urlparse(info_file.url).path)\n\t\t\t\tmodule_name = info_file.name\n\n\t\t\t\tif name:\n\t\t\t\t\tname = name[name.find(\".\")+1:]\n\t\t\t\telse:\n\t\t\t\t\tname = \"%s%s."}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\n\tzip_fp = zipfile.ZipFile(zip_path, 'r')\n\tdata = zip_fp.read(ZIP_VERSION_PY)\n\n\tdef _explore_zipfile_recursively(target, skip_dirs):\n\t\t\n\t\tif skip_dirs == True:\n\t\t\tfound = False\n\t\t\tfor root, dirs, files in os.walk(target):\n\t\t\t\tfor file in files:\n\t\t\t\t\t\tif file!= 'README':\n\t\t\t\t\t\t\tprint(\"{}: {}\".format(root, file))\n\t\t\t\t\t\t\tfound = True\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\n\t\t\t\t\n\t\t\treturn found\n\t\t\t\n\t\telse:\n\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# read from zipfile\n\tzf = zipfile.ZipFile(zip_path,\"r\")\n\t\n\t# read group 'package'\n\tname = None\n\t\n\t# loop through files and build dictionary\n\tfor file in zf.namelist():\n\t\t# if file name is None, that means package was not parsed\n\t\t# for each time\n\t\tname = file\n\t\t# loop through the child files (for example'my_package' as a root directory)\n\t\tfor child in file.children:\n\t\t\t# remove leading '/' from path\n\t\t\tname = os.path.join(child.get"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\nfilename = zip_path.split('/')[-1]\nfiles = []\nfor file_name in os.listdir(basedir):\n    if file_name.endswith('.zip'):\n        zip_info = zipfile.ZipFile(os.path.join(basedir, file_name)) # TODO: fix this\n    elif file_name.endswith('.zip.zip'):\n        zip_info = zip.ZipFile(os.path.join(basedir, file_name))\n\tbreak\n    else:\n        raise Exception(\"File name not in zip!! \")\n\tfiles.append(file_name) # TODO: fix this"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t\n\tzip_zip = zipfile.ZipFile(zip_path, 'r')\n\tnames = zip_zip.namelist()\n\tzip_names = {}\n\tfor name in names:\n\t\tzip_names[name] = zip_zip.read(name)\n\t\n\treturn zip_zip, zip_names\n\ndef load_feature(file_path, filename):\n\t\"\"\"\n\tLoad Feature in `file_path`\n\t\n\tArgs:\n\t\tfile_path: file_path\n\t\n\tReturns:\n\t\tfeature: \tthe feature instance\n\t\"\"\"\n\treturn Feature("}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\n\t# Extract all tags with regex\n\tallRegexTokens = re.findall(TAG_REGEX_STRING, zip_path)\n\t\n\t# Convert all regex tokens\n\tallRegexTokenRegex = (lambda token: \"\".join(map(lambda t: t[1], allRegexTokens)))\n\t\n\t# Iterate through all tags and count occurrences of matched tags\n\tfor tag, regex in allRegexTokenRegex:\n\t\tmatchCount += 1\n\t\tif re.search(regex, tag):\n\t\t\t# Find occurence of any regex\n\t\t\tmatchCount += 1\"\"\"\nmbed SDK\nCopyright ("}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\n\tzip_path += \".zip\"\t\t\t\t\t\t\t\t# change this to use.zip extension\n\tp = Path(zip_path)\t\t\t\t\t\t\t\t\t\t\t\t# get the path of this function\n\tfiles_list = p.files_list\t\t\t\t\t\t\t\t\t\t\t# get the list of the file that we want to search for\n\treturn files_list\n\n\ndef files_list_info(path):\n\t\"\"\"\n\tReturn the file path of a file\n\tIt is a wrapper around os.listdir.\n\t\"\"\"\n\tfiles_list = []\t\t\t\n\tdirfiles = os.listdir(path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\tzip_name_list = []\n\t\n\n   \t# read the archive and extract name list\n\tarchive = zip_file.read(namelist_end,namelist_start)\n\tnamelist_end = archive.find('2318')\n\n\t# construct the appropriate list for writing the file\n\tname_file = archive[namelist_end + 4:]\n\tlist_name_list = open(name_file,\"w+\")\n\t\n\t# loop through the file and write the rest of the name file\n\tfor line"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t\n\tzip_contents = zipfile.ZipFile(zip_path, 'r')\n\tzip_info = zip_contents.infolist()\n\tzip_info.sort()\n\t\n\tif not zip_info:\n\t\tprint(\"No files in zipfile.\")\n\t\treturn\n\t\n\treturn map(lambda x: os.path.basename(os.path.dirname(x)), zip_info)\n\ndef load_files_into_zip():\n\t\"\"\"\n\tLoad input files into zipped location\n\t\n\tExample:\n\t\n\t```\n\t>>> import glob\n\t>>> files_list = ['/usr"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\t# Read the contents of the zip file into a list\n\treturn [file_content for file_content in zip_file.namelist() if file_content.endswith('.tif')]\n\nif __name__ == '__main__':\n\tbase_path = '.'\n\tfiles_list = files_list_from_zipfile()\n\t# Run the functions\n\tmain(files_list)# Copyright 2010 Mark Diekhans\n#\n# This file is part of tagfs-e.\n#\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t\n\tzip_dir = os.path.dirname(zip_path)\n\tfiles = os.listdir(zip_dir)\n\tf_list = []\n\tfor f in files:\n\t\t# If the path isn't an absolute file, we just ignore it.\n\t\tif not f.startswith(os.path.abspath(zip_dir)):\n\t\t\tcontinue\n\t\tf_list.append(f)\n\treturn f_list\n\ndef find_extrema(basepath, pathlist=[]):\n\t\"\"\"Return the directory relative to `basepath`\n\t\n\tThis method returns"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\n\n\tfiles = []\n\tfor fname in tar.getnames():\n\t\tfile, ext = os.path.splitext(fname)\n\t\tif ext in (\".tgz\", \".tar.gz\", \".tar.bz2\"):\n\t\t\tcontinue\n\n\t\tif fname.lower().endswith(\".tar\") or fname.lower().endswith(\".tgz\"):\n\t\t\tfiles.append( (fname,file) )\n\treturn files\n\n\n\n\n# from IPython import embed; embed();\nfrom IPython.display import Image, display\n\n##from IPython.utils.text import dedent\n##@interact\n##def show():\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\ndef get_file_list_from_zipfile(zip_path):\n\treturn_list = []\n\n\tf = zipfile.ZipFile(zip_path, 'r')\n\n\tif f.testzip() == 0:\n\t\treturn return_list\n\t\t\n\t# open zipfile and get file list\n\tfor f1 in f.namelist():\n\t\ttmp = f1.split('.')\n\t\tif len(tmp) == 2 and tmp[1] == '.zip':\n\t\t\treturn_list.append(tmp[0])\n\n\treturn return_list\n\ndef compare_time(t1"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\treturn w_st.sub(u\"w:st=\\\"\", content)\n\ndef get_line_numbers(line):\n\t\"\"\"\n\tReturn the line number for a given line\n\t\"\"\"\n\tline_number = 0\n\tfor word in LINUX_QUOTES.split(line): #split long word on a space\n\t\tline_number = line_number + 1\n\treturn line_number\n\nclass GrepFile:\n\t\"\"\"\n\tClass for creating a gcode text that is part of a text (i.e., code\n\tfiles), a file"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ndef fix_namespace_prefix_s(content):\n\t\"\"\"\n\t\n\tFormat a namespace prefix definition string such as 'g:local \"'. Warns:\n\t\t'type=\"foo-namespace\".' = \"foo namespace\"\n\n\t\tNote: It must start and end with a leading dot\n\t\n\t\"\"\"\n\tif isinstance(content, str):\n\t\treturn content\n\n\tif isinstance(content, int):\n\t\tif content in [\"'\", '\"']:\n\t\t\treturn None\n\t\treturn content + '\"'\n\treturn content\n\ndef fix_namespace_prefix(text):\n\t\"\"\"\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ndef fix_namespaces(content, tag, ns_map, namespaces=None, prefix=None):\n\t\"\"\"\n\tTranslate XML to 'w:v=\"' and insert a namespace prefix if missing\n\t'ns_' to the 'w:t=\"' prefix if not 'ns_'  is present in\n\t\" content is a dictionary containing the mappings between the namespace \n\tof content and its namespaces.\n\tUse prefix='w:v' and namespaces='w', respectively, when the content is \n\t\t\"text/xml; charset=us-ascii\", it replaces the prefix of text/xml, \n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\trep = re.compile('w:st=\"[^\"]*?\"')\n\t\tfor key in re.findall(rep, content):\n\t\t\tif key in ['xmlns:x','xmlns:xs','xmlns:xsi','http://' ]:\n\t\t\t\tpass\n\t\t\telif key in ['xmlns:w','http://' ]:\n\t\t\t\tpass\n\t\t\telif key == 'xmlns:atom':'http://www.w3.org/2005/Atom' or key == 'xmlns:atomsi':\n\t\t\t\tpass\n\t\t\telif key == 'xmlns:cl' and key == 'xmlns:dsc':\n\t\t\t\tpass\n\t\t\telif"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ndef fix_namespace_prefix_v(content):\n\t\"\"\"\n\tConvert text that defaults to 'v=w/' to 'v=w'\n\t\"\"\"\n\t\ndef _get_content(html):\n\t\"\"\"Extract comments and remaining info in <html> tags\n\t\"\"\"\n\t\ndef _check_for_duplicate_line(line, m=None):\n\t'''\n\tReturn [True|False] if the line is in a duplicate block starting with ``'''\n\tm1 = re.search('  *[^{]*?}', line)\n\tif m1 and m"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\n\n\tdef fixup_word(matchobj):\n\t\tword = matchobj.group(1)\n\t\t\n\t\tif word[:2] == '__':\n\t\t\t# this matches common patterns\n\t\t\tword = word[1:]\n\t\tif matchobj.group(2) is not 'w:':\n\t\t\t## also matches a word boundary\n\t\t\tword = word[2:]\n\t\t\t\n\t\tif word.startswith(\"'\"):\n\t\t\tw_prefix, reloc_flag = w_ns.match(word)\n\t\t\tif not reloc_flag:\n\t\t\t\tword = w_prefix + word\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ndef fix_xml_namespaces(xml_string):\n\t\"\"\"\n\tConvert string with :namespace for xml\n\t\"\"\"\n\tif xml_string.startswith('http://stun.tudelft.nl/core/'):\n\t\txml_string = xml_string.replace('http://stun.tudelft.nl/core/', 'http://stun.tudelft.nl/core/')\n\n\tif xml_string.startswith('http://www.w3.org/XML/1998/namespace'):\n\t\txml_string = xml_string.replace('http://www."}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\n\n\t# Replace whitespaces with spaces and spaces\n\tcontent = content.replace(' ', '_')\n\tcontent = content.replace('&', '-')\n\tcontent = content.replace('-l ', '')\n\n\t# replace all spaces with one of the specified length\n\tfor c in content:\n\t\tif c.isspace(): continue\n\n\t\tcontent = content.replace(c,'')\n\n\treturn content\n\n\n#------------------------------------------------------------------------------\n\nclass LSConf(object):\n\t\"\"\"Class for controlling the (list of) LSN values\"\"\"\n\tdef __init__(self, filename=None, mode=0644,"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tr = re.compile('<w:st=\"([^\"]*)\">([^<]*)</w:st>').split(content)\n\t\tif r and len(r) == 2:\n\t\t\tw = r[0]\n\t\t\tif w == '<w:st=\"\\': return\n\t\t\treturn w\n\t\treturn content\n\n\ndef fix_doctype_children_w(doctype, name):\n\t\"\"\"\n\tSome parts\n\t:param doctype: DocType of the document\n\t:param name: Name of the doctype\n\t:type doctype: string\n\t\"\"\"\n\t# get all children\n\tdoctype ="}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ttry:\n\t\tif not len(content):\n\t\t\tif re.search(\"'('|[^']*)'\", content):\n\t\t\t\tcontent = content[1:-1]\n\t\treturn content\n\texcept:\n\t\tpass\n\n\ndef add_anchor_type(elem, content):\n\t\"\"\"\n\tAdd anchor type to element object.\n\t\"\"\"\n\tif (elem.get('kind') == 'anchor' and content not in ALLOWED_ANCHOR):\n\t\telem.text = content\n\n\telse:\n\t\telem.set('type', 'anchor')\n\t\n\t# check if anchor already in"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t return sorted(map(lambda x: x.get(\"pubdate\"), pubdate_xpaths))\n\ndef match_pubdate_or_year(node, pubdate_xpaths):\n\t\"\"\"\n\tReturns the first match in the pubdate format or year format\n\t\"\"\"\n\t return sorted(map(lambda x: x.get(\"pubdate\") or x.tag, pubdate_xpaths))\n\ndef isodate(node):\n\t\"\"\"\n\tTests if the node is part of an xsl:Abstract\n\t\treturns a (namespace, tag) tuple\n\t\tReturns (None, namespace"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tdates=list(node)\n\t\ttry:\n\t\t\ty=dates[-1]\n\t\texcept:\n\t\t\treturn None\n\t\tif y<pubdate_ymin:\n\t\t\treturn dates[-1]\n\t\telse:\n\t\t\treturn dates[-1]\n\treturn None\n\ndef match_datetype(node, datetype_xpaths):\n\t\"\"\"\n\treturns the first match in the datetype_xpaths list that is either a string of XML node strings or a list of datetypes \n\t\"\"\"\n\t\ndef match_datex(node, datex_xpaths):\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tfirst_pubdate_id = node.pubDate(pubdate_xpaths[0])\n\t\treturn first_pubdate_id\ndef match_content_type(node, content_type_xpaths):\n\t\"\"\"\n\tReturns matched content type of node\n\t\"\"\"\n\t\tmatch = content_type_xpaths.match(node.contentType)\n\t\tif not match:\n\t\t\treturn False\n\t\tfirst_type = match.group('content-type')\n\t\treturn first_type\ndef match_type(node, content_type_xpaths):\n\t\"\"\"\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tresult = []\n\tfor node in node.childNodes:\n\t\tif node.nodeType == Node.ELEMENT_NODE and node.tagName == 'pubdate':\n\t\t\tpubdate_xpaths.append(node.getAttribute('pubDate'))\n\n\t\tresult.append(node)\n\treturn result\n\n\ndef _check_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tCheck that the pubdate and node's publication date are the same as the first\n\tfor any of the given node's xpaths.\n\t\"\"\"\n\tfor node in node.childNodes:\n\t\tdate_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t return any(re.match(p, pubdate_xpaths) for p in node.getchildren())\n\n\ndef match_author_xpaths(node, author_xpath, author_matches_xpath):\n\t\"\"\"\n\tReturns the first match in the author_xpath list\n\t\"\"\"\n\n\t\n\n\tauthor_xpath.append(node)\n\n\tauthors = node.getchildren()\n\n\tif len(authors) > 1 and len(author_matches_xpath) - len(author_matches_xpath)!= 0:\n\n\n\t\tauthor_xpath.remove(node)\n\n\t\tauthors."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tfor node in node:\n\t\t\tif node.tag == 'pubDate':\n\t\t\t\treturn node.text\n\n\t@staticmethod\n\tdef match_isbn(node, isbn_xpaths):\n\t\t\"\"\"\n\t\tReturns True if the node is a isbn, False otherwise\n\t\t\"\"\"\n\t\tfor node in node:\n\t\t\tif isbn_xpaths.findall(node.tag):\n\t\t\t\treturn True\n\n\t@staticmethod\n\tdef match_abstract(node, abstract_xpaths):\n\t\t\"\"\"\n\t\tReturns the first match in the abstract_xpaths list \n\t\t\"\"\"\n\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tpubdate = node.attr('pubdate')\n\tif not pubdate: \n\t\treturn \"\"\n\n\treturn pubdate.getText().lower()\n\n\nclass EntryList:\n\tdef append(self, node):\n\t\t# Append the nodes from the node to the node's subtree\n\t\tparent = node.getparent()\n\n\t\t# For each sub-node, make a list of entry nodes\n\t\tnode_list = [ entry for entry in parent.children\n\t\t            if self.match_pubdate(entry, node_list) ]\n\n\t\t# Add the entry nodes\n\t\tfor node in node_list"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tfor xpaths in pubdate_xpaths:\n\t\t\tfor match in xpaths:\n\t\t\t\tif match.tag == \"pubDate\":\n\t\t\t\t\treturn \"true\"\n\t\treturn None\n\ndef match_pubdate_ns(node, pubdate_xpaths):\n\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\tand at most once (and if none of them match)\n\t\"\"\"\n        for xpaths in pubdate_xpaths:\n                for match in xpaths:\n                        if match.tag == \"pubDate\":\n                                return match.find_next_si"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tif(len(pubdate_xpaths) > 0):\n\t\t\tif(len(pubdate_xpaths) > 1):\n\t\t\t\treturn node\n\treturn \"\"\n\n\ndef get_filename(file_path):\n\t\"\"\"\n\tReturns the filename for the file specified\n\t\"\"\"\n\t\treturn file_path.split('\\\\')[-1]\n\nif __name__ == '__main__':\n\ttry:\n\t\toptions = parser.parse_args()\n\t\tfile_name = get_filename(options.file)\n\texcept AssertionError:\n\t\texit()\n\n\tif options."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t return node.get('pubdate', \"\").strip(\"$#\").encode(\"utf-8\")\n\n# vim: set ts=4 sw=4 ai et\n# -*- encoding: utf-8 -*-\n##############################################################################\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as published\n#    by the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n\n\t# Extract the article or abstract\n\tissue_elements = tree.xpath(\"article|abstract|author\")\\\n\tissue_title_elements = issue_elements[1:]\n\n\t# Extract the date of the issue\n\tissue_date_element = issue_elements[11]\n\n\tissue_publication_element = issue_elements[13]\n\tissue_number = issue_elements[15]\n\n\t# Extract the booking year\n\tissue_booking_element = issue_elements[16]\n\n\t# Extract the number of the paper\n\tissue_number_element = issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\nclass ErrorWithBadXML(Exception): pass\n\n\ndef _generate_error_with_bad_xml_string(element_name, element_value):\n    \"\"\"\n    Creates a problem with an XML string containing the failure.\n\n    :raises: ErrorWithBadXML\n    :return: A solution with a failure value.\n    \"\"\"\n    try:\n        xml_parser = ET.XMLParser(remove_blank_text=True) \n        xml_parser.feed(element_value)\n    except ET.ParseError, e:\n        raise ErrorWithBadXML, \"Problem in '{0}'\".format(element_name)"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n\n\tissue_element=issue.find('%s/issue-body/issue-elements' % _issue_url).text\n\n\tpattern = issue_element.split()\n\n\tnumber = re.search(\"<i>-\", pattern[0])\n\tsuppl = re.search(\"<b>-\", pattern[1])\n\n\treturn (number,suppl)\n\n\ndef _extract_issue_element_from_title(title):\n\tpattern = title\n\tpattern=pattern.replace(\"<a\", \"Affichage\");\n\tpattern=pattern.replace(\"<b\", \"Affichage and"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n\n\tissue_number = None\n\tsuppl_number = None\n\n\tnumber_text_list = issue_text_list(issue)\n\tsuppl_text_list = issue_suppl_text_list(issue)\n\n\tif number_text_list is None or supppl_text_list is None:\n\t\treturn None, None\n\n\tissue_number = number_text_list[0][\"issue\"][\"number\"]\n\tsuppl_number = supppl_text_list[0][\"suppl_number\"]\n\n\treturn issue_number, suppl_number\n\n\ndef _extract_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tsubdocument_root = lxml.html.fromstring(issue)\n\t\tissue_number_root = subdocument_root.xpath('//article/div[1]/h1/text()')\n\t\tissue_number = issue_number_root[0].strip()\n\t\tissue_suppl = subdocument_root.xpath('//article/div[2]/h1/text()')\n\t\tissue_suppl_sugar = issue_suppl[0].strip()\n\t\tissue_number_sugar = issue_number_sugar[0].strip()\n\t\tdoc_number_root = issue_number"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n#\tExtracting numbers and superfluents (office, industries etc)\n#\tIssue number - Numbers (number, number+suppl)- office, industries (if applicable)\n\n\tissue_element = issue_from_contents(issue)\n\n#\tExtracting volume or volume numbers (for example, 'S')\n\n\tvolume_element = volume_from_contents(issue)\n\tvolume_number_element = volume_element.number\n\tvolume_suppl_element = volume_number_element - volume_number_element\n\t#extracting"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tissue_number = issue.find(\"span\", {\"itemprop\": \"issue_number\"})\n\t\tnumber = issue_number.find_all(text=re.compile(r'number:'))\n\t\t\n\t\tsuppl = str(number[0].get_text())\n\t\treturn number[1].text, suppl\n\t\t\t\n\treturn None, None\n\n# Create a dictionary from the document and the issue\n# - The issue element containing the data\n# - The number of the issue element\ndef create_issue_dictionary():\n\tissue_dictionary = {}\n\t\n\t# Extract the fields associated"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tnumber = re.search(\"([0-9]+)\", issue.text).group(1)\n\t\tsuppl = re.search(\"([0-9]{5})\", issue.text)\n\t\tif number and number.isdigit():\n\t\t\tnumber = int(number)\n\t\telif number and number.title()[:-1] == \"suppl\":\n\t\t\tnumber = int(number.title().replace(\"suppl\", \"\"))\n\t\telif number and number.title()!= \"suppl\":\n\t\t\tnumber = re.sub(\"\\(.*\\)\", \"\", number)\n\t\telif number and number.title()!= \"Sup"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n          issue_number     = issue.childNodes[1].childNodes[0].nodeValue\n          suppl_number      = issue.childNodes[1].childNodes[2].childNodes[0].nodeValue\n          return issue_number, suppl_number\n\n\nclass XmlTextExtractor(TextExtractor):\n    \"\"\"\n    Class for extracting text from each DOM node\n    \"\"\"\n    # Constructor.\n    #\n    def __init__(self, inputDir, outputDir, doxygenFileName, xsltFileName, verbose=False):\n        \"\"\"\n        Constructor of the class\n        @param inputDir: The directory where the xml files are located.\n        @"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\telement = issue.find(\"element\")\n\telements = [(element.text.strip(), element.text.replace(\"&amp;\", \"&\"))) for element in element.getchildren()]\n\n\tnumber = int(elements[-1][0])\n\tsuppl = [\"C\", \"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n\treturn number, suppl\n\n\ndef _extract_number_and_subtypes_from_issue_element(issue):\n\t\"\"\"\n\tExtract the possible values of number and subtypes from the contents of issue.\n\t\"\"\"\n\t\t"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\n\n#def make_str(x, indent=0):\n#\tif x is not None:\n#\t\tif isinstance(x, type):\n#\t\t\treturn '\"%s\"' % x.replace('\"', '\\\\\"')\n#\t\treturn str(x)\n#\t\treturn \"%s\" % x\n#\n#\telse:\n#\t\trepr_str = self.repr_val(indent)\n#\t\treturn repr_str\n\n\"\"\"\n\nPython Interchangeable Virtual Instrument Library\n\nCopyright (c) 2013-2017 Alex Forencich\n\nPermission"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n#\t\t# This function was based off of a Douglas Adams\n#\ndef prettyEsc(self):\n\t\"\"\"\n\tReturn an easy formatted string representing a Douglas Adams.\n\t\"\"\"\n\n\t# If the file name ends with one '/' it will be displayed after the name\n\n\t#\tCheck for the delimiter\n\tname=os.path.basename(self.id)\n\n\n\t# if self.id[-1]!= os.path.basename(self.id+os.path.sep):\n\t# \tname=os.path."}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n#\tPrint the list of all items in the LispArray object.\n#\tExample:\n#\tl = [ [ 0, 0], [ 1, 2], [ 3, 4], [ 5] ]\n#\n#\tpprint(l, indent=4)\n\n\t#\n\t#\tPrint an integer.\n#\n\tdef toInt(self, num, max=None, min=None):\n\t\t\"\"\"\n\t\tConverts an integer to a character string.\n\t\tnum can be positive or negative. Also works on positive numbers, \n\t\tand non-integers:\n\t\t\t-"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t        \n\t#if debug:\n\t#\treturn pprint(self.__str__())\n\t#else:\n\t#\treturn pprint(self.__dict__)from __future__ import absolute_import\n\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.utils.decorators import method_decorator\n\nfrom sentry.conf import settings\nfrom sentry.stacktraces import StacktraceProcessingError\nfrom sentry.plugins import plugins\nfrom sentry.utils.sdk import configure_scope\n\n\n@method_decorator(plugins.is_webhook_public)\ndef webhook(callback_name,"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\t\n\tdef __eq__(self, other):\n\t\t\n\t\t# compare type\n\t\tif not isinstance(other, self.__class__):\n\t\t\treturn False\n\t\t\t\n\t\t# compare \n\t\tif debug:\n\t\t\tprint \"\\t\" + other\n\t\t\n\t\t# compare \n\t\treturn self.get_pretty_repr(indent=indent+2) == other.get_pretty_repr(indent=indent+2)\n\t\t\n# end class Serializable\n\n# end class Serializable\n\n\n\n\nimport numpy as np\n\nfrom mlens.util.functions import as_floatX\nfrom..core.geometry import Geometry\nfrom"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\t\n\tdef prettyPrint(self, indent=0, debug=False):\n\t\n\tdef __getitem__(self, item):\n\t\tvalue = self.getValue(item)\n\t\t\n\t\tif self.Type == list or self.Type == dict:\n\t\t\treturn \"[ \" + str(value) + \" ]\"\n\t\t\n\t\tif not len(value):\n\t\t\treturn ''\n\t\tif isinstance(value, dict):\n\t\t\treturn \"[ \" + str(value) + \" ]\"\n\t\tif self.Type == bool or self.Type == int:\n\t\t\treturn str(value)\n\t\tif self.Type"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tif not (isinstance(self.value, Node)):\n\t\t\traise TypeError(\"Cannot prettyify %s which is not a Node or string\" % self.__class__.__name__)\n\t\treturn PrettyPrinting(indent, self.value)\n\n\tdef is_valid(self):\n\t\treturn Node.is_valid(self)\n\n#######################################################################\n#\n#  UnaryOp utilities\n#\n#######################################################################\n\ndef _safe_pprint(v,**kwargs):\n\tdef _pprint(s,indent,left=4,right=4,*args,**kwargs):\n\t\tif left<=0"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t \n\tlines = []\n\tfor (lineno, line) in enumerate(self._code):\n\t\tif debug:\n\t\t    line = line.encode('utf-8')\n\t    if lineno == self.__last_lineno-1:\n\t\traise Exception('Line %d doesn\\'t match with any code.'%(lineno))\n\t    lines.append(line)\n\t    line = line.rstrip()\n\t    if debug:\n\t\t    assert isinstance(line,str)\n\n\tfor line in lines:\n\t    if line is None or line is '':\n\t\tcontinue\n\n\tif debug:\n\t    print \"### %s\" %(line)\n\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\ndef indentToString(indent_block, indent):\n\tout = indent_block[indent:] + '\\n'\n\tout += indentString1(indent, indent)\n\tfor line in out.split('\\n'):\n\t\tout = indentString1(indent +'   ', line)\n\treturn out\n\ndef wrapOutput(output, start_of_line = 0, indent = 0):\n\tif start_of_line == -1:\n\t\tstart_of_line = len(output) - 1\n\toutput = str(output)\n\tlines = output.split"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t # TODO: Rethink these are the slowest, but the better ones.\n\t # The method is too much quick, especially on single line headings\n\t # or just a few empty lines. It works.\n\n        if not self.header: self.header = \"\"\n\n        result = []\n        if self.options:\n            result.append( \"Options:\\n\" )\n            for option in sorted( self.options ):\n                result.append( \"   %s: %s\\n             %s\" % (indent, str(option), pretty_repr(self.header, indent + 2, verbose = debug) )"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\targs = tuple(arg.as_ordered_terms()) # make sure we start as a non-identity expression\n\t\tif len(args) == 1: args = args[0] # should be the eliminated element of args\n\t\t\n\tnew_args = []\n\tfor arg in args:\n\t\tnew_arg = arg._eval_absorb(self)\n\t\tif new_arg is NotImplemented: return new_args\t# XXX: use.args instead of.args as input argument...\n\t\tnew_args.append(new_arg)\n\t\t\n\treturn sum(x*y for"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n\t  n = len(args)\n\n\tif n == 1:\n\t    return [args[0]]\n \n\tif len(args) == 2:\n\t    return self._make_table([(args[i], args[i+1]) \n\t\t\t    for i in range(n*2, n*2+1)])\n\treturn self._make_table([(args[i], \n\t\t\t\t\t[self._make_table([(args[j], args[j+1]) for j in range(i)]\n\t\t\t\t\t   for i in range(n)]))\n\t\t\n\ndef factor(self, t):\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n    # Use the same name as absorb and clean-up\n    absorb.__name__ = 'absorb([%s])' % repr(args)\n    return ArithRef(args)\n    \ndef __rand__(self, x_width, y_width):\n    \"\"\"Construct `Expr`s for the random variables\"\"\"\n    return Expr(self, x_width, y_width)\n\ndef _eval_int(expr):\n    \"\"\"Evaluate numerical expression `expr` for type int.\"\"\"\n    iexpr = expr.find_instantiation()\n    if iexpr == None:\n       "}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\ndef reciprocal(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions and returns a new list of expression\n\treciprocals.\n\t\n\t.. seealso::\n\t\n\t    :func:`sympy.functions.elementary.exponential`\n\t       \n\t       \n\t       \n\t        >>> from sympy.functions.elementary.exponential import _pair_regular as \\\n\t        >>> _pair_regular([2, 2], [0, 0])\n\t        [2, 0, 1]\n\t       \n\t        >>> _pair_regular([2, 2], [1, 2, 1])\n\t       "}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n\tassert len(args) >= 4\n\tfor i in args[1:]:\n\t    if i.is_Number:\n\t\ti = Integer(i)\n \tnewargs = [i]\n \tfor i in args[2:]:\n\t    if i.is_Number:\n\t\ti = Integer(i)\n \tfor j in newargs:\n\t\tnewargs.append(j)\n\t\tfor i in args[1:]:\n\t\t    if i.is_Number:\n\t\t\ti = Integer(i)\n\t\tnewargs.append(i)\n \texpr = Mul(*newargs)"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n\tif len(args) < 2:\n#\t\tprint \"Exactly two expressions expected\"\n\t    \n\t    raise ValueError(\"%s is not a valid expression of type %s\" % \\\n\t\t\t\t\t\t                (str(args[0]), type(args[0])))\n\tif type(args[0])!= type(\"\"):\n\t    raise ValueError(\"%s expected, got %s\" % (str(args[0]), type(args[0])))\n\tif type(args[1])!= type(\"\"):\n\t    raise ValueError(\"%s expected, got %s\" % (str(args[1]), type"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t \n    def conj(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return the conjugate of the\n\texpression resulting from the concatenation of all the previous\n\texpressions.\n\t\"\"\"\n\t\n\tif len(args) == 0:\n\t    raise Exception('Empty argument list for absorb.')\n\t\n\targ0 = args[0]\n\targ1 = args[1:] \n\n\t# First, check that argument lists have the same number of arguments\n\tif len(arg0)!= len(arg1):\n\t    raise Exception('Argument lists do not have the same number of"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n\treturn [ [ a | -b for a in args] for b in args ]\n\n    def absdiff(self, args):\n        \"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expressions\n\tapplying absolute differences.\n\t\n\tAbsolute differences::\n\t\n\t    A - B = A\n\t\n\t    A | -B = A\n\t\n\t\"\"\"    \n\ttry:\n\t    args = iter(args)\n\t    n = len(args)\n\t    for i in xrange(n):\n\t\tif n % 2 == 0 and (args[i].is_Mul or args[i]."}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n        result = []\n\tfor expr in args:\n\t    result.extend(list(expr.args))\n\t    result.append(NegativeAbsorption(expr))\n\treturn result\n\nclass IntegerMatrix(matrix):\n\n    def __new__(cls, *elements):\n        return super(IntegerMatrix, cls).__new__(cls, elements)\n\n    def __add__(self, other):\n        if other == 0:\n            return IntegerMatrix()\n        new = IntegerMatrix([0]*self._size)\n        new.elements = [self[i] + other[i] for i in range(0, len(self"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\t    result = []\n\tself.stack = [(self.A & ~self.B & ~self.C) | self.B for i in range(0, len(args))]\n\tresult.append(self.A & self.B)\n\tresult.extend(args)\n\treturn \"\".join(result)\n\n\"\"\"\n\n(C - C) & (A - A) >> A and ~(A & C), A & (~A & ~C), A & ((B & C) ^ C) >> ((B & A) ^ C)\n\"\"\"\n\n@n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t     \n\t\n\t\tdef register_listeners(func, data, channel=None, skip_if_exists=False):\n\t\t\tchannel = hook.bot.get_channel(channel or None) or channel\n\t\t\tif not hook.ignore_listeners and channel:\n\t\t\t\tif not channel.id in hook.listeners:\n\t\t\t\t\thook.listeners.append(channel.id)\n\t\t\thook.ignore_listeners = False\n\t\t\tfor hook_list in self.listeners.values():\n\t\t\t\tif hook_list[1] == hook:\n\t\t\t\t\tfunc = func.handler(hook_list[0])\n\t\t\t\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n#\n#\n# Registry-base interface functions:\n#\n\n\nclass HookRegistry(BaseHookRegistry):\n\t\"\"\"\n\tRegistry-constructor which defines a hook function for loading and\n\tunloading registered listeners.\n\t\n\tThis registers three methods:\n\t    _add_listener(str:_str_, str:_str_)\n\t\"\"\"\n\t#\n\t#   Base class hooks\n\t#\n\tdef _add_listener(self,_str_,str_):\n\t\tstr=\"listeners \" \\\n\t\t\t+str+ \"(\" \\\n\t\t\t+self._base_class._type_string_to"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\t    def wrapper(func):\n\t\tself.on_hooks.setdefault(hook.key, []).append(func)\n\t    \n\t    return wrapper\n\t\n\t## Extend the default behaviour of the class.\n\tdef apply(self, cls):\n\t\treturn cls(func=self.on)\n\t\n\t## Extend the default behaviour of the class.\n\tdef on(self, cls):\n\t\tcls.on_hooks = dict()\n\t\tself.on_hooks.update(dict(cls.on_hooks))\n\t\treturn cls\n\t\n\tdef add(self, cls):\n\t\treturn"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\tdef _wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tDecorator method to add new handler to the registry.\n\t\t:param \\*hook: :class:`Hook` to add handler to. \n\t\t\n\t\t.. note::\n\t\t\n        \tArgs are the same as the original handler. \n\t\t:param args: :class:`list` of positional arguments for the handler.\n\t\t:param kwargs: :class:`dict` of keyword arguments for the handler.\n\t\t\n\t\t\n\t\t.. seealso:: http://pylonsproject.org/projects/pylons/en/latest-releases/pyramid"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\t    \n\t    \n\t    \n\t    \n\t    \n\t\n\t\n\tdef on_pre_enter(self, **kwargs):\n\t\t\"\"\"\n\t\tCalled before the plugin's enter handler is fired.\n\t\t\n\t\tThis function is invoked prior to the core startup thread for a new \n\t\tfrontend plugin.\n\t\t\"\"\"\n\t    \n\t    \n\t    \n\t    \n\t    \n\t    \n\tdef on_post_enter(self, **kwargs):\n\t\t\"\"\"\n\t\tCalled after the plugin's enter handler has loaded for data. \n\t\tThis function is invoked after the core startup thread for a new \n\t\tfrontend plugin.\n\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\t    \n\tdef _add_hook(cls):\n\t\tcls.hooks.append(hook)\n\t    \n\treturn _add_hook\n\ndef hook(spec, *args, **kwargs):\n\t\"\"\"\n\tDecorator function to register a hook type.\n\t\n\tArgs:\n\t    spec (str): Hook type, eg \"myapp.model.mymodule.MyModule\"\n\t    *args (str): Hook arguments.\n\t    kwargs (dict): Keyword arguments for the hook.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook type.\n\t\n\t\"\"\"\n\t    \n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\tdef decorator(cls):\n\t\n\t\tif cls.hasHook:\n\t\t\t\n\t\t\tfor k,v in cls.__dict__.iteritems():\n\t\t\t    getattr(cls, k).append(v)\n\t\telse:\n\t\t\tcls.hasHook = True\n\t\t\n\t\t\n\t\treturn cls\n\treturn decorator\n\t\t\t\t\n\ndef fire(event, function):\n\t\n\t# Get the instance\n\tinstance = event.get(\"instance\")\n\t\n\t# Get the hook to do this\n\thook = getattr(instance, \"hook\")\n\thook_args = hook.get(\"args\", [])\n\t\n\t# Register the hook\n\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n@Hooker.listen('before_server_start')\ndef before_server_start(config):\n\t\"\"\"\n\tCalled by :class:`Hooker.start() <Hooker.start>`.\n\t\n\tIn the hook this is called before the server is started.\n\t\n\tArgs:\n\t    config (Config): Global configuration.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n# Copyright (c) 2012 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\t\t\n\t# The decorator for this class\n\tdef __init__(self, hook):\n\t\t\"\"\"\n\t\tSet a new member function to handle this function.\n\t\t\n\t\tArgs:\n\t\t    hook (HookType): Hook attribute for which to register the handler.\n\t\t\n\t\tReturns:\n\t\t    callable: Decorator for registering listeners for the specified\n\t\t    hook\n\t\t\"\"\"\n\t\n\t\tself.__hook = hook\n\t\t\n\tdef __get__(self, obj, objtype):\n\t\tif obj is None or obj is False:\n\t\t\treturn self.__hook\n\t\tmember = self.__hook\n\t\thook = member.__get__(obj, obj,"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \t\n\t    if hasattr(self, hook.__name__):\n\t        self.hooks[hook.__name__] = hook\n\t    else:\n\t        raise HookHandleError(\"hook with name %s not found.\" % hook.__name__)\n\t\nclass HookHandleException(Exception):\n\tdef __init__(self, value):\n\t\tself.value = value\n\t\n\tdef __str__(self):\n\t\treturn repr(\"Couldn't handle value %r\" % self.value)\n\"\"\"This module contains the general information for Dakota modules.\n\nSee the file README for more details.\n\"\"\"\n\nfrom"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\treturn {\n\t\t\"name\": user\n\t}# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, unicode_literals, print_function\n__author__ = 'katharine'\n\nimport re\nimport requests\nimport urllib\n\n\ndef get_content(url):\n    \"\"\"Gets the raw content of an ICU website.\n\n    Args:\n        url ( str ): the URL to perform the IMCU requests.\n\n    Returns:\n        A list of dictionaries of data from the page. For more information\n        about the available IMCU sites for each I"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tetcd_template = {\n\t\t'timeout': False,\n\t\t'username': user,\n\t\t'password': None,\n\t\t'version': None,\n\t\t'data_dir': None,\n\t\t'token': None,\n\t}\n\n\ttry:\n\t\twith open(etcd_conf) as f:\n\t\t\tdata = yaml.safe_load(f)\n\t\t\tuser = data['username']\n\t\t\tetcd_host = \"%s:%s@%s:%s\" % (user, etcd_host, etcd_port, data['token'])\n\t\t\tetcd"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\t# configures database and creates default values\n\tdbname ='seahorse'\n\tuser = user.strip()\n\thost = etcd_host\n\tport = etcd_port\n\n\t# configures connection\n\tconn = MySQLdb.connect(host=host, user=user, passwd=host, db=dbname)\n\treturn conn\n\n# end of configuration definition\n\n\n# -*- coding: utf-8 -*-\n# ------------------------------------------------------------\n# pelisalacarta - XBMC Plugin\n# Canal para TV4Play\n# http://blog.tvalacarta."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\ttry:\n\t\t\tconfig = dict(username=user,password=pwd,host=host,port=port)\n\t\t\t# Set up a database connection and run configuration.\n\t\t\tconn = MySQLdb.connect(host=host,port=port)\n\t\t\t# The connection information is stored in an array of dictionaries, keyed like:\n\t\t\t#\t   { \"engine\":<engine_name>, \"config\":{\"host\":\"%s\",\"port\":\"%d\"} }\n\t\t\t\n\t\t\t# Set up a data cursor and fetch results.\n\t\t\tcurs = conn.cursor()\n\t\t\tcurs.execute(\"\"\"\n\t\t\t\tSELECT dat"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tconfig = copy.copy(user_configuration)\n\tconfig[\"local_config\"][\"host\"] = etcd_host\n\tconfig[\"local_config\"][\"port\"] = etcd_port\n\tconfig[\"local_config\"][\"password\"] = \"\"\n\t\n\tif user == \"demouser\":     \n    \tconfig[\"local_config\"][\"user\"] = \"demo\"\n\telif user == \"demo-user\":\n    \tconfig[\"local_config\"][\"user\"] = \"demo2\"  \n    return config\n\n\n\ndef get_database(user, root_pw=\"\", root_database=\"\", database_path=\"\"):\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\n\tapp.config['SECRET_KEY'] = \\\n    \t\tapp.config['SECRET_KEY'].strip() + \"_\" \n\tapp.debug = True\n\tapp.run(host=etcd_host, port=etcd_port)\n\n\treturn {\n\t    'engine': 'django.db.backends.postgresql_psycopg2',\n            'HOST': etcd_host,\n            'NAME': app.config['DB_NAME'],\n\t    'USER': user,\n\t    'PASSWORD': app.config['DB_PASSWORD'],\n\t    'HOST_OPTIONS': {},\n\t    'PASSWORD_HASH': app.config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tconfig = {\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port\n\t}\n\t\n\tif etcd_host == \"localhost\":\n\t\tconfig[\"protocol\"]='http'\n\telse:\n\t\tconfig[\"protocol\"]='https'\n\tconfig[\"hostname\"] = etcd_host\n\t\n\tif etcd_port!= 0:\n\t\tconfig[\"port\"] = etcd_port\n\t\n\treturn config\n\ndef generate_random_string(length):\n\t\"\"\"\n\tGenerates a 'random string' of the specified length, \n\tconsisting"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tconf = {\n\t\t# The maximum size of a group of objects allowed in a deployment\n\t\t# service.  The recommended size is 10,000 for an admin user.\n\t\t'max_objects_per_group': 20,\n\t\t# The max size of a user on the group that is explicitly used by the\n\t\t# deployment service.\n\t\t'max_users': 20,\n\t\t# The max size of a user on the group that is currently\n\t\t# deployed on the group.\n\t\t'max_users_per_role': 0,\n\t\t# The size (in chars"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t    \t\n\twith io.open(join(etcd_dir, \"configs\", \"configuration.ini\"), \"r\") as f:\n\t\n    \t\t# The configuration name (without path) is important here. We \n\t\t# only want to generate it once.\n\t    \t# We will try again on the default site.\n\t    \tconf_name = f.readline().strip(\" \\n\").strip(\" \\t\\n\").strip()\n\t    \t\t# Try to read the key.\n\t    \tif conf_name == \"\":\n    \t          \t \t\n\t\t    print(\"ERROR: No value for \""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\treturn {\n\t\t\"user\" : user,\n\t\t\"etcd_host\" : etcd_host,\n\t\t\"etcd_port\" : etcd_port,\n\t}\n\n\ndef set_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tSets a basic, and optional config.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is None:\n\t\treturn datetime.timezone.utc\n\t\n\tif dt.tzinfo is not None:\n\t\tif dt.tzinfo.utcoffset(dt) is not None:\n\t\t\treturn dt.astimezone(dt, is_dst = True)\n\t\n\treturn datetime.timezone.utc\n\n\ndef get_timezones(preferred_use_utc = False):\n\t\"\"\"\n\tGets a list of (preferred) time zone names.\n\t\n\tIf the given preferred time zone is not known yet or if the\n\tpreferable parameter is true, all time zones will be added.\n\t\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tassert dt.tzinfo\n\tif dt.tzinfo.utcoffset(dt) is not None:\n\t\twhile dt.tzinfo.dst() == dt.utcoffset(dt):\n\t\t\ttimestamp = dt.replace(tzinfo=None)\n\t\t\tdt = dt - epoch\n\t\treturn timestamp\n\t\n\tfrom dateutil.tz import tzoffset\n\ttz = tzoffset(dt)\n\t\n\treturn dt + datetime.timedelta(seconds=tz.utcoffset(dt))\n\n\n\ndef datetime_date(dt):\n\t\"\"\"\n\tGiven a datetime in the current timezone, calculates an\n\taware datetime in an unambiguous"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t    \n\tif type(dt) is datetime.datetime:\n\t\treturn dt\n\telif type(dt) is str or dt is None:\n\t\treturn None\n\telif type(dt) in [int, float, long]:\n\t\t# We're in epoch milliseconds or a float. If this is too low, we just\n\t\t# treat the whole microseconds differently by skipping it.\n\t\treturn (dt - datetime.datetime(1970,1,1,0,0,0)).total_seconds() + dt.microsecond\n\t\n\t# Nope, it's ambiguous (returns the same datetime) as"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\t# If we're already a date based one, then we want to consider the\n\t# ambiguous date into its local time zone.\n\t\n\tif self.type is not None:\n\n\t\tif self.tzname is None and self.utcoffset is not None:\n\t\t\tself -= _fromutc(self.utcoffset)\n\t\t\tdt = dt - self.utcoffset\n\n\t\telif self.tzname is not None:\n\t\t\t\n\t\t\t# local time => UTC\n\t\t\t# naive : same timezone\n\t\t\t# aware : same timezone (i.e. not ambiguous)\n\t\t\tself -= _fromutc(self.tzname)\n\t\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\t# Our earliest representable date, including a \"time interval\" for the\n\t# earliest time while others are identified as ambiguous.  Thus,\n\t# although we don't even need the first occurring in the ambiguous\n\t# time interval, we will have to take the date in order to\n\t# represent it.  This is used to force a fuzzy search since the\n\t# start of this iteration. \n\td = date.today()\n\td = d.replace(year=d.year+1)\n\n\t# Get a reference to our timezone-aware datetime."}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n    # This function looks at the datetime's offset by the time field.\n    #\n    # Returns the time relative to UTC, without using timezone offsets on\n    # that.\n    #\n    # :param offset:\n    #     The offset in minutes from UTC, at which the datetime is in the\n    #          timezone.\n    #\n    # :return:\n    #     A new :class:`datetime.datetime` object with the offset and\n    #         time adjusted to the timezone if the specified one,\n    #         otherwise the timezone is offset in minutes.\n\t#     \n\t#     :raises ValueError: if the datetime"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\t# For each timezone, we create a new copy but informing the \n\t# timezone, possibly breaking things. It has the same signature as\n\t# we do when actually parsing a datetime (which was parsed before).\n\tfor zone in dt.tzinfos:\n\t\tzone_copy = copy(zone)\n\t\tif isinstance(zone, tzinfo):\n\t\t\tzone_copy = zone_copy.replace(tzinfo=None)\n\t\ttimezone_copy = copy(zone_copy)\n\t\tfor attr in _ALLOWED_ATTRS:\n\t\t\tsetattr(timezone_copy, attr, getattr(zone"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n##\n# @brief Unit testing facility\n##\nclass Test_UTC(Base_Test):\t\n\t\n\tdef __init__(self, timezone=\"UTC\"):\n\t\n\t\t\"\"\"\n\t\t:param:\n\t\t\ttzinfo:\n\t\t\t\n\t\t:param:\n\t\t\tdescription:\n\t\t\t\n\t\t\"\"\"\n\t\t\n\t\tself.timezone = timezone\n\t\t\n\t\tself.start = datetime.now()\n\t\tself.end = self.start + datetime.timedelta(self.timezone.utcoffset)\n\t\t\n\tdef test_date_creation(self):\n\t\n\t\t\"\"\"\t\n\t\ttest that date generation occurs at every *valid* datetime.\n\t\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif not dt.tzinfo:\n\t\treturn dt\n\t\n\tif dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt\n\t\n\t# Look for fractions of second from UTC to +00:00\n\tfraction = None\n\t(fraction1, fraction2) = _split_tzinfo(dt, fractional=1)\n\t\n\t# Get the fractions of the second occurring in the desired\n\t# timezone\n\t(fraction1, fraction2) = _parse_relativetime(fraction1 + fraction2)\n\t\n\t# If we have a non-zero fractional fraction, parse fra"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t \n\tif dt.tzinfo == _UTCTimeZone:\n\t\treturn self._utc\n\telif dt.tzinfo is None:\n\t\treturn None\n\traise ValueError(\"Don't know how to compute tzinfo of \" + str(dt))\n\t\nclass _TimeInUtc(int):\n\t\"\"\"A time-in-Utc representation.\"\"\"\n\tdef __new__(self, val=' '):\n\t\t\"\"\"Create a (possibly) a new TimeInUtc object.\n\t\t\n\t\t:param int val:\n\t\t\tThe new TimeInUtc value.\n\t\t\n\t\t:returns:\n\t\t\tA time-"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(self, \"years\"):\n\t    self.years=0\n\tyears=self.years\n\tmonths=self.months\n\tweekdays=self.weekdays\n\tdays=self.days\n\t\n\t\n\t\n\tdiff=relativedelta(years=years, months=months, \n\tweeks=weekdays, days=days)\n\tdiff=diff.replace(\n\t\t\t\tmonth=self.months+self.weeks,\n\t\t\t\tyear=(self.years-1),\n\t\t\t\tday=self.days )\n\n\t\n\t\n\treturn diff\n    \n\n# =============================================================================\n#                              class Date, time and"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\timport datetime as dt\n\tdt.normalize = lambda d: relativedelta(d) / 31\n\treturn dt\n\ndef _from_months(d, m):\n\tfrom dateutil.relativedelta import relativedelta\n\t(y, m, d) = [int(y) for y in d.split('-')]\n\treturn relativedelta(y, m, d)\n\ndef from_weekdays(d):\n\t'''\n\tFrom wikipedia, returns a date that is the last Monday of the week given.\n\t\n\t>>> from_weekdays(date(2013,1,18), week"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\tfrom datetime import relativedelta as _relativedelta\n\treturn relativedelta(*_parse_relative_time(self))\n\n\ndef today(year=None):\n\t\"\"\"\n\tReturn the current date in the given year.\n\t\n\tThis can be used to round the date to a specific time point in \"natural\n\ttime\" (like \"Belgium\", \"1:30am\").\n\t\"\"\"\n\tfrom datetime import date, timedelta as _timedelta\n\t\n\tif year is None:\n\t\treturn datetime.today()\n\telse:\n\t\treturn date(year, 1, 1)\n\ndef"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\tif self._relative is None:\n\t\treturn None\n\t\n\t\nclass weekday(relativedelta):\n\t\"\"\"\n\tA weekday.\n\t\n\tThe class weekday is defined as follows :\n\t\n\t>>> date(2007, 4, 8)\n\trelativedelta(weekday=TU)\n\t>>> relativedelta(weekday=MO(-1, -1))  # Monday\n\trelativedelta(weekday=TH(-1))\n\t\n\t:constructors:\n\t\n\t    * **weekday:nth_weekday** (weekday number)\n\t\t\n\t    +---------------+---------------\n\t\t* TOTAL\n\t\"\"\"\n\t\n\t#: Monday\n\tHOUR = 1"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\n\ndef timedelta(days=0, seconds=0, microseconds=0, milliseconds=0):\n\t\"\"\"\n\tReturn a :class:`dateutil.relativedelta.relativedelta` object based on\n\tthe given :class:`relativedelta` object, which by default uses milliseconds\n\tand then precision.\n\t\n\t>>> relativedelta(days=1.5, hours=2, months=4, days=1.5,\n\t...\t\t\t\t\t  seconds=5.5, microseconds=123456)\n\trelativedelta(days=+1, hours=+14)\n\t\n\tSimilar to dateutil.relativedelta, timedelta, with microsecond"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\tyear, month, day = self._getYears(self.getStartDate(), self.getEndDate(), self.getStartHour(), self.getStartMinute(), self.getEndHour(), self.getEndMinute())\n\t\t\n\t\nclass YearDelta(object):\n\t\"\"\"\n\tProvide the delta between two dates.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\n\t:return:\n\t    Returns a :class:`relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t__slots__ = ("}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(self, datetime.timedelta):\n\t\traise TypeError(\"Only date objects can be normalized\")\n\tdts = self.days*24*60*60\n\treturn relativedelta(dts, dts)\n\t\t\t\t\t\t\nclass Day(DateTime, Duration):\n\t\"\"\" An observing recurring interval of the year (1-indexed) \"\"\"\n\t__slots__ = [\"weekday\", \"n\"]\n\tdef __init__(self, n=1, normalize=True, weekend=None, holidays=None):\n\t\tif normalize:\n\t\t\ttimedelta.__init__(self, n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n        return self._normalize()\n    @classmethod\n    def fromtuple(cls, t):\n        \"\"\"\n        Create a relativedelta from a :class:`dateutil.relativedelta` object.\n        \n        :param t:\n            A :class:`dateutil.relativedelta.relativedelta` object.\n        \n        :return: A :class:`dateutil.relativedelta.relativedelta` object for the given\n            :class:`dateutil.relativedelta` object or ``None``.\n        \n        :raises ValueError:\n            When ``t`` is not an instance of :class:`dateutil.relativedelta.relativedelta`\n            or a Python ``datetime`` s or a datetime string.\n        \n        This"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t    \n\td = None\n\tif self == relativedelta(days=1):\n\t    d = relativedelta(days=2)\n\t\t\n\treturn relativedelta(years=d.years, months=d.months, days=d.days)\n\n# vim:noexpandtab:ts=4\n#!/usr/bin/env python\n#-*- coding: UTF-8 -*-\n#\n# This file is part of the NNGT project to generate and analyze\n# neuronal networks and their activity.\n# Copyright (C) 2015-2019  Tanguy Fardet\n\nimport argparse\nfrom"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\t    return relativedelta(self.days, self.hours, self.minutes, self.seconds)\n\t\n\tdef __add__(self, other):\n\t    if type(other) == basestring:\n\t\treturn relativedelta(self.days + other[1:],\n\t\t\t\t\t\t\t    self.hours, self.minutes, self.seconds + other[1:],\n\t\t\t\t\t\t\t    self.microseconds)\n\t    if type(other) == type(self):\n\t\treturn self + relativedelta(self.days,\n\t\t\t\t\t\t\t\t    self.hours, self.minutes, self.seconds + other,\n\t\t\t\t\t\t\t\t    self.microseconds)\n\t    return NotImplemented\n    __radd__ ="}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\nimport sys\nimport datetime\ntry:\n    _test_unicode(True)\nexcept UnicodeEncodeError:\n    _test_unicode(False)\nelse:\n    def _test_unicode(value):\n        \"\"\"Change the string using the encoding declared inside a utf-8 string\"\"\"\n        if value.find(u'\\ufffd') >= 0:\n            value = value.replace(u'\\ufffd',\"\")\n            value = value.replace(u'\\xfd\\xfe','=')\n            value = value.replace(u'\\xfe','|')\n            value = value.replace(u'\"','&quot;')\n            return value\n       "}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n        import sys, codecs\n        return codecs.lookup(namefunc).encode('iso8859-1')\n        \n    codecs.register(\n        __builtin__,\n        \"_is_unicode\",\n        1,\n        False,\n        'iso8859-1'\n    )\n\ndef _is_unicode(namefunc):\n    try: namefunc.encode('iso8859-1')\n    except: # assume it's already Unicode string\n        return False\n    else:\n        result = False\n        chr = chr\n    try: name = namefunc.decode('iso8859-1')\n    except: # assume it's already Unicode string\n        result = False"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef _getchar(c):\n    \"\"\"\n\tGet character in given byte code (or an exception thrown by the char_decode function)\n\t\n\tc-encountered character which contains the character encoding that was not valid \n\tReturn None for non-inexistent character\n\t\"\"\"\n\ttry:\n\t\treturn chr(c).decode('cp1252')\n\texcept:\n\t\treturn None\n\t\t\n\nif __name__ == '__main__':\n\tclass Test_UTF16(Test_UTF8):\n\t\t\"\"\" Unicode encoding test \n\t\n\tExample test:\n\t\t# latin-1 code page 446"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\t# Convert Python 2 to Unicode\n\timport sys\n\tif type(namefunc) == str:\n\t \tnamefunc = unicode(namefunc.decode(), *encodings[PYTHON_ENCODING_NAMEEN])\n\ttry:\n\t \tname = namefunc.decode()\n\texcept UnicodeDecodeError:\n\t \tname = namefunc.decode().encode()\n\n\treturn name\n\n#----------------------------------------------------------------------\nif PY3:\n\tdef is_py3(obj):\n\t    return isinstance(obj, str)\n\telse:\n\t    return type(obj) == types.ModuleType or isinstance(obj, unicode)\n\n\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\t# set the encoding for the console as well, and replace it with proper locale\n\timport locale, sys\n\tfrom locale import setlocale, LC_TIME  # pylint: disable=F0401\n\timport locale\n\t\n\t# setup Unicode output to be same as original string (usually the default encoding of locale.getpreferredencoding() of python is utf-8)\n\tsetlocale(LC_TIME, 'ascii')\n\t\n\t# get the locale information\n\tlocale.setlocale(locale.LC_TIME, 'english')\n\t\n\t# we"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n    if namefunc is not unicode:\n        namefunc.encode(\"utf-8\")\n    encoding = sys.getfilesystemencoding()\n    return s.encoding if encoding is None else s.encoding.replace('\\0', '')\n\ndef to_unicode_or_bust_encoding(t):\n    \"\"\"\n    Encode the given bytes as 'encoded' strings, even for characters that can't be represented in the given encoding.\n    \"\"\"\n\n    if isinstance(t, unicode):\n        return t\n    elif isinstance(t, str):\n        return t.encode(getdefaultencoding() or getdefaultencoding())\n    else:\n        return t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\t# Get unicode output back\n    # I'm not sure if this should be the right way to do this without converting to a string to be\n    # unicode it produces the wrong output for Unicode encodings to be utf8 for whatever reason.\n    \n    # Get character set output back\n    # Get character set state back\n    # Get character state back\n    \n    # Get buffer state back (if any)\n    # Get char set buffer state\n    # Get window contents (if any)\n    # Get control entry buffer state\n    \n    # Return the unicode codec state\n    # Return'mbcs' or 'ascii' depending on the current locale settings\n    #"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\tdef tzname_in_utf8(namefunc):\n\t\t\"\"\"\n\t\tChange bytes output into bytestrings in UTF-8\n\t\t\n\t\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\t\tto unicode strings\n\t\t\"\"\"\n\t\n\t\n\tdef replace_by_name(replacement, namefunc):\n\t\"\"\"\n\t\tSubstitute unicode characters\n\t\n\treturn unicode() with replacement as a unicode\n\n\t\"\"\"\n\t\n\t# Convert unicode string to byte array\n\tunicode_buffer_ptr = POINTER(BYTEARRAY)()\n\tunicode_buffer_ptr.buffer = ctypes.create_"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\t\n\treturn namefunc()\n\n\ndef _test():\n\timport doctest\n\tdoctest.testmod()\n\n\nif __name__ == '__main__':\n\t_test()\nimport inspect\nimport sys\nimport unittest\nimport pkgutil\nimport pkgutil as import_pkg\nfrom unittest.util import safe_doctest, eq_\n\nsimple_types = (\n    type, int, float, complex, str, list, tuple,\n)\nbytes_types = (int,)\ntry:\n    import codecs\n    unicode()\nexcept ImportError:\n    unicode = str\n\ntry:\n    import cPickle as pickle "}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef tzname():\n\t\"\"\"\n\tChange back to Unicode strings\n\t\"\"\"\n\treturn \"\".join([chr(x) for x in range(sys.maxunicode)/32])\n\t\n\t\ndef decode_utf8(string, encoding):\n\t\"\"\"\n\tReturn the unicode decoded into utf_8, with \"decoding encoding\" type\n\t\"\"\"\n\treturn string.decode(encoding)\n\t\ndef decode_utf_16(string):\n\t\"\"\"\n\tReturn unicode decoded, but with \"decoding encoding\" type\n\t\n\treturn string.decode(encoding)\n\t\n\t\nclass Test_Gather:\n"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\treturn \"autopep8 \" in read_pipe(['git', 'tag', '--dirty', 'HEAD'])\n\n\nsetup(name = 'autopep8',\n\tversion = get_versions(),\n\tdescription = 'A tool that automatically formats Python code to conform '\tlong_description=readme.read(),\n\tlicense = 'Expat License',\n\tauthor = 'Samuel Cabralski',\n\tauthor_email ='scocrackingh@gmail.com',\n\turl = 'http://autopep8.readthedocs.io',\n\tscripts = ['autopep8"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\n\n\tversion = 0\n\tf = open(VERSIONLOGFILE)\n\tfor line in f.readlines():\n\t\n\t\tline = line.strip()\n\t\t\n\t\tif line == \"\":\n\t\t\tcontinue\n\t\t\t\n\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\n\t\tm = VERSION_NAME_MATCHER.match(line)\n\t\tif m == None:\n\t\t\tprint(\"Couldn't determine version name\")\n\t\t\tsys.exit(2)\n\t\telse :\n\t\t\tprint(m.group(1))\n\t\tm = VERSION_REGEX_MATCHER.match(line)\t\t\n\t\tif m =="}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\treturn \"2.14.18\"\n\n\tversions = get_versions_dict()\n\treturn versions[\"version\"]\n\ndef run_custom_query(query):\n\t# if query is none, start a new run on all databases\n\tif query is None:\n\t\tquery = queries.custom_query\n\n\tif query is None:\n\t\treturn \"1\"\n\n\t# run once\n\trun_sql_in_parallel(query)\n\ndef run_custom_script(path, query=None):\n\t\"\"\"\n\tRun a custom web ui in a sub thread."}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\n\t\n\tglobal version_file\t\n\tglobal version_data\n\t\n\tversion_file = version_file or open(get_version_file(),\"r\")\n\tversion_data = version_file.read()\n\tversion_file.close()\n\t\n\treturn version_data\n\t\ndef clean_outdated(name):\n\t\"\"\"\n\tRemove outdated entries from list\n\t\"\"\"\n\tif name in frappe.db.sql_list(\"select name from tabDocType where ifnull(issingle,0)=1\"):\n\t\twebnotes.conn.sql(\"DELETE FROM `tabCustom Field"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\treturn VERSION\n\n# Use the docstring from the Python standard library if possible (and ignore\n# non-python files with the \"if __name__ == '__main__')\".\n# Otherwise construct a docstring from it.\ndef read(*paths):\n\t\"\"\"\n\tRead a list of paths.\n\t@param paths: a list containing paths\n\t@type paths: string\n\t@return: the text lines of an input text file\n\t@rtype: list of strings\n\t@note: to run the example without arguments, use \\n\\n and then use\n\tthe readlines module instead"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\tif not settings().getBoolean([\"devel\", \"versionControl\"]):\n\t\t\treturn []\n\t\treturn __versions.versions\n\tversions = property(get_versions)\n\tdef get_all_versions(key):\n\t\tversions = __versions.all().ancestor(key)\n\t\tprint(\"Version info for key\", key, versions)\n\t\treturn versions\n\tall_versions = property(get_all_versions, doc=doc)\n\n\tdef clear_versions(key):\n\t\tversions = __versions.all().ancestor(key)\n\t\tfor version in versions:\n\t\t\tversion.delete"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\timport frappe.utils\n\n\t\tversions = {}\n\t\tfor name in frappe.get_all(\"Version\", fields=[\"name\", \"parent\"], filters = {\"parent\": name}):\n\t\t\tversions[name.name] = _get_version(name.parent)\n\n\t\tif versions and versions.get('parent'):\n\t\t\tversions.update(versions.get('parent'))\n\n\t\treturn versions\n\n\treturn dict(unknown=\"something unparsable\")\n\ndef _get_version(parent):\n\tversion = None\n\tfor fname in frappe.get_all(\"Version\", fields=[\"name\", \"parent"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t # If we can't figure this out, the git config may be out of date.\n\t release = version[0:version.find('.')]\n\texcept ValueError:\n\t\trelease = ''\n\treturn release\n\nsetup(\n\tname = 'django-celery',\n\tversion = get_versions(),\n\tdescription = 'Run the Celery process',\n\tlong_description = README,\n\tclassifiers = [\n\t\t'Development Status :: 5 - Production/Stable',\n\t\t'Environment :: Web Environment',\n\t\t'Framework :: Django',\n\t\t'Intended Audience :: Developers',\n\t\t'"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t \n\tassert True if not check_version(), \"must run before calling process_version()\"\n\tp = subprocess.Popen([\"make\"],stdout=subprocess.PIPE)\n\tversion = p.communicate()[0]\n\tif p.wait()!= 0:\n\t\traise NotThisMethod(\"Could not determine version\")\n\treturn version\n\n# Change to the absolute path to your project root\nsys.path.append(GetAbsolutePath())\n\n# We defer to a DJANGO_SETTINGS_MODULE already in the environment. This breaks\n# if running multiple sites in the same mod_wsgi process"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t # get_version_info(\"1.12.0\")\n\t return {\n\t\t\t\"MAJOR\": 1,\n\t\t\t\"MINOR\": 1,\n\t\t\t\"MICRO\": 1,\n\t\t\t\"BUILD\": 0,\n\t\t\t\"LIST\": 1\n\t\t}\n\ndef get_version_info(filename=None):\n\t\"\"\"\n\tGet version information from __about__.\n\t\"\"\"\n\tversion_info = {}\n\n\tif filename is not None:\n\t\t# now try to read it from the package version.log\n\t\tfilename = os.path.join(filename, \"__about__.py\")"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\t\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"]}\n\t\n\tstylepath = style.get(\"git-style\", \"git-style\")\n\t\n\tstylenames = [x for x in pieces.get(\"git-style-names\", \"git-style\").split(\",\") if x]\n\tstylepaths = {x: stylepaths[x] for x in stylenames}\n\t\n\tstyle = Style(stylepath)\n\t\n\tif not style."}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\t\n\tstyle_list = style.split(\" \")\n\t\n\tstyle_dict = {\n\t\t# set styles for individual level:\n\t\t\"Normal\": [],\n\t\t#\"Debug\": [],\n\t\t\"Debug.0\": [],\n\t\t\"Error.0\": [],\n\t\t\"None\": [],\n\t\t\"False\": [],\n\t\t#\"Verbose.0\": [\"0\", \"1\", \"2\", \"9\", \"6\", \"9.9\", \"7\"],\n\t\t\"Verbose.1\": [\"1\", \"2\", \"3\", \"5\", \"7\", \"9.9\", \"10\", \""}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\n\tif style == 'dark':\n\t\tstyle = 'primary'\n\n\telif style =='very dark':\n\t\tstyle = 'intermediate'\n\n\telif style == 'light':\n\n\t\tstyle = 'default'\n\n\telif style == 'dark':\n\n\t\tif platform.system().lower()!= 'linux':\n\t\t\t#raise ValueError(\"You must run on Linux, as light mode must be linux.\");\n\t\tstyle = 'default'\n\n\telif style == 'light':\n\n\t\tif platform.system().lower() == 'darwin':\n\t\t\t#raise ValueError(\"You must run on Mac OS X"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\t\n\tstylesheet=StyleSheet()\n\tfor name,value in sorted(style.keys()):\n\t\tstylesheet.set(name=name, value=value)\n\n\tstyles[name]=\" \".join(pieces)\n\t\n\tstylesheet.save(str(stylesheet))\n\t\ndef print_style():\n\t\"\"\"\n\tPrint a human-readable version of the stylesheet object.\n\t\"\"\"\n\t\n\tprint StyleSheet.HEADER\n\ndef print_pieces(pieces):\n\t\"\"\"\n\tRender the given pieces into the requested style.\n\t\"\"\"\n\t\n\tstylesheet=StyleSheet()"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\nclass ColorFilter(Filter):\n\n\tdef start(self, style, firstLineNumber, lastLineNumber, width):\n\t\tfilteredStyle = \"\"\n\t\tfor x in map(lambda x: style.filter.apply(x), pieces):\n\t\t\tif x == style.color:\t\t filteredStyle += \" \"\n\t\t\tfilteredStyle += x[1].format(firstLineNumber+x[0], lastLineNumber+x[1]) \n\t\tif (width < 2):\n\t\t\tcolorIndex = 2\n\t\telse:\n\t\t\tcolorIndex = 2\n\t\t\n\t\tstartIndex = 0\n\t\tend"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\ndef version_as_string(version_str):\n\t\"\"\"\n\t\"\"\"\n\tif len(version_str) < 5:\n\t\treturn '0.0.0'\n\tif len(version_str) < 6:\n\t\treturn '{0:4.1f}'.format(version_str)\n\tnewstr = str(version_str)\n\tnewvers = []\n\tfor version in ['0','1','2']:\n\t\tnewvers.append(int(newstr[0:2])*10000 + int(newstr[3:5]))\n\tif"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\t\n\tpieces = merge_dividers(pieces)\n\tdata = [data]\n\tnum_colors = len(set(style.color_map.keys())) if style.color_map else 1\n\t\n\tstyles = []\n\tfor idx in range(len(data)):\n\t\tcol = []\n\t\tcolors = []\n\t\tfor i in range(num_colors):\n\t\t\tif (idx in style.palette_map):\n\t\t\t\tcol.append({\n\t\t\t\t\t'bgcolor': style.palette_map[style.palette_map[idx].name],\n\t\t\t\t\t'style': style."}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n\n\trendered_styles = {\n\t\t\"basic-css\": (\"css/basic.css\",),\n\t\t\"basic-js\": (\"js/basic.js\",),\n\t\t\"basic-ui\": (\"css/basic.css\",),\n\t\t\"basic-lua\": (\"css/basic.css\",),\n\t\t\"basic-objdump\": (\"css/basic.js\",),\n\t\t\"basic-string\": (\"css/basic.css\",),\n\t\t\"basic-table\": (\"css/basic.css\",),\n\t\t\"file-map-rss\": (\"css/file-map.rss\",),\n\t\t\""}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n######################\n# Main:\n######################\ndef main():\n\t\"\"\"\n\tRun the application.\n\t\"\"\"\n\tos.system(\"./clean_up_old.sh\")\n\nif __name__ == \"__main__\":\n    main()\n# Copyright 2013, OpenStack, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n  \ndef build_version(pieces, style):\n\t\"\"\"\n\tReturn a version string, with the given pieces packed together.\n\t\"\"\"\n\tversion = \"unknown\"\n\n\tif pieces[\"closest-tag\"]:\n\t\tfor info in pieces[\"closest-tag\"].split(\",\"):\n\t\t\tif len(info) > 1:\n\t\t\t\tversion = info[0]\t\t# Always strip the 0x, keep 32-colons\n\t\t\t\tbreak\n\n\tif not style or style == \"default\":\n\t\treturn version\n\n\t# Identify the type\n\tif pieces[\"error\"]:\n\t"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\tif len(pieces)==0:\n\t\t\treturn pieces[0]\n\t\telse:\n\t\t\treturn pieces\n\t\ndef do_find_index_of_key_by_value(dict, key, list_to_find):\n\t#print 'In: dict', dict\n\tfor key_name, list_entry in dict.iteritems():\n\t\t#print 'In: key', key_name, 'in the list of search_key_field[search_key_field_name]'\n\t\t\n\t\tif (key_name in list_to_find):\n\t\t\t#print 'In:"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\ndef quote_if_necessary(p, context):\n\tfor x in p:\n\t\tif x.strip() == \" \":\n\t\t\treturn \"\\\"\" + x + \"\\\"\"\n\t\telse:\n\t\t\treturn x\n\t\t\ndef check_list_len(str, n=3):\n\t\tnum = str\n\t\twhile len(num) < n:\n\t\t\tnum = \"0\" + str + str\n\t\treturn num\n\ndoctype_table = {\n\t\"Sales Invoice\": [\n\t\t[\"Account\", \"income_account\",\"selling_cost_center\",\"cost_center\"],\n\t"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n\n\t# remove one char at the beginning if needed\n\tif pieces[0][-1] in \"rZkNn\":\n\t\tpieces = pieces[1:]\n\t\t\n\t# find one of:\n\t\n\t# a+\n\tif pieces[0][-1] in \"aZ\":\n\t\tpieces = pieces[1:]\n\t\n\n\t# get the plus part without the ending 'b'\n\tlast_bit = \"\".join(pieces[::2])\n\t# in case there's a new bit, add its extra digits\n\tif len(pieces) == 2:\n\t\tpieces[1]"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n    res = \"\"\n\n    if len(pieces) > 0:\n\tres = res + pieces[0]\n\tres = res + pieces[1]\n\n    return res\n\nclass Node:\n    \"\"\"\n\tNode class that creates an ATOM_NODE node from parsed data\n\t\"\"\"\n    # This class is an ATOM node class, it provides a default root node with a single root\n\t# the root node defines all its children's attributes.\n    def __init__(self):\n        self.attributes={}\n        self.attributes_count={}\n        self.attributes_list=[]\n\n    def get_name(self"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n\n\tif len(pieces)==0:\n\t\treturn None\n\t\n\tlastval=None\n\tfor p in pieces:\n\t\tif p in [\"+\", \"-\"] and p!= lastval:\n\t\t\tlastval = p\n\t\t\tprint(\"plus or dot\",lastval)\n\t\t\treturn None\n\t\n\treturn lastval\n\n\t\ndef find_all_words(text):\n\n        print(\"finding all the words\",text, \"in\")\n\twords = re.split(' ', text)\t\n\t#print(words)\n\t\n        words = [l + \"_words\" for l in words]\n        words"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\ndef comma_then_dot(pieces):\n\t\"\"\"\n\n\tSplit by colons and return dot if it were, and return the two joined strings.\n\t\"\"\"\n\twords = []\n\tword = pieces[0]\n\twordLen = len(word)\n\twhile len(word) < wordLen:\n\t\tword = word + \" \"\n\t\tif \",\" in word:\n\t\t\twords.append(word.split(\",\"))\n\t\telif \",\" in piece:\n\t\t\twords.extend(piece.split(\",\"))\n\t\telif \",\" in word:\n\t\t\tprint(\"Error in comma_"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n#     if len(pieces) < 2:\n#         return ''\n#     return pieces[0] + ',' + pieces[1]\n#\n#     return pieces[0] +'' + pieces[1] +'' + pieces[2]\n#  \n#  def parse_range(s):\n#     m = []\n#     try:\n#         toks = s.split('-')\n#         while toks:\n#             if len(toks) == 1:\n#                 toks.remove('0')\n#             m.append(int(toks[0"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\tfirst, rest = pieces[0], pieces[1:]\n\tif not rest:\n\t\treturn pieces\n\ttry:\n\t\trest = list(rest.pop(0))\n\texcept IndexError:\n\t\treturn pieces\n\tif not rest:\n\t\treturn first\n\treturn '/'.join(rest) + '/'\n\nclass OrderedSet(utils.Hashable, Iterable, set):\n\t\"\"\"Implements a set which keeps elements added/removed from each other\n\n\tThis class does not implement __delitem__() and __iter__()\n\tit does not offer any of the functionality required for this\n\tclass -"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t return pieces[0]\n\ndef with_int(value):\n\tresult = int(value)\n\tif result < 0:\n\t\traise ValueError(value)\n\treturn result\n\ndef maybe_convert_to_int(value):\n\t\"\"\"\n\tConvert value to an integer. If no integer provided or an integer of\n\ttype 'int', return value. If number, return Integer(i) else return None.\n\t\"\"\"\n\ttry:\n\t\treturn int(value)\n\texcept ValueError:\n\t\treturn None\n\ndef parse_val(val):\n\t\"\"\""}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\treturn [pieces[0]] + pieces[1:]\n\treturn [pieces]\n\ndef single_dot(pieces):\n\t\"\"\"\n\tReturn a + if we don't already have one, else return a.\n\t\"\"\"\n\t\treturn pieces[0]\n\ndef split_by_regex(text, regex, groups = 1):\n\t\"\"\"\n\tSplit a text into subgroups of at most `groups` matches a partial\n\tregex, returning a tuple of its name and a list of groups.\n\n\tArgs:\n\t\ttext: The text being splitted\n\t\tregex: The regex"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tif isinstance(commands, str):\n\t\t\tcommands = \" \".join(commands.split())\n\n\t\tif \"shell\" in commands:\n\t\t\tcmds = [ \"sh\", \"-c\", commands ]\n\t\telif \"su\" in commands:\n\t\t\tcmds = [ \"su\", \"-c\", commands ]\n\n\t\tif verbose:\n\t\t\tprint(\"Executing command: \", \"%s %s\"%(cwd, \" \".join(cmds)))\n\n\t\tos.chdir(cwd)\n\n\t\ttry:\n\t\t\tret     = subprocess.call(cmds,stdout=stdout,stderr=devnull,env=env,cwd="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tif isinstance(commands, list):\n\t\tcommands =''.join(commands)\n\telse:\n\t\tcommands = shlex.split(commands)\n\t\t\t\n\t(stdin_pipe, stdout_pipe)  = os.pipe()\n\tstdin = None\n\tif cwd:\n\t\tstdin_pipe = Popen(cwd, stdin=stdin_pipe, stdout=stdout_pipe, stderr=stderr_pipe, env=env)\n\t\tstdin_pipe.stdin.write(stdin)\n\tstdin_pipe.stdin.close()\n\t\n\tout_pipe, stderr_pipe = os.pipe"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(cmd(commands, args, cwd))\n\n\tif hide_stderr:\n\t\tcmd(commands, args, cwd=cwd)\n\n\tretval = call(commands, args, cwd=cwd, env=env)\n\tsys.exit(retval)\n\ndef show_warnings(warnings):\n\t\"\"\"\n\tShow errors and warnings in the console.\n\t\"\"\"\n\tlevel1 = \"Warning\"\n\tlevel2 = \"Error\"\n\twarnings = sorted(warnings)\n\tfor warning in warnings:\n\t\tmessage = warning['msg']\n\t\tif"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t\n\t# TODO: get rid of this if path doesn't exist\n\t# If path is not existing, we'll create it (and create if it doesn't\n\t# exist and write there).\n\tif cwd is None:\n\t\tcwd = os.path.dirname(os.path.realpath(sys.argv[0]))\n\t\t\n\tprint\n\tprint \"Executing: \" + str(commands)\n\tprint \"Arguments: \" + str(args)\n\tprint \"Working Dir: \" + cwd,\n\tprint\n\n\t# Create the new command, if required\n\tif not env"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\tc = _get_cmd(commands, path=cwd, prompt=False, quiet=False, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\tif c:\n\t\tif cwd is not None:\n\t\t\tresult = c(c, cwd=cwd)\n\telse:\n\t\tresult = 0\n\treturn result\n\ndef command(cmd, cwd=None, env=None):\n\t\"\"\"\n\tCall the given cli command(s).\n\t\"\"\"\n\treturn run_command(['/bin/bash', '-c', cmd], cwd=cwd"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t\n\t# Add our own environment to the context\n\t# This has the highest chance of triggering errors which\n\t# can cause errors when using these commands.\n\t\n\tos.environ[\"PATH\"] = commands[:]\n\tcwd = cwd or os.getcwd()\n\t\n\t# Do our own command and get its stdout so we can\n\t# feed it all in.\n\t\n\tif verbose:\n\t\tsys.stdout.write(\"executing: \" + \" \".join(commands) + \"\\n\")\n\t\n\t# Run the process we were given.\n\tcwd_stdout, cwd_stderr, cwd_rc ="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tlog = make_logger()\n\n\t\targs = \" \".join(args)\n\t\tif args.startswith(\"/\"): # hack for Windows path.\n\t\t\targs = args.replace(\"/\", \"\\\\\")\n\t\t\t# args = args.replace(\"\\\\\", \"/\")\n\t\t\targs = args.replace(\"//\", \"/\")\n\n\t\ttry:\n\t\t\targs = shlex.split(args)\n\t\t\tcommand = args[-1]\n\t\texcept:\n\t\t\t# In case there's only one argument: the command string is not recognized.\n\t\t\treturn 1\n\n\t\ttry:\n\t\t\tretval = commands[command](*args, cwd"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\n\tif dry_run:\n\t\treturn\n\n\tif not shell:\n\t\twarnings.warn(RuntimeWarning(\"No shell required for %s\" % str(tool).replace('\\'','\\'\\'\"',1)))\n\t\treturn(0)\n\n\tif not kwargs:\n\t\tkwargs = {}\n\n\tlogging_level = logging.DEBUG if verbose else logging.INFO\n\tif hide_stderr:\n\t\tlogging_level = None\n\t\n\tlogging.getLogger(\"pip._internal.utils\").setLevel(logging_level)\n\n\tif sys.stdout.isatty() and suppress_stdout:\n\t\tlogging.getLogger"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\t# Redirect the output to /dev/null if verbose is set to True\n\t\tif verbose:\n\t\t\tprint(cmd2str(commands[0]))\n\n\t# Start the child\n\tfrom. import __version__\n\n\tp = __import__('py_compile')\n\tif hasattr(p, 'compile'):\n\t\tscript = p.compile('','exec')\n\telse:\n\t\tscript = __import__('py_compile2')\n\t\tscript.compile = __import__('py_compile')\n\t\tscript.install_lib = __import__('py_compile2')\n\n\t\tif hasattr"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n#\tif type(cmd) is not type([]):\n#\t\tcmd = [ cmd ]\n\t\n\t#\t# Make sure we get the right cwd\n\t#\tif not cwd:\n#\t\tcwd = os.getcwd()\n\t\n\tprint(\"Running:\")\n\t\n\ttry:\n\t\toutput = subprocess.check_output(commands\n\t\t\t+ args.command + [ args.get_script(args, cwd) ] )\n\texcept Exception as Err:\n\t\t#logger.error(\"Error while executing: \" + str(Err))\n\t\t#return (output, False, False"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\tfrom warnings import warnpy3k\n\troot = get_root()\n\tsetup_cfg = os.path.join(root, \"setup.cfg\")\n\twith codecs.open(setup_cfg, \"r\", \"utf8\") as f:\n\t\treturn json.loads(f.read())\n\n\ndef get_cmdclass():\n\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n        from distutils.cmd import Command\n\n        class cmdclass(Command):\n            user_options = []\n\n            def initialize_options(self):\n                pass\n\n            def finalize"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\n\t\t\n\t# any values that are useful to give to the user\n    # on the top level, these are passed to the versioneer.py file\n    # to find the setup file(s), so they must be defined on a\n    # per-parse-line file.\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"\"\n    cfg.versionfile_source = \"kronos/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n    \nclass Not"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef get_cmdclass():\n\t\"\"\"\n\tCreate, populate and return commands for running Versioneer.\n\t\"\"\"\n\tcmdclass = versioneer.get_cmdclass()\n\tupdate_cmdclass_toctree_function(cmdclass)\n\tversioneer.get_cmdclass = get_cmdclass\n\treturn cmdclass\n\ndef cmdclass():\n\t\"\"\"\n\tCreate, populate and return the custom commands given by the setup\n\tcall.\n\t\"\"\"\n\tglobal VERsioneer\n\tversioneer.VCS = \"git\"\n\tversioneer.versionfile_source = \"versioneer.py"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef get_cmdclass():\n\t\"\"\"\n\tReturn the VersioneerCmd class used by Versioneer. It is used to create\n\tone of the subpackages, initializing it, and then use versioneer.\n\t\"\"\"\n\tcmdclass = {}\n\tcmdclass[\"_versioneer\"] = VersioneerCmd\n\tcmdclass[\"versioneer_git_version\"] = git_version\n\tcmdclass[\"versioneer_console\"] = \"git\"\n\tcmdclass[\"versioneer_error\"] = error_count\n\treturn cmdclass\n\nclass NotThisMethod(Exception):\n\tdef __init__(self,"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef versions_from_file(filename):\n\t\"\"\"Find the get_versions from a file ending in.pyc by looking for the\n\tfinding the updater version (\"variables\"), and using it to do the\n\tautomatic imports.\n\t\"\"\"\n\t# Open in read the contents of the file\n    with open(filename) as f:\n\t\tcontents = f.readlines()\n\t# Break the content into lines, starting with the next line.\n\t# Ignore any blank lines\n    contents = [line for line in contents if line[0]!= '#']\n\t# The version line must"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef get_cmdclass():\n\t\"\"\"\n\tGet the VersioneerConfig() constructor.\n\t\"\"\"\n \n\tcmdclass = get_cmdclass()\n\tcmdclass.update(versioneer.get_cmdclass())\n\tglobal ENABLE_PYSIDEJAR\n\tENABLE_PYSIDEJAR = cmdclass['--quiet']\n\treturn cmdclass\n\ndef parse_config_file(filename, verbose=False):\n\t\"\"\"\n\tParse a config file and return the VersioneerConfig() object.\n\t\"\"\"\n\timport re\n\tfrom setuptools import find_packages, setup\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\n\t\n\t# TODO: move to somewhere common, e.g., \"setup.py sdist\"\n\tif os.path.exists('MANIFEST'):\n\t\tos.remove('MANIFEST')\n\ttry:\n\t\tcmd = \"python setup.py versioneer\"\n\t\tout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).communicate()[0]\n\texcept OSError:\n\t\traise NotThisMethod(\"version.py not found in VERSION file\")\n\t\n\tinfo = out.split('.')\n\t# add a -dirty suffix to tag info if present\n\tif 'dirty' not"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\timport configparser\n\t\tconfig = configparser.SafeConfigParser()\n\t\tconfig.read(\"../../setup.cfg\")\n\t\tmetadata = dict(config.items(\"metadata\"))\n\t\toptions = dict(config.items(\"options\"))\n\t\treturn VersioneerConfig(metadata=metadata, options=options)\n\nclass NotThisMethod(Exception):\n\t'''Exception raised if a method is not valid for the current scenario.'''\n\tdef __init__(self, method, is_ok):\n\t\tself.method = method\n\t\tself.is_ok = is_ok\n\tdef __str__(self):\n\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\timport subprocess\n\n\t\t# Versioneer has some undocumented configuration values.\n\t\t# (https://github.com/warner/python-versioneer)\n\t\t# Versioneer >= 3.3.3 and the GREENIR project homepage.\n\t\tneeds_sphinx = '3.3.3'\n\t\texclude_dirs = ['../..']\n\t\tsetup_versioneer_file ='setup.cfg'\n\n\t\twith open('setup.cfg') as f:\n\t\t\ttext = f.read()\n\t\t\tconfig = subprocess.Popen(args, stdout=subprocess.PIPE,\n\t\t\t\tstderr=subprocess"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef versions_from_parentdir(parentdir_prefix, root, verbose=False):\n\t\"\"\"Try to determine the version from the parent directory name.\n\n\tparentdir_prefix is the root of the source tree.\n\n\tverbose is set to True to enable more diagnostics.\n\tReturns string in the form 'X.Y.Z' where 'X' is the version number.\n\n\t Raises RuntimeError if parentdir_prefix is not the root of the source\n\ttree. If verbose is False fail directly.\n\n\tReturns empty string if parentdir_prefix is the supplied path.\n\t"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\n# This little lookup is required for\n# traditional setuptools/distribute projects.\ndef pkgconfig(*packages, **opts):\n\t\"\"\"Register paths for third-party packages to sys.path.\"\"\"\n\treturn importlib.import_module(\n\t\tpkgutil.get_importer(*packages),\n\t\tpkgutil.PACKAGES_PACKAGE,\n\t\tpkgconfig.split() + list(opts['packages']), **opts)\n\nspr_cmd = None\nfor cmd in SPR_COMMANDS:\n\tif cmd.name =='svn':\n\t\tsvn_cmd = cmd\n\telif cmd."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\n\n\tdef vcs_handler(self):\n\t\tvcs.register_vcs_handler(self.vcs_path, self.method, self)\n\t\n\treturn vcs_handler\n\t\n\n\n\n\n\n\n# (c) 2013-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\n\n\t\tdef decorator(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapped(self, *args, **kwargs):\n\t\t\t\t# print \"Checking VCS handler for {}\".format(method)\n\t\t\t\ttry:\n\t\t\t\t\treturn func(self, *args, **kwargs)\n\t\t\t\texcept PermissionDenied:\n\t\t\t\t\treturn HttpResponseForbidden()\n\t\t\treturn wrapped\n\n\t\treturn decorator\n\n\n\ndef register_vcs_handler_with_local_file(vcs, method):\n\n\tlocal_file_path = vcs.local_file_path\n\n\tdef my_fake_function(func2):\n\t\t@"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\nvcs_handler = register_vcs_handler\n\n#################\n# VCS related signals #\n#################\n\n@signals.setting_changed\ndef _vcs_set():\n    \"\"\"\n    Sets the correct VCS repository for the repository tree. Only called\n    from the repository instance (the one which called this method is in)\n    for any repo that has configured VCS support.\n    \"\"\"\n    if vcs.type in vcs_settings.MIGRATION_MODULES:\n        return # This is already handled below\n    vcs_settings.VCS_REPO_FS_DEFAULT = VCS_REPOSITORY_FS_DEFAULT\n\n\n@"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(vcs, f)\n\t\t\treturn f\n\t\treturn decorate\n\n\tdef register_vcs_alias(vcs, func):  # decorator\n\t\"\"\"\n\t\tRegister a custom alias for a VCS. Ex. vcs_alias(\"bumpversion\")\n\t\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_alias(vcs, func)\n\t\t\treturn f\n\t\treturn decorate\n\n\tvcs.register_vcs_alias = register_vcs_alias  # for backward compat\n\n\tdef setenv(key, value):"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tvcs.register_vcs_handler(method, vcs_handler)\n\treturn vcs_handler\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,env=None):\n\t\"\"\"\n\tRun a series of commands. If commands is a list, run it through\n\tthe appropriate command, otherwise pass the command that was\n\tenpended. This is useful for debugging commands that may not\n\twork in all circumstances.\n\n\t:param commands: The command, or a list of commands.\n\t:type commands: str or"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\n\n\tdef __init__(self, vcs):\n\t\tself.__vcs = vcs\n\t\tself.__action = None\n\n\tdef __call__(self, controller, **kwargs):\n\t\tif self.__action:\n\t\t\treturn getattr(controller, self.__action)(*kwargs)\n\t\traise NotImplementedError, \"Don't know how to handle %s\" % self.__vcs\n\n\n\tdef handle(self, request):\n\t\tif self.__vcs._server:\n\n\t\t\t# we need to be in the thread, since the request body won't\n\t\t\t# have the message body while the repository was created\n\t\t\tif request."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs(vcs, method, f)\n\t\t\treturn f\n\t\treturn decorate\n\treturn decorate\n\ndef create_vcs_method(vcs_instance, name, version, port, vcs_config,\n\t\t\t\t VCS_CONFIG_DEFAULTS, *vcs_args, **vcs_config_args):  # decorator\n\t\"\"\"\n\tCreate a new method which will execute a VCS command.\n\n\t:param vcs_instance: *type* :class:`vcs.VCS`\n\t:param name: Repository name\n\t:param version: The version\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\ndef make_vcs_handler(vcs, method, *args, **kwargs):\n\t\"\"\"\n\tDecorator that injects into the main VCS interface (or in one of its\n\tsubclasses of the VCS interface. This decorator must be\n\tregistered in order, otherwise the implementation will not be\n\tfound.\n\t\n\tNote that this will continue to be registered as the handler.\n\n\t:param vcs: a VCS interface class\n\t:param method: a callable which will be passed the vcs as first parameter\n\t\tto the decorator\n\t\"\"\"\n\tvcs.add_vcs_handler(vcs, method"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t       \n\tvcs.register(vcs, method, priority=0)\n\n\ndef remove_vcs_handler(name):  # decorator\n\t\"\"\"\n        Decorator to mark a method as the handler of a VCS.\n\n        Note that this does not need to be decorated by a decorator.\n    \"\"\"\n       \n\tvcs.remove(name)\n\ndef is_valid_vcs_url(url):\n\t\"\"\"\n        Determine if a provided URL has any vcs url information, so that it can be used to determine whether \n        the VCS should be checked on the URL.\n\t\"\"\"\n\n\t# Strip of vcs version ("}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    total = 0\n       \t   if validate_objects:\n\t    total += 1\n       \tfor o in self.get_objects(include_parents=False):\n\t\troot = self.get_container(o.get_id())\n\t\tfor x in o.get_content():\n\t\t    if self.validate_container_hierarchy(x, validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t    if x.get_type()!= Container: continue\n\t\t    try:\n\t\t\t    xa = self.get_access_control_list("}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n  \n\tif not isinstance(self._config, ContainerConfigDict):\n\t    raise KeyError(\"Config must be a container\")\n\t\n\t# Check if we have object validation\n\tvalidated_objects = dict((i.name, i) for i in self.get_validated_objects())\n\t\n\tfor config_obj in validated_objects.values():\n\t    validated_config_root = validated_config_obj.configuration\n\t    \n\t    # Do validation of validation objects\n\t    if self.get_validated_objects().issuperset(validated_objects):\n\t\tvalidated_objects.pop(validated_objects.index((self._"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n        print('Start validation for %s objects...' % (len(self.getObjects())))\n        print(self.getHierarchy())\n        if validate_objects:\n\t    if check_digests:\n\t\tdigest_ok = True\n\t\tfor oid in self.getObjects():\n\t\t    if not validate_oid.has_key(oid):\n\t\t        digest_ok = False\n\t\tif not validate_objects and not show_warnings:\n\t\t    print(\"Validating digests...\")\n\t\ndef validate_hierarchy_obj(self, parent, oid, depth):\n\t\"\"\"Checks to see if a specified parent is valid"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\t\n\twarnings = OrderedDict()\n\tmessages = OrderedDict()\n\t\n\tdef get_container(container):\n\t\tassert hasattr(container, 'oid_id'), \"oid table expected\"\n\t\t# TODO: check the oid_id and isntree valid?\n\t\tobj = self.context.get_container(container)\n\t\tif not obj.oid_id:\n\t\t\twarnings['oid_id'] = self.get_oid_id(self.context)\n\t\t\treturn None\n\t\telse:\n\t\t\treturn obj\n\t\n\tdef is_valid(self):\n\t\t\"\"\"\n\t\tChecks storage roots"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    ## FIXME: Make this general!\n\t    ## FIXME: Use self.logger\n\t    self.logger.debug(\"Validate the database hierarchy: %s\" % pprint.pformat(self.storage_tree))\n\t    ## FIXME: The below code is pretty handy. What it does is, if the caller wants to\n\t    ## validate the hierarchy of the objects. Ideally, no need to care about the tree,\n\t    ## but I'm going to complain of this, if any of the objects are missing or invalid.\n\t    ##\n\t    ## We have to check all objects in the database for children of the tree\n\t    ## to"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t# print(\"Validating storage root tree: \" + str(len(self.storage_tree)))\n\t# print(len(self.storage_tree))\n\t\n\t# Check if object IDs given is an integer\n\tvalid = True \n\t\n\tfor i in xrange(len(self.storage_tree)):\n\t    id = self.storage_tree[i]['id']\n\t    if id > 0: \n\t        if id >= self.obj_cnt:\n\t        \t   continue\n\t    \n    \t        # Skip objects that don't have the correct parent. We can skip these\n\t        if not check"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    self.num_objects = 0\n\t    self.good_objects = 0\n\t    self.errors = {}  # keys: type, errors\n\n\nclass ValidateObjectInfo(GenerateIssuer):\n    \"\"\"\n\tValidate object/container info.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\n\tdef __init__(self, obj, container):\n\tGenerateIssuer.__init__(self, obj)\n\tself.container = container\n\n    def generate(self, generate_certs_skips="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    \n\t    \n\t\t# find objects recursively to check for duplicates\t\t\n\t\tself.object_tree.ensure_sanity()\n\t\t \n\t\t# validate objects only\n\t\tself.validate_identities()\n                self.validate_digests()\n\t\tself.validate_parents(check_digests)\n\t\tobj_hashes = set([(parent.object_hash, parent) for parent in self.parents])\n                   \n\t\tself.num_objects_checked_correctly = 0\n\t\t\t\t\n\t\t# get all sub-objects to check for possible errors\n\t\tself.bad_checksums = set()"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    # validate object names and digests\n\t  self._validate_object_names(validate_objects)\n            self._validate_digests(validate_objects, check_digests, show_warnings)\n\t    self._show_objects(validate_objects)\n\t    self._show_digests(validate_objects)\n\t    self._show_stats(validate_objects)\n\t    if self._num_objects or self._good_objects:\n\t      self._result = True\n\t  else:\n\t      # no valid objects\n\t      self._result = False\n\telse:\n\t    self._result = False\n\n\treturn self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n\t    if validate_objects:\n\t\tif check_digests:\n\t\t    if not self._verify_all(validate_objects):\n\t\t\treturn None\n\t\telse:\n\t\t    if not self._verify_single_digest(validate_objects):\n\t\t\treturn None\n\t\t    if not self._verify_only_one_digest(validate_objects, True):\n\t\t\treturn None\n\t    if show_warnings:\n\t\treturn self.verify_warnings(validate_objects, check_digests)\n\n\tdef get_data(self, obj):\n\t    \"\"\"\n\t    Returns the json-encoded data in an object.\n\t"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\tocfPath = None\n\tif self.ocfPath == None:\n\t\traise Exception(\"Must create storage root before initialize.\")\n\tlog = logging.getLogger(__name__)\n\t\n\tif self.cFile:\n\t\tlog.warning(\"Setting up storage path %s...\", self.cFile)\n\t\tocfPath = self.cFile\n\t\n\tif self.createFolder:\n\t\t# create and initialize the root directory\n\t\tif ocfPath!= None:\n\t\t\tmkdir(ocfPath, True)\n\t\t\ttry:\n\t\t\t\tos.makedirs(ocfPath, True)"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tself[\"c_type\"]           = C_NONE\n\t\tself[\"c_title\"]           = u_(\"OFI Controller\")\n\t\tself[\"gui_name\"]         = u_(\"OFI\")\n\t\tself[\"menu_name\"]        = u_(\"OFI Server Manager\")\n\t\tself[\"short_description\"] = _(\"Controller configuration (OFI)\")\n\t\tself[\"author\"]             = \"Ofir\"\n\t\tself[\"authors\"]          = [\"Jack\", \"Rapha\u00ebl Vinot\"]\n\n\tdef execute_cmd(self, cmd):\n\t\tif cmd is None:"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n#       open root dialog window\n#\n#     global wTree\n#     global wOptions\n#     global wWindow \n#\n#      # open root of the dialog window\n#      wOptions = wTree.create_resource(\"OptionDialog\")\n#      wWindow  = wTree.get_widget('OptionDialog')\n#\n#      # init\n#      self.wTree = wTree\n#      self.wOptions = wOptions\n#      self.wWindow = wWindow \n#\n#     # key bindings\n#      self.wTree.get"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\nif (len(sys.argv) < 2):\n\tself.help()\n\tsys.exit(1)\n\nfs = os.open(sys.argv[1], os.O_RDONLY)\n\n# Open our OCFL file in read mode\n\no = os.fdopen(fs, 'r+b')\n\n# Set up two blank segments, to read 0-4 sectors\no.read(46)\n\n# Write our file out as it is being read by at the time the size field\n# will be determined by the size of the input file"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\nclass TTFAxOS:\n\t\"\"\"\n\tA 'generic TTFAxOS' object.\n\t\n\tThis object implements functions that are used by different backends\n\tin TTFM functions.\n\t\"\"\"\n\tdef open(self, **kwargs):\n\t\t\"\"\"\n\tOpen the TTFM class.\n\t\n\tThe class is derived from pyTtfax.Ttfa_common.Ttfa_core, but it has\n\tbeen modified to use it independently.\n\t\"\"\"\n\t\t\n\t\tself.TtFont = new.instancemethod(new.instancemethod("}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\t# Start the root-level FUSE filesystem server.\n\t# This is a bit of a mess.\n\tself.__server = Server()\n\t\n\t# Start the OCFL directory server.\n\t# This can be done multiple times\n\tself.__directory = ServerDirectory()\n\t\n\t# Start the OS-dependent FUSE filesystem server.\n\t# This is a bit of a mess.\n\tself.__fs = ServerFS()\n\t\n\t# Start a OCFL server instance.\n\t# This is the OCFL server instance\n\tself.__server.register(self, 'local"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tif not self.create_new_root():\n\t\t\treturn\n\n\n\tdef create_new_root(self):\n\t\t\"\"\"\n\t\tCreates a new OCFL storage root without initializing its contents with the contents of the current directory.\n\t\t\"\"\"\n\t\tif not self.create_new_root():\n\t\t\tself.root = self.make_root()\n\t\t\tself.create_home_dir()\n\t\telse:\n\t\t\tself.root = self.make_root()\n\n\tdef create_home_dir(self):\n\t\t\"\"\"\n\t\tCreates a new OCFL storage root with"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\tprint \"* creating node at %s\" % self\n\tif not os.path.exists('/opt/users/openscad/storage/'):\n\t\tos.mkdir('/opt/users/openscad/storage/')\n\tif not os.path.exists('/opt/users/openscad/bin/'):\n\t\tos.mkdir('/opt/users/openscad/bin/')\n\n\tprint \"* initializing storage root...\"\n\tif not os.path.exists(self):\n\t\tos.mkdir(self)\n\n\tprint \"* starting storage...\"\n\tif os.path.exists("}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(self, 'f'):\n\t    logging.info('fs-fofs not initialized!!')\n\n\t    raise RuntimeError('Filesystem must be initialized before running this')\n\n        self.f = open(self.root, 'w', 0)\n\tif not hasattr(self, 'f2'):\n            logging.info('fs-fofs not initialized!!')\n\n            raise RuntimeError('Filesystem must be initialized before running this')\n\n        self.root_entry = self.f.create_group('fs_meta')\n        self.root_entry.attrs[\"name\"] = \"meta\"\n\n        #"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\tself.lock = threading.Lock()\n\tself.clear()\n\t\n\tself.data = dict()\n\tself.currentData = None\n\t\n\t\n\tself.reset()\t\n\n\t# ----------------------------------------------------------------\n\t#\n\tdef reset(self):\n\t\tself.data = dict()\n\t\tself.lock.acquire()\n\t\tself.clear()\n\t\t\n\tdef addData(self, key, val):\n\t\t\"\"\"\n\t\tAdd data entry to storage.\n\t\t\n\t\tArguments:\n\t\tkey\tRequired: The key whose value will be added.\n\t\tval       The value to be added.\n\t\t\n\t"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\n\tdef next(version):\n\t\tif version.find('.') == -1:\n\t\t\traise NotImplementedError('Unable to determine next version'\\\n\t\t\t\t'identifier for \"%s\" in %s' % (version, os.getcwd()))\n\n\t\tlast = version.rfind('.')\n\t\tnum = ''\n\t\tif last > 0: num = '.' + version[last+1:]\n\n\t\treturn (num, version)\n\n\tif 'Twisted/' in path:\n\t\timport pkg_resources\n\t\tresult = pkg_resources.get_distribution('Twisted')\n\t\tif result is None:\n\t\t\t"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\ndef next_release(release):\n\t\"\"\"\n\tGet next released part of package.\n\t\n\tWill raise InvalidVersion if no version matched\n\t\"\"\"\n\t\n\t# release = package + '-' + release\n\t# with self._lock:\n\t# \tself._lock.wait()\n\t\n\treturn release.split('-')[0]\n\ndef next_dev(release):\n\t\"\"\"\n\tGet next development part of package.\n\t\n\tReturns a float of an invalid version string (like '0.0.0', as\n\tlong integer). This assumes the following rules:\n\t\n\t+ If"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\t\n\tmatch = next_version_re.match(version)\n\t\n\tif match:\n\t\tmajor = int(match.group(1))\n\t\tminor = int(match.group(2))\n\t\tnext_version = version[major+1:]\n\t\treturn next_version\n\n\treturn version\n\n\n\n\ndef get_version_data(version):\n\t\"\"\"\n\tReturn (major, minor, next_version) for specified version.\"\"\"\n\tversion = re.search(version_separators[version]).group(1)\n\tnext_version = get_next_version(version)\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\treturn tuple(int(x) if x else 0 for x in version.split('.'))\n\n\ndef check_version(version):\n\t\"\"\"\n\tCheck that ``version`` string begins with a valid\n\tsuffix to avoid issues when the leading 0 is used for minor and\n\tversion suffix matching.\n\t\n\t\"\"\"\n\tsuffix = version[-1].lower()\n\tif suffix!= \"\" and version[0] not in ('', '.'):\n\t\tversion = version[1:]\n\t\n\treturn version\n\n\ndef check_version_order(version_list):\n\t\"\"\"\n\tCheck that ``"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\n\tif not version:\n\t\traise ValueError(\"Invalid version (must declare pattern).\")\n\n\tif len(version) > 10:\n\t\traise ValueError(\"Invalid version (can be a pattern or a number).\")\n\t\t\n\tversions = version.split('.')\n\tif len(versions)!= len(min(versions)):\n\t\traise ValueError(\"Invalid version (max is {})!\".format(len(min(versions))))\n\n\tminor = 0\n\tsuffix = version[1:]\n\n\ti = 0\n\t\n\twhile True:\n\t\ti = (i + 1) % len(version)\n\t\ttry:"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\tif '-' in version or '+' in version:\n\t\treturn version.split('-')[0] + '+' + version.split('+')[0]\n\telse:\n\t\treturn version.split('+')[0]\n\ndef next_suffix(version):\n\t\"\"\"\n\tNext version identifier following suffix.\n\t\n\tMust deal with both zero-prefixed and non-zero suffixed versions.\n\t\"\"\"\n\t\n\tsuffix = \"\"\n\tif '+' in version or '-' in version:\n\t\tsuffix = version.split(\"-\")[1]\n\telse:\n\t\ttry:\n\t\t\tsuffix = version[-1]"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tversion_num = int(version)\n\n\t\tif version_num >= 0: # version number out of bounds\n\t\t\treturn (version - 1)\n\t\telse:\n\t\t\traise ValueError,\t \t\"Can't determine version.\"\n\n\t\t   \n\texcept:\n\t\traise ValueError,\t \t\"\"\t  \t\t  \n\t\n\n\n\n\nif __name__ == \"__main__\":\n\t\n\t# print(next_version(\"0.9.1\"))\n\t# print(next_version(\"0.0.1-beta1\"))\t # \"0.0.1-beta"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\tsplit = version.rsplit('.', 1)\n\t\t\n\t\tif split[0] == '':\n\t\t\traise CMDBException('Malformed format: Version string should not have a number')\n\t\t\n\t\tif len(split) == 1:\n\t\t\treturn int(split[0])\n\t\t\n\t\treturn parse_version(split[0])\n\n\telse:\n\t\traise CMDBException('Malformed version: Wrong package name or version number')\n\ndef read_pkgver(name):\n\t\"\"\"\n\tRead package version using :py:func:`.pkgver` command.\n\t\n\t:arg name: the name of the"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\n\tversions = [version]\n##\t\tprint'ver', version, version_separator\n##\t\tif version_separator == '+':\n##\t\t\t# We have a version number here, but its beginning a number of digits\n##\t\t\t\t\t\tv = _get_next_version_match(version[:-1])\n##\t\t\tversions.append(int(v))\n##\t\t\t# We have to add a \"0\" before the version number for now\n##\t\t\tversions.append(0)\n##\t\t\t# This is just the normal version\n##\t\telif '-' in"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\t# Remove brackets and hyphens from the version.\n\tversion = version.partition(\".\")[0]\n\n\t# Strip forward slashes from the version.\n\tversion = version[version.index(\"://\"):]\n\n\t# Remove forward slashes from the version and replace hyphens with underscores.\n\tversion = version.lower().capitalize() + version[version.index(\"://\"):]\n\tversion = version.replace(\"-\", \"_\")\n\t\n\t# Remove dashes from version and replace hyphens with underscores to make it\n\t# easier to read\n\tversion = re.sub(\"[A-Za-"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tassert(version_dirs[0] == \"1\" or version_dirs[0] == \"2\" or version_dirs[0] == \"3\")\n\t\t# If version_dirs[0] is not in version_sequence, the validation tests will\n\t\t# throw an error\n\t\tif not version_dirs[1].isdigit():\n\t\t\traise VersionInventories.InvalidVersionInventories(version_dirs)\n\t\t# The \"version_dirs[0]\" part should only appear once, so as not to confuse\n\t\t# up the version checks on the right level (0"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\ndef validate_inventory_only_inventories(self, inventory):\n\t\"\"\"\n\tValidate only inventory, if it actually existed before.\n\t\n\tReturns a list of items to remove.\n\t\"\"\"\n\tvalidated = []\n\t\n\tfor file_type in inventory:\n\t\ttry:\n\t\t\tinventory_files_in_directory = os.listdir(directory)\n\t\texcept os.error, (errno, message):\n\t\t\tif errno!=13:\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\treturn []\n\t\t\n\t\tinventory_path = inventory_files_in_directory[0]\n\n\t\tfor inventory"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\t# we should be sure that this is valid and is at least the same as v2, so that\n\t# we can't use the version in the path as we don't want to allow anything else to do\n\t# this with invalidating the content that has a different value.\n\t#\n\tdef validate_all_version_dirs(self):\n\t\t\"\"\"\n\t\t@returns: Nothing\n\t\t@rtype: None\n\t\t\"\"\"\n\t\tversion_dirs = self._gather_directories_in_v3('/var/lib/zstack-woodpecker')\n\t\tversion_dirs"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\t# for internal use only; this is currently the only version we can ever support.\n\t# the current version has the same version structure as versions_inventories.txt\n\tversion_info = self.get_version()\n\tlatest_hash = min(v for v in version_info.versions if v.timestamp > version_info.timestamp)\n\tfor (version_dir, hash_tuple) in version_dirs:\n\t\tif version_dir in version_info.hashes:\n\t\t\tif (hash_tuple == (0, 0, 0) or\n\t\t\t   version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\t\n\tfor depth in depth_list:\n\t\t\n\t\t# check if there is an existing inventory file to validate\n\t\t\n\t\t# get the inventory version\n\t\tversion = ssmc_scan.get_inventory(self, 0, depth, version_dirs)\n\t\tif version is None:\n\t\t\traise FileNotFound(\"Inventory\", depth, 0)\n\t\t\n\t\t# check if it's not already checked in the database\n\t\tcheck_version_inventories(version)\n\t\t\n\t\t# get the content digests\n\t\tcache_filename = self._get_cache_filename(version)\n\t\ttry:\n\t\t\t#"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n#   This module is free software; you can redistribute it and/or modify\n#   it under the terms of the GNU General Public License as published by\n#   the Free Software Foundation; either version 2 of the License, or\n#   (at your option) any later version.\n#\n#   This module is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#   GNU General Public License for more details.\n#\n#   You should have received a"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\t# First, check that any of the root inventory's entries are properly\n\t# installed (all the entry files are installed)\n\tversion_dirs = set(version_dirs)\n\t\n\t# If the same directory name appears multiple times in the inventory,\n\t# then all files and folders have the same hash.\n\tversion_dirs = set(version_dirs)\n\tfiles = {}\n\tfolders = {}\n\t\n\tfor version_dir in version_dirs:\n\t\tfile_str = os.path.join(version_dir, \"version\")\n\t\t\n\t\tif not os.path."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\t\n\tdef validate_version_inventories(self, version_dirs):\n\t\t\"\"\"\n\t\tEach version SHOULD have an inventory up to that point.\t\n\t\t\n\t\tVersion directories are assumed to be in one of the version sequence.\n\t\tIf the version's inventory is not of the version sequence one\n\t\tis silently left for safety reasons.\n\t\t\n\t\tversion_dirs is a list of versions directory names.\n\t\t\"\"\"\n\t\n\t\tfor depth in range( 1, 2 + len(version_dirs) ):\n\t\t\tif len(version_dirs) > 0:\n\t\t\t\tversion_dirs.pop"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tinventory_sets = self.get_all_inventory_sets()\n\t\n\tfor version_dir in version_dirs:\n\t\t# only run check_required_file if there is a copy for a version directory\n\t\tif len(inventory_sets) == 1:\n\t\t\tself.check_required_file(version_dir)\n\t\t\n\t\t# validate the version directory itself...\n\t\tif not os.path.isdir(version_dir):\n\t\t\traise ValueError(\"version directory %s doesn't exist\" % version_dir)\n\t\t\n\t\n\t\t# create the version directory at this point\n\t\tself.__"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tfor version_dir in version_dirs:\n\t\ttry:\n\n\t\t\tversion_name = get_versioned_name(version_dir, self.file_index)\n\t\texcept ValueError:\n\t\t\treturn\n\t\t\n\t\ttry:\n\t\t\t\n\t\t\tinventory_path = create_inventory(inventory_size)\n\t\texcept ValueError as e:\n\t\t\tpass\n\t\t\n\t\ttry:\n\t\t\t\n\t\t\tcontent_digests, content_digest_timestamps = self._get_content_digests(version_dir)\n\t\texcept Exception as e:\n\t\t\tLog.debug(\"Exception getting content digests for '%s': %s\","}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n    if path[:1] == '/.': # / is a root directory\n        return OS_FILETYPE\n    elif path[:1] == '\\\\': # \\\\ is a file\n        return OS_ROOT\n    else:\n        return OCFL_UNKNOWN\n        \ndef resolve_path_info(path):\n    \"\"\"\n\tIf a given path is a directory return a tuple\n\t(directory_id, name, path_info) with a dictionary\n\tcontaining a directory information object representing \n\tdirectory properties.  directory_id may be a string\n\tcontaining any filename components.\n\t\"\"\"\n    if path[:1"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\tif not path: path = ''\n\t\n\tif path.strip() == \"\":\n\t\ttry: # see if it's a CVS directory\n\t\t\tpathtype = win32api.GetUserName()\n\t\t\tdirn = win32api.GetComputerName()\n\t\t\tpath = win32api.GetPathName(dirn)\t\n\t\texcept: return ''\n\t\n\tif pathtype == win32api.GetUserName():\n\t\treturn _(\"Directory\")\n\telif pathtype == win32api.GetComputerName():\n\t\treturn _(\"Computer\")\n\telif pathtype == win32api"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\tif path_type is \"root\":\n\t    return \"object\"\n\telif path_type is \"object\":\n\t    return \"file\"\n\telif path_type is \"file\":\n\t    return \"inventory\"\n\t\t\n\telse:\n\t    return \"other\"\n\n\nclass FileInventory(Inventory):\n\tdef get_source_name(self):\n\t\treturn \"file\"\n\n\tdef get_source_files(self):\n\t\tfor p in self._paths:\n\t\t\tfor c in p.children: \n\t\t\t\tyield c\n\n\tdef get_destination_names(self):\n\t\treturn []\n\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t    if path[:3] == \"root:\":\n\t        return \"root\"\n\t    elif path[:3] == \"object:\":\n\t        return \"object\"\n\t    elif path[:8] == \"file:\":\n\t        return \"file\"\n\t    elif (path[:13] == \"other:\") and (path[:13] == \"file\"):\n\t        return path[13:]\n\treturn \"\"\t\n\ndef get_root_type(path):\n\t\"\"\"\n\tReturn the root of some object.\n\t\n\tReturn a string or None.\n\t\"\"\"\n\tglobal roots\n\tglobal objtype\n\tglobal root"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\tif path.find(\"..\") == -1 and path.find(\"_\") == -1 and path.find(\"..\\\\\") == -1:\n\t        # This probably happens. It's an \"include\" directive.\n\t        return \"OBJECT\"\n\telse:\n\t        return path.split(\":\")[-1]\n\t\n# This will look up each directory of a file's inventory, with names\n# that make up the object types.\n# The data for each entry are of the format (sub, subdata) where\n# sub is a string representing the name with underscores (minus the\n#"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\n\tpath_parts = path.split(\"\\\\\")\n\t\n\t# FIXME: should not check any leading and trailing dot at the start?\n\treturn path_parts[0]\n\n\ndef check_path_types():\n\t\"\"\"\n\tCheck for the OS types of the root group of the FS root.  Checks \n\tthe validity of the root group by traversing the root's object and traversing\n\tthe parent's object (if any).\n\n\tOnly handles a few directories (i.e. the 'files' directory) and case-sensitivity \n\tif the root is a file"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\tif not path:\n\t\treturn 'Object'\n\t\n\tif path.count(\"/\") > 1:\n\t\traise ValueError, 'Expected \"root\" style at position 1 in file path'\n\t\n\treturn path.split(\"/\")[-1]\n\t\t\n\ndef path_is_directory(path):\n\t\"\"\"\n\tReturn ``True``, if the given path is one of the\n\t'obj' and 'dir' elements.\n\t\n\tThis is used in cases where the path is the root of the storage.\n\t\"\"\"\n\t\n\tif not path or path.count(\"/\") == 1:\n\t\treturn True\n\t\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n    if path in ('', '/'):\n\t# this is just an error. Let's not use the 'root' flag - it is the root? The idea is for a real system\n\t\tresult = False\n\t\t\n\tif path[:5] == '. ':\n\t# this is not an error\n\telse:\n\t    path = os.path.abspath(path)\n\tfilename = os.path.basename(path).split('.')[0]\n\tsplit_path = path.split('/')\n\t\n\tif filename[0] == 'root':\n\t    result = os.path.commonprefix((filename[1], filename"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\ndef list2cmdline(filelist, pathtype):\n\t\"\"\"\n\tReturn a list containing a listing of all the paths in filelist\n\t\n\tThe function will return an array of the right length of the following\n\tform::\n\t\n\t      [['..','root','object'], ['root','object','root']]\n\t\n\tThis function will return two arrays, the first being the filenames\n\tand the second the paths to each of the items in the array.  The\n\tlist will be empty if there were no paths in the input list.\n\t\"\"\"\n\n\tcmdline = list()\n\tfor path, type in filelist"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n    # Check for a variable that has path type.\n    if path in PATH_TYPE_MAPPING.keys():\n        return PATH_TYPE_MAPPING[path]\n    else:\n        return None\n\n\n#\n#\n#\nclass ocfl_type:\n\n    def __init__(self, tag, content_type):\n        \"\"\"\n        Return: a tuple ((path_type, error) where:\n\t\t    path_type should be either OCFL_Root or OCFL_Object\n\t\t    error is one of the following: \n\t\t\t\t\t'root': path is a file or directory\n\t\t    any error is False for non"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t    \n\tif self.__use_bz:\n\t    ret = self.__api_call('bookmarks/get', params)\n\t    return ret\n\n\tif 'bug_ids' in params:\n\t    bug_ids = [str(id) for id in params['bug_ids']]\n\t    # FIXME: Rewrite this code to use a list for this bug's bug_ids.\n     \tbugs_by_hash = {}\n\t    if type(bug_ids) is type([]):\n\t\tbug_ids = str(bug_ids)\n\t\tbug_ids_hash = []\n\t\ttry:\n\t\t    # First we"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\n\tfor bug_id in bug_ids:\n\t\tif self.session < 0:\n\t\t\tself.getStatus(bug_id, 'A')\n\t\tbug_ids[bug_id]['A'], status = self.request_bug(params)\n\n\tif status!= -1:\n\t\t#print 'Bugs not available'\n\t\treturn status\n\n\t#print_debug(bug_ids)\n\n\treturn status\n\n#def amend_openform(self, params, bug_ids, openform_ids):\n\t\"\"\"\n\tAmend the Bugzilla open form"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\nclass AmendUrl(Amend):\n\t\"\"\"\n\tAmend a bugzilla url\n\t\"\"\"\n\n\tname = \"Bugzilla url\"\n\tap = True\n\thas_output = True\n\t\n\tdef get_icon_name(self):\n\t\treturn \"https://bugzilla.mozilla.org/enter_bug.cgi?product=Addons&tab=tab_products\"\n\t\n\tdef get_description(self):\n\t\treturn \"Addons that have been added to the Launchpad Bugzilla URL\"\n\n\tdef get_long_description(self):\n\t\treturn 'This bug"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\n\tcmd = \"/usr/sbin/bzmerge %s\" %s %s\" %  (params[\"url\"], params[\"name\"], params[\"version\"])\n\t(status, output) = commands.getstatusoutput(cmd)\n\treturn status == 0\n\n\n################################# ACTIONS #################################\n\n\tdef load_all_brs(self):\n\n\n\t\n\n\tdef add_bz_to_list(self):\n\n\n\t\n\n\tdef add_bz_to_dict(self, data):\n\n\n\n\n\t\n\n\n\tdef add_bz_to_list_dict(self, data):\n\n\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\t\n\tbzparams_path = None\n\ttmp_dir = None\n\t\n\tfor dir in params[\"bzparams_dirs\"].split():\n\t\t\n\t\ttmp_dir = path.abspath(join(tmp_dir, dir))\n\t\t\n\t\ttry:\n\t\t\t\n\t\t\ttmp_dir = path.join(tmp_dir, str(params[\"project\"]) )\n\t\t\t\n\t\t\t# Check if the folder exists\n\t\t\tif not path.exists(tmp_dir):\n\t\t\t\t# Create a temp directory to use\n\t\t\t\ttmpdir = path.mkdtemp()\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tbzparams_path = path.join(tmpdir, \""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n        bug_ids = re.split(\",$\", bug_ids)\n        params1 = params.split(\",\")\n\n        if len(bug_ids)!= len(params1):\n            Log.Error(\"Bad number of params. %s!= %s\" % (bug_ids, len(params1)))\n                return\n\n        bug_ids = \" \".join(bug_ids)\n        if len(bug_ids) > 1 and bug_ids not in self.bugzillas:\n            Log.Debug(\"Bad bugids %s\" % bug_ids)\n            return\n\n        new_values = []\n        for i"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t return params.bugzilla_params\n\n \tdef send_email(self, bid, email, note=None, files=[], attachment=[], **kwargs):\n\tfrom utils import *\n\tfrom utils import send_report, send_email_async, get_text_message, send_email_to\n\n\tif not isinstance(bid, int):\n\t\tbid = bid.id\n\n\tif note:\n\t\tnote = str_decode(note)\n\n\temail = get_text_message(uid=email)\n\tif isinstance(email, unicode):\n\t\temail = email.encode(\"UTF"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\n\tbugids = to_list(bug_ids)\n\tself.update_status(\"Checking params %s\" % params)\n\tself.update_bug(params)\n\n\tparams['action']\t= 'amend_add'\n\tself.make_bzparams(params, bugids)\n\n\n\tparams['action']='amend_set'\n\tself.make_bzparams(params, bugids)\n\n\n\tparams['action']\t= 'amend_del'\n\tself.make_bzparams(params, bugids)\n\n\n\n\n\n\nfrom datetime import datetime"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n        new_params = {}\n\tif bug_ids!= None and len(params['bug_ids']) > 0:\n      \tfor bug_id in bug_ids.split(','):\n            params['bug_ids'] = bug_id\n\n        if self.use_api:\n                try:\n                    old_params = api_params\n                except:\n                    print \"api params in use_api are not available, using the old bugzilla params\"\n                else:\n                    new_params = self.make_new_params_internal(params)\n                    self.post_bug_url_with_params(params, bug"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n        url = self.session_url + '/ajax/view/add_bug_params'\n        print json.dumps(params, ensure_ascii=False)\n\t\n        print \"amending bz-params\"\n        try:\n        \tresponse = urllib2.urlopen(url, params).read()\n\n        \tticket_id = json.loads(response)['ticket_id']\n        \tresult = self.session_url + \"/view/get_ticket/%s\" % str(ticket_id)\n        \tprint \"got the ticket_id: %s\" % str(result)\n        \t#print json"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(nodes, collections.Mapping):\n\t\traise RuntimeError, \"Must be a mapping type\"\n\t\n\tnew_node_list = []\n\tdeep_merge_list = []\n\tfor node in nodes:\n\t\tif not isinstance(node, ruamel.yaml.nodes.ScalarNode):\n\t\t\tif deep_merge:\n\t\t\t\tnew_node_list.extend(deep_merge_nodes([node]))\n\t\t\telse:\n\t\t\t\tnew_node_list.extend(deep_merge_node([node]))\n\t\telse:\n\t\t\tif node.value not in deep"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\nclass ConfigurationNode:\n\tdef __init__(self, name, parent, flat_map, tags, *value_pairs):\n\t\tself.name = name\n\t\tself.parent = parent\n\t\tself.flat_map = flat_map\n\t\tself.map_tag = tags\n\t\tself.value_pairs = value_pairs\n\t\tself._name = None\n\t\tself.is_deep_mergeable = False\n\t\t\n\t\t# We could just set this to false because the flat map only contains a single child section.\n\t\t# But in the meantime, we want the flat map to"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tout = []\n\tfor node in nodes:\n\t\tif node.tag in node_type_remap.get(node.tag, []):\n\t\t\t# recursively merge the nodes of node to the right (preserving case, if the node already has a tag for the tag in it),\n\t\t\t# otherwise, split the node and perform the corresponding tags in that node\n\t\t\tdeep_merge_node(node)\n\t\t\tout.extend(node.get_content().split('\\n'))\n\t\telse:\n\t\t\t# if the node has no tag metadata - combine the current structure back and add the relevant"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\nclass MergeConfigLines(object):\n\t\"\"\"\n\tThis class is to handle merging of multiple line delimited configuration files as parsed, and to\n\tparse as appropriate for a \"bare\" configuration file.\n\t\"\"\"\n\tdef __init__(self, lines):\n\t\t\"\"\"\n\t\tConstructor\n\n\t\tArgts:\n\t\t    lines (list):  A list of borgmatic configuration records as returned by a \n\t\t      call to merge(lines).\n\n\t\t\"\"\"\n\t\tself.lines = lines or []\n\t\t\n\tdef deep_merge_nodes(self, nodes):\n\t\t\"\"\"\n\t\tGiven a nested borg"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\t    # Check for key mismatch\n\t    \n\t    deep_dict = {}\n\t    try:\n\t        deep_dict['yaml'] = merge_yaml_lists(yaml.load(data))\n\t\tdeep_dict.update(deep_dict['yaml'])\n\t    except:\n\t        deep_dict['yaml'] = data\n\t    \n\t    merged_dict = combine_dict(deep_dict)\n\t    nodes = deep_merge_nodes(nodes)\n\treturn nodes\n\n#: Mapping of yaml data types to the corresponding data types to use\n#: to construct from a non-top-level node.\nnode_data"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tresult = dict()\n\n\tfor path, value in nodes:\n\t\tresult[path] = deep_merge_nodes(value)\n\t\n\t\n\treturn result\n                \ndef deep_merge_dicts(dicts):\n\t\"\"\"\n\tTake an arbitrary number of borgmatic configurations and merge them into a new\n\tdictionary.\n\t\n\tReturns a new dictionary that merges the specified borgmatic parameters or\n\tstructures.\n\t\n\t\"\"\"\n\tresult = {}\n\t\t\t\t\n\tfor path, value in dicts:\n\t\tresult[path] = deep_merge_nodes(value)\n\t\t\n\treturn result\n\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\n\tmerged = OrderedDict()\n\twhile True:\n\n\t\tkey = None\n\t\tvalue = None\n\t\tmerge = None\n\t\tif node_list:\n\t\t\tkey = node_list.pop(0)\n\t\telif data_list:\n\t\t\tkey = None\n\t\t\tvalue = data_list.pop(0)\n\n\t\tkeys = key\n\t\tfor k in keys:\n\t\t\tif k in nodes:\t\t        merged[k] = deep_merge_nodes(nodes[k])\n\t\t\telse:\n\t\t\t\tmerged[k] = values[0]\n\n\t\tif key is not None:"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\n\tdef recursively_merge_sub_dictionaries(sub_dictionaries):\n\t\t\"\"\"\n\t\tRecursively create dictionaries containing sub-dictionaries from a given dictionaries; recursively build\n        recursively and assign any keys as sub-dictionaries of that dictionary to its corresponding sub-dictionary.\n        \n        Dictionaries in which the keys of the first dictionary are sub-dictionaries and whose values are instances of those 2nd-indexed\n        tuples. These dictionaries may have \"new/removed\" entries in the dictionary if they had been added or removed in the process.\n        \"\"\"\n\t\n\n\tresult_dictionary = {}\n\t\n\n\tfor key"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n###########\n#  PRIVATE FUNCTIONS\n#\n###########\n\ndef parse_arguments():\n\tparser = ArgumentParser()\n\tparser.add_argument(\n\t\t\"-l\",\n\t\tdest=\"local_config_file_path\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"A configuration file to load locally (default: config.yaml)\")\n\tparser.add_argument(\n\t\t\"-s\",\n\t\tdest=\"settings_file_path\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"A configuration file to load settings from"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\n\tdef __init__(self, node):\n\t\tself.node = node\n\t\tself.yaml_tag = node.tag\n\t\tself.yaml_value = node.value\n\n\n\tdef update(self, node):\n\t\t# the key (node) data pairs to be merged together for this node\n\t\t# (if the config uses key:yaml.org, assume we're overwriting it)\n\t\tkey = get_key(node)\n\n\t\tif key in set([\n\t\t\t('tag:yaml.org', self.tag)\n\t\t]):\n\t\t\treturn\n\n\t\t# If no mapping value"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog='mlsudiff')\n\tparser.add_argument('-n', '--name', action='store', default=None, dest='name',\n\t\t\t\t\t\thelp='the name of this file type')\n\tparser.add_argument('-o', '--overwrite', action='store_true', default=False, dest='overwrite',\n\t\t\t\t\t\thelp='overwrites the output file if it exists')\n\tparser.add_argument('--overwrite-out', action='store_true', default=False,\n\t\t\t\t\t\tdest='overwrite_out', help='overwrites the output file"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog=command,description=description,add_help=False,formatter_class=CustomHelpFormatter, add_read_only=False)\n\tparser.add_argument('-v','--version', action='version', version='%(prog)s 1.0', help='Output Cheetah 2.0 -- this is a version'+ version_string)\n\tparser.add_argument('-c','--command', action='store',choices=['read','write','read-write','create','execute','delete','rm'], default='read', help='Output Cheetah"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = ArgumentParser(prog='cobra.cli.action_cobra_client', usage='%(prog)s [options]',\n\t\t\t\t\t\t epilog='Run an action on the cobra server')\n\t\n\t# A group of options for our CLI subcommand\n\tparser.add_argument('-l', '--lang', help='Specify the language to use for this cobra action. May be specified '\n\t\t\t\t\t\t 'as an option or if -f is used.', metavar='LANG')\n\tparser.add_argument('-s', '--source', help='Specify the source name you want to send to the cobra server. May be"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog='parse', description='Execute a script, or a file containing the code in an arbitrary way.')\n\t\n\tparser.add_argument('-f', '--files', dest='files', required=True, help='File or URLs of.py script to parse, or a single URL containing URLs.')\n\t\n\tparser.add_argument('-p', '--pretty', dest='pretty', action=\"store_true\", default=False, help='Format the Python code with a minimal set of useful syntaxes')\n\n\treturn parser.parse_args(arguments)\n\"\"\"SCons.Tool"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=__doc__, description_text=__doc__, epilog=examples, formatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument('infile', type=str, help='The file containing the dataset.')\n\tparser.add_argument('outfile', type=str, help='The file to write the final dataset to. If -o not specified input is written to stdout. NOTE: output is not automatically generated.')\n\tparser.add_argument('-e','--engine', type=str, help='The processing engine to use. Must have been called first."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\tparser = ArgumentParser(prog=\"pyqasm_qasm\", description=\"Process QASM\") # Parse with optional arguments\n\t\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", help=\"Output directory\", default=\".\", required=True)\n\t\n\t# parse the arguments\n\targuments = parser.parse_args()\n\t\n\t# parse the output file, i.e. \n\t#    \"c:\\myfiles\\my_file_1.c\"\n\t#    \"c:\\myfiles\\my_file_2.c\"\n\t# for i in range("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ndef main(argv=sys.argv):\n\t\"\"\"\n\tCommand-line argument parsing, parsing, and validation \n\t\"\"\"\n\t\n\ttry:\n\t\targs = docopt(__doc__, version=__version__)\n\texcept DocoptExit as error:\n\t\tparser.exit(2, '{0}'.format(error))\n\n\tif not args['--help']:\n\t\tparser.print_help()\n\t\tsys.exit(-1)\n\n\t# Parse the arguments\n\tif 'debug' in args:\n\t\tg.set_debug(args['--debug'])\n\telif args['verbose"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\targs = []\t\n\n\tfor argument in map(lambda arg: argument.strip(), arguments):\n\t\targ = arg.strip()\n\t\tif arg in (\"--help\", \"-h\"):\t\n\t\t\tusage()\n\t\t\tsys.exit()\n\n\t\telif arg in (\"--verbose\", \"-v\"):\t\n\t\t\tverbose = True\n\t\t\targv.append(arg)\n\t\t\tif arg == \"-v\":\t\tverbose = False\n\t\n\t\telse:\n\t\t\tif arg[0] == \"-\":\n\t\t\t\targ = arg[1:]\n\t\t\t\n\t\t\tif arg == \"-h\":\n\t\t\t\tusage()\n\t\t\t\tsys"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--server-url', type=str, dest='serverurl', help='URL of the server that we are connecting to')\n\tparser.add_argument('--server-username', type=str, dest='username', help='Username from your Kerberos account')\n\tparser.add_argument('--debug', type=bool, default=False, dest='debug', action='store_true')\n\tparser.add_argument('--print-output', type=bool, default=False, dest='printoutput')\n\tparser."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n        parser = ArgumentParser(description=\"The macos console interface\")\n\n        # Add the common arguments\n        common_arguments(parser)\n\n        # Parse command-line arguments\n        parsed_args = parser.parse_args(*arguments)\n\n        # Return the parsed arguments\n        return parsed_args\n\ndef main():\n\t\"\"\"The main task to invoke. Runs the command-line arguments and parses the arguments.\n\t\"\"\"\n        main()\n\nif __name__ == \"__main__\":\n        main()\n\n# Copyright 2006-2014 Gentoo Foundation\n# Distributed under the terms of the GNU General Public License v2\n\n__"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tdef is_option(option):\n\t\t\"\"\"\n\t\tAn attribute is flag. The argument is parsed according to\n\t\t:py:func:`parser2rst`. Return True if option is specified in our argument object;\n\t\totherwise return False.\n\t\t\n\t\t@param option: The name of the option requested.\n\t\t@type option: str\n\t\t\n\t\t@rtype: bool\n\t\t@returns: ``True`` if a given option is present in our argument list, \n\t\t\t``False`` otherwise.\n\t\t\"\"\"\n\t\timport re\n\t\tmatch = RE_OPTION.match(option)\n\t\t\n\t\tif not"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tparser.add_argument(\n\t\t\"options_json\", nargs=\"+\", metavar=\"<options_json>\", help=\"JSON file containing options to parse. Must be specified as JSON.\")\n\t\n\tparser.add_argument(\n\t\t\"options_file\", nargs=\"+\", metavar=\"<options_file>\", default=os.environ.get('IN_MEMORY_FILE'), help=\"JSON file containing options to read from.\")\n\t\n\treturn parserfrom __future__ import absolute_import\n\nimport six\nimport sys\n\nfrom pip._internal.req.constructors import (\n    install_req_from_line"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n    try:\n        flags = list(parser.parse_args())\n    except Exception:\n        flags = DEFAULT_FLAGS\n    \n    if not flags:\n        flags.append(\"-h\")\n\n    return flags\n\n\n\ndef test_parse_args(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, assert that each argument in the parser\n    matches the flags.  This is a helper function: ``flag_count()``, ``flag_list()``.\n    This function will:\n\n    * If ``flags`` is empty, then we expect that ``flags`` does not appear as\n    flags; and\n    * Otherwise, an error will be raised."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\t# Get the argument flags for arguments that appear as positional args\n\tflagArgs = []\n\tflagArgs = []\n\tfor name in dir(parser):\n\t\tif not name.startswith('_'):\n\t\t\tflagArgs.append( name[1:].upper() )\n\n\t# Now get the argument names for the flags that specify the flag arguments\n\tfor flag in parser.option_list:\n\t\tname1 = flag[0]\n\n\t\t# If the flag name contains a space, then we care for that first\n\t\tif name1[0] == ',':\n\t\t\tflagName = name1"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\t#\n\t# Get the argument name\n\t#\n\targs, names = parser.parse_known_args()\n\n\t#\n\t# Get the parsed flags\n\t#\n\tflags = []\n\n\t#\n\t# Get flags with sub arguments\n\t#\n\tfor arg in names[:]:\n\t\tfor optname in args:\n\t\t\tfor s, argvalue in argvalue:\n\t\t\t\tflags.append((optname, shlex.split(s)), argvalue.pop(0))\n\n\t\tif len(flags) >= 1:\n\t\t\tflags.sort()\n\t\telse:\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\tflags = ['i', 'c', 'p', 'r', 't', 'v', 'f', 'w', \n\t\t'z', 'j', 'k', 'n', 'p', 'r','s', 'u', 't', 'v', 'w',\n\t\t'x', 'z', 't', 'w', 'x', 'c', 'x',\n\t\t'h', 'r', 'w', 'k', 'n', 'p', 'k', 'r','s', 'u','s', 't',\n\t\t'r"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\timport argparse\n\t# flags for argument parsing\n\tflags = []\n\n\tif ARGS.debug:\n\t\tflags.append(argparse.SUPPRESS)\n\n\t# the default format string for arguments\n\tformatter_class = argparse.ArgumentDefaultsHelpFormatter\n\tfor f in ARGS.options.strip().split(','):\n\t\tif f.startswith('--'):\n\t\t\tv = parser.get_default_values_for_help(f[2:])\n\t\t\t# print 'Argument {} = {} for {} ({})',f,v[0],v[1],f[2]\n\t\t\tflags.append"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tfor item in parser.option_list:\n\t\titem['nargs'] = parser.action.nargs\n\t\titem['choices'] = parser.action.choices\n\t\titem['help'] = parser.action.__doc__\n\t\titem['metavar'] = ''\n\nclass CmdBootstrapCLI(object):\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tUse the argparse library to bootstraps a CommandGroup to the CommandRunner. This is primarily used to work\n\t\tas a command-line.\n\t\t\"\"\"\n\t\tself.parser = None\n\t\tself.parser_flags ="}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\tdef has_arg(name):\n\t\tfor arg, val in vars(parser.parse_args).items():\n\t\t\tif arg.startswith(name):\n\t\t\t\treturn val[0]\n\t\treturn 0\n\n\tfor a in parser.option_list:\n\t\tif has_arg(a.dest):\n\t\t\tparser.parse_args.remove(a.dest)\n\t\t\tdel vars(parser.parse_args[0])\n\t\telif a.dest in parser.parse_args:\n\t\t\tparser.parse_args.remove(a.dest)\n\t\t\tif has_arg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tclass Flags:\n\t\tALLOW_EMPTY             = 1\n\t\tALLOW_ZERO             = 2\n\t\tALLOW_ONE_OF           = 4\n\t\tALLOW_ONE                = 8\n\t\tALLOW_ALL_TO_ONE        = 16\n\t\tALLOW_ONE_OF_MIDDLE   = 32\n\t\tALLOW_OPTIONAL          = 64\n\t\tALLOW_NULL              = 256\n\t\tALLOW_NESTED             = 512\n\t\tALLOW_OPTIONAL_MIDDLE     = 1024\n\t\tALLOW_NULL_MIDDLE     = 2048\n\t\tALLOW_OPTIONAL_OPTIONAL_MIDDLE = 4096\n\t\tAUTO_NOT"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n    parser = argparse.ArgumentParser()\n\n    parser.description = 'Performs execution of the pipeline defined in the module/command line!'\n\n\n    parser.add_argument('--version', action='version',version=version)\n    \n    parser.add_argument('-f', '--file', nargs='+', help='Input parameter file', required=True)\n    parser.add_argument('-c', '--config', nargs='+', help='Input parameter config file', required=True)\n\n    parser.add_argument('--debug', action='store_true', default=False)\n\n    parser.add_argument('--data-root', required=True"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=\"Run this script after setting the flags.\")\n\t\n\trequired_options = parser.add_argument_group('required sub arguments for this script')\n\trequired_options.add_argument('-H', '--host', default='localhost',\n\t\thelp='specify a host to access the local machine to the local machine')\n\trequired_options.add_argument('-v', '--verilog', action='version',\n\t\tversion='%(prog)s %(versionno)s')\n\n\toutput_group = parser.add_argument_group('output options')\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\treturn {\n\t\t'subparser' : getattr(argparse, arg).__name__,\n\t\t'parser' : getattr(argparse.ArgumentParser, arg).__class__.__name__\n\t\t}\n\n\ndef parse_args():\n\t\"\"\"\n\tParse the command-line arguments \n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse command-line arguments.')\n\n\tsubparser_name = 'parser'\n\tsubparser_arg = 'command'\n\t\n\t\n\tparser.add_argument('-v', '--version', action=version, version=\"%(prog)s version \" + __version__)\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=('Generate a set of command-line arguments and pass them in to marionette executable. You can set various parameters to use the run() function, which uses the Marionette executable.'\n\t\t                                       'To run a Marionette executable (e.g. com.marionette.MarionetteShell) you must also pass arguments for a -m argument and set the path to an alternate path. You can ignore the arguments you pass in with the options in the settings file!'))\n\tparser.add_argument('-m', '--mode"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif os.path.exists(args.arguments):\n\t\t\n\t\t\tconfig = ConfigParser.SafeConfigParser()\n\t\t\tconfig.read(args.arguments)\n\t\t\t# Now walk the entire directory and all sub-parsers\n\t\t\tfor d in config.sections():\n\t\t\t\tfor name, obj in d.items():\n\t\t\t\t\t# For all other names, we're just looking for the top level parser\n\t\t\t\t\tif inspect.isclass(obj):\n\t\t\t\t\t\tparsername = os.path.splitext(d.values()[0])[0]\n\t\t\t\t\t\tcommands.append( (parsername, obj) )"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Mortem shell wrapper.')\n\t\tparser.add_argument('cmd', help=argparse.SUPPRESS)\n\t\tparser.add_argument('--arg', '-arg', help='The arguments to pass along with this subparser', nargs='+', required=True)\n\t\tparser.add_argument('--return', '-return', help='If no argument is passed then pass the remainder through to the parser',action='store_true')\n\t\tsubparser = parser.add_subparsers(dest='command', help='sub-command help')\n\n\t\tparser_call = subparsers"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t # Argument names\n        cmd_args = arg_parser.parse_args()\n        \n        # Command line options\n        for name in cmd_args:\n            if name in unparsed_arguments[name]:\n                setattr(namespace, name, unparsed_arguments[name][name][0](cmd_args[name]))\n\n    except ArgumentError as error:\n        print(getattr(namespace, error))\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# -*- coding: utf-8 -*-\n# Copyright (C) 2016-Today GRAP (http://www.grap.co"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n    p = argparse.ArgumentParser(description=\"parse_arguments\")\n\tp.add_argument(\"-a\", \"--all\", action=\"store_true\", dest=\"all\", help=\"print all command line arguments\")\n\tp.add_argument(\"-c\", \"--command\", action=\"store_true\", dest=\"command\", help=\"print help for specific command\")\n\tp.add_argument(\"-c\", \"--config\", action=\"store_true\", default=False, dest=\"config\", help=\"print all command line arguments with specific config\")\n\tp.add_argument(\"args\", type=argparse.Namespace"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('config', metavar=\"config\", help=\"Command-line configuration parameters.\")\n\tparser.add_argument('-v', '--version', action=ToolVersion, nargs='?', const='1', help=\"Print this tool version number.\")\n\tparser.add_argument('-d', '--debug', action=\"store_true\", help=\"Enable debug output.\")\n\tparser.add_argument('-v2', '--verbose', action=\"store_true\", help=\"Enable verbose output.\")\n\tparser.add_argument('--version_v"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tif not os.path.exists(args.path):\n\t\tprint('File {} doesn\\'t exist ({}) - Exiting!'.format(args.path, args.format()))\n\t\tsys.exit()\n\t\n\t\n\tparser = argparse.ArgumentParser()\n\t\n\tparser.add_argument(\n\t\t'-a', '--address', \n\t\taction='store',\n\t\tdest='address'\n\t)\n\tparser.add_argument(\n\t\t'-c', '--command', \n\t\tnargs=1,\n\t\tdest='command'\n\t)\n\tparser.add_argument(\n\t\t"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\t__subparsers = subparsers\n\n\tdef _parse_subparser_actions(self):\n\t\tcommand_parser = self._get_command_parser()\n\t\taction_parser = self._get_action_parser()\n\t\t# TODO: check if this is safe.\n\t\t# action_args = action_parser.parse_args(namespace=self.namespace)\n\t\t# get_arg_tuples = action_parser.parse_known_args(args=action_args, namespace=self.namespace,\n\t\t#\thelp=False, metavar=self.metavar)\n\t\t#"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tdef get_subparser_list(action, parents):\n\t\timport argparse\n\t\timport os\n\t\t\n\t\targs = dict()\n\t\t\n\t\tif isinstance(action, type) and issubclass(action, argparse._SubParsersAction):\n\t\t\tparent = parents if parents else actions.get(action.__class__)\n\t\t\targs = parse_subparser_arguments([parent], parents)\n\t\t\n\t\treturn (args, args)\n# Copyright 2014 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tparsed = OrderedDict()\n\tfor arg in subparsers:\n\t\tparsed_arg = parsed[arg]\n\t\tif parsed_arg[\"action\"] == \"list\":\n\t\t\tparsed_arg[\"is_list_subparser\"] = True\n\t\t\tparsed_arg[\"choices\"] = \\\n\t\t\t\tsorted(\n\t\t\t\t\tself.parse_list_subparser_choices(\n\t\t\t\t\t\tparsed_arg[\"arg_list\"].pop(\"namespace\", [])\n\t\t\t\t\t),\n\t\t\t\t\treverse=True\n\t\t\t\t)\n\t\telse:\n\t\t\tparsed_arg[\"is_list_subparser\"] = False\n\t\t\n\t\targuments = parsed"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\nclass ArgumentParser(argparse.ArgumentParser):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(ArgumentParser, self).__init__(*args, **kwargs)\n\t\tself.__parser = None\n\t\t\n\tdef add_argument(self, *args, **kwargs):\n\t\ttry:\n\t\t\tcmd = super(ArgumentParser, self).add_argument(*args, **kwargs)\n\t\t\tcmd.add_argument(*args, **kwargs)\n\t\texcept argparse.ArgumentError as e:\n\t\t\tlogging.fatal(\"Error parsing arguments: %s. Arguments: %s\" % (e, args))"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\t# Default values\n\tparser_defaults = {\n\t\t\"branch\": None,\n\t\t\"commit\": None,\n\t\t\"user\": None,\n\t\t\"dateadded\": None,\n\t\t\"depth\": None,\n\t\t\"verbose\": None,\n\t\t\"ignore-externals\": None,\n\t\t\"include\": None,\n\t\t\"ignore-conflicts\": None,\n\t\t\"ignore-unresolved\": None,\n\t\t\"force-with-obsoletes\": None,\n\t\t\"ignore-use-commit-date\": 'False',\n\t\t\"ignore-obsolete\": None,\n\t\t\""}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n    \tparser = subparsers.add_parser(\"parse\", help=argparse.SUPPRESS)\n\n        subparsers.required = True\n   \n \tparser.add_argument(\"-s\", \"--subparsers\", nargs=\"+\", metavar=\"<subcommand>...\", type=parse_required, required=True)\n\n\tsubparsers = subparsers.add_parser(\"subparsers\", help=argparse.SUPPRESS, description=\"valid subcommands\")\n    subparsers.required = False\n    subparsers.add_argument(\"-h\", \"--help\", dest=\"help\", action=\"help\", help=\"show this help message and exit\")\n    subparsers.add_argument(\"-v\","}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tdef to_argparse_subparser(subparser, parents):\n\t\targs = argparse.Namespace()\n\t\targs.repo = None\n\t\targs.location = subparsers\n\t\topts = subparsers.add_parser(subparser.name, parents=parents)\n\t\treturn options(args), args.action, args\n\t\n\tparser = subparsers.add_parser('repository', parents=['repo', 'location', 'type'])\n\tparser.set_defaults(func=parse_repository)\n\t\n\taliasParser = commands.aliasParser()\n\taliasParser.add_arguments(parser, subparser."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\ndef parse_args():\n\t\"\"\"\n\tGet parser.parse_args()\n\tParse command line arguments.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\t\t\tprog='gargar_config', conflict_handler='resolve',\n\t\t\t\t\t\t\tadd_help=False)\n\tparser.add_argument(\n\t\t'repository', \n\t\tnargs='?', \n\t\thelp='Repository name to load for config')\n\tparser.add_argument(\n\t\t'-d', '--debug', \n\t\taction='store_true', \n\t\tdefault=False, \n\t\thelp='Turns on debugging of configuration"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tsubparser_actions = [\n\t\tsubparser_actions_with_defaults,\n\t\tsubparser_actions_with_metadata,\n\t\tsubparser_actions_with_tag,\n\t\tsubparser_actions_without_args,\t\t\n\t]\n\n\tparser_actions = [\n\t\tparser_actions_parse,\n\t\tparser_args,\n\t\tparser_actions_with_namespace,\n\t\tparser_args_and_subparser,\n\t\tparser_args_with_defaults,\n\t\tparser_version,\n\t]\n\n\tdef parse_action(name, action):"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\timport argparse\n\t\n\tparser = argparse.ArgumentParser(prog='subcommands', conflict_handler='resolve')\n\tparser.add_argument(\"-u\", \"--user\", \n\t                    required=True, \n\t                    help=\"User to pass to the subcommand\", \n\t                    dest='username')\n\tparser.add_argument(\"-p\", \"--password\",\n\t                    required=True, \n\t                    help=\"Password to pass to the subcommand\", \n\t                    dest='password')\n\tparser.add_argument(\"-s\", \"--scope\", \n\t                    required=True, \n\t                    help=\"Scope in which the subcommand should be applied. Should"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\nclass Config(object):\n    \"\"\"\n    A single configuration class that stores everything we've seen.\n    \n    Each key in the 'keys' class represents a group key in the configuration file.\n    Each key is a Python list, and each item is a dictionary.\n    \"\"\"\n    \n    def __init__(self, config_dict):\n        \"\"\"\n        @param config_dict -- The dictionary holding the configuration.\n        \"\"\"\n        self.config_dict = config_dict\n        \n        # A map from a file like object to a list of keys that have \n        # that key in the group config file.\n        self.map = {}\n        \n        # A"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef _parse_module_args(args):\n#\t\tGet args from the command line and parse the arguments\n\n#\t\tIf no module_args.list is specified, we assume all modules are\n#\t\tavailable and use the \"all\" module. If we specify a module_args\n#\t\twhich contain a list, we assume all modules are used.\n\t#\t\tIf only one module is available, we use the \"--module-list\"\n\tif len(args.module_args) == 0:\n\t\tmodule_args_list = []\n\telse:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='Builds new models from a given list')\n\tadd_args(parser)\n\t\n\tparser.add_argument('-l', '--link', help='the link of the model')\n\tparser.add_argument('-t', '--type', help='the type of model', choices=(\n\t\t'static', 'congress', 'transmission','slate-plater-decane',\n\t\t'stellate','star-plater-decane','star-plater-trasfer'\n\t), default='congress')"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\tcommands = {}\n\tcommands['start'] = CmdStart()\n\tcommands['stop'] = CmdStop()\n\tcommands['add_client'] = CmdAddClient()\n\tcommands['list'] = CmdList()\n\tcommands['show'] = CmdShow()\n\tcommands['clear'] = CmdClear()\n\tcommands['status'] = CmdStatus()\n\tcommands['join'] = CmdJoin()\n\tcommands['quit'] = CmdQuit()\n\tcommands['remove_client'] = CmdRemoveClient()\n\tcommands['show_all'] = CmdShowAll()\n\tcommands['"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tCreate a parser with arguments.\nparser = argparse.ArgumentParser(description='The main program.', )\n\nparser.add_argument('-v', '--version', action='version', version=str(pkg_resources.get_distribution('pyzabbix').version), help='Show version number.')\nparser.add_argument('--debug', action='store_true')\nparser.add_argument('--host', action='store', help='zabbix host to use.')\n\ndef make_vars():\n\t\"\"\"\n\tGet a dict of the vars we use in our main script.\n\t\"\"\""}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t # make sure the path is not a package directory or an exception\n\t if not os.path.isdir(path):\n\t\t\traise TypeError(path, 'not a package directory')\n\n\t\ttry:\n\t\t\tfrom importlib.machinery import SourceFileLoader\n\t\texcept ImportError:\n\t\t\traise ImportError(path, 'No module named _importlib')\n\n\t\timport importlib_metadata\n\t\tmodname = getattr(importlib_metadata, \"__name__\", \"__main__\")\n\t\tname = modname.rsplit(\".\", 1)[-1]\n\t\tm = sys.modules.get(modname, None)\n\t\tif m is None:\n\t\t\t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t     \n\tdef main():\n\t\targument_parser = argparse.ArgumentParser()         \n\t\targument_parser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"Version 0.1\")         \n\t\targument_parser.add_argument(\"-w\", \"--width\", metavar=\"WIDTH\", type=float, default=0.5)\n\t\targument_parser.add_argument(\"-l\", \"--height\", metavar=\"HEIGHT\", type=float,  nargs=3, default=0.5)\n\t\targument_parser.add_argument(\"-m\", \"--max\", type=int, default="}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\t\n\t# add top-level subparsers\n\tparser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\"-t\", \"--tab\", help=\"create the tabbed files\", action=\"store_true\")\n\tparser.add_argument(\"-v\", \"--variables\", help=\"display variables\", action=\"store_true\")\n\tparser.add_argument(\"-i\", \"--names\", help=\"name of variable to display\", action=\"store_true\")\n\tparser.add_argument(\"-y\", \"--yes\", help=\"print \\\"yes\\\" in terminal\", action=\""}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\t # Build a top-level parser with its subparsers and two additional\n\t\t# arguments: the parser, the subparsers and any additional args\n\t\t# passed to the parser factory\n\tparser = ArgumentParser(\n\t\tusage=\"Simple example for an XSD image file.\")\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='Subcommands', description='Other subcommands')\n\n\tparser.add_argument('infile', type=argparse.FileType('r'))\n\tparser.add_argument('outfile', type=argparse.FileType('w'))\n\tparser.add_argument('-c', '--cols"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\t# Main description\n\tdesc = 'Utility to simplify common and intuitive logic'\n\n\tparser = ArgumentParser(formatter_class=RawTextHelpFormatter)\n\tsubparsers = parser.add_subparsers(title='action', metavar='<action>')\n\tparser.add_argument('-v', '--verbose', action='store_true', help=\"more verbose logging\")\n\n\n\tparser.add_argument('-r', '--redo', metavar='<filename>', default=None,\n\t\t\thelp='Revert changes made to a file, no file path. Default %default')\n\tparser.add_argument('-"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t            )\n\ttry:\n\t\tnodes = deep_merge_dict(nodes)\n\t\n\t\tif not isinstance(nodes, dict):\n\t\t\traise TypeError ('`nodes` option must contain dictionaries')\n\t\tnew_configuration = {}\n\t\trecursive_remove_unreferenced_sections = True\n\t\tfor b in nodes.values():\n\t\t\tkey = str(b)\n\t\t\tif isinstance(key, str):\n\t\t\t\tkey = tuple(key.split(':', 1)) # remove the \":0\"\n\t\t\t\trecursive_remove_unreferenced_sections = False\n\t\t\telif isinstance(key, tuple):\n\t\t\t\tkey"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\treturn _to_deep_dict(_merge_node_and_list(nodes))# -*-python-*-\n# GemRB - Infinity Engine Emulator\n# Copyright (C) 2003 The GemRB Project\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tresult = {}\n\t\n\tif isinstance(nodes, list):\n\t\tresult[tuple(nodes)] = deep_merge_nodes(nodes[1:])\n\telif isinstance(nodes, MappingNode) and deep_merge_nodes(nodes.values())[1] is not None:\n\t\tresult[nodes.tag] = nodes\n\telse:\n\t\traise Exception(\"Unsupported data structure type.\")\n\t\n\treturn result\n\t\n#\n# Utility Functions\n#\n\ndef dict_union(d1, d2):\n\treturn [\n\t\t(dict_union(x, d2[x])"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tdef merge_dictionary_recursively(dictionary):\n\t\t\"\"\"\n\t\tRecursively merges dictionary recursively.\n\t\t\n\t\tThis is based on the algorithm described in the paper:\n\t\n\t\thttp://www.codinghorror.com/blog/2007/12/mergeing-yaml-data-structures/\n\n\t\t@type dictionary: dictionary\n\t\t@param dictionary: The dictionary to merge into the dictionary\n\t\t\n\t\t@rtype: dictionary\n\t\t@return: Merged dictionary with the merged values \n\t\t\"\"\"\n\t\tresult = {}\n\t\tfor key, value in dictionary.items():\n\t\t\t# Merge key into subdictionary"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\timport yaml\n\tyamlstring = yaml.dump(nodes, default_flow_style=False)\n\tprint yamlstring\n\treturn yamlstring\n\n\nclass BorgConfigManagerNode(Node):\n\t\"\"\"\n\tThe BorgConfigManager base class for a config file.\n\n\tAttributes:\n\t    - name (str) -- Name of the BorgConfigManager node.\n    \t\n\t    - settings (dict) -- An object containing the settings to configure this node. This is useful\n\t        when writing configuration files such as `logging.*`, and the config may get\n\t        very large after the data is re"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\t    # check the merged node values and grab the root\n\t    \n\t    nodes_by_tag_mapping = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n\t    for key, value in nodes.items():\n\t    \n\t    # go in reverse on the nodes and values in the list\n\t    for node_key, node_value in nodes.items():\n\t\tnode_value_key, node_value_value = (node_key, node_value)\n\t    \n\t    \n\t    # iterate over children\n\t    \n\t    for child_node_key in node_by_tag_mapping[node_value_"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\twhile nodes:\n\t\tnode = nodes.pop()\n\t\t\n\t\t# First check that the key exists\n\t\tkey_id = id(node)\n\t\tassert key_id not in node['tags'],'missing key \"%s\"' % key_id\n\t\t\n\t\t# Then check values\n\t\tparams = node['tags'][key_id]\n\t\t\n\t\tif 'value' in params and'recursive' in params['recursive'][node['tag']]:\n\t\t\t\n\t\t\t# If no'recursive' key exists with value \"None\"\n\t\t\tvalue = None\n\t\t\t\n\t\t\tif isinstance(params['value'], str):\n\t\t\t\t#"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\t\n\t# make sure we have all the elements we're going to need from this same borgmatic and have a mapping for each tag?\n\t# we're going to need all the mappings for this node that are in the mapping so we dont need to create new \n\t# nodes while keeping the mapping to keep track of which tags we've collected up by looking at the mapping node\n\t# to keep track of which tags mapping\n\tnodes_with_dups_and_keys = defaultdict(set)\n\t\n\tif \"tag"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\treturn _deep_merge_config_sections(\n\t\tnodes, _deep_merge_node_dict, recursive_mapping, recursive_list, _deep_merge_node_list, __BUILT__\"\n\t)\n\ndef _generate_merge_config_sections(nodes, deep_merge_node_list, recursive_mapping, recursive_list, __BUILT__=BUILT__):\n\t\"\"\"\n\tGiven a list of nodes and two lists of recursive configuration sections, try to build\n\ta dictionary representing each recursively configuration section. If the nodes are\n\tone of the"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tdef deep():\n\t\treturn deep_merge_nodes(copy.deepcopy(nodes))\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom mantid._utils import (StrVector, EnumVector, unlistify, null_argument,\n                         minmax, minvalue, isiterable, is_none)\n\n\n__all__ = [\n    'BoolVector', 'CharVector', 'BaseType', 'IntVector', 'LongVector',\n    'BoolArray', 'StringVector', 'BoolVector',\n    'ByteVector', '"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\targuments0 = parser.parse(arguments[1:])\n\targuments   = arguments0._actions\n\tactions       = actions._actions\n\taction_name    = actions._actions.pop('default')\n\t# print(\"them args are: %s\", arguments)\n\tif len(arguments) == 2 and'' in arguments[1]:\n\t\toptions =''.join(arguments[1].split()) if actions._actions else ''\n\t\tactions    = actions._actions\n\telif'' in arguments:\n\t\toptions = arguments\n\telse:\n\t\toptions = []\n\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ndef main(arguments):\n\t\"\"\"\n\tMain entry point\n\t\"\"\"\n\t\n\t# create a parser and setup the arguments parser\n\tparser = argparse.ArgumentParser(description=\"Run a RNA-Seq pipeline.\")\n\t\t\n\t# add options\n\tparser.add_argument('-p', '--process',  help='the path to the process the command was run on')\n\tparser.add_argument('-o', '--outgroup',  help='the path to the outgroup name. Should NOT include the name of the temporary '\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t 'directory, but may also contain parameters such as -l, or"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n    argument_parse_args = {}\n    argument_parse_args['config_file'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"configuration.yaml\") \n    argument_parse_args['quiet'] = False # optional - Do not set this!\n    argument_parse_args['debug'] = False # optional - Do not set this!\n    argument_parse_args['json'] = True # optional - Do not set this!\n\n    for argument_pair in arguments:\n        # print(\"\\n\\n\", argument_pair + \"\\n\", argument_parse"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n# call_process\ndef call_process(argumentlist_str):\n\t\"\"\"\n\tFunction to do the processing of the command line arguments.\n\t\"\"\"\n\tprint(\"Processing command-line arguments '{}'\".format(\" \".join(argumentlist_str)))\n\targuments = filter(None, map(lambda x: len(x) > 0, argumentlist_str.split(\" \")))\n\t(args, leftover) = parser.parse_args_to_args_tup(args=arguments)\n\treturn(extract_args_dict(leftover, args))\n\ndef extract"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\npargs = get_parser()\npargs.run(arguments=arguments)\nsys.exit(pargs.error_message)\n# Copyright 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n        parser = ArgumentParser(prog=programName)\n        parser.add_argument(\n        '-d', '--dir', type=str, action='store', dest='dir', help='directory to execute command on, default./tools'\n        )\n        parser.add_argument('-m', '--modules-dir', type=str, action='store', dest='modulesDir', help='directory to look for modules; default./modules'\n        )\n        parser.add_argument('-H', '--help', action='help', help='show this help message and exit')\n        parser.add_argument('-l', '--list',"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tif len(arguments) < 1:\n\t\tparser.print_help()\n\t\tsys.exit(1)\n\t\t\n\tusage_examples = False\n\tfor arg in arguments:\n\t\tif arg.find(\"=\") >= 0: usage_examples = True\n\t\tif arg.find(\":\")!= -1: usage_examples = True\n\t\tif arg.find(\"/\")!= -1: usage_examples = True\n\t\n\tif args_in_file.index(\"-h\")!= -1:\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\tfor i in range"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ndef main():\n\t\"\"\"\n\tReturns a :class:`argparse.ArgumentParser` object and the parsed arguments.\n\t\"\"\"\n\targs = get_arguments()\n\tparser = create_parser()\n\targuments = parser.parse_args(args[1:])\n\n\treturn parser, arguments\n\ndef create_parser():\n\t\"\"\"\n\tReturns an ArgumentParser instance with default values, and no options.\n\t\"\"\"\n\treturn argparse.ArgumentParser()\n\t\t\nif __name__ == \"__main__\":\n\tmain()\n\n\n\n\n# -*- coding: utf-8 -*-\n\n\"\"\"\n***************************************************************************\n    test"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(\n\t\t\tprog=\"runCommand\", \n\t\t\tdescription='Runs the server-side command-line execution interface of FrontEnd.'\n\t)\n\t\n\tcommand_line_opt = parser.add_argument_group(\"Input/output options\")\n\t\n\tcommand_line_opt.add_argument('-c', dest='command_line_args', help='The arguments to use with runCommand', action='store')\n\t\n\treturn parser.parse_known_args(arguments)\n\t\t\ndef run_freetools_command(command_line_args=[], **options):"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='Creates a new output directory with no data, using the given options.')\n\t\n\tparser.add_argument('filename', nargs=1,help=\"Filename of a data set\")\n\tparser.add_argument('-l', '--loglevel', nargs='?', default=\"warning\",type=str, help=\"Log level\")\n\targs = parser.parse_args(*arguments)\n\n\treturn args\n\n\nif __name__ == \"__main__\":\n\t\n\targs = parse_arguments()\n\n\t# This script is a part of the ROS package. It's the main entry point"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\top = parser.add_mutually_exclusive_group()\n\t\n\top.add_argument(\"--add-extra_flags\", dest=\"cmd_add_extra_flags\", action=\"store_true\",\n\t\t\t\t\thelp=\"Add extra options used in command-line invocation (see -h)\", default=False)\n\t\n\top.add_argument(\"--help\", \"-h\", dest=\"cmd_usage\", action=\"help\", help=argparse.SUPPRESS)\n\t\n\top.add_argument(\"--version\", \"-V\", dest=\"cmd_version\", action=\"version\", version=version)\n\top.add_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\treturn sum(i.flags for i in parser._actions if isinstance(i, argparse._SubParsersAction))\n\n\ndef get_args(args):\n\t\"\"\"Parse command line arguments\"\"\"\n\targs = list(args)\n\tpossibilities = []\n\twhile len(args) > 0:\n\t\tfor arg in args[0]:\n\t\t\tflag = None\n\t\t\targmatches = 0\n\t\t\tfor argstr, argpattern in argpatterns:\n\t\t\t\tif re.match(arg, argstr):\n\t\t\t\t\tflag = argstr\n\t\t\t\t\tbreak\n\t\t\tif not flag:\n\t\t\t\tif"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t \n\tdef parser_argument_flags(parser, name):\n\t\tp = parser.add_argument_group(name)\n\t\tp.add_argument('--name', dest='?', help='Command name. (Default: '+args['name'][0]+')')\n\t\tp.add_argument('--default', dest='?', help='Default value. (Default: '+str(defaults[p.NAME]))\n\t\tparser.add_argument(\"args\", nargs=argparse.REMAINDER)\n\t\n\t# Parse arguments first (--help, --version)\n\tflags = p.parse_args()\n\t\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef get_args(parser):\n\t# The parser takes into account options while arguments are provided to\n\t# the application from argparse\n\targs = []\n\tfor p in parser.option_list:\n\t\topt = Option(**p['opt'], description=p[\"descr\"])\n\t\tif 'default' in opt.opts:\n\t\t\targs.append(opt)\n\t\telse:\n\t\t\topts_for_args = {}\n\t\t\tfor optname, optparam in opt.opts.items():\n\t\t\t\topt_value = getattr(Opt, optname)\n\t\t\t\topts_for_args"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\treturn \",\".join(parser.parser.option_strings)\n\n### This function determines what arguments are available to a given plugin.\n###\ndef get_plugin_arguments(parser, plugin_name):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return the argument names available\n\tin the given plugin and its options. \n\t\n\tThe return value is a pair of lists, each item consisting of an argument\n\tname, and a dictionary of options. Each dictionary has the following keys:\n\t\n\t* long:\t  Boolean flag indicating whether or not this is an argument name.\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\treturn dict(\n\t\tidentification=PARSER_ID,\n\t\tdataType=PARSER_NAME,\n\t\tparserName=PARSER_NAME,\n\t\tversion=PARSER_VERSION,\n\t\t\n\t\toptions=FLAGS,\n\t\n\t\tdescription=DESCRIPTION,\n\t\tepilog=(\n\t\t'The flag file contains the definition of the value parsed by the --flagfile-line option.'\n\t\t),\n\t\n\t\tadvancedUsage=ADVANCEDUSAGE\n\t)\n\n\ndef main(argv=None):\n\t\"\"\"\n\tEntry point.\n\t\"\"\"\n\t\n\t# Command line arguments.\n\n\targv = argv or sys"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\t# Build parser.\n\tparser = argparse.ArgumentParser(description='Generate PHRED output.')\n\tparser.add_argument('--prefix', dest='prefix', default='.',\n\t                    help='Prefix of generated.pchred file names.')\n\n\t# Flags.\n\tparser.add_argument('--list-header', action='store_true',\n\t                    help='Print a section listing file header')\n\tparser.add_argument('--no-sections', dest='sections', dest='sections',\n\t                    help='Print a section listing file section name')\n\tparser.add_argument('--list-"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n    return ''.join(parser_flags_re.split(parser.get_default_opt_string()))\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n### BEGIN LICENSE\n# Copyright (C) 2010 Benjamin Althues\n# \n# This program is free software: you can redistribute it and/or modify it \n# under the terms of the GNU General Public License version 3, as published \n# by the Free Software Foundation.\n# \n# This program is distributed in the hope that it will be useful, but \n# WITHOUT ANY WARRANTY"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\t# get all the flags from a parser\n\tall_flags = []\n\ti = 0\n\twhile i < len(parser.option_list):\n\t\ti += 1\n\t\targ = parser.option_list[i]\n\t\tnext_arg = 0\n\t\tnext_data = ''\n\t\t# if the next argument is a comma-separated argument, parse into a flag\n\t\tif '=' in arg:\n\t\t\tif(arg.count('=') == 1):\n\t\t\t\tnext_arg = 0\n\t\t\t\tnext_data =''.join( parser.data )\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef parse_flags():\n\t\"\"\"\n\tReturns the parsed (ArgumentParser) arguments including all the unrecognized\n\targuments and their values.\n\t\n\t:rtype: tuple\n\t\"\"\"\n\t\n\tparser.add_argument(\"-f\", \"--filter\", default=\"\", dest=\"filter\",\n\t\t\t\t\t\tmetavar=(\"FILTER\",),\n\t\t\t\t\t\thelp=\"A Python expression to filter names, e.g. `a, b, c`\")\n\t\n\tparser.add_argument(\"-i\", \"--ignore\", nargs=\"+\", dest=\"ignore\",\n\t\t\t\t\t\tmetavar=(\"RE\", \"IGNORE\"),\n\t\t\t\t\t\thelp=\"A regular expression exclusion pattern matching the names of\")"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\t\n\tparser = OptionParser()\n\tparser.add_option( \"-H\", \"--help\", action = \"help\" )\n\tparser.add_option( \"-i\", \"--interactive\", action = \"store_true\" )\n\tparser.add_option( \"-l\", \"--local\", action = \"store_true\" )\n\t\n\t(options, args) = parser.parse_args()\n\treturn options, args\n\ndef parse_args():\n\t\"\"\"\n\tIf in settings file, check the paths and set those paths to bash completion scripts.\n\t\"\"\"\n\tif not len("}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t # We're not explicitly doing the work of parsing arguments here because we're\n\t # only using a script-level flag: -s/path/to/script.sh\n\t #\n\t # Thus we don't support the arguments that are passed to this script, or with\n\t # a separate parsing pass/fail/skip/succeed/skip as in the bash_completion program.\n\t # E.g.\n\t #   /path/to/script.sh echo Hello world\n\t #   /path/to/script.bat 2>&1\n\t #\n\t # But rather having to mess the output of bash's bash_completion"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\ndef bash_completion_path():\n\t\"\"\"\n\tReturn a bash completion path for borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n##\tget the commandline arguments used to determine the command-line argument parsers\n\tparser = ArgumentParser(description=\"\"\"\n\tCommand line argument parsers can be provided by using an interactive interface that\n\tincludes shell commands such as \"help\", \"setup\", \"check\", \"clean\" and \"config\".\n\tIf this isn't given, this script attempts to find an appropriate parser to\n\tbe"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\ndef execute(argv):\n\t\n\tcmd = ['bash'] + argv\n\t\n\tif len(argv) == 0:\n\t\tcmd.append(sys.executable)\n\t\n\treturn subprocess.check_output(cmd)\n\n# Copyright 2014 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nfrom __future__ import with_statement\n\nimport datetime\nimport time\nimport unittest\n\nfrom dashboard import datastore_hooks\nfrom dashboard import edit_point_event\nfrom dashboard import utils\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tret = \"\"\"echo '%s' | borgmatic -- bash completion\"\"\" % command\n\t\tif command.isalnum():\n\t\t\tret = ret.replace('\\\\','')\n\t\treturn ret\n\treturn ''\n\ndef shell_completion():\n\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\treturn \"\"\"echo '%s' | borgmatic -- shell completion\"\"\" % command\n\ndef bash_prompt():\n\t\"\"\"\n\tReturn a bash prompt script for the borg"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\t# Define the required arguments for the bash completion command.\n\t\tparser = argparse.ArgumentParser(add_help=True)\n\t\tparser.add_argument('query_file', nargs='?', help='Provide query information in the borgmatic')\n\t\targs = parser.parse_args()\n\n\t\t# Define the bash completion command.\n\t\tbash_completion_cmd_str = 'bash -c \"source {}/{}/{}\" >'\\\n\t\t\t\"-i {} | {}/output_files\".format(args.query_file[0], \n\t\t\t\t\t\t\t\t\t\t\t\t\targs.query_file[1],"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\"\"\"\nSupport for RESTful switches.\n\nFor more details about this component, please refer to the documentation at\nhttps://home-assistant.io/components/switch.restful/\n\"\"\"\nimport logging\n\nimport voluptuous as vol\n\nfrom homeassistant.components.switch import PLATFORM_SCHEMA\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.const import (CONF_NAME, CONF_VALUE_TEMPLATE,\n                                 STATE_OFF, STATE_ON)\nfrom homeassistant.helpers.entity import ToggleEntity\nfrom homeassistant.util import slugify\n\n_LOGGER = logging.getLogger(__name"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\ndef help():\n\t# parse command line arguments\n\tparser = argparse.ArgumentParser(description=__doc__)\n\t\n\t# add --quiet option\n\t#parser.add_argument(\"-q\", \"--quiet\", help=\"do not ask for confirmation. This flag overrides --complete\", action=\"store_true\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 0.0.9\")\n\n\t# parse command line arguments\n\targs = parser.parse_args()\n\n\t# import borgmatic commands\n\timport os\n\timport subprocess\n\n\t#"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='Borgmatic command-line utility for generating bash completion scripts for various use cases')\n\tsubparsers = parser.add_subparsers(help='sub-command help',description='valid sub-command help')\n\tqueryline_parser = subparsers.add_parser('query',help='query for a list of fasta files')\n\tcreate_parser = subparsers.add_parser('create-index',help='create the index for a list of fasta files')\n\tremove_parser = subparsers.add_parser('remove',help='remove the sequences from the"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\treturn [ \"bash\", \"-c\", str(script_path), script_path ]\n\tdef __main_loop__(self, cmd_args=(), exit_funcs=(lambda x: []), exit_callback=lambda x: None, interrupt_fun=lambda: None, interrupt_fun_args = [], interrupt_fun_defaults = []):\n\t\ttry:\n\t\t\t# Wait for the command to complete\n\t\t\tself.running = True\n\t\t\tself.running_loop = True\n\t\texcept AttributeError:\n\t\t\t# Hide the subprocess in non-interactive mode if the subprocess is not our own command handler\n\t\t"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t return {}\n#\n#  Copyright (C) 2014 Dell, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\targs = {}\n\tfor arg in unparsed_arguments:\n\t\targs[arg.parser.name] = arg\n\n\ttry:\n\t\ttry:\n\t\t\tparser = argparse.ArgumentParser(description='A tool for running the BedTool command. The supported options can be found in the following table:' + '\\n' + '\\n'.join('%r' % v for v in sorted(parser.print_args())))\n\t\t\tparser.add_argument('--version', action='version', version=version())\n\t\texcept:\n\t\t\tparser.print_help()\n\t\t\tparser.error(\"Could"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\tresult = {}\n\tresult['global'] = {}\n\tfor key, value in globals().items():\n\t\tresult['global'][key] = value\n\n\tparsed_arguments = list(map(type, unparsed_arguments))\t\n\n\t# Set up return arguments\n\tresult['global']['args'] = parsed_arguments\n\tresult['global']['global-args'] = parsed_arguments[2:]\n\n\t# Iterate over all remaining command-line arguments\n\tfor argument_index in range(0, len(unparsed_arguments)):\n\t\tcmd_arg = unparsed_arguments["}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\t# Parse given arguments.\n\targ_parser = argparse.ArgumentParser(\n\t\tdescription=\"Tool for building C code for the C compiler.\"\n\t)\n\n\t# Add global commands.\n\targ_parser.add_argument(\"filename\", metavar=\"filename\", type=str,\n\t                     help=\"filename to parse\")\n\targ_parser.add_argument(\"-C\", \"--noautoconfig\", dest=\"autoconfig\", action=\"store_false\", default=True,\n\t                     help=\"do not automatically configure the C compiler and use C extensions for faster compilation\")\n\n\t# Add arguments required"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\ndef get_args():\n\t\"\"\"\n\tSet up program arguments and return them.\n\t\"\"\"\n\t\nparser=argparse.ArgumentParser(description=__summary__, formatter_class=RawTextHelpFormatter)\nparser.add_argument(\"-e\", \"--extract\",help=\"Extract files to build folder\",required=True)\nparser.add_argument('-o', '--output',help=\"Output directory to put to\",required=True)\nparser.add_argument('-t', '--test',action='store_true')\nparser.add_argument('-i', '--install',action='store_true')\nparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog='bower-vm-builder')\n\tparser.add_argument('--package', dest='package',\n\t\t\t\t\t\thelp=\"package to build\")\n\tparser.add_argument('-B', '--branch', dest='branch',\n\t\t\t\t\t\thelp=\"package to build\")\n\tparser.add_argument('-C', '--checkout', dest='checkout',\n\t\t\t\t\t\thelp=\"checkout to use\",\n\t\t\t\t\t\taction='store_true')\n\tparser.add_argument('--with-xcode', dest='with_xcode',\n\t\t\t\t\t\thelp=\"build with xcode project\", metavar=\"xcode\")"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t if len(unparsed_arguments) < arg_count:\n\t\t\traise SystemExit(\"Too many arguments were found.\")\n\targs = {}\n\targ_namespace = {}\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\targ_namespace[arg['name']] = i\n\t\tfor name in arg['args']:\n\t\t\targs[name] = arg[name]\n\t\targ_namespace[arg['name']] = arg\t\n\treturn args, arg_namespace\n\t    \n\n\n\nif __name__ == \"__main__\":\n    # Parse command-line arguments\n    args, _ = parse"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='Make up your database and the database and set up an argparse.Namespace object')\n\tparser.add_argument('--noinput',\n\t\taction='store_false',\n\t\tdest='interactive',\n\t\thelp='Do not pass the input through argparse.ArgumentParser while parsing the arguments'\n\t)\n\tparser.add_argument('-db',\n\t\taction='store',\n\t\tdest='database',\n\t\ttype=str,\n\t\tdefault=default_db,\n\t\thelp= 'Path of the database which should be migrated/re-imported',\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\n\targsMap = {}\n\tfor arg in unparsed_arguments[1:]:\n\t\tif not isinstance(arg, (type)):\n\t\t\targsMap[arg] = getattr(args, arg)\n\n\tcommand_line_args, argv_map = {}, argz\n\tfor arg in unparsed_arguments[1:]:\n\t\tif not isinstance(arg, argparse.Namespace):\n\t\t\targsMap[arg] = getattr(args, arg)\n\n\targs = argparse.Namespace(**argv_map)\n\t\n\t# Allow users to add argparse \"global\" arguments to each function\n\tif args."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\targs = subparser.parse_args(*unparsed_arguments)\n\t\treturn vars(args)\n# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import (absolute_import, division, generators, nested_scopes, print_function,\n                        unicode_literals, with_statement)\n\nimport atexit\nimport os\n\nfrom colors import green\n\nfrom pants.backend.jvm.targets.jar_dependency import JarDependency\nfrom pants."}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t    \n\tparser_actions_base_usage = []\n\n\tdef add_subparser_action(action, args, prog, error_msg, extra_message):\n\t\taction_name = action.__name__ if action.__name__ else \"<unknown action>\"\n\t\tparser_actions_base_usage.append((action_name, prog, error_msg, extra_message))\n\t\t\n\tparser_action_groups = []\n\t\n\tfor action_name, parser_action_help in subparsers.items():\n\t\t# create and add help for all actions\n\t\tsubparser = subparsers.add_parser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\t# create the argument parser\n\tparser = subparsers.add_parser(name = \"parsing\", description = \"parse command line arguments\")\n\tparser.set_defaults(func = parse)\n\t\n\t# add the options\n\tparser.add_argument(\"-r\", \"--repository\", type = int, dest = \"repository\", help = \"repository of the command to run\",\n\t\t\t\t\t\t\t choices = [\"default\", \"master\", \"local\"], default = \"default\")\n\t\n\t# parse\n\targs = parser.parse_args(unparsed_arguments[2:])\n\t\n\tif args.repository == \"default"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tif len(unparsed_arguments) > 1 and len(unparsed_arguments) in (1, 2):\n\t\tparser = unparsed_arguments[0]\n\t\targs = unparsed_arguments[1:]\n\t\tparsed_args = parser.parse_args(args)\n\t\t\n\t\t# Make sure the number of subparsers is correct:\n\t\tif len(args.keys()) < len(parsed_args) + len(parsed_args):\n\t\t\traise argparse.ArgumentError(\n\t\t\t\t'%s: cannot perform subparser unparsed arguments with too many arguments: %d expected' % (args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\t# If this is being used as an action - just initialize\n\tif \"actions_subparsers\" not in frappe.local.flags:\n\t\tfrappe.local.flags.actions_subparsers = {}\n\n\t# Otherwise, build and register the parser\n\tparser = argparse.ArgumentParser(add_help=False)\n\t# This line isn't to be helpful, but helps us remember to use the subparser\n\tparser.add_argument(\"action\", type=six.text_type, nargs=1,\n\t\thelp=(\"A subparser's name to run. Example - 'run foo"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tfrom argcomplete.completers.complete import complete_prefix\n\n\tparsed_actions_ = subparsers.choices\n\n\tdef parse_subparser_actions(unparsed_actions, subparsers):\n\t\tfor action in parsed_actions_:\n\t\t\taction_data = {}\n\t\t\tif not action.option_strings:\n\t\t\t\taction_data.update(action.option_strings)\n\t\t\tif action.metavar is not None:\n\t\t\t\taction_data.update(action.metavar)\n\t\t\tfor name in action.option_strings:\n\t\t\t\taction_data[name] = True\n\t\t\tparsed_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\treturn unparsed_arguments\n\n\ndef run_with_repository(repository, argv=None):\n\t\"\"\"Runs the given subcommand with the given repository.\n\n\t:param str repository: the repo name to run the subcommand on.\n\t:param list argv: list of arguments that are accepted and their names (argv[0] is the\n\t'argv[1:]' most commonly we want to pass directly to subcommand).\n\t:returns: the requested subcommand (possibly prefixed with)\n\t\"\"\"\n\tfrom. import cmd\n\n\t# get the repository\n\ttry:\n\t\trepo_class"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\treturn parse_args(argparse.ArgumentParser, unparsed_arguments, subparsers)\n\n\ndef parse_args(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give all requested action's subparser a list of parsed arguments. This\n\tallows some additional parsing to be performed on an subparser, as opposed to\n\tparsing \"sub_parsers\". Returns nothing, as does a systematic parsing or command.\n\t\"\"\"\n\t\n\t\n\n\tparser = argparse.ArgumentParser(description=\"A wrapper for \" + __"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tif len(unparsed_arguments):\t# All remaining arguments go through\n\t\tparsed_arguments = unparsed_arguments[0]\n\t\n\telse:\t\t\t\t\t\t\t\t\t\t\t\t# All requested arguments\n\t\tparsed_arguments = []\n\t\n\ttry:\n\t\t# If the subcommand has a subparsers subcommand, look for any options\n\t\twhile (subparsers is not None and\n\t\t\t\tsubparsers[0].__class__.__name__.startswith('Subcommands')):\n\t\t\tsubparsers = subparsers[1:]\n\t\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\t\tif (subparser_name in subparsers"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tsubparser_map = {}\n\tsubparser_map['repository'] = argparse.Namespace(**unparsed_arguments)\t\t# set as an alias for subparser'repository'\n\tsubparser_map['help'] = argparse.ArgumentParser(**unparsed_arguments)\t\t# set as an alias for subparser 'help'\n\n\tfor subparser, arg_info in subparsers.items():\n\t\tname = arg_info.name[1:]\n\t\tif subparser in subparser_map:\n\t\t\tsubparser_map[subparser].__set_default_subparser(subparser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\tdef arg_action(arg, opts, *args, **kwargs):\n\t\t\"\"\"\n\t\tParse the given argument. In the form of key=val, the returned dict will be:\n\t\t{ 'arg': val }\n\t\t\"\"\"\n\n\t\topt = dict()\n\t\targs = []\n\t\twhile args:\n\t\t\tn = args.pop(0)\n\t\t\tif opts.has_key(n):\n\t\t\t\topt[n] = opts[n]\n\t\t\telse:\n\t\t\t\targs.append(n)\n\t\tk = opts.popitem() # pop off the value off\n\t\t"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='Simple python script demonstrating using the GIL')\n\tsubparsers  = parser.add_subparsers(help='Subcommand help')\n\tcommands   = map(subparsers.add_parser, sub_parsers.add_parser('test'))\n\targs        = parser.parse_args()\n\targs.func     = commands.test #print all the tests in the given submodulus\n\targs.name    = os.path.basename( args.func['test'] )\n\targs.path    = args.path[:-2"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef get_parser(parsername):\n\tif parsername is not None:\n\t\treturn get_parser(parsername).add_subparsers()\n\treturn argparse.ArgumentParser()\n\ndef add_parser(subparsers):\n\t\"\"\"\n\tAdd one or more subcommands to the parser.\n\t\"\"\"\n\tparser = subparsers.add_parser('run', description = 'Runs a job',\n\t\tdescription_epilog = 'If this command is called without any arguments it will exit.',\n\tepilog = 'If no command name is given it will call the add_subcommand (this is"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tdef parse_command_line():\n\t\t\"\"\"\n\t\tParse command line arguments.\n\n\t\tArgs:\n\t\t    None\n\n\t\t\tCommand line arguments.\n\t\t\"\"\"\n\n\t\tparser = argparse.ArgumentParser(description=__description__)\n\n\t\tsubparsers = parser.add_subparsers()\n\t\tsubparsers.required = True\n\n\t\tadd_command = parser.add_parser('add', help='Add a new player to your library or library archive')\n\t\tadd_command.set_defaults(action=Action('add_player_to_library', \"Add a new library to your library file\"))\n\t\tadd"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef parse_args():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"This program creates an EGAMS table of FASTA files for use in BWA.\",\n\t\tepilog=\"This program also takes into account all the necessary information.  \"\n\t\t\"It may be used to analyse and plot a given set of FASTA files.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tparser.add_argument('infile',\n\t                 help=\"The name of the FASTA file that you want to analyse.\",\n\t                 type=str, required=True)\n\t\n\tparser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#    def my_parse(self, arg):\n#\t        \"\"\"\n#\n#        Calls argparse.ArgumentParser.parse_args according to the\n#                                   argument parser.\n#\n#        @param arg:\n#        @return:\n#        \"\"\"\n#\n#        self.__parser.parse_args(arg)\n#\n#        return\n# \n#    def my_make_parser():\n#        subparser = argparse.ArgumentParser(\n\t\n#                description=\"This is my base parser which does some helpful in\"\n#                \"handling commands from an external"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n         \n\t# Build top-level parser:\n\t# 1. Parse the arguments:\t\t\n\t#    subparser.add_argument('-i', '--input', metavar='<Input Filename>', type=FilePath,\n\t#                               help='a file name')\n\tsubparser = subtool.add_parser('generate',\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t\thelp = 'Generate a script file using a directory to create a single module from each path.')\n\trequired = subparser.add_argument_group('required arguments')\n\trequired.add_argument('-g',"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\timport argparse\n\t\n\tpar_parser = argparse.ArgumentParser()\n\t\n\tpar_parser.add_argument('command', choices=['start','status','stop'])\n\tpar_parser.add_argument('name', type=str)\n\tpar_parser.add_argument('-v', '--verbose', action=\"store_true\")\n\t\t\n\tpar_parser.add_argument('-t', '--timeout', type=int, default=-100)\n\tpar_parser.add_argument('--file', type=str, help='Path to the file to open', default='')\n\t\n\tpar_parser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\tdef get_usage(args):\n\t\treturn args.parser.format_help()\n\t\n\tdef get_help(args):\n\t\treturn 'USAGE: %s [command] [options...] [args]' % get_usage(args)\n\t\n\tdef parse(args):\n\t\topts, args = parse_args(args)\n\t\tif args:\n\t\t\topts.args = args\n\t\t\n\t\treturn args# ----------------------------------------------------------------------------\n# Copyright 2015 Nervana Systems Inc.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\t\tThis is the parser builder's main loop.\n\n#\t\tWe first loop over the options and command line arguments.\n\n#\t\tWe are trying to parse the command line arguments (we already parsed\n#\t\tand parsed the remaining command line arguments).\n\n#\t\tIn all cases, we run any command line arguments.\n\ndef cmd_parser():\n\tusage = 'usage: %prog [options] arg1 arg2'\n\tparser = optparse.OptionParser(usage)\n\n\tparser.add_option('-s', '--string', dest='string',\n\t\t  "}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n# Get cli flag and print help\ndef parse_args():\n\t\"\"\"\n\tGet the command line arguments.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='The FlexGet is the'+ \n\t\t\t\t\t\t\t\t\t\t\t\t'greatest way to retrieve a'+ \n\t\t\t\t\t\t\t\t\t\t\t\t'single FlexID from the Dragon'+\n\t\t\t\t\t\t\t\t\t\t\t\t' website by opening the.gff file containing'+\n\t\t\t\t\t\t\t\t\t\t\t\t'the gff3 data files which contain'+\n\t\t\t\t\t\t\t\t\t\t\t\t'the gff3 IDs and their data.')\n\tparser.add_argument('infile',"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t    \n\t    \n\n\t\ttry:\n\t\t\ttotal = int(response['total'])\n\t\t\t# Generate the list of object IDs and return it:\n\t\t\tresults = [self.create_object(response['results'], self.config), total]\n\t\t\t# Get the maximum number of object IDs...\n\t\t\tmax_id = int(math.ceil(float(response['total']) / max_results))\n#\t\t\tprint(\"maximum_id:\", max_id)\n\t\t    \tprint(\"size of results:\", len(results))\n\t\t    \n\t\t\tresult_offset = 0\n\t\t\tpage_number = 1"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n    i = 0\n    while i < max_results:\n\tresponse = response + response\n    try:\n\tpage = html.fromstring(response)\n\treturn itertools.islice(page, max_results)\n    except HTMLParser.HTMLParseError, e:\n        return page\n    \n##\n# Class definitions for each of the supported HTTP responses\n# \n\nclass WapiResponseHandler(BaseHTTPRequestHandler):\n\n    def do_GET(self):\n        \"\"\"\n        Handle get request for WAPI.\n        \n        @param uri (str) wapi uri\n        @returns (int) error code (0:OK) and error"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t    num_pages = int(max_results / 10)\n\t    \n\t    objects = parse_page(response['page'], num_pages)\n\t    \n\t    for obj in objects['items']:\n\t\tif obj['title'] in _common_elements:\n\t\t\tyield {\n\t\t\t\t'title': obj['title'],\n\t\t\t\t'url': obj['url'],\n\t\t\t\t'display_timestamp': obj['disappear_on'],\n\t\t\t\t'type': 'object-next',\n\t\t\t\t'id': obj['id'],\n\t\t\t\t'image': _common_elements[obj['type']],\n\t\t\t        }\n\t\t\t   \n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t\n\tfor resp in response:\n\t\t\n\t\tyield resp\n\t\n# The WAPI request interface needs to return the object in the format:\n#\t\t{\n#\t\t\t\"id\": string(id),\n#\t\t\t\"name\": string(name),\n#\t\t\t\"created\": timestamp,\n#\t\t\t\"is_archived\": boolean(is_archived),\n#\t\t\t\"next_link\": string(next_link, \"utf-8\")\n#\t\t\t\"location\": string(location),\n#\t\t\t\"owner\": string(owner),\n#\t\t\t\""}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\twhile 1:\n\t\t\tyield response\n\t\t\tif max_results is not None:\n\t\t\t\tnum_results = int(response.get('pageSize', 0))\n\t\t\telse:\n\t\t\t\tnum_results = 0\n\t\t\tfor obj in response.get('objects', []):\n\t\t\t\tyield obj\n\t\t\t\t\n\t\t\tfor obj in response.get('nextPageToken'):\n\t\t\t\tfor obj in paging(obj['results'], max_results):\n\t\t\t\t\tif obj:\n\t\t\t\t\t\tyield obj\n\t\t\t\t\t\t\n\nclass _BulkJob(object):\n\t\"\"\"Runs a batch of objects to check for duplicate submissions.\n\t\n\tArgs"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\n\tdef get_results(response, max_results):\n    \t\t\"\"\"\n\t\tGet wapi json response object.\n\t\n\t\tArgs:\n\t\t    response (list): WAPI response.\n\t\n\t\tReturns:\n\t\t    object: wapi json response \n\t\t\"\"\"\n\t\t\n\t\tdata = []\n\t\t\n\t\tfor obj in response:\n\t\t\ttry:\n\t\t\t\tdata.append(obj.json())\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\treturn data\"\"\"\nThe config module contains the class Settings. Configuration is written to pick data\nwhen it was run when settings are changed.\n\"\"\"\nimport os\n\nimport json\nimport logging"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t\n\t# Grab object list\n\tobjects = []\n\tfor page in response:\n\t\n\t\tobjects += paging_objects(page)\n\t\n\t# Convert response into objects list ordered by page\n\t# sort according to page-number in last page as per page\n\t\n\tif 1 == max_results:\n\t\tobjects = objects[::-1]\n\t\n\treturn objects\n\t# Copyright 2017 Christoph Reiter\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation;"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n#    response = json_response(response)\n#    print('Page {} of {}'.format(response['data'], max_results))\n\n    data = response['data']\n\n    if not isinstance(data, list):\n        data = [data]\n\n    data = [data.pop()]\n    \n    if 'last_page' in data[0]:\n        data.extend(data[0])\n\n    total = data[0]['total_count']\n    if total > max_results:\n        data = data[0:max_results - total]\n     \n    return wapi.Response(response['last_page'] if"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t    \n\n\tcontent = \"\"\n\tcontent = response[0].getText()\n\n\tif content is not None:\n\t\tprint \"* Paging...\"\n\n\t\tfor row in content.split(\"\\n\"):\n\t\t\tif line.endswith(\": \" + row):\n\t\t\t\tcontinue\n\n\t\t\tp = row.split(\": \")\n\t\t\tp1 = []\n\t\t\tv1 = row.split()\n\t\t\t\n\t\t\tp2 = p1.pop()\n\t\t\t\n\t\t\tp3 = v1.pop()\n\t\t\t\n\t\t\ttry:\n\t\t\t\tp4 = p3[float(v1[0])]\t\t\t\t\t\t\t\n\t\t\texcept:\n\t\t\t\t#"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\n\tdef get_next_response(response):\n\t\t\"\"\"\n\t\tReturns list with all pages in response, sorted by page number.\n\t\t\"\"\"\n\t\telements = []\n\n\t\telements.append('<li id=\"pagination\">')\n\t\telements.append('<a href=\"?page=' + str(int(response['next']) - 1) + '&pagesize=' + str(response['number_page']) + '&ordering=' + str(response['count']) + '\" />')\n\t\n\t\telements.append('</ul>')\n\n\t\treturn elements\n\t\t\t\n\ndef listifying(objects):\n\t\"\"\"\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \treturn float(size) * 1024.0\n\n\tdef is_printable(string):\n\t\t\"\"\"\n\t\tChecks if a string is printable by checking if printable string is not empty.\n\t\tReturns:\n\t\t    Boolean representing whether or not a string is printable.\n\t\t\"\"\"\n\t\treturn len(string) <= 1\n\tif sys.version_info[0] == 2:\n\t\ttry:\n\t\t\t# Python 3+\n\t\t\treturn isinstance(string, (bytes, str, types.NoneType))\n\t\texcept NameError: # Python 2.6\n\t\t\t# Python 2.7+\n\t\t\treturn"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tmatch = NUMBER_REGEX.match(size)\n\t\tif match:\n\t\t\tnumber = match.group(1)\n\t\t\tvalue = match.group(2)\n\t\telse:\n\t\t\tnumber = SIZE_BYTES_MATCH.match(size)\n\t\t\tvalue = float(number)\n\t\t\t\n\t\treturn round(number * value, ROUND_DIGITS_MATCH)\n\texcept ValueError:\n\t\traise ValueError(\"Unrecognised file size '%s'\" % size)\n\t\t\ndef _get_date(month: int, day: int, year: str) -> str:\n\t\"\"\"\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \n    return SizeHelper(\"MB\", size).human_to_bytes()\n\n\ndef truncate(f: Callable) -> Callable:\n    \"\"\"\n    Yield files that are too long or too short\n\n    Example:\n\n      >>> file_iter = iter([\n    ...               f.partial(open, seekable = True),\n    ...               f.partial(open, whence = 0),\n    ...               f.partial(open),\n    ...               f.partial(open, mode = \"w\"),\n    ...               f.partial(open, mode = \"w\", truncate = 1),\n    ...               f.partial(open, mode"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \n\t    size = size.lower().strip().lower()\n  \n\t    if size == \"K\":\n\t        size = get_human_readable_size(size)\n\t    elif size == \"M\":\n\t        size = get_human_readable_size(int(size) / 1024)\n\t    else:\n\t        size = int(size)\t    \n\t    \n        return size\n\n\ndef human_readable(size: str):\n\t\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \n\t    if not isinstance(size, str):\n\t        raise TypeError(\"size must be a string.\")\n\t    else:\n\t    \treturn float(size) * 1024 * 1024\n\t\n\t@staticmethod\n\tdef human_readable_filesize(size: str) -> str:\n\t\"\"\"\n\tConvert human readable file size to human readable file size.\n\t\n\t\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A string representing a human readable file size\n\t\n\t    Examples::\n\t\n\t        >>> human_readable_filesize(\"500 KiB\")\n\t        500 ("}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \treturn \"%s %s\" % (size, 'GB')\n\texcept Exception as exc:\n\t    raise ValidationError(\"Size not readable: %s\" % (str(exc)))\n# Copyright 2014 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t # See also https://github.com/khalil/psutil/issues/637\n\t\tm = re.match(r'''\n\t\t\t^(?P<size>\\d+(?:\\.\\d+)?)\n\t\t\t(?P<suffix>\n\t\t\t(?P<multiplier>-?)\n\t\t\t(?P<suffix2>\\d+)\n\t\t''', os.path.basename(str(dir(psutil.Process())))\n\t\tif not m:\n\t\t\treturn SIZE_UNITS[\"BYTES\"]\n\t\tsize = (int(m.group(\"size\")), int(m.group(\"suffix\")))\n\t\tsuffix = m."}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n    bytes: Any = float\n    size_in_kib: Any = float\n    kib: Any = int\n    i = 1\n    if size.endswith(\"iB\"):\n        size = size[:-1]\n        size_in_kib = size[:-1]\n        kib = size[-1]\n    elif size.endswith(\"b\"):\n        size = size[:-1]\n        size_in_kib = size[:-1]\n        kib = size[-1]\n    else:\n        return size_in_kib\n    for i in range(1, kib + 1):\n        bytes = round(bytes"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tfsize = int(size)\n\t\tif fsize > 30:  # This can be a longer string than it should be fine!\n\t\t\treturn fsize\n\t\t\n\t\tisize = 1048576 * fsize\n\t\tif size < isize * 10:  # Another long string, should fit it into 16k\n\t\t\tsize *= 10\n\t\t\thsize = fsize\n\t\t\tfsize = int(size)\n\t\t\thsize = \"%iK\" % hsize\n\t\t\tsize = hsize[0 : size + 3]\n\t\telif size < isize:  # This could be bigger"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \n\t    bytes_with_sign = \"-\" # \"1\" -> \"-1\"\n\t    \n\t    bytes_with_frac = \".\"\n\t    for char in size:\n\t        bytes_with_sign += bytesep + char\n\t        for i in xrange(2, len(string.ascii_lowercase + string.ascii_uppercase)):\n\t\t    bytes_with_frac = bytes_with_dig + string.ascii_lowercase[i] + string.ascii_uppercase[i] + string.digits[i] + \"%\"\n\t\t    bytes_with_frac = bytes_with_frac + bytesep + string.digits["}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\tcombined = {}\n\tfor d in dicts:\n\t\tif not combined:\n\t\t\tcombined = d.values()\n\t\telse:\n\t\t\tfor (k, v) in d.items():\n\t\t\t\tcombined[k] = v\n\t\n\treturn combined\n\ndef dictsum(dlist):\n\t\"\"\"\n\tCombine values of a particular list.\n\t\n\t>>> dictsum([('a', 0), ('b', 1)])\n\t{'a': 6}\n\t\"\"\"\n\tcombined = {}\n\tfor d in dlist:\n\t\tcombined[d['a']] = d\n\t\n\treturn combined"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\treturn dict(sum(dicts, []))\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# The following has class definitions of standard key-value pairs used.\n\nclass DictProperty:\n\t\"\"\"An immutable non-relational key-value pair.\n\t\n\tThis key-value pair is used to store any instance of a specific kind\n\tof dictionary key, i.e. the name and the value.\n\t\n\tThe following example shows these fields:\n\t\t\n\t>>> d = DictProperty()\n\t>>> d.a = 1\n\t>>> d."}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\n\tdef _getval(l):\n\t\treturn {k:v for k, v in six.iteritems(l) if k in _dictsum.keys()}\n\n\tif not isinstance(dicts, list):\n\t\tdicts = [dicts]\n\t\n\treturn {k: _dictsum(d) for k, d in dicts}\n\n\ndef _dictlist(l):\n\treturn [k for k, v in six.iteritems(l) if v in _dictsum]\n\t\n\treturn [k for k, v in six.iteritems(l)]\n\n\ndef _dictany(dicts"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\treturn sum(dicts, 0)\n\treturn reduce(operator.add, dicts, ([],))\n\n\ndef _dictproduct(*dicts):\n\t\"\"\"\n\tCombine values of either lists or dictionaries, returning the dictionary.\n\t\n\t>>> _dictproduct([1,2,3])\n\t{1: 2, 2: 4, 3: 3}\n\t\"\"\"\n\td = {}\n\tfor d in dicts:\n\t\tif isinstance(d, dict):\n\t\t\td = _dictdict(d)\n\t\telse:\n\t\t\td = _dictint(d)\n\treturn d\n\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\t# TODO:\n\t# - make better names:\n\t# - better type-erasing.\n\t# - allow using a regular expression as this is more efficient than using\n\t#   string operations and much more portable:\n\t#   http://www.unicode.org/reports/tr44/\n\t# - when doing this, be a bad idea. Need to rewrite to eliminate\n\t#   the common case. The following code attempts to detect these, until\n\t#   we implement proper number conversion, and only use the regular\n\t#   expression:\n\t#"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\nfrom itertools import combinations, chain\nimport unittest\nimport sys\nimport collections\n\n__version__ = '4.0'\n__all__ = ['sugarify', 'dictcombine', 'total_ordering', 'totaltime', 'tally',\n           'tally_sorted', 'tally_sorted_by', 'is_sorted', 'tallydict', 'tallyfile',\n           'tallylist', 'tallylist_keys', 'tallylist_values', 'tallylist_items']\n\n\ndef _tally_sorted_by(tally, a):"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t  \n\tsums = {}\n\tfor d in dicts:\n\t\tsums[d] = d\n\tsums_len = len(sums)\n\t\n\t# Make sure the total lengths are consistent to the sum of\n\t# the different dicts\n\tif sums_len!= len(sums):\n\t\traise RuntimeError(\"dicts don't have the same length!\")\n\t\n\t# For each item in the current block\n\tfor k,v in sums.items():\n\t\tif not v:\n\t\t\tcontinue\n\t\t\n\t\t# If the key contains data and there is a suffix,\n\t\t# add in the suffix to the"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\nclass _OrderedDict(object):\n\t\"\"\"\n\tAn object that looks like a dictionary except `list`, but sorted\n\t(which is preferable) elements are discarded.\n\t\n\t>>> d = OrderedDict([('c', 3), ('b', 2), ('a', 1)])\n\t>>> _dictsum(d)\n\t{'a': 6, 'b': 2, 'c': 3}\n\t\n\tThe *key* argument to the ``dictsum`` is used to provide\n\tnormalised order in the output. This is useful if you want to use\n\tdicts in a product (for instance)"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\treturn sum([dicts[k] for k in _dictlist])\n\nclass _OrderedDict:\n\t\"\"\"\n\tA dummy class used by OrderedDict to override comparison of\n\tduplicate keys to a given value (which should be a dict).\n\t\"\"\"\n\tdef __init__(*args, **kwds):\n\t\tif len(args) > 1:\n\t\t\traise TypeError('expected at most 1 arguments, got %d' % len(args))\n\t\tif not args:\n\t\t\traise TypeError('expected at least 1 argument, got %d' % len(args))\n\t\tself = args["}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n_link_items = {}\n\t\n\tif isinstance(url, str):\n\t\ttry:\n\t\t\turl = url.format(params['data'])\n\t\texcept KeyError:\n\t\t\traise TypeError(\"Expected string url for \\\"data\\\"\")\n\t\t\t\n\t\twith open(params['data'], 'r') as f:\n\t\t\turl_items = map((lambda item: _replace_url_args(item[0], item[1]), f.readlines()))\n\t\t\tfor item in url_items:\n\t\t\t\tif len(item) == 2:\n\t\t\t\t\tkey, value = item\n\t\t\t\t\turl_items["}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n#\turl = url.replace('{url_format}', 'format')\n#\turl = url.replace('{url_format}', 'format')\n#\turl = url.replace('{url_format}', 'format')\n#\tdel url_args['format']\n#\tdel url_args['format']\n#\tdel url_args['format']\n#\turl = url.replace('{url_format}', 'format')\n#\turl = url.replace('{url_format}', 'format')\n#\tdel url_args['format']\n#\tdel url_args['"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n\n\t\n\tfor key, val in url_args.iteritems():\n\t\tif not isinstance(val, str) and key.startswith(\"_\") is False and isinstance(val, unicode):\n\t\t\t\t\n\t\t\tcontinue\n\n\t\telse:\n\t\t\tkwargs[key] = str(val)\n\n\ndef get_html(url, content_type=None):\n\n\t\"\"\"Returns HTML formatted response string\"\"\"\n\n\tfrom webnotes.utils import encode\n\n\t# if content type is specified, try to guess\n\tif content_type: doctype = content_type\n\n\telse:\n\t\tdoctype = frappe.db.get_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n\n\tfor i, arg in enumerate(url_args):\n\t\tnew_args = {}\n\t\tfor k, v in arg.items():\n\t\t\tif isinstance(v, basestring):\n\t\t\t\tnew_args[k] = unicode(v, 'utf-8')\n\t\turl = url.replace('{%s}' % i, new_args)\n\t\t\t\n\t\t\t\n\treturn url\n\n# \n# End of helper\n\n\n### Functions\n\ndef get_hostnames_list(hostname_list=None):\n\t\"\"\"\n\tGet all hostnames in a list.\n\t\"\"\"\n\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n\n\tif 'url' in url_args:\n\t\targuments = urllib.parse.quote_plus(\n\t\t\t\t\t('%s?api-key=%s&url=%s' % (\n\t\t\t\t\t\turl,\n\t\t\t\t\t\turllib.parse.quote_plus(url_args['api-key']),\n\t\t\t\t\t\turllib.parse.quote_plus(url),\n\t\t\t\t\t\turl_args['url']\n\t\t\t\t\t))\n\t\t\t)\n\telse:\n\t\targuments = url_args\n\n\treturn arguments\n\ndef generate_id():\n\t\"\"\"\n\tGenerate a secretid to be able to run multiple instances and then"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t \n\tfor key, value in url.iteritems():\n\t\tparts = urlparse.urlparse(key)\n\t\tif value:\n\t\t\tparts[0] = url_args.pop(0)[0]\n\t\t\n\t\t\treplace_index = 0\n\t\t\tfor part in parts:\n\t\t\t\treplace_index += 1\n\t\t\t\turl_args[replace_index] = parts[replace_index - 1]\n\n\t\t\turl_args[replace_index] = _quote_url(parts[0]) + url_args[replace_index][1]\n\t\telse:\n\t\t\turl_args[replace_index"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n#\tThe function should remove the leading / w/.\n\turl = url.split('/w/')\n\treturn _replace_url_args(url[len(url_args) :])\n\n\n#\tThe function should remove parts that are not a leading slash\n\t#\tExample: http://foo:bar\n\turl = url[len(url_args):]\n\turl = '/foo/' + url\n\treturn _replace_url_args(url)\n\n\t\n#############################################################################################################\n#\n#\tHelper functions\n#\n#############################################################################################################\ndef _is_valid_identifier"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n__author__ ='seanfitz'\n\n__source__ = 'https://github.com/daniara/culture_framework.git'\n__version__ = '0.0'\n__email__ = 'daniara@gmail.com'\n__license__ = 'MIT'\n__b64__ = False\n\ndef main():\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Command line utility for the culture framework',\n\t\tformatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\n\t\t'-s',\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n\n\tfor idx, arg in enumerate(url_args):\n\t\tif arg[0] == '+':\n\t\t\turl_args[idx] = arg[1]\n\t\telse:\n\t\t\turl_args[idx] = url_args[idx].replace(URL_ESCAPE, URL_ESCAPE + arg)\n\t\t\n\treturn url_args\n\t\t\t\t\t\t\n\nif __name__ == '__main__':\n\turl_in = ''\n\turl_out = ''\n\tif len(sys.argv) > 2:\n\t\tfor filename in sys.argv[2:]:\n\t\t\tif filename.startswith('--"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\ndef _handle_list_args(*args):\n\t\"\"\"\n\tFunction replaces any argument in args with values in argv\n\t\"\"\"\n\tfor i in args:\n\t\ttry:\n\t\t\tif isinstance(i, str):\n\t\t\t\targs[i] = urllib.quote_plus(args[i])\n\t\t\telif i in (True, None):\n\t\t\t\targs[i] = [urllib.quote(str(i))]\n\t\texcept Exception as e:\n\t\t\tif i in (True, None):\n\t\t\t\targs[i] = [urllib.quote_plus(str(i))]\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val is None or (val.lower in [\"\", None] + [\"\", \"None\"])\n\telse:\n\t\treturn val.lower().strip() in [\"none\", \"none\"]\n@conf\ndef is_dict_value_containing_key(val: Any, key: str, sep: Optional[str], val_type=str) -> bool:\n\t\"\"\"\n\tCheck if a value is a dict containing a key.\n\t\"\"\"\n\tif val is None or key is None or len(key.strip()) == 0:\n\t\treturn False\n\n\t# TODO: add"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t # The null case here is because there seem to be no unicode values in Unicode;\n\t\t#  this only happens when the string has one.\n\t\tnul_null_match(val, \"Null\")\n\t\treturn None\n\treturn val\n\ndef is_int(val: Any) -> bool:\n\t\"\"\"\n\tCheck if a value represents an integer in a given range (max 255).\n\t\"\"\"\n\tnul_range = [\n\t\t\"Int32\",\n\t\t\"Int64\"\n\t]\n\ttry:\n\t\tnul_type = nul_min ="}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val is not None and val is not ''\n\n#####################################################################\n#\n#  DIMENSIONS\n#\n#####################################################################\n\n# XXX Note the use of the nonlocal keyword so that the strings defined by\n# a.h file are now python keywords!\nget_keyword_kwlist = _keyword.get_keyword_kwlist\nget_keyword_kwlist.restype = POINTER(c_char_p)\n\n\ndef is_string_or_list(val: Union[StringType, Sequence[Any]], include_null: bool = False) -> bool:\n\"\"\"\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val!= ''\n\n\ndef bool_to_str(val: any) -> str:\n\t\"\"\"\n\tEquivalent to :func:`bool_true_false()` which casts anything to a\n\tstring. The string is cast to `bool` using default `str`.\n\t\"\"\"\n\treturn str(val) == 'true'\n\n\ndef str_to_bool(val: str) -> bool:\n\t\"\"\"\n\tCast a string to a boolean or a string value of the same length.\n\tIf str is `True` and `False` for the string this method"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn isinstance(val, str) and not str.isspace()\n\tif is_none_string(val):\n\t\treturn False\n\traise ValueError(\"%s is not a valid value for %s\" % (type(val), val))\n\ndef is_none_list(val: any) -> bool:\n\t\"\"\"\n\tCheck if a list represents a None value.\n\t\"\"\"\n\treturn isinstance(val, list) and not val or (is_none_string(val) and val.strip()!= '')\n\ndef is_not_in(val: any) ->"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t  # this method is overridden by the inheritors's code in this file.\n\t\t# it is used by the user to enforce the error message to be at a minimum.  As the\n\t\t# superclass does not know the existence of the null element of the element\n\t\t# or the empty string, we have this restriction and would have been redefined\n\t\t# by any other means.\n\t\treturn None not in unknowns and val is None\n\t#end def is_none_string\n\n\n# -----------\n# Class: test_noreturn_string\n# Module: test_noreturn_"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tif isinstance(val.value, str):\n\t\t\treturn val.value.lower() in ('none', 'n/a')\n\t\telse:\n\t\t\traise ParseError(\"Bad value. Use parse as the value for truth.\")\n\treturn (type(val.value) is str) or (val.value is None)\n\"\"\"\n\nPython Interchangeable Virtual Instrument Library\n\nCopyright (c) 2012-2017 Alex Forencich\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn len(val) == 0\n\tif not is_string(val):\n\t\treturn False\n\treturn True\n\ndef is_list(val: list) -> bool:\n\t\"\"\"\n\tTest string's length.\n\t\"\"\"\n\t\treturn len(val) > 0\n\ndef is_list_empty(val: list) -> bool:\n\t\"\"\"\n\tReturn True if the value is a scalar which is either [] or {}.\n\t\"\"\"\n\tif not is_string(val):\n\t\treturn False\n\tif len(val) == 0:\n\t\treturn True"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val is None or val == ''\n\n\t# Return one value by a comma, space or anything, whitespace.\n\t# Strings cannot end with a period or whitespace: the colon must be\n\t# preceded by a period.\n\treturn val or val.isspace()\n\n\n# This is a copy of code from python3.5's unittest.py for needle in tests.\n# It is copied from pyqode.py for the actual test cases and not in other\n# code.\n\n# Tests, if you change how things work, you may run them and want to change\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn isinstance(val, (str, bytes))\n\n\ndef is_string(val):\n\t\"\"\"\n\tCheck if a string represents a string value, which means it has\n\tno value.\n\t\"\"\"\n\treturn isinstance(val, str)\n\n\ndef is_iterable_or_string_type(val: Any) -> bool:\n\t\"\"\"\n\tCheck if a value is an iterable (like a list or a string) or a string value.\n\t\"\"\"\n\treturn isinstance(val, collections.Iterable) and not isinstance(val, six.string_types)\n\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\n\tclass BraceMessage(object):\n\t\t\"\"\"\n\t\tThis class represents the internal BraceMessage structure for an ElfFile.\n\t\tIt acts as an in depth-first manner:\n\t\t\tThis class keeps a stack of children\n\t\tFor each child type there is an item in child_types\n\t\t\"\"\"\n\t\t\"\"\"The list of children is stored as a list of the same length\"\"\"\n\t\tdef __init__(self, file_obj, children):\n\t\t\tself.file_obj = file_obj\n\t\t\tself.children = children\n\n\t\tdef __str__(self):\n\t\t\treturn \"[\""}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\tdef parse_command_line(args_string):\n\t\t\"\"\"\n\t\tParse a command line as either a name, description, or a positional argument.\n\t\t\"\"\"\n\t\n\n\n\t\tif len(args_string) >= 2:\n\t\t\tname = args_string[0]\n\t\telse:\n\t\t\tname = \" \".join(args_string).capitalize()\n\n\n\tdef help_doc(parser):\n\t\t\"\"\"\n\t\tShow a help message for the individual command (usage information or help in a specific\n\t\ttopic in the documentation), or a generic help for one sub-command in a specific"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tdef get_parser(field):\n\t\treturn getattr(namespace, \"_\" + field.name)\n\t\n\treturn sum(parser_flags(field) for field in dir(namespace))\n\ndef _parse(args):\n\t\"\"\"\n\tCreate a dictionary of options:\n\t\t\n\t* defaults: set all options with default values, or leave None if there are\n\t\tno arguments. \n\t\n\t* positional arguments: a dictionary of options and positional arguments\n\t\tof the form `dest=<option> = <name>=<value>` for each of these options, or\n\t\tdefault values if there is no"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\t# For the first argument, take the command name and strip everything in quotes.\n\tfirst = parser.prog.split()[0].replace(\"'\",\"\")\n\tif first.startswith(\"--\"):\n\t\tfirst = first[1:]\n\tfor o in first.split(\",\"):\n\t\to = o.strip()\n\t\tif o.startswith(\"-\"):\n\t\t\to = o[1:]\n\t\tflags.append((o,\"\"))\n\t\t\t\n\treturn flags\n\t\t\n# =============================================================================\n# MAIN\n# =============================================================================\n\nparser = OptionParser()\n\nparser.add_option(\"--outDir\", \"-"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tflags = {\n\t\t'usage': 'usage: %s [OPTIONS...] [PATH...]' % sys.argv[0],\n\t\t'version': version,\n\t\t'formatter': argparse.RawDescriptionHelpFormatter,\n\t\t'description':\n\t\t\t'Display data for configuration file parser',\n\t}\n\n\tfor option, default, convert in constants.OPTPARSE_HELP_ARGUMENTS:\n\t\tusage = getattr(parser.values, option, None)\n\t\tflags.setdefault(usage, getattr(DEFAULT_STORE.get(False), default))\n\treturn flags\n\n\ndef main():"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\treturn tuple(parser._group_actions)\n\t\nclass ArgumentParser(argparse.ArgumentParser):\n\t\"\"\"\n\tArgumentParser that makes it very helpful to write version information,\n\tfor debugging and interaction with Bill/Fame and it's command line.\n\t\n\tAlso sets up the argument parser and help message and provides a common interface\n\tfor displaying command-line arguments.\n\t\n\tAlso provides a custom help text string, using the `help` command-line options,\n\tsuch as ``--version`` or ``--config`` to set the program's version.\n\t\"\"\"\n\n\tdef __init__("}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t http://stackoverflow.com/questions/5326053/argparse-how-to-remove-all-flags-in-my-argparse-arguments\n\tparser.add_argument('--print-flags', '-V', dest='debug_level', default=None, type=int,\n\t            metavar='LEVEL', help=\"Print verbose debugging info (default = 0)\")\n\tparser.add_argument('--dump-flags', '-P', dest='dump_level', default=3, type=int,\n\t            metavar='LEVEL', help=\"Print debugging debug information (default = 3)\")\n\tparser.add"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tdef _is_required(val):\n\t\tif not (val is None):\n\t\t\tif val.required:\n\t\t\t\treturn True\n\t\treturn False\n\t\t\n\tfor arg in argtable:\n\t\tif _is_required(arg.argname):\n\t\t\tflag = \"required\" if arg.argname == arg.name else \"optional\"\n\t\t\tif arg.default and _is_required(arg.default):\n\t\t\t\tflag = \"default\"\n\t\t\tparser.add_argument(\"--%s\" % flag, *arg.flags, **getattr(parser.get_argumentdef(arg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t return \"h\" + \"v\" + \"x\" + \"o\"\n\ndef parser_usage(parser, add_help = True, auto_help = True):\n\t\"\"\"\n\tReturn a help string if it is running under python/ironpython/cmd-ing\n\twhen the first argument is not an option (which probably means it is not\n\ta long option). Set an argument to use defaults if missing in parser.\n\t:param add_help:  Bool, if set, add the ``--display`` key to the help string.\n\t:param auto_help: "}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\timport random\n\timport string\n\tname = 'ParserFlags'\n\tusage = '%s <source_files> <destination_files>'\n\n\tdescription = __doc__ and string.Template(\n\t\tstring.Template(\n\t\t\t'Usage: %s [OPTIONS] <source_files> [DIRECTORY] [FILE1 FILE2...]' % name,\n\t\t\t\tparser_class=argparse.ArgumentParser\n\t\t).substitute(name=name, **locals())\n\t\n\n\n\n\ndef main():\n\t# Get a CLI parser\n\tparser = argparse.ArgumentParser()\n\n\t# Init a sample command generator"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\treturn os.path.isdir(processed) \\\n\t\tor os.path.isfile(os.path.join(path_name, path_name)) \\\n\t\tor os.path.isdir(os.path.join(path_name, path_name, 'testfile')) \\\n\t\tor os.path.isfile(os.path.join(path_name, path_name, 'testfile', 'testfile.txt')) \\\n\t\tor os.path.isfile(os.path.join(path_name, path_name, 'testfile.txt.txt'))\n\n\ndef should"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\t# Make sure the path exists\n\tif PROC_PATH_NAME is None:\n\t\traise Exception, \"PROC_PATH_NAME attribute must be set.\"\n\t\n\tif path_name.startswith('/'):\n\t\traise Exception, 'PROC_PATH_NAME must not start with a slash.'\n\t\t\t\n\tabs_path_name = os.path.abspath(path_name[1:])\n\t\n\t# Check the real directory\n\tif PROC_PATH_NAME!= abs_path_name:\n\t\traise Exception, \"Only absolute paths are allowed!\"\n\t\t\n\t# Check the files and"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t    \n\t# get all existing items\n\titems = (\n\t     os.path.splitext(os.path.basename(path_name))[0]\n\t     for path_name\n\t     )\n\t# ignore hidden files in the path name\n\titems = os.path.join(path_name, '*.hidden')\n\t\n\tif processed and items in processed:\n\t    logging.warning('Removing an entry as part of the folder: '\n                        '{0}...'.format(path_name))\n\t\n\t# report path name\n\tlogging.info('Will remove the {0}...'.format(path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tif os.path.isfile(path_name):\n\t\tif verbose:\n\t\t\tprint \"The file %s already exists.\"\n\t\traise OSError(errno.ENOENT, path_name)\n\t\n\tif os.path.isdir(path_name):\n\t\treturn False\n\t\n\tdirs = set()\n\tfiles = set()\n\tfor f in os.listdir(path_name):\n\t\tif os.path.isfile(os.path.join(path_name, f)) and f.endswith(\".pyc\") and f!= \"__init__.py\":\n\t\t\tfiles.add(os."}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\tentry = False\n\tfor f in processed:\n\t\td = os.path.abspath(f)\n\t\tif d == path_name:\n\t\t\tif verbose:\n\t\t\t\tprint \"\\nProcessed path: \"+d\n\t\t\tentry = True\n\t\t\t\n\t\telif d.find(\"src/\") == -1:\n\t\t\tif verbose:\n\t\t\t\tprint \"\\nProcessed path: \"+d\n\t\t\tentry = False\n\t\t\t\n\t\telse:\n\t\t\td = d[len(path_name):]\n\t\t\tif verbose:\n\t\t\t\tprint \"\\tNope found entry: \"+d\n\n\n\treturn entry\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\tif os.path.isdir(path_name):\n\t\tif verbose:\n\t\t\tprint('Recursively processing path \"%s\".' % (path_name,),\n\t\t\t      callingModule=callingModule, warning=True)\n\t\tfoundFiles = set()\n\t\tif not os.path.islink(path_name):\n\t\t\treturn False\n\t\tif verbose:\n\t\t\tprint('  - Skipping file \"%s\".' % (path_name,), callingModule=callingModule)\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tbase, name = os.path.split(path_name)\n\t\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\t# Check that all paths exist\n\tfor path_name in _list_of_paths(path_name):\n\t\t\n\t\tverbose_print(\"checking {0}\".format(path_name))\n\t\t\n\t\t# Check that path names exist\n\t\tpath_name_str = _str_to_path_name(path_name)\n\t\t\n\t\t# If path name exists, exit\n\t\tif path_name_str.find(\"/\") == -1:\n\t\t\treturn False\n\t\telif path_name_str.split(\"/\")[-1] == path_name:\n\t\t\treturn False\n\n\t\t# Update the entry"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n    return path_name in processed\n\n\n# -----------------------------------------------------------------------------\n#\n# Functions to get the names of the files inside the given directory\n# -----------------------------------------------------------------------------\n#\ndef get_file_names(dir):\n\t\"\"\"\n\tGet a list of all the files in the given directory.\n\t\n\tArgs:\n\t    dir: Directory being walked\n\tReturn:\n\t    A list of the file names inside the given directory.\n\t\"\"\"\n\t\n    if os.path.exists(dir):\n        for file in os.listdir(dir):\n            path = os.path.join(dir, file)\n            if os.path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\tif processed and file_utils.file_exists(path_name):\n\t\tif verbose: print(path_name, '- processing')\n\t\t\n\t\tif verbose: print(processed_path_name, '- file' if processed else'file')\n\t\tfor ext in (\".dat\", \".py\"):\n\t\t\tif path_name and is_python_script(path_name + ext):\n\t\t\t\tprocessed.add(path_name + ext)\n\t\t\n\t\treturn True\n\t\n\treturn False\n    \n\n\n# The Hazard Library\n# Copyright (C) 2012 GEM Foundation\n#\n# This program"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\tp = Path(path_name)\n\tif os.path.isdir(p):\n\t\tif verbose:\n\t\t\tprint('Skipping directory %s.' % path_name, file=sys.stderr)\n\t\treturn False\n\telif os.path.exists(p + '.bz2'):\n\t\tif verbose:\n\t\t\tprint('Skipping bz2 file %s.' % path_name, file=sys.stderr)\n\t\treturn False\n\telse:\n\t\t# Check if all paths are unique and not longer than MAX_PATH_LEN\n\t\tfor path in p.rglob(\"*"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\tvertex0 = vertices[0]\n\t\tvertex1 = vertices[1]\n\t\tvertex2 = vertices[2]\n\t\tvertex3 = vertices[3]\n\t\tvertex4 = vertices[4]\n\t\tvertex5 = vertices[5]\n#\t\tvertex4 = self.intersect(vertex1, vertex2, vertex3, vertex4)\n#\t\tvertex3 = self.intersect(vertex1, vertex5, vertex4, vertex1, vertex5.union(vertex4, vertex3, vertex2))\n#\t\tvertex2 = self.intersect(vertex1, vertex"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n#    n = vertices.count(Vector2((p[0], ) + p[1]).exterior.coords)\n#    p = vertices.points\n#    e0 = vertices.exterior.coords\n#\n#    d0 = Point3(p[2] + e0.x, p[1] + e0.y)\n#    d1 = Point3(p[0] + e1.x, p[1] + e1.y)\n#\n#    d0.Xn = d0X\n#    d1.X"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tv = V2v(vertices[0], vertices[1], vertices[2])\n\tif v is None:\n\t\treturn None\n\t\n\tpoints = []\n\tfor i in xrange(4):\n\t\tfor j in xrange(4):\n\t\t\tpoints.append(V2point(v.x+i, v.y+j))\n\n\t\tfor i in xrange(4):\n\t\t\tpoints.append(V2point(min(points[i].x+1, points[i].y+1), max(points[i].x-1, points[i].y"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t                                                               A\n\t A = [0, 0, 0, 2, 2] # top-right\n\t B = [1, 1, 1, 0, 1] # top-left\n\t C = [0, 1, 1, 0, 1] # top-center\n\t C = [0, 0, 0, 1, 1] # top-radius\n\t D = [(1, 0), (0, 1), (1, 1)] # top-radii\n\t DBB = [(1, 1), (0, 1), (1, 0)] # top-right radii\n\t CD = [("}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\n\tif len(vertices)!=3:\n\t#a line segment\n\t\treturn None\n\n\ndef vertex3tuplelist(vertices):\n\t\"\"\"\n\treturn a list of tuples defining x,y,z of every vertex\n\t\"\"\"\n\tlist=((2,4),(-1,-2))\n\t#tuples needed\n\treturn list\n\n\ndef listtuplelist(vertices):\n\t\"\"\"\n\treturn a list representing list of tuples with x,y,z of every entry\n\n\t:param vertices: a list of tuples containing x,y\n    :type vertices: 3-argument bool,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\n\t#Firstly, the result of adding all three points, which is our starting vertex\t\n\t#(DAB, ABC, BCD, CDA)\n\t#\n\t#the return list is ordered as:\n\t# DAB, a, b, c, d, e, f\n\t#  C D\n\t# E C F\n\t#\n\t#for i in range(3):\t\t#create a list of four corner points to be added to the list of three sides (0,2,1), (10,10,1), (5,5,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tvertices = sorted([vertex3tuple(v) for v in vertices])\n\t\n\tn = len(vertices)\n\ti = 0\n\twhile i < n-1:\n\t\tvertices[i][0] = vertices[i][0] + vertices[i-1][0]\n\t\tvertices[i][1] = vertices[i][1] + vertices[i-1][1]\n\t\tvertices[i][2] = vertices[i][2] + vertices[i-1][2]\n\t\tvertices[i][3] = vertices[i][3] + vertices"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tvertex1 = []\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\tvertex1.append(vertices.pop(0))\n\t\n\tvertex2 = []\n\tvertex2.append(vertices.pop(0"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tpolygon = list()\n\tnum_of_vertices=len(vertices)\n\tvertex=vertices.pop(0)\n\t\n\tif(num_of_vertices>0):\n\t\tvertices=list(vertices)\n\t\tvertex_of_polygon = list()\n\t\tvertex_of_polygon_list= list()\n\t\t\n\t\tindex = 0\n\t\t\n\t\tfor edge in vertices:\n\t\t\n\t\t\twhile(index < num_of_vertices):\n\t\t\t\t#point has two vertices, so take average of each of them to produce a point\n\t\t\t\tx1,y1 = edge\n\t\t\t\tx2,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\t#loop over all points\n\tptList1 = []\n\tfor point in vertices:\n\t\tfor point2 in range(point + 1, len(vertices)):\n\t\t\tpts1 = vertexline(vertices[point])\n\t\t\tptList1.append([points1[0], points2[0]])\n\treturn [(pts1[0][1],pts1[0][2], pts2[0][2]) for p0, p1,p2 in pointList1]\n\t#return ((0,2),(2,0), 0)\n\ndef voronoi"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n# Helpers\ndef random_letters(num:int=24, a:int=3, b:int=6, c:int=48) -> Generator:\n\t\"\"\"\n\tSelect a randomly generated number from the given num.\n\t\n\tThe length of the array must be divisible by *a* -- it can contain any leading zeros.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\n\tr = []\n\twhile num >= 0:\n\t\tr.append(a / b if a % b < 0 else b - a)\n\t\tnum -= a\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t\n\ta = ''\n\tfor b in alphabet:\n\t\tif len(a) > a.index(b):\n\t\t\ta = a[:a.index(b)]\n\t\telse:\n\t\t\ta = a + str(b)\n\t\n\t# remove the final empty letter\n\tb = (b.upper() if len(b) < 4 else 0) + a\n\t\t\n\t# remove the first letter of the alphabet\n\tb = chr(b[0].upper() + (b[1] if len(b) > 1 else 0))\n\t\n\treturn b\n\ndef int_"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n# FIXME: The following numbers may in fact have incorrect results like these:\n#\tHave a warning if there is an incompatibility incompatibility on any input.\n#\tInvestigate this.\n#\tIf the input string contains an unexpected digit (e.g. 0x1) then \n#\twe have an interpretation that could not be repaired, e.g. the \n#\tinteger 3 as the input is not a valid integer.\n#\tThe reason behind this will include additional zeros to end all but \n#\tthey should be filled out.\n#\tThat"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tif number < -32 or number > 127:\n\t\traise ValueError(\"Invalid number (must be between -32 and 127)\")\n\t\n\tif not isinstance(alphabet, list):\n\t\traise ValueError(\n\t\t\t\"Invalid alphabet, got {}.\".format(alphabet.__name__)\n\t\t)\n\t\n\t# Let us find out what the minimum input value should be\n\tmin_value = max(0, number - min(number, 0))\n\t# We can now write a byte in the number\n\tstr_ = \"\"\n\n\tif number == 0:\n\t\tstr_ += alphabet[0]\n\t\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tif padding is None:\n\t\tpadding = len(alphabet)\n\t\n\treturn \"0\" + \"\".join([str(number), alphabet[i % len(alphabet)], padding])\n\t\n\ndef string_to_int_with_alphabet_as_int(string: str) -> int:\n\ttry:\n\t\treturn int(string)\n\texcept ValueError:\n\t\traise RuntimeError(\"Could not convert {} to a number\".format(string))\n\t\n\n\ndef str_to_int_with_alphabet_as_int(string: str) -> int:\n\ttry:\n\t\treturn"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t    \n\t# if no padding is given, then make the minimum\n\tif padding is None:\n\t\tpadding = int(math.log10(number))\n\t\n\t# convert the string to the desired alphabet\n\tx = [int(n) for n in number]\n\tx += alphabet\n\t\n\t# convert the number back to the desired alphabet\n\tx += [alphabet.index(x) for x in x]\n\t\n\t# convert them into hex\n\treturn ''.join('{:02x}'.format(n) for n in x)\n\ndef number_to_str(number: Union"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\n\tif isinstance(number, int):\n\t\tif padding:\n\t\t\tif number % alphabet[0:2]!= alphabet[1:2]:\n\t\t\t\traise ValueError(\"Invalid number to be padded.\")\n\t\telse:\n\t\t\tpadding = alphabet[number:2]\n\t\treturn f'{padding}{alphabet[len(number):]}'\n\t\n\tif isinstance(number, (list, tuple)):\n\t\tif isinstance(number, int) and (not isinstance(number, str)):\n\t\t\traise ValueError(\"Invalid number to be a string, given: {}\".format(number))\n\t\tnumber = padding(number"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\n\tnumbers = (str(number)).split(\"+\")\n\tfor number in numbers:\n\t\tif number == \"00\" or number == \"1\":\n\t\t\tnumber = \"\".join(alphabet)\n\t\tif not number:\n\t\t\tcontinue\n\t\tnumber_as_int = re.findall(r\"0*(\\d+)\", number)\n\t\tif len(number_as_int) == 1:\n\t\t\treturn number\n\t\tnumber_as_word = \"0\"\n\t\tfor n in number_as_int:\n\t\t\tnumber_as_word += format(n, '0o')"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n#    if len(alphabet) <= 1:\n#        raise ValueError(\"One or more words required\")\n   \n#    output = []\n    \n    first_bits = int_to_bits(number)\n    second_bits = int_to_bits(alphabet[-1])\n    if first_bits!= second_bits:\n        raise ValueError(\"Non-alphabet characters encountered in number\")\n                \n    for k in range(first_bits-1):\n        output.append(alphabet[k] * int_to_string(second_bits - first_bits))\n    \n    return \"\".join(output)    \n\n\"\"\""}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n# Split sans and lengths and build an integer for the sequence that will\n# fit each alphabet into a fixed output string.\n#\n# The original logic of this function is as follows:\n#\t1. Remove all trailing zeroes from sequence\n#\t2. Remove '0' from sequence\n#\t3. Split string to list\n#\t4. Generate the sequence as one string\n#\t5. Find longest repeated substring of list\n#\n# Output is\n#\t1. The fixed sequence.\n#\t2. The suffix string.\n#\t3. The resulting integer as"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\n\tdef _replace_register_number(flow_param, register_number, modulator):\n\t\tif flow_param.name.endswith('Register' + str(register_number)):\n\t\t\tregister_number = flow_param.value.split(':')[1]\n\t\tval = \"\"\n\t\tfor v in [register_number, register_value]:\n\t\t\ttry:\n\t\t\t\tval += _val_to_ieee_register_value(modulator, v)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t \tval += flow_param.value + '|' + str(v)\n\t\t\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\t# create the new flow.\n\tnew_flow = Flow(\n\t\t\tidentifier = flow_params['flow_identifier'],\n\t\t)\n\tflow_params['flow_identifier'] = new_flow.identifier\n\n\t# create the new register.\n\tregister_key = _get_value(flow_params['register_number'], register_value)\n\n\tif register_key in flow_params['flows_dict']:\n\t\t# replace the string value.\n\t\tflow_params['flows_dict'][register_key] = flow_params['flows_dict'][register_value]"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tdef replace_reg(flow_index, register_number, register_value):\n\t\t\"\"\"\n\t\tReplaces given register with given integer from flows if it exists\n\t\t\n\t\t:param flow_index: Flow index\n\t\t:param register_number: Reg number\n\t\t:param register_value: Key to be replaced by register number\n\t\t:return: Flow number stored and with replaced register, value stored (or not set)\n\t\t\"\"\"\n\t\t# if register_value is not set remove old value\n\t\tif(register_value is None):\n\t\t\t# check if flow number is defined\n\t\t\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tassert  flow_params[\"register_number\"] == register_number\n\tassert flow_params[\"register_value\"] == register_value\n\t\n\treturn _replace_register_helper(flow_params, register_number, register_value, _replace_register)\n\ndef _replace_register_helper(flow_params, register_number, register_value, key_value, _replace_register):\n\t# TODO\n\t#    This is not a good idea (because it is not used in all functions)\n\t\n\t# get the name of the register with values from the key value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tif not flow_params.has_key(flow_params_key):\n\t\tflow_params[flow_params_key] = {}\n\t\t\t\n\tif register_number in flow_params[flow_params_key]:\n\t\tdel flow_params[flow_params_key][0]\n\t\tflow_params[flow_params_key][register_number] = Register(flow_params[flow_params_key][register_number] + register_value)\n\t\treturn True\n\telse:\n\t\tprint \"register number '\" + register_number + \"' does not exist!"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tflow = flow_params['flow']\n\tregister = flow_params['register']\n\t\n\tadd_flow = register.copy()\n\tadd_flow[register_number:] = register_value\n\tadd_flow[-register_number:] = reg_add_value\n\t\n\treg = flow['reg'].copy()\n\treg[register_number:] = register_value\n\t\n\tadd_flow_with_length = reg_add_value.copy()\n\tadd_flow_with_length[register_number] = register_value\n\n\treturn _replace_register_to_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\n\t# Find value to replace based off the value of register number\n\tif flow_params.has_key(register_number):\n\n\t\t# Create lookup entry using register number as key\n\t\n\t\n\t# Find value to replace based off the value of register number\n\ttry:\n\t\tflow_params[register_number] = register_value\n\texcept:\n\t\tpass\n\n\t\t\ndef _find_next_flow(flow_params, num):\n\t\"\"\"\n\t\tFind the proper flow corresponding to the given flow key\n\t\n\t'num' - The flow number to be searched\n\t\n\t:param flow"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\t\n\tindex = flow_params.index(register_number)-1\n\treturn flow_params[:index] + register_value\n\t\t\n\t\n\t\t\n# Copyright 2010 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t #print 'flow_params',flow_params,register_number,register_value\n\t\t\n\t\tflow_number = register_number+1\n\t\t#print' register_number:',register_number,register_value\n\t\t\t\n\t\t#print'register number:',register_number,':',register_value\n\t\t\t\n\t\tif flow_number in flow_params.keys():\n\t\t\tfor value_name in flow_params[flow_number].keys():\n\t\t\t\tif len(flow_params[flow_number][value_name]) == 2:\n\t\t\t\t\t#print' ',flow_params[flow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tresult = {}\n    old_list = flow_params.get(register_number, [])\n    old_dict = flow_params.get(register_number, {})\n    for i in range(old_list.count(register_value)):\n        if old_list.count(register_value)[i] == register_number:\n            old_dict[register_value].append(i)\n        else:\n            old_dict[register_value] = [i, register_number]\n\tflows_to_update = old_list[:i] + old_list[i+1:]"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n# function to replace dots used in the message before the callback function\ndef replace_dots_func(message, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif isinstance(message, str):\n\t\tmessage = message.replace(\".\", \"/\")\n\tvalue = message\n\t# get the path of the target file if it exists\n\tif os.path.exists(value) == False:\n\t\t# create the value string\n\t\tvalue = str(os.path.join(arg, os.path.pardir))\n\t\t# make"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\ndef add_trailing_slash():\n\t\"\"\"\n\tAdd a trailing slash to the end of the given string if it contains one.\n\t\"\"\"\n\tif type(value) is str:\n\t\treturn \" - \" + value\n\tif type(value) is not type(0):\n\t\treturn value + \" \"\n\treturn value\n\ndef add_quotes():\n\t\"\"\"\n\tInsert a string with quotes.\n\t\"\"\"\n\treturn \"' \\\"'\" + get_quote_left()+ \"'\"\n\ndef _replace_single_back_space(str):\n\t\"\"\"\n\t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t try:\n\t\t if str(value).find('.')!= -1:\n\t\t\t\tt = \" \".join(value.split(\".\")[:-1])\n\t\t\t\treturn t\n\t\texcept:\n\t\t\treturn str(value)\n\telse:\n\t\treturn str(value)\n\n_filter_re = re.compile(r'(?<!\\\\),')\n\ndef is_in_path(path, search_path=None, file_paths=None, case_sensitive=False):\n\t'''Tests if input string is an in_path / file or a string'''\n\tif not '://' in path:"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t # if arg is present, replace all the '.'\n\t # otherwise the arg is not actually removed\n\t # with the correct value (without quotes)\n\tif arg.lstrip('.') in (None):\n\t\treturn value\n\tif not value:\n\t\treturn\n\t# if there is an odd number of backslashes\n\t# remove it and add 1\n\tescaped = ('.' + arg) * len(value)\n\targ = re.sub(r\"[\\/\\\\]\", r\"\\\\n\", escaped)\n\targ = re.sub(r\"[\\/\\\\]\", r\"\\\\n\", arg)\n\t# if"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t \n\t# arg contains all the dots to replace, each value is a string value\n\t# split each string into a list, join each string of space separated by new line character\n\t# arg values are separated by newline \n# comma can be quoted, but should be quoted by double quotes and quote by single quotes\n\t# we are going to deal with that and not worry about spaces we have\n\tstring = str(value)\n\tspaces = ['' ] \n\tsplits= ['', '\\t' ] \n\tspaces[0]= '' \n\tsplits[0]= string.split(value"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t #print \"replacing to \", arg\n\t\treturn re.sub('[ \\?]', '_', re.sub('\\\\', '', value))\n\t #string = str(value)\n\treturn arg + str(value)\n\t\t\t\n\n#this function is the main one\ndef run():\n    \"\"\"\n \tthe script\n \t\"\"\"\n    return 0\n\ndef exit(status): # exit with errorcode = 0\n    \"\"\"\n    exit with errorcode = 0\n    \"\"\"\n    exit(status)\n    return\n\ndef main():\n    global p, commandLine\n    #set command line arguments\n    arguments = sys.argv\n    #read"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\tif isinstance(value, str):\n\t\t\t# Convert a unicode string\n\t\t\tvalue=value.decode(encoding, \"ignore\")\n\n\t\tif isinstance(value, unicode_type):\n\t\t\tvalue=value.encode(encoding, \"ignore\")\n\n\t\tif isinstance(value, unicode):\n\t\t\tvalue=value.encode(encoding, \"ignore\")\n\n\t\treturn str(value)\n\telse:\n\t\treturn arg\n\ndef dict_to_unicode(arg):\n\t\"\"\"\n\tTransform a dictionary that contains unicode values to unicode\n\t\"\"\"\n\tif not isinstance(arg, dict):\n\t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n\n\timport re\n\n\t# replace all occurance of a '.' to''\n\treturn re.sub( '\\.|.|\\.|\\..|,', \n\t             '\\\\1.', \n\t             str( value ), \t   )\n\ndef replace_dashes(value, arg):\n\t\"\"\"\n\treplaces all values of '-' to arg from the given string\n\t\"\"\"\n\t\n\n\timport re\n\n\t# replace all occurance of a '-' to''\n\treturn re.sub( '\\.|\\.|\\..|,', \n\t             '_',  val, \n\t             )\n\n##########################################################\n#"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n#\tprint(\"Replacing dots in the input string\")\n\treturn re.sub(r'[^\\x00-\\x7e]+', '_', value)\n\n#\tprint(\"Replaces dots in input string\")\n\ndef remove_quotes(value):\n\t\"\"\"\n\tReplaces all value of '\" at the begining of \"' to''\n\t\"\"\"\n\treturn value.replace('\"', '')\n\n# def format_date(date):\n# \tnew_date = datetime.datetime(*time.strptime(date, \"%Y-%m-%d\").timetuple()[:5])\n# \t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t # replace each argument into its corresponding index\n\t\tfor i in range(0, len(arg)):\n\t\t    arg[i] = re.sub(r'^'+arg[i] + r'$', '/', arg[i])\n\n\t# Replace first occurence of the last occurence of the given string\n\tfor i in xrange(0, len(arg)):\t\n\t\targ[i] = re.sub(r'\\.'+arg[i], r'\\.', arg[i])\n\n\treturn arg\n\t\t\t\n\t\t\t\ndef reformat_strings(strings):\n\t\"\"\"\n\tReformat all"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t \n\tdef get_subclasses(cls):\n\t\tsubclasses = []\n\t\tfor child_cls in inspect.getmembers(cls, inspect.isclass):\n\t\t\tsubclasses.extend(get_subclasses(child_cls))\n\t\treturn subclasses\n\n\treturn get_subclasses(cls)\n\n\ndef get_args(cls):\n\t\"\"\"\n\tGet the list of arguments to pass to build-tests and build-all\n\n\targs - argparse.Namespace or argparse.Namespace object\n\t\"\"\"\n\tassert(type(cls.args) is argparse.Namespace)\n\treturn cls.args.items()\n\n\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\n\n\tdef findall(cls):\n\t\t\"\"\"Return all subclasses, recursively. \"\"\"\n\t\treturn [subclasses(x) for x in cls.__subclasses__()]\n\n\n\nclass TestCase:\n\n\t\"\"\"A base class representing a test case executed on an individual\n\tcase run. This function represents a single test case:\n\t- a single test case is subclassed from TestCase and tests\n  \t- that class represents a single test case\n\n   The main idea of the test case is to define a basic command line\n     to execute commands. Typically, the command is identified\n    \tthrough a list of commands, eg:\n\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n \tsubclasses = [cls, LazySubclass(Parent), LazySubclass(Child)]\n \tsubclasses.reverse()\n  \n \tsubclasses = tuple(subclasses)\n \tsubclasses += tuple(klass.__subclasses__() for klass in cls.__subclasses__())\n  \n \tfor subclass in subclasses:\n\t\tyield subclass\n\n# ------------------------------------------------------------------------------\nclass _LazyInheritTree(object):\n\tdef __init__(self, items = None, parent = None):\n\t\tself.__dict__['_children'] = []\n\t\t\n\t\tif len(items) > 0:\n\t\t\tfor item in items:\n\t\t\t\tself[item._id] = item\n\t\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\n\tm=cls\t# base name\n\tif m in cls.subclasses:\n\t\treturn m\n\n\tsubclasses=[]\n\tfor s in cls.__subclasses__():\n\t\tsubclasses.append(s)\n\n\tfor s in subclasses:\n\t\ts.subclasses=subclasses\n\treturn subclasses\n\n\n# TODO: fix this so we don't need a global cache\nclass cache_info(object):\n\t\"\"\"\n\tCache info for a single source file\n\n\tBy convention the following attributes come from the\n\tsource file:\n\n\t\t['line_number','module_name','hash_value',\n\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\nclass Spell:\n\tdef __getattr__(self, attr):\n\treturn lambda *args, **kwargs: setattr(self._baseclass, attr, *args, **kwargs)\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tself._method = getattr(self._baseclass, \"__%s__\" % args[0])\n\t\treturn args[0]._method(*args, **kwargs)\n\t\n\tdef __len__(self):\n\t\treturn len(self._bases)\n\t\n\tdef __getitem__(self, index):\n\t\tresult = self._bases[index]"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\t\n\t_subclasses = []\n\t\n\tdef _getClass(c):\n\t\t\"\"\"\n\t\tif a class instance is given as \"c\"\n\t\t\"\"\"\n\t\t\n\t\tcls = None\n\t\tfor c in c.__mro__:\n\t\t\tif c is cls:\n\t\t\t\tcontinue\n\t\t\t\n\t\t\ts = c.__module__ + '.' + c.__name__.__name__ + '.' + c.__qualname__\n\t\t\t\n\t\t\t\n\t\t\tif inspect.isfunction(s):\n\t\t\t\ts = s(c)\n\t\t\t\n\t\t\tif re.match(r'^\\s(_|$)', s):\n\t\t\t\tcls = s\n\t\t\t\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\t\n\t# Get all subclasses of a class, recursively\n\tsubclasses = [x for x in dir(cls) if isclass(getattr(cls, x, None))]\n\n\t# Return all subclasses and all non subclasses\n\tsubclasses.append(cls)\n\tsubclasses.extend(filter(lambda x: not x.startswith('_'), subclasses))\n\t\n\treturn subclasses\n\ndef flatten(x):\n\t\"\"\" Flatten an arbitrary object\n\t\"\"\"\n\t\n\tresult = []\n\t\n\tfor k in xrange(len(x)):\n\t\tif isinstance(x[k], list):\n\t\t\tx_result ="}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\nclass OCString(string.String):\n\t\"\"\"\n\tAn OC String class.\n\tClass has all methods that access an index\n\tinstead of name.\n\t\"\"\"\n\t\n\t# String.str2object() is much faster than str.atof(),\n\t# but we need an index.\n\t\n#\tclass Index(object):\n#\t\t\"\"\"\n#\t\tA simple class for Index objects.\n#\t\t\"\"\"\n#\n#\t\tdef __init__(self, name=\"\", value=None):\n#\t\t\t\"\"\"\n#\t\t\tCreate a new Index"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\n\tdef flatten(self, l):\n\t\t\"\"\"\n\t\tFlatten list\n\t\t\n\t\t:type l: list\n\t\t:rtype: list\n\t\t\"\"\"\n\t\n\n\tdef append(self, l, item):\n\t\t\"\"\"\n\t\tAppend item\n\t\t\n\t\t:param l:\n\t\t\tlist  --\tList of attributes / classes\n\t\t\t-\tString  --\tString\n\t\t:type item: object\n\t\t:rtype: object\n\t\t\"\"\"\n\t\tfor attr in self.attributes:\n\t\t\titem2 = attr.expand()\n\t\t\tif item in item2:\n\t\t\t\tl.append(item)\n\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tfor c in C.__subclasses__():\n\t\t\tyield c\n\n\treturn __subclasses__\n\n#--- Impl class ---\nclass Impl(object):  # noqa\n\t\"\"\"\n\tImpl\n\t\"\"\"\n\t__all__ = ('Interface', 'InterfaceClass', 'InterfaceBase', 'InterfaceInterface',)\n\n\tdef __new__(cls, pycore):\n\t\treturn Impl.__new__(cls, pycore)\n\n\tdef __reduce__(self):\n\t\treturn _make_repr(type(self), ())\n\n\t# -- ClassInterface ---------------------------------------------------------------------\n\n\tdef __classinit__("}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tint_chars = alphabet[1:]\n\t\n\treturn int(number_to_ordinal(str(int_chars.index(string))))\n\ndef string_to_number(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tint_chars = alphabet[1:]\n\t\n\treturn int(number_to_ordinal(str(int_chars.index(string))))\n\ndef string_to_roman("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tstring = string.lower().strip()\n\tfor n in alphabet:\n\t\tif n not in string:\n\t\t\traise ValueError(\"alphabet contains unknown characters %r\" % n)\n\t\n\tnumber = 0\n\tfor numeral in string:\n\t\tnumber = number * len(numeral) + numeral\n\treturn int(number)\n\n##\n# Emacs-like \"strings\" are used here because string ordering\n# can be in any order and can match the string-to-integer conversion rules.\n# Please refer to the \"string-to-integer\" utility function for\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n#\talphabet = alphabet[0]\n\tsequence_type = string[0] # str\n\tinteger_representation = string[1:len(string)] # str\n\tinteger_representation = int(integer_representation, 16) # int\n\tinteger = int(integer_representation) # int\n\tstring_representation = str(int(string_representation)) # int\n\t# 1-index_of_digit[] returns zero-based ordinals of digits...\n\talphabet.append(integer_representation)\n\n\t# 0-index_of_digit[] returns zero-based ordinals"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\t# Split by the spaces for each element\n\tstr_strings = []\n\tfor each_character in string:\n\t\tstr_strings.append(alphabet.index(each_character))\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\ndef make_valid_text(string: str) -> str:\n\t\"\"\"\n\tGet a valid ASCII string according to what is presented in the 'alphabet'\n\targument.\n\t\n\tIf all characters in the given string are valid a valid ASCII string will be returned.\n\t\"\"\"\n\t\n\t# Use regular expression to find all"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\n\t# the alphabet must contain at least two non-zero letters\n\t# so if a character appears twice, then the alphabet will be empty\n\tif not len(alphabet) == 2:\n\t\traise ValueError(\"The alphabet must contain at least two non-zero \"\n\t\t\t\t\t\t\t\"letters\")\n\n\tx = [ord(c) for c in string]\n\tx.sort()\n\n\treturn max(alphabet) if x[-1] == '0' else int(x[-1])\n\n\ndef string_to_int_list(string: str, alphabet: List[str]) -> ["}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\n\t# Convert to string representation (used by string_to_int)\n\treturn sum([int(s, radix=alphabet.index(c)) for s, c in zip(string, alphabet)], [][::-1]).index(str(string))\n\t\n\ndef decimal_to_str(f: decimal.Decimal)->str:\n\t\"\"\"\n\tConvert a decimal number and the input value into a string.\n\t\n\tThe returned value is a string of the following format:\n\t\t\n\t\t1000000000    3000000000\n\t\t12000000000    23000000000\n\t\t12345678000000000   0000000000\n\t\t1234567000000000   "}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n#   1) Make sure it's a positive integer.\n\tis_positive = lambda c: ord(c) >= 0\n\talphabet = list(map(is_positive, alphabet))\n#   2) Trim the given alphabet.\n#   3) Remove all the letters from the alphabet.\n#   4) Remove any duplicated letters.\n\tnum_alphabet = len(alphabet)\n\talphabet_index = list(range(num_alphabet))\n\talphabet_index_removed = list()\n\tfor i, c in enumerate(alphabet):\n\t#   3) Do"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\t# ToDo: Consider encoding error?\n\tfor i in range(len(alphabet)):\n\t\tif alphabet[i] == '0' or alphabet[i] == '1':\n\t\t\talphabet[i] = '1'\n\talphabet.sort()\n\treturn int(\"\".join(alphabet), 2)\n\t\t\n\"\"\"\nCreate a new word or return if the word is too short.\n\"\"\"     \ndef new_word(word: int, alphabet: List[str]) -> str:\n\tif len(word) > 2:\n\t\treturn '0' + word[-2:]"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\t\n\tif not isinstance(string, str): # type: ignore\n\t\traise TypeError('Given message is not of type str. {0!s}'.format(type(string)))\n\n\tint_result = 0\n\t\n\tfor char in string:\n\t\t\n\t\tint_result += alphabet.index(char) + 1\n\n\treturn ord(int_result)from pwn import *\nfrom math import cos, sin, tan\n\nfrom oppin_adjoint import Oppinadjoint\nfrom pwn_parameters import PWMatrix, PWVector\nfrom BroydenQuantumEOMV"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\talphabet = map(ascii_letters.capitalize, alphabet)\n\talphabet = sorted({'A', 'C', 'G', 'T', 'M', 'K', '#', 'J', 'M#', '#J', 'P#', 'O', 'U', 'V', 'X', 'C', 'G#', 'W', 'E'})\n\tnumber = 0\n\twhile True:\n\t\tnum, num_next = string_to_num(string, alphabet)\n\t\tnum += 1\n\t\tif num_next!= 126:\n\t\t\tbreak\n\t\tfor c in alphabet:\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\tarchive_url = url[url.rfind('/')+1:]\n\t  url = archive_url + '/' + url\n\t  response = urllib2.urlopen(url)\n     #\tprint(\"Downloading : \" + url)\n\t    file = open(destination_path + '/' + url, 'wb')\n\t    for chunk in response.read(BUFSIZE):\n\t    #\tprint(\"Adding \"+str(len(chunk))+\"  to archive...\")\n\t    \tchunk_str = open(destination_path + '/' + url, 'ab')\n\t    \tfile.write(chunk_str)  #.tar.gz"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\timport shutil\n\timport zipfile\n\timport tarfile\n\n\ttar = tarfile.open(url)\n\ttar.extractall(destination_dir_path)\n\n\twith open(destination_path + '_extract.tar.gz', 'wb') as f:\n\t\tf.write(tar.read())\n\n\treturn destination_path # return path for package and tarball\n\n\n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\tdownload(url if not url.endswith('/') else url + '/',destination_path,'tar.gz')\n\t\n\t# Extract.tar.gz files\n\twith tarfile.open(destination_path,'r:gz') as tar:\n\t\tfiles = list(tar)\n\t\t\n\t\t# Create a dictionary of name:md5 sums of file contents to number of parts: \n\t\tm = {}\n\t\t\n\t\t# Extract file contents from URL\n\t\tfor i in range (0, len(files)):\n\n\t\t\ttry : \n\t\t\t\t# Get next.tar.gz archive name and path, and use it's directory listing"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\ndef extract_targz(url, destination_path):\n\t\"\"\"\n\tGiven an url and a destination path, extract the tarfile package \n\ttar.gz file contained in each package.\n\t\n\tArgs:\n\t    url: url of the.tar.gz archive to extract\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    the path on disk corresponding to the tarfile package\n\t\"\"\"\n\n\tdef add_files(target_dir, files, filelist):\n\t\tprint \"addfiles  : \", target_dir, \", \", files\n\tfor source_file in filelist:"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\n\ttar_path = url[7:]\n\t\n\ttar_file = tarfile.open(tar_path)\n\ttar_file.extractall(destination_path)\n\t\n\treturn tar_path\n\n\ndef DownloadAndExtractTar(url, destination_path):\n\t\"\"\"\n\tGiven an url and a destination path, extract it into the same archive as the tar file.\n\tThen, if successfull, create a file path out of the same directory where the archive was extracted to.\n\tIf no exceptions are thrown, report the error.\n\t\n\tArgs:\n\t    url: url of the."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\n\ttemp_dir = 'tmp_{}'.format(os.path.basename(destination_path))\n\n\trequest = requests.get(url)\n\twith open(temp_dir, 'wb') as target_file:\n\t\tfor chunk in request.iter_content(chunk_size=65536):\n\t\t\t target_file.write(chunk)\n\n\ttar_path = 'tmp_{}'.format(temp_dir)\n\n\ttar = tarfile.open(tar_path, mode='w:gz')\n\ttar.add(temp_dir, arcname=os.path.basename"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n# main\nif __name__ == \"__main__\":\n\timport sys\n\tif (len(sys.argv) < 2):\n\t\traise ValueError(\"url or destination path required\")\n\t\n\tarchive_files = sys.argv[1:]\n\ttarget_path = sys.argv[2]\n\t\n\ttry:\n\t\tdownload_file(archive_files, target_path)\n\texcept requests.exceptions.HTTPError:\n\t\tsys.exit(\"An error occurred while trying to download file %s \" % archive_files)\n#########\n# Copyright (c) 2014 GigaSpaces Technologies"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\n\tdef download_url(self, url):\n\t\tresp = urllib2.urlopen(url)\n\t\tif resp.headers.get('content-encoding') == 'gzip':\n\t\t\tbuf = StringIO(resp.read())\n\t\t\tf = gzip.GzipFile(fileobj=buf)\n\t\t\tresp.close()\n\t\t\treturn f\n\t\telse:\n\t\t\traise Exception('Invalid response for url', url)\n\n\n\t\"\"\" Return the filename of 'desc' folder \"\"\"\n\t\n\t\"\"\" Returns the name of the folder\"\"\"\n\tdef get_packages(self, package):\n\t\tprint(\"Getting all packages"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n    if url is None or destination_path is None:\n        raise Exception('Please provide a valid url & optionally a destination path')\n    elif url[0:2]<>':\n        raise Exception('invalid URL (should contain - or,)')\n    elif url[0]<>':        \n        source = requests.get(url[2:], verify=True)\n        target = requests.get(destination_path, verify=True) \n        with open(source.filename, 'wb') as f:\n            for chunk in source:\n                f.write(chunk)\n        target.close()\n    #else it is a zipfile."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\n\twith open(destination_path, 'wb') as destination:\n\t\tfor chunk in requests.utils.chunkreadable(url, 4096):\n\t\t\tdestination.write(chunk)\n\"\"\"\nModels for User Information (students, staff, etc)\n\nMigration Notes\n\nIf you make changes to this model, be sure to create an appropriate migration\nfile and check it in at the same time as your model changes. To do that,\n\n1. Go to the edx-platform dir\n2../manage.py lms schemamigration student --auto description_of_your_change\n3."}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\tif os.name == \"nt\":\n\t\tif not utils.macOSList():\n\t\t\tprint(\"Error: os.macOS not found\")\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\tif not utils.macOSList():\n\t\t\tprint(\"Error: os.macOS not found\")\n\t\telse:\n\t\t\treturn False\n\treturn utils.macOSList()==False\n\n########## Check if OS is in a path\ndef OS_is_path():\n\t\"\"\"\n\tCheck if os is os.path\n\t\n\t:return: True if OS is os"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\tversion = platform.uname()[1]\n\tos_cmd = \"sudo macinfo --short\"\n\tsystem(\"sudo /opt/local/bin/hping2-linux -n 1 -c 1 %s\"%(os_cmd))\n\tmacstatus = os.system(os_cmd)\n\tif macstatus==\"MacOS\":\n\t\treturn True\n\telse:\n\t\treturn False\nif macstatus==None:\n\treturn False\nelse:\n\treturn macstatus == macstatus\n'''\nCopyright 2015 Sotera Defense Solutions, Inc.\r\n\r\nLicensed"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n@param(\"param1, <str>: Macro\", \"<str>\")\ndef MACROCheck(param1=1):\n\t\"\"\"\n\tChecks if the parameter is macOS, in a very simplistic manner\n\t:param param1: The parameter to check\n\t\t:type param1: anything\n\t:return: True if it is valid, else false\n\t:rtype: bool\n\t\"\"\"\n\tif sys.platform in [\"win32\", \"mingw32\", \"cygwin\"]:\n\t\tosname = \"macOS\"\n\telse:\n\t\tosname = \"posix\""}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\t\ndef path_exists_in_source(filepath,source):\n\n\t\"\"\"\n\tReturn path exists in the path\n\n\t:param filepath: Path to check\n\t:type filepath: str\n\t:param source: Source that we want to check\n\t:type source: str\n\n\t:return: True if the path exists\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\n\ndef run(command,shell_script=False,log_script=False,working_dir=None,verbose=True):\n\n\t\"\"\"\n\tStarts a shell command\n\t\n\t:param command: command to start"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\ndef macOSCheck():\n\tcmdStr = \"ls -l /sys/class/net -list\"\n\tmacOS = subprocess.Popen(cmdStr,stdout=subprocess.PIPE)\n\tmacOS = macOS.stdout.read().split()\n\tif sys.platform == \"darwin\":\n\t\ttry:\n\t\t\tos_macro = subprocess.Popen([\"/sbin/arp\", \"-s\", \"ARP\", \"ether\", \"-n\"],stdout=subprocess.PIPE)\n\t\t\tmacOS = os_macro.stdout.read().split()\n\t\t\tif len(macOS):\n\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\ndef checkMacOSX1():\n\tif os.uname()[0] == \"Darwin\":\n\t\treturn True\n\telse:\n\t\treturn False\t\n\ndef checkOSX():\n\tosName = os.uname()[4]\n\tif os.path.isfile(\"/usr/bin/osascript\"):\n\t\tprint \"osascript %s ok!\" % osName\n\t\treturn True\n\telse:\n\t\treturn False\t\n\ndef setOSX(osName):\n\tos.system(\"osascript -e \\\"set LC_ALL=en_US.UTF-8\\\" \\\""}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\tif os.name!= 'nt':\n\t\tprint 'This script is for MACOS only, will not check OS'\n\t\treturn False\n\n\tstatus = True\n\n\treturn status\n\n\ndef get_device():\n\t\"\"\"\n\tChecks if the os is Windows.\n\t\n\t:return: True is Windows\n\t:rtype: bool\n\t\"\"\"\n\n\t\n\tstatus = True\n\t\n\tos.system('python3 '+get_os_arch())\n\n\treturn status\n\n\ndef get_os_arch():\n\t\"\"\"\n\tGets the OS architecture.\n\t\n\t:return: OS architecture\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\tosMAC = False\n\t\n\ttry:\n\t\t# linux\n\t\tosMAC = open('/etc/diamond/settings.conf', 'r')\n\t\tosMAC_conf = osMAC.readlines()\n\t\tosMac_conf = [os.path.split(line)[1] for line in osMAC_conf]\n\t\tosMAC_conf.sort()\n\t\n\texcept:\n\t\t# mac\n\t\tosMAC = False\n\t\n\tif osMAC:\n\t\t# linux\n\t\tosMAC_conf = re.sub(r'(^.+\\.){1,3}(.*?)','"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\t# Check for OS\t\n\tif os.name == \"nt\":\n\t\tMACOS=\"/tmp/dss/pyMacOS-win32/WinNT/MacOS/MacOS32.app/Contents/Resources/Python/Python-4.6/Pygame/Config/MacOS.app/Contents/MacOS\"\n\telif os.name == \"posix\":\n\t\tMACOS=\"/tmp/dss/pyMacOS-posix/MacOS/MacOS32.app/Contents/Resources/Python/Python-4.6/Resources/Python-4.6."}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\treturn\n\nfrom os.path import isfile, join\n\nfrom.BaseParser import BaseParser\n\n\nif __name__ == \"__main__\":\n\n\tinput = \"1\"\n\toutput = \"out.txt\"\n\tdata = []\n\n\tif not isfile(input):\n#\t\tprint(\"Error in input file!\")\n#\t\texit()\n\t\treturn\n\n\treader = BaseParser(input)\n\tresult = False\t\n\ttry:\n\t\treader.ParseFile()\n\t\t\n\t\tparser = reader.GetParser()\n\t\tif parser is not None:\n\t\t\tdata ="}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tdef transform(item):\n\t\tfor k, itemlist in item.items():\n\t\t\tfor k2, regex in itemlist.items():\n\t\t\t\tval = regex.copy()\n\t\t\t\tfor i, item in enumerate(regex.items):\n\t\t\t\t\tregex[RegexKeys.REGEX_K_INDEX_INDEX[i]] = i\n\t\t\tresult = regex_re.sub(k.pattern, val)\n\t\t\tif result!= regex_re.pattern:\n\t\t\t\traise RegexMatchCompileError, \"{0}:\\t{1}\".format(type(item.pattern), regex)\n\t\t\telse"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t    \n\tregexes = {}\n\tfor key, value in item.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tregexes[key] = re.compile(\"(\" + key + \")(\" + str(value['a']) + \")\")\n\t\telse:\n\t\t\tregexes[key] = re.compile(\"%(\" + key + \")(\" + str(value) + \")\")\n\treturn regexes\n\ndef regex_dict(dict):\n\tr\"\"\" Convert a regex filter string into a python (re.RegexObject)\n\t\n\tArgs:\n\t    dict: a dict with the"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\t#print('Processing Regexes:')\n\t#pprint.pprint(regex_keys)\n\t\n\t#pprint.pprint([regex for key, value in regex_keys.iteritems()], stream=sys.stderr)\n\t\n\t#regex_regex = re.compile(r'[^a-zA-Z0-9_-]+')\n\t\n\tresult = dict()\n\tfor key in regex_keys.keys():\n\t\tresult[re.escape(key)] = value\n\t\n\treturn result\n\ndef get_regex_searches(regex_dict, rules, item, ignore=None):\n\t\"\"\"\n"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tregex_dict = build_regex_dict(re.escape(i) for i in regex_dict.keys())\n\t#build_regex_regex_dict(regex_dict)\n\t\n\t#remove keys that have a match\n\tregex_dict = regex_dict.keys()[2:-1]\n\t#build regex_regex_regex_dict\n\tfor k in regex_dict.keys():\n\t\tregex_regex_regex_dict = re.compile(k)\n\t\tregex_regex_regex_dict = regex_regex_dict.findall(k)\n\t\t#"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tpattern = re.compile(fnmatch.translate(r'.*[.]([a-z0-9-]+)'))\n\tres = dict() #result of regex search\n\tfor key, value in item.items():\n\t\tvalue = re.sub(pattern, key, value)\n\t\tres[value.lower()] = value\n\treturn res\n\n\n#print regex_keys('[abc]')\n#print regex_keys('')\n\t\n\n\t\n\t\n\t\n#print regex_keys('a*')\n#print regex_keys('c*')\n#print regex_keys(';')\n\t\n\n#print regex_keys"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tresult = {}\n\t\n\tfor key in list(searchall(search_pattern, item)):\n\t\tresult[key] = re.compile(search_string_pattern % item[key], re.IGNORECASE)\n\t\n\treturn result\n\n\n\"\"\"\n\tCheck whether a source file is present.\n\t\n\tArgs:\n\t    item: dict to check.\n\t\n\tReturn:\n\t    True is file exists.\n\t\"\"\"   \n\t\ndef check_file_exists(item):\n\t\"\"\"\n\tCheck whether a source file is present.\n\t\n\tArgs:\n\t    item: dict to check.\n\t\n\tReturns:\n\t   "}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tmatches = copy.deepcopy(item)\n\t\n\t# for each regex key, find the actual value\n\treal_keys = []\n\tpattern_keys = []\n\tvalue = None\n\t\t\n\tfor x, r in replacements.items():\n\t\tfor key, value in r.items():\n\t\t\tif value:\n\t\t\t\treal_keys.append({key: value})\n\t\t\telse:\n\t\t\t\tpattern_keys.append(key)\n\t\n\tfor x in pattern_keys:\n\t\tfor key, pattern in x.items():\n\t\t\tvalue = pattern[1]\n\t\t\tkey = re."}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tresult = dict(item)\n\tfor key in item:\n\t\ttry:\n\t\t\tresult[re.compile(key)]=item[key]\n\t\texcept:\n\t\t\tresult[key]=-1\n\treturn result\n\ndef parse_config(config_file='configure.yml', item_file='item.yaml', strict=False, sort_keys=True):\n\tif strict:\n\t\tassert os.path.isfile(config_file)\n\t\twith open(config_file) as f:\n\t\t\tconfig = yaml.load(f, Loader=yaml.Safe"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\n\trule_re = re.compile('\\$\\w+=\\w+')\n\tdef parse_rules(rules):\n\t\trules = rule_re.sub(lambda m: {re.findall(k, m)[0]: v for k, v in m.items()}, rules)\n\t\treturn rules\n\trules = parse_rules(rules)\n\tregex_re = re.compile('\\$\\w+')\n\t\n\tclass Pattern:\n\t\tdef __init__(self, *args):\n\t\t\t# the keys are matched against pattern, if anything. \n\t\t\tself.keys = []\n\t\t\tfor"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tnew_key_rex = {}\n\tfor key, value in rule_key_split.items():\n\t\tif value is None:\n\t\t\tcontinue\n\t\tnew_key_rex[key] = {}\n\t\tfor k,v in value.items():\n\t\t\tnew_key_rex[key][k] = re.compile(k)\n\t\n\tkeys_regex = {}\n\tfor key, value in new_key_rex.iteritems():\n\t\tfor k in keys_regex:\n\t\t\tv = key.replace(k, '|'.join(keys_regex[k]))\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n\t\n\tfor item in split(msg, '\\\\'):\n\t\tname = item[1:-1]\n\t\tif len(name.strip()) == 0:\n\t\t\tcontinue\n\t\t\n\t\ti = 0\n\t\n\t\twhile i < len(name):\n\t\t\tif i >= len(name) - 1:\n\t\t\t\tbreak\n\t\t\tif name[i] == '\"':\n\t\t\t\tif (name[i + 1] == '\"\"\"') or (name[i + 1] == \"'''\"):\n\t\t\t\t\tname = name[:i + 1] + name[i + 2:]\n\t\t\t\t\tcontinue\n\t\t\t\telif (name[i]"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\nclass _GetText(object):\n\n\t'''\n\tGet the text part without including quotation marks.\n\t'''\n\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself.text = None\n\t\tself.tag = None\n\t\tself.pos = -1\n\n\tdef _getText(self):\n\t\ttext = ''\n\t\tfor c in self.text:\n\t\t\tpos = self.pos\n\t\t\tif c == '\\n':\n\t\t\t\ttext += '\\n'\n\t\t\t\tself.text = self._getQuotedString()\n\t\t\t\treturn text\n\t\t\telif"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\nclass NameFixer:\n\t\"\"\" This fixer replaces quotation marks to the following unicode character: \n\thttps://gist.github.com/vamilk1/8e6c5db0acaa5b4db17a4ec2c7/source/tip/unicode-quotation-marks.js \n\t\"\"\"\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\t\n\tdef fixIdentifiers(self, text):\n\t\t\"\"\" Fix the identifiers to avoid problems in javascript\n\t\tUsed to make \"fixing\" certain characters.\n\t\t- Replace \"<"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n\t\n\tdef make(self, word):\n\t\t\"\"\"\n\t\tRemoves an excerpt from the given word.\n\t\tEdits:\n\t\t\t'Why is very strong, do not change and it will not do this again.'\n\t\tReturns:\n\t\t\t'Why is very strong, do not change and it will not do this again.'\n\t\t\"\"\"\n\t\t\n\t\tw = wikipedia.Page(self.site, word)\n\n\t\t# Get the wikipedia article from the page\n\t\tarticle = w.get()\n\n\t\t# Get the quote from the article\n\t\tquote = article.get()\n\t\t\n\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n#def remove_quote(name):\n#\t\"\"\"\n#\tRemove all quotes of a name from the names.\n#\tThe syntax is according to the XPath specification.\n#\"\"\"\n#\tpattern = re.compile(\"^'[a-zA-Z0-9\\+./:=?]+\")\n#\tfor item in pattern.findall(name):\n#\t\tm = regex.search(item)\n#\t\tif m is not None:\n#\t\t\tprint \"Searching for quote '%s'\" % item.encode(\"utf-8\")\n#\t\telse"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n# \t# Replace string values from string with\n# \t\t# the given names.\n\n# \t# Use re.sub to replace strings at runtime with\n# \t\t# the result\n\n# \t# Replace string values on string with\n# \t\t# the given names.\n\n# \t# Use re.findall to get substring occurrence\n\n# \t# Use re.search to obtain substring occurrences\n\n# \t# Use regex to search match sequences\n\n# \t# Replace string values for lists etc.\n\n# TODO: What about quotes?\n\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n__author__ = 'Elizabeth A. Fascio <eforcabina@gmail.com>'\n__version__ = '2.0'\n__copyright__ = 'Copyright (c) 2011 Elizabeth A. Fascio'\n__license__ = 'MIT'\n\nimport logging\n\nfrom.models import Quote\n\nlogger = logging.getLogger(__name__)\nclass QuoteManager(object):\n\t\n\tdef get_all(self, username):\n\t\t\"\"\"\n\t\tGet all quotes in the database where the word contain $quote or $quote_short_name\n\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ndef parseInjectFile(fileContents):\n\tinjDict={ }\n\t\n\tinjFile=open(filePath)\n\toutputDict={}\n\t\n\tn=1\n\tl=injFile.readline()\n\twhile l!= '':\n\t\tkey=l.split('=')\n\t\tkeyLength=len(key)\n\n\t\tif(keyLength<5):\n\t\t\tkeyLength=5\n\t\t\tkey=key[0:1]\n\t\t\n\t\tif(keyLength<17):\n\t\t\tkeyLength=17\n\t\t\tkey=key[0:2]\n\t\t\n\t\tl=injFile."}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ntry:\n\tinfile = open(inname)\n\tflines = list(infile.read().splitlines())\n\tinfile.close()\nexcept IOError:\n\traise IOError(\"Couldn't open file \\\"%s\\\". Make sure that \" \\\n\t\t  \"a text file exists and that it is open.\\n\" %(infile))\n\tsys.exit(0)\nfor oldline, newline in izip(flines, flines[1:]):\n\toldline = oldify(oldline)\n\tnewline = newline.replace(oldline, newline)\t\t# undo whitespace"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ttext = unicode(replace(name))\n\t\ttext = text.replace('\\n','')\n\t\t\t\n\ttry:\n\t\tf = open(localFile, 'a')\n\t\tf.write(text)\n\texcept:\n\t\tf = open(localFile, 'w')\n\t\tf.close()\n\t\tsys.exit(-1) \n\n\ndef removeText(originalString):\n\t\n\treturn RE_STRIP.sub(\n\t\tlambda match: RE_SPLIT.sub(\n\t\t\tlambda m: RE_BREAKBREAK.sub(m,repl_chars(match.group(1))),"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\n\tdef createcommandlist(self):\n\t\tself._opts.program = \"\"\n\t\tself._opts.verbose = False\n\t\tself._opts.command = \"\"\n\t\tself._opts.delimiter = \"\"\n\n\tdef getarglines(self):\n\t\treturn shlex.split(self._args)\n\t\t\n\tdef getargv(self):\n\t\tif self._args is None:\n\t\t\tself._args = []\n\t\telse:\n\t\t\tself._args = shlex.split(self._args)\n\t\t\n\t\treturn self._args\n\ndef __cutpath_to_filename(path):\n\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\t__version__ = \"1.22 beta\"\n\t__author__  = \"Giampaolo Rodola' <g.rodola [AT] gmail [DOT] com>\"\n\n\n\n\tdef __init__(self, argv, executable=None, shell=None):\n\t\t\"\"\"\n\t\tIf shell, initialize myself with it's argv.\n\t\n\t\targv: argv are command-line \n\t\texecutable: name of executable executable if not supplied.\n\t\t\t\t   If command-line is a string, the executable is\n\t\t\t\t   executable with shell expansion removed (via the shell\n\t\t\t\t   --as-command"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\nclass CommandSpec:\n\t\"\"\"A command-line argument parser.\"\"\"\n\tdef __init__(self, cmd_args = '', bufsize = 1):\n\t\tself.cmd_args = cmd_args\n\t\tself.bufsize = bufsize\n\t\tself.regex = ''\n\tdef __str__(self):\n\t\treturn''.join(shlex.split(self.cmd_args) + self.delimiter)\n\tdef parse(self):\n\t\tlines = []\t\t\n\t\tfor arg in self.cmd_args:\n\t\t\tline = arg\n\t\t\tif ':@' in line:\n\t\t\t\tline"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\t@staticmethod\n\tdef split(s):\n\t\t\"\"\"\n\t\tSplit a simple command line, returning a 2-tuple containing the program name and arguments.\n\t\t\n\t\t>>> re.split(\"\\\\s*\\\\n[\\\\c]+\\\\t\")\n\t\t['\\\\s', 'this', ':', 'cmd']\n\t\t\"\"\"\n\t\tprogram = \"\"\n\t\tcommand = \"\"\n\t\targ = \"\"\n\t\tfor i, x in enumerate(s.split(\"$\")):\n\t\t\tif i!= 0:\n\t\t\t\targ += x\n\t\t\t\tcontinue\n\t\t\telif arg:\n\t\t\t\tif arg[0] == \"'\":\n\t\t\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\tpass\n\t\t\t\t\n\t\n\tdef split(self, text):\n\t\t\"\"\"Split a str in 'text' on whitespace and return a list of the word\n\t\tcount in the expanded list.\"\"\"\n\t\t\n\t\tif text == '':\n\t\t\treturn []\n\t\t\n\t\telse:\n\t\t\tpos = 0\n\t\t\tif self.whitespace:\n\t\t\t\tpattern = re.compile(r'([\\W\\-]+)$')  \n\t\t\t\tdef m(x):\n\t\t\t\t\treturn pattern.search(text, x)\n\t\t\t\tscanner = re.scanner(m)\n\t\t\t\tlines = [line.rstrip() for"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\t__slots__ = (\"args\", \"mode\")\n\t\n\tdef __init__(self, args, mode = 0):\n\t\t\"\"\"Initializer. Requires the command line arguments ('args') and\n\t\t\t'section() method. 'args' is a list of arguments (arguments\n\t\t     starting with a space) and'mode' is the program mode\n\t\t        (mode)\n\t\t\"\"\"\n\t\tself.args = args\n\t\tself.mode = mode\n\t\tself.opts = None\n\t\tif self.mode!= None:\n\t\t\traise ValueError, \"mode must be None, True, False\"\n\t\t\n\t\tself"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\n\tdef __init__(self, args):\n\t\t\"\"\" Create args with leading separator, and remove whitespace. \"\"\"\n\t\tself.args = args.strip()\n\t\tparts = [s.lstrip() for s in shlex.split(self.args) if s.strip() is not'' and s]\n\t\tself.args = self._concat(parts)\n\t\n\tdef __str__(self):\n\t\t\"\"\" Convert to string in the form of 'this\\tstring'\"\"\"\n\t\treturn str(self)\n\t\t\n\tdef __call__(self, *args, **kwargs):\n\t\t\"\"\" Convert args"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\t__slots__ = (\"shlex\",)  # to mimic shlex.split\n\t\n\tdef __init__(self, shlex, **kwargs):\n\t\tsuper(shlex, self).__init__(**kwargs)\n\t\tif not hasattr(shlex, '__iter__'):\n\t\t\tshlex = shlex.split(shlex)\n\t\t\n\t\tif 'platform' in shlex: \n\t\t\tif re.search(r'^[-\\.]', shlex):\n\t\t\t\tregex2 = re.compile(r'^[-/]+\\S')\n\t\t\t\tregex3 = re.compile(r'^[-~^]\\S')\n\t\t\t\tregex4 = re.compile("}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\t\n\t__slots__ = ('path',)\n\t\n\tdef __init__(self, path):\n\t\t# we ignore trailing arguments from env vars in order to get\n\t\t# all invalid paths\n\t\tfor name in ['platform', 'args']:\n\t\t\tif name in env:\n\t\t\t\tdel env[name]\n\t\t\n\t\tself.path = path\n\t\n\tdef __call__(self, cmdline,  flags,  cwd = None, env = None):\n\t\tpath = os.path.join(cwd or os.getcwd(), self.path)\n\t\tenv = self.env if env is None else env\n\t\t\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tif 'python' in str(type(platform)) or not hasattr(os,'python2.3'):\n\t\traise Exception(\"Unsupported python version\")\n\t\n\tif len(arg)>1:\n\t\traise ValueError('Invalid argument, no more than 1 -- was %r' % arg)\n\targ = shlex.split(arg)\n\t\n\tcmd = arg\n\targv = []\n\t\n\tif len(arg)==0:\n\t\treturn(argv)\n\t\t\n\t# check for python version\n\tif len(sys.version_info)>2:\n\t\targv.append('--version')\n\t\targv"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef decompress(archive_path):\n\t\"\"\"\n\tDecompresses an external tarball (via gzip) by reading it from the repository\n\tof the specified path.\n\t\n\tThe archive_path name is a directory, e.g. /foo/archive/.\n\t\"mydata/archive\" (which must be empty) and doesn't have to be in the\n\tfirst level of the archive.\n\tAn archive that already exists is first created (by unzipping) and then\n\tused to hold the file.\n\t\n\t@param archive_path Path to the archive\n\t\"\"\"\n\ttry"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress(path: str, archive_path: str, target_path: str) -> str:\n\t\"\"\"\n\tPerform uncompression of the archive, or if the archive does not exist.\n\t\n\t@param path: The path to the uncompressed archive:\n\t\t\t\t  - path to the uncompressed archive (with no \".tar\" extension).\n\t@type path: str\n\t@param archive_path: The path to write the downloaded files to.\n\t@type archive_path: str\n\t@param target_path: The path to the uncompressed archive.\n\t@type target_path"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\tuncompress_cmd = \"uncompress --allow-root=/usr/local --no-check-certificate /usr/bin/tar zcvfzcvfzcvfzcvfzfjxf\"\n\tuncompress_cmd_args = shlex.split(uncompress_cmd)\n\tsubprocess.call(uncompress_cmd_args)\n\tuncompress_cmd_args.append(\"-X\", archive_path)\n\t\n\twith os.popen(uncompress_cmd) as command:\n\t\tcmd_output = command.read().split(\"\\n\")\n\t\n\t\tfor line in"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef decompress(archive_path):\n\t\"\"\"\n\tDecompress the archive\n\t\"\"\"\n\tcmd = [\"tar\", \"-tf\", archive_path, \"xfz\", \"-d\"]\n\tretval = subprocess.call(cmd)\n\tif retval!= 0:\n\t\tstderr(retval)\n\n\ndef uncompress(archive_path, dest):\n\t\"\"\"\n\tUncompress the archive\n\t\"\"\"\n\tcmd = [\"unzi\", archive_path, \"-b\", dest, \"-d\"]\n\tretval = subprocess.call(cmd)\n\tif retval!= 0:\n\t\tstderr"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress(archive_path):\n\t# Uncompress it\n\tsubprocess.call(['uncompress', archive_path])\n\t# Delete the archive\n\tsubprocess.call(['rm', '-rf', archive_path])\n\t# Remove the archive's files\n\tshutil.rmtree(os.path.join(settings.STAGING_ROOT, archive_path), ignore_errors=True)\n\t\n\"\"\"\n\tGiven an archive path, compress it.\n\tReturns a file repo url which can be used as origin url.\n\t\"\"\"\ndef compress(archive_path):\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress(archive_path, dest, quiet=False):\n\t\"\"\"Uncompress archive at ``archive_path`` with ``dest``.\n\t\n\tUses ``dpkg -e archive_path`` to decompress the archive over *dest*.\n\t\n\tIf ``quiet`` is True, the file system is written to the working directory\n\tfor insuring, will be left on the way and nothing will be written to disk.\n\t\"\"\"\n\twith open(archive_path, 'rb') as f:\n\t\tdata = f.read()\n\t\tm = re.match(r'[a-f"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\t# Remove the old archive_path first\n\t# This makes sure we're running on a linux\n\tproc = subprocess.Popen([\"ssh\", \"rm\", \"-rf\", \"-rf\", archive_path], shell=False)\n\t\n\t\n\t# We can use this to avoid a race condition if the archive path exists\n\tproc = subprocess.Popen([\"ssh\", \"-O\", archive_path], shell=False)\n\n\t# Now do a pull: make sure we don't have any issues with that (we want a bit of funky)\n\t# We only allow one issue as we have a"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\tif not os.path.exists(archive_path):\n\t\tlogging.debug(\"  [ERROR] Cannot find required archive specified: {0}\".format(archive_path))\n\t\treturn None\n\t\n\tsrc_md5sum = subprocess.Popen([\"md5sum\", \"-c\", archive_path], stdout=subprocess.PIPE)\n\tdst_md5sum = subprocess.Popen([\"md5sum\", \"-c\", \"d\", src_md5sum.stdout.read().decode()]).communicate()[0]\n\t\n\tif dst_md5sum.returncode == 0:\n\t\treturn dst"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef check_origin_repo_status(archive_path):\n\t\"\"\"\n\t\tReturn True if valid archive_path path is on origin repo.\n\t\tIf archive path refers to archive on the origin repo\n\t\tthen return False.\n\t\n\tInput:\n\t\tarchive_path: a str with path to the archive.\n\t\t\n\tOutput\n\t\tTrue if archive_path is on origin repo.\n\t\"\"\"\n\tcmd_result = subprocess.Popen([\"x\", \"archive\", archive_path],stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\tfor line in"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef compressArchive(archive_path):\n\t\"\"\"\n\tCompresses the archive provided as its compressed and decompressed\n\n\tArgs:\n\t\tarchive_path (str) = the path where the compressed archive will be stored\n\t\n\tReturns:\n\t\tReturns a path to the created archive on success,\n\t\t     usually an absolute path.\n\t\n\tRaises:\n\t\tOSError if no compression succeeded\n\t\t\n\t\n\t\"\"\"\n\t\n\tcmd_string = \"tar cvf - \" + archive_path\n\tprocess = subprocess.Popen(cmd_string, shell=True, stderr=subprocess.PIPE)\n\toutput_string"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef get_git_list(path):\n\t\"\"\" \n\tParse a git ls-tree and return a list of files\n\n\t@path: The path to the git repository\n\t@return: A list of the file names and the path\n\t\"\"\"\n\tl_git = subprocess.Popen('git ls-tree -r --name-only HEAD',shell=True, stdout=subprocess.PIPE)\n\tstdout, stderr = l_git.communicate()\n\tl_git.stdout.close()\n\treturn stdout.split()\n\n\"\"\"\n\tFind the files in all dirs of"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef parse(input):\n\tfilelist = []\n\tfor i in input:\n\t\tlst = i.split(\",\")\n\t\tfor j in lst[::-1]:\n\t\t\t#file_name = j.replace(\".xml\", \"_files.txt\")\n\t\t\tfile_name = j.replace(\".xml\", \"_files.txt\")\n\t\t\tfile_name = file_name.lower()\n\t\t\tif file_name in filelist:\n\t\t\t\tfilelist[file_name] = j\n\treturn filelist\n\t\ndef test():\n\tfilelist = parse(argv[1])\n\tif filelist == []:\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef getFilesAndFileNames(path):\n\t\"\"\" \n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\tcommand = \"git ls-tree --name-only --cached --others --exclude-standard \".encode(\"utf-8\") + \\\n\t\t\" --cached --others-only --exclude-standard-ignore=\".format(path)\n\tsubprocess.Popen(command, shell=True).wait()\t\n\tfilesAndFileNames"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef get_status(cmd):\n\tstatus = []\n\tproc = subprocess.Popen(cmd, stderr=subprocess.PIPE, shell=True)\n\tfor line in iter(proc.stderr.readline, ''):\n\t\tif not line:\n\t\t\tcontinue\n\t\telse:\n\t\t\tline = line.strip()\n\t\t\tif line.startswith('??'):\n\t\t\t\tcontinue\n\t\t\telif not line:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tstatus.append(line.strip('\\n').split('/')[-1])\n\treturn status\n#! /usr/bin/env python\n# -*- coding:"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef get_filelist(directory):\n\tfilelist = []\n\tfor filepath in directory:\n\t\t#if filepath[0]!= '.':\n\t\t#\tfilelist.append(filepath)\n\t\tfilelist = filelist + os.listdir(filepath)\n\treturn filelist\n\t\t\t\t\n\"\"\"\n\tGet the names of files under the current path, and sort them by filename (name of file in folder)\n\"\"\"\ndef get_file_names(directory):\n\tfiles = []\n\tfor f in sorted(os.listdir(directory)):\n\t\tfull_filepath = os.path.join(directory, f"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\ndef commit_with_message(message, branch):\n\tlog = subprocess.Popen(\"git log --pretty=format:'%ad [%an]%n%b %s'\", cwd=repo,\n\t\tstdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\n\tlog_lines = log.stdout.readlines()\n\t# log_lines = commit.splitlines()\n\tlast_commit_line = log_lines[-1]\n\treturn \"%s: %s\" %(message, last_commit_line)\n\n\ndef change_"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\tfnames=subprocess.check_output([\"git\",\"lf\",\"git-lf\",\"git-lf\"]).splitlines()\n\n\tfnames=['files.d','files.d/*.txt','files.d/']\n\n\tdnames=['files.d','files.d/a','files.d/b']\n\n\ta=dict()\n\tb=dict()\n\n\tfor n in fnames:\n\t\ta[n]=subprocess.check_output(['git','cat-file',n], stderr=subprocess.STDOUT).decode().replace('; ','').split('\\t')[1]\n\t\tb["}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef get_cmd_output1_to_output2(cmd_output1, cmd_output2, out_suffix, out_path = './.git/cmd_output_to_file_names.txt'):\n\n\tprint(\"Calculating the following files:\")\n\n\tfor filename in get_cmd_output1():\n\t\t\n\t\tfile_name = filename[2:];\n\t\tfile_suffix = \"\"\n\t\tout_file = filename.replace(out_suffix, \"\")\n\t\tprint(\"\")\n\t\t\n\t\tif file_name.lower().endswith(out_file.lower()):\n\t\t\tcontinue\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n# Get git and branch information\nrepo = sys.argv[1]\nsrc_folder = sys.argv[2]\ndest_folder = sys.argv[3]\n\n# Prepare the path for the script to be run from\n# e.g. git-rev-parse --show-toplevel\nargv = [\"./bin/git rev-parse\"]\nfor directory in [\"src\", src_folder, dest_folder]:\n    # Try to parse the git repository and check if the file already exists\n    if not os.path.exists(directory):\n        exit(\"Failed to find %s"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\n\ndef main():\n\tcmd=\"\"\n\twhile cmd!=\"\":\n\t\ttry: \n\t\t\tcmd=raw_input(\"Enter the number of files you would like to filter and/or file name: \")\n\t\texcept NameError:\n\t\t\tcmd=\" \"\n\t\tif cmd==\"1\":\n\t\t\ttry:\n\t\t\t\tresult=os.listdir(f)\n\t\t\texcept:\n\t\t\t\tprint f + \" has no corresponding path\"  \n\t\t\telse:\n\t\t\t\tf=f+str(result)\n\t\t\t\tf=f[:-3]+\"html\"\n\t\t\t\tprint f\n\t\t\t\tfile=open(\"c3"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t\n\t# Get the filename\n\tfilename = os.path.basename(filename)\n\t\n\t# Check the extname option\n\tif os.path.splitext(filename)[1]!= \".py\":\n\t\t\n\t\traise PythranSyntaxError('Filename must be of python form pythran.py')\n\t\t\t\t\n\treturn True\n\t\n\ndef check_format(format_to_test):\n\t\"\"\"\n\tCheck if a given file can be accessed and can properly read the data\n\t\n\tArgs:\n\t    format_to_test: File format identifier to check\n\t\t\n\tReturns:\n\t    A string whether the file can be"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\ndef is_type_supported(filename):\n\t\"\"\"\tCheck if the file is a type that this module allows\n\t\"\"\"\n\tfilepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)\n\t#print(file_regex.match(filepath, filename=filename))\n\tif os.path.exists(filepath):\n\t\tif not os.path.basename(filepath).lower().startswith(\".\"):\n\t\t\treturn False\n\t\tif re.search(type_pattern, filepath):\n\t\t\tif not os.path.exists(os."}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t\n\tif filename.__class__!= str:\n\t\treturn False\n\t\t\n\t# Find the module name\n\tfulldirname = os.path.dirname(filename)\n\t\n\tif os.path.basename(fulldirname) == \"__init__.py\":\n\t\tmod_name = file(fulldirname + \"/__init__.py\").read()\n\t\tif not mod_name == \"__init__.py\":\n\t\t\treturn False\n\t\telse:\n\t\t\tprint \"WARNING. Module name used in file '%s' \" % filename\n\t\t\tprint \"was passed instead of 'init.py' in the following \"\n\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t\n\t# this is the class object.\n\t# It's a proxy, so this must be a class\n\tclass _Plugin_class(object):\n\t\t\n\t\t# TODO: This is so I can say \"import \" when \"import \" is called\n\t#\t\t and a \"class Configurable\" object.\n\t\t\n\t# These two global variables have to be set to True as a reason why\n\t# you can't add those to the constants or variables at the same time\n\tdefault_section ='server'\n\tdefault_option ='server'\n\t\n\t# This indicates we'll use the module"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(filename):\n\t\treturn False\n\telse:\n\t\text, mode = mimetypes.guess_extension(filename)\n\t\tif mode is None:\n\t\t\treturn os.path.isfile(filename)\n\t\telse:\n\t\t\treturn ext in [\".txt\", \".py\",\".pyw\"]\n\t\n\t\n\ndef create_parser():\n\t\"\"\"\n\tCreate and return the argparse parser \n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(prog=\"Check\")\n\tparser.add_argument(\"-d\", \"--dir\", required=True,\n\t\thelp=\"Directory"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\ndef is_extension(filename):\n\t\"\"\"\n\tCheck if the filename extension is allowed\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturn:\n\t    True if supported, False otherwise\n\t\"\"\"  \n\tif filename[-3:] == \".py\":\n\t\tis_py = True\n\telse:\n\t\tis_py = False\n\treturn is_py\n\ndef get_name(filename):\n\t\"\"\"\n\tCheck if the filename is a name of a python module\n\t\n\tArgs:\n\t    filename: Filename to check\n\t\n\tReturn:\n\t    filename if supported, False if not a"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\n\treturn os.path.isfile(filename)\n\n\ndef is_image_type(file_type):\n\t\"\"\"\n\tCheck if the file type matches the file image type.\n\t\n\tArgs:\n\t    file_type: Type to check against\n\tReturns:\n\t    True if match, False otherwise\n\t\"\"\"\n\n\treturn file_type.lower() in ('image', 'png')\n\n\ndef check_if_compatible(filepath):\n\t\"\"\"\n\tCheck if the file exists.\n\t\n\tArgs:\n\t    filepath: Path to the file\n\tReturns:\n\t    True if it was found\n\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\n\tfilename = str(module).split('/')[-1]\n\t\n\t# If it's not there, make it a string and remove any non-alphanumerics before saving\n\tif filename!= \"settings\":\n\t\tfilename = str.replace(filename, \"settings\", \"\").lower()\n\t\n\t\n\t# Check if the file type was recognized correctly\n\tif ((self.filetype == \"csv\") and (filename.endswith(\".csv\")) is False):\n\t\treturn False\n\n\t# File can be a CSV file\n\telse:\n\t\treturn True\ndef check_file(filename):\n\t\"\"\"\n\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\n\ndef make_path_absolute(relative_path):\n\n    \"\"\" Makes absolute path absolute \"\"\"\n\n    path = os.path.abspath(os.path.expanduser(relative_path))\n    path = os.path.abspath(os.path.expandvars(path))\n\n    return path\n\n\nclass Validator(object):\n    \n    \"\"\" Simple Validator \"\"\"\n\n    def __init__(self, filename, value, *args, **kwargs):\n        self._filename = filename\n        self._value  = value\n        self._args     = args\n        self._kwargs   = kwargs\n\n    def __call__(self, var, context"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t\n\tmatch = False\n\t\n\tfor regex in os.listdir(directory):\n\t\tif re.search('.e', regex) and regex!= filename:\n\t\t\tmatch = True\n\t\t\t\n\treturn match\n\t\ndef isTypeCompatible(filename):\n\t\"\"\"\n\tCheck if the filename is type compatible\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t# Check if the directory exists\n\tif not os.path.exists(directory):\n\t\tresult = True\n\t\t\n\telse:\n\t\tif filename.rfind(\".\") == -"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef fromTimestamp(timestamp, ignoreHaveInterval=False):\n\t\"\"\"\n\tConverts a `datetime.datetime` timestamp to a number of seconds.\n\t\n\tIf `ignoreHaveInterval` is True then return None if the timestamp is None.\n\tIf `ignoreHaveInterval` is False then the returned value will be False.\n\t\"\"\"\n\tresult = None\n\tseconds = float(timestamp)\n\tif seconds:\n\t\tif seconds > 15.0:\n\t\t\tresult = datetime.timedelta(0, 0, seconds)\n\t\telif seconds < 1.0:\n\t\t\tresult"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef get_milliseconds(d):\n\t\"\"\"\n\tGiven a datetime.timedelta or number, return a corresponding number or\n\tNone if 'no unit of time'.\"\"\"\n\t\t\n\t(d, s) = str(d.seconds), \n\tif '.' in d: \n\t\td = d.split('.')[1]\n\td = float(d)\n\tif s:\n\t\treturn int(s)*48*60*60+int(d)\n\treturn None\n\ndef get_datetime_from_string(dstring):\n\t\"\"\"\n\tGiven a string of text, return"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\tif not frequency:\n\t\traise ValueError('Frequency not specified')\n\t\t\n\ttry:\n\t\tfrequency = str(frequency)\n\texcept BadFrequency as e:\n\t\traise ValueError('%s is not a valid frequency' % frequency)\n\t\n\tassert frequency == \"week\" # Week names are always used: \"Monday\", etc.\n\t\n\t\n\tif frequency in (\"1\", \"7\"):\n\t\treturn timedelta.from_weeks(week=4)\n\t\n\telif frequency.endswith('d'):\n\t\t# Note: Python's `timedelta.from_days` function cannot be used with negative\n\t\t#"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef parse_freq(freq, allow_none=True):\n\t\"\"\"\n\tGiven a frequency string and a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance.\n\t\n\tIf `allow_none` is True, raise an exception if the frequency cannot be\n\tparsed.\n\t\"\"\"\n\t\n\tif freq in (\"3 weeks\",\"1 day\",\"2 weeks\",\"4 weeks\",\"6 weeks\",\"12 days\",\"1 month\"):\n\t\treturn datetime.timedelta()\n\t\n\tif freq in (\"day\",\"day of week\",\"week of month\",\"week of year\",\"month\",\"month\",\"day of"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\tif isinstance(frequency, str):\n\t\tfrequency = frequency.upper()\n\t\tfrequency = datetime.timedelta(weeks=int(frequency))\n\t\ttry:\n\t\t\tfreq = datetime.timedelta(days=int(frequency))\n\t\t\tfreq = datetime.datetime(*freq)-freq\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Invalid frequency. Must be \" +  frequency)\n\t\n\tnum, unit = frequency.lower().split(' ')\n\thour, minute, second = num.split(':')[:2]\n\t# If no frequency was given, use now()\n\tif not hour:\n\t\t"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef weekday_names(freq_string):\n\t\"\"\"\n\tParse the string with one or more weekday names represented as a list.\n\t\"\"\"\n\tglobal weekday_names\n\tweekday_names = []\n\t\n\tm = re.search(r'[0-9]+([+-]?)+'*(0[0-9]+(\\.[0-9]+)?)', freq_string)\n\tif m:\n\t\t(weekday_nums, weekday_digits) = map(int, m.groups())\n\telse:\n\t\tweekday_nums = set(weekday_digits)\n\treturn weekday_nums"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef is_float(s):\n\ttry:\n\t\tfloat(s)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\ndef find_first_dollarsuit_freq(freq:str) -> datetime.datetime:\n\t\"\"\"\n\tSearches through a string of dollarsuit periods and returns the first of a date\n\tgiven.\n\n\tRaises ValueError if the dollarsuit period cannot be found.\n\t\"\"\"\n\tif not is_float(freq):\n\t\traise ValueError(\"Frequency is not a float\")\n\t\n\tif (len(freq)"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\nclass date:\n\t\"\"\"\n\tTime interval of a year of the form YYYYMMDD.\n\t\n\tTakes an optional time as argument.\n\t\n\tThe time is formatted as (YYYY,MM,DD)\n\t\n\tTo use this for years that need a time in the future it is needed.\n\t\n\t>>> date(2006, 10, 29)\n\tdatetime.date(2006, 10, 29)\n\t\n\tFor years greater than 2008, the month is taken as the start of the\n\tweek and days of the year.\n\t\n\t>>> date(2003, 10, 29, 13)\n\tdatetime."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef get_milliseconds(t):\n\t\"\"\"\n\tReturn time as int in milliseconds, with microseconds if necessary.\n\t\"\"\"\n\ttry:\n\t\tmillis = long(t)\n\texcept ValueError:\n\t\treturn None\n\tseconds = long(round(t.total_seconds() * 1000))\n\tif milliseconds and milliseconds % 10 == 0:\n\t\tmicroseconds_since_midnight = int((milliseconds - milliseconds) * 1000) % 1000\n\telse:\n\t\tmicroseconds_since_midnight = int(milliseconds)\n\treturn seconds, microseconds_since_mid"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\ndef epoch():\n\t\"\"\"\n\tReturns an integer representing the millisecond timestamp of the\n\ttime stamped as a floating point number.\n\t\n\tExample: 459292158.5\n\t\"\"\"\n\treturn datetime.datetime.utcfromtimestamp(float(nntplib._ns() + 373.15))\n\ndef timedelta(num, period):\n\t\"\"\"\n\tReturns a datetime.timedelta instance given a number and a unit of\n\tmillisecond, and frequency 'period'; if the given unit is 'years',\n\treturn a datetime.timedelta object representing 'n', 'h',"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\n\n\n\n\n\n\n\"\"\"\ndef hostInfo(host):\n\t\"\"\"\n\tChecks if the host is reachable through a HTTP (SSL,etc.) port\n\t\n\t:param host: the server string to check\n\t:type host: str\n\t\n\t:return: True if the host is reachable through a HTTP (SSL,etc.) port\n\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\texcept socket.error, msg:\n\t\treturn False\n\ttry:\n\t\t#ssl/ssl.conf#localhost\n\t\ts.settimeout"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\nclass HostHandler():\n\tdef __init__(self, hostip):\n\t\tself.hostip = hostip\t\n\t\t\n\t\tif hostip == '' :\n\t\t\traise Exception(\"hostip Not set\")\n\n\t\tif self.checkPort() == False:\n\t\t\traise Exception(\"Port is NOT set at host %s\" % hostip)\n\t\n\tdef checkPort(self):\t\n\t\ttry :\n\t\t\tself.s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tself.s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSE"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef check_if_name_is_ip(hostname):\n\t\n\ttry:\n\t\tsocket.inet_aton(hostname)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\ndef check_if_hostname_exists(host):\n\tip_address = socket.gethostbyname(host)\n\tif isIPAddress(ip_address):\n\t\treturn True\n\ndef check_if_port_exists(host):\n\t\"\"\"\n\tCheck to see if the port exists\n\t\n\t:param host:\n\t:return:\n\t\"\"\"\n\tPORT = 22\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\tif host is None:\n\t\treturn False\n\t\n\telse:\n\t\ttry:\n\t\t\tsocket.gethostbyname(host)\n\t\texcept socket.herror, e:\n\t\t\tif ( e.args[0] == socket.EAI_NONAME ):\n\t\t\t\treturn True\n\t\n\tif DEBUG:\n\t\tprint \"Host is a local host.\"\n\t\n\treturn False\n\t\ndef is_local(host):\n\t\"\"\"\n\t\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\tif re.search(r'^(http|https)',host) == None:\n\t\treturn False\n\t\n\tif re.search(r'^www.',host) == None:\n\t\treturn False\n\t\n\tif re.search(re.escape(host),\"localhost\") == None:\n\t\treturn False\n\t\n\tif re.search(r'^127\\.0\\.0\\.1',host) == None:\n\t\treturn False\n\t\n\tif re.search(r'^0.0\\.0.',host) == None:\n\t\treturn False\n\t\n\tif re.search(r'^(80|443)',"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef hostname():\n\ttry:\n\t\ttry:\n\t\t\thost = socket.gethostname()\n\t\t\treturn host\n\t\texcept:\n\t\t\treturn 'localhost'\n\t\n\texcept socket.error:\n\t\treturn 'localhost'\n\t\n\"\"\"\n\tConverts a host\n\treturns ip:address\n\t\"\"\"\n\t\ndef address(host):\n\ttry:\n\t\treturn socket.gethostbyname(host)\n\texcept:\n\t\treturn \"unknown\"\n\ndef format(addr):\n\ttry:\n\t\t(host, port) = addr.split(':')\n\texcept:\n\t\thost, port ="}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\thost = socket.gethostname()\n\t\t# We use all the socket module functionality\n\t\tsocket_ip = ipaddr.IPAddress(host)\n\t\tsocket_name = socket.getfqdnName() \n\t\ttry:\n\t\t\tresult = socket.connect_ex(socket_ip, socket_name) \n\t\texcept socket.error, ex:\n\t\t\tresult = False\n\t\tfinally:\n\t\t\tsocket.close()\n\texcept socket.error, ex:\n\t\tresult = False\n\treturn result\n\t\n\ndef sorry(msg, quit=False, prompt = \"Do you want"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\n\n\t\t\n\tdef init(self):\n\t\t\"\"\"\n\t\tPerforms necessary variables\n\t\t:return: none\n\t\t\"\"\"\n\t\t\n\t\t\n\t\tself.host = None\n\t\tself.port = None\n\t\tself.bind_address = None\n\t\tself.port_to_bind = [self.get_listen_ip()]\n        \tself.password = None\n        \tself.user = None\n\t\t\t\n\tdef get_listen_ip(self):\n\t\t\"\"\"\n\t\tAttempts to connect to a host, if we don't know about it\n\t\t\n\t\t:return: The server IP to connect to, if"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\tdef isHost(self, host):\n\t\t\"\"\"\n\t\tCheck if the host is the localhost\n\t\t\n\t\t:param host: The hostname or ip\n\t\t\"\"\"\n\t\treturn True\n\t\n\t\"\"\"\n\tReturns the port which the server is listening on\n\t:param port: The port in which to listen\n\t\"\"\"\n\tdef getServerPort(self):\n\t\t\"\"\"\n\t\tCheck if the port is what the server was listening on\n\t\t:param port: The port in which to listen\n\t\t\"\"\"\n\t\treturn 8001\n\t\n\t\"\"\"\n\tReturns whether or not the connection opened properly\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\n\n\nclass Host(IPython.InteractiveShell):\n\tdef __init__(self, host, port):\n\t\tself.connection = None\n\t\tIPython.InteractiveShell.__init__(self,\"localhost\",str(port))\n\t\t\n\tdef set_connection(self, connection):\n\t\tself.connection = connection\n\t\tself.execute(\"import connection\")\n\t\n\tdef run(self):\n\t\ttry:\n\t\t\tself.connection.run()\n\t\texcept Exception:\n\t\t\tend_str='No connection yet!'\n\t\t\tself.output+=\"<b>No connection</b><br><br>\"\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\treturn self._find_patterns(path_pattern_generator, self._load_patterns(\"*.txt.*\"))\n\t\n\t\n\tdef find_path(self, pattern):\n\t\t\"\"\"\n\t\tGiven a full path string, return a path fragment matching any patterns found there.\n\t\t\n\t\tFor example, given find_path('foo*.txt').\n\t\tIf you pass the \"pattern\" argument, the matching must be of the form\n\t\t('foo\\\\*', '*\\\\*')\n\t\t\"\"\"\n\t\tif not pattern:\n\t\t\traise AssertionError(\"pattern argument must be of the form '(foo\\\\*)')\"\n\t\t\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tpath_regex = re.compile(r\"'(?P<path>[^']+)' (.*)\"\t\t# e.g.: '/some/path/pp:root/somedir/**' or 'pp:root/somedir' )\n\t# regex_fn:\t\t- returns True if the pattern matches at least one match\n\t# path\t\t\t- path\n\tdef find_paths(path):\n\t\t\"\"\"\n\t\tGiven a path in a glob pattern, return a sequence of all matching paths.\n\t\t\"\"\"\n\t\t# match\n\t\tif path_regex.match(path"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\n\t# Convert glob patterns to glob patterns\n\tpattern_re = []\n\tfor i in range(len(pattern)):\n\t\tif pattern[i]: pattern_re.append(re.compile(pattern[i]))\n\t\telse: raise Exception(\"'%s' is not a pattern pattern\" % pattern[i])\n\n\t# Transform any remaining patterns\n\tfor p in pattern_re:\n\t\tfor f in iglob.glob(p):\n\t\t\tif os.path.isdir(f):\n\t\t\t\tpatterns.append(f)\n\t\t\telse:raise Exception(\"'%s' is not a directory"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\n\n##############\n# The functions below are the specific strings that should be shown in help output\n##############\n\ndef find_directories(d):\n\t\"\"\"\n\tGiven a directory tree search all the path for file or symlink paths and return\n\tthe path of those files.\n\t\n\tArguments:\n\t    d (string): A directory tree search string (relative or absolute)\n\n\tReturns:\n\t    dictionary. The keys are the file names or full paths relative to the\n\t   given dir tree, and the values are absolute paths.\n\t\n\tExamples:\n\t    >>> find_directories(\"foo.txt/bar.txt\")"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tif not paths:\n\t\tpmap = dict() # mapping of existing patterns\n\t\tpaths = [path.rstrip('/') for path in paths]\n\tglob_pattern = []\n\t\n\tif not patterns:\n\t\tpmap = dict()\n\t\n\tfor p,p_regex in pmap.items():\n\t\tpmap[p_regex] = []\n\t\tfor path in paths:\n\t\t\tpmap[p_regex].append(os.path.join(path,p))\n\t\n\tfor p in pmap:\n\t\tassert len(p) >= 2, \\\n\t\t\t'Paths"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tpath_dict = defaultdict(set)\n\t\n\tfor rule in pattern_iter(s):\n\t\ttry:\n\t\t\tpattern_name, pattern = RuleList.split_path(rule)\n\t\texcept ValueError:\n\t\t\t# pattern pattern not matched\n\t\t\tcontinue\n\t\t\n\t\tif pattern in path_dict[rule]:\n\t\t\tpath_dict[rule].add(pattern)\n\t\telse:\n\t\t\t# path list contains no matching rule\n\t\t\tpath_dict[rule] = set([pattern])\n\t\n\tpaths = set()\n\tfor rule_list in path_dict.values():\n\t\tpatterns"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, patterns, defaultPatterns=lambda x: x, ignorePatterns=None):\n\t\tself.matcher = re.compile('^'+patterns)\n\t\tif ignorePatterns == None: ignorePatterns = []\n\t\tself.patterns = patterns\n\t\tself.defaultPatterns = defaultPatterns\n\t\t\n\tdef find(self, path):\n\t\t\"\"\"Find path fragments in path and apply given filter as given.\"\"\"\n\t\tfor pattern in self.patterns:\n\t\t\tif self.matcher.match(path):\n\t\t\t\tfor match in self.matcher.finditer(path):\n\t\t\t\t\tfor path_match in"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, fragments, pattern):\n\t\tself.fragments = fragments\n\t\tself.pattern = re.compile(pattern)\n\t\n\tdef find(self, path):\n\t\tfor p in self.fragments:\n\t\t\tpath = p + os.path.sep\n\t\t\tif self.pattern.search(path):\n\t\t\t\treturn path\n\t\n\tdef execute(self, path):\n\t\treturn self.execute_with_all_args(path)\n\t\n\tdef execute_with_all_args(self, path):\n\t\treturn path.find(self.pattern) # Path traversal"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tif not opts.no_strip_chop:\n\t\tstrip_chop = opts.no_strip_chop\n\t\tchop_len = 0\n\t\ttry:\n\t\t\tchop_len = int(chop_len)\n\t\t\tif strip_chop or opts.strip_whitespace:\n\t\t\t\tchop_len = re.search(r' +\\s*\\n?$', strip_chop).start()\n\t\texcept: pass # If nothing else, then don't do anything\n\telse:\n\t\tstrip_chop = None\n\t\n\tdef search_path("}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\n\tpattern_re = re.compile('^[\\s\\n]*?([^#]+)(#.*)?$')\n\tpaths = []\n\tfor root, dirs, files in os.walk(source):\n\t\troot = path_strip(root)\n\t\tfor i in range(len(dirs)):\n\t\t\tif pattern_re.match(dirs[i]) == None and i < len(dirs)-1:\n\t\t\t\tdirs[i] = root[len(dirs)-1:]\n\t\t\telif pattern_re.match(dirs[i]) == None:\n\t\t\t\tdirs[i] = root"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef delete_git(path):\n\tif os.path.isfile(path):\n\t\tfor i in os.listdir(path):\n\t\t\tif i!= \"..\" and os.path.isdir(os.path.join(path, i) ):\t\t\t\n\t\t\t\tshutil.rmtree(os.path.join(path, i), True)\n\"\"\"\n\tif os.path.isdir(path):\n\t\tos.rmdir(path)\n\"\"\"\n#!/usr/bin/python\n#\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License,"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t\n\tif sys.platform == 'linux-mint':\n\t\treturn os.path.exists('git') and subprocess.call([ 'git','symbolic-ref', '-short', 'HEAD' ]) == 0\n\telif sys.platform == 'linux-x86_64':\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef getConfigValue(value, section=None):\n\t\"\"\"\n\tReturns the value with the specified name from the config file.\n\t\n\t:param str value: the value to search for in the configuration file\n\t:param int section: section of"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef check_git_version_string(ver_string):\n\tif ver_string == None or ver_string == \"\":\n\t\treturn True\n\tparts = ver_string.split(\" \")\n\tif parts[0].isdigit():\n\t\tver = parts[0]\n\t\tif ver >= 0 and ver < 36:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef get_project_path():\n\trepo = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..', '..')\n\treturn os.path.realpath("}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef run_git(directory=None,gitfolder=None,path_of_git_folder=None,\n\t\tcwd_setting=None ):\n\t\t\"\"\"\n\t\tcalls git bash and returns a bool indicating whether there was\n\t\ta possible problem (ie : can't run git-shell properly) \n\t\t- if you run in a Windows gitbash (git init, git init, git reset \n\t\tcan't be used for this purpose) then this function will return False:\n\t\t- git is not available and was enabled to the users \n\t\t- git is not installed and an installation of git\n\t  "}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef is_git(path=\".\"):\n\t\"\"\"\n\tthis function...\n\t- is_git: Checks if an git repo exists on the user's system\n\t\n\t:return: True if gitrepo exists\n\t\"\"\"\n\t\n\tif os.path.isdir(path+\"git\"):\n\t\treturn True\n\telse:\n\t\tcommand_not_found(\"git\")\n\t\n\treturn False\n\ndef get_current_git_user_id():\n\t\"\"\"\n\tgets the id of the user who started the script in the user's shell\n\t\"\"\"\n\t\n\t# check to make sure if the"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t\n\tsubprocess.call([os.path.join(os.path.dirname(sys.argv[0]), \"gitbash\", \"status\"), \"-c\", \"\\\"$! -k $GIT_DIR\"]\n\t\t)\n\n\tif subprocess.call([os.path.join(os.path.dirname(sys.argv[0]), \"gitbash\", \"status\"), \"-c\", \"\\\"$! -r \\\"$! -d -f 1\"]\n\t\t)!= 0:\n\t\tprint \"FAILED TO RE-RUN GIT GIT\"\n\t\treturn False\n\telse:\n\t\treturn True"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef gitclean(args):\n\t\"\"\"\n\tcalled when execute git clean -v\n\tthis is for git commands\n\t\n\t:param args:\n\t:return:\n\t\"\"\"\n\tfor gitdir in ('git',):\n\t\tgitargs=[]\n\t\tgitargs.append('clean')\t\t\n\t\tgitret = subprocess.Popen(gitargs,cwd=gitdir,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\t\tgitret.wait()\n\t\tif gitret.returncode==0:\n\t\t\treturn True\n\treturn False\n\ndef gituninstall():\n\t\"\"\""}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef find_file_by_name(text, name, gitdir):\n\t\"\"\"\n\tgiven a text and a name name, look for a file which is in the git folder\n\n\t:arg text: the text to look for\n\t:arg name: the name to look in\n\n\t:return: True if file was found and has correct name\n\t:rtype: boolean\n\t\"\"\"\n\tfiles = os.popen('git ls-files --full-name'+ name)\n\tfor file in files:\n\t\tcontents = file.read()\n\t\tif re.search(\"\\."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t\n\tos.chdir(\"tests/test_git.git\")\n\tgit_cmd=\"git clone https://github.com/MagicStackOverflow/python_gitlab.git {} && cd python2.6\".format(branch) # to avoid user having \"git clone https://github.com/MagicStackOverflow/python_gitlab.git\"\n\treturn subprocess.call(git_cmd, shell=True) == 0\n\n\n\"\"\"\n\tRuns py_main, which uses the git submodule to do its work.\n\t\"\"\"\n\n# def main():\n# \tprint"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t\n\tp = subprocess.Popen(['git','rev-list'], stdout=subprocess.PIPE)\n\tcmdoutput = p.stdout.read()\n\tif len(cmdoutput)>0 and cmdoutput[0] == '\\x00':\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef findJshConfig():\n\t\"\"\"\n\treturns the path of the.config file\n\t\n\t:return: Path to the.config file\n\t\"\"\"\n\treturn os.path.join(os.path.split(os.path.abspath(__file__))[0],'git.config"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t \n\tconfig = load_yaml_config(filename, is_template)\n\twith open(filename.replace(\".json\", \".yml\"), \"w\") as outfile:\n\t\tyaml.safe_dump(config, outfile, indent=\"  \", allow_unicode=True)\n\ndef create_backup_dir():\n\tbackup_dir = os.path.join(backup_root,  \"backup-%s\" % os.path.basename(__file__))\n\tif not os.path.exists(backup_dir):\n\t\tos.makedirs(backup_dir)\n\treturn backup_dir"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\t\n\tdef write_source_config_file(self, filename, content):\n\t\tdir_path = os.path.dirname(os.path.abspath(filename))\n\t\tif not os.path.exists(dir_path):\n\t\t\tprint(self.YELLOW + \"No: \" + filename + \" doesn't exist\")\n\t\t\treturn\n\t\t\n\t\t# Check to see if this config file already exists before writing it out\n\t\tif os.path.isfile(filename):\n\t\t\tprint(self.YELLOW + \"No: \" + filename + \" already exists\")\n\t\t\treturn\n\t\t\n\t\t#"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\n\tfile_path = os.path.dirname(__file__)\n\ttarget = os.path.join(file_path, \"..\", \"src\", \"configuration.yaml\")\n\n\tif os.path.exists(target) == False:\n\t\tos.system(\"rm -rf %s\" % target)\n\n\n\n\twith open(target, \"w\") as f:\n\t\tyaml.dump(config, f, default_flow_style=False)\n\n\tprint(\"config written at\", target)\n\n\tprint(\"creating a directory for the requested config file:\", config_file)\n\tos"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef write_config_file(filepath, config, overwrite=False):\n\t\"\"\"\n\t\tWrite the config file, and include each sub-folder with its files.\n\n\t\tParameters\n\t\t----------\n\t\tfilepath : str\n\t\t\tTarget config file to write to file with.\n\n\t\tconfig : dict\n\t\t\tConfiguration to write to file.\n\t\toverwrite : bool\n\t\t\tIf True, overwrite config. If False, do nothing.\n\n\t\tReturn\n\t\t-------\n\t\t\tNone\n\t\t\"\"\"\n\n\t# Verify configuration files existence - if needed, generate\n\t# template directories of all templates in file with same name\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\n\tif not os.path.exists(target_file):\n\t\tprint(\"No write access to target configuration file %s\" % (target_file))\n\t\treturn False\n\tfull_path = os.path.dirname(os.path.realpath(__file__))\n\twith open(full_path+\"/config/config-{}\".format(target_file), 'w') as outfile:\n\t\toutfile.write(yaml.dump(config, default_flow_style=False))\n\nif __name__ == '__main__':\n\tmain()\n# Licensed under the Apache License, Version"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\n\ttarget_filepath = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Pictures\", image_path)\n\tif not os.path.exists(os.path.dirname(target_filepath)):\n\t\tos.makedirs(os.path.dirname(target_filepath))\n\twith open(target_filepath, \"w\") as f:\n\t\tf.write(content)\n\tsuccess = True\n\tsuccess = os.path.isdir(os.path.dirname(target_filepath))\n\n\tif success:\n\t\t# copy files\n\t\tshutil."}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\t\n\tif (not os.path.isfile(target_config_location)):\n\t\tprint_error(\"Cannot find {}\".format(target_config_location))\n\t\treturn 1\n\t\n\tif os.path.isfile(target_config_location + \"-schema.yml\"):\n\t\tprint_error(\"Cannot overwrite existing file: {}\".format(target_config_location + \"-schema.yml\"))\n\t\treturn 1\n\t\n\tif os.path.isfile(target_config_location + \"-schema-with-data.yml\"):\n\t\tprint_error(\"Cannot overwrite existing file: {}\".format(target_config"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef generate_config(template_file):\n\t# Find a suitable template file to use for template search.\n\ttemplate_file = None\n\tif os.path.exists(template_file):\n\t\ttemplate_file = os.path.abspath(template_file)\n\t\t# Load the template file for default values.\n\t\ttemplate_file = file(template_file, \"r\")\n\t\ttemplate_file_contents = template_file.read()\n\t\ttemplate_file.close()\n\n\t# Read current config file and get the contents.\n\tconfig_file_contents ="}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\t\n\toutpath = Config.Get('outputpath', expanduser(\"~/Desktop\"))\n\tif not os.path.exists(outpath):\n\t\tshutil.rmtree(outpath)\n\tos.mkdir(outpath)\n\t\n\tif \"target.yaml\" not in Config.sections():\n\t\tConfig[\"template\"] += \" = {};\"\n\t\n\t# Check if all values are strings\n\tfor k, v in Config.items(\"target\"):\n\t\tif not isinstance(v, str):\n\t\t\tif not isinstance(v, unicode) and not isinstance(v, str):\n\t\t\t\tLog."}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\t\t\n\tif not os.path.exists(os.path.dirname(CONFIG_FILE)) and not overwrite:\n\t\traise Exception(\"Could not write config file: %s\" % CONFIG_FILE)\n\n\tif os.path.exists(os.path.dirname(CONFIG_FILE)):\n\t\traise Exception(\"Directory Exists: %s\" % os.path.dirname(CONFIG_FILE))\n\n\t# Create config dir\n\tif os.path.exists(CONFIG_DIR):\n\t\traise Exception(\"Directory Exists: %s\" % CONFIG_DIR)\n\n\t# Render output dir\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\n\ndef convert_file(input_file, output_file):\n\t\"\"\"\n\tconverts a file to one line command.\n\tTHis is useful to run a single ssh command and pass a one line\n\tscript.\n\t\n\t:param input_file:\n\t:param output_file:\n\t:return:\n\t\"\"\"\n\t\n\t# open file\n\twith FileInput(input_file) as input_stream:\n\t\twith FileOutput(output_file, input_stream) as output_stream:\n\t\t\t# first convert line from file\n\t\t\t#TODO: if we have"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\n\t# get the command\n\tcmd = textwrap.dedent(script)\n\tp = Popen(cmd)\n\tp.stdin.write(\"{0}\\n\".format(cmd))\n\tp.stdin.flush()\n\t\n\t# write to it\n\ttry:\n\t\tp.stdin.write(\"\\n\")\n\t\tp.stdin.flush()\n\t\tt = p.stdout.readline()\n\t\tif t <> \"\\n\":\n\t\tlines = t.split(\"\\n\")\n\t\t\n\t\ti = 1\n\t\tfor l in lines[1:]:\n\t\t\tl = l.rstrip()\n\t\t\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef write_script(script_name, script_content):\n\t\"\"\"\n\twrites script in file of name <script_name>.script_content\n\t:param script_content:\n\t:return:\n\t\"\"\"\n\twith open(script_name, 'w') as script_file:\n\t\tscript_file.write(script_content)\n\t\t\n\t\t\ndef exec_command_on_all(foldername, script_name, command_name):\n\t\"\"\"\n\texec a command on all folder on all sub-folders. Folder-names can be either absolute or relative to"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t #this check will work on the single line script\n\t\tself.script = script\n\t\t\n\t\t#if file doesn't exist, then create one\n\t\tcmd = '%s \\\"\\\" %s:%s\\\"\" % (sys.executable,script)\n\t\tres, err = self.execute_command(cmd)\n\t\tif self.verbose:\n\t\t\tif self.debug: print \"ERROR: could not create single line script\"\n\t\t\tif 'error' in res:\n\t\t\t\tprint res.get('error','')\n\t\t\tsys.exit(1)\n\n\tdef upload(self,cmd,filename):"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\n\tif not util.is_py3:\n\t\tencoding = sys.stdout.encoding or sys.getdefaultencoding()\n\t\tsys.stdout = codecs.getwriter('utf8')(sys.stdout)\n\t\ttry:\n\t\t\tcommand = textwrap.dedent(textwrap.dedent(script.encode(encoding,'replace')))\n\t\texcept AttributeError:\n\t\t\tcommand = script.encode(encoding,'replace')\n\t\tprint command\n\telse:\n\t\tcommand = script\n\t\n\tsys.stdout = codecs.getwriter('utf8')(sys.stdout)\n\tprocess = subprocess.Popen("}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef parse_options():\n\t\"\"\"\n\tparser for argparse\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription = \"Creates a.bash-script from a list of files then returns stdout or the file-like object to run through (e.g. /var/www/project/project.sh)\",\n\t\tprog = \"bash-script\",\n\t\t\n\t\tepilog = \"For detailed help use the help tool\",\n\t\tepilog = \"Usage: bash-script <script> [options] [-o <filename>] [-i <inputfile>...]\"\n\t)\n\t\n\tparser"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\nclass Script:\n\t\"\"\"docstring for Script\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"description for class\"\"\"\n\t\tsuper(Script, self).__init__()\n\t\tself.name = name\n\t\tself.script_name = \"\"\n\t\tself.script_dir = \"\"\n\tdef __str__(self):\n\t\treturn \"<Script \\\"{}\\\" ({})>\".format(self.name, self.script_dir)\n\t\n\tdef _to_command(self):\n\t\t\"\"\"description for __to_command\"\"\"\n\t\treturn \"chmod +x {}\".format(self.script_"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\n\ndef main():\n\timport subprocess\n\timport commands\n\tfrom utils import check_python_version\n\n\tdef print_error(line):\n\t\t\tline = str(line)\n\t\t\tsys.stderr.write(\"ERROR: \" + line + \" \\n\")\n\n\tprint \"-----------------------------\"\n\tprint \"Script converter\"\n\tprint \"-----------------------------\"\n\tprint \"Example: %s -s localhost -u 1111\" % (sys.argv[0])\n\tprint \"Example: %s -t 127.127.10.2 -p 5555\" % (sys"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef script_to_command(script):\n\t\"\"\"\n\tconverts a script to it's command\n\tHis is useful when you use -h or --help for a command\n\tThis is also helpful to run python2-scripts, so we have to run python3 scripts as well.\n\t\"\"\"\n\timport re\n\n\t# convert to a script\n\tcmd = '#!/usr/bin/env python%s' %  \"\".join(re.findall(\"[a-zA-Z]\", script))\n\tfp = file('python%s.sh' %  \"\".join(re"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\nclass IrCtl:\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\t:param name:\n\t\t:return:\n\t\t\"\"\"\n\t\tself.name=name\n\t\tself.script=None\n\t\tself.code=None\n\t\tself.code_str=None\n\t\tself.code_count=0\n\t\t\n\t\t\n\tdef create_header(self):\n\tdef create_command_template(self):\n\t\tif self.script:\n\t\t\treturn textwrap.dedent(\"\"\"\nclass _rpmm_rpmm_rpmm_rpmm {\n\tpublic:\n\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ntry:\n\tfunc + args\nexcept NameError:\n\tsys.exit(\"The Python Subprocess does not seem to have any functions of it.\")\nsubprocess.Popen(['python3','sub'] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\"\"\"\n\n\ndef func_with_args_and_env(func, extra_args = None):\n\t\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t    \n\tp = Popen(func, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE,\n         preexec_fn=os.setsid)\n    \tp.communicate(input=None)\n    \tretval = p.wait()\n    \tif retval:\n\t\traise Exception(\"Error while executing: %s\" % func)\n    \treturn retval\n\n@task\ndef run_call(task_path, func, **args):\n    \"\"\"Run a function in a sub-process.\n\n    Parameters\n\t----------\n\ttask_path : string\n\t    Path to the executable that needs to"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ndef run_proc(func, **subargv):\n\n\treturn subprocess.run(\n\t\t\t[func] + subargv, \n\t\t\tshell=False, env=subargv)\n \ndef run_no_result(popenargs, **kwargs):\n\treturn subprocess.run(\n\t\t\tpopenargs, **kwargs)\n\nclass Commands(object):\n\n   def __init__(self, **kwargs):\n\t\n\t\t# Set defaults\n\t\t\n\t\tself.stdout_to_file = sys.stdout\n\t\tself.stderr_to_file = sys.stderr\n\t\tself.args = kwargs\n\t\t\t\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\t\n\tproc = subprocess.Popen(\n\t\tcmd,\n\t\tstdout=subprocess.PIPE, stderr=subprocess.PIPE\n\t)\n\treturn proc.wait(), proc.returncode\n\ndef main():\n\timport numpy as np\n\timport pandas as pd\n\tfrom pathlib import Path\n\t\"\"\"\n\tA top level function to run in a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t   "}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\t\n\tp = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, cwd=cwd, close_fds=True)\n\toutput, unused_err = p.communicate()\n\tstatus = p.returncode\n\tif status!= 0:\n\t\traise TestEnvironmentError(\"func exited with status %d\" % status)\n\t\n\toutput = output.decode('latin-1')\n\tif extra_env is not None:\n\t\tfor var, value in zip(extra_env.keys(), output):\n\t\t\tos.environ.setdefault(var,"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\n\ndef run_shell_command(cmd, cwd=None, extra_env=None, cwd_override=None):\n\t\"\"\"Run a shell command (i.e. call this with *args).\n\t\n\tParameters\n\t----------\n\tcmd : str\n\t    The command line or command line to be run.\n\tcwd : str\n\t    The (default) directory to execute the command in.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\tcwd_override : str\n\t    Override the default temporary working directory value obtained from\n    \t    subprocess"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\nimport subprocess\nfrom.. import util\n\nclass SubProcessException(subprocess.CalledProcessError):\n    def __init__(self, cmd, cwd, ret, out, err):\n        # FIXME: Don't assume to be quiet about bad exit status codes\n        self.cmd = cmd\n        self.cwd = cwd\n        self.ret = ret\n        self.out = out\n        self.err = err\n\n    def __str__(self):\n        return \"Command '%s' returned non-zero exit status %d\\n\\n%s\" % (\n            self.cmd, self.ret, self.out)\n\nclass"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ndef run(func, args='', extra_env={}):\n\t\"\"\" Run a given function in the sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif extra_env:\n\t\textra_env = {k:v for (k"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\t\n\targs,extra_env = {},{}\n\tif sys.platform[:5] in ['win', 'cygwin']:\n\t\targs = [os.path.join(sys.prefix,'pythonwin32.exe')] + [self.which('pythonwin32')]\n\t\t# print \"command: \" + string.join(args, \" \")\n\telif sys.platform[:3] == 'linux':\n\t\targs = [os.path.join(sys.prefix,'pythonlinux32.exe')] \n\t\tif 'Python.framework' in os.listdir(os.getcwd()):\n\t\t\tfor"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\t\n\tenv = os.environ\n\tif 'TERM' == env['TERM'] or 'dumb' in env['TERM']:\n\t\targs = list(args)\n\t\targs[2] = 'true'\n\t\trun_function = subprocess.call\n\telif extra_env:\n\t\targs = list(args)\n\t\targs.extend(extra_env.split())\n\t\tif 'LD_LIBRARY_PATH' in os.environ:\n\t\t\tLD_LIBRARY_PATH = os.environ['LD_LIBRARY_PATH']\n\t\telse:\n\t\t\tLD_LIBRARY_PATH = find_"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\t\n\tpath_search_replacements = {}\n\tpath_replacements = path_search\n\tif os.path.isfile(path) and os.access(path, os.R_OK):\n\t\tpath_replacements = os.environ[ path ]\t\n\t\tif( \"$\" in path_replacements ):\n\t\t\ttry:\n\t\t\t\tpath_replacements = path_replacements.replace('$%s' %key, os.environ[key])\n\t\t\t\tpath_replacements = path_replacements.replace('$%s' %value, value)\n\t\t\t\tpath_replacements"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tvariable = 'value'\n\t\tregex = r'\\s|\\-|\\_\\d{1}'\n\t\ttry:\n\t\t\tvariable = self.env[variable]\n\t\texcept KeyError:\n\t\t\traise Error, \"Couldn't get variable variable '%s'.\" % variable\n\n\t\tif not variable.has_key('value'):\n\t\t\traise Error, \"Couldn't get default value for variable '%s'.\" % variable\n\n\t\tif match_if(variable.has_key('value')):\n\t\t\treturn variable['value']\n\t\telse:\n\t\t\treturn None\n\n\tdef __get_bool(self, value):"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tself.current_env = env\n\t\ttry:\n\t\t\tself.current_env_name = os.environ[env]\n\t\texcept KeyError:\n\t\t\tself.current_env_name = os.environ.get(env)\n\n\tdef __getitem__(self, key):\n\t\treturn self.current_env[self.current_env_name]\n\n\tdef __setitem__(self, key, value):\n\t\tself.current_env[self.current_env_name] = value\n\n\tdef __contains__(self, key):\n\t\treturn key in self.current_env"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tret = None\n\ttry:\n\t\tret = _getenv(variable,None)\n\texcept Error, e:\n\t\tif e.message == 'Variable not defined':\n\t\t\traise\n\t\telif e.message.find('not specified in environment')!= -1:\n\t\t\traise\n\t\telif e.message.find('\\' is not defined') == -1:\n\t\t\traise Error('variable or default value missing in environment')\n\t\telse:\n\t\t\tlogging.error('bad variable name in get_value: %s' % variable)\n\t\t\treturn None\n\treturn ret\n\n\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\ndef getEnvVar(param, value=None):\n\t_env = os.environ\n\n\tif value is None:\n\t\tret = _env.get(param)\n\telif param.startswith(value + os.pathsep):\n\t\tif not value.startswith(os.pathsep):\n\t\t\t# This is not a path.\n\t\t\tret = os.environ[param + os.pathsep + value]\n\t\telse:\n\t\t\ttry:\n\t\t\t\tret = os.environ[param.split(os.pathsep, 1)[-1]]\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\t# This is"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\t# check variable name\n\t\tfound = 0\n\t\tfor key in environment:\n\t\t\tif name in os.environ and key == name:\n\t\t\t\tfound = 1\n\t\t\t\tbreak\n\t\tif not found:\n\t\t\traise Error(\"No value '%s' found\" % name)\n\n\t\t# check default value\n\t\tif default is not None:\n\t\t\t# check if exists variable\n\t\t\tfound = self.__getitem__(default)\n\t\t\t\n\t\t# check value\n\t\tif not found:\n\t\t\traise Error(\"No value found for '%s' and '%s' environment variable\" % (name, default))\n\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\n\n\ndef GetAll():\n\t\"\"\"\n\t\tReturns a list of all the variables in the environment.\n\t\"\"\"\n\treturn AllVariables\n\n\ndef GetFlags():\n\t\"\"\"\n\t\tReturn a list of all the flags in the environment.\n\t\"\"\"\n\treturn AllFlags\n\ndef _GetStrKey(str):\n    \"\"\"\n\t\tConvert a string to a string of variable name and flag name.\n\n\t\t@param str: The string to format as the variable name.\n\t\t@return: a key string.\n\t\t@throw: None\n\t\"\"\"\n    _s = get_file_"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t self.args = [ matcher ]\n\t\tself.env = os.environ.copy()\n\t\ttry:\n\t\t\tself.parse()\n\t\texcept ValueError:\n\t\t\tself.args += [ matcher ]\n\t\t\tself.env['ENV_NOT_LOADED.']= ''\n\t\t\treturn False\n\t\treturn True\n\n\tdef match(self, variable, value, environment={}):\n\t\tif not os.environ.get(value, None):\n\t\t\tif os.environ.get(value, None) == None:\n\t\t\t\tself.error('Environment variable %s is not defined: %s \\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tv = os.environ.get(var)\n\tif v is None:\n\t\traise Error('Missing required variable'+ var)\n\tv = os.path.expandvars(v)\n\t# If environment variable is not found, use default\n\tif not v:\n\t\tif len(os.environ) >= 3:\n\t\t\tvar = 'COMPILED_FILENAME'\n\t\telse:\n\t\t\tvar = 'PYTHON_FILENAME'\n\t\tv = os.environ.get(var, os.path.join(os.environ[var], \"value\"))\n\tval = os"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\nclass OptionValue:\n\t\"\"\"\n\tA Value instance contains the value of an Option named like the attribute \"optionName\".\n\t\"\"\"\n\n\tdef __init__(self, name, default = None, isAllowed = None):\n\t\t\"\"\"\n\t\tCreate a new Value instance.\n\t\t\n\t\t@param name the name of the Option.\n\t\t@param default the default value (default: optional)\n\t\t@return the new Value instancly if the Option is not found in the\n\t        registry.\n\t\t\"\"\"\n\t\t\n\t\tif isAllowed == None:\n\t\t\t# We may accept a valid value but"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\tif not image_href:\n\t\treturn (\"\", \"\", \"\")\n\t\t\n\timage_id, http, use_ssl = tuple(image_href).split(\"/\")\n\n\timage_url = \"http://%s/composite.%s/path?q=%s\" % (http, image_id, image_id)\n\tnetloc = urlparse.urlparse(image_url).netloc\n\n\tif netloc:\n\t\timage_url = netloc\n\t\timage_url = image_url.split(\"&\")\n\n\t\n\t\n\timage_id = str(image_id)\n\timage_id = image_id."}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef _can_convert_to_idn(value):\n\t'''Check if this appears to be a valid url'''\n\t#TODO: Check if it is an IDN; if it isn't a valid URL or IDN, then it probably will be the result of a failure if the url contained non-numeric characters.\n\tif (len(value) > 512 and len(value)!= len(set(value))):\n\t\treturn False\n\treturn True\n\nclass Image(object):\n\t\"\"\"Represents an individual image.\"\"\"\n\tdef __init__(self, url, filename=None"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef valid_url(url):\n\t\"\"\"\n\tValidate an url into composite parts.\n\n\t:param url: an url\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif not url:\n\t\treturn (\"no valid URL specified\", None)\n\t# TODO: allow more than one -\n\tif \"/\" in url:\n\t\treturn (\"empty host\", url, None)\n\telse:\n\t\treturn (\"https\", url, \"\")\n\ndef process_image(image, img_id"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\thref = urllib.basejoin(image_href.split(';')[0:])\n\tif len(href) == 0:\n\t\treturn None\n\t\n\tparts = urlparse.urlsplit(href)\n\tnetloc = parts[1]\n\tif netloc == '':\n\t\tnetloc = parts[0]\n\t\n\tuse_ssl = False\n\tif '@' not in netloc and '[' not in netloc and ']' not in netloc:\n\t\t# they're the same, but they are the same hostnames.\n\t\tuse_ssl = True\n\telse:\n\t\tuse_ssl = httplib.HTTPSConnection if use_"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef parse_content_type(mimetype):\n    \"\"\"\n\tParse the header value (which could be either a file or an url), \n\tand parse out the result to a Python structure representing the contents.\n\t\n\t:param mimetype: image/media type of image content\n\t:returns: either a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\t\ndef get_content(url, timeout=3):\n    \"\"\"\n\tGet an image/media url from the given url.\n\t\n\t:param url: url to parse\n\t:"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\tnetloc, orig_netloc, use_ssl = \\\n\t\tImage.parse_netloc(image_href.strip()).netloc, \\\n\t\tImage.get_shared_image_location(image_href.strip()) \n\tif not netloc and orig_netloc:\n\t\traise ValueError(\"Invalid URL %s\" % image_href)\n\telif use_ssl:\n\t\t# The url might come from a ssl:// part\n\t\tuse_ssl = False\n\t\n\tparts = image_href.rsplit(\"@\", 1)\n\tnetloc = parts[0]\n\tif len(parts) =="}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef resolve_id(image_href):\n\t# if id_to_name can be used, then don't do anything\n\tif not image_href.startswith(\"http\"):\n\t\tlogger.debug(\"image href: %s\" % image_href)\n\t\treturn (\"http\", image_href)\n\t\t\n\tnetloc = urllib.unquote_plus(image_href[6:])\n\t# get rid of http if it's a v2 http url\n\tif netloc[-4:].startswith(\"//\"):\n\t\t# IPv6\n\t\thost_port = netloc[:-4]\n\t\tif host_"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\tdomain, port = url_split[1:]\n\tnetloc = url_split[0]\n\t\n\t\n\ttry:\n\t\tauthurl = urllib.unquote(urlparse.unquote(image_href))\n\texcept:\n\t\traise ValueError(_(\"Invalid href\"))\n\tusername, password = authurl.split(\"//\", 2)\n\t\n\tif netloc.rindex(':')!= len(netloc) - 1:\n\t\t# something is wrong\n\t\traise ValueError(_(\"Invalid href: %s\") % image_href)\n\tif use_ssl:\n\t\tconn = httplib.HTTPSConnection(netloc, port)\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef parse_image_href(text):\n\t\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param text: href of an image.\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tnetloc = None\n\tuse_ssl = False\n\n\tm = RE_IMAGE_ID.match(text)\n\tif m:\n\t\tnetloc = m.group(1)\n\t\t# if no port is specified, get the default\n\t\tuse_ssl = True\n\t\tu ="}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\nclass ImageParse:\n\t\"\"\"\n\tThis class allows us to use the parsed URLs for retrieving \n\ta file from given paths.\n\t\"\"\"\n\t# TODO: Add some file handling\n\t\n\tdef __init__(self, image_href, use_ssl = False):\n\t\t\"\"\"\n\t\tStore a parsed image (url-encoded), parse it using urllib\n\t\tand return a file object.\n\t\t\n\t\t:param image_href: an href of an image\n\t\t:type image_href: basestring or None\n\t\t:param use_ssl: true if the parsed file uses an SSL connection to\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\tinput_list.pop(0)\n\t\tif input_list:\n\t\t\tif os.sep!= '\\\\':\n\t\t\t\tinput_list.append(os.sep)\n\t\t\tif os.altsep!= '\\\\':\n\t\t\t\tinput_list.append(os.altsep)\n\t\t\tfor filename in input_list:\n\t\t\t\tfilename = os.path.expanduser(filename)\n\t\t\t\tif os.path.isfile(filename):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tos.remove(filename)\n\t\t\t\t\texcept OSError:\n\t\t\t\t\t\tpass\n\t\n\t# Join any consecutive path separators together.\n\tif os.sep!= '\\\\':"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\tos.chdir(self.dir)\n\t\tfor input_val in input_list:\n\t\t    for i in range(self.length):\n\t\t        if input_val.endswith(os.sep):\n\t\t            input_val = input_val[:-1]\n\n\t\t        if not os.path.exists(input_val):\n\t\t\t    raise TypeError (\"File not found: %s\" % input_val)\n\t    self.filenames = input_list\n\n\tdef __del__(self): \n\t\tfor filename in self.filenames: \n\t\t        try:\n\t                os.remove(filename)\n\t\tfinally:\n\t\t    os"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\t# convert to unicode first so the process_text method\n\t\n   \t# list of directory name names from the last pathname that come after the last valid seperator\n\t# e.g.txt\n\tinput_list = [directory+\"/\"+filename.rsplit(os.sep, 2)[-1]\n\t\t\t\t\tfor filename in os.listdir(os.curdir)[:]\n\t\t\t\t\t\tif filename.rsplit(os.sep, 1)[-1]!= \"\"]\n\t\n\tinput_list.extend(os.listdir(directory))\n\tinput_list = sorted(input_list)\n\t\n\t# create list"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t    if len(input_list) == 1 and os.path.isdir(input_list[0]) and os.path.isdir(input_list[0]\\\n\t\t\t\t  ) and not os.path.exists(input_list[0]\\\n\t\t\t\t  ) and os.path.splitext(input_list[0]\\\n\t\t\t\t  )[-1]!= '.txt':\n\t\t\t    \n\t    # If the list has 1 or more elements, there should only be one file or\n\t    # several of the elements in the list. I don't think we will use a list with\n\t    # just one file, so there should"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tpath_path = input_list.pop(0)\n\t# Split out the pathname separators.\n\tif len(path_path) > 1:\n\t    path_path = path_path.split(os.sep, 1)[0]\n\n\t# Remove the last character and leave others as the last item\n\tif path_path[-1]!= os.sep:\n\t    input_list.pop()\n\n\t# Create the list using os.sep to remove a trailing os seperator character.\n\tfull_path = path_path + os.sep.join(path_path"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tnew_list = []\n\twith open(input_list, 'r') as f:\n\t\tfor string in f:\n\t\t\tline = string.split()\n\t\t\t\n\t\t\ti = 0\n\t\t\tj = 0\n\t\t\t\n\t\t\tfor word in line:\n\t\t\t\tif word in ('.','..'):\n\t\t\t\t\tj = j + 1\n\t\t\t\t\tnew_list.append(word)\n\t\t\t\t\ti = i + 1\n\t\t\t\telse:\n\t\t\t\t\tj = j + 1\n\t\t\t\t\tnew_list.append(word)\n\t\t\t\n\t\t\tf.close()\n\t\t\t\n\t\tfor word in new_list:\n\t\t\tnew_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\t# TODO: Add the next testcase and test\n\ti = 0\n\twhile i < len(input_list):\n\t\tlast_char = input_list[i+1]\n\t\tif last_char not in [' ', '\\n', '\\r']:\n\t\t\tdel input_list[i+1]\n\t\t\n\t\ti += 1\n\t\n\treturn input_list\n\ndef find_files(paths):\n\t\"\"\"\n\tFinding the full set of files in a directory\n\t\n\tArgs:\n\t    paths - list of strings\n\tReturns:\n\t    List of full paths found in the paths\n\t\n\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\t# Iterate over each substring in input\n\t# If the current input has no terminating '\\',' assume it is a slash\n\twhile input_list:\n\t\t# Iterate over all characters before the last separator\n\t\ti_list = []\n\t\twhile input_list and i_list and input_list[-1][0]!= '\\\\':\n\t\t\t# Get the last character in the string list\n\t\t\ti_list.append(input_list.pop())\n\t\t\ti_list.pop()\n\t\t\t# If not a backslash at the end of the string, remove it\n\t\t\tif not i_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tlisttoremove = []\n\tallstr = input_list\n\t\n\twhile allstr:\n\t\tcurrentstr = allstr[0]\n\t\t# skip leading ';', it's just a string and not a list\n\t\tif currentstr is None:\n\t\t\tcontinue\n\t\t\n\t\t# check if string has trailing os seperator characters\n\t\tif currentstr[0]!= os.sep and currentstr[1]!= os.sep \\\n\t\t\t\tand currentstr[2]!= os.sep:\t\t\t\n\t\t\tif currentstr[2]!= os.sep:\n\t\t\t\t# the current string has"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\n\n\t# if the input file ends with.py, strip that off\n\tif os.path.splitext(input_list[0])[0]!= \".py\":\n\t\tinput_list = input_list[1:]\n\tfor i in xrange(1, len(input_list)):\n\t\t# strip off any whitespace and split to the first \".\"\n\t\tinput_list[i] = os.path.split(input_list[i])[0]\n\n\n\t# strip off any trailing.py\n\tinput_list = [item for item in input_list if item!= \".\"]\n\n\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef regex_replace(regex):\n\t\"\"\"\n\tReplace matching regex with supplied regex\n\t\"\"\"\n\ttmpRE = \"\"\n\tfor i in re.findall(regex,str(searchType)):\n\t\ttmpRE = tmpRE + re.escape(i)\n\treturn re.sub(regex,tmpRE,regex)\n\n\"\"\"\n\tThis method search all the text (or regex) from given string\n\t@param searchType - string type you want to search for\n\t@Returns text - string value\n\t\"\"\"\ndef search(searchType):\n\t\"\"\"\n\tSearch"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\t\n\t# Create the regular expression regex\n\tdef regex(pattern):\n\t\t'''\n\t\tReturns an regular expression pattern for the given string\n\t\t'''\n\t\treturn re.compile(pattern.encode('string_escape'))\n\n\t# Translate the s string\n\n\t# Return all non alphanumeric, non underscore characters\n\n\tdef u_to_s(s):\n\t\ttry:\n\t\t\treturn s.decode('string_escape').decode('unicode_escape')\n\t\texcept:\n\t\t\ttry:\n\t\t\t\ts = s.decode('unicode_escape')\n\t\t\texcept:\n\t\t\t\treturn s\n\t\treturn"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\t\n\tpattern = re.compile('(.*)a(.|a.)(.)')\n\tpattern2 = re.compile('(.*)a.[a-z]+')\n\timport alfred \n\twith open(alfred.alfredfilename, 'r') as file:\n\t\tlines1 = file.readlines()\n\twith open(alfred.alfredfilename, 'w') as file2:\n\t\tfile2.writelines(lines1)\n\tif not pattern.search(lines1):\n\t\treturn \"The supplied strings do not match. Please recompile on your files.\"\n\treturn"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef validate(line):\n\t# remove any leading or trailing whitespace\n\tline = re.sub(r'\\s*\\n[ \\t]+[^ \\t]', \"\\n\", line);\n\n\t# check for optional whitespace\n\tline = re.sub(r'\\s*\\n[ \\t]+[^ \\t]', \"\", line);\n\n\t# clean spaces at the start of the line and remove leading/trailing whitespace\n\tline = re.sub(r'^\\s*\\n\\s+', \"\\n\", line);\n\tline = re.sub(r'^\\s*\\n\\s*$', \"\","}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\t\t\n\tdef pattern_matches(self, regex_text):\n\t\t\"\"\"\n\t\t\tThis method checks the data to see if each string \n\t\t\tin the pattern matches that of the regex_text\n\t\t\"\"\"\n\t\tpattern = REGEX_PATTERNS_REGEX.search(regex_text)\n\t\tif not pattern:\n\t\t\treturn False\n\t\t\n\t\t# Replace each match by a regex\n\t\tregex_pattern = re.compile(PATTERN_REGEXP)\n\t\tfor regex in regex_pattern.finditer(self._regex):\n\t\t\treturn len(regex.groups()) > 0\n\t\t\n\tdef _is_"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\n\tinput_str = str(string)\n\n\tif input_str.find(\"{\") > 0:\n\t\t#input_str = input_str[:input_str.find(\"{\")]\n\n\t\t# This is used for finding the pattern of the whole string\n\t\t# TODO : Check the first regex match is what is needed as regex, not regex -> regex\n\t\toutput_regex = input_str[input_str.find(\"{\") + 1 : ]\n\t\t#print(input_str[0:len(output_regex)])\n\t\toutput_regex = output_regex[:output_str."}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\treturn re.compile(regex)\n\"\"\"\n\ndef main():\n\n\t# Extract the given lines from the text file\n\tlines = extract_strings()\n\t# Replace every word in lines\n\tfor line in lines:\n\t\tline.replace(\"@\", \"e\").replace(\".\", \".\")\n\t\n\t# Normalize and trim whitespace\n\treturn re.sub(\"\\s+\", \" \", lines).strip().split(\"\\n\")\n\t\nif __name__ == \"__main__\":\n\tmain()\nimport time\nimport warnings\n\nfrom django.conf import settings\n\nfrom cassandra.cqlengine import"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef regexfy (str):\n\t\"\"\"\n\t\tRecherche les caracteres d'une calever un string et le nombre dans \n\t\tr\u00e9cup\u00e9ration ou \u00e9gallement le nombre dans dans le cas \u00e0 v\u00e9l\u00e9ment \n\t\ten cas d\u00e9sactiver le cas dans la recherche \n\t\"\"\"\n\tre_1 = re.compile(\"(?<=.*)?.*(?P<dchar>\\S.*?)(?=.*)\", re.I)\n\tre_2 = re.compile(\"\\\\s*?(?=.*)|(?="}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef convert_pattern_to_regex(pattern):\n\t\n\t# Convert pattern into regex pattern\n\tpattern = re.sub('[-\\s]+','', pattern)\n\t\n\t# Convert regex pattern to string\n\tpattern = pattern.replace('.?[^.,?]','')\n\tpattern = re.sub('^[\\*_\\w\\_]*$', '', pattern)\n\tpattern = pattern.replace('..|[\\\\s]','')\n\t\n\t# Convert pattern of type str into dictionary\n\tpattern = re.compile('(['+re.escape(string) +'])').sub(\"'[ ]\", pattern)\n\t\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\t\n\tdef replaceNonAscii(self, value):\n\t\tvalueSplit = value.split(self.__delimiter)\n\t\t\t\n\t\treplace = [v for v in valueSplit if v.find(self.__delimiter)!= -1]\n\t\t\n\t\t# print \"replaceing : \" + str(replace) + \" with : \" + value\n\t\tif replace:\n\t\t\tprint(\"replacing : \" + str(replace[1:]) + \" with : \" + str(value))\n\t\t\tvalue = value.replace(replace[0], replace[1])\n\t\t\n\t\treturn value\n\t\n\tdef checkString(self,"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tret = subprocess.call(cmdstring)\n\t\tif returncode!= 0:\n\t\t\tprint \"ERROR: the command failed with exit code %d\" % returncode\n\t\t\tsys.exit(returncode)\n\n# ----------------------------------- \n#   run_batch.py\t\t\n# ----------------------------------- \ndef run_batch(cmdstring, **kwargs):\n\t\"\"\"\n\tRun a batch of commands using subprocess.Popen() and\n\treturn a generator of all output, in the same format as the result of\n\tsubprocess.Popen().\n\t\"\"\"\n\n\tret = []\n\tfor line in cmdstring"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef callCommand(command):\n\tp = subprocess.Popen(command, \n\t\tstdin=subprocess.PIPE, \\\n\t\tstdout=subprocess.PIPE)\n\treturn(p.communicate()[0].decode())\n\t\n\t\nif __name__ == \"__main__\":\n\t\"\"\"\n\t\tCall the shell command:\n\t\t\n\t\t$ cd schrodinger/src/myproject/SCHOOLINGERS/e_chrodinger2.cmake\n\t\t$ python e_chrodinger2.py --myproject./myproject/SCHOOLINGERS/e_chrodinger"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef write_to_file(cmd=None):\n\tcommand = \" \".join(cmd)\n\tif (not (system_command_result_re.search(command) or os.access(system_command_result_re.search(command), os.X_OK)) and \n\t\tnot (system_python_result_re.search(command) or os.access(system_python_result_re.search(command), os.X_OK))):\n\t\twith open('error_log.txt', 'a') as error_log:\n\t\t\terror_log."}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef RunCommand(command):\n\tif sys.platform == \"cygwin\":\n\t\treturn Popen(command, shell=True, stdout=subprocess.PIPE).stdout\n\telse:\n\t\tif not subprocess.call(command, shell=True, stdout=subprocess.PIPE).stdout and not subprocess.call(command, shell=True, close_fds=True).stderr:\n\t\t\treturn Popen(command, shell=True, stdout=subprocess.PIPE).stdout.read().rstrip()\n\ndef GetCurrentDir():\n    dir = os.path.dirname(os.path.realpath(__file__))\n   "}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t\t\n\t\n\t\n\tdef find_executable(self, command):\n\t\t\"\"\"\n\t\tFind an executable.\n\t\t\"\"\"\n\t\n\t\ttry:\n\t\t\treturn subprocess.check_output(command, shell=True)\n\t\texcept:\n\t\t\treturn False\n\t\t\t\n\t\t\t\n\t\n\t\n\tdef get_version(self):\n\t\t\"\"\"\n\t\tGet the version from the appium executable.\n\t\t\"\"\"\n\t\n\t\tif not os.path.exists(os.path.join(os.path.expandvars('$APPium'), 'Appium', 'Version.plist')):\n\t\t\tself.fatal('appium executable not found')\n\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n__author__ = 'Marvin Pinto'\n__version__ = '0.1'\n__date__ = 'March 11th 2006'\n__updated__ = 'March 10th 2006'\n\n\n# define authorship information\n__authors__         = ['Sebastian Urban']\n__contact__          = 'March 10th 2002'\n__copyright__       = '(C) 2005-2009 Marvin Pinto'\n__license__         = \"LGPL\"\n\nimport os\nimport sys\nimport tempfile\n\nfrom calibre.ebooks.metadata import authors"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef call_call(command):\n\t#print command\n\tsubprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\t\ndef get_output(command):\n\t#print command\n\tsubprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\t\n# call_and_extract_output(command, output)\ncall_call(\"cat \"+out_name)\n#get_output(command)\n\n# Copyright 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef run(cmd):\n\tp = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\treturn p.communicate()[0]\n\t\n\n#TODO: remove this when new script starts\ndef shellquote(s):\n\treturn \"'\" + s.replace(\"'\", \"'\\\\''\") + \"'\"\n\ndef shellquote_pipe(command):\n\ts_pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\treturn s_pipe.communicate()[0]\n\t\n\n\"\"\"\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef get_cmd(cmd):\n\treturn (\n\t\tsubprocess.call(\n\t\t\tcmd\n\t\t\t,\n\t\t\tshell=True\n\t\t).wait()\n\t\t)\n\t\ndef run_cmd(cmd):\n\tprint(\">> \", cmd)\n\tp = subprocess.Popen(\n\t\tcmd,\n\t\tshell=True\n\t)\n\t#print(p.error)\n\twhile True:\n\t\tline = p.stdout.readline()\n\t\tif not line: break\n\t\tprint(line.rstrip())\n\tp.terminate()\n\tp.wait()\n\n#"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef checkResult(commandIn):\n\t\"\"\"\n\tCheck if given command is succeed.\n\t\n\tArguments:\n\tcommandIn -- Command\n\t\tA tuple containing two elements:\n\t\t\n\t\t\t1. Output of STDOUT as unicode.\n\t\t\n\t\t\t2. Return code as int.\n\t\t\n\t\t\t3. Command's exit status.\n\t\"\"\"\n\tretCode = subprocess.call(commandIn, shell=True)\n\treturn(retCode)\n\n\"\"\"\n\tReturn stdout and stderr as integers to be used by RPi.\n\t\"\"\"\ndef raspiIsStdoutStandardOutput():\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\ndef ipv4_addresses_test():\n\t\n\t# Create ipv4 and test it \n\tipv4 = ipaddr.IPv4Address('192.168.0.0/24')\n\tipv4_addresses_in_network = [ str(r[1]) for r in pynub_eth0.get_nub_ethernet_addresses(4)['addresses'] ]\n\t\n\t# Network ranges for IPv4\n\tranges = [[ipv4_addresses_in_network[0]], [ipv4_addresses_in_network[4]], [ipv4_addresses_in_network[4"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\nclass Test_IPV4InterfaceAddress(unittest.TestCase):\n\t\n\tdef test_ipv4_address1(self):\n\t\t\"\"\" Test IPv4 address detection \"\"\"\n\t\t\n\t\tsuccess = False\n\t\ttry:\n\t\t\n\t\t\tself.assertEqual(\"192.168.0.1\", ipaddress.IPv4Address('192.168.0.1').ip)\n\t\t\tsuccess = True\n\t\t\t\n\t\texcept Exception as ex:\n\t\t\tprint \"EXCEPTION %s - %s\" % (sys.exc_info()[0], ex)\n\t\t\t\n\t\tfinally:\n\t\t\tself.assertEqual(0, success)\n\nclass"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\nhost1 = \"1.Host1\"\nhost2 = \"1.Host2\"\nhost3 = \"1.Host3\"\nhost4 = \"2.IP4\"\nhost5 = \"2.IP5\"\nhost6 = \"3.IP6\"\nhostA = \"1.A\"\nhostB = \"4.B\"\nhostC = \"5.C\"\nhost_d = \"5.D\"\nhostD = \"6.D\"\nhostC_d = \"6.D\"\nhostD_d = \"6.D."}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\n\nclass TestIPV4Network:\n\t\"\"\"\n\tIPv4 Network, IPv6 address, no need to be done\t\n\n\t\"\"\"\n\n\n\t# The ipaddress function should return a string\n\tdef assertIPV4Address(self, ipaddr):\n\t\tself.assertEqual(ipaddr_valid(ipaddr), True)\n\n\tdef setUp(self):\n\t\t\"\"\"\n\t\tRuns tests for this class\t\n\t\t\n\t\tSets up a simple IPv4 network object for later use\n\t\t\"\"\"\n\t\n\n\tdef tearDown(self):\n\t\t\"\"\"\n\t\tRuns tests for this class\t\n\t\t\"\"\"\n\t\t# reset the cached"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\nclass Ip4AddressTests (unittest.TestCase):\n\t\t\n\t\"\"\"Test Ip4Address\"\"\"\n\t\t\n\tdef setUp (self):\n\t\t\"\"\"Testcase setup\"\"\"\n\t\tself.ipv4_addresses = [\"192.168.0.0/24\"]\n\t\tself.ipv4_addresses.reverse()\n\t\n\tdef testAdd(self):\n\t\t\"\"\"Test case add\"\"\"\n\n\t\tself.ipv4 = ipaddress.ip_address(bytes(self.ipv4_addresses))\n\t\t\n\t\tresult = self.ipv4_addresses.add((self.ipv4, self.ipv4_addresses)-1)"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\ntest_ipv4_list.append(('127.0.0.1','10'),'1'*4,'1'*32,'0'*32,'0'*32,'0'*32,'127.0.0.1','10','127.0.0.1'))\n\n\"\"\"\nclass TestIPv6LocalRoutes(unittest.TestCase):\n\t\n\tdef runTest(self):\n\t\tprint(\"test_ipv6_local\")\n        \n\tdef tearDown(self):\n\t\tprint(\"End tests\")\t\n\n\n\t\t\n\"\"\"        \nclass TestIPv6AddressMap(unittest.TestCase):\n\n\tdef"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\trun()\n\"\"\"\nA module for managing and working with IPv6 addresses.\n\nThe main classes in this module include :class:`Subnet`.\nInstances of this class must be created by placing a class\nwithin this class, using the methods that create :class:`IPNetwork`\nand :class:`IPNetwork6Address` instances respectively.\n\nWhen an instance is created, the :meth:`BaseIP.create` and\n:meth:`BaseIP.associate_address` methods are called on\na real or very real IP address.\n\nA **"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\n\tdef testParse( self ):\n\t\tself.assertTrue( IP.IsValid( '127.0.0.1' ) )\n\t\tself.assertTrue( IP.IsValid( '0.0.0.0' ) )\n\t\tself.assertTrue( IP.IsValid( '255.0.0.0' ) )\n\t\tself.assertTrue( IP.IsValid( '255.255.255.255' ) )\n\t\tself.assertTrue( IP.IsValid( '0.0.0.255' ) )\n\t\t\n\t\tinvalid = '-1:255.255"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\ntest_ipv4_route_static = \"\"\"\n    bgp TEST_BGP_ADDRESS_OUT\n    address Age 127.0.0.1\n    route 10.0.0.0/24\n    next-hop $6 <vrf>\n    use-vrf {\n        route 10.0.0.0/24 10\n    }\n\tneighbor 127.0.0.0/8 192.168.1.1 255.255.255.0\n    }\n\n    router bgp vrf $6 ${ASR_IPv6_MCAST_ROUTE}\n    address-family ipv4 auto\n    vrf $1"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\t\n\t###############\n\t# Create test IP Address\n\t###############\n\tdef testIPtest01(self, net, IP):\n\t###############\n\t\n\t\t\n\t\n\t###############\n\t# Create test IPv4 address \n\t###############\n\tdef testIPv4test01(self, net, IP):\n\t###############\n\t\n\t\t\n\t\t\n\n###############\n\t# Test IP Address with Address\n\t###############\n\tdef testIPv4test02(self,net, IP):\n\t###############\n\t\n\t\t\n\t\t\n\t###############\n\t# Delete ip Address\n\t###############\n\tdef delIp"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\t# Start off by scanning the subgraph. If this fails, it means it's a leaf node\n\t# and the parent are at level depth 1 (which may not be any sub-class of\n\t# skos)\n\t# Note that there basically does not include the parents\n\t# TODO: does this also work with SKOS?\n\troots = set([root for (d,p,o) in graph.triples((None,RDF.type,None) and graph[(d,p,o)]['depth'],True)])\n\t## For each root, yield only pairs of triples"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, rdflib.RDFS.subClassOf):\n\trootset = set([])\n\tdef walk(node):\n\t\tif node[\"is_a\"] == False and all([not child for child in node[\"subedges\"]]):\n\t\t\twalk(node[\"subnodes\"][0])\n\t\t\tyield node[\"node\"]\t\n\t\t\t\n\t\telse:\t\n\t\t\tif node[\"is_a\"]:\n\t\t\t\tfor child in walk(node[\"subnodes\"][0]):\n\t\t\t\t\tv = rdflib.term.URIRef(\"%s:%s\" % (node[\"nodeid\"], child"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\troots = set()\n\t\n\tsearch = True\n\n\twhile search:\n\t\tfor s in set(e for e in graph.subjects(rdflib.RDFS.subClassOf)):\n\t\t\tif (s, None) not in roots and not search:\n\t\t\t\troots.add(s)\n\t\t\t\tsearch = False\n\t\t\telif (s, None) in roots and not search:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tsearch = False\n\n\treturn roots\n\t\ndef find_roots_by_class(subjects_a, subjects_b, parent=None):\n\t\"\"\"Returns all the"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, rdflib_class, direction=\"subClassOf\" ):\n\t\"\"\"\n\tGenerate a list of RDF objects which represent a given class of the given graph.\n\t\n\tThis is an adhoc generator which iterates through all the classes (e.g.\n\tsubClassOf and skos) and for each class graph, which are traversed until the\n\tclass list is exhausted\n\t\n\tThis allows for iterating over all the RDF objects in a given graph.\n\t\n\t@type graph: rdflib.Graph\n\t@param graph: Graph\n\t\n\t@type rdf"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\n\tdef _get_type(self, node):\n\t\ttypes = ['node','subPropertyOf','skos:subClassOf','subClassOf']\n\n\t\tfor t in types:\n\t\t\tif(subClassOf[t] == \"Concept\"):\n\t\t\t\treturn t\n\n\t\t#if the subject is an object\n\t\treturn node\n\n\tdef find_roots(self, graph, rdflib.RDFS.subClassOf):\n\t\t\"\"\" Returns an iterator that yields every root of node for each sub_class of this class \"\"\"\n\n\t\tfor node in self._get_children("}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, rdflib_subClassOf):\n\t# NOTE: This will return all roots of the triples that are not in\n\t# subgraph of the graph.\n\t\n\tgraph_roots = set()\n\tfor s, p, o in triples(graph):\n\t\trdflib.rdflib.Graph(o).bind('owl:NamedIndividual')\n\t\tfor p, oi in triples(p):\n\t\t\tgraph_roots.add((oi, rdflib.RDFS.subClassOf, o))\n\t\n\t# NOTE: We can do that all roots in"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, rdflib.RDFS.subClassOf):\n\t\n\t\n\t# the starting and ending positions can be None, since that is how to interpret\n\t# root(subClassOf): \n\t#\tfor s in graph.subjects(RDF.type, SKOS.broader):\n\t#\t\tif s!= rdflib.RDF.type:\n\t#\t\t\tcontinue\n\t#\t\tif s == rdflib.RDFS.subClassOf and graph.get_backbone() == rdflib.BNode(graph, SKOS.broader):"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\n\n\n\ndef find_roots(graph, rdflib.RDFS.subClassOf):\n\t\"\"\"\n\tfind_root_subtypes(graph, rdflib.RDFS.subClassOf, rdflib.RDFS.Class)\n\n\twill find all the class's descendants\n\t\"\"\"\n\troots = set()\n\t\n\tfor i, c in enumerate(graph.objects(subClassOf = rdflib.RDFS.Class)):\n\t\tdfs = c.subClassOf\n\t\tif issubclass(dfs, RDFS): \n\t\t\troots.add(dfs)\n\t\n\tfor id in graph"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\tdef find_roots(graph, rdflib_subclass):\n\t\t\"\"\"Retrieve a set of all the roots in some sort of transitive hierarchy.\"\"\"\n\t\t\n\t\tvisited_subclass = set()\n\t\t\n\t\tvisited_subclass.add(rdflib_subclass.store)\n\t\tvisited_subclass.add(rdflib_subclass.component)\n\t\t\n\t\tfor (sub_class, prop, sub_subclasses) in rdflib_subclass.subClassOf:\n\t\t\tif sub_class in visited_subclass:\n\t\t\t\t# Ignore self loops as they don't need to be visited"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, path, predicate=None):\n\t\"\"\"Returns a set of all the root elements of a given root.\n\t\n\tIf you don't want this to look at the root, use the predicate\n\t\"\"\"\n\traise NotImplementedError(\"Use find_roots instead.\")\n# coding: utf-8\n# pylint: disable=locally-disabled, unused-wildcard-import\n\"\"\"Constants for Google Assistant.\"\"\"\n\nfrom collections.abc import MutableSet\nfrom typing import Any\n\nfrom aiohttp.client_exceptions import ClientConnectionError, \\\n    ClientResponseError\nfrom homeassistant.const import"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\"\"\"\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport json\nimport platform\nimport datetime,time\nimport csv\n#import yaml\nfrom pprint import pprint\nfrom datetime import datetime, date, time\nfrom collections import OrderedDict\nimport re\nimport urllib2\nimport collections\nimport copy\nimport requests # pip install requests\nimport csv\nfrom.base_module_info import Base_ModuleInfo\nfrom.py_utility import Py_Parse, PyFile_Parse, PrettyFormatAny, FormatJSON\nfrom io import StringIO\n\n\nclass Py2C(object):\n\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\n\tdef __getitem__(self, key):\n\t\treturn self.find(key)\n\n\tdef find(dictionary, key):\n\t\t\"\"\"\n\t\tReturn value for key in dictionary\n\n\t\tDictionary has the following structure:\n\t\t* list\n\t\t* string\n\t\t* unicode (Python 2)\n\t\t\"\"\"\n\t\tif isinstance(dictionary, basestring):\n\t\t\treturn dictionary[key]\n\t\telif isinstance(dictionary, collections.Mapping):\n\t\t\tresult = {}\n\t\t\t# Get each entry individually for current level\n\t\t\tfor key in dictionary.keys():\n\t\t\t\tresult[self.find(key)]"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, contents):\n\t\t\"\"\"\n\t\tInitialize a new instance of the yaml file\n\t\t\n\t\t:contents:\tA parsed yaml document\n\t\t\"\"\"\n\t\t\n\t\tself.contents = contents\n\n\tdef dump(self, fh = None) :\n\t\t\"\"\"\n\t\tDumps a py2-unicode or py3-string to a string\n\t\t\"\"\"\n\t\t\n\t\tself.dump_string(self.contents)\n\t\tprint \"done dump\"\n\t\t\t\n\t\n\tdef dump_string(self, contents):\n\t\t\"\"\"\n\t\tThis function dumps to a string.\n\t\t\n\t\t:contents:\tA"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\n\n__all__ = [\"dumps\"]\n\n\nimport numpy, inspect, time, codecs, re, sys, codecs\n\ndef dumps(obj, file_name, **kwargs):\n\t\"Simple wrapper around `pickle.dump` that uses cPickle\"\n\tif isinstance(obj, numpy.ndarray) and scipy.sparse.issparse(obj):\n\t\tf = cPickle.dumps(obj, protocol=2)\n\telif isinstance(obj, numbers.Number):\n\t\tf = cPickle.dumps(obj, protocol=2)\n\telse:\n\t\tf = cPickle.dumps(obj, protocol"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\t\n\t__version__ = 0.1\n\t__author__ = 'Christoff Kok, Haaga Mackinson'\n\t__contact__ = 'christoff.kok@gmail.com'\n\t__copyright__ = 'Copyright 2014, Haaga Mackinson'\n\n\tdef __init__(self, dumpfile=None, encoding='utf-8', error_bad_doc=None):\n\t\t\"\"\"\n\t\tLoad dump\n\n\t\t:param str dumpfile: Name of file to load from\n\t\t:param str encoding: Encoding to load the file"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\t\n\ttext = []\n\tif not isinstance(input_file, file):\n\t\ttry:\n\t\t\tinput = open(input_file, \"r\", encoding='utf-8')\n\t\t\ttext = input.read()\n\t\texcept IOError as e:\n\t\t\ttext = \"ERROR:\\nERROR: Unable to open %s (%s)\\n\\n\" % (input_file, e)\n\t\t\t\n\t\texcept Exception as e:\n\t\t\ttext = \"ERROR:\\nERROR: Unable to open %s. Please run convert_template.py\\n\\n\" % input_file\n\t\t\t\n\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n# TODO: make this more robust to the formatting issues etc\n\n__all__ = [\n\t'YAML_Dumper'\n]\n\n\nYAML_Error = YAML_Error\n\nYAML_NodeType = Union[list, dict]\nYAML_EdgeType = Union[list, dict]\nYAML_MappingType = dict\n\nclass YAML_Scanner(yaml.Scanner):\n\tdef __init__(self, *args, **kwargs):\n\t\tkwargs['file_error'] = YAML_Error\n\t\tyaml.Scanner.__init__(self, *args, **kwargs)\n\n\tdef scan("}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\tif isinstance(value, unicode):\n\t\t\tvalue = value.replace(\"'\", \"''\")\n\t\telif isinstance(value, str):\n\t\t\tvalue = value.replace('\"','')\n\t\telif isinstance(value, list):\n\t\t\tvalue = map(unicode.__str__, value)\n\t\telif isinstance(value, tuple):\n\t\t\tvalue = tuple([unicode(item) for item in value])\n\t\treturn yaml.safe_dump(value, default_flow_style=False)\n\n\tdef write(self, output_format, content):\n\t\t\"\"\"\n\t\tDump to the given filename using specified format.\n\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ndef dump(v): \n\t\"\"\"\n\tDump an item as a list\n\t\"\"\"\n\tif isinstance(v, (list, tuple)):\n\t\tfor i in v:\n\t\t\tdump(i) \n\tif isinstance(v, dict):\n\t\tdump(v.items())\n\telif isinstance(v, basestring):\n\t\ttry:\n\t\t\to = StringIO()\n\t\t\tdump(_unicode(v), o, -1)\n\t\texcept:\n\t\t\to = StringIO()\n\t\t\tdump(_str(v), o, -1)\n\nclass YamlEncoder(json.JSONEncoder):"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ndef pprint(d, indent=\"\\t\"):\n\t\"\"\"\n\tPretty print d, indent to a specified number of spaces\n\t\"\"\"\n\t\n\t# remove newlines\n\tl = d.split(\"\\n\")\n\tr = [indent+l[0]] # put them back in the correct order\n\tfor l in l[1:-1]:\n\t\tr.append(l[0]=l[0]+\" \")\n\t# build a nice, ugly format string\n\tr = ', '.join(r)\n\t# convert to ascii\n\tr = ''.join(r)\n\t#"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nimport logging,time,traceback,sys,os,re\nfrom..loggers import LoggerManager\nfrom..version import __version__, __description__, __date__\n\n# set default formatter to standard\nformatter = logging.Formatter('[%(asctime)s] %(levelname)s:\\t%(name)s- %(message)s')\n\n# default settings\n#LOG_LEVEL = logging.INFO\n#LOG_PATH = '/var/log/app.log'\n#LOG_FORMAT = \"%(name)s: %(levelname)s: %(message)s\"\n\n# Logger for the application that"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass AppLogger(object):\n\t\"\"\"\n\t\tLogic of logging module\n\t\"\"\"\n\t_logger_factory = logging.Logger\t# create logger object\n\n\n\t\nclass LoggerFactory(object):\n\t\"\"\"\n\t\tClass for logging the console output to stdout\n\t\t\"\"\"\n\t@staticmethod\n\tdef stdout():\n\t\t\"\"\"\n\t\t\tReturn console logger obj\n\t\t\"\"\"\n\t\treturn AppLogger._logger_factory.stdout()\n\n\nif __name__ == \"__main__\":\n\n\tlogger = LoggerFactory.get_log()\n\nimport logging\nimport traceback\n\nfrom couchpotato.core"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\timport logging \n\tlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\tlogger = logging.getLogger()\n\tlogger.info(u'\\n%s' %''.join(map(lambda x: '%s' % x, sys.argv)))\n\tlogger.info(u'\\x1b[30m Running SinQueens %s' % os.getpid())\n\tlogger.info(u'\\x1b[1;"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n  \tdef __str__(self):\n        \tres = [self.appname,self.loglevel,self.logfile]\n        \t\tfor key in self.__dict__:\n        \t\t\tif self.__dict__[key]:\n        \t\t\tres.append('%s: %s' % (key,str(self.__dict__[key])))\n        \t\n        \t\treturn '\\n'.join(res)\n        \t\n        \t\n        \tsys.stdout = oldstdout\n        \tself.logger.info(self)\n\"\"\"\n# this logger will be auto-detected in the same time...\nclass AppLogger(Logger):"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, logPath):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself.logPath = logPath\n\t\t\n\t\t# Set up logging\n\t\t# create a rotating log file that logs even debug events\n\t\tlogfile = os.path.join(self.logPath,\"rotating.log\")\n\t\t\n\t\tself.logger = logging.getLogger('Main')\n\t\tself.logger.setLevel(logging.DEBUG)\n\t\thandler = logging.FileHandler(logfile)\n\t\thandler.setLevel(logging.DEBUG)\n\t\tself.logger.addHandler(handler)\n\t\t\n\t\t# Create Rot"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ndef log_message(message, level=logging.INFO):\n\t\t\"\"\"\n\t\tLog message to log file. You can pass the level name as an argument\n\t\t\"\"\"\n\t\tlog = logging.getLogger(\"CryptotGeneticHash\")\n\t\tlog.setLevel(level) # Make sure level is a string (not a number!)\n\t\tlog.message(message) # Add a message to show debug information\n\t\t#log.setLevel(level) # Make sure level is a string (not a number!)\n\tlog.fatal(\"ERROR: \"+message)\n\t\ndef errorExit(e"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nlogging.basicConfig()\t\nclass Loggers():\n\t\"\"\"\n\tLoggers:\n\t\tLogging logger\n\t\tLoggers get information\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tLog configuration.\n\t\t\"\"\"\n\t\t#create logger\n\t\tself.logger = logging.getLogger('logger')\n\t\tself.logger.setLevel(logging.ERROR)\n\t\tself.logger.addHandler(logging.StreamHandler())\n\t\t#load logger config files\n\t\tself.load_config()\n\t\tlog = logging.getLogger('logger')\n\t\tlog.setLevel(logging.ERROR)\n\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\n\nDEBUG_LOG_LEVEL = 2\nif DEBUG_LOG_LEVEL <= 1:\n\tLOG_LEVEL=\"DEBUG\"\nelif DEBUG_LOG_LEVEL <= 2:\n\tif os.environ.get(\"WERKZEUG_LOG_LEVEL\", \"DEBUG\") == \"DEBUG\":\n\t\tLOG_LEVEL=\"INFO\"\nelse:\n\tLOG_LEVEL=\"WARNING\"\n\nLOGGING = {\n  \"version\": 1,\n  \"disable_existing_loggers\": False,\n  \"formatters\": {\n        \"standard\": {\n            \"format\": \"%(asctime).19s %(name)s-%(levelname"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ndef setupLogger():\n\t# configure logger\n\tlogging.basicConfig(\n\t\tlevel=logging.INFO,\n\t\tfile=os.path.join(os.getcwd(),\"whisper.log\"),\n\t\tformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n\t\tdatefmt='%m-%d-%Y %H:%M:%S %p',\n\t)\n\t\n\t# create file handler\n\tf = logging.FileHandler(os.path.join(os.getcwd(),\"whisper.log/output.log\"))\n\tl"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\t\n\tdef initLogging(self,level='info'):\n\t\t\n\t\t# set up basic logging\n\t\tlogging.basicConfig(filename='log.log',level=level, format='%(levelname)s:%(asctime)s:%(funcName)s:%(lineno)d:%(levelname)s:%(process)d:%(thread)d: %(message)s', datefmt='%b %d %H:%M:%S')\n\t\t# a list of all loggers\n\t\t# (to not confuse with multiple app)\n\t\tlog1 = logging.getLogger()\n\t\t\n\t\t# add a file handler to remove all old log handlers\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\nclass T_array(object):\n\t\"\"\"\n\tClass for creating arrays with shape and dtype.\n\t\"\"\"\n\tdef __init__(self,shape,dtype):\n\t\tself.shape = shape\n\t\tself.dtype = dtype\n\t\t\n\tdef __getitem__(self,x):\n\t\treturn np.array(x)\n\t\n\t@classmethod\n\tdef get_array(self):\n\t\treturn T_array(self.shape,self.dtype)\n\n\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\n\n\ndef shape(object, dtype=float):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tobject : `ctypes.c_void_p`\n\tdtype :  `numpy.dtype`\n\t\t\n\tReturns\n\t-------\n\tshape : tuple\n\t    shape of the object array\n\t\"\"\"\n\t\n\tif not isinstance(object, ctypes.POINTER(c_int)):\n\t\n\t\tshape_array = object\n\t\tobj_shape = object.shape\n\telse:\n\t\tobj_shape = object\n\n\t\n\t\n\tshape_array = np.lib"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\"\"\"\ndef create_empty_array(shape, dtype=\"int32\"):\n\t\"\"\"\n\tCreate an empty array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\n\"\"\"\ndef create_empty_array_with_dtype(shape, dtype):\n\t\"\"\"\n\tCreate an empty array with a specific data-type.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array that should be returned\n\t\n\"\"\"\ndef get"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\tshape_l = (shape[0].item()//2,shape[0].item(),shape[1].item()//2)\n\tshape_u = (shape[1].item(),shape[1].item(),shape[0].item()//2)\n\tshape_w = (shape[0].item(),shape[1].item(),shape[0].item())\n\tshape_s = shape[0].item()//2\n\tshape_t = np.dtype(dtype)\n\tshape_l.extend(shape_l)\n\tshape_u."}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\t# Create array with 0's on diagonal\n\tnp.fill_diagonal(np.diag(np.ones((1, shape[1]))[0] / shape[1], 1), 0)\n\t\n\t# Create array with 1's on diagonals\n\tnp.fill_diagonal(np.diag(np.ones(shape) * shape[1], -1), 0.0)\n\t\n\t# Get dimensions of array\n\tshape_2d = shape[:]\n\tshape_3d = shape[1:]\n\t\n\t# If matrix has 4 elements it should be 2d\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\tshape = np.array(shape)\n\tdtype = np.dtype(dtype)\n\t\n\t# Create the array\n\tD1 = np.zeros(shape=shape, dtype=dtype)\n\t\n\t# Return the array\n\treturn D1\n\n\"\"\"\n\tFunction to create a 2d numpy array of shape.\n\t\n\tParameters\n\t----------\n\tn : `np.int32`\n\t    number of dimensions\n\t\"\"\"\n\ndef create_2d_2Darray_2D(n, dtype=\"int32\"):\n\treturn create_2darray(n, dtype)"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\n\tif (len(shape)!=2) or (len(shape)!=3):\n\t\traise ValueError('shape should be tuple of two or three elements')\n\n\tif shape[0]!=shape[1] or shape[1]!=shape[2]:\n\t\traise ValueError('shape should be shape a square array')\n\n\tif shape[0]==shape[1]:\n\t\tif not 'int' in data or data['int'] is None:\n\t\t\t# this will create a vector and set everything in the index. \n\t\t\t# this will go to a list which will go one row/"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\ndef _array2d(arr, dtype):\n    # If dtype is not None, we're to enforce the dtype.\n    # If in theory numpy returns a view then change it to a view.\n    \n    if dtype and not arr.dtype.isnative:\n        arr = arr.view()\n    return arr\n\ndef _array2d2(arr, dtype):\n    # When we don't know the length of its elements, we need to calculate size as the size of the subarray and use\n    # that instead.\n    # Note that even if one of arr is a np.matrix return type, then dtype"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\ndef make_ndarray(shape, dtype):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tnp.random.seed(1)\n\treturn np.random.randn(*shape).astype(dtype)\n\t\ndef test_array():\n\t\"\"\"\n\tTests the array class.\n\t\n\tTest the output variable type: int.\n\t\n\t\"\"\"\n\t\n\tshape = (3, 5)"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\tif len(shape) == 2:\n\t\treturn np.array(shape)*dtype\n\telif len(shape) == 1:\n\t\treturn np.fromstring(shape[0])\t\n\t\n\telse:\n\t\treturn np.array(shape)*dtype\n\ndef normalize(matrix, mean = None, stdev = None, **kwargs):\n\t\"\"\"\n\tNormalize by standard deviation.\n\t\n\tThe mean and stdev can be scalars only. The vector components \nare averaged with the given mean and standard deviation:\n\t\n\t* the mean, stdev components\n\t* all stdevs"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nfrom math import sqrt, atan, fabs\n#import matplotlib.pyplot as plt\nfrom scipy import exp, asinh\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import norm\nfrom scipy.special import lognorm\nfrom scipy.optimize import leastsq\nfrom scipy.stats import norm\nfrom. import *\nfrom.functions import *\nfrom. import __main__\n\ndef get_bounds(a,b):\n    '''\n        min_x - a, m_x - b\n        m_x and m_x are points that have min( x, m_x"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef meanplusZero2(size):\n\t\"\"\"\n\tA function that returns (a) a function that measures the value\n\tand returns it as the mean of a given distribution. This is\n\tthe function that is defined in the Gaussian distribution.\n\t\"\"\"\n\t#return (size * 2 - size / 2.) * np.exp(-(size**2) / 2.)\n\t#return -(size + size / 2.) * np.exp(-(size**2) / 2.)\n\treturn (size - size) * np.exp(- (size - size) / 2"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\n\tmu=50\t\n\tsigma=20\n\n\tX = np.random.normal(loc=mu, scale=sigma**2, size=100)\n\n\ty = X + np.random.normal(loc=mu, scale=sigma**2, size=100)\n\n\ts = np.empty((X.shape[0],))\n\ty0s = y[0]\n\ty0s[s] = y[0]\n\tfor r in range(1, X.shape[0]):\n\t\tX[r] = s*X[r-1"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nN=128\n\t\n\n\nclass ball(object):\n    \"\"\"\n        Convex ball with gaussian centered around 0.2\n    \n    Parameters:\n        f: function\n            Function which computes ball in one-dimensional coordinate space\n        k: float (optional)\n            Value of kernel used to map onto density\n\n    Returns:\n        ball: function\n            Method to return the gaussian on the ball with given data on the given kernel\n        N: int\n            Number of independent samples in the ball\n    \n    Example:\n        >>> # x = np.linspace(0.0,2.0,10)\n        >>> # b = b"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\n\tdef __init__(self,sigma=0.1,alpha=0.1):\n\t\t\"\"\"\n\t\tCreate Gaussian centered around the center 0.2.\n\t\tThe gaussian is defined by a GaussianKernel with the given parameters.\n\t\t\"\"\"\n\t\tself.sigma = sigma\n\t\tself.alpha = alpha\n\t\tself.x = np.linspace(-1e-5,1e-5,100)\n\t\tself.sigma_x = self.x[1] - self.x[0]\n\t\tself.sigma_y = self.x[2] - self.x"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\tpass\n\n#import pdb\n#import ROOT as r\n#ROOT.PyConfig.IgnoreCommandLineOptions = 1\n#from ROOT import pythonization\n#\n#from skimage import filters\n#from skimage.color import gray2rgb\n#from skimage import measure\n#\n#from skimage import morphology\n#import warnings\n#from collections import Counter\n#import collections\nimport os,sys\ntry: # for Python 3\n\tfrom urllib.request import urlretrieve\n\tfrom urllib.request import urlopen\n\timport urllib2"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n   def __init__(self, center, sigma):\n\t\tm = np.ones(len(center)) * sigma / np.sum(center)\n\t\tself.center = center\n\t\tself.sigma = sigma\n\t\tself.M = m\n\t\t#set input\n\t\tself._input_shape = None\n\t\tself._input_size = None\n\t\tself._n_centers = None\n\t\tself._centers = None\n\t\tself._n_params = None\n\t\tself._param_bounds = None\n\t\t\n\t@property\n\tdef input_shape(self):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\t\t\n\t# Set up our random generator\n\tnp.random.seed(0)\n\tsigma_range = np.arange(0.1, 0.5, 0.05)\n\t\n\t# Generate x and y coordinates for Gaussian\n\tx_coord = np.random.randn(50, npoints)\n\ty_coord = x_coord + sigma_range * np.random.randn(100, npoints)\n\n\t# Add a Gaussian noise to the data\n\tx_coord_noise = np.random.randn(500)\n\ty_coord_noise = np."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef make_gaussian(mu, sigma, mu_min, mu_max, sigma_min, sigma_max):\n\ta = mu\n\tB = sigma\n\n\tdef fun(val):\n\t\treturn a * np.exp(- 0.5 * (val * (1 + x))**2 + b)\n\n\tres = minimize(fun, [mu, sigma])\n\treturn res[0]\n\ndef make_gaussian_big_gaussian_gaussian(mu, sigma, mu_min, mu_max, sigma_min, sigma_max, f, gamma):\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nfrom numpy import inf as infinity\nfrom scipy import optimize\n\ndef pearson(a, b, size=1):\n    \"\"\" Calculate the pearson's chi-squared probability.\n    \"\"\"\n    return 2/(2*(norm(b-a)) + abs(norm(b-a)))\n\ndef polyfit(x, y, deg, rcond=3, full=False, wvar=5, eps=1e-8,\n            pexact=None, err=None, condition=None, **kwargs):\n    \"\"\"\n    This routine fits a line to a line given the points of"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\t\n\tdef _parse_configuration_file(self, configuration_filenames, logger):\n\t\t# Set up log file\n\t\tlog_file = self._get_log_file_handle_for_errors(configuration_filenames, logger)\n\t\tlogging.root.setLevel(logging.WARNING)\n\t\tlogging.root.setFormatter(logging.Formatter('%(asctime)-15s %(levelname)-8s %(message)s', '%Y-%m-%d %H:%M:%S'))\n\t\t\n\t\ttry:\n\t\t\tconfig_dict = {}\n\t\t\tprocessed_errors = set()\n\t\t\twith open(log_"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\n\tconfiguration_errors = {}\n\tfor configuration in config_filenames:\n\t\tlogger = logging.getLogger(configuration)\n\t\t# logger.debug('Parsing configuration %s', configuration)\n\n\t\tfor regex_config_file in configuration_files:\n\n\t\t\tconfiguration_error = {}\n\t\t\t# logger.debug('Loading configuration file \"%s\"', configuration)\n\n\t\t\ttry:\n\t\t\t\tfile_handle = open(regex_config_file, 'r')\n\t\t\t\tconfiguration_error = [re.match(regex, line) for line in file_handle.readlines()]\n\n\t\t\texcept Exception as e:\n\t\t\t"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\ndef extract_configuration_file(file_name, verbose):\n\t\"\"\"\n\tReturns a Configuration object parsed from the filename. If the file does not represent\n\ta specific configuration, this will return None.\n\t\"\"\"\n\ttry:\n\t\tif not file_name.endswith(\".conf\"):\n\t\t\terror_msg = \"Wrong config/configuration filename specified: \" + file_name + \".\"\n\t\t\terror_msg += \" Supported filenames: \" + \", \".join(FileMetaDataParser.supported_file_extensions)\n\t\t\terror_msg += ', '.join(ConfigFileMetaDataParser.supported_"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n# pylint: disable=R0912\n# disable=R0913\n\n_LOGGER = logging.getLogger(__name__)\n_LOGGER.setLevel(logging.INFO)\n\n_LOG_FORMAT = (\n    '%(asctime)s '\n    '%(levelname)s '\n    '[%(module)s:%(lineno)d] [%(name)s]'\n   '%(message)s'\n)\n_LEVEL_TO_LINE_MAP = {\n    'DEBUG': logging.DEBUG,\n    'INFO': logging.INFO,\n    'WARN': logging.WARN,\n    'ERROR': logging.ERROR,\n    '"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\t\n\tdef getFileParser(self):\n\t\ttry:\n\t\t\tmoduleNames = []\n\t\t\tfor configFileName in os.listdir(self.__defaultFileDir):\n\t\t\t\tconfigFileName = os.path.join(self.__defaultFileDir, configFileName)\n\t\t\t\tif configFileName[-3:] == \".cfg\":\n\t\t\t\t\tmoduleNames.append(self.__parseConfig(configFileName))\n\t\t\treturn ModuleInfoParser(moduleNames)\n\t\texcept ImportError:\n\t\t\traise Exception(\"Unable to find config file: %s\" % (os.path.realpath(__file__)))\n\t\n\tdef __parseConfig("}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\n\t_initialized = False\n\tdef __new__(cls, configFileName, logFileNameOrName, parserArguments):\n\t\t\"\"\"\n\t\tConstructor\n\n\t\tArgs:\n\t\t\tConfigFileName: string containing a configuration file.\n\t\t\tlogFileNameOrName: string storing the parsed configuration file name.\n\t\t\tparserArguments: parser options.\n\t\t\"\"\"\n\t\tif _initialized:\n\t\t\traise Exception(\"Configuration file not initialized!\")\n\t\t_initialized = True\n\t\t\n\t\ttry:\n\t\t\tconfiguration = ConfigParser.ConfigParser(interpolation = None)\n\t\t\tconfiguration.read(configFileName)\n\t\t\tlogFileNameOrName = configFileName"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\n\tdef get_configuration_log_filename(self):\n\t\treturn os.path.join(\n\t\t\t\tself.config_dir, \"%s_%s_%04d.conf\" % (self.config_filename, self.session_number, self.run_number) )\n\n\n\tdef get_configuration_directory(self):\n\t\t\"\"\"\n\t\tReturn the directory that the configuration file and log files are being written.\n\t\t\"\"\"\n\t\treturn os.path.join(\n\t\t\t\tself.config_dir, \"%s_%s_%04d.conf\" % (self.config_filename"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\n\tlogger = logging.getLogger(__name__)\n\n\n\tdef __init__(self, filenames=None, required=True):\n\t\t\"\"\"\n\t\tGiven a sequence of configuration filenames, load and validate each configuration file.\n\t\tIf the file is invalid, a ParseError is raised.\n\n\t\tValidates all configuration files in *filenames* and ensures that there is only one configuration file.\n\n\t\tThe ``required`` parameter determines whether the configuration file is required to succeed, and that\n\t\tthat configuration file must not exist when it is instantiated.\n\t\t\"\"\"\n\n\t\tself.__required = required\n\n\t\tparser = SafeConfigParser()"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nLOG_LEVEL = logging.WARNING\nLOG_FILENAME = 'parser.log'\n\n\tdef log_handler(logger, name):\n\t\t\"\"\"Record log messages one line per file of logger's 'name'.\n\t\t\"\"\"\n\t\tglobal LOG_FILENAME\n\t\ttry:\n\t\t\tlogger.log_record(name, LOG_LEVEL, ''.join(logger.buffer))\n\t\texcept Exception as e:\n\t\t\tlogging.warn(\"log_handler: %s\" % e)\n\t\t\treturn (name, LOG_FILENAME, None, None)\n\t\t\t\n\t\treturn (name, LOG_FILENAME, None"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\t\n\t# This is how the configuration files are read\n\tconfigs = list()\n\twith open(CONFIGFILE, 'rb') as f:\n\t\tread = f.read() # Just read in the config file.\n\t\twhile read:\n\t\t\tlines = read.split('\\n') # We are in this format; we still see a few lines\n\t\t\tif lines[__len__()-1] == '': continue # Ignore empty lines\n\t\t\ttry:\n\t\t\t\tvalues = [x.strip() for x in lines[lines.__len__()].split(',')] # We take the content of each line, and parse"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\tif isinstance(obj, string):\n\t\ttry:\n\t\t\tif os.path.exists(obj):\n\t\t\t\tf = open(obj,'rb')\n\t\t\telse:\n\t\t\t\tf = open(urllib.pathname2url(obj),\"rb\")\n\t\t\t# Read only the uncompressed bytes\n\t\t\tdata=f.read(4*4).decode('UTF-8')\n\t\t\tf.close()\n\t\t\treturn np.frombuffer(data,dtype='uint8')\n\t\texcept IOError:\n\t\t\treturn numpy.ndarray(dtype='uint8')\n\n\telif isinstance(obj, np."}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n        with open(obj, 'rb') as f:\n            return f.read()\n\n\ndef write_ascii_file(obj, fname, val):\n    \"\"\"\n    This function prints the bytes object to a file.\n    \"\"\"\n        with open(fname, 'w') as f:\n            f.write(val)\n\n\ndef write_ascii_numpy_list_to_file(fname, obj, data_list):\n    \"\"\"\n    Writes out a file in Numeric format to a file.\n    \"\"\"\n        if not data_list:\n            return\n        with open(fname, 'w') as f:\n           "}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\tif isinstance(obj, basestring) or isinstance(obj, numpy.ndarray): \n\t\tobj = obj.decode('utf-8')\n\n\treturn obj\n\n\n\ndef getBytes(obj, byteSize=None):\n\t\"\"\"\n\tReturns the bytes object corresponding to ``obj``\n\tin case it is a string using utf-8.\n\t \n\tIf `byteSize==None` then the returned object is non-empty. \n\tIf the `data=True` flag then the returned object is a numpy array.\n\t\"\"\"\n\n\tdata = None\n\n\tif not byteSize: \n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef read_bytes(fd,n_bytes):\n    \"\"\"\n       This function reads bytes from a file like object, in the form of a\n       buffer. \n\n    Parameters:\n    fd (str):      the file like object to be read from \n    n_bytes (int): number of bytes to read\n    Returns:\n    (str):       the string read and the number of bytes read\n    \"\"\"\n\tf = open(fd,\"rb\")\n\tbytes = numpy.fromstring(f.read(n_bytes),n_bytes)\n\treturn(bytes)\n    \ndef write_bytes(fd,a,"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t         return numpy.frombuffer(StringIO(obj))#### PATTERN | EN\n# -*- coding: utf-8 -*-\n# Copyright (c) 2010 University of Antwerp, Belgium\n# Author: Tom De Smedt <tom@organisms.be>\n# License: BSD (see LICENSE.txt for details).\n# http://www.clips.ua.ac.be/pages/pattern\n\n# Imports\nimport logging\nfrom os import path\n\n# Imports from Django\nfrom django.db import models\nfrom django.shortcuts import get_object_"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n#\tTODO: Change to return list of bytes if it is str?\n\n\treturn obj.encode(\"utf-8\")\n\t\ndef b2a_list(s, encoding = \"utf-8\", errors= \"strict\"):\n\t\n    \"\"\" \n    Convert string to list using specified encoding\n    \n    (c) 2015 Ryan Armstrong http://ryanarmstrong.com/\n       \n    \"\"\"\n    if isinstance(s, list):\n        return [b2a_list(x, encoding, errors) for x in s]\n    else:\n        return [s]\n\ndef b2a_"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef load_bytes(fh):\n\tout = fh.read()\n\treturn numpy.frombuffer(out,dtype=numpy.int32),numpy.int32\n\ndef load_str(fh):\n\tres = fh.read()\n\tif res[-1]==\":\":res = res[:-1]\n\treturn res\n#! /usr/bin/env python\n\n\"\"\"\n\"\"\"\n# this file is part of the Pameld source generator\n# https://github.com/hearsaycorp/Pameld/\n# :copyright: (c) 2013"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\t\n\tif isinstance(obj,str):\n\t\tif len(obj) == 0:\n\t\t\tobj = ''\n\t\tobj = obj.encode('utf-8')\t\n\timport sys\n\tif sys.version_info[0] == 3: \n\t\tif isinstance(obj, unicode):\n\t\t\tobj = obj.encode('utf-8')\n\treturn obj\n\t\t\n\n#def extract_obj_from_header(filename):\n#\t\"\"\"\n#\t\tThis function reads in a file a given\n#\t\tread in a header and extract objects, which can\t\tbe either"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\tif not isinstance(obj,basestring):\n\t\ttype_string = type(obj)\n\t\tif type_string is not unicode:\n\t\t\tobj = str(obj)\n\n\t\treturn obj\n\telse:\n\t\tif type(obj)==str:\n\t\t\tobj = obj.encode('UTF-8')\n\n\t# if it is a byte string like object, just return it.\n\t# this way we can make sure the bytes cannot be interpreted \n\t# as Latin-1.  This helps for very strange behaviour\n\tif isinstance(obj,bytes):\n\t\treturn obj\n\t\n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\t\n\tif hasattr(obj,\"__str__\"):\n\t\t# for python 3\n\t\tfrom io import StringIO\n\t\tl = [o.rstrip(\"\\n\") for o in obj]\n\t\treturn \"\\n\".join(l)\n\telse:\n\t\treturn obj.__unicode__()\n\t\n\n\ndef read_bytes(fname):\n\t\"\"\"\n\tA de-compress-decode iterator. \n\t\"\"\"\n\twith open(fname, 'rb') as f:\n\t\tfstr = StringIO(f.read())\n\treturn numpy.fromfile(fstr,dtype='uint8',count=1"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t    \n\t# Times per second - must be in range\n\tticks = ticks / float(365 * 24 * 60 * 60) + 1\n\t\n\t# Generate the time in seconds\n\ttime = ticks % 86400\n\tif tz is not None: time = datetime.fromtimestamp(time)\n\t\n\t# Initialize the DateTime object\n\treturn datetime.fromtimestamp(time)\n\nUTC = pytz.UTC\nEST = pytz.timezone('EST')\nEATME = pytz.utc.localize(datetime.utcnow())\nFEDNESDAY = datetime.strftime(EST, \"%Y-%m-%d"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\t# get a timezone - all timestamps are ignored\n\td = date2num(ticks, tz=tz)\n\t# first convert to floating\n\tticks_out = d + ts_from_decimal_year(d.year, d.tz, d.microsecond, d.resolution)\n\t\n\t# now calculate the fractional seconds past midnight\n\tfrac = 1000000 * (ticks_out // 1000000000)\n\treturn cls(ticks_out // 1000000000, frac)\n\n\n#!/usr/bin/env python3\n# Copyright (c) 2018 The Bitcoin Core developers\n# Distributed under"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tif not (0 <= ticks < 1 << 86400000000000):\n\t\traise ValueError(\"ticks out of bounds\")\n\t\n\tseconds = ticks + 60 * ts\n\tif tz is None:\n\t\ttz = _default_timezone\n\t\n\treturn datetime.utcfromtimestamp(seconds)\n\t\n\ndef from_datetime_tz(dt):\n\treturn datetime.utcfromtimestamp(dt).astimezone(UTC)\n\ndef to_datetime_tz(dt):\n\treturn datetime.utcfromtimestamp(dt).replace(tzinfo=_default_timezone)\n\n### Conversions ###\n\n\n#\n# Helper functions."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tif isinstance(ticks, int):\n\t\t\tticks = 1\n\t\telif isinstance(ticks, long) and (ticks > 127):\n\t\t\tticks = 0\n\t\telif isinstance(ticks, (int, float) and (ticks >= 0)):\n\t\t\tticks = ticks\n\t\t\n\t\ttry:\n\t\t\toffset = int(ticks) * (int(\"0000000000000\", 16) * 60)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Wrong ticks in ticks list\")\n\t\t\n\t\tticks = offset * (60 * 60 * 16)\n\t\t\n\t\tif tz:\n\t\t\toffset += tz\n\t\t\n\t\treturn Time(ticks * 10"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tsec = float((ticks - int(ticks)) * 1e6)\n\treturn datetime.fromtimestamp(sec, tz)\n\ndef from_time(time, tz=None):\n\t\"\"\"\n\tCreate a time from an iso8601 time string (yyyy-mm-ddThh:mm:ssZ).\n\t\n\t:param time: iso8601 time string of a time (yyyy-mm-ddThh:mm:ssZ)\n\t:type time: utime.mktime\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n#-----------------------------------------------------------------------------\n# Copyright (c) 2012 - 2017, Anaconda, Inc. All rights reserved.\n#\n# Powered by the Bokeh Development Team.\n#\n# The full license is in the file LICENSE.txt, distributed with this software.\n#-----------------------------------------------------------------------------\n''' The primary classes for data driven with Bokeh (http://bokeh.pydata.org)\n\n'''\n\n#-----------------------------------------------------------------------------\n# Boilerplate\n#-----------------------------------------------------------------------------\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nlog = logging.getLogger(__name__)\n\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tfrom.timeconv import TimeConverter\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be > 0\")\n\tnow = datetime.utcnow()\n\t\n\tif tz:\n\t\timport pytz\n\t\tlocal = pytz.timezone(tz)\n\t\tnow = pytz.utc.localize(local)\n\t\n\tif len(ticks) > 127:\n\t\traise ValueError(\"ticks must be <= 127 characters\")\n\t\t\t\n\tif now.tzinfo:\n\t\tto_use = now.tzinfo\n\telse:\n\t\tto_use = datetime.now\n\tdiff = now - to_use\n\t\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tif not ticks % 864000000:\n\t\traise ValueError, 'ticks is out of bounds'\n\n\treturn Time(ticks, tz=tz)\n\ndef from_milliseconds(secs, tz=None):\n\t\"\"\"\n\tCreate a time from milliseconds.\n\t\n\t:param secs: milliseconds since midnight.\n\t:type secs: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if secs is out of bounds\n\t    (0 <= secs < 100000000000)\n\t\"\"\"\n\t\n\t\n\tif"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn ( ticks - 2 * tz.utcoffset(ticks)) / 15\n\texcept AttributeError:\n\t\traise ValueError(\"ticks is out of bounds\")\n\t\n\treturn ( ticks - 2 * tz.utcoffset(ticks)) / 15\n\ndef to_secs(ts):\n\treturn ( ts + float(86400 * ts.microsecond / 1000.0)) / 10\n\ndef from_seconds(ts, tz=None):\n\treturn float( to_secs(ts) ) / 60 + float(ts % 1000 ) / 3600\n\ndef gmtime(ts):\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\t# TODO: This does not work, but in this case it does\n\t# not work. I don't know why not...\n\tticks = long(ticks + tz.utcoffset(ticks))\n\tif not (0 <= ticks < 86400000000000):\n\t\traise ValueError(\"ticks must be an integer > 0 and not None\")\n\t\n\treturn int(ticks * 1e6)\n\n\ndef to_ticks(t1, t2):\n\t\"\"\"\n\tCreate a timestamp from ticks (nanoseconds since midnight).\n\t\n\t:param t1:  nanoseconds since mid"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\t\t\n\thandler_dict = {}\n\tfor version in protocol_version:\n\t\ttry:\n\t\t\thandler_dict[version] = cls.get_handler(protocol_version)\n\t\texcept ValueError, e:\n\t\t\t# Version number invalid: raise exception\n\t\t\thandler_dict[version] = version\n\t\t\t\n\treturn handler_dict\n\t\n\t\ndef __create_handler_class(protocol_version, handler_cls_dict, protocol_handler_dict, prefix):\n\t\"\"\"\n\tReturn a handler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t    \n\tif protocol_version is None:\n\t\tprotocol_version = (2, 6, )\n\tif protocol_version not in protocol_handlers:\n\t\traise TypeError(\"not a supported protocol version (%d)\" % \n\t\t\tprotocol_version)\n\treturn protocol_handlers[protocol_version]\n\"\"\"\nProvides a high level client using HTTP (with support for persistent connections)\nBased on http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/473091\nOriginal author: Dave Backus, 2008.\n\"\"\"\n\nimport socket, BaseHTTPServer"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t    \n\tfor version in protocol_versions:\n\t\tif version == 3 and protocol_version is None:\n\t\t\t# legacy protocols lack protocol_version\n\t\t\t# and the protocol_version does not point to the version 2\n\t\t\ttry: version = protocol_version[0]\t\t\n\t\t\texcept IndexError: pass\n\t\tif not isinstance(version, int): raise TypeError(\"invalid protocol version\")\n\t\tversions = {\n\t\t\t\"3\": (0, 3), \n\t\t\t\"5\": (0, 5), \n\t\t}\n\t\tif not all(map(lambda x: x in versions, protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tdict = dict((v.strip(), True) for v in protocol_version.split(\".\"))\n\ttry:\n\t\tdict[protocol_version] = cls(protocol_version=protocol_version)\n\texcept (ValueError, TypeError):\n\t\traise protocol_error.ProtocolError(\"invalid protocol version (%s)\" %\n\t\t\t  type(protocol_version))\n\t\n\tif protocol_version is not None:\n\t\tdict[\"version\"] = protocol_version\n\t\n\treturn dict\n\n# vim:ts=4:sw=4:et:\n# This file is part of Copernic"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is not None:\n\t    return dict((v, k) for (k, v) in protocol_version)\n\telse:\n\t    return None\n\n\nclass ConnectionState(object):\n    \"\"\"\n    The current state of the connection, represented as a map from\n    State.IOException to L{error.ConnectionClosed}. This allows us to avoid errors\n    due to concurrent access problems.\n    \n    :ivar connection: The underlying connection\n    :ivar state: The current state of the connection\n    :ivar connected: A boolean indicating whether the connection has been\n        established\n    \"\"\"\n    \n    # TODO: maybe we can"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\thandlers = {}\n\tdef add(handler_class):\n\t\t\"\"\"\n\t\tAdd a new handler class to this BoltProtocolHandler\n\t\tobject. This method may be called multiple times\n\t\tby calling protocol_version()'s add() method and\n\t\tcall that protocol_version in a new handler class.\n\t\t\"\"\"\n\t\tif handler_class in handlers:\n\t\t\traise Exception(\"Duplicate handler class found\")\n\t\thandlers[handler_class] = handler_class\n\t\thandlers[protocol_version or None] = handler_class\n\t\treturn handlers[protocol_version or None]\n\t\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t \n\tmsg = {}\n\tif isinstance(protocol_version, tuple):\n\t\tprotocol, minor, release = protocol_version\n\telse:\n\t\tservice = protocol_services[protocol_version]\n\t\tversion, release = service.getVersion()\n\t\tprotocol = protocol_map[version]\n\t\trelease = version_map[release]\n\n\tfor k in protocol_versions:\n\t\tversionless_services = version_map.get(k, {None:versionless})\n\t\tversions = []\n\t\tfor ver in protocol_versions[k]:\n\t\t\tif not isinstance(ver,"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\t# only support those protocol versions for now\n\tversion_tuples = {\n\t\t(2, 0): (6, 9),\n\t\t(3, 1): (6, 11),\n\t\t(3, 2): (5, 4),\n\t\t(3, 3): (5, 0),\n\t\t(3, 4): (4, 0),\n\t\t(3, 5): (5, 1),\n\t\t(3, 6): (5, 2),\n\t\t(3, 7): (5, 3),\n\t\t(3, 8): (5, 4),\n\t\t("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocols = {}\n\t\t\n\tfor protocol_tuple, version in protocol_handler_map.items():\n\t\thandler_dict = dict(\n\t\t\t(protocol, {\n\t\t\t\t\"version\": version,\n\t\t\t\t\"protocol_version\": protocol_tuple[0],\n\t\t\t\t\"service_name\": protocol_tuple[1],\n\t\t\t\t\"cluster_id\": protocol_tuple[2],\n\t\t\t\t\"host\": protocol_tuple[3],\n\t\t\t})\n\t\t)\n\t\tif protocol_version not in protocols:\n\t\t\thandlers = {}\n\t\t\tversions = []"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\n\tif protocol_version is None:\n\t\tprotocol_version = VERSION_BOLT_ALL\n\t\n\thandlers = {}\n\tfor entry_point in version_tuple_by_protocol_version(protocol_version):\n\t\t# handle backward compat\n\t\tif entry_point == 3:\n\t\t\tif protocol_version >= V1:\n\t\t\t\tprotocols = {}\n\t\t\t\t\n\t\t\t\thandlers = [protocols[entry_point]]\n\t\t\telse:\n\t\t\t\thandlers = protocols[entry_point]\n\t\t\tentries = dict(entry_point)\n\t\t\t\n\t\t\thandlers.update(entries)\n\t\telse:\n\t\t\t"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\n\t# check for raw values\n\tif not values or len(values) < 3:\n\t\treturn\n\t\n\tdecoded = {}\n\tfor value in values:\n\t\tid_, timestamp = value.split(\",\")\n\t\tdecoded[id_] = timestamp.split(\"/\")\n\t\t\n\tbookmarks = GfxBookmark.bookmarks_from_raw_values(decoded)[:2]\n\tg = get_bookmark(bookmarks, True)\n\t\n\tfor id_, timestamp in g.iteritems():\n\t\tassert timestamp!= \"None\"\n\t\tassert not timestamp.startswith(\"GfxBookmark\")\n\t\n\tfor timestamp in g."}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\td = eval(values[0])\n\t\tif not isinstance(d, (list, tuple)): return\n\t\tb = Bookmark(d[1], values[1])\n\t\tif not b.valid: raise ValueError()\n\t\ta = Bookmark(None, values[2])\n\t\tif not a.valid: raise ValueError()\n\t\treturn b\n\texcept:\n\t\traise ValueError()\n\n# Copyright (c) 2012 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tbookmarks = []\n\t# Start with bookmark\n\ti = 1\n\twhile i < len(values):\n\t\tbookmark_str = values[i]\n\t\tbookmark = Bookmark(bookmark_str)\n\t\tbookmarks.append(bookmark)\n\t\ti += 1\n\t\n\t# Then close the bookmarks list\n\tdel bookmarks\n\t\n\treturn bookmarks\n\n\n\n# ------------------------------------------------------------------------------\n# Bookmarks class\n# ------------------------------------------------------------------------------\nclass Bookmark(object):\n\t\"\"\"\n\tHandles writing the bookmarks of a bookmark file in a JSON format\n\tthat can be read by other code, for instance to allow it"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport time\n\nfrom bisect import bisect\n\nfrom collections import defaultdict\nfrom datetime import timedelta\nfrom unittest import skipIf\nimport json\nfrom bson import ObjectId\nimport pymongo\nfrom pymongo.read_preferences import ReadPreference\n\nfrom six import PY3, b\n\nif PY3:\n    string_type = str\nelse:\n    string_type = basestring\n\n\nclass InsertError(Exception):\n    \"\"\"\n    Exception thrown when attempting to insert item into a collection\n    \"\"\"\n\n\ndef check_array"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\t\n\t\n\n\"\"\"\n    This file is part of Timeline.\n\n    Timeline is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    Timeline is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\n\tvalues = values.replace('\\\\n', '\\n')\n\ttags = values.split('\\\\n\\n')\n\tfor tag in tags:\n\t\tif tag.startswith('\\\\'):\n\t\t\tcontinue\n\t\ttag = tag.strip()\n\t\tvalue = dict(zip(tags, key.split(':', 1)))\n\t\tvalue[tag] = value.get(tag, default_value)\n\t\tnew_bookmark = cls()\n\t\tnew_bookmark = getattr(cls, tag)(value)\n\t\tnew_bookmark._raw_bookmark = value\n\t\tnew_bookmark._raw_bookmark"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tfor value in values:\n\t\tif not isinstance(value, str):\n\t\t\traise ValueError('Values must be strings: \"{0}\"'.format(value))\n\t\t\n\t\tbookmark, data = value.split(b' ', 1)\n\t\t\n\t\t# Ensure that the bookmark looks like a string value\n\t\ttry:\n\t\t\tbookmark = int(bookmarks[bookmark])\n\t\texcept (TypeError, ValueError):\n\t\t\traise ValueError('Bookmark \"{0}\" is not an int'.format(name))\n\t\t\n\t\tbookmark_name, bookmark_value = bookmark.split(b' ', 2)\n\t\t\n\t\tbookmarks[bookmark_"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tbookmarks = Bookmarks()\n\t\n\tfor value in values:\n\t\tdata = bookmarkStringFromValue(value)\n\t\tbookmarks[data.id] = data\n\t\n\treturn bookmarks\n\t\nclass BookmarkFormat(enum.IntEnum):\n\tBookmarks = 1\n\tBookmarkFormatExtended = 2\n\tBookmarkFormatBookmark = 3\n\t\n\tdef __init__(self, name, fmt):\n\t\tenum.IntEnum.__init__(self, name, 0, fmt)\n\nclass BookmarkFormatExtended(enum.IntEnum):\n\tBookmarkFormatExtendedBasic = 1\n\tBook"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tbookmark_count = len(values)\n\tret = cls()\n\t\n\tfor bookmarkstr in values:\n\t\tret.add_bookmark(bookmarkstr)\n\t\n\treturn ret\n\t\n\t# These methods handle the conversion\n\t@classmethod\n\tdef convert(cls, bookmark):\n\t\t\"\"\"\n\t\tReturn the Bookmark object of the corresponding format,\n\t\treturning any Bookmark() object it represents based on the\n\t\tstring representation passed in it.\n\t\t\n\t\t:param bookmark: An iterable or string containing the string\n\t\t\trepresenting an individual bookmark.\n\t\t:type bookmark: str or iter\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tmarker_dict = defaultdict(list)\n\tfor value in values:\n\t\tif value.startswith('#'):\n\t\t\tcontinue\n\t\tstripped_value = value[1:]\n\t\tif stripped_value[0] in '([{' and value[-1] in ['()','{'] and stripped_value[-1] == '}])':\n\t\t\tvalue = value[:-1]\n\t\tif stripped_value.startswith('{}'):\n\t\t\tmarker_dict[stripped_value[1]].append(stripped_value[2:-1])\n\t\t\tmarker_dict[stripped_value[1]].append(stripped"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\t# We can't make it compile-time constants from tuple elements.\n\t\t# But they're not hashable.\n\t\tif not hasattr(seq, '__getitem__'):\n\t\t\tseq = (seq,)\n\t\tif len(seq) == 1:\n\t\t\treturn seq\n\t\tif max(seq):\n\t\t\treturn (seq[0],)\n\t\telse:\n\t\t\treturn tuple(seq)\n\n\tif bufsize is None:\n\t\twhile True:\n\t\t\tseq = _get_seq_with_type(seq, bufsize=0)\n\tif bufsize == 0:\n\t\traise TypeError(\"Can't"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t  \n\tif bufsize is None:\n\t\tbufsize = len(seq)   # find optimal length\n\tif len(seq) < bufsize:\n\t\traise ValueError, \"input sequence too short to contain bufsize\"\n\treturn seq[:bufsize], seq[bufsize:]\n\n\ndef _get_base_type(base, n):\n\t\"\"\"\n\tGet 'base' of a non-negative integer `n`.\n\t\"\"\"\n\tsubtype = {'b':0,'B':1,'H':2,'I':3,'L':4,'Q':5}\n\tif n < 0:\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n#\treturn (int, type)\n\tif bufsize is None:\n\t\tif callable(seq):\n\t\t\treturn seq()\n\t\telse:\n\t\t\treturn seq\n\t\t\n\treturn (seq, type)\n\t\n#\treturn (int, type)\n\n#\treturn (Tuple, _get_seq_with_type)\n\ndef _next_seq(buf, bufsize):\n\tbuf_len = len (buf)\n\tif buf_len <= bufsize:\n\t\treturn buf[0:bufsize], buf[bufsize:]\n\telse:\n\t\ti = 0\n\t\tseq ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n# -- internal --\n\n\nclass SeqType(object):\n    \n    def __init__(self, seq=None, type=0 ):\n\tif seq:\n\t    # We must be conservative and accept the seq\n\t    # (or have it) as a single character\n\t    if type is None:\n\t\tassert 0 <= type <= len(seq)\n\t    else:\n\t\tassert type is None or type >= len(seq) - 1\n\t\ttype = len(seq) + 1\n\t    self._seq = seq\n\t    self._size = int(len(seq) / type)\n\n    def __len__(self):"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tif bufsize is None:\n\t\tbuf_size = 4096\n\telif bufsize < 4:\n\t\tbuf_size = int((bufsize-4)/4)\n\t\n\t\n\tfmt = \">I\"\n\twhile len(seq) > bufsize:\n\t\tx = struct.unpack_from(fmt, seq, 0)\n\t\tseq = seq[x:x+bufsize]\n\t\trec = []\n\t\tfor b in seq:\n\t\t\tif b!= 0xff: rec.append(b)\n\t\t\telse: break  # ignore terminating NUL bytes\n\t\tassert len(rec) =="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n# Use the new array module for new code.\ntry:\n\timport numpy._num_sa_constants\n\tassert all([x.typecode==numpy._num_sa_constants.typecode for x in seq])\n\tfrom.numeric import Int32\n\n\tif bufsize:\n\t\treturn [Int32(bufsize)]\n\telse:\n\t\treturn [Int32(0)]\nexcept (NameError, AttributeError):\n\tif not sys.version.startswith(\"2.2\"): \n\t\tdef get_seq_with_type(seq, obj, isinstance=isinstance):\n\t\t\tif isinstance("}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\ndef _get_seq(iterable):\n\t\"\"\"\n\tReturn an (obj-array, type) pair from (i, item).\n\tseq is a sequence type; types are structs with fields\n\ti, types is a tuple or list, each containing an (obj-array, type)\n\tentry. Otherwise, each field must be given a type or\n\telement list (e.g. list or tuple).\n\t\"\"\"\n\tif hasattr(iterable, '__iter__'):\n\t\treturn iterable\n\telse:\n\t\treturn (seq,)\n\ndef _infer_fieldlist_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\t\n\tif bufsize is None:\n\t\tdef _get_seq_with_len(orig, bufsize):\n\t\t\tseq_type = type(orig)\n\t\t\tif seq_type is TupleType: return ([seq], bufsize)\n\t\t\telif seq_type in SequenceTypes: return ([orig], bufsize)\n\t\t\telif seq_type is DictType: return ([orig], {})\n\t\t\telif seq_type in SetType: return ([orig], set())\n\t\t\telif seq_type is SetType: return ({0:orig}, set())\n\t\t\telse: return ([orig], {\"length\": bufsize})\n\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\t\n\tfor item in seq:\n                if type(item) == type:\n                        (item, type) = item, type\n                        break\n        return item\n\n\ndef _append_data(data, size, type = None):\n        \"\"\"Append some data to the given input.\n\t    Return the number of bytes written to *data*\n\t\"\"\"\n        # If we have nothing to append, just return\n        if len(data)==0:\n               return size\n\n        # Get data from buffer\n        datalen = len(data)\n        \n        if size == 0 or (size==1 and type==8):\n            return datalen*size"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tnew_bufsize = len(seq)\n\t\telse:\n\t\t\tnew_bufsize = len(seq) * bufsize\n\t\treturn (seq[i: i + new_bufsize] for i in range(0, len(seq), new_bufsize))\n\t\t# the seq is now a tuple of arrays\n\t\treturn zip(seq[::2], seq[1::2])\n\telif len(seq) == 1:\n\t\treturn seq[0]\n\telse:\n\t\treturn seq\n\nclass Record(object):\n\t\"\"\"\n\tClass for writing binary"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tif other == None:\n\t\tother, __ = self, other\n\t\n\tmax_val = max(other)+1\n\t\n\tif self.dtype == float: return float(self)\n\telif self.dtype in [int,float]:\n\t\tif __debug__:\n\t\t\t_z3_assert(is_numeric(other))\n\t\t\tassert(max_val <= 3)\n\t\t\t\n\t\t\tscale = max_val**(-1)\n\t\t\trescaled = numpy.empty(3, dtype=self.dtype)\n\t\t\tif __debug__:\n\t\t\t\t_z3_assert(scale.all"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif not isinstance(other, _HistogramBase):\n\t\t\traise TypeError(\"Expected instance of histogram.\")\n\t\t\n\t\tif other.get_scale_mode()!= self.get_scale_mode():\n\t\t\tif self.x >= other.x and self.y >= other.y:\n\t\t\t\t\ttmp = Scale()\n\t\t\t\t\ttmp.x = int(round(self.x))\n\t\t\t\t\ttmp.y = int(round(self.y))\n\t\t\t\t\treturn tmp\n\t\t\t\traise SizeMismatchError(self.get_size());\n\t\t\n\t\t\n\t\tif not other.is_empty() and not other.is_"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif self==other:\n\t\t\treturn\n\t\ttry:\n\t\t\tself.__nobj__ *= other\n\t\texcept:\n\t\t\traise \n\t\telse:\n\t\t\treturn\n\t\n\tdef copy(self):\n\t\t\"\"\"\n\t\tCopy the existing histogram from a new one, just\n\t\treconstitute its reference. Use :meth`__iadd__` for\n\t\toverwriting an histogram.\n\t\t\"\"\"\n\t\treturn self.copy()\n\t\n\tdef __iadd__(self, other):\n\t\tif not (isinstance(other, self.__class__)\n\t\tand self.scales==other.scales) or \\"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn\n\t\tif not (hasattr(other,'recompute') and \n\t\t\t\tother.recompute):\n\t\t\traise ValueError(\n\t\t\t\t\"scale() requires a recompute=True attribute\")\n\t\t\n\t\tif any(other == getattr(self, 'value', 0) for \n\t\t    other in self.buckets):\n\t\t\tsuperset = self.__class__() # avoid AttributeError\n\n\t\t# check whether the recompute histogram\n\t\tif other.value == 0:\n\t\t\tsuperset = superset.copy()\n\t\telif other.isConstant(ignore=['values']):\n\t\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n#\tc = self.copy()\n#\tc.size = self.size\n#\tc.scale(other)\n#\tself.update_hist(c)\n#\n#\tif recompute:\n#\t\tself.recompute()\n#\t\t#assert isinstance(self.get(0), HistogramSize)\n#\t\tself.recompute_size()\n\t\n\tif other is not None:\n\t\tif self.size!= other.size:\n\t\t\traise(TypeError, \"recompute histogram sizes of \"\n\t\t                     \"%s and %s do not match"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tf = self.copy()\n\tif other is not None: raise ValueError('Cannot multiply a histogram with another one!')\n\tf[:] = self\n\tif recompute: f._rescale()\n\tf._rescale()\n\treturn f\n\ndef dot(self, other=None):\n\t\"\"\"\n\tCompute or multiply the values of multiple *vectors*.\n\tConsecutive calls to dot on the same axis will return the result,\n\twith the same shape of the result.\n\t\n\t:exc:`.AlignValueError` will be raised if one tries to do that.\n\t\"\"\""}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n#\tdef _scale(x, other=1, recompute=False):\n\t\traise NotImplementedError()\n\t\n\t\n\tdef _min_y(self, bins=10):\n\t\traise NotImplementedError()\n\tmin_y = property(lambda self: _min_y(self), doc=\"minimum y limit\")\n\t\n\tdef _max_y(self, bins=10):\n\t\traise NotImplementedError()\n\t\n\tdef _avg_y(self):\n\t\traise NotImplementedError()\n\t\n\tdef _std_y(self):\n\t\traise NotImplementedError()\n\t\n\tdef __call__(self, *args, **"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n### Roboto/Algorimogenize\n# http://www.scriptoinfo.org/documentation/html/type-color-templates/\n\n#### Base classes and docstrings\nclass SizeBase(object):\n\t\"\"\"\n\tBase class for a fixed-size histogram.\n\t\n\tIf *recompute* is ``False``, then each histogram is recomputed.\n\tFor example, if length is ``32``, then the ``ValueError`` exception is raised.\n\t\"\"\"\n\t\n\t@abstractmethod\n\tdef __init__(self, size, length):\n\t\t\"\"\"\n\t\tCreate a new histogram"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n#\tdef empty(self, shape=None, dtype=float):\n#\t\traise ValueError(\"Cannot remove empty histrogram\")\n\t\n\t#\t\n\t#\tdef fill(self, x):\n\t#\t\t\"\"\"\n\t#\t\tReturns an array *x* with histrogram *y* of zeros, where\n\t#\t\t *y* is either *zeros* or *zeros*. Only bins in the\n\t#\t\tfilled region are retained.\n\t#\t\t\"\"\"\n#\t\treturn np.zeros(shape=x.shape, dtype=dtype"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n#\tN.B. If we didn't provide an out of _data (e.g., data not yet\n#  stored), it will not need to calculate.\n#\tif hasattr(other,'_data'):\n#\t\tself._data *= other._data\n#\t\tself._nbins = 1\n#\t\treturn self\n#\telif self.nbins!= other.nbins:\n#\t\tself._nbins *= other.nbins\n#\t\tself._data *= other\n#\t\tself._nbins = self.nbins\n#\t\t"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tdef get(self):\n\t\t\"\"\"Return the scale of the graph.\"\"\"\n\t\treturn 1\n\n\n## Base class for LiftyString and LiftyUnicode\n## subclasses.\n\nclass LiftyString(LiftyString):\n\t\"\"\"Base class for LiftyString and LiftyUnicode.\"\"\"\n\tdef __init__(self, description, is_ref=0, attributes=None):\n\t\t\"\"\"Create from LiftyString (and optional attributes).\n\t\t\"\"\"\n\t\tLiftyString.__init__(self, \"%s %s\" % (description, Lif"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\nclass Vertex(object):\n\t__slots__ = ['x', '_y', '_z']\n\tdef __init__(self, x, y, z=None, name=''):\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.z = z\n\t\tself.name = name\t\t\n\n\nclass Edge(object):\n\t__slots__ = ['v', 'e']\n\tdef __init__(self, v, e):\n\t\tself.v = v\n\t\tself.e = e\n\tdef __repr__(self):\n\t\ts = \"v"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif other is not None:\n\t\tif not isinstance(other, (list, tuple)):\n\t\t\traise TypeError(\"%s is not type %s\", str(other), str(type(other)))\n\t\tfor other_ in other:\n\t\t\ttry:\n\t\t\t\tfloat(other_)\n\t\t\texcept ValueError:\n\t\t\t\traise TypeError(\n\t\t\t\t\t\"other and other must be numeric scalar or 2-length iterable. Found: %s\",\n                    other_\n\t\t\t\t)\n\t\n\t\n\tself._check_node()\n\t\n\t\n\tif not (self._get_type() & Graph.Type.GRAPH or\n\t\t\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n        # The Graph *must* have the same dimensions.\n        if self.coords!= other.coords:\n            raise ValueError, \"Graphs must have different dimensions\"\n\n        # We'll use an unimplemented property from the base class\n        raise NotImplementedError(\"Abstracts must implement the scale method.\")\n\n\n    # Some common graph constants.\n    @property\n    def center(self):\n        \"\"\"\n        The :class:`pygraph.drawing.marker.CenterMarker` of this\n        graph.\n        \n       .. note::\n          This property is the vertex marker used to set the\n          center of its graph.\n        \"\"\"\n        return self._marker.v\n\n   "}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t if other is None:\n\t\t\treturn self\n\t\t\n\t\tmin,max = self.bounds\n\t\n\t\t# Check input\n\t\tif (other is None):\n\t\t\t# TODO raise other?\n\t\t\tself.__clear_bounds()\n\t\t\treturn (None, None, None, None)\n\t\t\t#raise ValueError, \"You have input outside of the graph bounds\"%self\n\t\t\n\t\t# Extract\n\t\tself._x += other.x\n\t\tself._y += other.y\n\t\tself._z += other.z\n\t\n\t\t# Get min and max\n\t\tmin = min + (max-min) / 2.0"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\n\told = self.copy()\n\tnew = new.copy()\n\tif not _is_valid_data(new):\n\t\traise TypeError('Only the first element can be an instance of GraphData class or a 2-dimensional numpy array')\n\tnew._scale_scalar(other)\n\treturn new\n\t\n\n\n##################################################################\n# Vector Graph\n#\n##################################################################\n\n# vim: sw=4:ts=4:sts=4:et\nimport os\nimport re\nimport itertools\nimport subprocess\nfrom.utils import *\nfrom.. import jmx\nfrom."}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t    \n\tif other is None: return self\n    \n\tif not self._have_node(_id=other.getId()): return other\n\tx = self._find_node(_id=other.getId())\n\tif x == None: \n\t\traise ValueError, 'cannot find node %s!' % other\n\ty = self._find_node(_id=other.getId())\n\tif y == None: \n\t\traise ValueError, 'cannot find node %s!' % other\n\tz = self._find_node(_id=other.getId())\n\tif z =="}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other :\n\t\t\tself.info(\"recalculating scales from %g -> %g\", other.center())\n\t\t\tself.info(\"...and setting values [%s %s %s]\",\n\t\t\t\t\tother.info[\"width\"], other.info[\"height\"], other.info[\"depth\"])\n\t\t\td = self.convert()\n\t\t\tif d == 1 :\n\t\t\t\tself.info(\"...done\")\n\t\t\telse :\n\t\t\t\traise ValueError(\"not a valid 2D map\")\n\t\t\treturn self\n\t\telse :\n\t\t\tself.info(\"...OK\")\n\t\t\treturn self\n\n\tdef copy("}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tdef nodes(self, size=None):\n\t\n\t\"\"\"\n\tReturns a list of nodes (as a :class:`Nod` object) ordered as: \n\n\t\t.. \n\n\t\t[ (coord1, coord2, z) ],\n\t\t...\n\t\t\n\t\"\"\"\n\t\n\tif size is None:\n\t\tsize = self.msize()\n\t\n\t# Create or retrieve nodes\n\tnew_nodes = []\n\tfor i, coord1 in enumerate(self.dcoords()):\n\t\tnew_nodes.append( Nod( coord1.nutr / size, coord1.shape"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\tother = {\n\t\t\t\tcoord : None\n\t\t\t\tfor coord in self.shape\n\t\t\t}[other]\n\t\t\n\t\tif isinstance(other, tuple):\n\t\t\t# It's already a tuple of integers\n\t\t\tfor i in range(other[3]):\n\t\t\t\ttry:\n\t\t\t\t\tother[i] = int(other[i])\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\n\t\telif other is None:\n\t\t\t# TODO\n\t\t\treturn\n\t\t\n\t\tself._scale(other)\n\t\n\tdef __getitem__(self, coord):\n\t\treturn self._scale(coord)\n\t\n\tdef __"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\thist = hist.copy() # we don't want to modify the original, otherwise it's just a shallow copy.\n\t\n\t\n\tgraph_x = make_value(hist[0])\n\tgraph_y = make_value(hist[1])\n\t\n\tgraph = pygraphviz.AGraph()\n\tgraph.bind('hist', hist_to_graph)\n\tgraph.set_name('hist')\n\tgraph_x.value_type.should_be('value')\n\t\n\tfor field_name in field_names:\n\t\tfield_value = get_coordinate(hist"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\t# get the value to get a histogram bin\n\tif not (hist is None):\n\t\treturn histogram.hist_bin(hist)\n\t\n\t# get the point to calculate the bounding box\n\tif points is not None:\n\t\tcoords = get_coordinate(points, col=False)\n\t\tbox = None if boxes is None else np.array(\n\t\t\t[(a, b) for a in coords]\n\t\t)\n\t\t\n\t\tif make_value is None:\n\t\t\tmake_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\t\n\t\t# Get the actual"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\t#TODO: Implement this for more options, like the histogram bin\n\t\t#is already created?\n\t\tif hist is not None:\n\t\t\thist.name = make_value(hist.name, default=make_value(hist, 0))\n\t\t\thist.name_ref = make_value(hist.name_ref, default=make_value(hist, 1))\n\t\t\thist.title_ref = make_value(hist.title_ref, default=make_value(hist, 2))\n\t\treturn hist\n# -*- coding: utf-8 -*-\n# Author: P"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\t# We can't do histogram here, since it *cannot* do\n\t# anything here, since we are adding it\n    # separately.\n\tif hist is None:\n\t\treturn None\n\t# Make sure we don't pass a non-numeric argument\n\tnumeric = make_value(hist, make_value=make_value)\n\t# Create the histogram\n\t# We'll use the histogram as the graph\n\t# and fill it with the value we got.\n\thist = hist.astype(numeric)\n\t# This is useful for plotting\n\tif get_coordinate is"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\n\tbins = []\n\tif isinstance(hist, int):\n\t\tif hist < 0:\n\t\t\traise ValueError(\"cannot have negative value\")\n\t\thisto = histogram(hist, make_value=make_value)\n\telse:\n\t\tif hist in hist_to_graph(hist, make_value=make_value):\n\t\t\traise ValueError(\"histogram already exists\")\n\n\t\tbins = hist_to_bins(hist)\n\t\tx = float(bins._get_value(get_coordinate))\n\t\ty = float(bins._get_value(get_coordinate))\n\n\t\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tbins = hist.bins.values()\n\tif make_value is None:\n\t\tmake_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\telif isinstance(make_value, int) or make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\n\tvalues = []\n\tfor i, (bin, hist) in enumerate(zip(bins, values)):\n\t\thist.bins = bins\n\t\thist.bins_data = hist.value_matrix()\n\t\thist.make_value = make_value(hist)\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\"\"\"Tests for aiohttp/protocol.py\"\"\"\nimport asyncio\nimport json\nimport zlib\n\nimport pytest\n\nfrom aiohttp import BaseProtocol\nfrom aiohttp.protocol import HttpRead, RawRequestMessage\nfrom aiohttp.helpers import _create_future\n\n\nasync def test_invalid_message(create_server, loop):\n    data = b'GET /path HTTP/1.1\\r\\nHost: example.com\\r\\n'\n\n    class CustomException(Exception):\n        pass\n\n    async def handler(request):\n        data = request.body.decode('utf8')\n        raise CustomException()"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tdata = hist.data.copy()\n\tdata = make_value(data, get_coordinate, field_names, scale = scale)\n\t\n\tlabels = []\n\tcolors = {}\n\tfor i, field in enumerate(hist.__data__[get_coordinate]):\n\t\tlabel = make_value(field, get_coordinate, field_names, scale = scale);\n\t\tif callable(label):\n\t\t\tlabel = label()\n\t\tcolors[i] = (label, color)\n\t\tlabels.append(label)\n\t\n\tif make_value:\n\t\treturn hist(data)"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tgraph_points = hist.xhistogram_bins(\n\t\tmin_val=0,\n\t\tmax_val=max(hist.xhistogram(make_value=make_value).max(), 0)\n\t)\n\t_plot_errorbars(hist, graph_points, x_min=0, x_max=max(hist.xhistogram(make_value=make_value).max(), 0))\n\t_plot_bin_centers(hist, graph_points, x_min=0, x_max= max(hist.xhistogram(make_value="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\thists_and_nodes = cbook.silent_list(\n\t\t\"find_hists\",\n\t\t\"\"\"\n\t\tFind named and non-empty histograms.\n\t\t\n\t\t@param nbins: A list or tuple containing the nbins to search. \n\t\tEach value gives the number of bins used.\n\t\t@returns: An iterator yielding (hists, nodes) pairs.\n\t\t\"\"\"\n\t)\n\t\n\tdef histogram_helper(hist):\n\t\t# If not bins are specified, select them from the list.\n\t\tbins = hist.bins(make_value)\n\t\t#"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif candidate.interface is not None:\n\t\tif not iface in candidate.interface.providedBy:\n\t\t\tif typ: return False\n\t\t\telse:\treturn\n\t\tif not tentative:\n\t\t\tfor t in tentative:\n\t\t\t\tif not t in candidate.interface.providedBy:\n\t\t\t\t\tif typ: return False\n\t\t\t\t\telse:\treturn\n\t\tif not typ:\n\t\t\tfor t in candidate.providedBy:\n\t\t\t\tif not t in iface.providedBy:\n\t\t\t\t\tif typ: return False\n\t\t\t\t\telse:\treturn\n\t\treturn False\n\n\tif tent"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    from zope.interface.verify import verifyObject\n    from Products.Archetypes.public import *\n    from Products.Archetypes.utils import t\n    from Products.Archetypes.PropertyManager import PropertyManager\n    \n    if component is None:\n        component = getUtility(IPublishingProvidedBy)\n    \n    # verify object\n    obj = None\n    \n    # perform discovery\n    try:\n        interface.providedBy(candidate)\n    except ValueError:\n        try:\n            iface.providedBy(candidate)\n        except errors.ComponentLookupError:\n            pass\n        else:\n            raise ValueError(\"Component not provided.\")\n    \n    validated = verifyObject("}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\t# Only proceed when we've resolved the interface\n\tif interface is not None:\n\t\n\t\n\t# Check that we're being called\n\tif iface is not None:\n\t\t\n\t\t# This *must* be a class method of a class derived from something\n\t\t# derived from the interface it provides - hence remember that\n\t\t# we're not trying to call the constructor directly - this code\n\t\t# should just be \"non-class\" - the interface itself\n\t\tif not hasattr(iface, '__zope_exposed__') \\\n\t\tor iface.__zope_exposed__ is zope.interface."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    # In order to solve a problem is the following:\n    #\n    # 1) We check interface (and not its subclasses)\n    # 2) We check candidates (but don't raise exceptions) and\n    #    (potentially) upgrade z3.\n    # 3) We check if the interface provides interfaces\n    # 4) We check _and_ return interfaces\n\n    # First, if the method we're testing implements the interface, and we're\n    #    implementing the correct signature, call the correct verification.\n    # \n    # In case we're not implementing a method we check for iface.providedBy\n    #\n    \n    #"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t    \n\t\n\tattr = getattr(iface, member)\n\t\n\tif not attr:\n\t\tiface.requiredMethods.append(member)\n\telif not callable(attr):\n\t\tif not member in iface.provided.iterkeys():\n\t\t\t# If the error was reported last, then assume it's a\n\t\t\t# missing required method in order to perform the test.\n\t\t\tattr = attr.requiredMethods.pop()\n\t\telse:\n\t\t\traise Invalid(member, iface,\n\t\t\t\t\t\t('missing method %s' % member)\\\n\t\t\t\t\t\t% attr)\n\telse:\n\t\tiface.providedMethods.append"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ndef get_iface(*interfaces):\n\t\"\"\"\n\tGet an interface object from a sequence of interfaces.\n\t\n\t:param interfaces: Interface descriptors to inspect. The interface names\n\t\t    must all be in the form 'klass.name' and the interface objects\n\t\t  always start with a \"iface\" prefix.\n\t\n\t:return: An interface object.\n\t\n\tThe :func:`resolve` function will return the appropriate interface object or None.\n\t\"\"\"\n\tif not interfaces:\n\t\traise TypeError(\"descriptor {0!r} has no interfaces.\".format(descriptor))\n\ttry:\n\t\tmodule = _imp"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn iface._verify(candidate)\n\texcept Missing as error:\n\t\targs = (\n\t\t\tiface,) + value.split('.') + (candidate,)\n\t\tiface.providedBy(iface, *args)\n\t\n\t\tif not isinstance(error, BadArgumentError):\n\t\t\texc = BadArgumentError(\"Missing required argument(s) %s\" % (error,))\n\t\t\tiface.parseCommand(args, iface.commands, error)\n\t\telse:\n\t\t\traise\n\t\n\t\n\tif hasattr(iface, 'providedBy'):\n\t\tiface.providedBy(candidate)\n\n\tif"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tiface_spec = iface.providedBy(candidate)\n\texcept RuntimeError:\n\t\t# We need to go to the next entry.\n\t\traise Invalid(\"%s matches more than one implemented by %s\" % (\n\t\t             candidate, iface,\n\t\t))\n\t\n\t# For the sake of completeness, report any problems\n\treturn iface_spec.providedBy(candidate) or \\\n\t       iface_spec.requiresMultiple(distinguishing(ie.IListening))\n\t# For the sake of presentability, report any problems\n\t# with more informative"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\n\tif tentative and ties.is_dictionary(candidate):\n\t\tif not types.callable(candidate):\n\t\t\tfor iface, candidate2 in candidate.interfaces():\n\t\t\t\t# Check that the interface provides the interface.\n\t\t\t\tm = iface.meta.interfaces[iface.uri].methods.get((iface.uri, iface.name))\n\t\t\t\t# Check that the interface provides the interface.\n\t\t\t\tif m:\n\t\t\t\t\tif not iface.providedBy(m):\n\t\t\t\t\t\tfor e in find(m, iface.operations, iface.attributes, []):\n\t\t\t\t\t\t\t_verify(e, candidate"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tinterface_provided, interface_provided_class = \\\n\t\t  _guess(iface, available_impls=iface.impls, is_a=(candidate and interface and candidate.providedBy(iface.providedBy(interface))))\n\t\n\tif interface_provided is not None:\n\t\tif any((candidate and iface.providedBy(candidate))):\n\t\t\t# The candidate class is present in the interface and it \n\t\t\t# provides it. Check it explicitly.\n\t\t\tobj_iface = interface_provided_class\n\t\t\tif _verify_class(obj_iface) == interface_provided:\n\t\t\t\t# There"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif not candidate.providedBy(objspec):\n\t\t# The candidate's class hasn't been provided yet.  Return\n\t\t# False here.\n\t\trv = False\n\telse:\n\t\ttry:\n\t\t\trv = candidate.providedBy(objspec)\n\t\texcept:\n\t\t\t# There was nothing more to verify.\n\t\t\trv = False\n\n\tif not kwattrs:\n\t\t# Method does not define any arguments.\n\t\tif not interface.providedBy(objspec):\n\t\t\t# This check is for consistency with the interface's\n\t\t\t# present() methods and/or methods"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    def assertIn(self, tentative, alternative, match_global=False):\n\t\"\"\"\n\tRaise L{LookupError} of the wrong type of interface.\n\t\n\tThis call to raise L{LookupError} of the wrong type of interface.\n\t\n\tThe interface *s* will try to locate the object specified by *tentative*,\n\twhich is part of the object tree and which the method *alternative*\n\tthose that exists, will try to use that object. If *match_global* is\n\t``True``, *match_global* will attempt to search for the matching object,"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\n\tif not candidate.providedBy(iface):\n\t\treturn False\n\tif not iface.providedBy(candidate):\n\t\t# Never report errors, since the interface they're using accept()\n\t\treturn False\n\t\n\t# Otherwise, check the type of the iface\n\tif not candidate.type():\n\t\t# Never report errors, since the interface they're using accept()\n\t\treturn False\n\t\n\t# Check the interface\n\tifaceMethods = zope.interface.implementedBy(iface, tentative)\n\tfor ifaceMethod in ifaceMethods:\n\t\ttry:\n\t\t\tifaceMethod.verify"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    try: \n        iface_iface = iface.providedBy(candidate)\n\n        # Class verification\n        if not issubclass(iface_iface, Interface):\n            raise Invalid(\n                \"Unrecognised interface type: {0!r}. Expected an Interface\".format(iface)\n            )\n\n    except Invalid as error:\n        errors = str(error).split(\"\\n\")\n        if len(errors) >= 1:\n            last_error = errors[0]\n            return last_error\n        raise InterfaceError(error)\n\n    except InterfaceError as error:\n        if error.__class__.__name__ == \"InterfaceError\":\n            raise Invalid(\""}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    from zope.interface import Interface\n\n    # FIXME: We should have a better API for this, so long as we provide\n    #    __implementedby__ and __targetmethods__ instead.\n    \n    try:\n        iface_iface = iface\n    except AttributeError:\n        iface_iface = Interface\n    try:\n        cand_iface = getComponent(iface_iface, providedBy=formerly)\n    except NotPlugged:\n        try:\n            cand_iface = getComponent(iface_iface, providedBy=attribute)\n        except NotPlugged, e:\n            raise Error(\"Unknown class for object '{}'\".format(object))\n       "}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t    \n\tif not tentative:\n\t\tfor iface in iface.providedBy(candidate):\n\t\t\tfor iface2 in iface.providedBy(candidate):\n\t\t\t\tfor iface3 in iface2.providedBy(candidate):\n\t\t\t\t\tif iface3 in iface3.providedBy(candidate): break\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\n\t\tfirstError = None\n\t\tfor iface in iface.providedBy(candidate):\n\t\t\tfor iface2 in iface.providedBy(candidate):\n\t\t\t\tif iface2 in iface2.providedBy(candidate):\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\t\n\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn zope.interface.verifyObject(iface, candidate, tentative=tentative)\n\texcept (KeyError, TypeError) as e:\n\t\tinterface_version = iface.__identifier__\n\t\tif not iface.__name__ == '__main__':\n\t\t\treason = sys.exc_info()[1]\n\t\t\tif reason!= \"No implementations found\" and not reason.startswith(\"ZSI\"):\n\t\t\t\traise\n\t\n\t\t\tlog.error(\n\t\t\t\t\"interface %s provides wrong functionality which will \"\n\t\t\t\t\"have been removed (see zope.interface.__name__ for \"\n\t\t\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif 'iface' in candidate.__qualname__.lower():\n\t\tcandidate = candidate()\n\t\n\t# All the interfaces are present, so all the attributes are there.\n\tfor iface in ifaceList:\n\t\tif iface.__qualname__.lower() not in listAttrs(candidate):\n\t\t\tcontinue\n\t\t\n\t\tfor attribute in ifaceAttrs(iface):\n\t\t\tif attribute in memberAttributes(candidate, attribute):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif isinstance(attribute, bool):\n\t\t\t\tif not tentative:\n\t\t\t\t\tmethod = True \n\t\t\t\telse:\n\t\t\t\t\tmethod = False\n\t\t\t\tcheckmethod = hasmethod("}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\t# If we have been called with *iface*, it's probably a z3 declaration,\n\t# so we should mark it as such. If we've been given a concreteized class of the expected type\n\t# then we can mark it with a z3.\n\t\n\tif iface.implementedBy(candidate):\n\t\treturn True\n\t\n\tif iface.operation.__kind__ =='method':\n\t\ttry:\n\t\t\tifaceObject = iface.factory.objectFor(candidate)\n\t\t\tif ifaceObject.providedBy(candidate, iface):\n\t\t\t\t# A candidate class and its __providedBy__ method"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n    if __debug__:\n        # We are going to try and identify the candidate. If we can't\n        # get the interfaces from the candidate then we try\n        # to use getInterfaces() to find a suitable method (if we didn't find\n        # one) - if all is well or we can find one with that method\n        candidate = candidate.interfaceClass\n\n    # This is the best way to check for obviously invalid interfaces\n    if not candidate:\n        # The first time the search has failed, it will tell us about\n        # the name that should be given.\n        # So we need to check the validity of the"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\ndef verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* is suitable for calling\n\t:meth:`__getClass`, :meth:`addClass`, :meth:`addObject` or :meth:`contains`\n\t\"\"\"\n\ndef verifySub(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ndef getSub(iface, candidate, tentative=False):\n\t\"\"\"\n\tGet the candidate's *candidate* if *candidate* is"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t return iface not in candidate\n\n\nclass ClassChecker(object):\n\t\"\"\"\n\tClass checker used to validate provided class objects.\n\t\"\"\"\n\n\tdef __init__(self, iface):\n\t\tself.iface = iface\n\n\tdef __call__(self, iface, *arg):\n\t\tif self.iface!= iface:\n\t\t\traise SaklientSlipperException(\"iface '%s' does not match iface '%s'\" % (iface, self.iface))\n\n###\n### Interface checking\n###\n\n\n# See if an interface is supported (a/if/interface has the attribute\n#"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n#    print('Verify: verifyClass')\n#    if (hasattr(iface,'name')):\n#        iface = iface.name\n#    import pdb; pdb.set_trace()\n    \n    try:\n        iface = netifaces[iface]\n        c = getclass(iface)\n        result = c.verifyClass(tentative)\n        assert result, \"Could not locate %s\"%iface\n    except TypeError, ex:\n        pass\n\n    try:\n        cand = netifaces[c]\n        c = getclass(c)\n        result = cand.verifyClass(tentative)\n        assert not result,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tiface_candidates = get_candidate_list()\n\t\t# If we already have one of the required options\n\t\t# (and no other values):\n\t\tif not iface_candidates:\n\t\t\tif len(iface_candidates) == 0:\n\t\t\t\tif not candidate in iface_candidates:\n\t\t\t\t\traise ValueError(\"No available options for '%s'\" % candidate)\n\t\t\t\tiface_candidates = iface_candidates.keys()\n\t\t\t\traise ValueError(\"No iface candidate found: '%s'\" % candidate)\n\t\t# We can have an existing candidate.\n\t\tiface = candidate\n\t\tif iface in iface_candidates"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tiface = iface.replace('.', '').lstrip()\n\t\tif iface == '':  # No interface\n\t\t\traise ValueError, \"The requested interface must not be blank!\"\n\t\tif not candidate == '':  # If there's still some text\n\t\t\tif tentative == False:\n\t\t\t tentative = verify.all(iface, tentative)\n\t\t\telse:\n\t\t\t tentative = verify.any(iface, tentative)\n\t\tif tentative == True:\n\t\t\tcandidate = iface\n\t\t\n\treturn candidate\n\t\t\ndef verifyIf(iface, candidate, tentative=False):\n\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\tfor iface in ifacelist:\n\t\tbase = getattr (iface.meta, \"iface\", None)\n\t\td = os.path.basename (base)\n\t\tif d==\"*\":\n\t\t\ttry:\n\t\t\t\t# if it's a directory, give it to the real root\n\t\t\t\t# of the file which is in the tree of the \"pretend\"\n\t\t\t\tif os.path.basename(os.path.dirname (d))==\"..\":\n\t\t\t\t\t# we're building a symlink to a directory\n\t\t\t\t\tparent = os.path.dirname (d)\n\t\t\t\t\t# to ensure we are"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\t\n\t# Verify that the passed in-parameter is compatible.\n\treq_is_compatible = True\n\tsatisfied = False\n\tif isinstance(candidate, iface) and \\\n\t   candidate.requiresCapability(IPerspective):\n\t    if isIETF:\n\t\tif (candidate.iface.providedBy(IE_IFACE) or\\\n\t\t   candidate.iface.address == \"127.0.0.1\" or candidate.iface.address == \"::1\"):\n\t\t    return True\n\t    else:\n\t\t# A universal storm with the same interface is supported\n\t\tif getHostByIP(candidate"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\t# Try to load as a class directly; the __import__ module\n\t\t# must agree with the contents of the class.\n\t\ttry :\n\t\t\treturn objcClassInfo[candidate]\n\t\texcept KeyError :\n\t\t\tif not tentative :\n\t\t\t\ttry :\n\t\t\t\t\treturn iface()\n\t\t\t\texcept:\n\t\t\t\t\t# we are here not allowing us to get an interface...\n\t\t\t\t\tpass\n\t\t\t# else we are not allowing us to load *something*\n\t\t\t# that is not a module...\n\t\t\traise ValueError(\"No module could be imported directly from\")\n\t\n\t# Try to load as a"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n#class IDL:\n#\t\"\"\"\\\n#\tClass representing IDL interfaces.\n#\t\"\"\"\n#\tdef __init__(self, name, parents):\n#\t\tself.__dict__[\"_parents\"] = parents\n#\t\tself.__dict__[\"_interfaces\"] = {}  # {interface: [interfaces]}\n#\t\t\n#\tdef getSpec(self, interface, ancestry):\n#\t\t\"\"\"\n#\t\tGet the IDL spec from the first ancestor that\n#\t\tis compatible with the prototype on the parent.\n#\t\t@param interface: Interface Object of the"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\ndef isiface(iface):\n\tiface = iface.lower()\n\tif iface == \"lo\":\n\t\tiface = config.av.pluginpath.strip('\"').rstrip(\"'\")\n\tif iface == \"webtap\" or \"wlan0\" in interface:\n\t\treturn \"Wi-Fi is connected to the wireless network.\"\n\telif \"wlan0\" in ifaces: return \"Wireless AP\"\n\telse: return \"Modem not supported (Wireless AP)\"\n\ndef getWlanConfigName(iface):\n\t# find wlan0\n\t"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n#\tif explicit_mc is None and explicit_mc!= 0:  # No explicit declarations; just use explicit declarations\n        return explicit_mc or explicit_mc & 8\n#    elif explicit_mc is not None:                      # No explicit declarations; just use implicit declarations\n        return explicit_mc\n#    elif explicit_mc is None:                      # Must explicitly declare explicit attributes at the declaration level\n        #        explicit and explicit mc\n        # TODO: I have not done this\n        return 0\n#    else:                          # It's explicit:\n#        raise ValueError('Explicit declaration with explicit declared attributes not found"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\t# Find the base classes\n\t# e.g. find first class method in myclasses\n\tfor base in bases:\n\t\tif issubclass(base, object):\n\t\t\tclassinfo = inspect.getmembers(base, inspect.isclass)\n\t\t\ttry:\n\t\t\t\tinstancemethod = classinfo[1]\n\t\t\texcept IndexError:\n\t\t\t\tcontinue\n\n\t\t\tif explicit_mc and (instancemethod.__name__.endswith('MC') or instancemethod.__name__.endswith('MCM')):\n\t\t\t\t# If we got an implicit MC MM, this is mmmmmmmmmmm...\n\t\t\t\t# and we"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\tif not explicit_mc:\n\t\t\tbaseclasses = []\n\t\t\tfor base in bases:\n\t\t\t\tif isinstance(base, tuple):\n\t\t\t\t\tif len(base) > 1:\n\t\t\t\t\t\tif isinstance(base[1], tuple):\n\t\t\t\t\t\t\traise TypeError('{}.__metaclass__ should be the metaclass tuple or list; it is ambiguous; use explicit_mc=%r' % (base[0], base[1]))\n\t\t\t\t\t\t\n\t\t\t\t\t\tbase = (base[0],) + base[1:]\n\t\t\t\t\telse:\n\t\t\t\t\t\tif isinstance(base, str):\n\t\t\t\t\t\t\tbase = (base,)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\tret = \"new \"\n\t\tmeta = klassmeta(args)\n\n\tif explicit_mc is not None and len(ret):\n\t\traise Exception(\"Explicit metaclass found to use metaclass \" \\\n\t\t\t\t\"for classmethods with multiple bases\")\n\n\tfor base in bases:\n\t\tbases_with_explicit_mc = []\n\t\tif issubclass(base, classmethods) and issubclass(base, extendedproperty):\n\t\t\tbases_with_explicit_mc.append(base)\n\n\t\tfor i in range(len(bases_with_explicit_mc)-1):\n\t\t\tderived_class = bases"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\t# Note: There are a few cases we want to make to get the correct subclass\n\t\t# for the given class (name/superclass)\n\t\tfrom haystack.utils import resolve_mro\n\t\tis_single = isinstance(bases[0], baseclasses)\n\t\tabstract = bases[0]\n\t\t# Find any duplicates\n\t\tmeta = {}\n\t\t# Look for parents\n\t\tparents = [\n\t\t\tbases.pop(0),\n\t\t\t(b, bases)]\n\t\tfor parents_cls in parents[::-1]:\n\t\t\t# Check if the base class has an abstract base\n\t\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n# This is not part of the public domain.\n# It is not imported here due to circular import.\n\n#\n# An abstract base class for AnnoyingClass\n#\n# An abstract base class for AnnoyingFunction\n#\n# This object does not have a real parent class for all AnnoyingFunction's.\n#\n# If these classes are not imported elsewhere, redefined them as static variables in the code.\n#\n# This class ensures that all the classes that inherit from AnnoyingClass are imported.\n#\n# The only important thing we could think of is that"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n       if explicit_mc is not None and explicit_mc!= '':\n    \t    bases = inspect.getmro(explicit_mc)\n    \n       inheritance = set([\n        ] + list(bases))\n       for base in bases:\n            for key in base.__dict__:\n                if ( key not in inheritance\n                or keyword.iskeyword(key)\n                or not explicit_mc( key, base.__dict__[key] )\n                ):\n                   inheritance.add(key)\n       return inheritance\n\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright 2012 Rackspace Hosting\n# All Rights Reserved"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\tif not explicit_mc and not hasattr(bases[-1], \"_metaclass\") and hasattr(bases[-2], \"_metaclass\"):\n\n\t\t# check which class from second base is used\n\t\tbases_bases = []\n\t\tbases_mro = []\n\n\t\twhile bases:\n\t\t\tbase_obj = bases.pop()\n\t\t\ttry:\n\t\t\t\tbases_bases.append(base_obj.__metaclass__)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\n\t\tif not bases_bases:\n\t\t\traise TypeError(\"No suitable ABC\")\n\t\telse:\n\t\t\tif len(bases_bases) > 1:\n\t\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\tif bases:\n\t\tbases = list(sorted(bases, key=itemgetter(0)))\n\tif explicit_mc:\t\t# explicit __metaclass__ (as in C++)\n\t\tbases = list(explicit_mc) + bases\n\treturn ABCMetaDict(bases, metaclass=[])\n\ndef findClassName(str):\n\t\"\"\"\n\t\tAttempts to find the class of the module with the provided\n\t\tshort-hand format string. If no valid method is available,\n\t\tNone is returned.\n\t\tIf no docstring is available, None is returned.\n\t\"\"\"\n\tret = []"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n#\tprint(\"classes\")\n#\tprint(findClass(\"java.lang.Class\"))\n\tprint(findClass(\"java.lang.Class;\", explicit=True))\n\tprint(\"  |-----\")\n\tfor superclass in bases:\n\t\tfor abstract in superclass.abstract:\n\t\t\tif abstract.__name__ == \"java.lang.Object\":\n\t\t\t\tspecial = abstract.super\n\t\t\t\tfor attribute in special.attributes():\n\t\t\t\t\tif attribute.isAttribute() and (special.getAttribute(attribute.getName()) or special.getAttribute(special).getShortForm()):\n\t\t\t\t\t\tif attribute.getName"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:   # D.pop(k)\n\t\t\treturn self.popfirst(key), self.pop(key)\n\t\telse:\n\t\t\treturn default, self.pop(key, default)\n\n\tdef has_key(self, key):\n\t\treturn hasattr(self, key)\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tprint the dictionary:\n\t\t{str: str}\n\t\t\"\"\"\n\t\treturn str(dict([(key, getattr(self, key)) for key in self.__slots__]))\n\n\tdef __str__(self):\n\t\t_"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t self[key] = default\n\t self.remove(key); return default\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tE.setdefault(k[,d]) -> v, set specified key to value.\n\t\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\t\t\n\t\treturn self[key]\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\tE.__eq__(other) <==> cmp(x, y) <==> cmp(x"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\tv=self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise KeyError(\"No value at [%s]\" % key)\n\t\t\tv=default\n\t\telse:\n\t\t\tdel self[key]\n\n\t\treturn v\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tk, v=self.popitem()\n\t\texcept"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise KeyError(key)\n\t\t\telse:\n\t\t\t\treturn default\n\n\tdef setdefault(self, key, default=__marker):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef __delitem__(self, key):\n\t\tif key in self.dict:\n\t\t\tself._removeitem(key)\n\t\telse:\n\t\t\traise KeyError(key)\n\n\tdef keys(self):\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t # FIXME this is the most efficient way to do this\n\t return self.pop(key, default)\n\npop.__doc__ = RowProxy.__doc__\n\nclass ValuesMixin(object):\n\t\"\"\"\n\tThe methods to add values to a table.\n\t\"\"\"\n\t__slots__ = ()\n\t_is_initialized = False\n\t__fields__ = ()\n\tdef __init__(self, dbfile):\n\t\t\"\"\"\n\t\t@param dbfile: The underlying database instance\n\t\t\"\"\"\n\t\tself._dbfile = dbfile\n\t\tself.__load()\n\n\tdef __load"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t return dict.pop(self, key, default)\n\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tD.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od\n\t\tIf key is not found and default is None, missing key is returned.\n\t\t\"\"\"\n\t return self.setdefault(key, default)\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), return and remove and return some (key, value) pair\n\t\"\"\"\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\treturn self.popall(key).pop(0)[1]\n\n\tdef update(self, *args, **kwargs):\n\t\tdict.update(*args, **kwargs)\n\t\treturn self\n\n\t#\n\t# D.popitem()\n\t#\n\tdef popall(self, key, *args):\n\t\tif len(args) > 1:\n\t\t\traise TypeError, \"pop expected at most 2 arguments, got \" + repr(1 + len(args))\n\t\tif key == default: return args[0]\n\t\tif not args:\n\t\t\traise KeyError, key\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\tv = self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise KeyError(key)\n\t\t\telse:\n\t\t\t\treturn default\n\t\telse:\n\t\t\tself.__popval()\n\t\t\treturn v\n\n\tdef popitem(self):\n\t\t\"\"\"D.popitem()[0] -> (k(''), v), remove and return some (key, value) pair as a 2-tuple.\"\"\"\n\t\ttry:\n\t\t\tv = self[KEY]\n\t\texcept KeyError:\n\t\t\traise KeyError(KeyError(key))\n\t\telse:\n\t\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t self.fail(key)\n\t return default\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple;\n\t\traise KeyError if D is empty.\n\t\t\"\"\"\n\t try:\n\t\t\tk, v = next(iter(self))\n\t\texcept StopIteration:\n\t\t\t raise KeyError(k)\n\t\telse:\n\t\t\treturn v\n\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tD.setdefault(k[,d]) -> D.get(k,"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError, e:\n\t\t\tif default is self.__marker:\n\t\t\t\traise KeyError('Default value (%s) is not found.' % default)\n\t\t\telse:\n\t\t\t\traise KeyError('Key not found: %s' % key)\n\t\telse:\n\t\t\tdel self[key], value[1]\n\t\t\treturn value[0]\n\n\tdef popitem(self):\n\t\tx = self.items()[0]\n\t\tdel self[x[0]]\n\t\treturn x\n\n\tdef __eq__(self, other):\n\t\t"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t # popitem() is not available on Mac OS X, so this is\n\t\t# the recommended way to ensure the portability of\n\t\t# the target process always happens.\n\t\tx, y = os.waitpid(-1, 0)\n\t\tif x:\n\t\t\tself._unlock_console()\n\t\t\tself._write(str(x))\n\t\t\tself.output(\"<E\\x08\")\n\t\telif y:\n\t\t\tself.output(\"<E\\x08\")\n\n\tdef _write(self, line):\n\t\tself.__locked = True\n\t\twhile self.__locked:\n\t\t\ttry"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\nclass Counter(object):\n\tdef __init__(self):\n\t\tself.n = 0\n\t\n\tdef inc(self):\n\t\tself.n = self.n + 1\n\t\t\n\t\ndef popsum(pop, n=1):\n\t\"\"\"\n\tTake an n-element ``n`` from the list that was popped.\n\n\tUsage: ``Populate(3, 'ABCDEFG')``\n\n\t* `pop` is a ``list``\n\t* `n` is optional. If `n` is ``None``, the last ``n`` elements\n\t\t  will be popped and `"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t return self._popitem(key, -1)\n\n\t__contains__ = popitem.__contains__\n\t__next__ = popitem.__next__\n\t__reduce__ = popitem.__reduce__\n\t__repr__ = popitem.__repr__\n\t__getnewargs__ = popitem.__getnewargs__\n\nclass Sorted(list):\n\t\"\"\"\n\tOrdered/hashable object list base class.\n\tClass instances are immutable.\n\n\tExample\n\t=======\n\t>>> a = Sorted([1, 2, 3])\n\t>>> a.pop()\n\t1\n\t>>> a"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn self.dct.popitem(last = False)\n\nclass Bunch(dict):\n\t\"\"\"\n\tA :class:`dict` like object. Supports both normal and hashed keys\n\t\"\"\"\n\n\n\t__slots__     = ('key', 'value', 'dct')\n\n\tdef __init__(self, key, value = None):\n\t\ttry: self.key  = key\n\t\texcept AttributeError: self.key  = None\n\t\ttry: self.value = value\n\t\texcept AttributeError: self.value = None\n# \tdef __getattr__(self, item):\n# \t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\n\tkey, value = self if self.key is not UNSET else self.key.pop()\n\tif value is UNSET:\n\t    raise KeyError(key)\n\telse:\n\t    return key, value\n\n\n# end of file\n# encoding: utf-8\n# author: wangtao\n\n\"\"\"\nTest suite for pptx.oxml.parser.etree.ElementTree\n\"\"\"\n\nfrom __future__ import absolute_import, print_function, division\nimport pytest\n\nfrom. import ElementTree, utils\n\nfrom. import UnitTest\n\n\nclass Pptx"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\ttry:\n\t\t\twhile 1:\n\t\t\t\tkey = popitem()\n\t\t\t\tif key:\n\t\t\t\t\tyield key, getdeepattr(self.cfg, key)\n\t\t\t\telse:\n\t\t\t\t\traise StopIteration\n\t\texcept StopIteration:    # raised by popitem() when empty\n\t\t\traise KeyError(\"popitem() failed: popitem() \" \\\n\t\t\t\t\"returned item\")\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tIterate over the (key, value) pairs in the config store.\n\t\t\"\"\"\n\t\tfor key in iter(self.cfg):\n\t\t\tvalue = getdeepattr("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t self.purge(self.get(self.current_page))\n\t return self.get(self.current_page)[1]\n\n\n\tdef delete_value(self, key, value):\n\t\t\"\"\"\n\t\tRemove a `value` from the cache, if valid. Also remove the document with keys:\n\t\t\t* if `value` is in the cache, make a new `DELETED` list for it\n\t\t\t\telse add it to the cache, if so\n\t\t\t* if `value` is not in the cache, make a new `DELETED` list for it\n\t\t\t\telse add it to the cache,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n\n\n# vim: ts=8 sw=8 noet\nfrom __future__ import division\nimport warnings\n# scipy.spatial imports imports\nfrom scipy.special import orth_discrepancy\n# scikits.talkbox is not shipped with SciPy/SciPy/scipy/scipy/special.py\n\n\ndef levy_stable_genextreme(X,\n                             mu=None, phi=None,\n                             Sigma=None,\n                             Cv2=None,\n                             random_state="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t return self.__dict__.pop(0)\n\tpopitem = popitem\n\t\n\tdef clear(self):\n\t\t\"\"\"Removes everything.\"\"\"\n\t\tself.__dict__.clear()\n\t\n\tdef __iter__(self):\n\t\t\"\"\"Iteration function.\"\"\"\n\t\treturn self\n\t\n\tdef next(self):\n\t\t\"\"\"\n\t\tReturns next item in the dictionary, else None.\n\t\t\n\t\tUse items() to request the next batch of items.\n\t\t\"\"\"\n\t\tx = self.popn(dict)\n\t\tself.__dict__.update(x)\n\t\treturn x\n\t\n\tdef __nonzero__(self):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif self.list is None:\n\t\t\traise KeyError(self)\n\t\tassert self!= self.list\n\t\treturn self.list.pop(0)\n\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return the most recently popped value in the `dict`.\n\t\t\"\"\"\n\t\treturn self.dict.pop(0)\n\n\n\t# -------------------------------------------------------------------------------\n\n\tdef close(self):\n\t\tself.dict.close()\n\t\tC.__del__(self)\n\n\n\t#----------------------------------------------------------------------\n\n\tdef size(self):\n\t\treturn len(self.dict)\n\n\n\t#----------------------------------------------------------------------\n\n\t"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t return map(lambda x: self.pop(x)[0], self.iteritems(), self.iterkeys())\n\n\tdef setdefault(self, key, default):\n\t\treturn map(lambda x: x.setdefault(key, default), self.iteritems(), self.iterkeys())# -*- coding: utf-8 -*-\nimport json\nimport re\n\n\nclass VanillaAPIError(Exception):\n    template = u\"Vanilla API Error! Response from varnish-cache-tool {0}\"\n\n    def __init__(self, message):\n        if isinstance(message, list):\n            self.errors ="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t self.items    = ((key, value) for key, value in self.items)\n\titems = property(items)\n# -*- coding: utf-8 -*-\nu\"\"\"Algorithms for spectral clustering.\"\"\"\nfrom __future__ import division, unicode_literals, print_function\n\nimport numpy as np\n\nfrom..core import AggOp\nfrom..externals.six import string_types\n\n\nclass SpectralClusterer(AggOp):\n    \"\"\"Perform spectral clustering of data.\n\n    See:\n    ----------\n    - <https://en.wikipedia.org/wiki/spectral_clustering>`"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t    \n\tif len(self) == 0:\n\t    raise IndexError, \"remove(0) failed\"\n\tkey = self[0]\n\tdel self[0]\n\tif self[index] == key:\n\t    return (key, self[0][1])\n\telse:\n\t    return (\"pop\", (), [ (\"key\", key) ])\n\n    def index(self, value):\n\t\"\"\"\n\tReturn the first time a value was added or removed from the `self`.\n\t\"\"\"\n\t# If it's a container this is a container and the value\n\t# in that container is"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\nclass OrderedDict(dict):\n\t\"\"\"\n\tDict that has a key, value pair for every element in the\n\tordered dict.  After use, adds the `key` next to the dict,\n\tand returns the previous value.\n\t\"\"\"\n\tdef __init__(self, d=None):\n\t\tdict.__init__(self)\n\t\tself.keyOrder = []\n\t\tif d: self._order = d.keys()\n\n\tdef __delitem__(self, key):\n\t\tdict.__delitem__(self, key)\n\t\tself.keyOrder.remove(key)\n\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n        ret = self.data.popitem(False)\n        self.last = self.last.pop(self.key_for_item(), None)\n        self.data.clear()\n\t\n        return ret\n\n    def __delitem__(self, key):\n\n\tdel self.data[key]\n\t\t\t\n\treturn self\n\n    def __setitem__(self, key, value):\n\n\t# if we've reached the end of the list, delete it\n\tif key > self.last.last:\n\t    self.last.remove(key)\n\n        ret = self.data[key] = value\n\tself.last"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t # TODO: We should use a different mechanism than `iter_next(self)`?\n\t # http://stackoverflow.com/questions/328470/iterating-through-lists\n\t # https://docs.python.org/2/library/functions.html#itertools-recipes\n\t def popnext(self, key, *args):\n##\t\treturn list.pop(self, key, *args)\n\t\t\tfor value in self[key] + args[:-1]:\n\t\t\t\tself.remove(key)\n\t\treturn popnext\n\t\n\tpop = popitem\n\n## {{{ http://code"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t return self.ui.get( 'popitem' )\n\n\tdef pop(self, *args, **kwargs):\n\t\t'''\n\t\tRemove and return the most recently used key/value pairs, where\n\t\tkey is the string key (eg \"foo\" for the first key) and value is the\n\t\tany value (eg any instance of this class). If the key does not exist\n\t\tor no values have been assigned, then a KeyError exception is raised.\n\n\t\tReturns a tuple of (key, value) for the most recently used key, and\n\t\tand where values is a tuple of (key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\treturn self.popitem\n\t\n\tdef __len__(self):\n\t\t\"\"\"\n\t\t>>> i1 = dict(a=1, b=2, n=3)\n\t\t>>> i1.get(5)\n\t\t1\n\t\t>>> i1['a']\n\t\t1\n\t\t>>> i1.get(4)\n\t\t1\n\t\t>>> i1.get(9, 10)\n\t\t11\n\t\t>>> i1.get(99, 100)\n\t\t99\n\t\t>>> i1.get(9999)\n\t\tTraceback (most recent call last):\n\t\tKeyError: '"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n\n\nclass _dict_update(dict):\n\t\"\"\"\n\tA dict which keeps a *copy* of values.\n\t\"\"\"\n\tdef __setitem__(self, key, value):\n\t\tself.setdefault(key, []).append(value)\n\t\n\tdef __strip_quotes(self):\n\t\tnew_items = []\n\t\tfor k, v in self.items():\n\t\t\tk = k.strip('\"')\n\t\t\tif k!= '' and (k[0] == \"'\" and k[-1] == \"'\" and k!= \"'\"):\n\t\t\t\tif v.strip('\"')!= v:\n\t\t\t\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t return self.filter(lambda (k, v): v[v.find(self.key)>-1].pop())\n\t#\n\t# Private methods\n\t#\n\n\tdef remove(self):\n\t\t\"\"\"\n\t\tRemove the most recently used key. (Like L{L{L{remove}}})\n\t\t\"\"\"\n\t\treturn self.filter(lambda k, v: v.pop(k, None))\n\t#\n\t# Misc\n\t#\n\n\tdef lock(self, llock=0):\n\t\told = self.__dict__.get('_lock', None)\n\t"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\n\n\t# TODO: add an option to automatically delete all items\n\t# def clear(self, *a, **kw):\n\t# self.dict.clear(key=self.id, **kw)\n\t# TODO: add more options (with _deleted=False)\n\t# def delete(self, *a, **kw):\n\t# self.dict.delete(key=self.id, **kw)\n\n    def sort(self, key=None, reverse=False, reverse_index=False):\n        if key is None:\n            return sorted(self.dict.iteritems(),"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\treturn self._memo[id(self)]\n\n\tdef __contains__(self, key):\n\t\treturn self._memo.__contains__(key)\n\n\t__contains__.func_annotations = {\n\t\t\t'key' : func_annotations.String(\n\t\t\t\tdoc=\"String that replaces %s with the %s key\",\n\t\t\t\tmaxstringsize = 100\n\t\t\t),\n\t\t}\n\nclass Stackable:\n\t\"\"\"\n\tA stackable deque that holds (id, data) pairs.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._d = deque()\n\t\tself."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tfor i in range(len(self)):\n\t\t\tobj = self[i]\n\t\t\tval = self.delete(obj)\n\t\t\tif val is not None:\n\t\t\t\tyield key\n\t\traise KeyError(key)\n\n\nclass O(object):\n        \"\"\"Special type indicating an array data structure (list, etc.).\n\t\t\"Array\" data structures are identified by tuples of some arbitrary\n\t\tstructure.\n\t\tThe first object in the sequence is always assumed to be the first\n\t\tof its fields, while the remaining object in the sequence is always assumed\n\t\tto be an iterator and a single sequence may"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tif hasattr(self, '_map'):\n\t\t\tfor (key, value) in self._map.items():\n\t\t\t\tdatapoint = (key, value.__dict__)\n\t\t\t\tdel self._map[datapoint]\n\n\t\treturn self._map.popitem()[1]\n\n\tdef popitemlist(self):\n\t\t\"\"\"\n\tReturn a SortedMap of the items in the map at the top-level;\n\tso only one level remains.\n\t\t\"\"\"\n\t\tmymap = SortedMap()\n\n\t\tfor (key, value) in self._map.items():\n\t\t\tmymap[key]"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t self._check_alive()\n\t # TODO: This should not be used by code below since it is not in\n\t\t#      this module.\n\t value, key = self._queue.pop(0)\n\t return key, value\n\n\tdef peekitem(self):\n\t\"\"\"\n\tRemove and return the ``(key, value)`` pair most recently used.\n\t\"\"\"\n\t self._check_alive()\n\t return self._queue[-1][1], self._queue[0][0]\n\n\tdef push(self, key, value):\n\t\t\"\"\"\n\tAdd and return the `(key, value)`"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\n\n\tdef push(self, item, itempath):\n\t\t\"Set an item to be pushed to `itempath`. This is only called on the set items.\"\n\t\t# if path already has path we have to remove so it\n\t\t# will get updated\n\t\t# path\n\t\tpath = self.path + '/' + itempath\n\t\tif not path in set(self.tree):\n\t\t\traise KeyError\n\t\telif path in self:\n\t\t\tdel self[path]\t# remove value\n\t\telse:\n\t\t\tself[path] = item\n\n\tdef pop(self, item):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\nclass Stack(BaseList):\n\t\n\tdef append(self, x, value):\n\t\tBaseList.append(self, x, value)\n\n\tdef pop(self):\n\t\treturn BaseList.pop(self, 0)\n\n\nclass StackEmpty(object):\n\t\n\tdef __init__(self):\n\t\tself.head = self.tail = None\n\t\tself.length = 0\n\t\t\t\n\tdef push(self, x):\n\t\tself.length += 1\n\t\tself.head.val += x + self.tail.val\n\t\tself.tail = self.head.next\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\treturn self.map.pop()\n\t\n\tdef findall(self, predicate):\n\t\t\"Find all items that match the given predicate.\"\n\t\tseen, count = [], 0\n\t\tfor x in self.map.items():\n\t\t\tif not predicate(x):\n\t\t\t\tseen.append(x)\n\t\t\t\tcount += 1\n\t\treturn seen, count\n\t\n\tlookup = find\n\n\tdef __repr__(self):\n\t\tkeys = self.map.keys()\n\t\tkeys.sort()\n\t\tif self.lookup:\n\t\t\titems = ', '.join(repr(key) for key in keys"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\n\t    (key, val) = self.next()\n\t    if len(self ) > 0:\n\t\tif key in self.keymap:\n\t\t    if self[key]!= val:\n\t\t\tdel self[key]\n\t\t\n\t    return (key, val)\n\t    else:\n\t\treturn None\n\t   \n\tdef setdefault(self, key, value = None):\n\t\"\"\"\n\tReturn the value of a key if it is in the cache and has not expired.\n\t\"\"\"\n\texpires = self.expires\n\treturn (key, value) if expires and expires > time.time() else None\n\n_"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\n        if(os.path.join(self.path,'last'))!='/':\n\t\n                if not os.path.isdir(self.path) or not os.access(self.path,os.W_OK):\n\t\t    os.makedirs(self.path)\n\n                        os.system('rm -f'+ self.path)\n                        os.rmdir(self.path)\n#                            os.makedirs(self.path)\n                        self._logger.debug(\"[GalaxyPlugin: Popitem] %s directory\" % os.path.abspath(self.path))\n                        sys.stdout.flush()\n                       "}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn self._cache.pop((self._key(0), self._key(0), None))\n\n\tdef getvalue(self, i):\n\t\t\"\"\" Return the value of item `i`. If `i` is an integer, return\n\t\t`None`. \"\"\"\n\t\treturn self._cache.get(self._key(i))[1]\n\n\tdef _getkey(self):\n\t\t\"\"\" Recover the next value in the cache. The next value is returned\n\t\twhen the cache is exhausted (missing from the cache). Returns `None`\n\t\tif the cache is empty. \"\"\"\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn random.getrandbits(16)\n\t\n\t# Returns a random key or a random value, whichever is smaller.\n\tdef get(self, key, default = None):\n\t\treturn self.pop(key, default)\n\n\tdef get1(self, key, default = None):\n\t\tif default is None:\n\t\t\tdefault = self.get(key, key)\n\t\treturn pickle.loads(self.get(key, default))\n# Copyright (c) 2014 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n#"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tif self.head is None: raise ValueError(\"deque popitem should be called \"\n\t\t\t\t\t\t\t\t \"before popitem!\")\n\t\th, v = self.head\n\t\tself.head = None # break reference to previous item\n\t\twhile v is not None:\n\t\t\tyield v[0], v[1]\n\t\t\tv = self.head\n\t\traise LookupError(self.head)\n\t# get an item from the front of the queue\n\tdef __getitem__(self, index):\n\t\t\"\"\"\n\t\tLike L{_list}[idx] but deals with the head and tail elements\n\t\t\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn random.choice(self._items)\n\t\t\ndef _hashdict(d):\n    \"\"\"\n\tGet a hashable `d` dictionary mapping each `key` to a `value` dictionary.\n    The `value` dictionary contains (key, value) pairs that uniquely identify the key, and whose value\n    will be a list of `(value, key)` tuples.\n    \"\"\"\n    h = {}; v = {}\n    for key in d:\n        val = d[key]\n        if isinstance(val, d):\n            h[key] = val\n        else:\n            v[key] = val\n    return h,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\npopitem = popitem\n\nclass _FileLikeIO(object):\n\t\"\"\"\n\tFile-like object that returns a file object that can be used\n\tas a replacement for the standard input.\n\t\"\"\"\n\t__slots__ = ('file','mode','buffering', 'encoding', 'errors','mode_code')\n\t\n\tdef __init__(self):\n\t\tself.file = _FileLike.input\n\t\tself.mode = 'r'\n\t\tself.buffering = 1\n\nclass _UnicodeIO(_FileLikeIO):\n\t\"\"\"\n\tUnicode-like"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\n##__________________________________________________________________||---------------------------------------------------------|\n\nclass RandomListError(Exception):\n\tdef __init__(self, msg):\n\t\tException.__init__(self, msg)\n\tdef __str__(self):\n\t\treturn self.__class__.__name__ + ':'+ self.msg\n\n\n##__________________________________________________________________||---------------------------------------------------------|\n##__________________________________________________________________||---------------------------------------------------------|\n\nclass RandomIntegerError(Exception):\n\tdef __init__(self, msg):\n\t\tException.__init__(self, msg)\n\tdef __str__(self):\n\t\treturn self.__class__.__name__ +"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\ndef reverse_dict(indict):\n\t\"\"\"\n\tMake a reverse mapping from the indict to an iterator that is suitable in most cases.\n\t\"\"\"\n\ttry:\n\t\tkdata = indict.items()\n\texcept AttributeError:\n\t\tkdata = enumerate(indict)\n\tfor key, value in kdata:\n\t\tyield value\t\t\n\t\n\nclass Roller:\n\t\"\"\"\n\tThe rim and related routines are the basic interface to generate and return rolling\n\torders.\n\t\"\"\"\n\tdef __init__(self, indict, rolling_"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\trandom = self.random\n\t\tkey = tuple(random())\n\t\ttry:\n\t\t\titem = self.head[key]\n\t\texcept KeyError:\n\t\t\ttry:\n\t\t\t\titem = self._head.next()\n\t\t\texcept:\n\t\t\t\tif not self.head:\n\t\t\t\t\traise\n\t\t\t\tpass\n\t\telse:\n\t\t\tdel self[key]\n\t\t\titem = item[3]\n\n\t\tself.head[key] = (item, random.random(), key)\n\n\tdef shuffle(self):\n\t\t\"\"\"\n\tShuffle the buffer.\n\t\"\"\"\n\t\tself.random = self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn (self.key, self.value)\n\n\tdef random(self):\n\t\t\"\"\"\n\t\tReturn a random element from the list.\n\t\t\"\"\"\n\t\treturn self._random()\n\n\tdef shuffle(self):\n\t\t\"\"\"\n\t\tReturn a random element from the list.\n\t\t\"\"\"\n\t\tif not self._shuffle():\n\t\t\t# The list is the same as random\n\t\t\treturn self._random()\n\t\treturn self._items[self._seed]\n\n\tdef _shuffle(self):\n\t\t\"\"\"\n\t\tTake a random element, and shuffle it.  Returns a random tuple."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tval = self.pop()\n\t\tif val[0] == value:\n\t\t\treturn key, val[1]\n\t\telse:\n\t\t\treturn None, val\n\n\tdef clear(self):\n\t\tself.cleardict()\n\n\tdef clone(self):\n\t\tnew = Lorz()\n\t\tnew.clear()\n\t\tfor index in range(len(self.keys())):\n\t\t\tnew[self.keys()[index]] = self[index]\n\t\treturn new\n# -*- coding: utf-8 -*-\n\n\"\"\"\n    Exodus Add-on\n    Copyright (C)"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\t@staticmethod\n\tdef _regex_in(patterns: List[Pattern], line_number: int) -> Pattern:\n\t\"\"\"\n\tGet the regular expression expression (``re.match(pattern, line_number)``).\n\t\n\tReturns the regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\tpattern = Pattern()\n\tregular = pattern.search(patterns[line_number])\n\treturn regular\n\n######################################################################\n# Classes\n######################################################################\n\nclass CommandProcessor(AbstractIRCommandProcessor):\n\t\"\"\"\n\tCommand processor for handling data produced by a command.\n\t\n\tThe data"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\n\tregex_string = r'''\\b(define-macro ([^,]+;[^,]+,?)+ )\n#\ndef define_macro (text):\n\t'''\n\tParse a macro definition text and return the definition variable.\n\t\n\tArguments:\n\ttext-A string to interpret.\n\t\nReturns:\n\tDefinition variable\n\t'''\n\tglobal _definition\n\t\n\tm = _definition_re.search(text)\n\tif not m:\n\t\treturn '' # pragma: no cover\n\t\n\treturn m.group(1)\n\n\ndef parse (text: str, re"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\n\t@property\n\tdef regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tRegular expression string or compiled regular expression to be matched for matching.\n\t\t\n\t\t:rtype: :obj:`re.Pattern`\n\t\t\"\"\"\n\t\treturn _compile(self._expr)\n\t#end def\n\n\n# end class Parameter\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nUtilities to extract Sherpa results from the Sherpa data set\n\"\"\"\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t \n\treturn re.compile(self.__pattern, self.__flags)\n\nclass InternPattern(InTypeObject):\n\t\"\"\"\n\tPattern which can be entered in any of the given list of type objects.\n\t\n\tContains methods that can be used to enter the interpretation code\n\tor a single instance with a specific interpretation code. \n\t\n\tThis is an abstract base class which serves as a template for :class:`InTypeObject`s.\n\t\"\"\"\n\t \n\t__pattern = None\n\t\"\"\"The pattern that will be matched against the interpretation code.\n\t\"\"\"\n\t__interpretation"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\nclass _OutFilePattern(Pattern):\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\t\n\tReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\tThe ``line`` parameter of the regular expression (:class:`re.Pattern`)\n\tcontains the location of the in-file and the optional filename.\n\n\tThe pattern is an input file, which represents one or more lines of\n\ttext.  It can be used to pass the text of the file to the\n\toutput stream in case of a multipart form, so that\n\tit writes as :class:`"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t  # In [1]: self.out_str is the matched text string.\n#        return re.compile( r'[ \\t]*(\"(?:\\\\\\\\)*\"|[ \\t]*\\{[^\\}]*\"|[^\\}]*\\})|(?!\\*)(\\\\\\\\|\\'\\'|#[^\\n])\\n)', re.S|re.X)\n\n\tdef _get_in_style_pattern(self) -> str:\n\t\t\"\"\"\n\t\tReplace the original IN_STYLE definition with the right template\n\t\texpression.\n\t\t\n\t\tReturns the original IN_STYLE expression (:class:`re.Pattern`).\n\t\t\"\"\"   #"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\t@classmethod\n\tdef compile_parameter(cls, parameter_name: Optional[str]) -> \"re.Pattern\":\n\t\t\"\"\"\n\t\tCreate the pattern for a given :class:`re.Match` parameter.\n\t\t\n\t\t.. seealso:: :func:`re.compile`\n\t\t\n\t\t\"\"\"\n\t\treturn cls.PATTERN_CLASS_TEMPLATE.compile(\"${%s}\" % parameter_name)\n\t\n\t@classmethod\n\tdef check_parameter(cls, parameter_name: Optional[str] = None) -> \"re.Match\":\n\t\t\"\"\"\n\t\tCheck if parameter is available in the user input\n\t\t\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\nclass RegexpLink(Link):\n\t\"\"\"\n\tLink that represents a regex matcher. In addition to normal\n\texpressions with simple strings or `None` as replacement parameter, such as from\n\t`(?P<regex_string>\\w)_pattern` will ensure no string-specific matching will halt\n\tthe regex.\n\t\n\tFor example:\n\t\n\t>>> re._create_in_regex().pattern\n\t'.*(?P<pattern>(?P<re_string>[a-z]+)_pattern):(?P<re_string2>[a-z]+)$'\n\t\"\"\"\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\treturn self._regexp('')\n\t\n\t@classmethod\n\tdef _to_regex(cls, value: str) -> str:\n\t\tif cls._stringmatcher:\n\t\t\treturn cls._stringmatcher.format(value)\n\t\treturn value\n\t\n\tdef _update_name_with_value(self, match, newmatch):\n\t\t# The parameter name\n\t\tname = match.group('name')\n\t\tnewmatch = match.group('final')\n\t\t# Match\n\t\tif newmatch == oldmatch:\n\t\t\tval = name\n\t\telse:\n\t\t\tval = newmatch\n\t\t#"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\t_re_parameter_regular = re.compile('(' + '(?:[^)\\\\/\\\\\\\\]+) *:? *')\n\n\tdef _get_parameter_regular(self, pattern, name):\n\t\t\"\"\"\n\t\tReturns the matching Regular Expression in a given pattern named\n\t\t\"name\". The returned Regular Expression object is not modified.\n\t\t\n\t\tNote that a :class:`re.Pattern` object returns **None** if the parameter\n\t\tdoes not exist in this Pattern.\n\t\t\"\"\"\n\t\tresult = _re_parameter_regular.match(pattern.name)\n\t\tif result is None:"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n_converterClass = type(converter_class_or_factory(Converter))\n_converter = _converting._Converter(_converterClass, _converter_args, converter_args_doc)\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2014 Agile Business Group sagl\n#    (<http://www.agilebg.com>) (<lorenzo.battistini@agilebg.com>)\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\tif self._parameter_style_converter is None:\n\t\traise ValueError(\"Cannot map a parameter style converter that has not been initialized yet.\")\n\t\n\treturn self._parameter_style_converter._convert(self._value)\n\nimport os\nimport sys\nimport tempfile\nimport signal\nimport warnings\n\nimport numpy as np\nimport numpy.lib.stride_tricks as py_strided\nfrom numpy.compat import asbytes, asunicode\n\nif sys.version_info[0] >= 3:\n    import cPickle as pickle\nelse:\n    import pickle\n\ndef _get_conv_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n_PARAMSTYLE_CONVERTER = {_KEYWORD_CHAR: _KEYWORD_CHAR2CHAR,\n                                _KEYWORD_SPACE: _KEYWORD_SPACE,\n                                _KEYWORD_BRACE: _KEYWORD_BRACE2CHAR,\n                                _KEYWORD_BACK_QUOTE: _KEYWORD_BACKQUOTE,\n                                _KEYWORD_START_QUOTE: _KEYWORD_START_QUOTE2CHAR}\n\"\"\"\nMapping from keyword or character sets to parameter names and types. \n\"\"\"\n\n_PY2 = sys.version_info[0] == 2\n_PY3 = sys.version_info[0] == 3\n_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\nclass _parameter_style_converter_class(_parameter_style_converter):\n\t\"\"\"\n\tConvert a Parameter Style into a `Converting._ParameterStyleConverter`.\"\"\"\n\tdef __init__(self, new_style_converter):\n\t\tself._new_style_converter = new_style_converter\n\t\tself._converters = {}\n\t\n\tdef get(self, key):\n\t\t\"\"\"\n\t\tRetrieves the converter associated with the given key. The specified\n\t\tformat is returned as a dictionary of conversion classes (see\n\t\t`ConvertingParameters` and `ParameterValue` for more information).\n\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\tdef to_json(self, **kw):\n\t\t\"\"\"\n\t\tReturn the instance as a JSON string.\n\t\t\"\"\"\n\t\t\n\t\treturn json.dumps(self, **kw)\n\t\n\tdef to_latex(self):\n\t\t\"\"\"\n\t\tReturn latex string\n\t\t\"\"\"\n\t\t\n\t\tstring = \"\\n$$$\"\n\t\t\n\t\treturn string\n\n\nclass _converting_n(object):\n\t\"\"\"\n\tThis class is used for the conversion classes when a _converting variable appears in a log scale.\n\t\n\tThe _converting variable is usually an integer, because it is often interpreted by a"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\t_converter_map = {\n\t\t# a function\n\t\t'boolean': _convert_boolean,\t\t# boolean; this is the python value type\n\t}\n\n\tdef create(self, value: _converting._Converter, _type : type) -> bool:\n\t\t\"\"\"\n\t\t:type value: str\n\t\t:param value: the python value this converter is for.\n\t\t\n\t\tIt takes the following arguments:\n\t\t\n\t\t* `value`: the Python value you're converting.\n\t\t* `type`: the python type.\n\t\t\n\t\t:raises TypeError: if this parameter is not an instance"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t \n\tclass _Converter(_converter.Converter):\n\t\"\"\"\n\tBase class to be implemented by the converter.\n\t\n\t\"\"\"\n\t\n\tdef _setup_flags(self, c_name) -> _converting._ConvertingFlags:\n\t\treturn False\n\t\t\n\tdef _post_init(self) -> None:\n\t\tself.__converters[-1] = _converter.Converter(_converter(self))\n\t\t\n\tdef _create_flags(self) -> _converting._ConvertingFlags:\n\t\treturn _converting._ConvertingFlags()\n\t\t\n\tdef _parse(self, s: str) ->"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\ndef _create_variable(name):\n\t\"\"\"\n\tCreate the parameter name and unit for the parameter object(s).\n\t\n\tArguments:\n\t\tname (str): The name of the parameter object.\n\t\n\tReturns:\n\t\tstr\n\n\t\"\"\"\n\t\ndef _create_function(name, type):\n\t\"\"\"\n\tCreate the parameter function and the type function of the parameter object(s).\n\t\n\tArguments:\n\t\tname (str): The name of the parameter object.\n\t\ttype (function): The parameter function type.\n\t\n\tReturns:\n\t\tstr\n\n\t\"\"\"\n\t\ndef _"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\nclass _converting_format_string(_converting._ConvertingFormatString):\n\t\"\"\"\n\tFormat string for parameter style conversion.\n\t\n\tThe format string that can be specified during format string translation process.\n\t\n\t.. seealso:: `the documentation <http://docs.python.org/release/2.7.2/api/java/util/format.html#format-string-option-mapping>`__\n\t\"\"\"\n\t\nclass _converting_parse_paramvalue(_converting._ConvertingParameterValue):\n\t\"\"\"\n\tParse parameter value.\n\t\"\"\"\n\nclass _"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\treturn _converting._Converter(self)\n\t\n\t\n\tdef apply_style_rules(self, table: Table) -> int:\n\t\tstyle_rules = self.style_rules()\n\t\ttable.apply_table(style_rules)\n\t\treturn 0\n\t\t\n\tdef styles_style(self, **kwds) -> 'Style':\n\t\t'''\n\t\tReturn the Style object applied to the style string. \n\t\t\n\t\tThe arguments can be any valid argument but the style argument is returned.\n\t\t\n\t\tThe style argument values are assumed to be Python style objects. \n\t\t\n\t\tSee also :meth:`"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 22.16 GiB total capacity; 20.58 GiB already allocated; 25.88 MiB free; 21.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\tdef _parse_datetime(self, timestr, **kwargs):\n\t\n\t\tif isinstance(timestr, datetime.datetime):\n\t\t\t\n\t\t\tif default is None:\n\t\t\t\tdefault = datetime.datetime.now()\n\t\t\tif isinstance(timestr, basestring):\n\t\t\t\treturn default.replace(microsecond=0)\n\t\t\tdate = default\n\t\t\tdate = datetime.datetime.strptime(timestr[:11], \n\t\t\t\t\"%Y-%m-%d %H:%M:%S\")\n\t\t\t\t\t\n\t\t\ttry:\n\t\t\t\treturn date.replace(tzinfo=self._tzinfo)\n\t\t\texcept AttributeError:\n\t\t\t\tpass"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t\n\tif default is None:\n\t\tdefault = datetime.datetime.now()\n\t\n\tif isinstance(timestr, basestring):\n\t\tt = parse(timestr, default, ignoretz=ignoretz, tzinfos=tzinfos)\n\t\tdefault = datetime.datetime(*t[0:6])\n\telif isinstance(timestr, (int, long)):\n\t\ttype_ = float\n\t\tts = timestr\n\t\ttt = time.strptime(ts, \"%Y-%m-%dT%H:%M:%S\")\n\t\ttt = time.strptime(tt, \"%Y%m%dT"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n        if isinstance(timestr, _unicode):\n            timestr = _split_cookie(timestr)\n        timestr = (\n            # This is the final time - the ``parse_*_exc`` return value.\n\t    _normalize_time(timestr[0])\n           .replace(hour=0, minute=0, second=0, microsecond=0)\n           .astimezone(self._current_timezone)\n        )\n        try:\n            return self._parse(timestr[1], **kwargs)\n        except ValueError as e:\n            msg = \"Failed to parse {0}: {1}\".format(timestr"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t\n\t# Use the parser classes default handler to remove redundant\n\t# keywords. E.g. to not lose data and won't do it in the class\n\t# __init__, however if overriding handle_error is not desired\n\t# set clobbering the class defaults will fail.\n\tdef __init__(self, **kwargs):\n\t\tself.case_insensitive = False\n\t\tself._parsed_args = {}\n\t\tself._error = False\n\t\tself.tzinfos = {}\n\t\t\n\t\tself._set_kwargs(kwargs)\n\t\t\n\t\t# if 'fuzzy_with_tokens' in kwargs"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t\n\tif default is None:\n\t\tdefault = datetime.datetime.utcnow().replace(hour=0, minute=0, second=0)\n\tdefault = self._parse(default, ignoretz, tzinfos).timetuple()\n\t\n\t# If we were given a string with tzinfo \u2014 return it\n\tif isinstance(tzinfos, str):\n\t\ttzinfos = self._parse(tzinfos, ignoretz, tzinfos)\n\t\n\t# Otherwise return the first found\n\tif len(tzinfos) > 0:\n\t\treturn tzinfos[0]\n\t\n\t# If the parsed date is in"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\t    \n\t    \n         if default is None:\n\t\t    default = datetime.datetime.now().replace(microsecond=0)\n\t#end\n\n\tresult = self.parseString(timestr, default, ignoretz, tzinfos)\n\t\n\tif not ignoretz:\n\t    result = self._fix_tzoffset(result)\n\tif result:\n\t    tzinfo = tzinfos.get(\"BRST\")\n\t    if tzinfo is not None:\n\t\ttzinfo = tzinfo.astimezone(tzinfo)\n\t    result = tzinfo.tzname(result)\n\treturn result\n\t# end parse\n#: end parse\n\ndef parse"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\t# This was originally based on code posted on the internet Tony Stoll\n\t# (http://sqlalche.org/lang_keywords.html)\n\tif isinstance(timestr, datetime.datetime):\n\t    s = timestr\n\t    if s.tzinfo is None:\n\t\t if parse_only:\n\t\t\t# See the comment in tzfile above.\n\t\t\t# \n\t\t\t# The parser assumes one of the following formats:\n\t\t\t# \n\t\t\t#  A blank tzinfo in any of the follow-up forms:\n\t\t\t# \n\t\t\t#  - YYYY[-MM[-DD]]\n\t\t"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\t# This regexp matches the following:\n\t# 1) the optional currency symbol '(?P<sign>[-+])'.\n\t# 2) the number followed by a period.\n\t# 3) the terminating ``,``\n\t# 4) the content of the line\n\t# 5) the terminating newline character.\n\t\n\ttokens = {}\n\tfor match in self.date_re.finditer(timestr):\n\t    tokennum, freqstr = match.groups()\n\t    if tokennum!= '':\n\t        tokens[tokennum] = int(freqstr)\n\t    elif freqstr:\n\t"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t    matches = self.DATE_RE.finditer(timestr)\n\t    if matches is None:\n\t        raise ValueError(\"Unknown string format: {0!r}\".format(timestr))\n\t    groups = matches.groups() if matches.groups() else ()\n\t\n\t    result = self._parse(groups, default=default, ignoretz=ignoretz, tzinfos=tzinfos)\n\t\n\t\tif not result:\n\t\t\tresult = default\n\t\tself._matches = [tuple(m) for m in matches]\n\t\n\t\n\treturn result\n\n\nclass UTC(datetime.tzinfo):\n\t\"\"\"\n\tA UTC tzinfo"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\t    \n\t    \n\t    \n\t    \n\t# Use dateutil's parser to split strings as datetimes\n\t# with the given pattern.\n\t\n\tstrptime_kwargs = {}\n\tif 'fuzzy_with_tokens' in kwargs:\n\t\t# Force the use of `ignoretz` override if time zones are set\n\t\t#\tin the future.\n\t\tif ignoretz and isinstance(ignoretz, basestring):\n\t\t\tignoretz_match = re.VERBOSE | re.ASCII\n\t\telse:\n\t\t\tignoretz_match = None\n\t\tignoretz = False\n\t\tfor pattern in TimeRE."}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\ttz = None\n\tdt_prev = None\n\tts = None\n\t\n\t# TODO: use tz?\n\tif self.datetime:\n\t\tdt = self.datetime\n\t\n\tif dt:\n\t\tif dt_prev and self < dt_prev:\n\t\t\t\n\t\t\treturn dt_prev\n\t\telse:\n\t\t\ttz = dt.replace(tzinfo=UTC)\n\t\t\tts = utctimestamp(dt.tzinfo)\t\t\t\t\t\n\n\t\tif ts is None:\n\t\t\treturn ts\n\t\telif ts <= ts.replace(tzinfo=None) - tz.utcoffset() or \\\n\t\t\t\t\tts <= self."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is None:\n\t\treturn dt\n\n\tif not hasattr(self, \"fold\"):\n\t\tself.fold = False\n\n\t\n\tif not hasattr(self, 'tzinfo') or self.tzinfo is None:\n\t\tself.tzinfo = fromutc(dt)\n\t\n\tif dt.tzinfo is None:\n\t\n\t\traise ValueError(\"%r: Not a valid timezone object!\" % dt)\n\n\n\t#\n\t# This function is part of the public API for the \"fold\",\n\t# which allows us to get a time difference, as a \"folding\"-style\n\t# for"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo:\n\t\tdt = tzinfo(dt.tzinfo)\n\n\t# If the provided date is naive, return it.\n\tif dt.tzinfo is None:\n\t\treturn dt\n\t\n\treturn dt.replace(tzinfo=FUND_TZINFO).astimezone(TZINFO).replace(tzinfo=None)\n\t\n\ndef fromordinal(ordinal, offset=0, default_unit=PERSIAN):\n\t\"\"\"\n\tGiven an ordinal, returns a :class:`datetime.date` object representing that\n\tpartof that ordinal, usually slightly in the local timezone.\n\t\n\t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is not None:\n\t\treturn dt + timezoneutcdelta(dt)\n\t\n\telse:\n\t\treturn dt\n\t\n# *****************************************************************************\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\t# We'll define this here instead of in the end because\n\t# 1. We'd have to go back and let the caller deal with local\n\t# timezones until we're in the ambiguous time range.\n\tdiff = dt - self\n\t\n\t# This means that we have our ambiguous time range, but in the past\n\t# range we know we're in the target timezone\n\tfor localdt, foreigndays in diff.days():\n\t\tis_ambiguous = localdt.is_ambiguous()\n\t\tif is_ambiguous:\n\t\t\treturn localdt\n\n\nUTC = pytz.utc\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tdt = dt.replace(tzinfo=None) # naive \n\t\n\treturn dt.astimezone(self.tz)\n\n  def tzlocal(self, dt):\n\treturn self.fromutc(dt).astimezone(self.tzlocal_usec_off)\n\n  def tzutc(self, dt):\n\treturn self.fromutc(dt)\n\n\nclass DurationField(StringField):\n\t\"\"\"\n\tA form field that stores a timedelta for storing a fixed duration.\n\t\n\tThe field must have a fixed delta.\n\t\n\t.. attribute:: new_duration\n\n   .. attribute:: old_duration\n\n   "}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif isinstance(dt, datetime.datetime):\n\t\treturn dt\n\tif not isinstance(dt, datetime.date):\n\t\tdt = dt.replace(tzinfo=datetime.timezone.utc)\n\t\n\tdt = dt.astimezone(pytz.utc)\n\t\n\treturn dt\nimport argparse\nimport sys\nfrom collections import defaultdict\nimport operator\n\nfrom py_db import dbt\nfrom py_db import dbt_mysql\n\n\nparser = argparse.ArgumentParser(description='Create a MySQL Snowflake backup file. '\n                        'A new version will be created.\\n'\n                        'If"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\n\tif self.is_ambiguous(dt):\n\t\traise Exception(\"ambiguous datetime: %s\" % dt)\n\t\n\n\tif dt.tzinfo!= self._tzinfo:\n\t    raise Exception(\"wrong tzinfo: %s\" % dt)\n\n\t    \n\ttznaive = dt - timedelta(0, self.dst_tz_offset)\n\ttzaware = tznaive - tznaive.utcoffset()\n\ttzaware_offset = tzaware.days * _intervalTypes[tznaive.weekday()]\n\tnew_dt = tzaware_offset.seconds + tzaware_offset.microseconds / 1000000\n\t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif util.is_naive(dt):\n\t\treturn dt.replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).replace(tzinfo=None)\n\treturn dt\n \nfrom copy import copy\nimport pytest\nfrom mock import patch\n\nfrom mitmproxy.io import tnetstring\n\n\ndef test_tnetstring_parse():\n    tnetstring_data_text = b\"A test's string.\"\n    tnetstring_data = tnetstring.parse(tnetstring_data_text)\n    assert tnetstring_data.type =="}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n#        print dt.tzname(datetime.max)\n#        print dt.tzname(datetime(1900, 01, 01))\n        tzname = self.tzname(dt)\n        now = datetime.datetime.utcnow().replace(tzinfo=None)\n\n        if not now.tzinfo == None:\n            # America/New_York tzinfo. In Python,\n            # datetime.tzinfo.dst() will produce a correct datetime.tzinfo\n            # object of the same size. So, we roll our own\n            # tzinfo to get the same timezone.\n            tz = pytz.timezone('America/New_York')\n           "}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\treturn dt.replace(tzinfo=tzinfo).strftime(\"%Y%m%d%H%M%S%f\")\nfrom rest_framework import serializers\nfrom. import serializers as g_serializers\nfrom rest_framework.reverse import reverse\n\nfrom wagtail.core import blocks\nfrom braces.views import LoginRequiredMixin\nfrom rest_framework.views import JSONResponse\nfrom rest_framework.renderers import JSONRenderer\nfrom wagtail.core.models import Page\nfrom wagtail.core.utils import resolve_model_string\nfrom rest_framework.compat import OrderedDict\nfrom wagtail.core."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n    return parser.get_datetime(dt, tzinfo=tzinfo)\n\ndef guess_timezone(req=None):\n    \"\"\"\n    Returns the current timezone based on the Accept-Language HTTP header.\n    \n    :param req:\n\t    An instance of a ``webob.Request`` subclass or compatible\n\t    class that provides the HTTP header, or a pre-configured request\n\t    object which provides a time zone specifier. If no explicit timezone or\n\t    no timezone is specified, the current timezone will be used.\n\t\n\tFor more on how to vary the look of the header, see the module's\n    documentation.\n\t\n\t.. doctest::\n\t\n\t   "}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\t# we accept either datetime.datetime or datetime.date objects\n\tif isinstance(dt, (datetime.datetime, datetime.date)):\n\t\treturn dt\n\telif isinstance(dt, datetime.datetime):\n\t\t# is it already a naive datetime\n\t\treturn dt.replace(tzinfo=default_tzinfo)\t# already in UTC right now\n\t\n\traise ValueError(\"A naive datetime object must be passed \" + repr(dt))\"\"\"\nDjango settings for open_news project.\n\nGenerated by 'django-admin startproject' using Django 1.9.2.\n\nFor more information on"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\tif dt.tzinfo is None:\n\t\tdt = datetime.datetime.now(tzinfo)\n\tif isinstance(tzinfo, basestring):\n\t\ttzinfo = pytz.timezone(tzinfo)\n\t# TODO: replace this with a timezone-naive one\n\telif tzinfo is pytz.utc:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\t\ndef iso8601_from_timestamp(timestamp):\n\tif timestamp is None:\n\t\treturn None\n\tif is_naive(timestamp):\n\t\treturn iso8601_from_naive(timestamp)\n\treturn datetime.datetime"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\ndef parse_date(date):\n\t\"\"\"\n\tParses a date and returns a :py:class:`datetime.date`.\n\t\n\t:param date:\n\t    The :py:class:`datetime.date` object from a string.\n\t\n\t:return:\n\t    Returns an :py:class:`datetime.date` object. If no date is \n\t  present, then returns ``None``.\n\t\"\"\"\n\t\n\tif date is not None:\n\t\treturn make_aware(date)\n\telse:\n\t\treturn None\n\ndef parse_time(t):\n\t\"\"\"\n\tParses a time string"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n    return default_tzinfo(dt, tzinfo).replace(tzinfo=None)\n \ndef default_date_format(d):\n    \"\"\"\n    A :py:class:`datetime.date` class decorator which formats ``YYYY-MM-DD`` \n    dates into a format suitable for *dump()*.\n    \n    If no value is given for *d*, a built-in :py:class:`datetime.date` is used.\n    \n    Note that if the strings in the strings are identical then datetime values and\n    datetimes (which are also immutable) *are* to be included as strings, then any\n    individual items of the list"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    \n\t    if dt.tzinfo is None:\n\t\treturn dt\n\t    else:\n\t\treturn tzinfo.localize(dt)\n\n\t    \n    def datetimeToString(dt=None, dt_format=\"%a %b %d %H:%M:%S %Y\"):\n\t\"\"\"\n\tReturns the specified datetime as a string that can be used for\n\tformatting dates (which also accepted by ``datetime.strftime()``).\n\tIf ``dt_format`` is None, the current date and time is encoded with\n\tthe current locale.\n\tThis function will return ``None``. A full conversion is required.\n\tTo"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\tif dt is None:\n\t\treturn DEFAULT_STRICT\n\t\n\telif isinstance(tzinfo, int):\n\t\ttzinfo = tzinfo_class(tzinfo)\n\t\n\telif isinstance(tzinfo, tz.tzinfo):\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\nfrom django.conf import settings\nfrom django.utils import translation\nfrom django.core.exceptions import ImproperlyConfigured\n\nfrom zerver.models import get_client, get_user_profile_by_id\n\n\n# These user IDs are used by the main Tornado server to denote when\n# settings"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\t# If there's nowhere after this then this would be a\n    # good idea to pass a tzinfo to \"tzoffset\" / \"tzstr\" / \"tzical\" time zone name\n    _tzinfo = tzinfo if isinstance(tzinfo, str) else AmbiguousTimeError\n\t\n\tif not hasattr(_tzinfo, 'zone'):\n\t    _tzinfo.zone = DEFAULT_ZONE\n\t\n\treturn _tzinfo.zone()\n\n\ndef parse_tzinfo(tzinfo_str):\n\t\"\"\"\n\tParse a time zone string, returning a :py:class:`datetime.tzinfo` object.\n\t\n\t:param tzinfo_"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\t# Set the tzinfo to UTC\n\tif tzinfo is not None:\n\t\tmake_aware = lambda ts: timezone.make_aware(dt, tzinfo)\n\telse: \n\t\tmake_aware = lambda ts: datetime.datetime.fromtimestamp(ts)\n\n\treturn make_aware(datetime.datetime)\n\n\ndef _guess_datetime_format(datetime_format, allowed_formats):\n\t\"\"\"\n\tHandles special cases of format conversions with the following rules:\n\t\n\t\n     * ``{_dayfirst}``: 1st, January, 1970\n     * ``{_dayfirst_alpha"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t # Strip off beginning and end characters.\n\t if before.is_eof or after.is_eof:\n\t\t\treturn\n\n\t\t# Split the file into two sets.\n\t\tself.source_slice = (before, after)\n\t\tself.start, self.end = self.split(0, 2)\n\t\tself.pos = self.start + len(self.source_slice[2])\n\t\tself.file = self.source_slice[0][0]\n\t\tself.pos += len(self.source_slice[2])\n\n\t\t# The offset after splitting starts.\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t # Make sure last byte is always '\\n'\n\t if not self.text:\n\t\t\traise ValueError('No file specified for the \"before\" argument.')\n\t if before[-1:]!= '\\n':\n\t\t\traise ValueError('No \"before\" argument specified.')\n\t else:\n\t\t\tself.text = self.text + '\\n'\n\t\tbefore = before[1:]\n\t\tafter = after[1:]\n\tif not text:\n\t\traise ValueError('No file specified for the \"before\" argument.')\n\tif len(before) < len(after):\n\t\traise ValueError('No newline between the"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\nclass CutTokeniser:\n\n\tdef __init__(self):\n\t\tself.clear()\n\n\tdef clear(self) -> None:\n\t\t\"\"\"\n\t\tRemove leading data from line number code.\t\n\t\t\"\"\"\n\t\tself.line_number_start = 0\n\t\tself.line_number_end = 0\n\t\tself.line_number_start_line = None\n\t\tself.line_number_end_line = 0\n\t\tself.line_end_index = 0\n\t\tself.last_newline_index = 0\n\t\n\tdef line_number_start(self"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t gl.cut_chars = True\n\t self.call_on_change(\"cut-characters\")\n\t\t\nclass CutLinesCache:\n\t\"\"\"\n\tCache of cut lines. Doesn't modify it's state to allow \n\tintervening text being inserted at the end of a line with\n\tpreviously appended data.\n\t\"\"\"\n\n\tdef __init__(self) -> None:\n\t\tself.cache = {}\n\n\tdef add(self, string: str, length: int = None) -> None:\n\t\t\"\"\"\n\t\tStore a cut line.\n\n\t\tArgs:\n\t\t    string: The"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\t\n\tself.clear_all()\n\t\n\tbefore_str, after_str = self._separate_line_sep(before, after)\n\t\n\ttext = before_str\n\ttext_without_comment_str = remove_comment_text(text)\n\t\n\twith open(text_without_comment_str) as f:\n\t\tdata_to_cut = f.read().replace('\\n', '')\n\twith open(text_without_comment_str, 'w') as f:\n\t\tf.write(data_to_cut)\n\t    \n\t\t\t\t\n\t# Remove double space"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t self.cut_chars = bytearray(before.replace(b\"\\n\", b\"\\r\") + after.replace(b\"\\n\", b\"\\r\")).decode(\"utf-8\")\n\n\nif __name__ == \"__main__\":\n\n    import datetime\n\n    def timestamp(original: bytes) -> str:\n        offset = 0x8000000\n\n        if original:\n            # The final day, i.e. 1/1/2011 at 00:00:00\n            #\n            # Saturday, midnight, 4/5/2015 at 27:00:00\n            #\n            # We need to deal with the"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\t\n\tself._text += before.encode('ascii', 'backslashreplace').decode('ascii')\n\tself._text += after.encode('ascii', 'backslashreplace').decode('ascii')\n\n#\n# Parse the whole text.\n#\n\nif __name__ == '__main__':\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description=\"Extracts text from CD-ROM archives.\")\n\tparser.add_argument('input', nargs=1, help=\"C-style archive file.\")\n\tparser.add_argument('output', nargs=1, help=\"C-style output"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t    \n\t    if before[:1]!= after[:1]:\n\t\traise ValueError(\"Invalid begin and/or end of the line, expected both bytes\")\n\t    self.begin_delimiter = before[:1]\n\t    self.end_delimiter = after[:1]\n\n\tdef _get_string(self) -> str:\n\t    # Get the string split from the string data\n\t    index = self.offset - self.offset % self.chars\n\t    end_index = self.index + self.length + len(self.string)*1\n    \t\n\t    while index < end_index and self.chars[self.index - end"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n#\t\tif before.find('/*')!= -1:\n#\t\t# if'' not in before:\n#\t# \t\tbefore +=''\n#\t\tnew_after = b''\n#\t\twhile True:\n#\t\t\tnew_after = after\n#\t\t\tidx = next_idx(after, 0, len(after)) + 1\n#\t\t\tif b'\\n' == after[idx]:\n#\t\t\t\tstart_idx = idx + 1\n#\t\t\t\tend_idx = (idx + 1), next_idx(after, idx +"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\tfile_len = str_file_len(before)\n\t\tif self.path_separator_len!= None:\n\t\t\tafter = self.path_separator_len * 2 + after\n\n\t\twith open_file_as_stream_or_iterable(\n\t\t\t\tbefore, newline='',\n\t\t\t\tencoding=self.encoding, errors='replace',\n\t\t\t\tmode='r', encoding_errors='strict') as fp:\n\t\t\twith open_file_as_stream_or_iterable(\n\t\t\t\tafter, newline='',\n\t\t\t\tencoding=self.encoding, errors='replace') as fp2:\n\t\t\t\t"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\n\t# Get request.\n\ttry:\n\t\t#print(f\"Processing \" + request)\n\t\tif request.check_message(request.message, True):\n\t\t\t#print(f\"Request is Message Message. Message is valid.\")\n\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\t#print(f\"Request is Message Message. Message is an API request.\")\n\t\t\t\tpayload = request.message.payload\n\t\t\t\t#print(f\"Payload is {payload}\")\n\t\t\t\tif \"content\" in payload.keys():\n\t\t\t\t\t#print(f\"Payload has content {payload['content"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn request.get_host().split(':')[1] == settings.DIASPORA_PUBLIC\n\texcept ValueError:\n\t\treturn False\n\nclass RequestInfo:\n\t\"\"\"\n\tRequest information from the request body.\n\t\n\tUsed by NgWeb's `<RequestInfo>`.\n\t\"\"\"\n\tdef __init__(self, method, name, args, kwargs):\n\t\t# args will be set by the template loader\n\t\tself.args = _convert_args(args)\n\t\tself.method = method\n\t\tself.name = self.name.rstrip()"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t\n\tif request.match(request.method, \"POST\"):\n\t\tif request.headers.get(\"X-Appium-Request-Version\") == \"1.0\":\n\t\t\treturn True\n\t\treturn False\n\t\n\treturn False\n\ndef identify_response(response: ResponseType):\n\t\"\"\"\n\tAttempt to identify whether this is a Diaspora response.\n\t\n\tReturn boolean or None based on the response status code.\n\tThe check when the response was valid.\n\t\"\"\"\n\tif response.getcode() == 503:\n\t\treturn True\n\telif response.getcode"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\n\tfor key, value in request.args.items():\n\t\tif key == \"user\":\n\t\t\tif value is not None:\n\t\t\t\treturn False\n\t\t\treturn True\n\n\treturn True\n\n\ndef get_message(request: RequestType):\n\t\"\"\"\n\tGet the message for a request. The request type is passed in as an argument to the parser.\n\t\n\tIf we get a proper message from the request, we return the message.\n\t\"\"\"\n\tmessage = request.get_message()\n\n\tif message is None:\n\t\treturn \"\"\n\n\tif \"user\" in message and"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t\n\t# If we can get an authentication token, try to see if the key is from the \"WWW-Authenticate\" header.\n\tauth_token = request.META[\"WWW-Authenticate\"]\n\ttry:\n\t\t# The authentication token is provided in a cookie, so use that to do the check.\n\t\t# If the request cannot be successfully authenticated, then the user is an admin request.\n\t\tauth.get_user(token=token)\n\texcept: # If we get an auth token, then we check if all the data we need.\n\t\n\t\ttoken, sep, cookie = auth_"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t \n\tif \"payload\" in request.GET and request.GET[\"action\"] == \"submit\":\n\t\t# Attempt for a 'preview' for a'submit'\n\t\tif request.method == \"POST\" and request.POST[\"action\"] == \"verify\": \n\t\t\t# We require a 'preview' with an assertion and then we try again, to avoid any leaks.\n\t\t\tmessages.error(request, _(\"Please submit data before asking again.\"))\n\t\t\treturn HttpResponse(\"Your answers entered were wrong. Here are the answers you just submitted: {}\".format(request.GET[\"payload\"]))\n\t\treturn"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n#\t\ttry:\n#\t\t\tif not is_public_msg(request, BADMESSAGE):\n#\t\t\t\thandle_error(request, APIKEY_INVALID_BEACON_DATA, BADMESSAGE)\n#\t\t\tif not is_public_msg(request, BADMESSAGE2):\n#\t\t\t\thandle_error(request, APIKEY_INVALID_BEACON_DATA, BADMESSAGE2)\n#\t\t\tif not is_public_msg(request, BADMESSAGE3):\n#\t\t\t\thandle_error(request, APIKEY_INVALID_"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\n\t# Check that current is a legacy payload, then look for a matching legacy version.\n\tddes_oldversion = get_bliauth_ddes_oldversion(type)(request_json_params_dict['data'])\n\n\toldversion_v = None\n\ttry:\n\t\toldversion_v = get_bliauth_oldversion[\"user\"][p2p_api.bli_user_id(request_json_params_dict['data']['user'])['data'][0]['data']['version']['number']\n\t\n\texcept:\n\t\t# Old CBRL"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\n\ndef is_legacy_request(request: RequestType, message: RawMessage):\n    \"\"\"\n    If this matches with the message we can assume it is a Diaspora request and return to the request.\n    Also check for the \"is_legacy\" message if it is.\n    \"\"\"\n\n    if is_request_legacy(request):\n        if message.is_native_message():\n            return True\n\n        try:\n            message.check_is_legacy_message(message.get('body'), request.get('headers'))\n        except:\n            logger.warning(\"Skipping message '{}' is not a legacy or legacy"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t\n\t## Check if it has a payload key.\n\t# If it is, request a public message.\n\tif request.type == RequestType.MESSAGE.value:\n\t\ttry:\n\t\t\trequest = PublicMessage.objects.get(payload_key__isnull=True)\n\t\t\treturn request\n\t\texcept PublicMessage.DoesNotExist:\n\t\t\tpass\n\n\t## Check if it has a payload value.\n\t# If it has, request a private message.\n\telif request.type == RequestType.MESSAGE.value:\n\t\ttry:\n\t\t\treturn UserMessage.objects.filter(recipient="}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t # If we have a matrix, we need to parse. This can be optimized later\n\t if \"Matrix\" in request.form and request.form[\"Matrix\"] == \"true\":\n\t\t\tif \"name\" in request.form:\n\t\t\t\tif \"matrix_type\" in request.form:\n\t\t\t\t\tif request.form[\"matrix_type\"].lower() == \"single\":\n\t\t\t\t\t\tif request.form[\"matrix_type\"].lower() == \"all\":\n\t\t\t\t\t\t\treq = request.form[\"matrix_type\"].lower() + request.form[\"matrix_type\"].lower()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\treq ="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n#\t\t# If it is a matrix request, then convert back to string\n#\t\tif isinstance(request, MatrixRequest):\n#\t\t\treturn request.name.decode() == request_name or request == request.value\n#\t\telse:\n#\t\t\t# Shouldn't occur?\n#\t\t\traise NotImplementedError(\n#\t\t\t\t\t'Request not of required type '\n#\t\t\t\t\t'for matrices'\n#\t\t\t\t)\n\n\t# If not, return the name of the request (which could be empty...)\n\tif isinstance(request, MatrixRequest):\n\t\treturn"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\tis_mat = (request.requestType == \"GET_MAT\")\n#\t\tif is_mat:\n#\t\t\tif request.args[\"mode\"] == \"READ\":\n#\t\t\t\tis_mat = (request.args.get(\"mat\", \"True\") == \"true\")\n\n\t\t# This is for getting the raw view of the request. We may be able to detect if\n\t\t# a request is a matrix request.\n\t\tif is_mat:\n\t\t\tmatrix_name_in_request = \"GET_MATRIX\" + request.args.get(\"mat\", \"\").lower"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n#!!! TODO:!!!!!!!\n#!!! \n#!!!!!!!!!!\ndef check_request(a:Matrix) -> matrix:\n\tassert isinstance(a, MatrixRequestContext)\n\treturn a.get_request(MatrixRequestType.REQUEST) == a.get_request(MatrixRequestType.REQUEST)\n\ndef get_request(a:MatrixRequestType) -> Matrix[MatrixRequestRequestType]:\n\tassert isinstance(a, MatrixRequestContext)\n\t\n\tif  len(a.get_request(MatrixRequestType.REQUEST)) == 0:\n\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n\t\n\treturn request.form.get('status', None) == '1'#!/usr/bin/env python\n#\n# Copyright (C) 2007 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\trequest.determine_permission()\n\t\treturn True\n\n\tdef get_redirect_url(request: RequestType) -> str:\n\t\tredirect_page_url = reverse('webpages:redirect', args=[request.slug])\n\t\treturn redirect_page_url\n\n\tdef check_permissions(request, obj) -> bool:\n\t\treturn obj.is_staff\n\n\tdef check_object_permissions(request, obj) -> bool:\n\t\treturn obj.is_superuser\n\n\tclass Meta:\n\t\tpass\n\n\ndef build_email_message_context(request: HttpRequest, message:"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n\t\n\t# if is type, we assume it's a plain list\n\tif isinstance(request, list) if type(request) else request is None:\n\t\treturn False\n\telse:\n\t\t# if type then it's a matrix request\n\t\treturn (type(request) in PRIMITIVE_TYPES)\n\ndef detect_request(request: RequestType) -> bool:\n\t\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n\t# if is type, we assume it's a plain list\n\tif isinstance(request, list) if type(request) else"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\tif (isinstance(request, MatrixRequest)) and \\\n\t\t   (hasattr(request, 'is_matrix') and request.is_matrix) or \\\n\t\t   (hasattr(request, 'is_matrix') and request.is_dense) and \\\n\t\t   request.is_matrix:\n\t\t\t# this matrix is already loaded, simply return it\n\t\t\treturn True\n\t\treturn False\n\t# this function will check whether the matrix has been loaded\n\t# because it can have a matrix_request\n\tif hasattr(request,'matrix_request'):\n\t\treturn is_matrix(request.matrix"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn True\n\n\nclass MatrixItem:\n\t# the parent of the Matrix, this item holds a parent matrix for use by the MatrixItem\n\t_item: Optional[MatrixItem] = None\n\n\t@staticmethod\n\tdef _get_child_for_matrix_item(object_type, item_type) -> Optional[MatrixItem]:\n\t\t\"\"\" Returns the child matrix item that a specific MatrixItem should represent\n\t\t\"\"\"\n\t\tif isinstance(obj, MatrixItem):\n\t\t\tif item_type and item_type in obj._item:\n\t\t\t\treturn obj._item[item_type]"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn request.client.get_param(\"identify_request\")\n\treturn False\n\n@register_request(None, str)\ndef identify_request_str(request: RequestType) -> str:\n\t\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn request.client.get_param(\"identify_request_str\")\n\n@register_request_type()\ndef identify_request_type(request: RequestType) -> BoolType:\n\t\"\"\"\n\tTry to identify whether this is a Matrix request\n\n\tReturns:\n\t\tTrue -- '"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\tif not hasattr(dt, \"__format__\"):\n\t\t\tdt.__format__ = \"dt\"\n\t\treturn dt.__format__\n\telif isinstance(dt, datetime.datetime):\n\t\treturn format_dt(dt.isoformat() + \"Z\")\n\telse:\n\t\treturn \"\"\n\ndef format_duration(duration):\n\tout = \"\"\n\tif frappe.local.flags.in_test:\n\t\tout = frappe.render_template(\"duration_header\",\n\t\t\t{\"duration\": duration*1000}, {\n\t\t\t\t\"is_current_day\": True,\n\t\t\t\t\"time"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_dts(dts, dt):\n\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef make_dt(dt):\n\t\"\"\"\n\tCreate a datetime from a date, datetime in the way that D* nodes expects.\n\t\"\"\"\n\tdate = dts.strftime('%d-%b-%Y')\n\tdate = date.strip()\n\t\n\tdt = dbobjects.base.Datetime(date[0:4]) \n\t\n\treturn dt\n\n# Dates\n# ----------------------------------------------------------------------------\n\ndef validate_date("}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.replace(tzinfo=None)\n\ndef format_d(dt):\n\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n__all__ = [\n\t\"time_to_datetime\",\n\t\"datetime_to_time\",\n\t\"datetime_to_rfc_822_time\",\n\t\"format_dt\",\n\t\"format_d\"\n]\nimport unittest, random, sys, time\nsys.path.extend"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\n\t\n\tdt_formats = [\"%Y-%m-%d %H:%M:%S\",\n\t\t\t     \"%Y-%m-%d\",\n\t\t\t     \"%m/%d/%Y %H:%M:%S\",\n\t\t\t     \"%m/%d/%y %H:%M:%S\",\n\t\t\t     \"%m/%d/%y %H:%M\"]\n\t\t\n\tfor fmt in dt_formats:\n\t\tparts = dt.split(\" \")\n\t\tdttime = parts[1]\n\t\tif len(parts) > 2:\n\t\t\t\n\t\tnow = datetime.datetime.now()\n\t\t\n\t\tif dttime == \"now\":"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\n# The following classes define the node types in the source code below.\nclass Node:\n\t\"Node class to hold the information about a node.\"\n\tdef __str__(self):\n\t\treturn self.name\n\nclass Edge:\n\t\"Edge class.\"\n\tdef __init__(self,source,target):\n\t\t\"Edge constructor\"\n\t\tself.source = source\n\t\tself.target = target\n\t\n\t\tself.incoming = []\n\t\tself.outgoing =[]\n\t\t\n\tdef __str__(self):\n\t\tprint self.name\n\t\tprint self.source.name\n\t\tprint"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t    \n\t# make sure the duration and date in format_dt are convertible to a string\n\tseconds = dt.hour*3600 + dt.minute*60 + dt.second\t\n\tparts = dt.strftime(\"%I%p\")\n\tmilliseconds = int(parts)\n\tmillisecond = dt.strftime(\"%H%M%S\")\n\tseconds += 1000000000*(millisecond-millisecond)/1000\t\n\thours = int(parts)\n\tminutes = int((dt.hour)*60)\n\tseconds += 1000000000*(milliseconds-millisecond)/3600\n\t\n\tseconds"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_t(t):\n\t\"\"\"\n\tFormat a time in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_string(s):\n\t\"\"\"\n\tFormat a Python string in the way that D* nodes expect.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tfrom sys import argv\n\t\n\tif len(argv) >= 2:\n\t\ttry:\n\t\t\tf = open(argv[1], 'r')\n\t\texcept IOError:\n\t\t\tprint 'Error in input file. \\\n\t\t\t\tUsage: node_parser.py [-s <"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t \n\tif dt.tzinfo:\n\t\tdt = dt.astimezone(tz=timezone)\n\treturn dt.strftime('%Y-%m-%d %I:%M%p')\n\n@task(default=True)\ndef get_task_details(id):\n\t\"\"\"Creates a dictionary with metadata about the task details.\"\"\"\n\tlog.info('Retrieving task details for task %s' % id)\n\t\n\ttry:\n\t\tr = requests.get(\"http://localhost:8888\", headers={\"User-Agent\": \"Doriano\", \"X-Token\": os.environ[\""}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_date(date):\n\t\"\"\"\n\tFormat a date in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_h(val):\n\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_t(val):\n\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_r(val):\n\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\ndef format_s(val):\n\t\"\"\"\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\n\n\tfrom collections import defaultdict\n\treturn \"T\"\t\t\n\n\n\n\ndef get_name(elem):\n\t\"\"\"\n\tGet a name node\n\n\t\"\"\"\n\t\n\tdef get_text(elem):\n\n\t\tt = elem.getAttribute(\"type\") # get type of first Element object\n\t\tn = None\n\t\tif t: # try to figure out type\n\t\t\tn = elem.getAttribute(\"name\") #try to get the name out of type\n\t\tif n:\n\t\t\treturn n\n\t\tif elem.parentNode == None: # get this particular Node\n\t\t\treturn elem.nodeName\n\t\telse:"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\t# Regular expression used to find the tags. Ignore leading chars where the text is whitespace. This is safe because a space has no spaces in a row or column.\n\tdef find_tags_regex(text: str, tags: Iterable[str]) -> Optional[Set[str]]:\n\t\tmatch: re.compile(r\"\"\"\n\t\t\t\t^(?:\t(?:\t\t(?P<type_tag> [^<!%-]{0,2}) # Tag keywords start with a letter\n\t\t\t\t\t(?:\t(?:\t\t(?P<argname>(?:(?:\t\t\\[[^]]*?(?:,?)?"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n#\tdef find_tags1(text, replacer: Callable = None) -> str:\n#\t\t\"\"\"\n#\t\tReturn a set of all whitespace sequences (including whitespace, including puncutation, comments\n#\t\tand carriage returns.\n#\t\t\"\"\"\n#\t\tresult = list()\n#\t\tfor c in text:\n#\t\tresult.extend(re.split(r\"[\\s]+\", c))\n#\t\treturn result\t\n\t#\tresult = \"\"\n\t#\tfor c in text:\n#\t\tresult.append"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n#\tRemove all non-whitespace/newlines and tabs from the beginning of the string\n#\twith the code block.\n\ttxt = text.lstrip(\"\\n\")\n\n\t# Iterate through the code block and identify lines\n\tdef find_tags_and_replacer(match:re.Regex, replacer:str):\n\n\t\t# If there is any whitespace or line break at the end of the line, there's\n\t\t# nothing to remove.\n\t\ttry:\n\t\t\tcomment = replacer(txt[-1])\n\t\texcept TypeError:\n\t\t\t# no match; so look"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\ttags = set()\n\tif text.startswith('@'):\n\t\ttags.add(text[1:])\n\telif '{' in text:\n\t\ttags.add(text[:text.find('{'), text.find('}') : text.find('}')+2])\n\telse:\n\t\ttags.add(text)\n\t\n\tif replacer == None:\n\t\tresult = []\n\telse:\n\t\tresult = re.finditer(replacer, text)\n\t\tfor m in result:\n\t\t\ttags = tags.union(set(m.groups()))\n\treturn tags, text\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\t# Replace each tag with the corresponding replacer function\n\treplacer = replacer or lambda x: x \n\t\n\ttext = replacer(re.sub(r\"(\\s+?)(\\d+)\", r\"<i>\\1</i>\", text))\n\t\n\t# Replace each of the tags with a list of text tags (e.g. if the regex matches the\n\t# text of the first word of the original string), or append them together,\n\t# if they match, the result will have the original string, so they are appended to\n\t# the result list as"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\ttags = []\n\twith replacer(text) as match:\n\t\tfor match in re.finditer(TAGS_PATTERN, text, re.MULTILINE):\n\t\t\ttags.append(match.group(1))\n\t\t\n\t\tmatches = list(match for match in re.finditer(TAGS_PATTERNS_PATTERN, text, re.DOTALL))\n\t\t\n\t\tif not matches:\n\t\t\tlog.debug('No tags found')\n\t\t\treturn set(), ''\n\t\t\n\t\tfor i, match in enumerate(matches):\n\t\t\ttags.append((match.start(1), match.group(2)))\n\t\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\n\tstart = text.find(\"<a\")\n\tif start >= 0:\n\t\tstart += 2\n\t\tend = text.find(\"</b>\", start)\n\t\ttext = text[start:end]\n\t\tstart = text.find(\": \")\n\t\tif start >= 0:\n\t\t\tstart += 1\n\n\ttag_list = []\n\n\ttag_list.append((_is_tag(text, \"<b>\", True), \"f\"))\n\t\n\tmatches = _re_tag_start.finditer(text, start + 1)\n\tmatches = list(matches)\n\tfor match in matches"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\ttags: List[str]\n\ttext_replacements: List[str]\n\t\n\t# TODO: Allow to check for the first character of text, e.g. \"class C(object):\\nreturn C()\\n\"\n\tif isinstance(text, str): \n\t\ttext = text[1:]\n\t\n\tfor replacer in replacer:\n\t\t\n\t\t# TODO: Optimize for text content\n\n\t\ttags = find_tags(text, replacer)\n\t\t\n\t\ttext = re.sub(r\"^([A-Z]{2,4})\\s*(.*)\", \"\", text)"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\ttagset = set()\n\n\t# For text -> text -> tagset\n\ttry:\n\t\ttext = text.split()\n\texcept AttributeError:\n\t\treturn tagset, text\n\t\n\t\n\t# For other tags, find tags in text\n\tfor c in text:\n\t\ttag = find_tags(c, replacer, 1)\n\t\tif tag:\n\t\t\ttagset.add(tag)\n\t\n\t\n\treturn tagset, text\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\tif replacer is None:\n\t\ttag_pattern = re.compile(r\"(\\b[A-Za-z][A-Za-z0-9]*)(\\w{6,})\\s*\")\n\t\ttags = set()\n\t\tmatches = tag_pattern.finditer(text)\n\t\tfor match in matches:\n\t\t\toffset = len(match.group(1))\n\t\t\tmatched_tag = match.group(1)[offset + 1:-offset]\n\t\t\tvalue = match.group(2)[offset + 1:-offset]\n\t\t\ttags |= set([matched_tag"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\twhile text:\n\t\ttext = text[-1]\t\n\n\n\t\tmatch = re.search('href=\"(http:|/wiki/)(?P<domain>', text)\n\t\tif not match:\n\t\t\tprint \"Warning! couldn't find the domain for '%s' in '%s'\" % (text, text)\n\t\telse:\n\t\t\thref = match.group(\"domain\").strip(\"/wiki/\")\n\n\t\t\tfor (domain, title) in pparser.parse_dom_list(href, recursive=False):\n\t\t\t\ttext = text.replace(title, domain)\n\n\t\ttext ="}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t\n\tfor link in text.find_all([]):\n\t\t\n\t\tif \"http://\" in link.get(\"href\", \"\"):\n\t\t\t\n\t\t\tlink[\"target\"] = link[\"href\"]\n\t\t\tlink[\"type\"] = \"text/plain\"\n\t\t\t\n\t\t\t\n\t\t\t# strip links in text\n\t\t\ttext = text.replace(link[\"href\"], \"\", 1)\n\t\t\t\n\t\t\t# break up links\n\t\t\ttext = text.replace(link[\"href\"][0:20], \":\", 1)\n\t\t\tlink[\"href\"] = \"/\".join([\"http://redd.it/\", link[\"href\"].replace(\"/\", \"_\")])\n\t\t\t\n\t\t\n\t\t#"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t\n\t# find all links on text\n\ttext_links = []\n\tfor index, link in enumerate(text):\n\t\ttext_links.append(\"\"\"<{id}/>\"\"\".format(id=index))\n\n\t\t\n\t# process each link\n\tfor index, link in enumerate(text):\n\t\t# check if this link has been processed\n\t\tif '#' in link:\n\t\t\t# get rid of whitespaces on start and end\n\t\t\ttail1 = link.rsplit('#', 1) \n\t\t\ttail2 = text.split('#', 1)\n\t\t\ttext_links.append(\"\"\"<{id"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\ndef process_links(links):\n\t\"\"\" \n\tExtracts and processes a link from text, using the Text Markup Language.\n\t\"\"\"\n\tfor link in links:\n\t\tlink_text  = link['text']\n\t\tresult  = \"\" \n\n\t\tresult = link['link'].replace(\"\\n\", \" \")\n\t\tif link_text: result += \"[\" + result + \"]\"\n\t\tif len(link['text']) > len(result): result = link['text']\n\n\t\tlinks[link['text']] = str(result)\n\ndef process_links_and_clean"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t\n\t# remove external links, so that we can easily update the link's\n\t# parent. We also remove internal URL schemes from the text (if the text has\n\t# no encoding, like for example), but not other markup.\n\tfor match in re.finditer(r'(<a.+>)([^<]+?)\\([^\\)]+)(?:\\(([^\\)]+)\\)',\n\t\t   text, re.UNICODE):\n\t\ttry:\n\t\t\tlink, _external = match.groups()\n\t\t\tif _external:\n\t\t\t\tlink.set('href', link.get('href').strip('://?'))\n\t\t\t\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t \n\tfor i, (start, end) in enumerate(common_link_boundaries):\n\t\tif i in text:\n\t\t\tstart, end = text[i], text[end]\n\t\tattr_list = common_attribute_split.findall(text[i][start:end])\n\t\tif len(attr_list) > 0:\n\t\t\tattr_pair = (attr_list[0], parse_link(attr_list[0]))\n\t\t\tattr_list[0] = (attr_pair[0], attr_pair[1], attr_pair[2])\n\t\telif"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n    def process_tail(text):\n        \"\"\"\n        Processes html entities.  For links without trailing space, also\n        stripping whitespace from inside an inline-tag to enclose entity refs (href).\n        \"\"\"\n        if text:\n            if text.startswith('<body>') or text.startswith('</body>'):\n                if text.endswith('</body>'):\n                    entity['text'] += text[len('</body>'):]\n                else:\n                    entity['text'] += text\n            else:\n                entities.append({'start': text[1:].strip(), 'end' : text.strip()})\n    \n    entity['text'] = remove"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t \n\tfrom text_util, text_diff\n\n\t_parse_text = text_diff.getDiff(text)\n\n\tif _parse_text is None:\n\t\treturn\n\n\tfor match in _parse_text.finditer(\"link\\{([^\\}]*)\\}\", _parse_text.lines):\n\t\ttag, attrs, link, tail = match.groups()\n\t\ttag = tag in HTML_OPENING_TAGS\n\t\tattrs = dict(attrs)\n\t\tattrs.update(attrs_copy) # Copy attributes of elements\n\t\tif _parse_text.head_attrs.get"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t                \n\tdef replace_entities_with_local_links(text):\n\t\t\"\"\"Replace entities with local links.\"\"\"\n\t\tfor entity in find_entities(text, entity=entity_name_link):\n\t\t\t# Replace links with local links within this link\n\t\t\tlink = linkify_entity_with_local_links(entity)\n\t\t\ttext = text.replace_links_with(link)\n\t\t\n\t\ttext = text.replace_external_links(entity_url_link)\n\t\treturn text\n\n\t# process links inside text and links in text with links\n\ttext = process_text"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\t# Linkifying links\n\tcontent = text.replace('\u00b6', 'a')\n\n\t# Remove links\n\ttext = re.sub(r'<link.*?(?=\\W)>', '\\\\1', text)\n\n\t# Replace text and links, removing any unnecessary links\n\tfor link in IGNORE_LINKS:\n\t\ttext = text.replace(link, '')\n\n\t# Add links\n\tif 'links' in IGNORE_LINKS:\n\t\ttext = replace_link_tags(text, IGNORE_LINKS)\n\n\t# Replace link and image links in text"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\tremote_url = urllib_parse.urlparse(url).netloc\n\t\tif remote_url!= 'github.com':\n\t\t\t# Handle the case where the request is for a github user\n\t\t\treturn 'git'\n\t\tremote_url = urllib_parse.urlparse(url).path\n\t\tif remote_url.startswith('blob:'):\n\t\t\treturn 'blob'\n\t\telif not remote_url.startswith('https://'):\n\t\t\traise ValueError('Unsupported remote URL')\n\t\treturn 'git'\n#\n# Copyright (c) 2013 Big Switch Networks, Inc.\n#\n# Licensed under"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\toutput = False\n\t\tcontent_type = mimetypes.guess_type(url)[0]\n\t\tif content_type is None:\n\t\t\tlog.error('File \"%s\" not found in the package at \"%s\".\\n' % (\n\t\t\t\turl, package_path))\n\n\t\treturn content_type\n\n\tlog.debug('Fetching content type of file at \"%s\".\\n' % url)\n\n\tcontent_type = fetch_from_url(url)\n\n\treturn content_type\n\n\ndef fetch_from_url(url: str) -> str:\n\tif"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\n# \t# Read the HTTP response\n# \ttry:\n# \t\tresp = requests.get(url, timeout=None)\n# \t\tcontent_type = resp.headers[\"content-type\"]\n# # \texcept:\n# \t\treturn None\n\t# \treturn content_type\n\tif resp is None:\n\t\t# \traise Exception(\"Failed to fetch the content-type from the remote url\")\n\t\tprint(\"Failed to fetch the content-type from the remote url\")\n\t\treturn None\n\t\n\t# Attempt to find out the content type encoding ("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\tif not url: return None\n\n\t\twith closing(urlopen(url)) as http_file:\n\t\t\tbuffer = http_file.read()\n\t\treturn content_type_from_buffer(buffer, default='application/octet-stream')\n\ngithub_client_id = extract_github_client_id(url)\n\ndef is_github_auth_url(url: str) -> bool:\n\t\"\"\"\n\tReturn whether the remote url is a GitHub repo URL and not a Github login.\n\n\tGitHub URL URLs need to have a trailing slash so we check for this automatically"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\twith open(os.path.join(BASE_DIR,'static/websites/site_content_type.json'), 'r') as fileobject:\n\t\t\tdict_ = fileobject.read()\n\t\t\tcontent_type_dict = ast.literal_eval(dict_)\n\t\t\tif content_type_dict is not None:\n\t\t\t\treturn content_type_dict['content_type']\n\t\t\telse:\n\t\t\t\treturn None\n\treturn None\n#!/usr/bin/env python2\n#\n# Copyright (c) 2010,2011 Phil Christensen\n#\n#"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\ndef set_version(file: File, version: str) -> None:\n\t\"\"\"\n\tSet the version of the file.\n\n\t:param file: File to set version.\n\t:param version: Version that the file should have.\n\t\"\"\"\n\tcontent_type, encoding = mimetypes.guess_type(file.name)\n\tif content_type is None:\n\t\traise OSError(errno.ENOENT, \"File not found\")\n\t\t\n\turl = urlparse(url)\n\t\n\tif int(version) <= 3:\n\t\tversion = tuple(map(int, version.split"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t http_client = HttpResponse(\n\t\tjson.dumps({\n\t\t\t'error': False,\n\t\t\t'message': None,\n\t\t})\n\t)\n\tr = requests.head(url)\n\treturn extract_content_type_from_r(r)\ndef fetch_content_type_from_r(r: HttpResponse) -> Optional[str]:\n\thttp_client = HttpResponse(\n\t\tjson.dumps({\n\t\t\t'type': r.headers.get('content-type', '')\n\t\t})\n\t)\n\ttry:\n\t\traw_content_type = r."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\n\n\ttry:\n\t\treturn requests.head(url, auth=(config[\"http_login\"], config[\"http_password\"]), verify=config[\"http_verify\"])\n\texcept Exception as e:\n\t\tprint(repr(e))\n\n\ndef fetch_link(origin: BaseLocation, resource: BaseResource, resource_type: ResourceType, destination: BaseFileDestination):\n\t\"\"\"\n\tFetch the remote link to retrieve the content of the remote resource.\n\tThe input file can be downloaded from the URL.\n\tThe type of the remote resource.\n\tTo download it back, just use"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\ndef path_join(url: str, path: str) -> str:\n\t\"\"\"\n\tJoin a remote url with multiple segments and escape the remaining\n\tsegments from the joined path.\n\t\"\"\"\n\tparts = list(urllib.parse.urlsplit(url))\n\treturn urllib.parse.urlunsplit(parts)\n\ndef make_dir(dir_path: str) -> None:\n\t\"\"\"\n\tMake an empty directory at the given path.\n\t\"\"\"\n\tif not dir_path.endswith(\"/\"):\n\t\tdir_path += \"/\"\n\tif not os.path"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\turl = urlopen(url, timeout=5).readline().split(\";\")[0]\n\t\tif url.startswith(\"HTTP\"):\n\t\t\tcontent_type = 'application/octet-stream'\n\telse:\n\t\traise ValueError('bad url: %s' % (url,))\n\treturn content_type\n\n\ndef fetch_response_time(resp: object) -> Optional[Tuple[int, int]]:\n\t\"\"\"\n\tFetch the HTTP time that comes back from the remote server.\n\t\"\"\"\n\tif resp is None:\n\t\treturn None\n\n\treturn time.mktime(time."}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tinput_data = [\n\t\t\t\".-*@\\*$%@^@~`\",\n\t\t\t\"@\\\"#$%&()*+\\+,\\-./:;?@\\[\\\\\\]^_`{|}~%\",\n\t\t]\n\t\toutput_data = [\".\"] + ([\"\"]) * 6\n\t\ttag_list = [(w, n.replace(\"'\", \"\")[0]) for w, n in tag_data(tag)]\n\n\t\tfor word in input_data:\n\t\t\tfor i, tag in enumerate(tag_list):\n\t\t\t\tsent = []\n\t\t\t\tfor j in input"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\ndef test_badstring(str, str_type: str) -> bool:\n\t\"\"\"\n\tTest if an invalid string is returned as an error. This ensures it\n\treturns False if it's not an error, and returns True if it's a tag.\n\t\"\"\"\n\tif str_type =='string' and str!= b'\\0':\n\t\treturn False\n\tif str_type =='string' and str == u'':\n\t\treturn True\n\treturn False\n\ndef test_badnum(num: str) -> bool:\n\t\"\"\"\n\tTest if an"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\n\t\n\ti = 0\n\twhile True:\n\t\tif str(tag) in str(i):\n\t\t\treturn False\n\t\ti += 1\n\t\tif str(tag) not in str(i):\n\t\t\treturn False\n\t\ti += 1\n\t\t\n\treturn True\n\ndef is_positive(string: str) -> bool:\n\t\"\"\"\n\tTest if string is a positive integer.\n\t\n\t:param string:\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tfloat(string)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\ndef is_negative("}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t return bool(tag in tag_mapping)\n\n\ndef split_by_punct(string: str) -> list:\n\t\"\"\"\n\tSplit a string into list of punctuation characters.\n\t\"\"\"\n\treturn list(string.split(\" \"))\n\ndef split_by(word: str, tags: List[str]) -> List[str]:\n\t\"\"\"\n\tSplit word on it's separator and return list of words.\n\t\"\"\"\n\tout = []\n\tfor tag in tags:\n\t\tif tag.endswith(\" \") and tag not in BOLD_TAG_SET:\n\t\t\tcontinue"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tfor word, count in sorted(tags.items(), key=lambda x: x[1], reverse=True):\n\t\t\tif word in wordlist and count > 0:\n\t\t\t\treturn True\n\n\n\ndef parse_userconfig(text: str) -> Tuple[int, Mapping[str, str]]:\n\t# pylint: disable=no-name-in-module\n\t\"\"\"\n\tParse a user configuration block in the format\n\t(user:password)\n\twith the following keys:\n\t  user,password,password_repeat - password, password\n\t\"\"\"\n\tkey_and_val_"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\nword_list = [[], [], [], [], [], []]\n\ndef test_wrd(wrd: str) -> bool:\n\t\"\"\"\n\tTest the function that is run as a wrd in a list.\n\t\"\"\"\n\twith open(wrd) as f:\n\t\tline = f.readline()\n\t\tif not line: break\n\t\twords = line.split()\n\t\t# word count start with 0\n\t\tcount = 0\n\t\t# word length start with 1\n\t\tfor wrd_word in words:\n\t\t\tif wrd == wrd_word[0"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\n\n\ndef test_name(name: str) -> bool:\n\t\"\"\"\n\tTest that the name is a valid name.\n\t\"\"\"\n\t\n\n\ndef count_words(count: int) -> int:\n\t\"\"\"\n\tCount the number of text, tags, and punctuation characters in the given language.\n\t\"\"\"\n\treturn len(re.findall(r\"([a-z][a-z0-9_]*)\", text1)) + len(re.findall(r\"[a-z][a-z0-9_.]\", text2)) + len(re.findall("}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\n\n# -*- coding: utf-8 -*-\n\nimport os\nfrom io import PathLike, BytesIO\n\nimport numpy as np\n\nimport pytest\n\nfrom pandas.compat import PY35, StringIO, PY3\nfrom pandas import compat\n\nfrom pandas.io.excel import ExcelFile, ExcelWriter, read_excel\nfrom pandas.io.excel.comments import (\n    delimiter, escape_decimal, escape_nan, escape_strings,\n    quote_sheet, set_option, _get_default_writer, DEFAULT_ENCODING,\n    _get_xlsx_writer, register_writer,"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\n\t\n\t# test_word\n\tif word.is_text:\n\t\tif word.has_form(tag, [\"n\"]):\n\t\t\treturn word.has_form(tag, [\"n\"])\n\t\telse:\n\t\t\treturn word.has_form(tag, [\"n\"])\n\t\n\t# test_prefix\n\t# if a preposition/prefix contains a digit, ignore them:\n\telif re.match(r\"[\\+\\d]\\d\", tag, re.I):\n\t\treturn False\n\t\n\t# test_suffix\n\tif tag.endswith(\"s\"):\n\t\treturn False\n\n\t"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\treturn bool(re.search(TAG_PATTERN, tag))\n\ndef parse_tagged_str(tagged_str: str) -> ParseResult:\n\t\"\"\"\n\tIf we're not using pre-built parser, build a parser that uses\n\ttokens as tags to look for tagged words.\n\t\"\"\"\n\ttry:\n\t\treturn map(parse_tagged_str, tagged_str.split())\n\texcept ValueError:\n\t\traise ValueError('failed to parse tag: %s' % tagged_str)\n\nclass ParserData(object):\n\tdef __init__(self, words"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\n    #if not elem.tag:\n        #return elem.attrib.items()\n    #result = {}\n    #for node in elem.getchildren():\n        #key = str(node.tag) + ':' + node.attrib.items()\n        #result[key[2]] = node.attrib.get('id')\n    #return result\n    \n\ndef import_xml(filename, xmlns=\"http://soap.sforce.com/2006/04/metadata\"):\n    \"\"\"\n    Import the content of an xml file and return it as a dict.\n    \"\"\"\n\n    d = xmldict.parse(filename)\n   "}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\ndef xml_element_as_dict(elem_name,elem):\n\t\"\"\"\n\tConverts a xml element given by xml_element_as_value() back into a dict, keyed to tag name.\n\t\n\tThe first item in the list is ignored, the rest are ignored and so it is a child.\n\t\"\"\"\n\tchildren=set()\n\ttags=elem.tag_name\n\tfor i in range(0,len(elem.getchildren())):\n\t\ttry:\n\t\t\tattrs = elem_name[i].attributes\n\t\t\tif attrs.has_key('name'):\n\t\t\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\tfor child in node:\n\t\t\tif child['tag'] == 'body':\n\t\t\t\tcontinue\n\t\t\tyield child\t\t\t\n\tret = OrderedDict()\n\tret['_children'] = list()\t  # ordered list of children nodes\n\tfor child in xml_children(node):\n\t\tif child['tag'] in ['li', 'table']:\n\t\t\t# add child node's children into this children\n\t\t\tret['_children'].append(child)\n\treturn ret\n\ndef xml_has_attr(node, attr):\n\t\"\"\"\n\tCheck if <attr> has an attribute <attr>\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tchild_dict = {}\n\tfor child in node:\n\t\tif child.tag in dict_keys:\n\t\t\tchild_dict[child.tag].append(child.text)\n\t\telse:\n\t\t\tchild_dict[child.tag] = [child.text]\n\treturn child_dict\n\tfrom common_fixtures import *  # NOQA\n\n\n@pytest.fixture(scope='session')\ndef zabbix_repository(appliance, zabbix_machine_ip, zabbix_machine_username, zabbix_machine_password,\n                         zabbix_machine_ip, z"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n#\t\tFor all children of parent\n#\n#\t\tprint(\"parsing %s\" % node.name)\t\t\t\n#\t\tfor child in node:\n#\t\t\tchildren_as_dict(child)\n#\n#\t\tprint('')\n#\t\t#\n#\t\t#\n#\t\t#\n#\t\n#\t\tfor k,v in sorted(node.__dict__.items(), key=lambda x: x[0]):\n#\t\t\tif isinstance(v, Node):\t\t\n#\t\t\t\t# we've seen this in the list, don't"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tchildren = list()\n\t\n\tfor child in node:\n\t\tassert( isinstance(child, dict) == True )\n\t\tif 'children' in child.keys():\n\t\t\tchildren.append( child['children'] )\n\t\t\n\t\telse:\n\t\t\tchildren.append( child )\n\t\n\tassert( isinstance(children[0], basestring) and children[-1] == '' )\n\t\n\tret = {}\n\t\n\tfor key in children[0].keys():\n\t\tif key == '_':\n\t\t\tret[key] = children[0][key]\n\t\telse:\n\t\t\tret[key] = convert"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\t# The structure of the XML tree is:\n\t# http://effbot.org/zone/element-namespaces.htm\n\telem_dict = {'elem':{}, 'children':{}}\n\t\n\tfor child in node:\n\t\tif child.tail:\n\t\t\tchild_dict = xml_children_as_dict(child)\n\t\t\t# <elem>\n\t\t\tif child.get('elem'):\n\t\t\t\tassert (child.get('elem') == None or child.get('elem') == child_dict['elem'])\n\t\t\tif child.get('children'):\n\t\t\t\tchild_dict"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tresult=dict()\n\tif node.tag == 'children': \n\t\ttmp=list()\n\t\ttags=['Name', 'Description']\n\t\tfor item in node:\n\t\t\tif item.tag=='type':\n\t\t\t\tname=item.get('name', '')\n\t\t\t\tif not name:\n\t\t\t\t\tname='Unnamed'\n\t\t\t\t\tif not item.get('label'):\n\t\t\t\t\t\tname='Unnamed'\n\t\t\t\t\tdata_type=item.get('label','')\n\t\t\t\t\tif not data_type:\n\t\t\t\t\t\tdata_type='string'\n\t\t\t\t\ttmp.append(name)\n\t\t\t\t\ttag"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tchildren = defaultdict(list)\n\tfor childnodes in node:\n\t\ttags = childnodes.keys()\n\t\tchildren[tags].append(node.attrib[tags[0]])\n\treturn children\n\t\n\t\n\t\n\t\t\ndef tag_keys(tag):\n\t\"\"\"\n\tGenerate the tag names for a node\n\t\n\tAssume all the tag names will be unique.\n\t\n\tArgs:\n\t\ttag: tag name\n\t\n\tReturns:\n\t\ttagnames: dict\n\t\t\"\"\"\n\ttagnames = dict(sorted(list(tag.keys())))\n\treturn tagnames\n\t\n\t\n\ndef extract"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tnode_to_children = {}\n\tnode_order = {}\n\tis_root = False\n\t\n\tfor child in node.getchildren():\n\t\t\n\t\tis_root_node = False\n\t\tchildren = child.getchildren()\n\t\t\n\t\t# skip empty and unlisted children\n\t\tif children:\n\t\t\tcontinue\n\t\t\n\t\tfor child in children:\n\t\t\t\n\t\t\tif child.tag in [\"tag\"]:\n\t\t\t\tis_root = True\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tchildren_list = children_as_dict(child)\n\t\t\t\n\t\t\tif is_root:\n\t\t\t\tnode_order[child.tag] = ["}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tsender_match_id = check_sender(sender_handle)\n\tsender_match = find_sender_match(sender_match_id)\n\t\n\tsender_handle_matches.append(sender_match_id)\n\t# TODO: Check the author match!\n\t# TODO: Check the subject match(s)\n\t# TODO: Check the object match(s)\n\ndef reformat_entities(text, to_process):\n\t\"\"\"\n\tConverts the text of a list of Texts into a formatted list.\n\t\n\tGiven a list of texts, process them for"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\n\t# We need to check the number of bytes in each data chunk up to make sure we can properly handle more than\n\t     # one chunk if the sender has it yet\n\texpected_number_of_bytes = 0\n\t\n\t# Check and see if it is a match for the sender\n\tif not (is_checksum(sender_handle) and is_checksum(entity_handle)):\n\t\traise InvalidPayload(\"Cannot handle match! Expected %s, I am expecting %s, received %s\" % (expected_number_of_bytes, sender_handle, entity_handle))\n\n\t#"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tmessage = 'Your user is neither the diasp so hardly can send messages or change the recipients of someone else.'\n\t\n\tif get_sender_metadata_field('sender'), get_sender_metadata_field('receiver'):\n\t\tsender_match = get_sender_metadata_field('sender') == message\n\telse:\n\t\tsender_match = False\n\t\n\tif get_recipient_metadata_field('sender'), get_recipient_metadata_field('receiver'):\n\t\trecipient_match = get_recipient_metadata_field('receiver') == message\n\telse:\n\t\trecipient_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\ndef check_sender_and_entity_handle_match_and_owner(sender_handle,\n\t\t\towner_handle, entity_handle):\n\t\"\"\"\n\tEnsure an item is matching either of sender's handle and entity handles.\n\t\n\tIf either the sender's handle or entity handles differ, the check fails.\n\t\"\"\"\n#\tprint(\"sender: {},  entity: {}\".format(sender_handle, entity_handle))\n\t# Check that both are the same\n\tif sender_handle == entity_handle and sender_handle!= None:\n\t\t# We just created the object,"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\n\tsender_name = sender_handle.handle\n\tif get_value(sender_name) == str(entity_handle):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef get_value(x):\n\t\"\"\"\n\tRetrieves the value from a header or entity.\n\t\n\tThis is useful if you use a Header handler to determine the value from a header or an entity.\n\t\"\"\"\n\t\n\tif isinstance(x, (list, tuple)):\n\t\treturn x[0]\n\telif hasattr(x, 'get'):\n\t\treturn x.get()\n\telse"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n@receiver(pre_save, sender=Message, dispatch_uid=\"pre_save_message\")\ndef auto_resend_user_message(sender, **kwargs):\n\tinstance = kwargs[\"instance\"]\n\tinstance.author = instance.author.nick\n\tinstance.save()\"\"\"Test class for Domain objects\n\n:Requirement: Domain\n\n:CaseAutomation: Automated\n\n:CaseLevel: Acceptance\n\n:CaseComponent: Hosts\n\n:Assignee: jdupuy\n\n:TestType: Functional\n\n:CaseImportance: High\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\t# Note: This function is also called with'sender' and 'entity_handle'\n\t# as described in src/entities.h, but we cannot guarantee that there are no changes in the sender and\n\t# entity since the receiver will not receive the payload anymore.\n\tdef _get_or_throw(sender_handle, entity_handle):\n\t\tsender_entity = get_actor_or_error(sender_handle)\n\t\tif sender_entity is not None:\n\t\t\tif entity_handle == sender_entity.key:\n\t\t\t\treturn None # no need to call error"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\ndef check_read_status():\n\t\"\"\"Check if we have read status in the message we are interested in.\n\t\n\tParameters\n\t----------\n\tsender_handle: int\n\t\tSender handle\n\tentity_handle: int\n\t\tEntity handle\n\t\"\"\"\n\tif (sender_handle is None):\n\t\treturn False\n\tif (entity_handle is None):\n\t\treturn False\n\treturn 1\n\ndef validate_sender():\n\t\"\"\"Check incoming sender.\n\t\n\tParameters\n\t----------\n\tsender_name: name\n\t\tSender name\n\tsender_email: string\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\n\tsender_user = get_email(sender_handle, True)\n\tentity_user = get_email(entity_handle, True)\n\t\n\tif (sender_user, entity_user) == (None, None):\n\t\t#If both the sender and entity have the same user object, we need to log a warning for them.\n\t\t\n\t\twarnings.warn((\"sender ({}) and {{}} have different user objects.\\n\"\n\t\t\t\t\t\t\"For this please contact the diaspora-discuss@discuss.net, diaspora-discuss@discuss.org,\\"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\ndef build_payload(sender_handle, entity_handle, recipient_handle):\n    \"\"\"\n    Build the payload for the sender and entity.\n    \n    Arguments:\n     sender_handle\n     entity_handle\n     recipient_handle\n    \n    Returns:\n     (header, body, sender_name, sender_email, recipients, receiver, receiver_name)\n    \"\"\"\n    \n    result = (\n      (sender_handle,\n      entity_handle,\n      recipient_handle,\n      recipient_name,\n      sender_email,\n      sender_timestamp,\n      \"1\", # time_sent\n      \"0\"),\n      (\"0\", entity_handle,\n     "}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif document_path is None:\n\t\tdocument_path = \"docs/no_such_file.txt\"\n\t\n\tdef get_nodes(url, document_path):\n\t\t\"\"\"\n\t\tGenerate the nodes.info file.\n\t\t\n\t\tSee spec: http://nodeinfo.diaspora.software\n\t\t\n\t\t::arg url: A string URL or path to a document or a path to a document.\n\t\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t\t:returns: dict\n\t\t\"\"\"\n\t\tnodes = {}\n\t\t# TODO: use real file"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# read the full document, if it's there\n\tif not document_path:\n\t\ttry:\n\t\t\turlhttp = urllib.urlopen(url)\n\t\t\tdoc = minidom.parse(urlhttp)\n\t\t\t\n\t\t\turl = doc.getElementsByTagName('front')[0].getElementsByTagName('link')[0].firstChild.data\n\t\texcept IndexError, e:\n\t\t\tcontinue\n\n\tif not os.path.exists(document_path):\n\t\tos.makedirs(document_path)\n\n\turl_parse = urlparse(url)\n\thostname = url_parse.hostname\n\tbase_path = url_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# Get DocumentPath using NodeInfo\n\tif document_path:\n\t\tdocument_path = url\n\n\t\n\t# Try to retrieve a Well-Known Document at the given URL\n\ttry:\n\t\tdoc = urllib2.urlopen(document_path)\n\t\n\texcept urllib2.URLError, e:\n\t\traise NodeInfoError(\"Nodeinfo server could not find document at: \" + document_path)\n\n\t# Parse WebContents of the web document (should be able to access the data again)\n\t#print doc\n\tdom = minidom.parseString(doc)\n\n\t# Find"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# Make sure url is well-known:\n\tif not URL_SCHEME.match(url):\n\t\treturn None\n\tif document_path is None:\n\t\tdocument_path = document_path.rsplit(URL_SCHEME, 1)[-1]\n\tcontent = None\n\theaders = {'Content-type': 'text/html', 'User-Agent': 'CiteULia HTTP Hosting Magic Browser'}\n\t# TODO: Implement 'wget', not 'get' yet.  Use the server if it has a body, see example below\n\trequest = Request(url"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tassert document_path is not None, 'No file specified'\n\t\n\turlparts = urlparse.urlparse('%s/ws%s' % (catalog_urlroot, url))\n\tif document_path:\n\t\turl_parts = urlparse.urlparse(document_path)\n\telse:\n\t\turl_parts = dict(enumerate('ws://') + range(3), **urlparts)\n\t\n\t# Try to find an appropriate ws:tag:nodeinfo node that\n\t# matches nodeinfo_info document_spec\n\ttry:\n\t\tminfo_node = minfo.node"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tpath = get_full_path(document_path)\n\t\n\tif \"path\" not in nodeinfo_well_known:\n\t\tmsg = 'File path specified ({}) is not a well-known document (missing.well-known extension?)'\n\t\traise web.notfound(msg.format(document_path, os.path.basename(document_path)))\n\t\n\tpath = utils.relpath(path, os.path.dirname(url))\n\t\n\tresp = utils.get(HTTP_HEADER.BASE + path, headers=HAL)\n\tif resp.status_code!="}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# Get the document path from the url (if it's an absolute path)\n\tif os.path.isabs(url):\n\t\treturn url\n\t\n\tif document_path is not None:\n\t\t# Try to find the document and file name without the extension,\n\t\t# then return it as-is (no path)\n\t\turl = os.path.normpath( os.path.join(document_path, url.split('/')[-1]))\n\t\t# Return immediately if the URL corresponds to a local file\n\t\treturn get_nodeinfo_well_known_document(url, document_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif not doc_path:\n\t\tdoc_path = get_url_from_path(url)\n\t\n\tif isinstance(document_path, bytes):\n\t\tdocument_path = get_nodeinfo_well_known_document(\n\t\t\tdoc_path, 'document.xml.rels')\n\t\t\t\t\t\t\n\treturn dict(\n\t\tdocument_path = os.path.abspath(document_path),\n\t\ttimestamp = os.path.getmtime(doc_path)\n\t)\n\t\ndef create_document_page(url, document_path=None):\n\t\"\"\" Create a Document"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\n\tif not document_path:\n\t\traise Exception(\"Either a valid document path or document_path must be provided.\")\n\t\n\tdoc_id = get_nodeinfo_document_by_id(url, document_path)\n\t\n\tnode_info = NodeInfo(None, doc_id, None, None, set(), set(), set(), False, False, [])\n\t\n\tif doc_id:\n\t\tnode_info.id = doc_id\n\n\treturn node_info\n\n\n@app.errorhandler(404)\ndef not_found(e):\n    return render_template('"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tnode_info = {\"name\": document_path is not None, \"url\": url}\n\t\n\t# If path has been passed in, make it absolute, strip first '/'\n\tif document_path:\n\t\tdocument_path = \"%s/%s\" % (document_path.strip('/'), \"/\" if not \"/\" in document_path else document_path)\n\telse:\n\t\tdocument_path = \"\"\n\t\n\treturn node_info\nimport urllib.request as urllib2\nfrom requests import get\nfrom sys import version_info\nimport os\n\nPROJECT_NAME = 'opencl"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\tdoc.start_block('signature', public_key)\n\trootwrap(doc, signature['document'])\n\t\n\t# Verify public key from the signed signature.\n\tverify(public_key in signature['document'], public_key)\n\n\t# Verify the signature.\n\tdoc1 = doc.first()\n\tif not doc1:\n\t\treturn\n\n\t# Verify key.\n\tverify(isinstance(doc1['key'], rsa.RSAPublicKey), \n\t\tsignature['key']['algorithm'], public_key)\n\t(rsakey, _) = rsakey\n\tassert rsa"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\nclass Verify(object):\n\tdef __init__(self):\n\t\t# The signature to be verified.\n\t\tself.signature = \"\"\n\t\n\tdef log(self, log_level, message, detail=(\"Signing request to %s at level %d\" % (pub_public_key, log_level)), code=None):\n\t\tglobal log\n\n\t\t# The signature for the message is a string containing a list containing the\n\t\t# requested public key, and the message contents.\n\t\tself.signature = message\n\t\t\n\t\t# The message format for the error.\n\t\tself.error_message"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t \n\tsignature = ElementTree.tostring(signature)\n\tcheck_xml_encoding_utf8.verify(signature)\n\n\tpublicKeyBytes = doc.getElementsByTagName(\"publicKeyBytes\")[0]\n\tdocument_tree = etree.parse(doc)\n\troot = document_tree if doc.nodeName == \"document\" else ET.Element(\"document\")\n\n\t# Check that the private key is not a public key (i.e. a string type).\n\ttry:\n\t\tpublicKeyBytes.remove(b\"-----BEGIN PRIVATE KEY-----\\n\")\n\texcept ValueError:\n\t\t#"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\t# Parse signed XML\n\telements = signature.keys()\n\tfor e in elements:\n\t\tif e!= public_key:\n\t\t\traise AssertionError\n\ttry:\n\t\t# Check the signatures are all the same\n\t\tkeys = ['nonce','signature']\n\t\tfor k in keys:\n\t\t\tif signatures[k]!= doc[k]:\n\t\t\t\traise AssertionError\n\texcept AssertionError:\n\t\t# Unable to verify the signatures.\n\t\treturn\n\t\t\n\t# Verify the elements are the same length\n\tkey_length = len(keys)\n\tfor i in xrange(key_length"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t    \n\t# make sure the signature is right\n\tpublic_key_pem = public_key.exportKey(\"PEM\")\n\tsignature_pem = public_key_pem.exportKey(\"PEM\")\n\tpublic_key_pem.verify(signature_pem, KEY_PICKLE_ORDER)\n\tif doc!= None and \"revokedBy\" in doc.keys():\n\t\tassert doc[\"revokedBy\"] == \"octopus-issuer\" # it's verified that the doc is signed (and not signed)\n\t\tverify_data_element(doc[\"revokedBy\"],"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t# Verify for valid signature length using RFC 1871\n\tif len(signature)!= 16:\n\t\traise InvalidMessage(\"Not signed (%i bytes).\" % len (signature))\n\n\t# Encode the signature length\n\tencoded_sig = base64.encodestring(signature)\n\t\n\t# Generate the signature\n\tenc_sig = encrypt(enc_sig, str(time.time()))\n\n\t# Convert it into string\n\treturn str(public_key).translate(enc_sig)\n\n\t\n\n\n#\n# Generate message from keys\n#\n# Message is a tuple (public_key"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t \n\t# Verify the public key and public key fields match in the signature\n\t#\n\tmsg = sig.verify(signature, public_key)\n\n\t# Get the root element\n\troot = ET.fromstring(b''.join(msg))\n\n\t# Verify that the signature does not yet contain any top-level\n\t# elements in the message.\n\t#\n\tif len(msg) == 0:\n\t\tprint(\"*** FAIL ***\")\n\t\tprint(\"*** Message is: %s\" % doc)\n\t\tprint(\"*** Expected: <element>... <%s>\" %(root."}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\ndef process_message(message, signature):\n\tif signature!= \"\":\n\t\tmessage = parse(message)\n\n\t\tsender_data = build_sender(message.sender_host)\n\n\t\tsubject_list = sender_data[message.sender_type]\n\t\tresponse_list={\n\t\t\t\"digest\": subject_list[0]\n\t\t}\n\n\t\tif message.content.startswith(\"{\"):\n\t\t\tsubject = response_list[1]\n\n\t\t\tcontent = subject\n\t\telse:\n\t\t\tsubject, content = subject_list[1], content\n\n\t\tsubject_hash = hashlib.sha256"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n#    def verify_signature(signed, doc, msg):\n\t#    print(msg)\n \n#    document = Document(public_key, verify_keys=[verify_signature])\n#    document.sign(doc, signers={'publickey': signator})\n#\n#    signable = False\n#    try:\n#\tverify_signature(public_key, doc, signature)\n\n\n# ======================================================================\n#\n\n#    class Test_signers(unittest.TestCase):\n#\n#        def setUp(self):\n#            self.m = Message"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n# Check signature\nverify_msg_signature(public_key, doc)\n\n# Check if there is a signature\nverify_signature(public_key, doc)\n\n# Generate the signed message document\ns = S.sign(public_key, signature)\n\n# Print the signed output to the output stream.\ns.Print()\n\n# Encode the signed message document as PEM string\ndoc = open(\"signed-message.xml\", 'wb')\ndoc.write(s.as_pem())from django import forms\nfrom django.utils.translation import ugettext_lazy as"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\tdng_app_id = document[5:]\n\tname = document[:8].decode()\n\t#TODO: check if this is a valid name or number.\n\tif name == \"default_api_key\":\n\t\tapp_id = get_default_appid()\n\telse:\n\t\traise ValueError(\"Invalid Doc2document '{}' in: '{}'\".format(name, document))\n\t\t\n#\tTODO: Add all the defaults to the appid\n\t\n\tif not app_id:\n\t\traise ValueError(\"Invalid Doc2document '{}' in: '{}'\".format(document, document"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\treturn Document(str).to_dict()\n\t\n\n\t@staticmethod\n\tdef write_new_document(document: str) -> str:\n\t\treturn document + '\\n'\n\n\t@staticmethod\n\tdef read_new_document(document: str) -> Dict:\n\t\treturn Document(document).to_dict()\n# Copyright 2010 Canonical Ltd.  This software is licensed under the\n# GNU Affero General Public License version 3 (see the file LICENSE).\n\n\"\"\"Support for sending email.\n\nSee also: `doc/email.py`\n\"\"\"\n\n__metaclass__"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\n\ttry:\n\t\tddes = json2dnd(document)\n\texcept ValueError as ve:\n\t\tlogging.error(str(ve))\n\t\t\n\treturn ddes\n\"\"\"\nUnit tests for the stem.descriptor.server_descriptor.\n\"\"\"\n\nimport unittest\n\ntry:\n  import unittest.mock as mock\nexcept ImportError:\n  import mock\n\nimport stem.descriptor.server_descriptor\n\n\nclass TestServerDescriptor(unittest.TestCase):\n  @mock.patch('socket.socket.socket')\n  def test_defaults(self, mock_s):\n    \"\"\"\n    Constructs"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\tresults = DocumentIO.serialize(document, pretty=False)\n\tresults = json.dumps(results, indent=4, separators=(\",\", \": \"))\n\t\n\ttry:\n\t\tdata = json.loads(results)\n\texcept ValueError:\n\t\ttry:\n\t\t\tdata = json.loads(results.replace(\"\\\\\", \"/\"))\n\t\texcept (UnicodeDecodeError, UnicodeEncodeError):\n\t\t\tdata = results\n\n\tdata = decode_webfarsight(data)\n\treturn data\n\ndef parse_diaspora_m3u(document: str) -> Dict:\n\t\"\"\"\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\n\tparsed_doc = json.loads(document)\n\n\t# Get the fields of this document\n\tfields_data = parsed_doc[\"fields\"]\n\t\n\t# Parse for json.dumps\n\tfor field in fields_data:\n\t\tif field == \"data\" and str(fields_data[field][0]) == 'object':\n\t\t\tsubdocument_id: int\n\t\t\tdata: List[Dict] = [field[\"value\"]]\n\treturn parsed_doc\n\n\ndef parse_freenote_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tParse F"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\ndef parse_diaspora_finger(document: str) -> Dict:\n\tdef parse_diaspora_result(doc: str, response: str) -> Dict[str, Any]:\n\t\t# type: (str, Any) -> Dict[str, Any]\n\t\tdoc_data = get_doc(doc)\n\t\tif not doc_data:\n\t\t\t# not a doc, nothing to parse\n\t\t\treturn {}\n\t\tdoc_results = parse_doc_result(doc)\n\t\tif not doc_results:\n\t\t\t# doc is empty, probably a"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\t# JSON format and XRD format\n\txrd_type = {\n\t\t'text_id': {\n\t\t\t'type': str,\n\t\t\t'id': str,\n\t\t\t'text': str,\n\t\t\t'xrdName': (lambda x: None, lambda x: None)\n\t\t},\n\t\t'ven': {\n\t\t\t'type': str,\n\t\t\t'id': int,\n\t\t\t'text': int,\n\t\t\t'xrdName': (lambda x: None, lambda x: None)\n\t\t},\n\t\t'username': {\n\t\t\t'type': str,"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t \n\t# check valid json if no data\n    try:\n        document = json.loads(document)\n    except ValueError:\n        raise ValueError(\"No JSON structure could be parsed\")\n    \n    server = document['server']\n    port = port_re.search(server)\n    if port:\n        port = port.group(1)\n    \n    return {\n       'server': server,\n        'port': int(port)\n    }\n        \n    \ndef write_webfinger_list(document: str, data: List[Tuple[str, str]]) -> bool:\n\t\"\"\"\n\tIf this function writes a list of web"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\txml = ET.fromstring(document)\n\texcept ET.ParseError:\n\t\tlog(\n            1,\n            \"Can't get XML from the Diaspora webfinger (%r).\",\n            document\n        )\n\t\n\treturn xml\n\t\ndef parse_europe_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tParse Europe Webfinger which is either in HTML format (old) or XRD (old).\n\t\n\thttps://europewebfinger.github.io/webfinger/discovery/webfinger.html\n\t\"\"\""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\tlogger.info(f\"Converting Diaspora webfinger to HTML for '{document}'...\")\n\n\tdocument_dict = json.loads(document)\n\n\ttry:\n\t\tassert type(document_dict) is dict\n\texcept AssertionError:\n\t\tlogger.error(f\"Document file does not seem to be formatted properly with {document}.\")\n\t\n\twebfinger_dict = {\n\t\t\"id\" : document_dict.get(u\"data\", \"\"),\n\t\t\"created_datetime\" : fdatetime.datetime.strptime(document_dict.get(u\""}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\t\t\tresponse = http.request.send(handle)\n\t\texcept HTTPError as err:\n\t\t\tif err.response.text == \"HTTP/1.1 500 Internal Server Error\":\n\t\t\t\traise HTTPInternalServerError\n\t\t\telif err.response.text == \"HTTP/1.1 502 Bad Request\" or err.response.status_code == 404:\n\t\t\t\traise WebfactionRequestError\n\t\t\telif (err.response.text == \"HTTP/1.1 500 Internal Server Error and Server is Upgrade Required\" or\n\t\t\t\t\t  err.response.status_code == 500 or err."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n#\t# Reading document\n#\tif webfinger is not None:\n#\t\twebfinger_document = None\n#\t\twith open(os.path.abspath(webfinger)) as ifile:\n#\t\t\twebfinger = json.load(ifile)\n#\t\t\twebfinger_document = webfinger.get(\"document\")\n#\t\tif webfinger_document is not None:\n#\t\t\treturn webfinger_document[\"document\"]\n#\t\twebfinger_document = None\n#\t\twebfinger"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\tif handle == \"text\":\n\t\t\treturn find_content_text_by_tag('title', 'text', tree_level=1)\n\t\tif handle == \"html\":\n\t\t\treturn found_html2text(\n                find_content_text_by_tag('title'),\n                tree_level=1)\n\t\tif handle == \"xhtml\":\n\t\t\treturn found_xhtml2text(\n                find_content_text_by_tag('title'),\n                tree_level=1)\n\t\tif handle == \"xhtm\":\n\t\t\treturn found_html2text(\n                find_content_"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\tif client_started.get() is True:\n\t\t\t# If client hasn't yet heard of the Internet to connect...\n\t\t\t#... try again...\n\t\t\ttry:\n\t\t\t\timport urllib.request\n\t\t\t\thandle = urllib.request.urlopen( urllib.request.Request(url=\"http://www.rfc7033.eu/cgi-bin/webfinger-get-version.cgi\"))\n\t\t\t\tclient_started.set()\n\t\t\texcept urllib.error.HTTPError as e:\n\t\t\t\t# Handle the following errors...\n\t\t\t\t# https://bugs.launchpad.net/bleach-"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t with urllib.request.urlopen(handle) as response:\n\t\t\tsoup = BeautifulSoup(response)\n\t\t\tbody = soup.find(\"body\")\n\t\t\tif body is None:\n\t\t\t\tlog.error('Cannot found webfinger document')\n\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\tsoup.extract()\n\t\t\t\treturn body.text\n\treturn None\nfrom nose.plugins.skip import SkipTest\nfrom nose.tools import assert_raises\nimport numpy\n\nfrom scipy.stats import rv_continuous\n\nfrom dipy.data import get_sphere\n\n\ndef test_rv_continuous():\n    n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\traise NotImplementedError(\n\t\t\t\"No support for fetching webfinger document.\"\n\t\t)\n\twebfinger_doc = document_lookup(handle)\n\tif webfinger_doc is not None:\n\t\treturn webfinger_doc\n\treturn None\n\ndef try_retrieve_webinar(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve a WebFinger document from the online service.\n\t\"\"\"\n\twebinar_doc = document_lookup(handle)\n\tif webinar_doc is not None:\n\t\treturn we"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n# Fallback to pure Python implementation\ntry:\n\t_cached_webfinger = None\n\tif 'f' in globals():\n\t\t_cached_webfinger = webfinger()\n\telse:\n\t\tsys.exit(\"f is a built-in feature\")\nexcept ImportError:\n\t# When in a normal way, let's run the \"import webfinger\" entry point.\n\timport webfinger\n\t_cached_webfinger = webfinger\n\ndef save_webfinger_document(file_name: str, webfinger_document:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\t# RFC7030 4.2 Document (WebFinger Object)\n\n\t\thandle_file = io.StringIO()\n\n\t\ttry:\n\t\t\tf = urlopen(path=handle_file, timeout=5)\n\t\texcept Exception as e:\n\t\t\t# If we fail to open file, try again after a time\n\t\t\ttry:\n\t\t\t\tf = urlopen(path=handle_file)\n\t\t\texcept Exception as e:\n\t\t\t\tlogger.warning(f'Try again after a read in a file, trying again after {time.time() - t} seconds.\\n')\n\t\t\t\tcontinue\n\t\telse"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n## Given a URL\n## @code\n## py:param str\n## py:exception ValueError\n## py:keyword int start_url, end_url\ndef get_url_from_url(url: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve a RFC7033 webfinger document. If it fails and a URL isn't\n\tidentified, just return `#id`.\n\t\"\"\"\n\n## Given a URL\n## @code\n## py:param str\n## py:exception ValueError\n## py:keyword int start_url, end_url\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t # If it doesnt throw an HTTP 403 error, there's no other option\n        # to call\n        # raise tornado.web.HTTPError(403)\n\ttry:\n\t\treturn _retrieve_webfinger_document(handle)\n\texcept HTTPError as exc:\n\t\tif exc.code == 403:\n\t\t\treturn None\n\ndef try_get_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t # If it doesnt throw an HTTP 403"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# Retrieve remote Diaspora webfinger document\n\tdocument = get_remote_doc_from_handle(handle, \"_id\")\n\t\n\t# Parse XML document\n\ttree = ElementTree.fromstring(document)\n\troot = tree.getroot()\n\tdocList = _get_doc_list(root, docTree)\n\t\n\t# Build Diaspora Webfinger document \n\tdoc = None\n\tif (\"adType\" in root):\n\t\t# Ad types\n\t\tfor aType in _get_adType(root):\n\t\t\tif (\"name\" in aType"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\tret_val = {}\n\t\n\thandle.send(get_remote_document_from_doaj(db_connection, \"doaj/doaj.doaj.xml\"))\n\thandle.close()\n\t\n\thandle = httplib.HTTPConnection(db_connection, 10)\n\tresult = retrieve_and_parse_doaj(handle)\n\thandle.close()\n\t\n\t#if the result is not a xml response, we're good\n\tif not result:\n\t\treturn dict()\n\t\n\t#parse remote document as xml into a DOM object\n\tdoc = minidom.parseString(result"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tcontent = read_file(handle)\n\t\n\treturn parse_and_save(content)\n\t\n\n\ndef make_new_item(doctype, fieldname, label, parent, attachments, content_change, notify=False):\n\t\"\"\"\n\tExtracts and appends the item and the child items to the parent document\n\t\n\t:arg doctype: DocType of the new document class\n\t:arg fieldname: Field in which the new item gets inserted\n\t:arg label: Label for the new item\n\t:arg parent: Parent document. Will be \"None\"\n\t:arg attachments: Dict"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\thandle.send_request(http.GET)\n\thandle.handle_request()\n\tquery = ElementTree.XML(handle.response)\n\tra_client.parse_document_list(query)\n\treturn dict(get_diaspora_webfinger_list(query))\n\ndef get_diaspora_webfinger_list(query):\n\t\"\"\"\n\tParse a single Diaspora webfinger document and return an\n\tdict of diaspora_webfinger_list entries\n\t\n\t:arg query: Diaspora"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\n\ttry:\n\t\tdoc = webfinger_client.get_document(HANDLE, handle)\n\t\tif doc:\n\t\t\treturn_dict = copy.deepcopy(doc.webfinger_data)\n\t\t\treturn_dict['url'] = doc.webfinger_url\n\t\t\treturn_dict['data'] = doc.webfinger_data\n\t\t\treturn return_dict\n\t\telse:\n\t\t\treturn None\n\texcept URLError as err:\n\t\t# HTTPError with URLError: 404\n\t\tprint(\"URLError = \" + str(err.reason))\n\t\tif err.reason == httplib"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\tfrom frappe.utils.datautils import DataUtils\n\te = Document()\n\tresult = {}\n\tr = e.retrieve_data()\n\tif not r:\n\t\treturn result\n\telse:\n\t\tr = DataUtils.XML_Parser().parse_string(r,'result','result', True)\n\t\td = Document()\n\t\ttry:\n\t\t\td.parse_xml(handle)\n\t\t\tresult.update(d.to_dict())\n\t\texcept DocumentFormatError:\n\t\t\t# Parsing went wrong with the server.\n\t\t\traise e\n\treturn"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t return handle.retrieve()\n\t\ndef retrieve_file_info(handle):\n\t\"\"\"\n\tRetrieve a file info from a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle from retrieve\n\t:returns: Info dict\n\t\"\"\"\n\t\ndef retrieve_and_parse_file_info(handle, file_handle, offset, length):\n\t\"\"\"\n\tRetrieve a file info from a remote Diaspora webfinger document.\n\t:arg handle: Remote handle from retrieve\n\t:arg file_handle: Path to get info from\n\t:arg"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tdoc = xml.dom.minidom.parse(handle)\n\texcept xml.parsers.expat.ExpatError:\n\t\tprint \"File %s doesn't seem to be XML!.\" % handle\n\t\treturn {}\n\n\ttry:\n\t\t# Retrieve all documents\n\t\tdata = doc.getElementsByTagName('document')\n\t\tfor element in data:\n\t\t\tread_document(element)\n\t\treturn data\n\tfinally:\n\t\tdoc.unlink()\n\n\nif __name__=='__main__':\n\tpass\nfrom unittest import TestCase\n\nimport unittest\n\nimport numpy"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\turl \t\t= \"https://dasulohaproxy.appcdn.com/v2/doc/v2doc/getdlink.do?id=1&url={0}\".format(handle)\n\thtml \t\t= urllib2.urlopen(url).read().decode(\"utf-8\")\n\n\turl_data \t= re.findall('data-file=\"(.*?)\"', html)[len(html)-1]\n\t\n\tfor line in html_to_datadict(html):\n\t\tif \"file\" in line:\n\t\t\turl_data \t= get_url"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\trequest = urllib2.Request(\"http://api.dalandp.com/ws/1/query\", urllib.urlencode({ 'handle': handle }))\n\n\n\treturn json.loads(urllib2.urlopen(request).readall().decode(\"utf-8\"))['document']['doc']['title']\n\ndef retrieve_and_parse_do(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Do document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\trequest = urllib2.Request(\"http://www.douban."}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\turl = make_request(host, '/RID.djsidsu.nl', 'Host/Diaspora host meta')\n\txml = etree.parse(urllib2.urlopen(url))\n\treturn xml.getroot()################################################################################\n#\n# This program is part of the IBMD Project.\n#\n# Copyright (C) 2014 IBSmd.\n# \n# This program can be used under the GNU General Public License version 2\n# You can find full information here: http://www.nascd.org/license\n#\n################################################################################\n\nimport logging"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\trd = None\n\t\n\ttry:\n\t\trd = DiasporaHostMeta.objects.get(host=host)\n\texcept DiasporaHostMeta.DoesNotExist:\n\t\trd = None\n\t\n\tif not rd:\n\t\trd = DiasporaHostMeta(host=host, host_path=host_path)\n\t\n\tif rd.host_meta_type!= RDX_HOSTMETA_TYPE_NFS:\n\t\traise InvalidArgument('Host metadata is not NFS')\n\t\n\treturn rd\n\ndef retrieve_racesrcname_meta(racesrc"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tif host.find('://')!= -1:\n\t\treturn host\n    \n\turl    = 'http://%s%s.sdo#action=get?type=host' % (host, WSDL_VERSION)\n\trequest = urllib2.Request(url)\n\trequest.add_header('Authorization', \n\t\t'Basic %s' % (base64.b64encode(userpass)).strip())\n\t\n\tdata = StringIO.StringIO()\n\twhile True:\n\t\ttry:\n\t\t\trequest.request(requestbody=data)\n\t\texcept:\n\t\t\tif 'Connection"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\treturn api.xapi.find_object(xapi.model.system_preferences,\n\t\t\t\t\t\t\t'XRDI_HOST',\n\t\t\t\t\t\t\t{'key': host},\n\t\t\t\t\t\t\tNone)\n\ndef retrieve_diaspora_resource_list(server, hostname, username):\n\t\"\"\"\n\tRetrieve a remote Diaspora resource-data list.\n\t\n\t:arg server: Virtual host server\n\t:arg hostname: Host to retrieve from (ex. 'localhost')\n\t:arg username: The user to access from a remote DC\n\t:returns: List of data servers\n\t\"\"\"\n\t\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\t# Get Diaspora data\n\tresponse = requests.get(\"http://%s%s\" % (host, \"/dynamicSearch/%s/1\" % query.replace(\" \",\"%20\")))\n\t\n\t# Parse XML data\n\tdata = BeautifulSoup(response.text)\n\t# Check it's valid XML\n\tdata = data.find(\"data\", {\"search_info\": {\"host\":host} })\n\tif data is None:\n\t\t#print \"Can't get Diaspora metadata from host %s\" % host\n\t\treturn None\n\t\n\t# Return a Diaspor"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tdoc_path = '/diaspora/host-meta/{}.xml'.format(utils.url_encode_safe(host))\n\t\t\n\ttry:\n\t\t\n\t\tdom_el = EdobeXml.Element('host-meta')\n\t\t\n\t\tfor meta in host.findall(\".//dict/item/dictionary/item\"):\n\t\t\t\n\t\t\tif not hasattr(meta, 'properties'):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tfor element in meta.properties.array:\n\t\t\t\tvalue_el = EdobeXml.Element('value')\n\t\t\t\tvalue_el.text = unicode(element.text).encode(encoding='"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\treturn retrieve_substring(host, u'diaspora_host_meta')\n\nclass Edit(RESTModel):\n\n\t\"\"\"\n\tEdit documents.\n\t\n\t* **uid** - a unique identifier for this object\n\t* **title** - the title of this document\n\t\"\"\"\n\n\t# this resource's session key\n\tsession_key = fields.StringField(u\"session key\")\n\t\n\t# this resource's ID\n\tresource_id = fields.StringField(u\"Unique ID of the object\")\n\t# this resource's title\n\ttitle = fields.StringField(u"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\t# TODO: The below code will be replaced with a proper module structure that can be used outside RXRD\n\treturn XR_DIASPORAGED_HOST_META.replace('X-XRD-DISCOVERY', host)\n\n\ndef create_diaspora_host(resource_root, meta=None):\n\t\"\"\"\n\tCreate a remote Diaspora host instance. The remote host is set on ``resource_root``.\n\t\n\t:arg resource_root: Path to remote Diaspora resource\n\t:param meta: Optional, pre-filled with"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tlogging.debug(\"Retrieving %s\", host)\n\n\t\tresponse = connection.request('%s/host-meta' % BASE_URL, None, {'Accept' : 'application/xhtml+xml'}, headers={'Accept-Encoding' : 'gzip'}, method='HEAD')\n\n\t\thost_tag = etree.fromstring(response.content).find('.//DADRIVERA/Host')\n\t\treturn host_tag[0]\n\t\n\texcept socket.error, error:\n\t\tlogging.error(\"Cannot retrieve DADRIVERA host-meta document: %r, error"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\ndef retrieve_diaspora_host_stream():\n\t\"\"\"\n\tRetrieve a remote Diaspora host-stream document (only)\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\ndef retrieve_diaspora_host(hostname, port=8765):\n\t\"\"\"\n\tRetrieve a remote Diaspora host document.\n\t\n\t:arg hostname: Host name to retrieve from\n\t:arg port: Port to retrieve\n\t:returns: None\n\t\"\"\"\n\t\ndef retrieve_diaspora_"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tif isinstance(data, basestring):\n\t\tdata = parse_qs(data)\n\t\n\tif data.get('auth_token', None) is None and 'X-CSRFToken' in os.environ:\n\t\tresponse = requests.post(\n\t\t\t\turl + \"/login/\", \n\t\t\t\tauth=(os.environ['X-CSRFToken'], ''), \n\t\t\t\tdata=data,\n\t\t\t\ttimeout=int(timeout))\n\t\t\n\t\treturn False, response\n\t\n\telif data.get('auth_token', None) is None:\n\t\tif 'X-CSRFToken' in os.environ:"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\t# Build request\n\treq = urllib2.Request(url, data)\n\tif kwargs:\n\t\treq.get_method = lambda: kwargs.get('method')\n\t\n\t# Make request\n\ttry:\n\t\tif method == \"GET\" and kwargs.get(\"retry\", True):\n\t\t\tres = urllib2.urlopen(req)\n\t\t\treturn res.getcode(), res.read()\n\t\telif method == \"POST\" and kwargs.get(\"retry\", True):\n\t\t\tcontentType = kwargs.get(\"type\", None)\n\t\t\tif contentType is not None:\n\t\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tr = None\n\ttry:\n\t\turl_data = None\n\t\tif data and isinstance(data, basestring):\n\t\t\turl_data = {'data':data}\n\t\t\n\t\tr = requests.post(url, params=url_data, **kwargs)\n\t\t\n\t\tif r.status_code is not None:\n\t\t\tmsg = \"POST to %s failed with code %d\"%(method,r.status_code)\n\t\t\tif timeout and isinstance(r.headers, dict):\n\t\t\t\tdata = r.headers\n\t\t\telse:\n\t\t\t\tmsg += \"POST to %s failed with headers"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tif timeout is not None:\n\t\treturn send_timeout(url, timeout, method=method)\n\telse:\n\t\treturn send_connection(url, data, method=method)\n\n\nclass Paste(object):\n\t\"\"\"\n\tWrapper for paste.deploy.deploy and paste.sync\n\tUsed by deploy/sync management command\n\t\"\"\"\n\tdef __init__(self, application_path=None, app_name=None, source=None, destination=None,\n\t\t\t\thost=None, port=None, debug=False, ssl_pem=None, cert_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tlogger = logging.getLogger(\"werkzeug\")\n\t\n\ttry:\n\t\t# Prepare body\n\t\ttry:\n\t\t\tsize = int(kwargs.pop('size', '500'))\n\t\texcept KeyError:\n\t\t\tsize = 500\n\t\t\n\t\t# Prepare headers\n\t\tif 'User-Agent' not in session:\n\t\t\tsession['User-Agent'] = session['User-Agent'].lower()\n\t\t\n\t\t# Prepare timeout\n\t\tif timeout is None:\n\t\t\ttimeout = 10\n\t\t\n\t\theaders = {\"Content-Type\":\"application/x-www-form-urlencoded\",\n\t\t\t\t\t\"X-Verify-User-Agent"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tif data.has_key('filename'):\n\t\tf=open(data['filename'], 'rb')\n\telse:\n\t\tf=None\n\t\n\ttry:\n\t\tr = requests.post(\n\t\t\turl=url,\n\t\t\tdata=data,\n\t\t\ttimeout=timeout,\t\n\t\t\tmethod=method,\n\t\t\tcookies=cookiejar,\n\t\t\traise_status=True,\n\t\t\tverify=False\n\t\t)\n\t\tres = r.raise_for_status()\n\t\t\n\t\t#if we got a json then go and return the object\n\t\tresult = json.loads(res."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\t# Make request\n\ttry:\n\t\tresponse = requests.post(url, data=data, timeout=timeout, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\tmessage = \"Request timed out\"\n\t\traise ConnectionError(message)\n\texcept requests.exceptions.HTTPError:\n\t\tmessage = \"HTTPError while sending %r to %r\" % (data, url)\n\t\traise ConnectionError(message)\n\texcept requests.exceptions.ConnectionError as exc:\n\t\tmessage = \"ConnectionError({0}): {1}\".format(type(exc).__name__, exc)\n\t\traise ConnectionError"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\turl = get_url(url)\n\theaders = {'User-Agent': 'pySamtools/' + py_version,\n\t\t\t\t'Content-type':'application/x-www-form-urlencoded'}\n\t\n\theader_str = '\\r\\n'.join(['<meta name=\"author\" content=\"',\n\t\t\t\t\t\t\t'\"http://mastodon.com/mastodon/\">',\n\t\t\t\t\t\t\t'\"',\n\t\t\t\t\t\t\t\"'><input type=\"hidden\" name=\"author\" '\", required>\"',\n\t\t\t\t\t\t\t'\"type\": \"text\"',\n\t\t\t\t\t\t\t'\"value\": \"'\n\tfor header in headers"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\ndef redirect(url, code=None, data=None, timeout=10, method=\"get\", *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a redirect to the given url.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.get``.\n\t\n\t:arg url: Full url to redirect to, including protocol\n\t:arg code: Reason code\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response ("}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\theaders={\n\t\t\"Content-Type\": \"text/plain\",\n\t\t\"Location\": url\n\t}\n\t\t\n\thost={}\n\tif sys.version_info < (2, 7):\n\t\thost['protocol']='https'\n\telse:\n\t\thost['protocol']='http'\n\ttry:\n\t\tconn=HTTPConnection(host=host,timeout=10)\n\texcept (IOError, OSError), e:\n\t\tif e[0]==32 and e[1]==127: # Operation not permitted\n\t\t\tprint \"You need internet access.\"\n\t\t\tsys.exit(1"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\t\n\tkey = key if type(key) == dict else key.keys()\n\tfor k in keys:\n\t\tdict_insert(dic, val, k, *k.split('.'))\n\n\ndef tuple_equal(i1, i2):\n\tif isinstance(i1, tuple):\n\t\treturn tuple_equal(i1, i2)\n\telse:\n\t\t# if i1 is None, I have no idea what to happen\n\t\treturn i1==i2\n\n\ndef dict_almost_equal_keys(dic1, dic2, key1, key2):"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\t\n\tkeys = keys[:]\n    for subkey in keys:\n\tsubkey = key + '.' + subkey\n\tdic = dic.get(subkey, {} )\n\tdic[subkey] = val\n\n\"\"\"\nclass SqlField:\n\tdef __init__(self, kk):\n\t\tself.kk = kk\n\t\t\n\tdef get_(self, name, *args, **kwargs):\n\t\treturn self.kk.fields.get(name)\n\t\n\tdef __getitem__(self, name):\n\t\tif isinstance(name, (int, long, float)):\n\t\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\t\n\tif isinstance(dic, dict):\n\t\todict = odict.copy()\n\t\t\n\t\tif len(keys) == 1:\n\t\t\todict.update({key: dic[keys[0]])\n\t\telse:\n\t\t\tfor k in keys:\n\t\t\t\todict[k] = dic[k][key]\n\t\t\t\t\n\t\tif odict.__class__.__name__ == \"defaultdict\":\n\t\t\todict.update([dict_insert(dic, v, key, *keys[1:]) for v in val])\n\t\telse:\n\t\t\todict.update({'{}_insert'.format("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\n\t# check if key is in keys\n\tif key in keys:\n\t\treturn # no, return the value\n\tkey = keys.index(key)\n\n\t# build a dict\n\td = {}\n\tfor key in keys[1:]:\n\t\td[key] = dic[key]\n\treturn d\n\n\n'''\n\tclass ListNode(Node):\n\t'''Base class for lists'''\n\t\n\tdef __str__(self):\n\t\tlst = []\n\t\tnode_names = dir(self)\n\t\tfor elm in filter(is_not_a_list,"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tif isinstance(val, basestring):\n\t\tif keys:\n\t\t\tfor val in frappe.db.get_value(val, ['parent', key]):\n\t\t\t\tnested_dict = d.setdefault(val, {})\n\t\t\t\tnested_dict.update(keys)\n\t\t\t\td[key] = nested_dict\n\t\t\t\tif not nested_dict:\n\t\t\t\t\tbreak\n\telif val and isinstance(val, dict):\n\t\tnested_dict = d.get(val, {})\n\t\tif nested_dict:\n\t\t\tfor d in nested_dict:\n\t\t\t\tnested_dict[d[key]]"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\n\tret=dict_insert(dic, None, key, *keys)\n\tif not ret: raise KeyError(\"key %r not found in dictionary\" % key)\n\telse:\n\t for item in ret:\n\t\t\tret[item]=dic[item]\n\treturn ret\n\ndef _filter_data(self, func, data):\n\t\"\"\"\n\tfilter a list of nested data into a new list of nested dicts\n\t\n\twhere nested dicts are in the form:\n\t\n\t{'a':[{'b':'1', 'c':'3'}]}, # {'b':'2'}, # {'c"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\t\n\tdict_insert(dic, val, key, *keys)\n\t\n\t\n# --------------------------------------------------------\n#  SPLIT EFFECTORS\n# --------------------------------------------------------\n\n# A generic SPLIT tester that just executes the tester, and thus\n# provides a mechanism for running doctests.  The main question is\n# how to execute the input arguments and outputs from the given tester\n# (ie, the file which generated this script).  Also, the tester has two\n# modes: 'doctest' (the standard doctest module in Python syntax): Run the\n# doctest tests and dispatch sys.exit(). The '"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\ndef list_append(dic, key, *keys):\n\t\"\"\"\n\tappend a value to a nested list\n\t\n\tto append value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  list_append({}, 'val', 'key2'.split('.'))\n\t\n\t:param dic: a dictionary object to append the values in\n\t:param val: a value to append to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n#\n#\n#\ndef dict_search(dic, key, condition):\n\t\"\"\"\n\thelper function to find sub-dictionary values that match the given condition\n\t\n\t:return: the value, if exists\n\t:rtype: any\n\t\"\"\"\n\t\n\tfor d in dic:\n\t\tif condition(dic.get(d)) == 0:\n\t\t\treturn d\n\t\n\treturn None\n\t\t\n\t\n#\n#\n#\ndef get_key(name, d):\n\t\"\"\"\n\treturns the object key in the list of dicts that contain a given key\n\t(dictionary key's full path"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\t\n\tfor key in keys:\n\t\tif key in dic:\n\t\t\tdic = dic[key]\n\t\telse:\n\t\t\tdic = {}\n\t\tdict.setdefault(key, []).append(val)\n\n# ----------------------------------------------------\n# test code\n# ----------------------------------------------------\ndef test():\n\td = { 1:(2, {'y': 'hello'}), 2:(obj(), {'z' : 3}) }\n\tprint list(d.keys())\n\tt = (1, 2)\n\tprint t[:]\n\tt.sort()\n\tprint t.pop()\n\tfor t_"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t \n\titype_base_dir = settings_dirs['itype_base_dir']\n\tbase = os.path.join(itype_base_dir, itype_options['type'])\n\tos.mkdir(base)\n\tfor section in itype_options['sections']:\n\t\tfor file_name in itype_options['files'][section]['files']:\n\t\t\tif 'base' in file_name \\\n\t\t       and not os.path.isfile('{0}/CMakeLists.txt'.format(os.path.join(base, file_name))):\n\t\t\t\tprint (\""}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n#    from pyraf import iraf\n#    iraf.add_default_config(True)\n#    config = iraf.config.ConfigNode()\n#    config['dir'] = settings_dirs['default_project_dir']\n#\n#    for name in config.ls():\n#        print(\"[%s]\\n%s\" % (name, config[name]))\n#\n##    iraf.set_search_paths(['.'])\n##    iraf.init_data()\n\n    if spec_option['create_dir_structure"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n        print \"Creating the new complex type \", typ_short_name, \"...\"\n\n\t\tsettings_file = settings_dirs.create_path(\"complextype\", \"settings.txt\")\n\t\tsettings_file.close()\n\n\t\tname, ext = os.path.splitext(typ_short_name)\n\t\tprint ext\n\t\tcreate_file_and_close(name, \"settings_\" + ext, settings_file)\n\n\n\t\t#Create a new \"complextype\" instance, used by i_settings_from_module\n\t\tdef instance_arghandler(arg): \n\t\t\tif arg==\""}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\t\n\t# Define in_file_name/in_format_args to be used for the in_file_name output files\n\tin_file_name = os.path.basename(in_file_name) \n\tin_format_args = spec_option.get(\"file_format_options\",{})\n\tin_file_args = parse_argv(in_file_name, in_format_args)\n\n\tsettings_dirs[\"in_file_name\"] = in_file_name\n\tsettings_dirs[\"in_format_args\"] = in_format_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n        if is_suse_type_of:\n            suse_type = settings_dirs[\"SUS\"]\n            # suse_type = \"suse\"\n        elif suse_mode == \"suse\":\n            suse_type = settings_dirs[\"SUS\"]\n        else:\n            raise SuseError(f\"Unsupported suse mode '{suse_mode}'.\")\n\n        # Generate the new complex type\n\n        complex_type = StructType(name=suse_type,\n                               itemsize=len(settings_dirs[\"SUS\"]))\n\n        # Make sure our field is of sufficient size\n        if"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\n\tout = open(settings_dirs[\"settings_file_name\"], \"w\")\n\n\tout.write(\"// %s\\n\" % spec_option)\n\n\tout.write(\"#include \\\"\" + \\\n\t\t           settings_dirs[\"setting_dir\"] + \"/\" + \\\n\t\t  \"/conf/IniType.h\\\"\\n\")\n\n\n\tout.write(\"#ifdef INCLUDE_INI\\n\")\n\tout.write(\"#include \\\"%s.h\\\"\\n\\n\"%(spec_option))\n\n\tout.write(\"#else\\n\\n\")\n\n\tout.write(\"#undef INCLUDE"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\treturn_val = []\n\t\tout = []\n\t\tfor d in settings_dirs:\n\t\t\tout += get_type_string_(d, spec_option)\n\n\t\treturn return_val, out\n\n\treturn lambda p: os.system(p)\n\n\ndef generate_default_settings_structure(opts):\n\t\"\"\"\n\tGenerate the default settings structure\n\t\"\"\"\n\tout = { 'default_settings': [ ],'settings_dir': None }\n\n\tfor opts_spec, opts_var in opts:\n\t\tif opts_spec.startswith('settings_') or opts_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\tlist_of_name = 'inisyn_type%s' % spec_option\n\n\tprint '\\n'\n\tprint '# -*- coding: utf-8 -*-'\n\tprint '# encoding: utf-8'\n\tprint \"#\" + list_of_name\n\n\tprint '# setting default for new type: '\n\tprint \"     \\\"_default_inisyn\\\";\\n\"\n\ndef main():\n\n\tusage_str = \"\"\"usage: inisyn.py -f, --config=\"config.ini\" -H \"\"\"\n\t\n\tcmd = \"\"\n\tif len("}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n    if spec_option =='spec':\n\t\tif 'complex' not in settings_dirs:\n\t        raise FileNotFoundError('Error: Path to the complex type file does not exist')\n        else:    \n            if 'type' not in settings_dirs:\n                raise FileNotFoundError('Error: Path to the type file does not exist')\n \n        print('Creating new types complex...')\n        complex_file, complex_file_extension = \\\n            gen_file_in_complex()\n        print_comment = '@type: Python\\n'\n        out_complex.write(complex_create_file(complex_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n#\tCheck if we have this type already\n\tfile_exists(settings_dirs[spec_option])\n\n\tif spec_option in settings_dirs:\n\t\tsettings_files = settings_dirs[spec_option]\n\t\tfile_exists = True\n\t\t\n\t\tif file_exists:\n\t\t\tsettings_files.remove(spec_option + \".new\")\n\t\t\tif settings_files:\n\t\t\t\tsettings_files.insert(0, spec_option + \"-new\")\n\t\t\n\t\tsettings_files.sort()\n\n\t\t# remove duplicates\n\t\tfiles_to_remove = []\n\t\tfor config"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\tdef __init__(self):\n\t\t# TODO: Add the class that contains the configuration. The __init__() should probably be removed in Airflow.\n\n\t\tself.config_manager = None\n\t\tself.config_file = None\n\n\t\tself.config_content = {}\n\n\t\t# The following is meant as a cache of values, and the contents\n\t\t# can be used as values per-instance by the class, so that we can overwrite it\n\t\t# by copying any pre-calculated values.\n\t\tself.cache = {}\n\n\n\tdef add_to_cache(self, key,"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n    return cls.pluginManager.get(\"ansible_config_manager\")\n#!/usr/bin/python\n#\n# (c) 2016, Russell Teague\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY;"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\treturn ConfigManager.getInstance()\n\n\n# import the necessary packages\nfrom app import app\nfrom app.views.view_classes import ViewList\nfrom app.views.view_classes import ViewDetail\nfrom app.views.view_classes import ListView\nfrom app.views.view_classes import CreateView\nfrom app.views.view_classes import UpdateView\nfrom app.views.view_classes import DeleteView\nfrom app.views.view_classes import get_view_objects\nfrom app.views.view_classes import BaseListView\nfrom app.views.view_classes"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n#pylint: disable=C0111\ndef list_options():\n\t\"\"\"\n\tGet the available options.\n\t\"\"\"\n\treturn \"commandline\",\n\t\t\nclass _CommandlineOptions(object):\n\t\"\"\"\n\tUsed by commands.\n\t\"\"\"\n\t__opts = {\n\t\t\"type\"    : (str, __name__),\n\t\t\"label\" : (str, __name__),\n\t}\n\t__args    = \"<item_name>=<group_name>... >\"\n\tdef __init__(self, type, label, desc):\n\t\tself.type"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\t# Initialize the ansible config manager.\n\n\tcls.ansible_config_manager = ansible_config_manager()\n\n\t# Check if the config manager has already been made.\n\n\tif cls.ansible_config_manager is None:\n\t\t# Initialize the global config manager.\n\n\t\tconf_manager = ConfigManager()\n\n\telse:\n\t\t# Read the data in config manager\n\n\t\tconf_manager = cls.ansible_config_manager.read()\n\n\t\tassert(isinstance(conf_manager, ConfigManager))\n\n\t\tconf_manager.__name__ = app_name\n\n\t\tconf_manager"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\t\n\treturn _config_manager_\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n\"\"\"\nCopyright (C) 2015 Marius Saul\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\tif not cls.__name__ == \"AnsibleConfigParser\":\n\t\traise RuntimeError, \"must be an AnsibleConfigParser instance\"\n\n\t## the config manager handles the different configurations\n\t## that each configuration file uses to get\n\t## additional configuration for the main config file\n\t## and should not be in the future.\n\tcfgm = ConfigParser()\n\t## if there are files in a config_dir\n\tcfgm.load(\".config\")\n\tcfgm.set(\"config\", cls.__name__.lower()+\"-configs\", cls.config_dir)\n\t## use the default instance of Config"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n    # If we try it as is, we're running in \"manage\" mode.\n    # We don't want to require Ansible to be executed as the script.\n    if running_in_windows():\n        ansible_config_manager.reload_ansible_initscripts()\n\n\ndef setup_environment(params):\n    \"\"\"\n    Sets up the module to be used to manage py2exe and run.\n    \"\"\"\n\n    # If py2exe is not found, then we don't need to do anything.\n    # Just call sudo to a location that's a directory.\n    if try_to_get_home():\n       "}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t return __ansible_config_manager\n\n\n# TODO: Move this to a function in case it works.\ndef ansible_callback(f, type, obj):\n\t\"\"\"\n\tRuns the command and sets some default values and save the defaults.\n\n\t:param f:\t\tPython callback\n\n\t:param type:\t\ttype of command (e.g. ssh, lvs) or \"ssh\" for a remote machine\n\t:param obj:\t\tparameter value to set the defaults\n\t:return:\tNone\n\n\t\"\"\"\n\tif hasattr(obj, callback):\n\n\t\tfor group"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\tglobal _config_manager\n\tif _config_manager == None:\n\t\traise RuntimeError(\"You don't call this function!\")\n\n\treturn _config_manager\n\n\nimport argparse\nimport logging\nfrom os import path, remove\n\nimport yaml\n\nfrom. import conf as app_conf\nfrom. import utils as ut\nfrom. import logger as vlogger\nfrom.. import config\nfrom..model import Model\n\n\n__version__ = \"5.3.16\"\nLOG = logging.getLogger(__name__)\n\nconf_file = \"config.yaml\"\nconfig = None\n"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls._get_workspaces()\n\n\nclass WorkspaceManager(object):\n\t\"\"\"\n\tThe object used by all workspaces\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._workspace = None\n\t\tself._workspace_managers = weakref.WeakValueDictionary()\n\n\t\tself._workspace_managers_lock = threading.RLock()\n\t\tself._workspace_managers_map = WeakValueDictionary()\n\t\tself._workspace_managers_lock.release()\n\n\tdef create_workspace_manager(self, workspaceName='default'):\n\t\t\"\"\"\n\t\tCreates a new workspace manager"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls._active_workspace_manager\n\tdef get_active_workspace_manager(cls):\n\t\t\"\"\"\n\t\tGets the current active workspace manager or None.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif cls._active_workspace_manager is not None:\n\t\t\t\treturn cls._active_workspace_manager\n\t\t\telse:\n\t\t\t\treturn None\n\t\texcept AttributeError:\n\t\t\tassert False\n\tdef set_active_workspace_manager(cls, new_value):\n\t\t\"\"\"\n\t\tSets the value of the active workspace manager.\n\t\t\"\"\"\n\n\t\tcls._active_workspace_manager ="}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\t\n\tworkspace_manager = WsManager.Instance()\n\treturn workspace_manager\n# Copyright (C) 2008 Valmantas Paliksa <walmis at balticum-tv dot lt>\n#\n# Licensed under the GNU General Public License Version 3\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\treturn globals()[cls.__name__+':workspace.workspacemgr']\n##############################################################################\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\n# Produced at the Lawrence Livermore National Laboratory.\n#\n# This file is part of Spack.\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\n# LLNL-CODE-647188\n#\n# For details, see https://github.com/spack/spack\n# Please also see the NOTICE and LICENSE files"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\n\treturn cls._workspace_manager\n\n\n######################################################################\n# @name  class_for_class\n# @brief class for the class to be created\n######################################################################\nclass class_for_class:\n\t\"\"\"\n\tClass for making a class for a class to be created.\n\t\"\"\"\n\n\tdef __init__(self,class_name):\n\t\t\n\t\tself.__name = class_name\n\t\tself.__module = None\n\t\tsuper(class_for_class,self).__init__()\n\n\n\n\tdef __str__(self):\n\t\treturn \"(module : \" + self.__module.__name__ + \")"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\tworkspace_manager = None\n\t\tworkspace_manager_factory.workspace = 'local'\n\t\tworkspace_manager_factory.workspace_manager = cls.config['workspace_manager']\n\n\treturn workspace_manager\n\t# @export_section\n\tdef workspace_manager_changed(self):\n\t\t\"\"\"\n\t\tHandle a change in the configuration file.\n\t\t\"\"\"\n\t\tself._workspace_manager.workspace_changed()\n\n\t# @export_section\n\tdef workspace_manager_reset(self):\n\t\t\"\"\"\n\t\tReset settings of the workspace manager.\n\t\t\"\"\"\n\t\tself._"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\n\treturn _workspace_manager\n\"\"\"\nThe MIT License\n\nCopyright (c) 2007-2010 Leah Culver, Joe Stump, Mark Paschal, Vic Fryzel\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nf"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\t\n\tworkspace_path = getattr(cls,'_workspace_path')\n\tif workspace_path in Workspace.workspace_managers:\n\t\treturn Workspace.workspace_managers[workspace_path]\n\traise ValueError('Workspace \"%s\" does not exist.'%workspace_path)\n\ndef workspace(cls, name=None, path=None):\n\t\"\"\"\n\tObtaines the workflow (workspace) in an existing workspace by name; no name is enforced.\n\tThe following operations are performed:\n\t\t\tWorkspace.set_name(workspace_name)\n\t\tworkspaceName.set_"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\n\tif not hasattr(cls, '_wsman'):\n \t   # Allowing for single workspace manager.\n\t#\t\tif not (cls._wsman and isinstance(cls._wsman, cls.Workspace)):\n \t\treturn None\n\treturn cls._wsman\n\n \n #\n # Get the default workspace manager.\n #\n def get_default(cls):\n\t\"\"\"\n\tObtain the default workspace manager.\n\t\"\"\"\n\n\tif not cls._wsman:\n \t\treturn None\n\t\n\treturn cls._wsman._wsmanager.get_default()\n\n\n#\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t return get_workspace_manager().get_provider(cls, 'workspace_manager')\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass WorkspaceManager(object) :\n\t\"\"\"\n\tObject storing all the workspace manager options.\n\t\"\"\"\n\tdef __init__(self) :\n\t\tself.__workspace_managers = []\n\n\tdef __contains__(self, workspace_manager_option) :\n\t\t\"\"\"\n\t\tReturns if a workspace manager has the specified option.\n\t\t:param workspace_manager_option:\n\t\t:return: the specified workspace manager or None\n\t\t\"\"\""}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\treturn PluginManager.instance()\n# vim:syntax=python fileencoding=utf-8:ts=4:sw=4:sta:et:sts=4:ai\n#!/usr/bin/env python\n# coding: utf-8\n\n#  Example usage:\n#      python dj_to_dict.py -s -l 10 -d 4 --limit 2\n#\n#             dj_to_dict.py -s -l 10 -d 4 --limit 20\n#\n# This is a short script that uses the NCBI North Way"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\treturn PluginManager.instance(cls)\n\nclass PluginInstanceManager(object):\n\t\"\"\"\n\tManages a plugin instance\n\t\"\"\"\t\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreates the plugin instance.\n\t\t\"\"\"\n\t\tself._plugins = []\n\t\tself._registered_plugins = []\n\t\n\n\tdef add_plugin(self, plugin):\n\t\t\"\"\"\n\t\t Adds a plugin instance\n\t\t\"\"\"\n\t\tself._registered_plugins.append(plugin)\n\t\t\n\tdef get_plugins(self):\n\t\t\"\"\"\n\t\tReturns a list of all registered plugins."}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\tglobal _PLUGIN_MANAGER\n\tif _PLUGIN_MANAGER is None:\n\t\t_PLUGIN_MANAGER = PluginManager()\n\treturn _PLUGIN_MANAGER\n\n\nclass PluginManager(object):\n\t\"\"\"\n\tA simple and easy way to get and set the plugins manager instance.\n\n\tIf you call `manager.get_plugin_manager()`. then it must be defined\n\twithin :mod:`octoprint.plugin.core`. Likewise, you can define your own manager\n\tinstead of using the `~octoprint.plugin.StartupPlugin` class directly.\n\n\tIf a plugin manager"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\t\n\tif cls.default_manager is None:\n\t\timport default_manager\n\t\treturn default_manager.plugins_manager\n\t\n\tif hasattr(_PluginManager, 'plugins'):\n\t\tdef getPluginInstance(name):\n\t\t\treturn _PluginManager.plugins.get(name.lower(), None)\n\t\t\n\t\treturn getPluginInstance\n\t\n\tplugin_manager = cls.default_manager._plugin_manager\n\t\n\tplugin_manager.plugins = []\n\tplugin_manager.plugin_classes = PluginManager.plugins_list\n\tplugin_manager.available_plugin_classes = plugin_manager.plugin_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn _pluginmgr\n\n\n#------------------------------------------------------------------------------\n# InteractiveShellApp(Configurable) configuration\n#------------------------------------------------------------------------------\n\n## A mixin for applications that start InteractiveShell instances.\n#  \n#  Provides configurables for loading extensions and executing files as part of\n#  configuring a Shell environment.\n#  \n#  The following methods should be called by the :meth:`initialize` method of the\n#  subclass:\n#  \n#    - :meth:`init_path`\n#    - :meth:`init_shell` (to be implemented by the subclass)\n#    - :"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\t##~~ Plug-ins\n\n\thooks[\"octoprint.plugins.softwaremanager.get.myplugins\"]    = [\"myplugins\"]\n\thooks[\"octoprint.comm.protocol.verisuregui\"] = [\"octoprint.comm.protocol.git.versioncheck\", \"octoprint.comm.protocol.git.version\", \"octoprint.comm.protocol.git.repository\"]\n\thooks[\"octoprint.comm.protocol.gcodetool\"]            = [\"octoprint.comm.protocol.gexecute.common.gitcommand\"]\n\t"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\t\n\tif not cls.__plugins:\n\t\tfrom octoprint.settings import settings\n\t\tfrom octoprint.plugins.pluginManager import PluginManager\n\t\tif not settings['server']['plugins']: # do not instantiate them\n\t\t\tfrom octoprint.server.util import rpcclient\n\t\t\tplugins_key = \"octoprint.server.plugins\"\n\t\t\tplugin_version = \".\".join(map(str, rpcclient.version()), settings['server']['plugins'])\n\t\t\t\n\t\t\tplugin_manager = PluginManager(key=plugins_key, version=plugin_version)\n\t\t\tcls.__plugins = plugin_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\nclass PluginManager_Stub(PluginManager):\n\t\"\"\"\n\tStub used to define plugin managers.\n\tThis is used in subclasses to define custom classes;\n\tthey are automatically called if a plugin manager\n\tis to be used to get its own plugins.\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.registry = {}\n\t\n\tdef addPlugin(self, pluginName, baseClass, baseObject):\n\t\t\"\"\"\n\t\tAdd a plugin class to this class.\n\t\t\n\t\tIf one plugin is not named, it is created and returned.\n\t\t\n\t\tKeyword arguments:\n\t\tpluginName"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\t# check to see if plugins folder exists\n\tplugins_folder = None\n\t\n\t# check for valid plugin folder\n\tfor plugin_folder in [ 'plugins', 'defaultplugins', '/etc/xinetd.d/plugins', '/usr/local/XinetD' ]:\n\t\tif os.path.isdir(plugin_folder):\n\t\t\tplugins_folder = os.path.abspath(os.path.expanduser(plugin_folder))\n\t\t\tbreak\n\t\n\tif not os.path.isdir(plugins_folder):\n\t\traise Exception(\"Cannot find plugins folder!\")\n\t\n\treturn cls("}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\ndef main(argv=None):\n\t\"\"\"\n\tGet a reference to the default Tortoisengine main file (or passed that is\n\talready there)\n\t\"\"\"\n\tprint \"This is a Tortoisengines main file.  Use --help for more information.\"\n\tprint \"Note: this is a very important command-line interface and may be used in the future.\"\n\tprint \"Please see help for more information.\"\n\n\tif argv is None:\n\t\targv = sys.argv\n\tif len(argv) == 0:\n\t\targv = sys."}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tlogger = logging.getLogger(__name__)\n\t\tlogger.debug(\"{0}\".format(spec_content))\n\n\t\tdata = cls.load(spec_content)\n\t\tif not data:\n\t\t\traise IRValidatorException(\"Spec File is invalid, can't validate\")\n\n\t\t# make sure the content exists\n\t\tif not cls.check_valid():\n\t\t\traise IRValidatorException(\"{0}\".format(data))\n\n\t\t# validate it\n\t\ttry:\n\t\t\tvalidator = get_validator(data)\n\t\t\tok = validator.validate(spec_content)\n\t\texcept ValidationError as e:"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Missing spec content\")\n\t\n\t\n\tfrom os import path\n\timport os\n\n\tdata = yaml_content\n\t\n\tif not isinstance(data, dict):\n\t\tlogger.error(\"spec_content should be dict\")\n\t\traise IRValidatorException(\"The spec file must be a valid YAML file.\")\n\t\t\n\tif not spec_content:\n\t\tlogger.error(\"data should include a spec file\")\n\t\tmessage = \"spec content is missing\"\n\t\traise IRValidatorException(message)\n\t\t\n\t\n\tfields = {}\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\ndef validate_file(cls, spec_content=None):\n\t\"\"\"\n\tValidates that path to spec_file matches spec filename\n\t\n\t:param spec_content: filename of spec file\n\t:raise IRValidatorException: when file extension is invalid\n\tis missing in spec file\n\t\"\"\"\n\text = os.path.splitext(spec_content)\n\tif ext.lower() not in ('.yaml', '.yml'):\n\t\traise IRValidatorException(\"spec file extension not recognized: %s\" % ext)\n\treturn spec_content\nfrom datetime import datetime\r"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\t# validate required fields\n\tvalid = super(IRValidator, cls).validate(spec_content)\n\t\n\tif not valid:\n\t\tmessage = 'invalid {0}: \\'{1}\\''.format(cls.__class__.__name__, spec_content)\n\t\traise IRValidatorException(message)\n\treturn cls().validate(spec_content)\nfrom __future__ import absolute_import\n\nfrom datetime import time, datetime\n\nfrom sentry.tagstore.v2.endpoints import Endpoint\nfrom sentry.utils.dates import convert_to_datetime\n\nfrom.base import"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\timport yaml\n\treturn yaml.safe_load(spec_content)\n\n@validate_from_content\ndef create_with(spec_file_name):\n\t\"\"\"\n\tcreates a yaml structure from spec file\n\t\"\"\"\n\tfrom octoprint.util import atomic_write\n\tfrom octoprint.settings import settings\n\tdata = yaml.safe_load(spec_file_name)\n\twith atomic_write(data, settings, default_flow_style=False) as printer:\n\t\tprinter.print_debug(printer.parse_debug_log)"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\t# check parameters\n\t# verify that spec_content is of type validations.Spec\n\tif not isinstance(spec_content, dict):\n\t\traise IllegalValueError(\"SPEC content is not of dict type\")\n\t\n\t# check that all mandatory fields are provided\n\n\trequired_fields = cls.required_fields\t\t\t\t\n\t\n\tfor field in required_fields:\n\t\t# verify that field is a dict type\n\t\tif not isinstance(field, dict):\n\t\t\traise IllegalValueError(\"field is not of dict type (expected a dict type)\")\n\t\t\n\t\t# verify provided field is a dict; raise"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n# pylint: disable-msg=W0403\n\n\ttry:\n\t\timport yaml\n\t\timport yaml.constructor\n\texcept ImportError:\n\t\traise IRValidatorException('PyYAML package is required for Validation.py:'+ spec_content)\n\n\tif yaml.__version__ >= '3.0':\n\t\tspec_content = yaml.dump({\"yaml_spec\": Spec.from_yaml(spec_content)})\n\t\tlogger.info(\"Converted YAML: %s\", spec_content)\n\n\tdata, errors = utils.validate(spec_content, cls.configuration.definitions)"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t    \n\t# Validate spec content, if mandatory check for that\n\trequired = ['doc_id']\n\n\t# Validate spec dictionary and throw exception\n\tmeta = {}\n\tfor key in required:\n\t\tvalue = spec_content.get(key)\n\t\tif value is None:\n\t\t\traise IRValidatorException('Required field %s must be defined.' % key)\n\t\tmeta[key] = value\n\tif meta['doc_id'] is not None:\n\t\tmeta['doc_id'] = str(meta['doc_id'])\n\n\tif not meta['doc_ref"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\t# validate required data\n\tvalid = {\"data\": {}, \"errors\": {}}\n\tfor path, parser in SpecParser().get_parsers().items():\n\t\trequired_fields = cls.parse_required_fields(path)\n\t\tfor field in required_fields:\n\t\t\tkey = field.get(\"name\")\n\t\t\tval = field.get(\"value\")\n\t\t\tif key not in spec_content:\n\t\t\t\traise IRValidatorException(\"missing required {} attribute {}\".format(key, val))\n\t\t\tif val is None and parser is not None:\n\t\t\t\tvalidated = validate_data_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n# -------------------------------------------------------------------\n# Main\n# -------------------------------------------------------------------\nif __name__ == \"__main__\":\n\t# Create a validator for the spec_root path (with no path)\n\tspec_validator = IRValidator(spec_root=os.getcwd())\n\tif not spec_validator.validate_spec_content():\n\t\tprint \"Spec validation failed with spec file content...\"\n\telse:\n\t\tsys.exit()#!/usr/bin/python\n\nimport os\nimport socket\nimport subprocess\nimport tempfile\nimport shutil\n\nimport config\nimport helpers as ht\nimport lvm"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n#TODO Add in irods.io.common as needed\n\n#Import the IR definitions that are included only for readability with the POS-MARS IR definitions.\n\n#Import the IR definitions that must appear in the pom/metadata file.\n\n#Import the PMI schema file.\n\nimport datetime\nimport numpy as np\nimport os\nimport pandas as pd\n\n#Import the irods file with date time\n\nimport pynbody\n\ni_time_now = datetime.datetime.now()\n\n#Import the IRS definitions that"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tif yaml_file:\n\t\tif not os.path.exists(yaml_file):\n\t\t\traise IRValidatorException(\"YAML file %s doesn't exist\" % yaml_file)\n\t\t\n\t\ttry:\n\t\t\t# Read file\n\t\t\tdata = open(yaml_file).read()\n\t\t\t# Validate data\n\t\t\tif not len(data) > 0:\n\t\t\t\traise IRValidatorException(\"Invalid YAML content\")\n\t\t\telif yaml_file[-3:]!= \"i\":\n\t\t\t\traise IRValidatorException(\"Invalid YAML content (wrong order)\")\n\t\t\t# Validate contents\n\t\t\treturn yaml"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n#    parser = None\n#\n#    if yaml_file is not None:\n#\t    with open(yaml_file, 'r') as stream_handle:\n#\t\t\n#            try:\n#                parser = yaml.load(stream_handle)\n#                validate_data = parser.get(\"validate\", {})\n#\n#            except yaml.YAMLError as error_message:\n#                if error_message == \"YAML file is not well formed\":\n#                    raise IRValidatorException(\"YAML file is not well formed\")\n#                if \"data\" in"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tif not yaml_file:\n\t\traise IRValidatorException(\"YAML file must be given!\")\n\t\n\tdata_loaded = {}\n\t# create an empty dictionary that will be filled with data\n\t# data are stored in json array\n\t# The keys in this dictionary is the class name,\n\t#  value is attribute from the dict\n\tdata = {}\n\t\n\tdef __check_data(class_name, data, data_type, data_name, data_key=None, data_list=None):\n\t\t\"\"\"\n\t\tRecursivly get or create a data object"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\ndef validate_from_dict(cls, yaml_dict):\n\t\"\"\"\n\tLoads & validate that a dictionary has all required fields\n\t\n\t:param yaml_dict: Dictionary with data loaded from a YAML file\n\t:raise IRValidatorException: when mandatory data is missing in dictionary\n\t:return: Dictionary with data loaded from a YAML dictionary\n\t\"\"\"\n\tpass\n\ndef validate_from_list(cls, yaml_list):\n\t\"\"\"\n\tLoads & validate that a list has all required fields\n\t\n\t:param yaml_list: List with data loaded from a YAML file\n\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\treturn yaml_file, cls.validate_from_file_schema(yaml_file)\n\ndef validate_from_file_schema(validator_name):\n\t\"\"\"\n\tLoads a YAML file, given a validator name\n\t\n\t:param validator_name: Validator name\n\t:rtype: dict\n\t\"\"\"\n\timport re\n\timport inspect\n\t\n\timport re\n\tfrom itertools import chain\n\t\n\tschema = import_string(validator_name)\n\t\n\tif not callable(schema):\n\t\traise IRValidatorException('Validator %s was not callable' % validator_name)"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tyaml_dict = cls._get_dict_from_file(yaml_file)\n\trequired_keys = [\n\t\tu'maps',\n\t]\n\t\n\tclass Error(Exception):\n\t\t\"\"\"\n\t\tAn error occurred while trying to validate the data\n\n\t\t:param mandatory_data: Metadata for an attribute has not found\n\t\t:param mandatory_data_type: Attribute type provided does not exist\n\t\t:raise: IRValidatorException if metadata was not valid\n\t\t\"\"\"\n\t\tpass\n\t\n\tdef __init__(self, mandatory_data=None, mandatory_data_type="}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tyaml_data = load(yaml_file)\n\n\n\traise_error_if_not_schema(yaml_data)\n\n\tif \"type\" not in yaml_data:\n\t\traise IRValidatorException(\"YAML file does not define \" + yaml_file)\n\n\ttype_schema = yaml_data[\"type\"]\n\tname_schema = yaml_data[\"name\"]\n\n\tvalidation_schema = \"type-validation\"\n\tvalidation_class = IRValidator\n\n\tif type_schema not in [\"string\", \"email\", \"list\"]:\n\t\traise IRValidatorException(\""}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\t##\n\t# IR data validation\n\t#\n\t\n\t##\n\t## get required data\n\t##\n\t## @param data: Dictionary with valid data\n\t## @return True if valid, raises exception otherwise\n\t##\n\tdef validate(self, data):\n\t\t\"\"\"\n\t\tValidates the IR data\n\t\t\n\t\t:param data: Dictionary with valid data\n\t\t:return: Boolean\n\t\t\"\"\"\n\t\t# Make sure our data is valid\n\t\timport yaml\n\t\tvalid = True\n\t\tmsg = yaml.safe_dump(data, default_flow_style=False,"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tvalid_errors = {}\n\t\n\tif not yaml_file:\n\t\tload_file = cls.load_file(yaml_data)\n\t\tif not isinstance(load_file, basestring):\n\t\t\traise IRValidatorException('Not a YAML file: %s' \\\n\t\t\t\t% yaml_file)\n\t\telse:\n\t\t\tif yaml_data.startswith('!!'):\n\t\t\t\traise IRValidatorException(\"YAML version %s is obsolete\", \\\n\t\t\t\t\t\t\"Found unexpected YAML format version(s): %s\" \\\n\t\t\t\t\t\t% (yaml_data.split('>##')[-1],"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n#===============================================================================\n# Parse the spec file.\n#===============================================================================\n\ng_parsers = {}\ng_parsers[DSM_DEFINITION] = re.compile(r'^\\s*([0-9A-Za-z_/.]+)\\s(0x[0-9a-fA-F]+)', re.IGNORECASE)\n#g_parsers[DSM_DEFINE] = re.compile(r'^(\\w+)=([\\s\\w]+),')\ndef _parse(spec_arg):\n\t\"\"\"\n\tParses the spec file and returns the constructed list of groups\n\t\"\"\"\n\t\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\ndef _set_type(name):\n\tif args.type:\n\t\treturn \n\telse:\n\t\treturn ''\n\nclass Selection(object):\n\t\"\"\"\\\n\tA selection pattern.\n\t\"\"\"\n\t\n\tdef __init__(self, name, sel1={}, sel2={}):\n\t\tself.name = name\n\t\tself.sel = sel1\n\t\tself.sel2 = sel2\n\n\tdef __repr__(self):\n\t\treturn '<SEL %s(%s) %s>' % (self.name, self.sel, self.sel2 if self.sel2"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tdef find_files(self, path, pattern):\n\t\t\"\"\"\n\t\tGet all files matching match(path pattern)\n\t\t\"\"\"\n\n\t\tfor (dirpath, dirnames, filenames) in os.walk(path):\n\t\t\tfor filename in fnmatch.filter(dirnames, pattern):\n\t\t\t\tyield filename\t\t\t\t\n\n\n\tdef process(self, filename):\n\t\t\"\"\"\n\t\tProcess the file\n\t\t@param filename file to process\n\t\t\"\"\"\t\n\n\t\ttry:\n\t\t\ttree = ET.XML(filename)\n\t\texcept ET.ParseError as err:\n\t\t\tlog(\"Error: %s\" %"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tdef filter_groups(self, group_list, file_list, is_included=False, file_type='include'):\n\t\t\"\"\"\n\t\tFilters the files of files that are not directly mentioned in groups or\n\t\tif they are not. If the include is 'only' one that matches any of the\n\t\tinclude directives in the groups is included. If the include is 'only'\n\t\tthe file is included if it matches any of the include directives in the\n\t\tgroups.\n\t\t\"\"\"\n\t\tinclude_groups = []\n\t\tfull_include_paths = []\n\t\t\n\t\tif is_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\t\n\t# parse 'include' line\n\tdef _include(self, line):\n\t\tsourcename=line.strip()[:-1]\n\t\tsourcenames=[]\n\t\tsourcenames=filter(lambda s: s.startswith(\"include\"), sourcenames)\n\t\t\n\t\tfor sourcename in sourcenames:\n\t\t\tif (sourcename in self.section_order):\n\t\t\t\tif (sname+'(' not in self.conf) and (name in self.section_order):\n\t\t\t\t\tself.conf.append(sname+'(\"'+name+')')\n\t\t\t\telse"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tif not isinstance(parser_dict, dict):\n\t\treturn\n\tparser = self._parser.copy()\n\tparser_dict = parser_dict.copy()\n\tfor key, item in parser_dict.items():\n\t\tif hasattr(item, KEY_GROUP_DEFAULT):\n\t\t\tgroup_def = '%s_DEF_%s' % (parser.name, key)\n\t\t\tparser.set_defaults(**{\n\t\t\t\t'group_default' : group_def, \n\t\t\t\t'group_list' : items(item, self.group_list),\n\t\t\t\t'group_def"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tif self.__class__.__name__ == \"Group\":\n\t\treturn_list = []\n\t\tfor i,item in enumerate(self.list):\n\t\t\treturn_list.append( (i,item) )\n\t\tself.include_groups = \" \".join(return_list)\n\t\n\n\nclass IncludeGroups(Directive):\n\t\"\"\"\n\tAdds the include groups to the file\n\t\"\"\"\n\n\tdef handle_command(self):\n\t\tinclude_groups = self.arguments[0]\n\n\n\t\tself.include_groups = _include_groups(self.parser_dict)"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\t# Define a mapping of group names to commands for the extension.\n\tgroup_commands = {\n\t\t'read': Command(\"read\", \"Reinitialize\", [\"command\"]),\n\t}\n\n\t# Define a mapping from command names to their handler function objects.\n\text_handlers = {\n\t\t'init': CBCommandHandler(),\n\t\t'remove': CBRemoveHandler()\n\t}\n\n\t# Define the default extension parser.\n\tdefault = {\n\t\t'parser': {\n\t}\n\n\n\t# Define a mapping from command names to their respective handler function names.\n\t"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\t\n\t# Define the arguments under which the include section\n\t# resides.\n\tself._use_include = None\n\tself._use_exclude = None\n\tself._include_groups_map = None\n\tfor section in parser_dict['include']:\n\t\tif self._include_groups_map and section!= \"Group\": continue\n\t\tif self._use_include is not None:\n\t\t\tinclude_options = parse_include_options(\n\t\t\t\tsection,\n\t\t\t\tself._use_include,\n\t\t\t\tself._use_exclude,\n\t\t\t\traise_on_error = True)"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tself.section_order = []\n\tsection_header = None\n\n\tfor entry in parser_dict.keys():\n\t\tdef_text = parser_dict[entry].strip()\n\t\tif def_text == \"\":\n\t\t\tcontinue\n\t\telif def_text[0] == \"@\": # Skip comments\n\t\t\tcontinue\n\t\telif self.section_parser(def_text):\n\t\t\tself.section_order = [ self.section_parser(entry) ]\n\t\telif self.section_comment(def_text):\n\t\t\tsection_header = entry\t\n\t\t\t\t\n\tself.section"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tp = SpecParser(shlex.split(self.spec_file), self.specs, [], self.options, self.action)\n\t\tif self.options.type == \"env\":\n\t\t\tenv_values = {\n\t\t\t\t\"OSM\": self.options.osmc,\n\t\t\t\t\"ARCH\": self.options.arch,\n\t\t\t\t\"MAKEFILE\": os.path.join(self.options.outdir, \"Makefile.nexe\"),\n\t\t\t\t\"ARCHFLAGS\": env_options(self.options.osmpath, \"ENV\", self.options.arch, env_options_nix"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t self._spec = {}\n\t\tif SCons.Util.is_List(self.sources):\n\t\t\tfor s in self.sources:\n\t\t\t\tself._add_silent(s)\n\n\tvars = prop_dict(prop_dict(self._properties.copy()))\n\tfor key, val in self.env.items():\n\t\tval = str(val)\n\t\tif '%(' not in val:\n\t\t\tval += '\\n  %s: %s\\n' % (env_var_name(key), val)\n\t\tself._add_silent(key)\n\t\tself._"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t self.set = self.defaultset\n\tdef get_keyword_defaults(self):\n\t\tself.kwlist = filter(lambda x: x[:8]!=\"_\", self.kwlist)\n\t\tself.pos = 0\n\t\tif hasattr(self.imp, 'end'):\n\t\t\tself.imp.end = self.imp.end\n        return self\n\tdef set_arg_types(self):\n\t\tself.imp.end = self.imp.start\n\tdef search_one(self, args):\n\t\tfor i in args:\n\t\t\ttry:\n\t\t\t\tif self"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n#-------------------------------------------------------------------------------\n\nclass BaseBuilder:\n    \"\"\"\n\tInterface to the build steps builder\n\t\"\"\"\n    def build_step_str(self, src=None):\n        \"\"\"\n\tReturn a source string representing the specified step.\n\tIf no source string is specified a temporary file is produced.\n\tWhen invoked from the outside code, the output file is written\n\tinto that named file.\n\t\n\t    Returns None if the build is requested and it cannot be created.\n       \t\"\"\"\n        return ''\n\n\n#-------------------------------------------------------------------------------\n#  BuildFactory\n#-------------------------------------------------------------------------------\n\nclass BuildFactory:\n    \"\"\"\n\tDefault"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n\t\n\tdefaults = property(_get_defaults, _set_defaults)\n\n\tdef _get_defaults(self):\n\t\timport re\n\t\tkw_str = \"\"\n\t\t\n\t\tfor line in self.spec:\n\t\t\tline = line.strip()\n\t\t\tif not line:\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif line[0:2] == \"*/path\":\n\t\t\t\tkw_str = kw_str + \"remote.\" + self.remote[1:]\n\t\t\t\tline = \"/\".join(line.split())\n\t\t\telif line[0:2] == \"*/dest\":\n\t\t\t\tkw_str ="}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tif self.build_system =='waf':\n\t\t\tself.get_waf_vars(self.source)\n\t\t\tself.all_envs.update(self.all_envs.get(self.for_machine,{}).get(self.variant,{}).get(self.language,{}))\n\t\t\tself.all_envs.update(self.all_envs['build'].get(self.variant,{}))\n\t\tfrom utils import Utils\n\t\tp=Utils.subst_vars(self.locals,\"p\")\n\t\tp.update(self.env)\n\t\tself"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tspec = {}\n\t\tsource = {}\n\n\t\tspec['src'] = self.path\n\t\tspec['dep'] = []\n\n\t\tif self.__path:\n\t\t\tspec['dest'] = self.__path\n\n\t\tfor x in self:\n\t\t\tif x in spec:\n\t\t\t\tspec[x].update(source[x])\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\t\tif not x in source:\n\t\t\t\tsource[x] = {}\n\n\t\t\tspec['src'][x] = source[x]\n\n\n\t\treturn spec\n\n\n\tdef resolve_tool_source(self):\n\t\t\"\"\"Resolve any references"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n#\t\tprint(self._args)\n\t\targ_specs = self._args.values()\n\t\t\n\t\t# convert types\n\t\targ_types = [a.strip() for a in arg_specs]\n\t\t\n\t\targ_keys = [a.strip() for a in arg_specs]\n\t\t\n\t\tspec_keys = [a.strip() for a in arg_specs]\n\n\t\tfor t,key in enumerate(spec_keys):\n\t\t\ttry:\n\t\t\t\tval = self.opts[t][key]\n\t\t\texcept KeyError:\n\t\t\t\ttry:\n\t\t\t\t\tval = args_spec_defaults"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\t# NOTE: The spec requires a list of sources if they need to be\n\t\t# resolved. If it's not, a source is required but that it will do\n\t\t# relative to the list of sources supplied.\n\t\treturn []\n\n\tdef resolve(self, args):\n\t\t\"\"\"\n\t\tResolve sources specified by args. Return list\n\t\tof sources that are needed (or None) for each.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\t@staticmethod\n\tdef get_arg_spec(args, kwargs):\n\t\targ_spec = deepcopy(self.get_arg_spec_defaults())"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n\n\t\t# Set defaults\n\t\tself.cfg.setdefault(\"paths\", [])\n\t\tself.cfg[self.target]            = []\n\t\tself.cfg[\"directories\"]      = []\n\n\t\t# Check all specs for all given options\n\t\tfor key in self.cfg[\"args\"]:\n\t\t\tfor target in self.cfg[\"targets\"]:\n\t\t\t\tpath = self.get_target(target)\n\t\t\t\tif path is not None:\n\t\t\t\t\tif self.path_exists(path):\n\t\t\t\t\t\tself.cfg[\"remove_args\"].append(key)\n\t\t\t\t\t\tself.cfg[\"delete_args\"].append"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated_params = {}\n\t\tdeprecated_params.update(dict(\n\t\t\tdeprecated_param_description=None,\n\t\t\tdeprecated_param_name=None,\n\t\t\tdeprecated_param_value=None,\n\t\t\tdeprecated_return_value=None,\n\t\t\tdeprecated_return_doc=None,\n\t\t\tdeprecated_return_arg=None\n\t\t))\n\n\t\tdeprecated_param_type = {\n\t\t\t'Dynamic Link': 'Dynamic Link',\n\t\t\t'Dynamic Link Context': 'Dynamic Link Context',\n\t\t\t'Dynamic Input Prefix': 'Dynamic Input Prefix',\n\t\t\t'Dynamic Input Prefix Context"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tresult = {}\n\n\t\tif not hasattr(self, 'deprecated_args'):\n\t\t\tif '-' in self.block:\n\t\t\t\tdeprecated_args = frappe.db.sql(\"select `tabDeprecated` from `tabLegacy`\")\n\t\t\t\tresult = {\n\t\t\t\t\t'header': [_('Deprecated') +'(DEPRECATED]'+ deprecated_args)\n\t\t\t\t}\n\t\t\telse:\n\t\t\t\tresult = frappe.db.sql(\"select `tabDeprecated` from `tabUser` where parent=%s\", self.user)\n\t\treturn result\n\n\t@property\n\tdef user(self):\n\t\tif frappe"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated = {}\n\t\tdeprecated['uselib_store'] = self.__class__.uselib_store\n\t\tdeprecated['lib_store'] = self.__class__.lib_store\n\t\tdeprecated['ffi_validate'] = self.__class__.ffi_validate\n\n\t\tdeprecated.update(self.__class__.deprecated)\n\t\tdeprecated.update(self.uselib_store)\n\t\tdeprecated.update(self.ffi_validate)\n\t\tdeprecated['ffi_sign'] = self.__class__.ffi_sign\n\n\t\treturn deprecated\n\t\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor entry in self._deprecated_options:\n\t\t\tif entry in DEPRECATED_OPTIONS:\n\t\t\t\tif entry not in DEPRECATED_OPTIONS and entry not in DEPRECATED_OPTIONS.get(entry) \\\n\t\t\t\t\t\t\tand entry in DEPRECATED_OPTIONS.get(entry, {}):\n\t\t\t\t\tdeprecated_args[entry] = DEPRECATED_OPTIONS[entry]\n\t\treturn deprecated_args\n\n\tdef _merge_deprecated_options(self):\n\t\tfor entry in self._deprecated_options:\n\t\t\tif entry in self._deprecated_options and \\\n\t\t\t\tentry!= \"versions\" and entry"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor k in self.arguments:\n\t\t\tif self.arguments[k] is not None:\n\t\t\t\tif k in DEPRECATED_TYPES:\n\t\t\t\t\t# We have to remove duplicates from deprecated argument lists\n\t\t\t\t\tdeprecated_args[k[len(\"deprecated_\"):]] = self.arguments[k]\n\t\treturn deprecated_args\n\n\tdef get_option_values(self):\n\t\t\"\"\"\n\t\tReturn an array of optparse values.\n\t\t\"\"\"\n\t\treturn [o for o in self._options]\n\n\tdef format_help(self):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor arg_name in dir(self):\n\t\t\tif \"_\" + arg_name[0:1] == \"_\":\n\t\t\t\tcontinue\n\n\t\t\tdeprecated_args[arg_name] = self[arg_name]\n\n\t\treturn deprecated_args\n\n\tdef append_deprecation_line(self, message):\n\t\tfrom textwrap import dedent\n\n\t\tif self.deprecated_args.get(message):\n\t\t\tdoc = \"!!! DEPRECATED\"\n\t\t\tif hasattr(self, 'deprecated_args_deprecated'):\n\t\t\t\tdeprecated_args = self.deprecated_args_deprecated\n\t\t\t\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn dict(\n\t\t\toptions_deprecated = dict(\n\t\t\t\tdisable_lines = 1,\n\t\t\t\tadd_newline = 1,\n\t\t\t\tindent = 1,\n\t\t\t\tno_color = 0,\n\t\t\t\twrap = 1,\n\t\t\t\thelp = 1,\n\t\t\t\tsilent = 1\n\t\t\t)\n\t\t)\n\ndef load_plugins():\n\tfrom octoprint.plugin import plugin_manager\n\tglobal __all__\n\t\n\tparser = argparse.ArgumentParser(add_help = True, description = _(\"''\\\n\tPlugins\n\t================\n\t- Plugins are a way of not having configuration files on"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\ndef deprecated_args(fn):\n\t\"\"\"\n\tDescribe the parameters of a deprecated function\n\t\"\"\"\n\tf = inspect.getfullargspec(fn)\n\targs = []\n\tfor arg in f.args:\n\t\tif arg in deprecated_args.__dict__:\n\t\t\tdel deprecated_args.__dict__[arg]\n\t\t\targs.append(deprecated_args.__dict__[arg])\n\treturn args\n# -*- coding: utf-8 -*-\n# Generated by Django 1.10.4 on 2017-12-24 00:34\nfrom __future__ import unicode_literals\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn {\n\t\t\t\"'--version\" : \"deprecated\",\n\t\t\t\"--deprecated\" : \"True\",\n\t\t}\n\n\tdef ensure_deprecated_options(self, config):\n\t\t\"\"\"\n\t\tCheck that deprecated options are valid. This is\n\t\tcurrently only used for the\n\t\t[config] section of the config.\n\t\t\"\"\"\n\t\tif not config.deprecated:\n\t\t\treturn\n\n\t\toptions = self.get_deprecated_options()\n\t\tfor option, (default, arg) in options.items():\n\t\t\tif option in config.deprecated:\n\t\t\t\tparam = arg.replace(\"_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn {\n\t\t\t'docuser': {\n\t\t\t\t\t'action':  'store_true',\n\t\t\t\t\t'deployer':'store_true',\n\t\t\t\t\t'note':      'The documentation for this tool/application.',\n\t\t\t\t\t'objtype':   'object',\n\t\t\t\t\t'name':    '',\n\t\t\t\t\t'filename':    '',\n\t\t\t\t\t'readonly':  True,\n\t\t\t\t\t'disabled':  False,\n\t\t\t\t\t'mandatory': True}},\n\t\t\t'semane': {\n\t\t\t\t\t'action':  'store_true',\n\t\t\t\t\t'deployer':'store_true',"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t \n\tcli_args['arg_help_text'] = cli_args['argument_help_text'] +''\n\targs = vars(cli_args)\n\tfor arg in args:\n\t\thelp_text = arg.replace('\"', r'\\\"')\n\t\tif cli_args[arg] == '?':\n\t\t\tdoc = 'Deprecated: %s. Use argument --%s to get help.'\\\n\t\t\t          'See documentation for valid options.' \\\n\t\t\t          % (arg, arg)\n\t\t\tfor d in dir(os.path.realpath(doc)):\n\t\t\t\tif d.startswith"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\n\tif not self.debug:\n\t\treturn\n\tvalid_arg_keys = [arg_key for arg_key in answer_file_args.keys() if arg_key[:2]!= '__']\n\tfor i in valid_arg_keys:\n\t\tprint \"Warning: Argument arg %s is deprecated and will be removed from configuration.\" % i\n\tprint\n\t\n\tif self.is_debug_output:\n\t\tsys.stdout.write(\"\\n\")\n\t\tsys.stdout.write(\"{}\\n\".format(\"\\n\".join(answer_file_args)))\n\t\tsys"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tif cli_args[\"--config_file\"] is None:\n\t\tself.logger.fatal(\"Error: Missing --config_file argument.\")\n\t\tsys.exit(1)\n\t\treturn\n\t\n\tif cli_args[\"--verbose\"]:\n\t\tself.logger.setLevel(logging.DEBUG)\n\telse:\n\t\tself.logger.setLevel(logging.INFO)\n\t\n\tconfig_data = load_configs(cli_args[\"--config_file\"])\n\t\n\tif cli_args[\"--debug\"]:\n\t\tself.print_config(cli_args, answer_file_args)"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n#TODO: remove these: \"execute\"\n\t#TODO: remove this: get rid of unreachable code (as it can be done in a different language)\n\t#TODO: test and set it to something else for safety\n\tdef _format_deprecation(self, cli_args, answer_file_args):\n\t\t#print \"WARNING: \", cli_args\n\t\t#print \"WARNINGS:\", answer_file_args\n\t\tpass\n\n\tdef _run(self, cli_args):\n\t\t\"\"\"\n\t\tRun the CLI.\n\t\t\n\t\t:param cli_args: the parsed CLI"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\ndef validate_args(cli_args, answer_args):\n\t\n\tif not cli_args or not answer_args:\n\t\treturn False, None\n\tif len(cli_args)!= 2 or not isinstance(cli_args[1], dict):\n\t\treturn False, None\n\tif len(cli_args)!= 3:\n\t\treturn False, None\n\telif len(cli_args) == 3:\n\t\treturn validate_arg_deprecation(cli_args[2], answer_args[2])\n\treturn True, None\n\ndef init_cli(cli_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tmessage =''.join([cli_args['cli'], answer_file_args['cli']], \n\t\t           file=self.cli_details, file_content=answer_file_args['cli_desc'])\n\t\traise self.cli_errors.ArgumentDeprecated(\"{0} (See docs).\".format(message))\n#!/usr/bin/python\n\n# Copyright (c) 2015,  BROCADE COMMUNICATIONS SYSTEMS, INC\n\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\ndef check_api_name(fnapi):\n\t\"\"\"\n\tChecks if the function description is unique.\n\t\n\t:param fnapi: the name of the function\n\t\"\"\"\n\tfn = fnapi.split(\".\")\n\tname = fn[0]\n\n\tllen = len(name)\n\n\tif llen == 1 and name[0] == 'help':\n\t\treturn False\n\n\telif llen > 3 and name[1] == 'help':\n\t\treturn False\n\treturn True\n\ndef check_api_version(api):\n\t\"\"\" Checks if the API version"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\tif len(cli_args)!= len(answer_file_args):\n\t\t\tprint(\"The number of positional arguments in the arguments dictionary \"\n\t\t\t\t            \"({}) don't match the number of keyword \"\n\t\t\t\t            \"arguments ({}!= {}).\".format(\n\t\t\t\t            cli_args, len(answer_file_args), len(cli_args)))\n\t\t\tprint(\"The number of keyword arguments in the argument dictionary \"\n\t\t\t\t            \"({}) don't match the number of keyword \"\n\t\t\t\t            \"arguments ({}!= {}).\".format(\n\t\t\t\t\t\tanswer_file_args, len(cli_args), len(answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tif '--no-color' in cli_args and not answer_file_args.get('--nocolor', 0):\n\t\tself._raise_help_deprecated('The --no-color flag is no longer supported')\n\t\n\tif '--color' in cli_args and not answer_file_args.get('--color', 0):\n\t\tself._raise_help_deprecated('The --color flag is no longer supported')\n\t\n\tif '--style' in cli_args and not answer_file_args.get('--style', 0) and answer_file_args.get('--stylesheet', 0):"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tif cli_args.deprecation_message is not None:\n\t\tfor arg in cli_args.deprecation_message:\n\t\t\tself._warn('%s' % arg)\n\ndef main():\n\tparser = argparse.ArgumentParser(description='SvnCheck: The command line interface of SvnCheck utility')\n\tparser.add_argument('-v', '--verbose',\n\t\t\t\t\t\thelp='Enable verbose messages',\n\t\t\t\t\t\taction='store_true'\n\t\t\t\t\t\t)\n\tparser.add_argument('-d', '--debug',\n\t\t\t\t\t\thelp='Enable debugging messages',\n\t\t\t\t\t\taction='store_true'"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\treturn self.optparser.get_all(command_name)\n\t\n\tdef is_option_set(self, option_name):\n\t\t\"\"\"Return true if the specific option is set\"\"\"\n\t\treturn self.optparser.get_option_set(option_name)\n\t\t\n\t\n\tdef parse_command_line(self, argv = None):\n\t\t\"\"\"\n\t\tParse the command line and return options as a dictionary\n\t\tThe dictionary has the following layout:\n\t\t - option_name : the name of the the option and its value\n\t\tThe option_name will have the name of the option"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t return [\n\t\tOption(\"-o\", \"--option\", dest=\"key_value\",\n\t\t\t   help=\"additional option with \\\"key=value\\\"\"),\n\t\tOption(\"-i\", \"--info\", dest=\"info\", \n\t\t\t   help=\"show some information about this cmd\"),\n\t\tOption(\"-d\", \"--debug\", dest=\"debug\", action='store_true'),\n\t\tOption(\"-v\", \"--version\", dest=\"version\", type=\"int\",\n\t\t\t   help=\"version of the plug-in to grab info; 0 is unlimited\"),\n\t\tCommandOption(name=\"--node\",\n\t\t\t\thelp=\"path to node device,"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\t(opts, \n\t\targs, \n\t\tcommand_name, \n\t\tparser, \n\t\tinput_params, \n\t\toutput_params, \n\t\tretreiving_err, \n\t\tretreiving_command,\n\t) = (self.command_name, self.parser, self.input_params, self.output_params, self.retreiving_err, self.retreiving_command)    \n\t\n\tresult = parser.parse_args(args)\n\tif not result:\n\t\t# No arguments -> show version\n\t\tparser.print_help()\n\t\tsys"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\t# We can not just use list, since there's only\n\t# two. What's the point? Is there a better test than this.\n\tcmdline = subprocess.check_output(self.command_cmdline + [command_name, self.options])\n\tcmd = shlex.split(cmdline, comments=True)\n\tif isinstance(cmd, list) and len(cmd) > 1:\n\t    cmd = cmd[1]\n\tif not cmd:\n\t# we assume only one option\n\tcmd = ''\n\tcmd = cmd.split(self.options)\n\tcmd"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\treturn self.cli_command_options(command_name)\n\n\tdef cli_command_options(self, command_name):\n\t\t\"\"\"\n\t\tRetrieves all the options for the specified command\n\t\t\n\t\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t\t:return: list of all command options\n\t\t\"\"\"\n\t\tif command_name not in self.cmd_options().keys():\n\t\t\traise ValueError, \"unknown command name\"\n\t\telse:\n\t\t\treturn self.cmd_options()[command_name]\nimport inspect\n\n\ndef is"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\tr = self.client.CMD_GET_CLI + [command_name]\n\t\t# make sure there is only one\n\t\tfor i in range(1, len(r)):\n\t\t\tif six.text_type(r[i]) == six.text_type(\" \"):\n\t\t\t\treturn \"\\t\"+r[i]\n\t\treturn None\n\n\tdef get_options(self, command_name):\n\t\tif self.is_running(command_name):\n\t\t\t# we do not support any options for this command\n\t\t\t\n\t\t\tself.client.stop()\n\t\t\t\t\n\t\treturn OptionParser.get"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\tparsed_command_line_parameters = None\n\tparams = self.get_full_options(command_name)\n\t\n\t\n\tfor param in params:\n\t\ttry:\n\t\t\tkey, val  = param\n\t\t\t# if not key: continue\n\t\t\tif val: parsed_command_line_parameters = key, val\n\t\n\t\texcept ValueError, e: pass\n\t\t\n\treturn parsed_command_line_parameters\n# Copyright (C) 2019 ycmd contributors\n#\n# This file is part of ycmd.\n#\n# ycmd is free software: you can redistribute it and/"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\tcmd = [self.base_cmd]\n\tcmd += [command_name]\n\treturn parse_kv_file(cmd, 0)\n\n\n# This is the main entry point of the script\ndef main(argv=None):\n\tif argv is None: argv = sys.argv\n\t# We first need to parse the args to see if the\n\t# command has been registered. If it hasn't it's\n\t# a help command to run it from the command line,\n\t# run it from sys.argv.\n\tif len(argv) == 1:\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\n\tdef get_parser(self):\n        \"\"\"\n\t\tThe parser instance associated with the tool\n\t\tReturns: a valid argparse.OptionParser() instance\n\t\t\"\"\"\n        # We will have 2 types of tool commands - both those from the\n        # argparse namespace:\n        #   help                   : parser function\n        #   version                   : parser function\n        return self\n\n\n\n# Basic class to hold the options from a user specified parser and\n# an option group. This class is heavily based on the use of optparse and the\n# argparse built-in classes. This will provide us the option class\n# from the parser"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t     \n\tdef get_parser(self, command_name):\n\t\t\"\"\"\n\t\tGet the options object for the specified command\n\t\t\n\t\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t\t:return: the command's options\n\t\t\"\"\"\t\t\t\n\t\treturn (self.commands[command_name], None)\n\t\n\t\t\t\n\tdef prepare_args_for_options(self, options_list):\n\t\t\"\"\"\n\t\tPrepare all the options for the specified command. Called during the initialization\n\t\tof the options list of the class.\n\t\t\n\t\t:param options"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn self.parser.option_spec.get(command_name.replace('-', '_'), '').split('.', -1)\n\n\tdef create_parser(self, prog_name, parser_name, parser_description, prog_options, usage, description):\n\t\t\"\"\"\n\t\tReturns the parser object that will be used for processing the option.\n\t\t\"\"\"\n\t\treturn ArgumentParser(prog=prog_name, formatter_class=RawDescriptionHelpFormatter, \n\t\t\t\t\t\t\t\t\t usage=usage, description=description, \n\t\t\t\t\t\t\t\t\t prog_options=prog_options)\n\n\tdef add_options(self, parser):"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn self.command_names[command_name].get(argument_name)\n\n\tdef set_option(self, command_name, argument_name, value):\n\t\tcommand = self.command_names[command_name].get(argument_name)\n\t\tsetattr(command, argument_name, value)\n\n\tdef remove_option(self, command_name, argument_name):\n\t\tcommand = self.command_names[command_name]\n\t\tif argument_name in command.arguments.keys():\n\t\t\tdel command.arguments[argument_name]\n\n\tdef get"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef _get_command(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGet the string with name |command_name| that corresponds to `argument_name`.\n\t\t\"\"\"\n\t\tcommand_name = self.sanitize_name(command_name)\n\t\targument_name = self.sanitize_name(argument_name)\n\n\t\treturn 'command {0}'.format(command_name) +'' + argument_name\n\n# -*- coding:utf-8 -*-\n#\n#    Copyright 2013 Cloudscaling Group, Inc\n#\n#    Licensed"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\tif self.options and self.options[command_name].has_key(argument_name):\n\t\t\treturn self.options[command_name][argument_name]\n\t\telse:\n\t\t\traise KeyError, (command_name, argument_name)\n\n\n\tdef get_option(self,command_name,argument_name):\n\t\t\"\"\"\n\t\tGets the specified option value.\n\t\t\n\t\tcommand_name is the name of the command to find the option.\n\t\targument_name is the name of the argument.\n\t\t\"\"\"\n\t\tkey = (command_name,argument_name)\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn self.__command_configs[\"get\"][command_name][\"params\"][argument_name]\n\t\n\tdef __getitem__(self, command_name):\n\t\t\"\"\"\n\t\tTakes in a command name, returns the value of that option\n\t\tin this command.\n\t\t\"\"\"\n\t\treturn self.__command_configs[command_name][\"params\"][True]\n\t\n\tdef __contains__(self, argument_name):\n\t\t\"\"\"\n\t\tChecks to see if the specified argument_name is contained within\n\t\tthis command. Returns False if not.\n\t\t\"\"\"\n\t\treturn argument_name in self.__command_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t self.log.info(\"Get a module for a specific module\")\n\t\ttry:\n\t\t\tspec = self.commands[command_name](self, \"%s_options\" % (command_name))\n\t\texcept:\n\t\t\treturn False\n\t\tif arg_name in spec.positional_args:\n\t\t\treturn spec.positional_args[arg_name]\n\t\treturn spec\n\n\tdef get_argument_spec(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified argument name.\n\t\t\"\"\"\n\t\tself.log.info(\"Get a module"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\n\n\tcommand_options = configuration.get(command_name, {}).get(argument_name, [])\n\tfor option_spec in command_options:\n\t\tif (command_option_syntax is not None) and (len(split_option_syntax(option_spec)[0]) > 2):\n\t\t\toption_name = (option_spec[:2]).split(',')\n\t\t\toption_spec = option_spec[2:]\n\t\n\t\n\tif (command_name == \"<init>\"):\n\t\treturn\n\tif (command_name == '<init-self>'):\n\t\tself.initialize_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.options[command_name, argument_name]\n\t\texcept KeyError:\n\t\t\traise KeyError(command_name + \" \" + argument_name + \" not found\")\n\t\n\tdef has_option_spec(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tRetrieves the specification for the specified option name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tretval = self.options[command_name, argument_name]\n\t\t\treturn retval == argument_name\n\t\texcept KeyError:\n\t\t\treturn True\n\t\n\tdef get_sub_options(self, command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\tif command_name not in self._commands: \n\t\t\traise CommandNotFoundError, \"'%s' command not found in commands loaded.\" % command_name\n\t\tspec = self._commands[command_name][argument_name]()\n\t\tif not spec: raise ArgumentNotFoundError, \"'%s' command not found: %s\" % (command_name, argument_name)\n\t\treturn spec\n\n\tdef get_arg_dict(self, command_name):\n\t\t\"\"\"\n\t\tReturns the argument dictionary for the specified command name.\n\t\t\"\"\"\n\t\tcommand = self._commands[command_name][COMMAND_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t return self._option_spec_cache.get(command_name, {}).get(argument_name, {}).copy()\n\n\tdef get_option_spec_args(self, command_name):\n\t\t\"\"\"\n\t\tGets the arguments of the specified option from the command line and extracts all arguments\n\t\t\"\"\"\n\t\tself._option_spec_cache = {}\n\t\tself._option_spec_cache_dirty = True\n\t\tfor argument_name in self._get_option_spec_args_from_argv_config():\n\t\t\tret = self.get_option_from_argv_config"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t return args.strip().replace('\\\\,', ',').split(',')\n\n\tdef check_args(self):\n\t\t\"\"\"\n\t\tcheck the sensitivity\n\t\t:return: Boolean\n\t\t\"\"\"\n\t\tif self.check_if_mandatory_args():\n\t\t\tself.error ='mandatory arguments'\n\t\t# if no errors\n\t\tif len(self.args) < 1:\n\t\t\tself.error = 'no arguments'\n\t\t\t\n\tdef build_arg_strings(self, args):\n\t\t\"\"\"\n\t\tbuild the arguments\n\t\t:return: strings, and the arguments\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\nSEMI_RE = re.compile(r'_\\d+')\n\nclass SilenceFilter(logging.Filter):\n\t\"\"\"\n\tAdd the silence log handler to the logging library (if any)\n\t\n\t:param b: boolean - If ``True`` or a string is expected,\n\t\tlog message will be sent\n\t\"\"\"\n\t_filter_type = logging.Filter.BOL\n\n\tdef filter(self, record):\n\t\t\"\"\"\n\t\tFilter the record to remove silenced warnings\n\n\t\t:param record: A record\n\n\t\t:return: None (keep original)"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\t\n\tsilent_args = {}\n\tif self.is_silent(\"set_silent_args_bool\"):\n\t\tsilent_args[\"silent_args_bool\"] = True\n\t\t\n\tsilent_args[\"args\"] = args\n\treturn silent_args\n\t\n\t\n\tdef get_silent_options(self, args):\n\t\"\"\"\n\tlist of options detected by the client.\n\t\n\t:param args: List of raw arguments, or list containing a raw argument name.\n\t:return: List of options\n\t\"\"\"\n\t\n\t\n\tsilent_args = {}\n\tif self.is_silent(\""}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\t\n\tsilentArgs = []\n\t\n\twhile self._state == 'waiting for state':\n\t\targs = args.contents\n\t\t\n\t\tif args == '': break\n\t\t\n\t\targs = args.strip()\n\t\t\n\t\tif len(args) > 0:\n\t\t\tsilentArgs.append(args)\n\t\t\t\n\t\tself._state = 'parsing'\n\t\t\t\n\tif not silentArgs:\n\t\treturn []\n\t\n\treturn silentArgs\n\t\n@python_2_unicode_compatible\nclass ParserMixin(object):\n\t\"\"\"\n\tA mix-in which converts the parsed input to a unicode string,\n\tif the argument to"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n    def get_args_text(self, args):\n\t\"\"\"\n\tget the args text for the command \n\targs text is a string containing the arguments\n\ttext will be sent to the command, as specified in the\n\toptions string\n\t\"\"\"\n\n\t\n    def get_body_args(self, args):\n\t\"\"\"The body text of the command\"\"\"\n\t\t\n\t\n\"\"\"\n----------------------------------------------------------------------------------\nGeneric command base class\n\n\n\"\"\"\n\nclass GenericCommand(object):\n\n\n    \"\"\"\n    The generic command encapsulates the commands of the\n    abstract command.\n\n    To implement a generic command using concrete\n    inheritance you"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\tsilent = []\n\t\tif args:\n\t\t\tfor arg in args.split(','):\n\t\t\t\tif not arg: continue\n\t\t\t\tif arg[0] not in ['silent', 'force', 'on']:\n\t\t\t\t\tsilent.append(arg)\n\t\telse:\n\t\t\tsilent = args\n\t\t\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t\tparse arguments\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\tparse = []\n\t\tif args:\n\t\t\tfor arg in args.split(','):\n\t\t\t\tif not arg:"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t # self.logger_instance.critical(\"SILENT\")\n\t return []\n\t\n\t\n\tdef add_silence(self, silence_value, silence_type='', speak_mode='', silenced=True):\n\t\t\"\"\"\n\t\tAdds silence to a given silence value\n\t\t\n\t\t:param silence_value: silence value\n\t\t:param silence_type: type of silence to add\n\t\t:param speak_mode: mode of silence\n\t\t:param silenced: Add silenced value\n\t\t:return: void\n\t\t\"\"\"\n\t\tsilent ="}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\t\n\tsilent_args = []\n\t\n\tfor arg in args:\n\t\t# remove this from the list of all unknown variables - used to be\n\t\t#  only removed once this function is complete.\n\t\tsilent_args.remove(arg.replace('.', '_').replace('/', '_'))\n\n\treturn silent_args\n\n\ndef print_error(*args):\n\t\"\"\"\n\tPrint the error msg and exit\n\t\n\t:param args: A list of args.\n\t\"\"\"\n\tprint_out(' '.join([str(a) for a in args]), 1)\n\n\nclass Silence:\n\t\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n        # if none of the arguments are not defined return None.\n        if type(args) is not list: \n            return args\n\t\t\n\t\t# split string into command and parameters\n\t\tcmdline = map(str.strip, args.split(\",\"))\n\t\tparams = map(str.strip, args.split(\",\"))\n\t\targs = map(str.strip, args.split(\",\")))\n\t\targs = map(str.strip, args.join(\",\"))\n\t\targs = map(str.strip, args.split(\",\")))\n\t\t\n\t\t# remove any commas\n\t\targs = filter(lambda x: x"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n    def get_silent_output(self, args):\n\t\"\"\" \n\tlist of silenced output from given command\n\t\n\t:param args: The received arguments, with first line of output.\n\t:return: list\n\t\"\"\"\n\t\n    def clear(self):\n\t\"\"\"\n\terase all buffers (including pointers and data).\n\t\"\"\"\n\t    \n    def get_silent_output_buffer(self, args):\n        \"\"\"\n\tlist of silenced output buffer from given command \n\t\n\t:param args: The received arguments.\n\t:return: list, slienced output buffer names"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif len([s for s in arg if not s in self.get_required_args()]):\n\t\t\t\tself.send_response({'ERROR': 'Missing argument: \"%s\"' % arg})\n\n\t\treturn True\n\n\tdef send_response(self, code, **kwargs):\n\t\t\"\"\"\n\t\tSends the HTTP response.\n\t\t\"\"\"\n\t\tif 'client_id' not in kwargs:\n\t\t\tkwargs['client_id'] = self.client_id\n\t\t\tdel kwargs['client_id']\n\n\t\tif'scope' not in kwargs:\n\t\t\tkwargs"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\ndef make_validator(args=None):\n\t\"\"\"\n\tMake a validator for your application.\n\tIf no arguments provided, the validator checks for the given arguments,\n\tso it will throw the InvalidArgumentException() exception to\n\tprevent the validation.\n\t\"\"\"   \n\tif args is None:\n\t\t# Initialize the argument parser to None.\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='An application validation command line tool')\n\t\t# Create an argument parser. (This takes the string in as the first\n\t\t# argument to the application to be processed) and a general"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\t\n\targs = self._unpack_args_and_validate_args(args)\n\t\n\tfor name, value in args.items():\n\t\tif not isinstance(value, Argument):\n\t\t\tself.add_error('argument', 'Argument error:'+ str(value))\n\t\n\n\tparams = self.get_all_params()\n\tfor name, value in params.items():\n\t\ttry:\n\t\t\tvalue = self._pack_value(name, value)\n\t\t\tif len(args) == 0:\n\t\t\t\tself.add_error('parameter', 'Missing argument:'+ name"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg not in self.parameters.keys():\n\t\t\t\traise KeyError(\"Missing required argument %s.\" % arg)\n\n\tdef validate_arguments(self, arg_spec):\n\t\tif set(self.parameters.keys()) - set(arg_spec.keys()):\n\t\t\traise ValueError(\"Invalid arguments: %s\" % arg_spec.keys())\n# ##### BEGIN GPL LICENSE BLOCK #####\n#\n#  This program is free software; you can redistribute it and/or\n#  modify it under the terms of the GNU General Public License\n#  as"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(args, \"requires\"):\n\t\traise Exception(\"Required arguments not found.\")\n\n\n\tif not hasattr(args, \"requires_name\"):\n\t\traise Exception(\"Required name not found.\")\n\n\n\tfor arg in args.__contains__(\"_apiParams\"):\n\t\tif arg in self.require_params:\n\t\t\treturn 1\n\t\t\t\n\tself.__validate_requires_args(args)\n\t\n \tfor arg in args.__contains__(\"_headers\"):\n\t\tif arg in self.require_headers:\n\t\t\treturn 1\n\t\t\t\t\n\tself.__validate_requires_args(args)\n               \n "}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n#-------------------------------------------------------------------------------------------------------------\n#   Test Commands\n#-------------------------------------------------------------------------------------------------------------\n\t\nif __name__ ==  '__main__':\n\tif len(sys.argv) < 3:\n\t\tprint \"Usage: python -m <module> <module>...\"\n\t\tprint \"           <module>           [Module options]\"\n\t\tprint \" \tExample: python -m mutexanager.py modules/mutixanager.py\"\n\n\timport getopt\n\ttry:\n\t\topts, args = getopt.getopt(sys.argv[1:], 'hD:r:vM')\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor k, v in self._required_args.items():\n\t\t\t# Check for required values (empty string or missing keys)\n\t\t\tmissing_keys = [arg for arg in vars(args)\n\t\t\t\tif arg not in config or arg not in config[arg]]\n\t\t\tif missing_keys:\n\t\t\t\tsargs = [\"%s (value: required)\" % arg for arg in missing_keys]\n\t\t\t\tsargs = \" or \".join(sargs)\n\t\t\t\terror = \"Missing required argument '%s' in %s.\" % \\\n\t\t\t\t\t(args[0], self._name)\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t self.module_args = args\n\n\tdef get_import(self):\n\t\t\"\"\"\n\t\tReturn import object.\n\t\t@rtype import object\n\t\t\"\"\"\n\t\treturn self\n\n\tdef get_class(self):\n\t\t\"\"\"\n\t\tReturn class object.\n\t\t@rtype class object\n\t\t\"\"\"\n\t\treturn self.__class__\n\n\tdef validate_arguments(self):\n\t\t\"\"\"\n\t\tValidate arguments.\n\t\t\"\"\"\n\n\t\tis_import = self.get_import() is not None\n\n\t\timport_options = {\"module\": self.module_name}\n\t\tfor opt in self"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tif len(args) > 0:\n\t\t\tfor arg in args:\n\t\t\t\tif not arg_checker.is_positive_integer(arg):\n\t\t\t\t\tprint \"Argument '\" + str(arg) + \"' is not a positive integer\"\n\t\t\t\t\treturn False\n\t\telse:\n\t\t\tprint \"Argument '\" + str(args) + \"' has not been provided\"\n\t\t\treturn False\n\nclass NAFLETimulator(NumericalMathMethods):\n\t\"\"\" \n\tA numerical implementation of the NumericalMathMethods class.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.type"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tif args.required_args is None or args.required_args_set is None:\n\t\t\treturn False\n\n\t\terrors = []\n\t\tfor arg in reversed(args.required_args):\n\t\t\t# Check optional arguments \n\t\t\tif arg not in self.settings.required_options:\n\t\t\t\terrors.append(str(arg) +'is missing')\n\n\t\tif errors and self.settings.valid_options are None:\n\t\t\terrors.append(str(args.error_message) +'is missing')\n\n\t\t# Raise an error if there is some missing options, else return False\n\t"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t# -*- Mode:Python; indent-tabs-mode:nil; tab-width:4 -*-\n#\n# Copyright (C) 2016-2017 Canonical Ltd\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t    \n\t# Get the list of options specified on the command argument.\n\tif self.options:\n\t    return _get_conditionally_optional_args(\n\t             command_name, options_spec, self.options)\n\n\t# Get the list of command argument that matched.\n\telse:\n\t    command_arguments = _get_specified_if_needed(\n\t\t    command_name, options_spec)\n\t    return [_get_conditionally_optional_args(command_name, options_spec,\n\t\ti) for i in command_arguments]\n\n \ndef _check_or_reject_if_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\t\n\targs = options_spec['args']\n\t# args = options_spec\n\n\tpossible_keywords = ['required_when']\n\tfor keyword in possible_keywords:\n\t\tif keyword in args:\n\t\t\targs.remove(keyword)\n\t\t\tbreak\n\t\n\tif len(args) == 0:\n\t\tmsg = 'No additional arguments were entered'\n\t\tself.ui.showMessage(msg)\n\t\treturn []\n\t\n\t# Check arguments and return types\n\t#\tThis will help identify conflicting positional arguments.\n\trequired_when = args[0][0]\n\t\n\n\treturn"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\t\t\n\targs = []\n\tfor option in options_spec:\n\t\topt_name = option\n\t\topt_val = options_spec[option][1]\n\t\t\n\t\tif (opt_name in self._required_when) and (opt_val is not None):\n\t\t\targs.append('--' + opt_name)\n\t\t\targs.append(opt_val)\n\t\t\t\t\n\t\telse:\n\t\t\targs.append('-' + option)\n\t\t\targs.append(opt_val)\n\t\t\t\t\n\t\n\t\n\targs.extend(options_spec[command_name]['args'])\n\t\n\treturn args\n\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t    \n\targs_with_default = [command_name]\n\trequired_when = \" \".join(args_with_default)\n\trequired_when = required_when.lower()\n\n\tvalid_options = [o for o in options_spec if getattr(options_spec, o)]\n\n\t# Check only one command name\n\tif len(valid_options) == 1:\n\t\tcmd_name = valid_options[0]\n\t\tcmd_value = valid_options[1:]\n\telse:\n\t\tcmd_name = valid_options\n\t\tcmd_value ="}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\t\n\tif isinstance(options_spec, dict):\n\t\trequired_args = []\n\t\tfor name, opt in options_spec.items():\n\t\t\tdefault = self.__get_field(command_name, opt)\n\t\t\trequired_args.append((name, default))\n\t\t\t\n\telif isinstance(options_spec, list):\n\t\trequired_args = self.__list_fields(command_name, options_spec)\n\t\n\t\t\n\telse:\n\t\traise TypeError, \"%r is not of %s type\" % \\\n\t\t\t(command_name, self.__class__.__name__)\n\t\t\t\n\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\n\tcommand_impl = command_name.lower()\n\toptions_path = os.path.join(self.base_dir, command_name)\n\tif options_spec and'required_when' in options_spec:\n\t\trequired_when = options_spec['required_when']\n\t\tif required_when == 'yes':\n\t\t\tcommand_impl +='--%s' % required_when\n\t\telse:\n\t\t\tcommand_impl +='--%s' % required_when\n\n\treturn command_impl, options_path\n\n\nclass _CmdLineRunner(object"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t    \n\t# split options_spec string into list, order is important\n\toptions_spec = options_spec.split(separator)\n\trequired_when = []\n\tfor option in options_spec:\n\t\tif \"required_when\" in option:\n\t\t\tfor optional_argument in option.split(separator):\n\t\t\t\tif optional_argument not in required_when:\n\t\t\t\t\trequired_when.append(optional_argument)\n\treturn required_when\t\t\n\nclass CommandFactory(object):\n\t\"\"\"\n\tCreates a command factory.\n\t\"\"\"\n\t\n\tdef __init__(self"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\tif command_name not in self.parser.parameter_spec[options_spec]:\n\t\t\treturn []\n\t\targs = self.parser.expand_parameter_spec_and_args(\n\t\t\toptions_spec, command_name, args)\n\t\treturn args\n\n\tdef _get_option_args(self, command_name, args_spec, options,\n\t\"\"\"\n\tList all specified command's positional arguments.\n\t\n\tIf no positional arguments, the result is empty.\n\t\n\t:param command_name: the command name.\n\t:param args_spec: option specification.\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\t\n\treturn []\n\t\t\n\nclass CommandListOptions(BaseListOptions):\n\t\"\"\"\n\tCommand list options.\n\t\"\"\"\n\t__slots__ = (\"command_name\", \"option_spec\", \"optional_if\", \"arguments\",\n\t\t\t \"prompt_option\", \"shortlist\", \"longlist\", \"aliases\")\n\n\tdef __init__(self, command_name, options_spec, arguments, ignore_opts):\n\t\t\"\"\"\n\t\tCreate list options.\n\t\t\n\t\t:param command_name: the command name.\n\t\t:param options_spec:  the options specification.\n\t\t:param"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass AsciiAddressError(SAIError):\n\t\"\"\"\n\tIndicate that there was input arguments invalid.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass AsciiNotFoundError(AsciiiError):\n\tdef __init__(self, domain, interface, domain_index):\n\t\tself.domain = domain\n\t\tself.interface = interface\n\t\tself.domain_index = domain_index\n\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of the error message.\n\n\t\t:return: The error message string.\n\t\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif len(args) > self.LENGTH:\n\t\traise exceptions.ArgumentError(\"Invalid arguments\")\n\t\n\treturn True\n\n##\n# Received value types:\n#\n\ndef validate_data_type(value, data_type):\n\t\"\"\"\n\tCheck if value of `data_type` is supported as a specified data type. If not, \n\traise an exception.\n\n\t:param value: The value to validate.\n\t:param data_type: The data type to be considered.\n\t:return: True if valid, False otherwise.\n\t\"\"\"\n\tif"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self._minlen or len(args) < self._minlen: \n\t\t\tself._raise_error('length_arguments', 'len', len(args), 'is greater than length specified for args')\n\nclass MaxLength(Operation):\n\t\"\"\"\n\tRaise Error when value for length of string is too long.\n\t\n\t:param minlen: The minimum length allowed.\n\t\"\"\"\n\tdef _raise_error(self, what, idx, len_):\n\t\tif (idx+1) < self._minlen or (idx+1) > self._"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tassert (len(args)!= len(self.args))\n\t\ttry:\n\t\t\tfor idx, arg in enumerate(args):\n\t\t\t\tif arg > arg_count(self.args[idx]):  # pragma: no cover - this will raise ValueError if length is shorter than length specified: length = self.args[idx] + 1\n\t\t\t\t\traise ValueError()\n\t\t\t\tif (idx > arg_count(self.args[idx]) - 1) and ((idx - arg) < 0 or arg > arg_count(self.args[idx + 1])):  # pragma: no cover - this"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\n\tfor name, arg in args.items():\n\t\t\ttry:\n\t\t\t\tif (arg > 0 or len(arg) > 1) and not is_integer(arg):\n\t\t\t\t\tself._has_negative_length(name, arg)\n\t\t\texcept:\n\t\t\t\treturn\n\t\t\n\treturn self._has_negative_length(self.get_type(), int(args))\n\t\ndef parse_boolean_arguments(arg):\n\t\"\"\"\n\tParse boolean argument string from arg string.\n\t\n\tReturn a string containing arguments that are in the argument string.\n\t\n\tThe check is case insensitive.\n\t\n\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t \n\tfor arg in args:\n\t\tif len(arg) < 1:\n\t\t\traise ValueError(\"Argument missing: length specified\")\n\nclass PacketHeader:\n\t\"\"\"\n\tAn AMQP packet header.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\n\t@staticmethod\n\tdef unpack_packet(header_str_bytes):\n\t\t\"\"\"\n\t\tUnpacks a single AMQP packet header.\n\t\t\n\t\t:param str header_str_bytes: AMQP packet header string as a ``bytearray``.\n\t\t:returns: A ``MessagePacket`` instance with the appropriate protocol"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\n\tif len(args) > 2:\n\t\traise Exception(\"Expected at most 2 arguments, got zero.\")\n\t\t\t\n\t# Make sure to return a tuple after length\n\tif len(args) == 2:\n\t\tlength = args[1]\n\telse:\n\t\tlength = 0\n\tself._arguments.pop()\n\treturn length\n\"\"\"\nTests the course_wiki app.\n\"\"\"\nfrom contentstore.tests.utils import AjaxEnabledTestClient, CourseTestCase\nfrom contentstore.utils import delete_course_and_groups\nfrom xmodule.modulestore import ModuleStoreEnum\nfrom xmodule."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\targs = args.strip()\n\tif len(args) in (0,1,2):\n\t\traise ValidationError, self.error_msg_invalid, args\n\targs = args.lower()\n\tif args in ('left', 'right'):\n\t\traise ValidationError, self.error_msg_invalid, args\n\tif args == 'bottom':\n\t\tif not args == 'left':\n\t\t\traise ValidationError, self.error_msg_invalid, args\n\telif args == 'top':\n\t\tif not args == 'right':\n\t\t\traise ValidationError, self.error_msg"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif len(args) > 63:\n\t\treturn \"The length of variable \" + self.get_var_name(args[0]) + \" exceeds 64 characters.\"\n\nclass UnrecognizedArgumentError(Problems):\n\tdef __init__(self, arg_error_list = []):\n\t\tif not arg_error_list:\n\t\t\tException.__init__(self,\"Could not get the argument(s) \" + \"<\" + \", \".join(arg_error_list))\n\t\n\tdef error(self, args):\n\t\tstr_args = \", \".join(str(arg) for"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif len(args)!= len(self.arguments):\n\t\traise InvalidLength(\"Arguments are not equal length (expected = %d, received = %d)\" % (len(self.arguments), len(args)))\n\t\n\ndef is_null_string(x):\n\t\"\"\"\n\tCheck whether a given string is null. \n\t:returns: True, False or None.\n\t\"\"\"\n\treturn (isinstance(x, str) and x.strip()!= \"\")\n\t\ndef is_invalid_str(x):\n\t\"\"\"\n\tCheck whether a given string is invalid.\n"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tchoices = []\n\t\n\tfor choice in args[\"choices\"]:\n\t\tchoices.append({\"value\": choice})\n\t\n\targs[\"choices\"] = choices\n\t\t\t\t\n\treturn args\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('dashboard', '0001_initial'),\n        ('core', '0001_initial'),\n        ('dashboard', '0001_initial'),\n        ('core', '0002_auto_20150904_"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tdef validate_values(self):\n\t\"\"\"\n\tCheck if values are correct\n\t\"\"\"\n\t\t\n\t\tif self.valid_options!= False:\n\t\t\targuments = dict(args)\n\t\t\t\n\t\t\t# For every possible type, check its available choices\n\t\t\tfor args_type in self.valid_options:\n\t\t\t\targs_present = False\n\t\t\t\t\n\t\t\t\tfor argument_name, arg_args_type, arg_arg_type, arg_default in arguments.iteritems():\n\t\t\t\t\tif arguments[argument_name]!= None and arg_args_type == arg_arg_type:\n\t\t\t\t\t\n\t\t\t\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif self.choices:\n\t\tfor item in self.choices:\n\t\t\tif item not in args:\n\t\t\t\targs[item] = None\n\n\nclass SelectFromChoicesForm(Form):\n\t\"\"\"Form used for selecting one of several choices from a user.\"\"\"\n\t\n\tdef _is_in_choices(self, choices, user):\n\t\tfor item, val in choices:\n\t\t\tif item is choice:\n\t\t\t\treturn True\n\t\treturn False\n\n\ndef validate_choices_from_user(user, choices):\n\t\"\"\"\n\tCheck if the user has selected one of several choices from"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif args:\n\t\tfrom webnotes.widgets.form.load import validate_required_for_item\n\t\t\n\t\tvalidate_required_for_item(args)\n\t\targs['type_name'] = self.doctype + \"-\" + args['doctype']\n\t\tdel args['doctype']\n\t\n\telse:\n\t\targs['type'] = self.doctype + \" Item\"\n\t\tdel args['doctype']\n\t\t\ndef get_controller(arg1, arg2=None):\n\t\"\"\"\n\tGet controller from arg1, arg2- = screen name / doctype\n\t\"\"\"\n\tif arg2"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tfor arg in args:\n\t\t#check if argument value is a valid choice\n\t\tif isinstance(arg, tuple) and not arg[0].value:\n\t\t\t#make sure it is a tuple and not a string\n\t\t\tif tuple(arg[0].value) == (0, 0, 0) and not isinstance(arg[1], tuple):\n\t\t\t\traise Exception(\"Invalid choices values, answer contains: ('0', '0') and ('1', '1'), but is: '%s'\" % str(arg))    \n\t\t\telif tuple(arg[0].value) == (0"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t return len(args) == 1 and args[0] in self._get_choices()\n\n\n\tdef validate_choice(self, value):\n\t\t\"\"\"\n\t\tCheck if given value is one of the available choices.\n\t\t\n\t\t:param value: The value to check\n\t\t\"\"\"\n\t\tchoices = self._get_choices()\n\t\tif value not in choices:\n\t\t\traise ValueError('invalid value, must be one of {}'.format(choices))\n\t\treturn value\n\n\n\tdef _get_choices(self):\n\t\t\"\"\"\n\t\tReturns a list with the possible choices for this field.\n\t\t\n\t\tThis may"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\t# Checks if we have choices.\n\tif self.is_enum():\n\t\tif len(args) > 0:\n\t\t\treturn get_translated_text(args[0]).upper() == get_translated_text(args[0]).upper()\\\n\t\t\t\t                   and get_translated_text(args[1]).upper() == get_translated_text(args[1]).upper()\n\t\n\telif self.is_boolean():\n\t\tif len(args) == 1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\ndef get_language_data(language):\n\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t \n\tfor name, value in args.items():\n\t\tif value not in self.choices:\n\t\t\traise ValidationError(\"Not a valid choice for {}\".format(name))\n\n\tdef validate_choices_condition(self, args):\n\t\tfor condition in args:\n\t\t\tif condition not in self.conditions:\n\t\t\t\traise ValidationError(\"Not a valid choice: {}\".format(condition))\n\n\tdef validate_choices_operator(self, args, value):\n\t\tfor name, value in args.items():\n\t\t\tif name.startswith(\"operator_\") and value:\n\t\t\t\tif name.split(\"_\")[0"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tdef validate_iter(self, value):\n\t\t\"\"\"\n\t\tCheck the value can be iterated over.\n\t\t\n\t\t:param value: The received argument.\n\t\t:return: Return whether it's a tuple or a generator.\n\t\t\"\"\"\n\t\t\n\t\tif callable(value):\n\t\t\tvalue = value()\n\t\t\n\t\ttry:\n\t\t\treturn value\n\t\texcept:\n\t\t\ttype, value, tb = sys.exc_info()\n\t\t\traise ValidateActionException(\"Value not available with valid options (type, value).\")\n\n\t####################################################\n\t##\n\t## Interface to the plugin\n\t##\n\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\n\tif not isinstance(args, (tuple, list)) and not isinstance(args, tuple):\n\t\targs = (args, )\n\n\tchoices = []\n\tfor x in args:\n\t\tchoices.append(x)\n\tchoices = set(choices)\n\n\tif len(choices) > 1:\n\t\tchoices = \", \".join([unicode(i) for i in choices])\n\telse:\n\t\tchoices = \"multiple-choice\"\n\n\targs = {\n\t\t\"choices\": choices,\n\t}\n\treturn args\n\n\nclass MultipleChoiceSelect(Select):\n\t\"\"\""}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n        \t# min maximum = maximum allowed\n\t\tminimum = None\n\t       \n\n\t\ttry:\n\t\t\tminimum = int(self._params.get(\"_min_min_val\", 1.0))\n\t\texcept (TypeError, ValueError):\n\t\t\terrors.UIError(\n\t\t\t\t\"Spoke has to be specified an integer with an upper bound.\"\n\t\t\t)\n\n\t\ttry:\n\t\t\tmaximum = int(self._params.get(\"_max_max_val\", 25))\n\n\t\texcept (TypeError, ValueError):\n\t\t\terrors.UIError(\n\t\t\t\t\"Spoke has to be specified an integer with an upper"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass ValidateBoolean(Validator):\n\t\"\"\"\n\tCheck if value of arguments is between boolean values for the format 'TRUE|FALSE'.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\tValidator.__init__(self, *args)\n\t\tfrom pycopia.SMI import SMI\n\t\tself._SMI = SMI\n\t\n\tdef execute(self, instance):\n\t\tif bool(self._SMI.QueryValue(self.encode()))!= self._SMI.QueryValue(self.encode()):\n\t\t\traise ValueError"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tmin = None\n\tmax = None\n\t\n\tif args[\"maximum\"]:\n\t\tmin = args[\"minimum\"]\n\t\tmax = args[\"maximum\"]\n\t\n\tif args[\"minimumTarget\"]:\n\t\tminTarget = args[\"minimumTarget\"]\n\t\tminTarget = minTarget.resolve(dict)\n\t\t\n\tif args[\"maximumTarget\"]:\n\t\tmaxTarget = args[\"maximumTarget\"]\n\t\tmaxTarget = maxTarget.resolve(dict)\n\t\n\tif args[\"targetRange\"]:\n\t\tminTarget = args[\"targetRange\"][0]\n\t\tmaxTarget = args[\"targetRange\"][1]\n\t\n\tif"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tdef add_entry(self, entry):\n\t\t\n\t\t# Get the field name\n\t\tfield_name = _get_field_name(entry, self.fields)\n\t\t\n\t\ttry:\n\t\t\t# Split into three pieces. Each piece is encoded as a fixed-length string:\n\t\t\t# \"<field_name>__<field_name>\". Be careful, these are all of the arguments.\n\t\t\t# They can be given as any integer or float arguments separated by commas:\n\t\t\t#   * int(max_len), but for int and float arguments, the entire field_name\n\t\t\t#  "}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n# \tdef validate_string(self, value):\n# \t\t\"\"\"\n# \t\tCheck that the argument is string\n# \t\t@param value: The value.\n# \t\t@return: Returns True,False.\t\n# \t\t\"\"\"\n# \t\treturn True\n\n# \tdef validate_int(self, value):\n# \t\t\"\"\"\n# \t\tCheck that the argument is a positive integer.\n\t\n\n# \t\tReturns:\n# \t\tReturns True,False\n# \t\t\"\"\"\n\t\n# \tdef validate_float(self, value"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tfor k,v in args.items():\n\t\tif not k =='min':\n\t\t\treturn False\n\t\tif ((self.parse_int(\"min:\" + args[k]) <= parse_int(v)) or\n\t\t\t(self.parse_int(\"max:\" + args[k]) >= parse_int(v))):\n\t\t\treturn False\n\t\n\treturn True\n\n\ndef validate_max_values(self, args):\n\t\"\"\"\n\tCheck if value of arguments is between maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tfor k,v"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass IntegerValidator(Interface):\n\t\"\"\"\n\tValidates that an argument is an integer.\n\t\n\t:param minv:\n\t\tThe minimum value of the argument.\n\t:type minv:\n\t\t:class:`int`\\ s \n\t:param maxv:\n\t\tThe maximum value of the argument.\n\t:type maxv:\n\t\t:class:`int`\\ s \n\t:param minv_regex:\n\t\tThe regular expression that must matches this value. This \n\t\tregex can be used to match the number for which a minimum is \n\t\tencountered and a maximum is dis"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\t# Check range boundaries\n\tif len(args) < 2:\n\t\treturn \"ERROR: Minimum and maximum arguments are not specified!\"\n\tmin_valid, max_valid = self.validate_range(args[0], args[1], \n\t\t\t\t\t\t\t\t\t\t\t self.options.minval, \n\t\t\t\t\t\t\t\t\t\t\t self.options.maxval, \n\t\t\t\t\t\t\t\t\t\t\t self.options.allow_none)\n\t\t\n\treturn min_valid, max_valid\n\t#!/usr/bin/env python3\n# Copyright (c) 2012 The Bracketcoin Core developers\n# Distributed under the MIT software license, see"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tdef check_args_validity(self, args, minimum=None, maximum=None):\n\t\t\"\"\"\n\t\tCheck if the values of args are valid.\n\t\t\n\t\t:param args: The received arguments.\n\t\t:type args: (dict)\n\t\t:param minimum: The minimum value.\n\t\t:type minimum: (int)\n\t\t:param maximum: The maximum value.\n\t\t:type maximum: (int)\n\t\t:rtype: (int, int)\n\t\t\"\"\"\n\t\tminimum, maximum = self.check_num_args(args, minimum, maximum)\n\t\tmin"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t # Check if min and max are given and check they are within constraints\n\t\tmin_value = None\n\t\tif'min' in args and'max' in args and args['min'] <= args['max']:\n\t\t\tmin_value = args['min']\n\t\tif 'length' in args and'max' in args and args['length'] >= args['max']:\n\t\t\tmax_value = args['max']\n\t\tif'min' in args and'max' in args and args['min'] <= args['length']:\n\t\t\tmin_value = args['min']\n\t\tif 'length"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\topt_count = len(spec_option)\n\t\tif opt_count == 0:\n\t\t\tif not self.default_argumet is None:\n\t\t\t\tif self.default_argumet.startswith(type_name):\n\t\t\t\t\tif self.default_argumet.lstrip(type_name) in [\"True\", \"False\"]:\n\t\t\t\t\t\traise ValueError(\"Invalid complex argumet {}\".format(\n\t\t\t\t\t\t\toption_name))\n\t\t\t\t\treturn self.default_argumet\n\t\t\t\telse:\n\t\t\t\t\t# We have a mandatory argument\n\t\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\traise ValueError"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\n\tclass CComplexArgumet(ComplexArgumet):\n\t\t\"\"\"Build the complex argument type\n\t\t\n\t\t:param args: list of complex arguments\n\t\t:type args: list of CComplexArgumet\n\t\t:param opts: the command line options\n\t\t:type opts: CComplexArgumet\n\t\t\n\t\t\"\"\"\n\n\t\tdef __init__(self, name, arguments, opts = None):\n\t\t\t\"\"\"\n\t\t\t:param name: the complex argumet name\n\t\t\t:type name: CComplexArgumet or str\n\t\t\t:param arguments: the command line options\n\t\t\t:type arguments"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\treturn complex_type_from_command(subcommand, type_name, option_name,[ 'int', 'float' ])\n\n\tdef build_parameter_type(self, subcommand, option_name, option_description):\n\t\t\"\"\"\n\t\tBuilds the complex parameter type at the command section \n\t\tof the command section of the plugin.\n\t\t\n\t\t:param subcommand: the command name\n\t\t:param type_name: the complex type name\n\t\t:param option_name: the option name\n\t\t:param option_description:\n\t\t:return: the complex type instance\n\t\t\"\"\"\n\t\t#"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\tctype, args = parse_complex_args(args[1:], option_name, type_name)\n\t\t\n\t\tif ctype:\n\t\t\targ_spec = ctype.lower().replace('-', '_')\n\t\t\targ_type = arg_spec\n\t\telse:\n\t\t\targ_spec = ''\n\t\t\t\n\t\targument = arg_type + arg_spec + args[2:]\n\t\t\n\t\treturn type(argument, {}, {})\n\n\ndef parse(name_or_list, namespace=[], command_name='command', options=None):\n\t\"\"\"\n\tParse arguments from a string containing a Python module command or\n\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\t\n\t\tcomp_type_name = self.spec_type_name(type_name)\n\t\tcomplex_type = self.__get_complex_type(\n\t\t\tcomp_type_name, option_name, comp_type_name)\n\t\t\n\t\tif complex_type not in self._complex_types:\n\t\t\tself._complex_types.append(complex_type)\n\t\t\n\t\treturn complex_type\n\t\n\tdef __get_complex_type(self, comp_type_name, option_name, comp_type_name):\n\t\tcomp_type_name = comp_type_name."}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\tself.create_option(option_name, subparser=subparser, argument_type=type_name, help=type_name.capitalize())\n\n\tdef create_option(self, option_name, subparser, argument_type, help=None):\n\t\t\"\"\"\n\t\tBuild an option for a given option name\n\t\t\n\t\t:param option_name: the name of the option\n\t\t:param subparser: the SubParser object\n\t\t:param argument_type: an argument type for a given subparser\n\t\t:param help: the help string\n\t\t:return: None\n\t\t\"\"\""}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\targ_parser = Subcommand(subcommand, command_name=type_name,\n\t                            subobject_name='argument_parser',\n\t                            docstring='Argument parser.',\n\t                            help='Specify the complex type.',\n\t                            mandatory=True)\n\n\t\targ_parser.argparser.add_argument(\n\t\t\t'--type',\n\t\t\tdest='type',\n\t\t\ttype=type_name,\n\t\t\thelp='Type of the argument.',\n\t\t\trequired=True)\n\n\t\targ_parser.argparser.add_argument(\n\t\t\t'--argument-handler',\n\t\t\tdest='argument_handler',\n\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\t#print \"Creating complex argument type for: %s/%s/%s\" % (type_name, option_name, spec_option)\n\t\targ_type = self.object_types_dict[type_name].create(type_name, subcommand, option_name, option_name, type_name, spec_option)\n\t\t#print \"Object Type created: %s\" % arg_type\n\n\t\t#Create the object type\n\t\t#create_obj_type(self.object_types_dict[type_name], arg_type, self.cmd_desc, arg"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\t\n\t\tsimpleType = _create_complex_type(self, subcommand, type_name, option_name)\n\t\t\n\t\t# parse the spec file\n\t\tspec_parser = YamlParser(spec_option)\n\t\targs = spec_parser.parse_args()\n\t\t\n\t\t# return the complex instance\n\t\treturn ArgType(simpleType, args)\n\n\tdef parse_argument(self, arg_dict, arg_type ):\n\t\t\"\"\"\n\t\tParse a positional argument dict as an argument type\n\t\t\n\t\t:param arg_dict: the argument dict\n\t\t:param arg_type: the argument type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\tself.logger.log('Processing', 5)\n\t\tcomplex_type = self.build_complex_type(type_name = type_name,\n\t                                             subsection = subcommand.upper())\n\t\tself.do_build_command_args(subcommand, **spec_option)\n\t\tcomplex_instance = self.do_complex_type_build(subcommand)\n\t\tself.logger.log('done', 5)\n\t\treturn complex_instance\n\n\tdef do_build_command_args(self, subcommand, **spec_option):\n\t\t\"\"\"\n\t\tBuild a command's"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\t# Special handling of control arguments:\n\t# We ignore control arguments but want to ignore it because we need\n\t# to use default arguments.\n\tcontrol_args = {k: v for k, v in vars(self.control).iteritems()\n\t\tif self.control_args.get(k) is not None}\n\n\tif self.control_args.get('config') and\\\n\t\tself.control_args['config'].get('input_required'):\n\t\tcontrol_args['config']['input_required'] = False\n\t\t\n\t# Handle simple, advanced or advanced arguments."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tnested = {}\n\t\n\toptionals = \"\"\n\t\n\tfor option in args:\n\t\tif option == '--advanced':\n\t\t\tadvanced = True\n\t\telif option == '--include':\n\t\t\tinclude = True\n\t\telif option == '--exclude':\n\t\t\texclude = True\n\t\telif option == '--merge' or option == '--merge-type':\n\t\t\t# convert single integer value to number\n\t\t\tvalue = int(options[option][0])\n\t\t\toptions[option][0] = str(value)\n\t\t\tdel options[option]\n\t\telif option == '--merge-type':\n\t\t\t# convert"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\t# First, gather all the arguments and call them.\n\toptions = self.parse_nested_arguments(args.get(\"$options\"))\n\targ_list = args.get(\"$arg_list\")\n\targ_list = arg_list.split(',')\n\targs.pop('$arg_list')\n\targ_list = [i.strip() for i in arg_list]\n\targs.pop('$arg_list')\n\t\n\t# Third, create a dictionary of the args.\n\tnested_custom = options.get('variables', dict())\n\tnested_custom.extend"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\targs = args.split()\n\tsimple_args = [arg for arg in args if is_single_arg(arg)]\n\tnested_args = []\n\tmulti_arg_args = []\n\tcontrol_args = []\n\targ_count = len(args)-1\n\twhile arg_count <= multi_arg_args:\n\t\tif arg_count == 1:\n\t\t\tpass # Don't repeat this loop, since the command line takes care of them\n\t\telif arg_count == 2:\n\t\t\targ_count = int(args[1])\n\t\t\tif arg"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tresult = dict()\n\tnested = dict()\n\tnested_args = list()\n\t\n\t# First add all the global args so they can be passed directly to the \n\t# yml file \n\tfor arg in args:\n\t\t\n\t\tif len(arg) == 0:\n\t\t\tcontinue\n\t\t\t\n\t\tif ('type' in arg and args[arg]['type'] == 'list'):\n\t\t\targ = self.clean_list(arg)\n\t\t\targ = self.clean_list(arg)\n\t\t\n\t\telif ('type' in arg):\n\t\t\tresult[arg] = self.clean_list"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\targs = args.strip().split(\"\\n\")\n\tif len(args)!= 0:\n\t\targs = [args[0],' '.join(args[1])]\n\t\n\tcmd_lst = []\n\tfor arg_set in args:\n\t\tcmd_lst.append(arg_set.split(\"arg.\"))\n\t\n\treturn cmd_lst, args[1:]\n\t\n# ==============================================================================\n\tclass AnsiblePlayBook(AnsiblePlayBookBase):\n\t__name = \"Ansible Playbook\"\n\t__type ='module'\n\t__version = \"2.0\"\n\t\n\tdef"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tfor argument in args.keys():\n\t\tif 'control_args' in argument:\n\t\t\targs[argument]['control_args'] = arg\n\t\telif 'nested_args' in argument:\n\t\t\targs[argument]['nested_args'] = arg\n\t\n\treturn args, arg\n\t\n\nif __name__ == '__main__':\n\n    arg = demisto.argument_parser()\n    arg.add_argument('-a', '--api-url', required=True, type=str, \n                        help='A URL containing the API server.', default=None)\n    arg.add_argument('-v', '--verbose"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\tcustom_args = args.copy()\n\t\tcustom_args['control_args'] = []\n\t\tif 'nested_args' in custom_args:\n\t\t\tfor nested_arg in custom_args['nested_args']:\n\t\t\t\tcustom_args['control_args'].append({nested_arg[0]: nested_arg[1]})\n\n\t\tif 'custom_args' in custom_args:\n\t\t\tfor custom_arg in custom_args['custom_args']:\n\t\t\t\tcustom_args['control_args'].append({custom_arg[0]: custom_arg[1]})\n\n\treturn"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\targs = args.copy()\n\tnested_args = kwargs\n\t\t\n\t\n\t\n\t\n\t# Replace each inclusions with custom arguments.\n\tfor key, value in args.items():\n\t\t# Don't overwrite the other arguments.\n\t\tif isinstance(value, dict):\n\t\t\targs.pop(key)\n\t\t\n\t\tif key not in args: arg_value = value\n\t\t\targs.pop(key)\n\t\t\tnested_args[key] = arg_value\n\t\n\treturn args, nested_args\n\t\n\t\n#\n# Class: Builder\n#\n\nclass Builder(yaml.YAM"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tif args is None:\n\t\targs = []\n\t\n\tif isinstance(args, (str, unicode)):\n\t\tif args.startswith('_'):\n\t\t\targs = args[1:]\n\t\t\tdel args[0]\n\t\targs = ['ansible_var_params=' + e for e in args if '=' in e]\n\tif isinstance(args, unicode):\n\t\targs = args.encode('utf-8')\n\targs += ['ansible_mode=' + e for e in args if 'ansible_' not in e]\n\targs = json.loads(''.join(args))\n\t\n\textended_"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif isinstance(extra_vars, basestring):\n\t\tdef replace_dict_with_separator(match):\n\t\t\tdict[match.group(0)] = extra_vars_dict.get(match.group(0), '')\n\t\t\treturn match.group(0)\n\t\textra_vars = replace_dict_with_separator(extra_vars)\n\t\textra_vars = replace_dict_with_separator(extra_vars, extra_vars_dict)\n\n\treturn extra_vars\n\n\ndef get_cmd_output(cmd, cwd, quiet = False, check = False,"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\ndef merge_from_dict(a, b):\n\t\"\"\"\n\tMerge contents from dict ``a`` \n\t:param a: dict\n\t:param b: dict\n\t:return: True if success\n\t\"\"\"\n\tfor key, val in vars_dict.items():\n\t\tif key in b:\n\t\t\tcontinue\n\t\tif key in a:\n\t\t\tcontinue\n\t\ta[key] = val\n\treturn True\n\ndef get_env_vars(vars, user=None):\n\t\"\"\"\n\tRead values from the ``frappe.local`` env and merge them\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\ndef get_asset_path(path, filename):\n\t\"\"\"\n\tReturn path in image for a file\n\t:param path: File path\n\t:param filename: File name\n\n\t:return path with correct extension, unless exists\n\t\"\"\"\t\t\n\tnew_path = os.path.join(path, filename)\n\tif not os.path.isfile(new_path):\n\t\treturn None\n\treturn new_path\n\n\ndef remove_file(filepath):\n\t\"\"\"\n\tDelete a specified file from the system\n\t\n\t:param filepath: File path\n\t:type"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif extra_vars:\n\t\tassert isinstance(extra_vars, list)\n\t\tnew_vars = list()\n\t\tfor vars_var in extra_vars:\n\t\t\tname = vars_var.name\n\n\t\t\tif name in vars_dict:\n\t\t\t\tif name!= '__name__' + name:\n\t\t\t\t\traise ValueError(\"Multiple %s vars with same name %s and same value for argument %s\" % (extra_vars, name, name))\t\t\n\t\t\telse:\n\t\t\t\tnew_vars.append(vars_var)\n\n\t\tvars_dict = new_vars\n\n\treturn vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\tif not isinstance(extra_vars, (list, tuple)):\n\t\textra_vars = [extra_vars]\n\t\n\tif not isinstance(vars_dict, dict):\n\t\tvars_dict = {\n\t\t\t\"_\":\tvars_dict,\n\t\t\t\"_\":\tvars_dict\n\t\t}\n\t\n\tnew_vars = []\n\tfor _key in extra_vars:\n\t\tif not isinstance(_key, tuple):\n\t\t\t_key = _key,\n\t\tnew_vars.append(_key)\n\t\n\tfor (key, val) in ((k, v) for k"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif not extra_vars:\n\t\textra_vars = []\n\n\tkeys = list(vars_dict.keys())\n\tfor k in keys:\n\t\tif k in extra_vars:\n\t\t\tdel vars_dict[k]\n\n\tfor k,v in extra_vars:\n\t\tif k in keys:\n\t\t\tvars_dict[k] = v+keys\n\n\n\nclass WafParser(object):\n\t\"\"\"\n\tParses a xml file for the ``waflib/extras/waf/include`` feature.\n\t\"\"\"\n\tdef __init__(self, file):"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n@task\ndef shell(cmd, **kw):\n\t\"\"\"\n\tRuns, returns ``(returncode, stdout)`` tuple where the ``returncode``\n\tand ``stdout`` are the OS-level of the command's stderr stream captured\n\treceived.\n\t\n\t:param cmd:\n\t\tCommand to run, like ``run`` or ``shell``, **kw\n\t:param stdin:\n\t\tThe file-like object used to receive stdin data.\n\t:param stdout:\n\t\tThe file-like object used to receive stdout data.\n\t:param stderr:\n\t\tThe file-like object used to"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\tif not extra_vars:\n\t\t# remove the keys that are not in vars_dict but also remove the extra variables\n\t\tfor k in list(vars_dict.keys()):\n\t\t\tv = vars_dict[k]\n\t\t\tif len(v) > 0:\n\t\t\t\tif k!= \"id\":\n\t\t\t\t\tdel vars_dict[k]\n\n\t# add a copy of vars_dict\n\tglobal extra_vars\n\textra_vars = []\n\tfor k in vars_dict:\n\t\tif k not in extra_vars:\n\t\t\textra_vars.append(k"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\tif extra_vars is None:\n\t\textra_vars = []\n\t\n\tif isinstance(extra_vars, basestring):\n\t\textra_vars = [extra_vars]\n\t\n\tfor var in extra_vars:\n\t\tvar_dict.setdefault(var, [])\n\t\n\treturn extra_vars\n\t\ndef escape_backslashes(data):\n\tdata = data.replace(\"\\\\\\\\\", \"\\\\\").replace(\"\\\"\", \"\\\\\\\"\")\n\t\n\tfor pattern, replacement in [(\"\\\\\", r\"\\\\\"), (\".\", r\"\\\"), (\"\\n\", r\"\\n\")]:\n\t\tdata = pattern.sub(replacement"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\tif isinstance(vars_dict, basestring):\n\t\t# convert vars_dict to dict of vars\n\t\tvars_dict = parse_vars(vars_dict)\n\tif extra_vars:\n\t\tif not isinstance(extra_vars, list):\n\t\t\textra_vars = [extra_vars]\n\t\tfor extra_var in extra_vars:\n\t\t\tif not isinstance(extra_var, basestring):\n\t\t\t\traise errors.WafError('Invalid \"extra-vars\" entry', node=node_id,\n\t\t\t\t\tcontext=context)\n\t\t\textra_vars.append(extra_"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t\n\t# add to arguments dict\n\textra_args = {\n\t\t'ansible_args': {'playbook_path': playbook_path},\n\t\t'debug': verbose if verbose else None}\n\t\n\t# read existing arguments from file\n\tf_args = open(ir_plugin.plugin_file, 'r')\n\ttry:\n\t\tfor line in f_args:\n\t\t\tif line.startswith('#') or not line.strip(): continue\n\t\t\tm = re.match(\".*?(#|#)\\s*$\", line).group(1)\n\t\t\tvar_name = m[2:-"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\tcmd_options = {\n\t\t\t'delete' : verbose and 'force' or 'always'\n\t\t}\n\n\t\tif verbose:\n\t\t\tcmd_options['_ansible_verbose'] = verbose\n\t\t\tcmd_options['_ansible_quiet'] = True\n\n\t\tcmd = get_cmd(**cmd_options)\n\t\ttry:\n\t\t\tr = run_playbook(ir_workspace._con, ansible_path(ir_plugin._con,\n\t\t\t\t'ansible_playbook.as_dict'), cmd)\n\t\texcept RecoverableError as e:\n\t\t\tif verbose:\n\t\t\t\tsys.stderr."}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t\n\tif type(args)!= dict:\n\t\traise Exception('The playbook to invoke must be a dict. Got {}'.format(args))\n\t\n\tlogger.debug('Calling playbook {}', playbook_path)\n\t\n\tfor k,v in args.iteritems():\n\t\tif type(v) == DictType:\n\t\t\tv['type'] ='script'\n\t\t\tif verbose:\n\t\t\t\tv['verbose'] = True\n\t\t\t\n\t\t\tv['vars'] = copy(_extra_vars(v))\n\t\t\t\n\t\t\tv['_ds_file'] = os.path.join(ir_workspace.get_workspace(),\n\t\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif verbose: print \"Preparing playbook_path (%s)\" % playbook_path\n\t\t\n\texcept NameError:\n\t\tif verbose: print traceback.format_exc()\n\t\treturn\n\ttry:\n\t\ttry:\n\t\t\twith NamedTemporaryFile(prefix='ansible-playbook-',delete=False) as f:\n\t\t\t\tf.write(ir_workspace.name+\" \"+ir_plugin.name+\" (\"+str(ir_plugin.version)+\")\")\n\t\t\twith NamedTemporaryFile(prefix='ansible-playbook-',delete=False) as g:\n\t\t\t\tg.write(ir_workspace"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, ir_workspace, ir_plugin, verbose=None, extra_vars=None, ansible_args=None):\n\t\tif verbose is not None:\n\t\t\tself.verbose = verbose\n\t\tif extra_vars is not None:\n\t\t\tself.extra_vars = collections.OrderedDict(list(extra_vars.items()) + [\n\t\t\t\t'name', 'author', 'email','status', 'url',\n\t\t\t\t'github_owner', 'github_repo', 'github_username', 'github_url', 'github_description',\n\t\t\t\t'git_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\n\tif verbose:\n\t\tprint(\"  * Using ansible-playbook(%s, %s)\" % (repr(ir_plugin), repr(ir_workspace.name)))\n\t\tprint(\"  * Using %s as ansible-playbook(%s, %s, %s)\" % (repr(ansible_args), repr(ir_plugin), repr(ansible_workspace.name), repr(playbook_path)))\n\t\n\treturn ir_plugin.run_plugin_playbook(ir_workspace.name, ansible_args, ansible_args[\"extra_vars\"])\n\ndef ansible_playbook_file(ir"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t    \n\tsys.stdout = StringIO()\n\t__log_invocation()\n\tconsole_width = os.isatty(1)\n\tlog = cli.Log('ansible')\n\t_log_invocation('ansible-playbook {}',''.join(playbook_path), verbose)\n\n\tfor item in ir_workspace.plugins:\n\t\titem_info = item.kwargs\n\t\t_log_invocation('ansible-playbook plugin-{}'.format(item_info['plugin_id']),''.join(item_info['name']), verbose)\n\t_log_invocation('ansible-playbook {}', '"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\t# Set the plugin to 'default'\n\t\targs_dict = dict(\n\t\t\tplugins = [\n\t\t\t\tdict(\n\t\t\t\t\ttype='docker_hook',\n\t\t\t\t\tplay=None,\n\t\t\t\t\tcommand='default',\n\t\t\t\t),\n\t\t\t\tdict(\n\t\t\t\t\ttype='docker_pull_image',\n\t\t\t\t\tregistry=plugins_registry(),\n\t\t\t\t\targs_schema=plugins_schema()\n\t\t\t\t)\n\t\t\t]\n\t\t)\n\n\t\tif verbose is not None:\n\t\t\targs_dict['_ansible_verbose'] = verbose\n\t\tif extra_vars:\n\t\t\targs_dict['extra_vars'] ="}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n        \n        # Check if host exists before continuing\n        ansible_host    = get_ansible_host(ir_workspace, ir_plugin)\n        if _check_host_exists(ansible_host):\n            return _run_plugin_playbook_ansible_playbook(ir_workspace, ir_plugin,\n\t\tansible_args, verbose, extra_vars, ansible_host)\n    else:\n        # Check if any host exits before continuing\n        ansible_host    = get_ansible_host(ir_workspace, ir_plugin)\n        if not ansible_host:\n            print \"Could not run Ansible\"\n            return ("}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\targv, remainder = split_cli(ir_workspace)\n\ttry:\n\t\tp = pexpect.spawn(argv=argv)\n\t\tstdoutin, stdoutout = p.communicate()\n\n\t\t# We're dealing with the fact that the args were successfully\n\t\t# populated. Ideally, the playbook has started and finished successfully,\n\t\t# or it was successfully stopped.\n\t\tif verbose is not None:\n\t\t\tstdoutin = stdout_in + \"\\n\\n\"\n\t\n\t\t\tstdoutout = None\n\t\t\tif extra_vars is not None:\n\t\t\t\tstdoutout = stdout_"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\t# read in the arguments\n\targuments = vars_dict['_raw_parameters']\n\tcli_options = vars_dict['_cmd_line_options']\n\n\t# read in the ir_args\n\twith open(ir_workspace.working_dir + \"/ir.yml\", \"r\") as irm:\n\t\tir_args = yaml.load(irrm.read())\n\t\n\t# create vars_dict\n\tvars_dict = yaml.load(vars_dict)\n\t\n\t\n\t# run the ansible command\n\t# if a test method provided check that it is callable\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t    \n\tprint(\"Running ansible cli:\\n=============\\n\")\n\tif vars_dict is None:\n\t\tvars_dict = get_vars_default(cli_args)\n\tif vars_dict is None:\n\t\tprint('No vars file specified')\n\t\treturn 1\n\tvars_dict = _replace_vars(vars_dict)\n\tconfig = configparser.ConfigParser()\n\tconfig.optionxform=str\n\n\ttry:\n\t\tconfig.read(vars_dict)\n\texcept NoSectionError:\n\t\tprint('There were no sections for this profile, exiting...')\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n        # Check for missing arguments in vars dict\n        if vars_dict['vars']:\n            missing_vars_list = list(vars_dict['vars'].keys())\n            if missing_vars_list and len(missing_vars_list) == 0:\n               vars_dict['vars'] = {}\n\n        # Re-init vars if vars_dict does not contain all default_vars\n        # Check for all required args\n        if not vars_dict:\n            cli_args = []\n            # No vars given to run playbook\n            cli_args.append(os.path.basename(vars_dict['cli'].command_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\t# If variables were specified in any cli args, use them\n\t\t# else default to empty dict\n\t\tcli_vars = dict()\n\t\tcli_vars[\"script\"] = \"\"\n\t\tcli_vars[\"env\"] = \"\"\n\t\t# Pass variables as CLI args\n\t\tfor key in vars_dict[cli_args][0]:\n\t\t\tcli_vars[cli_args][1][key] = vars_dict[cli_args][1][key]\n\t\t# Create an instance of the interpreter class\n\t\tinterpreter = InfraredInterpreter(cli_args, vars_dict, ir"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t#get yaml file name from the template and the input to run\n\t#if is it a yaml file name, extract vars\n\tif ir_plugin.yaml_file_name:\n\t\t#resolve vars\n\t\tvariables_str = ir_plugin.yaml_file_name\n\t\tfor index, var in enumerate(variables_str.split(\",\")):\n\t\t\t\n\t\t\t#resolve var key\n\t\t\tkey = variables_str[index]\n\t\t\tvar = variable_replace(variables_str[index], key, var)\n\t\t\t\n\t\t\t#check vars format\n\t\t\tif var.template_vars and var."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\t# Run ansible tests\n\tfor env_var in os.environ:\n\t\tenv[env_var] = os.environ[env_var]\n\t\n\t# Set shell env\n\tif 'SHELL' in vars:\n\t\tcmd = \" \".join([\"sshpass\", \"-p\", vars['SHELL']])\n\t\tif 'SSHPASS' in vars:\n\t\t\tcmd = [\"sshpass\", \"-p\", vars['SSHPASS']]\n\t\t\n\t# Set user shell env\n\telif 'USER' in vars and 'SHELL' in vars['USER']:\n\t\tcmd = subprocess.list2cmdline(['"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\tif cli_args.get('dry_run'):\n\t\tif vars_dict and vars_dict['__ansible_verbosity'] >= 2:\n\t\t\t# print out extra vars before processing any more\n\n\t# Set up the arguments parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Ansible inventory validation tool', \n\t\tepilog=''' \nThis tool makes use of a playbook using ansible-playbook (ansible inventory.yml)\n(and any ansible arguments) to validate inventory.yml values. The script\nsets up the ansible playbook from the command line arguments, so you can also add \nadditional arguments to"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\n\ttry:\n\t\treturn cli(**ir_plugin.run_args(vars_dict))\n\texcept Exception, e:\n\t\tlogger.exception(e)\n\n\"\"\" Run Ansible cli\n\"\"\"\ndef run_ansible(cli_args, vars_dict, ir_workspace, ir_plugin, debug=False,*argv,**kwargs):\n\t\"\"\" Will run ansible cli with vars dict\n\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args:  list of command line arguments\n\t:param ir_workspace: An Infrared"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\t# if no args given in ansible_run\n\tif not vars_dict:\n\t\t# if vars_dict is None, that means we are doing a no-op\n\t\treturn 0\n\n\t# run the playbook, but only if there was actually one argument\n\tif 'command' in vars_dict:\n\n\t\tcli = AnsibleCLITask(vars_dict['command'])\n\n\n\n\t# get a reference to the current plugin\n\tcurrent_plugin = None\n\n\t# if we are in the middle of an IR, do not look for the last plugin\n\tif ir_plugin."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\n\tprint \"Running Ansible with vars: \",vars_dict\n\n\ttry:\n\t\tenv_vars = get_env_variables(vars_dict)\n\texcept AnsibleUndefinedVariable as e:\n\t\tprint \"Failed to load Ansible environment vars. Run `sudo ansible-playbook --list-vars` to fix.\"\n\t\tsys.exit(1)\n\n\n\tansible_vars = {}\n\n\n    if vars_dict:\n    \tfor item in vars_dict:\n\t\tif vars_dict[item] in ['True', 'False', 'None']:\n\t\t\tansible_vars[item] = vars_dict"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tfor k in values_dict:\n\t\ttry:\n\t\t\tvalues_dict[k] = unicode(values_dict[k].encode('utf-8'), 'utf-8')\n\t\texcept:\n\t\t\tvalues_dict[k] = values_dict[k].decode('utf-8')\n\n\nclass ArgHandler:\n\t\"\"\"\n\tClass to handle argparse options. Arguments supplied will not \n\tcontain the command itself.\n\t\"\"\"\n\tdef __init__(self):\n\t\n\t\tself.cli_args = {}\n\t\t\n\tdef add_cli_object(self, name, class"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tdef _create_args(self):\n\t\tdefault = {\n\t\t\t\"name\" : \"\",\n\t\t\t\"value\" : \"\",\n\t\t\t\"help\" : \"\",\n\t\t\t\"required\" : False}\n\t\t\n\t\t# Make sure name is defined and that given a value\n\t\tif 'name' in values_dict:\n\t\t\tfor name in values_dict['name']:\n\t\t\t\ttry:\n\t\t\t\t\tparam = _get_param(name)\n\t\t\t\t\tparams_strval = param.value\n\t\t\t\t\tparam_types_str = get_types_convert_to_bool(param)\n\t\t\t\t\tdefault["}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tdef _is_string_string(self, value):\n\t\tif type(value) == types.StringType:\n\t\t\treturn True\n\t\tif value[-1] in value[0]:\n\t\t\treturn True\n\t\treturn False\n\t\n\tdef _convert_int_to_bool(self, value):\n\t\ttry:\n\t\t\treturn int(value, 0)\n\t\texcept ValueError or TypeError as e:\n\t\t\traise ValueError(\"Invalid numeric value for int '%s: %s\"' % (value, e.message))\n\t\t\t\n\tdef _is_int(self, value):\n\t\tv"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t# Get arguments and parse\n\tif not len(values_dict):\n\t\treturn None\n\t\t\n\tcmd = values_dict.get(\"command\", None)\n\targs = values_dict.get(\"args\", None)\n\t\n\t# If no cmd or args passed, set command to default\n\tif cmd == None or cmd == \"\":\n\t\tcmd = self.default_command\n\t\n\t# Add the special options\n\tcommon_options = {}\n\tfor key, value in values_dict.items():\n\t\tif value == None:\n\t\t\tcommon_options[key] = \"--%s"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\tdef parse_command_line(\n\t\tself,\n\t\tcommand_line,\n\t\targ_list=None,\n\t\tdebug=False\n\t):\n\t\t\"\"\"\n\t\tThis function parses the CLI interface for command line.\n\t\tIt uses the default values in argparse.\n\t\tCLI interface options parsed from the provided CLI interface\n\t\tparser_params is then parsed using these options:\t\n\t\t-arg_list: If a command line option contains a variable as argument\n\t\t-debug: If set to true - additional detailed information will be displayed\n\t\t\t\t\t\tas well\n\t\tThe remaining args that are"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tfor k, v in values_dict.items():\n\t\tif v is None:\n\t\t\tcontinue\n\t\tself[self._get_name_for_key(k, v)] = v\n\n\n    \n\n# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4\nimport re, sys, os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'lib'))\nimport Utils\nfrom Config import Config\nfrom Logger import *\n\nsys.path.append(os.path.join(os.path.dirname(__file"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\n\t_command_template = \"\"\"\ndef execute(command, stdout, stderr, return_status, return_error):\n\tif return_status!= 0:\n\t\tresult = process.returncode;\n\t\tif result.lower() == FABRIC_RESULT_NO_MORE:\n\t\t\treturn False\n\telse:\n\t\tlogging.debug('execution failed for command')\n\t\tlogging.debug(\n\t\t\t' stdout:'+ stdout if stdout else ''\n\t\t\tif stdout else 'None'\n\t\t)\n\t\treturn False\n\n\t_output_template = \"\"\"\ndef _unmap("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t    \n\tfor key in values_dict:\n\t\ttry:\n\t\t\tvalues_dict[key] = _value_helper(parser_name, key, values_dict[key])\n\t\texcept TypeError as vex:\n\t\t\tself.error(\"Values can't be converted into CLI arguments: %s\" % vex)\n\t\t\traise\n\t\n\treturn values_dict\n\t\t\n\t\t\n\t\t\n#!/bin/env python\n\nimport baseband_interface\nimport sys\n\n\n\ndef parse_options(cmdline_args):\n    shortopts = \"v:n:h\"\n    longopts = [\"v\"]\n\n   "}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tdef arg_check(self):\n\t\t\"\"\"\n\t\tChecks whether the arg values_dict parameters are valid.\n\t\t\n\t\t:return: True on valid arg values and False otherwise\n\t\t\"\"\"\n\t\t# If the value from the args is not an array\n\t\tif not isinstance(self.values_dict, (tuple, list)):\n\t\t\traise ValueError('The value of the args {0} must be an array.'.format(self.name))\n\t\ttry:\n\t\t\tif len(self.values_dict)!= len(set(self.args)):\n\t\t\t    raise ValueError('The values_dict"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tfor arg, value in values_dict.items():\n\t\tvalues_dict[arg] = self._convert_value(arg, value)\n\n\n\tdef _convert_value(self, arg, value, target_type=None):\n\t\t\n\t\tif target_type in ['integer', 'boolean', 'bytes', 'hex', 'int', 'enum']:\n\t\t\tvalue_type = 'number'\n\t\telse:\n\t\t\tvalue_type = self.guess_type(arg, value)\n\t\n\t# convert command line arguments\n\tif isinstance(arg, CmdlineArgument):\n\t\tfor arg"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t    from itertools import chain\n\t    result = {}\n\t    for name in os.listdir(plugin_dir):\n\t        if not name.lower().endswith('.py'):\n\t            continue\n\t        result[os.path.lower(name)] = getattr(sys.modules[__name__], name)\n\t    \n\t    return result\n\ndef get_plugins(plugin_dir):\n\t\"\"\"\n\tReturns all plugin's classes\n\n\t:param plugin_dir: A path to the directory of the plugin\n\t:return: A list contains plugin's classes\n\t\"\"\"\n\t\n\tif not plugin_dir in sys.path:"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tplugin_spec_filename = os.path.join(plugin_dir, \"plugin_spec.json\")\n\t\n\tif not os.path.exists(plugin_spec_filename):\n\t\treturn dict()\n\t\n\twith open(plugin_spec_filename) as f:\n\t\tplugin_spec = _json.load(f)\n\t\n\treturn plugin_spec\n\ndef get_plugin_spec_file_from_location(plugin_type, plugin_dir):\n\t\"\"\"\n\tRetrieves the plugin spec file, and creates a plugin definition dictionary\n\t\n\t:param plugin_type: The"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\text = dict(\n\t\tplugin_dir = plugin_dir,\n\t\tprefix\t = None,\n\t\tprefix_len = 0,\n\t\tfiles      = [],\n\t)\n\t\n\tif os.path.exists(os.path.join(plugin_dir, '__init__.py')):\n\t\ttry:\n\t\t\texec(compile(open(os.path.join(plugin_dir, '__init__.py')).read(), 'EGG_PLUGIN=1') in _dict)\n\t\texcept Exception as e:\n\t\t\traise AnsibleError(\"\"\"Python plugin %s could not be loaded due to dependencies."}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tplugin_spec_list = []\n\tplugin = octoprint.util.dict_get_any(config._plugin_overrides, plugin_dir, plugin_spec_name, allow_unset=False)\n\t\n\tif plugin is None:\n\t\treturn None\n\telse:\n\t\t#merge in any overrides\n\t\tfor override in defaults:\n\t\t\tif config._overrides[override].pop(\"type\", None) is not bool:\n\t\t\t\toverride_value = config._overrides[override].pop(\"_value\", None)\n\t\t\t\tif override_value is not None:\n\t\t\t\t\tplugin_spec"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tplugin_spec = {}\n\t\t\n\troot_dir = os.path.dirname(plugin_dir)\n\tplugin_dir = os.path.join(root_dir, \"plugin\")\n\t\n\t# create a list of plugins\n\tplugin_list = find_plugins_from_dir(plugin_dir) \t\n\t\n\t# combine the path and plugin name into a valid plugin_spec\n\tfor path in plugin_list:\n\t\tspec = {}\n\t\tspec['name'] = os.path.basename(path)\n\t\t\n\t\tif not spec: \n\t\t\traise Exception(\"No file or directory"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\n\tproperties = {}\n\tif not os.path.exists(plugin_dir):\n\t\treturn properties\n\tproperties_file = os.path.join(plugin_dir, \"properties.conf\")\n\tif os.path.exists(properties_file):\n\t\tproperties = yaml.safe_load(open(properties_file).read())\n\t\tif ('plugin' in properties):\n\t\t\tfor k, v in plugin_properties_merge(properties).items():\n\t\t\t\tproperties[k] = prepare_config_dict(v)\n\treturn properties\n\ndef build_config_dict"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\t# This is so we can iterate over a list of all plugins and do\n\t# the deep copy\n\tplugins = []\n\tfor filename in os.listdir(plugin_dir):\n\t\tif not fnmatch.fnmatch(filename, '*.py'):\n\t\t\tcontinue # Skip files that end in.py\n\n\t\tfilename = os.path.join(plugin_dir, filename)\n\n\t\t# Read and parse the plugin name\n\t\tif filename.endswith(\".py\"):\n\t\t\tparsed_plugin_name = basename(filename)[:4]\n\t\n\t\t\ttry:\n\t\t\t\tplugin_settings = PluginSettings.parse"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\treturn plugin_spec_flatten_dict\n\ndef get_plugin_spec_config_dict(plugin_dir):\n    '''\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flat dict contains a plugin's configuration\n\t'''\n    \n    return plugin_spec_config_dict\n\ndef get_plugin_spec_manifest_dict(plugin_dir):\n    '''\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return:"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tif not hasattr(plugin_dir, \"yaml_snippet\"):\n\t\traise ValueError(\"plugin_dir MUST be a directory path.\")\n\t\t\n\tif not plugin_dir.endswith(\"/plugins\"):\n\t\traise ValueError(\"plugin_dir MUST end with.yaml_snippet and not.plugins\")\n\t\t\n\treturn yaml.safe_load(open(plugin_dir + \"/plugins/yaml_snippet\"))\n\nclass PluginManager(object):\n\t\"\"\"\n\tThis class defines all of the PluginManager plugins.\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.plugins = {}\n\t\n\t\n\tdef"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\ndef get_plugin_spec_get_all_properties(plugin_dir):\n\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A dict containing all plugin's properties\n\t\"\"\"\n\t\ndef get_plugin_spec_get_all_parameters(plugin_dir):\n\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flat dict containing all plugin's parameters\n\t\"\"\"\n\t\ndef get"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tself.config = ConfigValue(os.environ[\"CONFIGFILE\"])\n\t\tself.config._config_modified()\n\n\tdef __str__(self):\n\t\treturn str(self.config.filename)\n\n\tdef write(self):\n\t\tcontent = \"\"\n\n\t\tfor (key, val) in self.config.items():\n\t\t\tcontent += \"%s=%s\\n\" % (key, val)\n\n\t\tcontent += get_value_recursive(self.config[\"ROOT\"]) + \"\\n\"\n\n\t\topen(self.config[\"LOCAL_ROOT\"] + \"/config.txt\", \"w"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif not os.environ.has_key(\"CONFIG_FILE\"):\n\t\t\treturn\n\n\t\ttry:\n\t\t\twith open(self[\"CONFIG_FILE\"], \"r\") as f:\n\t\t\t\tself[\"APP_CONFIG\"] = f.read()\n\t\texcept IOError, e:\n\t\t\tprint \"Error while loading configuration file '%s' (%s)\" % (self[\"CONFIG_FILE\"], e)\n\t\t\traise\n\n\tself[\"CONFIG_FILE\"] = DEFAULT_CONFIG_FILE\n\n\treturn self[\"CONFIG_FILE\"]\n\n# vim: filetype=python\n# Copyright 2014 Pants project contributors ("}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tself.config={}\n\tdef inject_environ(self):\n\t\t\"\"\"Set or unset the environment variable\"\"\"\n\t\tif not os.environ:\n\t\t\tprint \"Warning: The environment variable is not set! Set it to \\\"unset\\\" to get a good behavior.\";\n\t\t\tprint \"Defaulting to \\\"unset\\\"...\"\n\t\t\tos.environ={}\n\t\tself.config.update({\n\t\t\t'PATH':'',\n\t\t\t'PYTHON':None,\n\t\t\t'PYTHONDEBUG':None\n\t\t})\n\n\tdef set(self, name, value, append=False):\n\t\t\"\"\"\n\t\tSet a"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\n\t    if self.config_path:\n\t\tenv_path = self.env_path\n\telse:\n\t    env_path = env['PATH']\n\n\t# try to find the site specific env variable\n\tenviron = os.environ\n\tif 'PYTHONHOME' in environ: # This only for debugging\n\t\thome = environ['PYTHONHOME']\n\t\tenv_path = [home, env_path]\n\telse:\n\t    env_path = env['PATH']\n\n\tif env_path and env_path[0] == ':':\n\t    if not ':' in env_path[0]:\n\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif not hasattr(self,'config_path'):\n\t\t\tself.config_path = 'config'\n\t\tif self.config_path not in self.conf:\n\t\t\tself.conf[self.config_path] = dict()\n\t\t\n\tdef generate_config(self):\n\t\"\"\"\n\tMake the configuration path, if it is undefined.\n\t\"\"\"\n\t\tif not hasattr(self,'config_path'):\n\t\t\tself.config_path = 'config'\n\n\tdef write(self):\n\t\t\"\"\"\n\t\tWrite the configuration file. The configuration\n\t\tis written to the <out"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif self.config:\n\t\t\ttry:\n\t\t\t\tconf.load(self.config)\n\t\t\texcept Exception as e:\n\t\t\t\tself.logger.info(\"Failed to load config\")\n\t\t\t\traise e\n\n\tdef get_config(self):\n\t\t\"\"\"\n\t\tReturn the configuration as a dictionary.\n\t\t\"\"\"\n\t\treturn self.config\n\n\"\"\"\nBase data structure and utilities for pandas objects.\n\"\"\"\n\nimport numpy as np\n\n\ndef _possibly_unify_to_empty(data):\n    \"\"\"If the input data is empty, replace it with None.\n    \"\"\"\n    if"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif self.name is Undefined:\n\t\t\traise ConanException(\"Undefined variable name: %s\" % self.name)\n\t\tif os.environ.has_key(self.name) == False:\n\t\t\tprintError(\"Can't find '%s':%s\" % (self.name,os.environ.keys()))\n\t\t\traise ConanException(\"can't find '%s':%s\" % (self.name,os.environ.keys()))\n\t\tdel os.environ[self.name]\n\n\nclass Utils(object):\n\t\"\"\"\n\tObject that represents and encapsulate"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\twith open('/etc/nginx/sites-available/domain0/domain0.pem', 'w') as fh:\n\t\t\tfh.write(\"\"\"server {\n\tlisten         188.249.64.2:9000\n\troot           /\n\ttls_verify      off\n\ttls_client_cert /etc/nginx/sites-available/domain0/domain0.pem\n}\n\"\"\")\n\t\tprint \"Configured a site for domain0.pem by placing /etc/nginx/sites-available/domain0.pem by your pom. The domain0.pem is now"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tos.environ['PATH'] = \"%s:%s:%d\" % (\n\t\t\tself.basedir, self.config.name, self.config.name)\n\n\t\timport configobj\n\t\tconfigobj.ConfigObj.__init__(self)\n\n\tdef check_config_paths(self):\n\t\t\"\"\"\n\t\tCheck that paths are valid.\n\n\t\tNote: don't call this as it calls the\n\t\t`ConfigObj.get() <ConfigObj.expand_paths()`,\n\t\talso call this recursively since we don't want\n\t\tto modify our configuration objects to depend on\n\t\tour parents"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\t(config, args_used) = Config(), ConfigArgs()\n\n\t\t# make a copy so we don't mess up the variable when changing it\n\t\tenv_var = copy.copy(os.environ.get('NOCOLOR'))\n\t\tfor key in args_used.keys():\n\t\t\tif key not in env_var.keys() and env_var.get(key):\n\t\t\t\tenv_var[key] = args_used[key]\n\n\t\tself.config = env_var\n\t\treturn config\n\n\tdef get_config(self, key=None, env_var=None"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tfor sc_section in root_subparsers:\n\t\t\n\t\t# Initialize \n\t\tself.add_cli_arguments(subparser)\n\t\t\n\t\t# Add any command-specific options\n\t\tadd_parser_arguments(subparser, sc_section)\n\t\t\n\t\t# Add any other CLI specific options to the command-specific options\n\t\tadd_parser_options(subparser, sc_section)\n\t\n\t# Add command-specific help options\n\tadd_parser('usage', help='Command entry point help', 'Shows a usage guide on the command line.')\n\n\t# Add cli-specific help options\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n@add_gui_option(\"-v\", \"--verbose\", \"--verbose\", dest=\"verbose\", \n    action=\"store_true\",\n    help=\"make more verbose output\")\n\t\n\t\n@add_gui_option(\"-q\", \"--quiet\", dest=\"quiet\",\n    action=\"store_true\",\n    help=\"suppress non-error messages\")\n\t\n\t\n@add_gui_option(\"-v\", \"--version\", action=\"version\", version=\"%%(prog)s (vers %%(ver)s)\"%globals())\ndef main():\n\t\"\"\"\n\tThe main entry point.\n\t\"\"\"\n\t\n\tlog.level = Log"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tself.spec_cli_args.add_argument(\n\t\t\t'--spec-path',\n\t\t\ttype=str,\n\t\t\thelp='Specify an alternate path to search for spec files.',\n\t\t\tdefault='/specs',\n\t\t\trequired=False\n\t    )\n\t\n\t\n\t\n\tself.spec_cli_args.add_argument(\n\t\t\t'--spec-filters',\n\t\t\ttype=str,\n\t\t\tchoices=['*'],\n\t\t\thelp='The filters to apply when doing spec files.',\n\t\t\tdefault='*:'\n\t\t)\n\t\t\n\t\tself.spec_cli_args.add_argument(\n\t\t\t'--"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tconf_parser = self._conf_parser(root_subparsers)\n\texcept (OSError, IOError):\n\t\tprint >>sys.stderr, \"Error: Unable to open the configuration files\"\n\t\tprint >>sys.stderr, \"Make sure the /etc/contrail/\" \\\n\t\t\t\"contrail.conf exists.\" \\\n\t\t\t\" If there's another issue, make sure the current user set \" \\\n\t\t\t\"the correct one.\"\n\t\tsys.exit(1)\n\t\n\tfor obj_conf in conf_parser:\n\t\tobj_obj = obj_conf"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tsub = subparser.add_parser('extend',\n\t\tdescription = 'Intermediate'+ ctime(version=__version__) + \\\n\t\t' interface.')\n\t\n\tsub.add_argument(\n\t\t'--all',\n\t\taction ='store_true',\n\t\thelp = '(Re)enable all the subroutines.',\n\t\tdest = 'all')\n\tsub.add_argument(\n\t\t'--dry-run',\n\t\taction ='store_true',\n\t\thelp = '(Re)just print the generated commands.',\n\t\tdest = 'dry-run')\n\tsub"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\t\n\tpass\n\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tcli_parser.add_argument(\"spec_file\", \n\t\t\tmetavar=\"SPEC_FILE\", \n\t\t\thelp=\"the path to the spec file to be parsed. \" \\\n\t\t\t\"If this is None, then all spec files are parsed\",\n\t\t\tnargs='?')\n\tcli_parser.add_argument(\"--force\", \n\t\t\tnargs='?', type=int, \n\t\t\tdest='force', \n\t\t\tdefault=3,\n\t\t\thelp=(\"Force overwrite of intermediate file.\"),)\n\tcli_parser.add_argument(\"--skip\", \n\t\t\thelp=\"skip the first N positions of the merged read.\", \n\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsuper(SkyCoord, self).extend_cli(root_subparsers)\n\t\t\n\t\t# Add the instrument command\n\t\tparser = set_basic_command(self, SkyCoord.instrument_commands())\n\t\tparser.add_argument(\"--instrument\",\n\t\t                   choices=SkyCoord.instrumented_instruments(),\n\t\t                   help=\"Selects instrument\")\n\t\t\n\t\t# Add the start/stop command\n\t\tparser.add_argument(\"--start\",\n\t\t                   action=\"store\", type=int, dest=\"start\",\n\t\t                   help=\"Starting number for the scan\")\n\t\tparser.add_argument(\"--"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tcli_root_subparsers = _make_cli_subparsers(root_subparsers)\n\t\n\tcli_subparsers = cli_root_subparsers.add_subparsers(metavar='SUBCOMMAND')\n\tcli_subparsers.required = True\n\t\n\tsubcmd = _make_subarg_parser(cli_root_subparsers.name,\n\t\t'subcommand', \n\t\t'add sub command')\n\t\n\tcmd_subparsers = cmd_root_subparsers.add_subparsers(metavar='COMMAND')\n\tcmd_subparsers.required = True\n\tcmd_"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(self, \"cli_options\"):\n\t\topt_group = self.parser.add_argument_group(\"Common Options\", \"Configuration specific to all commands\")\n\t\tself.opt_group = opt_group\n\t\n\tfor subcommand in self.cli_options.itervalues():\n\t\tparser_kw = getattr(self, subcommand.name.lower(), None)\n\t\tcli_group = getattr(root_subparsers, subcommand.name.lower(), None)\n\t\t\n\t\tparser_args = getattr(subparser, \"args\", None)\n\t\tif parser_args:\n\t\t\tparser"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\n\tret = path\n\tfor i in xrange(len(path) - 1, -1, -1):\n\t\tsplit = path[i:i+2]\n\t\ti_split = split.split('@')\n\t\twhile i_split and i_split[0] in root:\n\t\t\ti_split.pop(0)\n\t\twhile len(i_split) > 0 and i_split[-1] in root:\n\t\t\ti_split.pop()\n\t\tif not i_split:\n\t\t\tdel path[i]\n\t\tif i_split[0] in"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\n\troot = path_strip(path)\n\n\tif root == \"/\":\n\t\treturn \"\"\n\t\t\n\tif len(root) == 0:\n\t\troot = \"/\"\n\n\treturn root.replace(\";\", \"/\").replace(\",\", \"\").replace(\".\", \"\")\n\n\ndef file_hash(path, blocksize=8192):\n\n\twith open(path) as f:\n\t\ttry:\n\t\t\treturn hashlib.sha1(f.read()).hexdigest()\n\t\texcept OSError:\n\t\treturn \"unknown\"\n\n\ndef get_checksum(path):\n\tif os.path.exists(path) and os.path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\t\n\tif root == path or root == '.' or root == '\\\\':\n\t\treturn\n\t\t\n\tif len(path) == 0:\n\t\treturn\n\t\t\n\tfor path2 in path.split('\\\\'):\n\t\tif len(path2) > len(root):\n\t\t\tpath2 = path2 + '\\\\'\n\n\t\tpath2 = path2.replace(path2, '', 1)\n#\n# @BEGIN LICENSE\n#\n# Psi4: an open-source quantum chemistry software package\n#\n# Copyright (c) 2007-2019 The Psi4 Developers.\n#\n# The"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\t\n\tpath = path.rstrip(\"/ \") # FIXME: remove in v1.1\n\tpath = os.path.normpath(path)\n\tif path not in (\".\", \"..\"):\n\t\treturn False\n\tif os.path.isabs(path) or \"..\" in path or len(path) > 0:\n\t\treturn False\n\telse:\n\t\treturn True        \n\ndef check_file_permission(filename, mode=\"w\", max_perms=10):\n\tfile_mode = os.stat(filename).st_mode if stat.S_ISREG(mode)"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\n\tdef makedirs_count(path, count):\n\t\tif count == 0:\n\t\t\ttry:\n\t\t\t\tos.makedirs(path)\n\t\t\texcept Exception as error:\n\t\t\t\tif error.errno!= errno.EEXIST:\n\t\t\t\t\traise Exception(\"Could not create the directory %s\" % path)\n\t\telif count == 1:\n\t\t\tos.makedirs(path)\n\t\telif count > 1:\n\t\t\traise Exception(\"Found more than one %s, you should use the force option\" % count)\n\t\telse:\n\t\t\traise Exception(\"WTF?\")\n\n\t\n\treturn [path for"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\t#if len(root) > 0 and root[-1] == '/'\tand len(root[0:-1]) > 0:\n\t\t#\tpath = root + path[1:]\n\treturn path\n\n\ndef process(fname, path, root):\n\t\"\"\"\n\tProcess the file at path. If root is a string, search the path for\n\tthis prefix instead.\n\t\"\"\"\n\t# Strip the final part of the path\n\tpath = strip_root(path, root)\n\n\t# Replace paths with empty strings\n\tif path == []:\n\t\treturn\n\tfor"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t      # os.walk(root, topdown = True)\n\tfor root2 in path:\n\t\tif len(path) > 1 and (root2!= root or root == root2):\n\t\t\tremove_path(path, root2)\n\t\telif len(path) > 1 and is_ignored_path(path):\n\t\t\tshutil.rmtree(path)\n\t\t\ttry:\n\t\t\t\tos.rmdir(path)\n\t\t\texcept (OSError, IOError) as _:\n\t\t\t\tpass\n\ndef remove_file(path):\n\tfor root, subdirs, files in os.walk(path):"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n#\n#  WARNING: the \"dirname(_) is already used by Pyflag\", but\n#  has been moved into the normal class since the fix was\n#  added for glibc.\n#\n# WARNING 2:  as of 3.2\n#\n# See the similar warning in\n#   http://bugzilla.gnome.org/show_bug.cgi?id=652125\n#   from pysqlite2.dbapi2 import DBImpl\n#\n# WARNING: the \"dirname(__file__) is not used because\n#  this module"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\n\ttry:\n\t\tos.remove(path)\n\texcept OSError:\n\t\tpass\t\n\ndef write_content(path, content, makedirs=False, chmod=None):\n\t\"\"\"\n\tWrite content to file or directory, return full path to file.\n\t\n\t:param path: File/Directory path\n\t:param content: Content for file/directory\n\t:param makedirs: Create parents if missing\n\t:param chmod: File permissions\n\t:return: File path\n\t\"\"\"\n\t\n\tif not makedirs:\n\t\tmakedirs(path)\n\n\tf=None\n\t\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t \n\ttry:\n\t\ttree = ET.parse(path) \n\texcept ET.ParseError as error:\n\t\tlog.debug('Error parsing element %s, skip it: %s', path, error)\n\t\treturn None\n\t\t\n\tif len(path.strip()) == 0:\n\t\tlog.warning('Not root node: %s', root)\n\t\treturn None\n\n\tpath_re = re.compile(r'(//(\\S+))\\d+')\n\tpath_root = path_re.search(root)\n\t\n\tif path_root is None:\n\t\tlog.warning('"}]}
