{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn [time.hour, time.minute, time.second, nanoseconds]\n\n\n# Helper functions\n\ndef is_timedelta_like(timedelt):\n\t\"\"\"\n\tCheck if passed timedelta is datetimelike.\n\t\"\"\"\n\treturn isinstance(timedelt, datetime.timedelta)\n\n\ndef add_delta(delta):\n\t\"\"\"\n\tAdd delta to timedelt (like integer/float) or global timedelta.\n\t\"\"\"\n\tif isinstance(delta, datetime.timedelta):\n\t\tdelta = delta.days*24*3600\n\n\treturn time"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = settings.TIME_ZONE\n\n\t\tif tz == 'America/Los_Angeles':\n\t\t\ttz = pytz.timezone('America/Los_Angeles')\n\n\t\ttz = tz.normalize(tz)\n\t\ttz = tz - settings.TIME_ZONE\n\n\t\tt = datetime.time(11, 23, 22, 00, 00, 00, 1, 0)\n\t\ttz = tz.normalize(tz)\n\n\t\ttime = datetime.datetime.fromtimestamp(t, tz)\n\n\t\treturn time\n\n\ndef hydrate_date"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\tif tz == \"UTC\":\n\t    tz_ = UTC\n\telse:\n\t    raise ValueError(\"tz must be UTC\")\n\treturn _hydrate_time(nanoseconds, tz)\n\n\ndef time() -> datetime:\n\t\"\"\"\n\tReturn the current time in the form of HH:MM:SS z, s, and Z:MM\n\t:param tzinfo:\n\t:param tz:\n\t:return: datetime\t\"\"\"\n\t    return _get_current_datetime()\n\n\n\ndef localtime(tz: Optional[Union[str, datetime.tzinfo]] = None) -> Optional"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\t\n\tif tz is None:\n\t\ttz = time.timezone\n\t\n\treturn Time(us + nanoseconds, tz = tz)\n\ndef parse_iso_time(string, tz=None):\n\t\"\"\"\n\tParse ISO 8601 and optionally timezone.\n\n\t:param string:\n\t:param tz:\n\t:return: datetime.time instance of `Time`\n\t:raise ValueError: in cases of malformed time\n\t\"\"\"\n\ttry:\n\t\tstring = time.strptime(string, \"%Y%m%d\")\n\texcept ValueError:\n\t\ttry:\n\t\t\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\tif tz is None:\n\t\tutctime = _time_tzinfo()\n\telse:\n\t\tutctime = tz.utc\n\tutc = utc.replace(microsecond=0, nanoseconds=nanoseconds, minutes=0, seconds=0, \n\t\tmilliseconds=NanosecondsToTime(float(nanoseconds)),\n\t\tmicroseconds=NanosecondsToTime(None), microsecond=0, \n\t\t\tyear=1970)\n\ttime_delta = nanoseconds - utctime\n\tnut = datetime()\n\treturn time_delta.total_seconds()\n\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\tres = None\n\tif tz is not None:\n\t\tres = time.gmtime()\n\t\ttzdiff = time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.gmtime(tz.hour, tz.minute, tz.second).timetuple()).split('.')[0] - res[0]\n\t\ttz_delta = tzdiff / 3600.0\n\t\ttime_delta = time_delta.split('T')[0] - tz_delta\n\t\ttzdiff -= tzdiff.microsecond / 1000\n\t\tres = time.strftime(\"%H:%M:%S\", time"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\n\tif tz is None:\n\t\ttz = tz_tz\n\ttry:\n\t\treturn nanoseconds.to_datetime() + tz.utcoffset(tz) * nanoseconds + nanoseconds.utcoffset(tz)\n\texcept Exception as e:\n\t\t# XXX: print(e)\n\t\traise Exception(\"failed to convert to datetime, {0}, {1}\".format(e.__class__.__name__, e))\n\t\t# XXX : print(type(e))\n\n\n\t# print(\"time to utc: {0}\".format(long(nanoseconds.to_datetime())))\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n#---------------------------------------------------------------------------\n\t\nif __name__ == '__main__':\n\n\t# Create a context object for the python interpreter. This must be the context object of the Python interpreter.\n\t#\n\t# This will be a copy that allows the original context to be updated in a way that can be run in-game threads.\n\t#\n\t# The default behaviour is to use\n\t#\n\t# from cytoolz import c as cy\n\t#\n\t# def simple_test_context(ctx):\n\t# \tc = c.Context.from_values(**ctx)\n\t#"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn Time(nanoseconds, tz)\t\t\n# LocalTime implementation\n\ndef localhour(dt):\n\treturn dt.hour\n\ndef localmin(dt, other=0, tz=None):\n\tif dt is None:\n\t\treturn 0\n\tt = dt.time()\n\tif other <= 0:\n\t\treturn (t + datetime.timedelta(seconds=-other)).hour\n\telse:\n\t\treturn (t + datetime.timedelta(seconds=+other)).minute\n\t\n\ndef localsec(dt):\n\treturn dt.microsecond*1.0E6\n\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tvalue = nanoseconds if tz is None else tz.localize(value)\n\t\treturn local_time(value, nanoseconds)\n# -*- coding: utf-8 -*-\n# Copyright (c) 2016-2019, Ericsson AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn datetime.timedelta(0)\n\texcept Exception as e:\n\t\tlogging.error(\"Could not calculate time dehydration: %s\" % str(e))\n\t\treturn None\ndef is_aware(value):\t\"\"\"\n\tCheck if `value` can be represented as `datetime.datetime`.\n\n\t:param value:\n\t:type value: datetime.datetime\n\t\"\"\"\n\t\treturn is_aware(value)\n\ndef is_aware_today(value):\t\"\"\"\n\tCheck if `value` can be represented as `datetime.datetime`\n\t\"\"\"\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ndef dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\t\ndef dehydrate_string(value):\t\"\"\"\n\tDehydrator for `string` values.\n\n:param value:\n:type value: basestring\t\n:return:\t\"\"\"\n\t\n\t\ndef dehydrate_datetime(value):\t\"\"\"\n\tDehydrator for `datetime` values.\n\n:param value:\n:type value: datetime"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn max([1, int(value.seconds)])\n\n@contextmanager\ndef add_timeout(secs=DEFAULT_TIMEOUT):\n\t\"\"\"\n\tA decorator which can execute any function before timing out.\n\t\"\"\"\n\n\tdef inner(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tstart_time = time.time()\n\t\t\twhile True:\n\t\t\t\ttries = 0\n\t\t\t\tresult = function(*args, **kwargs)\n\t\t\t\tif tries == 0 and time.time() - start_time >= THROTTLE_LIMIT:\n\t\t\t\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn value\n\n\tdef dehydrate(value):\n\t\treturn value.total_seconds()\n\n\thour = dehydrate(value)\n\tminute = dehydrate(value)\n\n\t# get delta for minutes\n\tdelta = relativedelta(minutes=minute)\n\n\t# compute duration in seconds\n\ttotal_seconds = delta.seconds//60\n\n\t# compute total minutes\n\thour *= 60\n\tminute *= 60\n\n\t# compute fraction of time remaining from current time (seconds since epoch)\n\tdelta *= 100000000\n\n\treturn total_seconds, hour, minute,"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ttry:\n\t\t\treturn datetime.timedelta(seconds=value)\n\t\texcept KeyError:\n\t\t\traise ValueError('timedelta {} is not a timedelta')\n\t\t\t# we could try to do this by using datetime.datetime.now() or datetime. timedelta().to_integral_value() or get_datetime()\n\t\t\t# but in a more general-purpose fashion it should be good enough to be more appropriate.\n\ndef seconds_to_seconds(timedelta):\n\t\"\"\"\n\tConvert seconds to seconds.\n\n\t:param timedelta:\n\t:type timedelta: datetime.timedelta\n\t:return:\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tvalue = value * -1\n\t\tnew_value = timedelta(days=value.days) # convert days to nanoseconds\n\t\treturn new_value\n\t\"\"\"\n\tdef __init__(self, start_date, end_date=\"today\"):\n\t\tself.start_date = start_date # start date in the future\n\t\tself.end_date = end_date # end date in the future\n\tdef to_dict(self):\n\t\t\"\"\"\n\t\tCreate a dictionary representation of this object.\n\t\t\"\"\"\n\t\tresult_dict = {\n\t\t\t\"date\": self."}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tif not isinstance(value, datetime.timedelta):\n\t\t\tvalue = datetime.timedelta(seconds=value)\n\t\tvalue = value/timedelta(0, 3600) # In hours, in minutes\n\t\treturn value\n\ndef dehydrate_float(value, unit='minutes'):\n\t\"\"\"\n\tDehydrator for `float` values.\n\n:param value:\n:type value: float or int or float\n:return:\t\"\"\"\n\t\tif isinstance(value, (int, float)):\n\t\t\tvalue = float(value)\n\t\tif isinstance(value, compat.integral"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tm = r'[1-9][0-9]|' + r'(?:[1-9][0-9]+)'\n\t\tm = m.replace(' +','' + r'{0}[^0-9]')\n\t\tm = m.replace('-','')\n\t\tvalue = value.replace(',', ',')\n\t\tvalue = value.strip()\n\t\tvalue = int(value)\n\t\tm = m.findall(m)[0]\n\t\tvalue = value.rstrip('0')\n\t\tvalue = int(value)\n\t\tvalue = float(value)\n\t\tif value < 0"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tvalue += value\n\t\treturn timedelta().total_seconds()\n\t\n\tdef get_weekend_duration_for_month_and_day(self, month, day=0, week=0, month_offset=0):\n\t\tmonth = self.__convert_month(month)\n\t\tday += 1\n\t\tday_offset = self.__convert_day(day, day_shift=False)\n\t\tyear = month\n\t\tif isinstance(week, datetime):\n\t\t\tmonth_offset = (month_offset - week.day) % 7\n\t\telif isinstance(month, int):\n\t\t\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tvalue = value.days * 24 * 60 * 60\n\t\tif value == 1429728700:\n\t\t\tvalue = value.replace(year=1984)\t# Birthday\n\t\telif value == 1852695:\n\t\t\tvalue = value.replace(year=1965)\t# Birthday\n\t\telif value == 19364517:\n\t\t\tvalue = value.replace(year=1982)\t# Birthday\n\t\telif value == 115185400:\n\t\t\tvalue = value.replace(year=1989)\t# Birthday\n\t\telif value"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tvalue = float(value)\n\t\tif value < 1e-6: value += 1e-6\n\t\telif 1e-5 < value < 1000.: value += 1000.\n\t\t\tif 6 <value < 1000.: value = 1000e+6\n\t\telif int(value) == 0:\n\t\t\treturn 1\n\t\telse:\n\t\t\treturn 0.5 * value / 100.\n\nget_base_time_base_for = dehydrate_time\n\n\n\n\nclass GestureTime(Equilibrium):\n\tdef __init__(self, start, end, step"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tvalue = float(value)\n\t\treturn float(1) * (1. - (value-1.0))\n\t\t\n\ndef dehydrate_offset(value):\t\"\"\"\n\tDehydrator for `offset` values (for example negative or positive days).\n\n:param value:\n:type value: datetime object\n:return:\t\"\"\"\n\t\treturn value - int(value.days / 7)\n\t\n\ndef dehydrate_hour(value):\t\"\"\"\n\tDehydrator for `hour` values (for example 1:00am, 1:05am"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value.microseconds\n\t# end def dehydrate\n\n\ndef _format_time(tm, time):\n\tif tm.type == \"hour\":\n\t\tseconds = time_hours[tm.hour][0]\n\telif tm.type == \"minute\":\n\t\tseconds = time_minutes[tm.minute][0]\n\telse:\n\t\tseconds = time_hours[tm.hour][1]\n\tseconds_format = \"{:02}:{:02}:{:02}:{:02}:{:02}:{:02}\".format(\n\t\thours=time.hour, minutes"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\n\tif value is not None:\t\t\n\t\ttime_unit = get_timeunit(value.timevalue)\n\t\t\n\t\tif time_unit == 'day': # one day\n\t\t\treturn time_value('days') + 'T'\n\t\telif time_unit == 'year': # one year\n\t\t\treturn time_value('years') + 'T'\n\nfrom __future__ import print_function\n\nimport datetime\nimport json\nimport os.path\nimport re\nimport time\nimport subprocess\nfrom abc import ABCMeta, abstractmethod\n\nfrom bson.objectid import ObjectId\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn 1 if value <= 0 else -1.0\n\n\tdef parse_time(timestr):\n\t\tif timestr in (\"minute\", \"hour\"):\n\t\t\treturn float(timestr)\n\t\telse:\n\t\t\treturn int(timestr)\n\n\tdef to_json(self, value, time_unit_field=\"hour\"):\n\t\t\"\"\"\n\t\tConvert time value string (e.g. 2h, 3w, 3M, etc) to JSON format.\n\t\t\"\"\"\n\t\treturn valtojson(value)\n\n\tdef to_json_datetime(self, time_value"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value * (3600 * 24 * 3600) + 86400 * 60 * 60\n\n\t@staticmethod\n\tdef set_time(value, name='Time', dt=0):\n\t\t\"\"\"\n\t\tSetter for `time`.\n\t\t\"\"\"\n\t\tvalue = int(value) * 3600 + value * 60 + (dt > time.time()) / 60\n\t\traise InvalidArguments(\"Time must be an integer\")\n\n\t@staticmethod\n\tdef parse_time(value):\t\"\"\"\n\t\tParses time `value` as an integer.\n\t\t\"\"\"\n\t\treturn int(value) * 24 * 3600 +"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tvalue = float(value)\n\t\tvalue = int(round(value, 0))\n\t\tvalue = timegm(value.timetuple())\n\t\treturn value\n\n\nclass TimeFormatter(object):\n\t\"\"\"\n\tFormat formatter for time\n\t\"\"\"\n\n\tdef __init__(self, fmt):\n\t\t\"\"\"\n\t\t:param fmt:\n\t\t:type fmt: datetime.datetime format string\n\t\t\"\"\"\n\t\tdatetime_format = '%Y-%m-%d %H:%M:%S'\n\t\tself._fmt = fmt.replace('%(m_%Y') + 'T', datetime_format)"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn int(value)\n\n\ndef format_timespec(t, format_spec):\n\t\"\"\"\n\tConvert time for timestamps to string.\n\n:param t:\n\t:type t: Time\n:param format_spec:\n\t:type format_spec:\n\t:return:\n\t:rtype: str\n\t\"\"\"\n\treturn \"{0}. {1} {2}\".format(date.strftime(format_spec),\n\t\tstr(t),\n\t\tdate.strftime(format_spec))\n\n\ndef decode_time_string(t):\n\t\"\"\"\n\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\n\treturn to_epoch(getattr(instance, name)).isoformat()\n\t\nclass TaskError(Exception):\n    pass\nclass SqlAlchemyError(Exception):\n    pass\nclass CeleryNotInstalled(TaskError):\n    pass\n\ndef setup_logging(config_file, db_url, redis_host):\n\tfrom logging import handlers\n\tfrom logging import config as logConfig\n\tlogConfig(config_file, level='%(log_level)s', format='%(asctime)s:%(module)s:%(funcName)s:%(message)s', datefmt='%Y-%m-%d %H"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\n\n\tvalues = value.split(\":\")\n\tt = int(values[0])\n\n\treturn t\n\t\n\n\ndef calc_plautern_times(time, values):\n\t\"\"\"\n\tCalculate plautern times (per second)\n\t\"\"\"\n\tseconds = int(values[11])\n\t\n\treturn time - seconds\n\n\ndef _date_to_hrs(date):\n\t\"\"\"\n\tConverts a date string (or other string) to a HR string.\n\tTries to convert to HRS using \"date\" instead (when it has one).\n\t"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn Point(value[0], value[1])\n\tfrom sympy import * # sympy requires sympy for Python 2\n\n# The function returns the first derivative.\ndef get_first_derivative(z):\n\t\"\"\"\n\tReturns the first derivative from the Z number of the supplied z object.\n\tThis is the function.\n\tExamples:\n\t>>> get_first_derivative(1)\n\t1\n\t>>> get_first_derivative(-1)\n\t(1, None)\n\t>>> get_first_derivative(1)\n\t1\n\t>>> get_first_"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.x / PRECISION\n\t\n\tdef hydrate_unit_conversion(value):\n\t\t\"\"\"\n\t\tHydrator for Unit. Converts from the standard to units.\n\t\t\"\"\"\n\t\treturn value / 100.0\n\t\n\thydrate_precision = -100.0\n\tunit_precision = -1000.0\n\t\n\tdef hydrate_to_string(value):\n\t\t\"\"\"\n\t\tDehydrator for values with units in the format 'f s'.\n\t\t\"\"\"\n\t\treturn value\n\t\t\n\tdef hydrate_to_point(value):\n\t\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\ndef create_point_object(name, values):\n\t\"\"\"\n\tCreate a Point object.\n\n :param name:\n\t:type name: String\n\t:param values:\n\t:type values:\n\t:return: The created Point object\"\"\"\n\n\t\n\t\t\n\ttry: \n\t\treturn create_obj(name, values)\n\texcept InvalidParameterException:\n\t\treturn create_obj(['', '0'], values)\"\"\" Tests for the DarkColor class\n\"\"\"\n\nimport argparse\nimport unittest\n\nimport numpy as np\nfrom sklearn.metrics import average_precision_score,"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.exterior\n\t\n\tdef get_point(value, key, max_distance=None, max_intensity=None, use_angle=True):\n\t\t\"\"\"\n\t\tReturns next point along current map. If key = coordinate, returns point\n\t\tclosest. If key = map coordinate, returns None. If max_distance is set,\n\t\treturns maximum distance from closest to key (default: None)\n\t\tand returns last key with distance > 0.\n\t\t\"\"\"\n\t\tif isinstance(value, Point):\n\t\t\ttry:\n\t\t\t\tpoint = value.equivalence\n\t\t\texcept AttributeError:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.real\n\n\t# --- Data to save the point data\n\tdef save(self, filename = None):\t# @UnusedVariable\n\t\t# TODO\n\t\tpass\n# Copyright (c) 2015. The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\nfrom tracing.core import traceable\nfrom tracing.core import tracing_category\nfrom tracing.core.tracing_category import TracingCategory\nfrom tracing.core.platform import tracing_category_filter\n\nfrom third"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\ndef dehydrate_unit(value):\n\t\"\"\"\n\tDehydrator for Unit data.\n\n:param value:\n:type value: Unit\n:return:\t\"\"\"\n\t\t\n\t\ndef dehydrate_coordinates(value):\n\t\"\"\"\n\tDehydrator for Coordinate data.\n\n:param value:\n:type value: LineString\n:return:\t\"\"\"\n\t\ndef dehydrate_polygon(value):\t\"\"\"\n\tDehydrator for Polygon data.\n\n:param value:\n:type value: Polygon\n:return:\t\"\"\""}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif type(value) == NumberType or type(value) == NumericType:\n\t\t\treturn value\n\t\telse:\n\t\t\treturn value * 10\n\treturn dehydrate_point\n\t\n\t\ndef plot_series_point(series, data):\t\"\"\"\n\tPlot series and return its data point.\n\n:param series:\n:type series:\tpd.Series\n:param data:\n:type data:\tpd.DataFrame\n:return:\t\"\"\"\n\tmin, max = np.min(data), np.max(data)\n\tlabel_str = '{0:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\n\ndef is_inout(vertex_name, point):\n\t\"\"\"\n\tReturn 1 for vertex, 0 for non-vertex.\n\t\"\"\"\n\tif len(vertex_name) < 5:\n\t\treturn 0\n\n\telif len(vertex_name) > 5 and vertex_name[-5:] == \"-1\":\n\t\tif len(vertex_name) >= 5:\n\t\t\treturn 1\n\t\telse:\n\t\t\treturn -1\n\telse:\n\t\treturn 0\n\n\n\n\ndef get_inout_points(name):\n\t\"\"\"\n\tGet list of in-oids"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\ttemp = value.x()\n\t\tn = (float(temp) + float(value.y())) / 2\n\t\t\n\t\tnew_point = Point(0, n)\n\t\tnew_point.set_position(value.x(), value.y())\n\t\treturn new_point\n\t\t\n\t\t\n\tdef move_point_by_point(self, old_point, new_point):\n\t\t\"\"\"\n\t\tMove point to new location, not touch current point.\n\t\t\n\t\t:param old_point:\n\t\t:param new_point:\n\t\t:return:\n\t\t\"\"\"\n\t\tnew_point"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\n\timport numpy as np\n\tvalue = value.tolist()\n\t\n\treturn []\n\t\t\n\t\t\n#----------------------------------------------------------------------\t\n# create_dataset\n#----------------------------------------------------------------------\t\ndef create_dataset(dataset_name, data, data_value):\n\t\"\"\"\n\tCreates a dataset with data as the input.\n\n\t:param dataset_name:\n\t:param data:\n\t:type dataset_name: string\n\t:param data_value:\n\t:type data_value: (np.float32, np.float64, np.float32, np.float64)\n\t:return:"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t return self._keys\n\n\tdef keysByID(self, key_id):\n\t\t\"\"\"\n\t\tReturn the records with the given key ID.\n\t\t\"\"\"\n\t\treturn self._database.keysByID(key_id)\n\n\tdef delete(self, record_id):\n\t\t\"\"\"\n\t\tDelete a record and all its modifications.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdel self._database[record_id]\n\t\texcept KeyError:\n\t\t\traise KeyError(\"No record named %s\" % record_id)\n\t\tself._database[record_id] = None\n\n\tdef __"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._keys\n\tdef keys_by_type(self):\t\"\"\"\n\tReturn the keys of the record as a list of tuples (type, start, end,\n\tcolor).\n\n:return: list of (prefix, key_name), (start-key_name, end-key_name),\n\tor (both, sorted(list of keys)))\n\n\"\"\"\n\tdef to_line(self, indent = 0):\n\t\t\"\"\"\n\tMake sure there are any padding with tabs.\n\n\t:return: a str containing the record contents.\n\t\"\"\"\n\t\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\n\t\n\t\n\treturn key_values(self.key_type)\n\n    def keys_list(self, key_list):\n\t\t\"\"\"\n\t\tReturn a list of the specified keys.\n\t\tkey_list: list of key_names\n\t\t\"\"\"\n\t\n\t\t\t\n\t\treturn key_list\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tReturn the values of the key_values.\n\t\t\"\"\"\n\t\n\t\treturn [v.value for v in self.keys()]\n\n    def __repr__(self):\n\t\t\"\"\"\n\t\tReturn: String representing this key_id.\n\t\t:return: String representation"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.__keys\n\n\tdef get(self, key, default=None):\t\t\"\"\"\n\tGet the value indicated by key. If key is not present,\n\tit will return the default value (if this is not defined)\n\tin case the record has no key.\n\t\"\"\"\n\t\tif key == None and self.has_key(key):return default\n\t\treturn self.get(key)\n\t\n\tdef getKeyByIndex(self, index):\n\t\t\"\"\"\n\tGet the key for a specific index.\n\tThe key must be positive and larger to be included in"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn list(self._items)\n\tdef values(self):\t\"\"\"\n\tReturn the values of the record.\n\n:return: list of values\t\"\"\"\n\t\treturn list(self._items)\n\tdef reset(self):\t\"\"\"\n\tReset the object by clearing all elements.\n\n:return: None\t\"\"\"\n\t\tsuper(Record, self).reset()\n\t\tself.data = [self, None]\n\ndef make_dict(field_name, values):\n\tkey_field = \"key_\" + field_name\n\tval_field = \"value"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.keys_of_record\n\n\tdef keys_of_record(self, key):\n\t\t\"\"\"\n\t\tReturn the keys of a given record.\n\n\t\t:param key: record key\n\t\t:return: list of keys\n\t\t\"\"\"\n\t\treturn self._keys_of_record(self.records[key])\n\n\t\n\tdef keys_of_key(self, key):\n\t\t\"\"\"\n\t\tReturn the keys of the given key.\n\n\t\t:param key: key to search\n\t\t:return: list of keys\n\t\t\"\"\"\n\t\treturn self.keys[key]\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._record.keys()\n\nif __name__ == \"__main__\":\n\timport sys\n\tfrom optparse import OptionParser\n\tparser = OptionParser()\n\tparser.add_option('--prefix', help='Prefix to process the record.')\n\tparser.add_option('--port', type='int', default=-1,\t# This may not be used in conjunction with an int() function or keyword arguments. If the user supplies it no host port, the user defaults to the free port configured by the OS's command line argument.\n\t\tdefault=-1,\n\t\thelp='Port used when running"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._keys.keys()\n\n\tdef __setitem__(self, key, value):\n\t\tself._values[key] = value\n\n\tdef __delitem__(self, key):\n\t\tdel self._values[key]\n\n\tdef __getslice__(self, idx, j):\n\t\treturn self._values[slice(idx, j)]\n\n\tdef __setslice__(self, idx, j, k):\n\t\tself._values[slice(idx, j)] = k\n\n\ndef copy(obj):\n\t\"\"\"\n\tCopy the given object or dictionary of objects\n\n\t:param obj"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn list(self.__record.keys())\n\tdef keysContainsRecord(self, key):\t# py2/3 compatibility\n\t\treturn key in self.__record\n\tdef isRecordInRecord(self, key):\t# py2/3 compatibility\n\t\treturn key in self.__record\n\tdef isRecordUnique(self, key):\t# py2/3 compatibility\n\t\treturn key in self.__record\n\tdef __eq__(self, other):\n\t\tif isinstance(other, basestring):\n\t\t\treturn self.string == other\n\t\tif isinstance(other, Record):\n\t\t\treturn"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn sorted(os.listdir(self.datadir))\n\t\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tGet one of the keys for this record type.\n\t\t\n\t\t:param key: key name the type we want to get\n\t\t:return: key value of record, or None if the key doesn't exist\n\t\t:rtype: str\n\t\t\"\"\"\n\t\treturn self.data[key]\n\t\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet an item stored in record, by name or uuid\n\t\t\"\"\"\n\t\tkey = '.'.join([str(x"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tport, protocol = cls._build_protocol(protocol_version)\n\t\thandlercls = dict(\n\t\t\t(name, handler_cls(ver=version, protocol=protocol))\n\t\t\tfor name, version in cls.protocol_handlers()\n\t\t)\n\t\thandlers = {}\n\t\tfor cls, protocols in cls.protocol_handlers().items():\n\t\t\thandler_cls = handlers.get(cls)\n\t\t\tif handler_cls is not None:\n\t\t\t\ttry:\n\t\t\t\t\thandler_cls(port, protocol)\n\t\t\t\texcept ValueError:\n\t\t\t\t\tcontinue\n\t\treturn handlers\n\n\t@classmethod\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tversion = cls.VERSION_SPEC\t# None meaning, a default version\n\thandlers = {}\t# dict for storing handlers\n\tif version == UNKNOWN_PROTOCOL_VERSION:\n\t\treturn handlers\n\telif version is None:\n\t\traise TypeError('cannot determine protocol version')\n\telse:\n\t\traise TypeError('version must be given')\n\t\n\thandler_list = {}\n\tfor handler in cls.SUPPORTED_BINARY_PROTOCOL_VERSIONS:\n\t\tif handler in handler_list:\n\t\t\traise TypeError('cannot specify more than one supported'\n\t\t\t       'binary protocol version')\n\t\thandler_list"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif portable_protocols == 1 or portable_protocols == 2:\n\t\tcls._thrift_port = (min(min(portable_protocols or []), 3),\n\t\t\tmax(max(portable_protocols or []), 5))\n\t\telse:\n\t\t\tcls._thrift_port = (min(min(portable_protocols), 3),\n\t\t\t\tmax(max(portable_protocols or []), 5))\n\n\tdef _register_specific_protocols(cls, protocols):\n\t\t\"\"\"Register Bolt protocol handlers for all relevant protocols.\n\t\tAny protocol in a tuple should be"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\ndef initialize_protocol_handlers(protocol_version):\n\t\"\"\"\n\tInitializes protocol handlers for the client. If protocol version is\n\tNone, the default implementation will be used.\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\tif protocol_version not in protocol_handlers:\n\t\traise TypeError(\"protocol version not supported: %s\" % str(protocol_version))\n\t\n\thandler_class = None\n\ttry:\n\t\thandler_class = protocol_handlers[protocol_version]\n\texcept KeyError:\n\t\traise TypeError"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif not protocol_version:\n\t\t\tprotocol_version = cls.PROTOCOL_VERSION\n\t\tif protocol_version not in cls.SUPPORTED_PROTOCOL_VERSIONS:\n\t\t\traise TypeError(\n\t\t\t\t\"Invalid bolt protocol version: %s\" %\n\t\t\t\t(str(protocol_version),)\n\t\t\t)\n\t\treturn cls.SUPPORTED_PROTOCOL_VERSIONS[protocol_version][0]\n\n\t@classmethod\n\tdef load(cls, config: dict) -> 'Bolt':\n\t\t\"\"\"Load a Bolt config from a dictionary.\"\"\"\n\t\tversion_tuple = tuple(cls.load_version_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\treturn dict([(1, (1, (2, 3))),\n\t\t\t\t\t\t\t(3, (2, 5))])\n\t\telse:\n\t\t\treturn {}\n\t\t\n\tdef __init__(self, handlers):\t\t\"\"\"\n\tConstruct protocol handlers with valid handlers\n\t\n\t:param dict handlers: dict of all valid handlers\n\t:return: None\n\t\"\"\"\n\t\ttry:\n\t\t\tsuper(BoltProtocolHandler,self).__init__(handlers)\n\t\texcept TypeError:\n\t\t\traise TypeError(\"protocol_version must be numeric\")\n\t\t\n\tdef init_protocol(self"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# This function seems to be happy with the old Bolt 5.5.0\n\t\tif protocol_version is None:\n\t\t\tif not hasattr(cls, 'protocol_version'):\n\t\t\t\tcls.protocol_version = {}\n\t\t\t\tcls.protocol_version['7.0'] = cls.default_protocol_version\n\t\t\t\tcls.protocol_version['8.2'] = cls.default_protocol_version\n\t\t\t\tcls.protocol_version['9.1'] = cls.default_protocol_version\n\t\t\t\tcls.protocol_version['1.0'] = cls.protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \n\tall_bases = set()\n\tsupported_versions = (\n\t\t\t((3, int)) # first version\n\t\t\t,)\n\t\t)\n\t\tfor version_tuple in list(protocols_version):\n\t\t\tversion_type, version_str = version_tuple[0]\n\t\t\tif version_version in supported_versions:\n\t\t\t\tall_bases.add(version_type)\n\t\t\t\treturn version_handler_base(version_type, version_str)\n\t\t\telif version_type >= 3:\n\t\t\t\tversion, ver = version_tuple\n\t\t\t\tif version not in all_bases"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version:\t\t\t\t# if supplied, convert it to an integer version\n\t\t\tprotocol_version = int(protocol_version)\n\t\telse:\t\t\t\t# otherwise it is None\n\t\t\tprotocol_version = None\n\t\n\t\tif protocol_version is None:\n\t\t\t# this will raise if the default protocol version is missing.\n\t\t\tprotocol_version = DEFAULT_PORTS\n\n\t\tif protocol_version not in VERSION_INFO:\n\t\t\t# determine the current Bolt protocol version or empty\n\t\t\ttry:\n\t\t\t\t# if the protocol version is specified in the mapping then use the\n\t\t\t\t# given"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t_version_map = cls.protocol_version_mapping\n\t\t_version_map.update(protocol_version or ())\n\t\tfor handler_class in cls.protocol_handlers:\n\t\t\tif protocol_version is not None and handler_class.__module__.startswith(\".\"):\n\t\t\t\thandler_class = getattr(handler_class, protocol_version)\n\t\treturn _version_map.get(protocol_version) or cls.handler_for_version(protocol_version)\n\t######################################################################################\n\tdef _validate_supported_version(version):\n\t\tif isinstance(version, string_types"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif timeout is None:\n\t\t\treturn\n\tif timeout < 0:\n\t\ttimeout = 0.0\n\tquery_log = \"MATCH (a:Transaction) RETURN count(a) AS transactions\"\n\n\tmeta = {\n\t\t'query': query_log\n\t}\n\tresponse = dbms.setTXMetaData(server, dbname, username, password, metadata, timeout)\n\n\tif 'procedures' in meta.keys():\n\t\tprocedus = meta['procedures']\n\t\tprint(\"Procedures:\", procedus)\n\n\ttransactions = []\n\tif"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\t\n\tif timeout is None:\n\t\ttimeout = DEFAULT_TRANSACTION_TIMEOUT\n\n\tif isinstance(metadata, dict):\n\t\tmetadata = {k:v for k, v in metadata.iteritems()}\n\telif isinstance(metadata, str):\n\t\tmetadata = {'name':metadata}\n\telif callable(metadata) or callable(metadata.set):\n\t\tmetadata = metadata()\n\n\tassert len(metadata) == 0, \"empty metadata\"\n\t\t\n\tlogging.debug(\"Starting %s\", name);\n\tdbms.set_meta(name, metadata)\"\"\"\nThe base class for A"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif timeout is not None:\n\t\t\treturn\n\t\treturn _set_timeout(timeout)\n\n# =======================\n# Database-related functions\n# =======================\n\ndef _set_timeout(timeout):\n\tfrom neo4j.commons.decorators import retry\n\tif not isinstance(timeout, float):\n\t\t_logger.error(\"transaction timeout must be a float\")\n\t\treturn None\n\n\ttimeout = max(timeout, DBMS_TIMEOUT)\n\tretries = retry(lambda x: x.run(\"SELECT Time(id) LIMIT %s\" % timeout))\n\tif"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\t\n\t# parse time stamp from configuration file\n\tfrom neo4j_cli import settings\n\tfrom neo4j import dbms\n\t\n\tif metadata is None:\n\t\tmetadata = {}\n\t\n\tif settings.DBMS_USE_DATETIME:\n\t\t# this is not a database that is capable of working with dbms (dbms.py is a wrapper around the method'setMetadata')\n\t\tmetadata = store.getMetaData()\n\n\tif 'query' in metadata['statement']:\n\t\tprint(\"WARNING:  MySQL statement was a database issue.\\n\", \"WARN\")\n\t\tif"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n    @classmethod    \n    def _check_transaction_timeout(cls, timeout):\n        if timeout and (timeout > 0):\n            raise Exception(\"Transaction timeout must be >= 0\")\n               \n    @classmethod\n    def _transaction_start(cls, timeout, data_dict_in_dict, data_dict_out_dict, context=None, verbose=False):\n        \"\"\"\n        :param timeout:\n        the transaction timeout. \n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif not metadata:\n\t\t\tprint(\"no metadata\")\n\t\t\treturn None\n\t\tassert(not isinstance(metadata, dict))\n\n\t\tif not key_required:\n\t\t\tprint(\"no key_required\")\n\t\t\treturn None\n\t\tif not type_required:\n\t\t\tprint(\"invalid type_required\")\n\t\t\treturn None\n\t\tif not isinstance(timeout, float):\n\t\t\tprint(\"invalid type_required\")\n\t\t\treturn None\n\t\tif not isinstance(operation, str):\n\t\t\tprint(\"invalid operation_type\")\n\t\t\treturn None\n\t\tif not type_valid_for_operation_types"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n__all__=['query','transaction']\n\ndef query(query=None, metadata=None, timeout=None):\n\t\"\"\"\n\tExecutes a given query (or the input) with the given metadata.\n\n\t\t@param query: query to execute\n\t\t@type query: str\n\t:param metadata: the metadata dictionary to be passed to the query\n\t:type metadata: dict\n\t:param timeout:\n\tthe transaction to be timed out, or the specified timeout (in seconds) from\n\t\tan optional argument ``None`` (as in :meth:`transaction.timeout`).\n\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t    \n    @app.wrapper\n    def innerLoop(dbms, *args, **kwargs):\n        \"\"\"\n        Start the transaction loop. It creates the transactions. It has two inputs: ``dbms``, the query and transactions to run (which can be used with the 'in' operator). First an input tuple should be the dbms object, the name of the input tuple. Second of, this tuple can be the cursor to run queries, or the name of the output tuple as a tuple, in which case the tuple will be iterated using the same name. This is required inside dbms.transaction.in which it will be"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif metadata is None:\n\t\t\treturn lambda *args,**kwargs: None\n\t\treturn UnitOfWork(metadata, timeout)\n\n''' Functions on :class:`~neo4j.query.query`.\n\n\tThe :meth:`~neo4j.query.session.run` method calls it for each named parameter in this :class:`~neo4j.query.query`.\n\n\tFor example, the following uses ``dbms.setTXMetadata``::\n\n\t\tdbms.update(query.Query(\"person.age\")(1))\n\t\tdbms.setTXMetadata('name')\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif not time.time() >= timeout:\n\t\t\treturn None\n\t\tlogger.info('Querying %s in %d seconds', metadata['dbms'], time.time() - time.time())\n\t\tcursor = dbms.getCursor()\n\t\tcursor.execute('MATCH (a:Person) RETURN count(a) AS persons')\n\t\tresult = cursor.next()\n\t\trecords = result.single()\n\t\tdbms.log(records)\n\n#: A callable that returns a generator for the output of queries. It will be called whenever the\n#: transaction is first executed, and its"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\ttry:\n\t\t\treturn self._keys[key]\n\t\texcept:\n\t\t\tself._emptyIndex(key)\n\n\t@index.setter\n\tdef index(self, item, index):\n\t\t\"\"\"\n\t\tSet the specific index on the item.\n\n\t\t:param item: index (int)\n\t\t\"\"\"\n\t\tself.index = index\n\n\tdef select(self, **kargs):\n\t\t\"\"\"\n\t\tSelect the item and select it.\n\n\t\tAny items from *args, **kwargs may be selected here. If *selection*\n\t\tis called this method is invoked to see a selection"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn id(key)\n\ndef main():\n\tk = Key()\n\tk.key = 'value'\n\tkv = KeyValue()\n\tkv.key = 'key'\n\ttry:\n\t\tprint (\"[index] %s %s[%s]\"%k.value)\n\t\tfor i in range(20):\n\t\t\tk.index(i)\n\texcept Exception as e:\n\t\tprint(\"[%s] %s[%s] %s[%d](%s)\"%\n\t\t\t(e.__class__.__name__, e.args, e.key, str"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tc = self.get_data_by_name(key)\n\t\treturn bisect.bisect(c, 1)\n\n\tdef refresh(self):\n\t\t\"\"\"\n\t\tRefresh the item list based on the given data.\n\n\t\tThis method must be called before the refresh() was called,\n\t\tin order to ensure that it is always called only if it is first\n\t\tcreated. In this event, the data that you're fetching may be invalidated\n\t\tfrom the system. Do not call refresh() manually to update the item list.\n\t\t\"\"\"\n\t\tc = self.get_data_by"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.items().index(key)\n\n\tdef index(self, keys):\n\t\t\"\"\"\n\t\tReturn the specified indexes with all the given keys. If the item doesn't exist,\n\t\traise IndexError.\n\t\t\"\"\"\n\t\tkeys = [s for s in keys if s in self.keys()]\n\t\tif len(keys) == 0:\n\t\t\traise IndexError(\"The requested item does not exist.\")\n\t\tresult = [index for index in self.all(keys)]\n\t\tif len(keys) == 0:\n\t\t\traise IndexError(\"There are no items to enumerate.\")\n\t\treturn"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._get_item(self._id, self.get_id_from_index(key))\n\t\n\tdef _get_item(self, item, iid):\n\t\t\"\"\"\n\t\tGet a particular item.\n\t\n\n\t\t:param item: the item\n\t\t:param iid: the item id\n\t\t\"\"\"\n\t\tdata = self._get_item_data(item, iid, data)\n\n\t\t# Only the data we need is indexed\n\t\tif not data:\n\t\t\treturn -1\n\n\t\t# Convert index from 0 to the number of items in the list\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif key == None: raise IndexError(\"No Index!\")\n\t\tif key not in self._current_key_set:\n\t\t\traise ValueError(\"Unable to get index for key: {} to {}\".format(key, self._current_key_set))\n\t\treturn self._current_key_set.index(key)\n\n\tdef add(self, key, value):\n\t\t\"\"\"\n\t\tAdd new values.\n\n\t\tArgs:\n\t\t  - key: a key\n\t\t  - value: an object of type IIndex, (object, int)\n\n\t\tRaises:\n\t\t  - ValueError if value does not exist.\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._cur.execute(u\"SELECT `id` FROM `key` WHERE `key`=?\", (key, ))[0][0]\n\t# \u8054\u7cfb\u7edf\u6307\u5b9a\u65b9\u6848\u5982\u4e0b\uff1akey[0] + key[1] * (self.parent.count() * self.parent.mapsize)\n\t# \u5247\u5b9a\u4e4b\u524d\u90a3\n\t# \u8005\u5b9a\u4e4b\u540e\u90a3\u4e2a\u7684\u80c1\u6bb5\u65f6\u5982\u4e0b\u8f7d\u662f:\n\t# \t("}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.items.index(key)\n\n\tdef append(self, item, value):\n\t\t\"\"\"\n\t\tAdd a new item into the list of items\n\n\t\t:param item: item to append\n\t\t:param value: value string\n\t\t\"\"\"\n\t\tself.items.append([item, value])\n\n\n\tdef extend(self, items):\n\t\t\"\"\"\n\t\tExtends the list of items\n\n\t\t:param items: items to extend\n\t\t\"\"\"\n\t\tself.items = items + self.items\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturn iterator as a list"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._index.value(key)\n\t\t#raise Exception(\"No index\")\n\tdef _convert(self, doc):\n\t\t\"\"\"Convert the item to a value.\"\"\"\n\t\traise NotImplementedError\n\n    def set(self, key, value, **kwargs):\n\t\t\"\"\"Set the key/value pair in the document.\n\t\tDefault is to set the key/value pair, with the same default as the\n\t\tkey. Use 'value' argument to change the value.\"\"\"\n\n\t\traise KeyError(\"Key '%s' not found.\" % key)\n\t\tfor k, v in kwargs.items():\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self._items[key].index\n\ndef insert(self, index, value):\n\t\"\"\"\n\tInsert an item into the _list.\n\n:param index: index of the item\n:param value: item to insert in the list\n:type value: item\n\n:rtype: int\t\"\"\"\n\t\tif len(self._list) == -1:\n\t\t\tself._list.append(self._start.clone())\n\t\telif index > self._index:\n\t\t\tself._list[index-1].append(value)\n\t\telse:\n\t\t\tself._list["}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif self._is_current_record():\n\t\t\treturn self.get_values(*keys)\n\t\n\tdef set_values(self, *keys):\n\t\t\"\"\"\n\t\tSet the values of the record to include only certain values. If no\n\t\tvalues are included, set all values to their values.\n\n\t\t:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n\t\t:return: None\n\t\t\"\"\"\n\t\tcheck_keys(keys, self._is_current_record())\n\t\tself.delete_all()\n\t\tfor key"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tkeys = set(keys)\n\t\ttry:\n\t\t\tkeys_len = len(keys)\n\t\texcept TypeError:\n\t\t\tkeys_len = 0\n\t\ttry:\n\t\t\tkeys_list = [int(o) if isinstance(o, int) else o\n\t\t\t\t\t\t\t for o in keys]\n\t\texcept TypeError:\n\t\t\tkeys_list = [o[:keys_len]]\n\t\tif keys_len > 0:\n\t\t\tif keys_list[0] <= len(self._fields):\t\n\t\t\t\treturn [self._fields[keys_list[0]]]\n\t\t\telse:\n\t\t\t\traise"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tvalue_ids = self.values_list(keys)\n\t\tvalue_counts = {}\n\t\tfor value_id in value_ids:\n\t\t\tvalue_counts[str(value_id)] = {}\n\t\t\tif value_id:\n\t\t\t\tvalue_counts[str(value_id)].update(value_id)\n\n\t\treturn value_counts\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"Return a record with the given key, or None\"\"\"\n\t\tif key is None:\n\t\t\treturn None\n\t\treturn self.values(key)[0]\n\n\tdef iteritems(self"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tres = []\n\t\tcurrentitem = self._obj_get(keys)\n\t\tfor item in currentitem:\n\t\t\tres.append(self._obj_get(item) or \"\")\n\t\treturn res\n\n\tdef keys(self):\n\t\treturn self.keys_arr\n\n\tdef __setitem__(self, key, item):\n\t\traise NotImplementedError\n\n\tdef __iter__(self):\n\t\treturn iter(self.keys())\n\n\tdef keys_arr(self):\n\t\t\"\"\"\n\t\treturn all available keys\n\t\t\"\"\"\n\t\tkeys = self.keys()\n\t\treturn self"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif self._keys_filter is None or (isinstance(keys, (list,tuple)) and keys):\n\t\t\treturn list(keys)\n\n\t\tif self.options.values:\n\t\t\treturn self.options.values\n\n\t\treturn self._values_filter(None, dict_from_iterable(keys))\n\n\n\tdef from_dict(self, d):\n\t\tif self._keys_filter is None or (isinstance(d, dict)):\n\t\t\treturn dict((k, d[k]) for k in d.iterkeys())\n\t\telse:\n\t\t\treturn self._values_filter.from_"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif key is None: keys = self.keys()\n\t\telse: keys = [key]\n\t\ttry:\n\t\t\treturn [int(v) for v in keys if v in self[key]]\n\t\texcept ValueError:\n\t\t\tpass\n\t\t#if not keys: # return all, if asked for all\n\t\t#else: keys = self.data.keys()\n\t\t#data = [self[key] for key in keys]\n\t\t#return data\n\n\n# class RawData:\n# \t\"\"\"\n# \tThis class contains all the RawRecord class instances,\n#"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tvalues = list()\n\t\tfor key in keys:\n\t\t\tif type(keys[key]) == type([]):\n\t\t\t\tvalues += list(self.get_record(key))\n\t\t\telif type(keys[key]) == dict:\n\t\t\t\tfor k, d in iteritems(keys[key]):\n\t\t\t\t\tvalues += d.get(k, [])\n\n\t\t\telif type(keys[key]) == list:\n\t\t\t\tvalues = list([\n\t\t\t\t\tself.get_record_dict(k) if name in d else value\n\t\t\t\t\tfor name, value in iteritems(keys[key])\n\t\t\t\t])\n\t\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif len(keys) < 0:\n\t\t\treturn None\n\t\treturn list(self._values(keys))\n\n# Returns a reference to an initialized instance of a query entry for the\n# requested entity, optionally filtering to include only certain values\n# by index or key\ndef __entity_to_value(cls):\n\ttry:\n\t\treturn cls._values(key=cls.index)\n\texcept BaseException, ex:\n\t\traise\n\n# Returns a reference to a reference to a query value for the\n# records associated with the requested entity\ndef __value_to_record("}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif keys:\n\t\t\tkeys = [self.makekey(key) for key in keys]\n\n\t\tvals = []\n\t\tif self.index:\n\t\t\tvalslist = self.index(keys)\n            if valslist is None:\n\t\t\t\t# do nothing\n\t\t\t\treturn valslist\n\t\t\tfor key in keys:\n\t\t\t\tvalslist.append(self.makevalue(self.get(key, None)))\n\t\treturn valslist\n\n\tdef makevalue(self, value, datatype='string'):\n\t\t\"\"\"Create a JSON-encoded textvalue. If the type is'string' this is\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tkeymap = self.keymap\n\t\tindices = self.indices or ''\n\t\tif isinstance(keys, basestring):\n\t\t\ttry:\n\t\t\t\tindex = self.to_intseq(keys)\n\t\t\t\tif keymap and keys:\n\t\t\t\t\tkeymap = self.keymap.getMapping(keymap)\n\t\t\t\t\tkeys = self.to_valueseq(keys)\n\t\t\t\treturn self.values(indices, index)\n\t\t\texcept LookupError:\n\t\t\t\t# fallback for values in the keymap\n\t\t\t\tself.values(indices, None)\n\t\t\t\treturn None\n\t\telif iterable(keys)!= keys:"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif not keys:\n\t\t\treturn None\n\t\tif not keys:\n\t\t\traise IndexError('\"keys\" is required')\n\t\tresult = {}\n\t\tfor index, field in enumerate(keys):\n\t\t\tresult[field] = self._fields[field].read(self)\n\t\treturn result\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"Retrieve field and index from `key` record and return it\"\"\"\n\t\tif isinstance(key, slice):\n\t\t\treturn self.get(key.indices(self.indices.keys()))[0]\n\t\telse:\n\t\t\treturn self._fields["}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif keys is None:\n\t\t\tkeys = self.keys()\n\t\tif isinstance(keys, dict):\n\t\t\tkeys = keys.items()\n\t\telse:\n\t\t\tkeys = tuple(keys)\n\t\treturn {key:val for key, val in get_data(keys, self)}\n\tdef keys(self):\n\t\t\"\"\"return the keys of the items\"\"\"\n\t\treturn tuple(self.keys())\n\tdef add(self, *args, **kwargs):\n\t\t\"\"\"add a new *value* to this record, if the field will be added to the\n\t\tdatabase\"\"\"\n\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tassert len(keys) > 0, 'Key parameters not specified'\n\t\tassert isinstance(keys[0], str),  'Keys must be strings'\n\t\tassert not isinstance(keys[1], (list, tuple)),  'Keys must be tuples'\n\t\treturn dict([(k, v) if isinstance(v, list) else v for v in keys])\n\ndef get_index_or_key(idx):\n\t\"\"\"\n\tReturn either the current index to record the item according to the key\n\tgiven in :attr:`indexes`. If no current index is provided, it will\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif len(keys) == 0:\n\t\t\treturn {}\n\t\treturn dict(zip((\"rec[]\")*len(keys), (self[key][0] for key in keys)))\n\n\tdef __setitem__(self, key, val):\n\t\tif len(self) > 0:\n\t\t\tkey.val = val\n\t\t\tif key.index >= 0:\n\t\t\t\tkey.index += len(self.data(key.index))\n\t\t\t\tkey.val.val = val\n\t\t\telse:\n\t\t\t\tkey.val.val = self.data(key.index)\n\t\telse:\n\t\t\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif isinstance(keys, (int, long)):\n\t\t\tkeys = [None] + keys\n\n\t\treturn self.get(keys)\n\n\tdef append(self, key, value):\n\t\t\"\"\"\n\t\tAppends the given key/value pair as the value for the item identified.\n\n\t\t:param key: key of the item to add\n\t\t:param value: value to add to item\n\t\t\"\"\"\n\t\tself.put(key, value)\n\n\tdef update(self, iterable):\n\t\t\"\"\"\n\t\tUpdates this record.\n\n\t\t:param iterable: iterable to update (item->value"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\n\t\n\tfor id in keys:\n\t\titem = self._fields.get(id, {})\n\t\tif not item:\n\t\t\traise KeyError(\"No record with %s id found.\" % (self._fields,))\n\t\tif len(item) > 1:\t\t\traise IndexError(\"Multiple values for a record\")\n\t\t\n\t\t# If an index specified, add this record and only keep that\n\t\tif item[0] is not None and id not in self._fields:\n\t\t\titem[0] = self._fields[id]\t\t\t\t# Create this record\n\t\n\treturn {}\n\ndef _makedict"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif not isinstance(keys, (list, tuple, set, set)):\n\t\t\tkeys = (keys,)\n\t\tif \"index\" in keys:\n\t\t\tk = [item for item in self.collection if item[0] == keys[0]]\n\t\t\tkeys = list(set(k))\n\t\tif \"key\" in keys:\n\t\t\tk = [item for item in self.collection if item[0] == keys[0]]\n\t\t\tkeys = list(set(k))\n\t\treturn dict(keys)\t\n\n    def to_item(self):\n        \"\"\"\n\tUse this method"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tkeys = tuple(keys or ())\n\t\tif not keys:\n\t\t\tkeys = self.fields\n\t\tif not isinstance(keys, tuple):\n\t\t\tkeys = tuple(keys or (len(self) + 1,))\n\n\t\treturn dict([(key, self[key]) for key in keys])\n\n\nclass Mapper(object):\n\t\"\"\"\n\tA key-based mapping implementation for a collection of attributes\n\tfor multi-inheritance and indexing.\n\n\tFor example::\n\n\t\tmap_attrs = Mapper.attrs\n\t\t# This will iterate over 'person' records, with key = 'name"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif self.index or self.keys:\n\t\t\treturn self._data\n\t\tkey = Key(self.index or self.keys)\n\t\tif any(key not in keys for x in self._fields):\n\t\t\traise IndexError(_('%(key_id)s not in record') %\n\t\t\t\t dict(key_id={'key_id': self.index,\n\t\t\t\t\t\t\t'key_id_value': key._id}))\n\t\treturn dict(zip(self._fields, keys))\n\n\tdef __repr__(self):\n\t\treturn '%r:%r' % (self.__class__.__name__, getattr"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tif not keys:\n\t\t\tvalues = {}\n\t\t\tfor key in self._keys:\n\t\t\t\ttry:\n\t\t\t\t\tvalues[key] = int(self._records[key].values)\n\t\t\t\texcept (ValueError, KeyError):\n\t\t\t\t\traise IndexError(\"Record %s is missing a %s key\" % (self._key, key))\n\n\t\t\tif not values:\n\t\t\t\tvalues = {}\n\t\t\t\tfor key in keys:\n\t\t\t\t\tvalues[key] = 0\n\n\t\t\t\t# default to nothing\n\t\t\t\t\tvalues = self._keys # this is the default behaviour (no index or key)\n\n\t\t\treturn values\n\t\t"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\ta = self.pop_u16_u16s()\n\t\tsize = a[0]\n\n\t\tif size >= 0xE:\n\t\t\ta = a[2:]\n\t\t\ta = a[:-2]\n\t\t\tsize = calcsize(''.join(a))\n\t\telse:\n\t\t\tprint >> sys.stderr, 'Error: No data left in pop_u16!'\n\t\t\treturn 0\n\t\treturn size\n\n\tdef read_u32(self):\n\t\t\"\"\"\n\tRead a big-endian 32-bit unsigned integer.\n\t\"\"\"\n\t\treturn self.read_int(2"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tdata = self.lcm_data[self.lcm_data.rfind(\"\\0\")];\n\t\treturn self.lcm_data[:(data & 0xF00) - 1]\t\t\t\t\n\t\t\n\nclass lcm_data(object):\n\t\"\"\"\n\tRead LCM Datum file, and write to a data file.\n\t@param filename: path to the file to write data to.\n\t\"\"\"\n\tdef __init__(self, filename):\n\t\tself.log = logging.getLogger(\"lcm_data\");\n\t\tself.lcm_data = codecs.open("}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self._pop_u16(self._pop_u8(1), self.max())\n\t\t\nclass _u16(object):\n\t\"\"\"\n\tU-16-bit scalar value generator protocol.\n\t\"\"\"\n\t__slots__ = ('_data', '__min', '__max','min_value', '_value')\n\t\n\t# A 2-tuple with length 2 that stores '1' as the key and '0', and\n\t# '1' for the maximum value, and '-1' for the minimum value.\n\t_key_size = 2, 2\n\t\n\tdef"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\ts = self.__encode_value(self.__pop_binary(self.__size)[2:])\n\t\treturn util.pack('<H', (util.unpack('<L', s)[0]) & 0xFFFFFFFFFFFF)\n\n\tdef pop_u32(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\t\ts = self.__encode_value(self.__pop_binary(self.__size)[2:])\n\t\treturn util.pack('<H', (util.unpack('<L', s)[0]) & 0"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tb = self.input.read(1)\n\t\treturn struct.unpack('B', b)[0]\n\n\nclass Input(bytes):\n\t\"\"\"\n\tReads one or 2 encoded bytes, using the read1 byte, read2 bytes\n\tand the output byte to the output pipe. The read1 value specifies the\n\toffset of the byte, and the read2 value specifies the value that the byte\n\twill be written. The output byte is read from the output pipe immediately.\n\t\"\"\"\n\n\tr0 = re.compile(b'\\x00\\x00\\x00', re"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self.u16\n\ndef read_hex_bytes(f, length):\n\t\"\"\"\n\tRead a bytearray of given length bytes which is a signed integer.\n\tWhen a bytes exceeds the largest length the next byte may be\n\tused as an unsigned integer, and that is the big endian byte.\n\n\t@type f: list of int\n\t@param f: read the given bytes\n\t@type length: L{int}\n\n\t@param length: the number of bytes remaining to read\n\t\"\"\"\n\tif len(f) < length:\n\t\traise ValueError(\"length"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\t(result, _) = self.unpack(\"!B\",self.read(4))\n\t\tassert result == 254\n\t\treturn result\n\n\tdef get_u16(self):\t\"\"\"\n\tGet the first 4 bytes of data.  If the first byte is not present,\n\tNone and the length is 1.\n\t\"\"\"\n\t\t(result, _) = self.unpack(\"!B\",self.read(4))\n\t\tassert result == 254\n\t\treturn result\n\n\tdef get_u32(self):\t\"\"\"\n\tGet the first 32 bytes of data.  If the first byte is"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self[0:2]\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last 8 bytes of data, returning them as a big-endian\n32-bit unsigned integer (32-bit).\t\"\"\"\n\t\treturn self[2:3:8]\n\ndef pop_u64(self):\t\"\"\"\n\tRemove the last 8 bytes of data, returning them as a big-endian\n64-bit signed integer (64-bit).\t\"\"\"\n\t\treturn self[2:6:8]\n\ndef get_address(address):\t\"\"\"\n\tReturn"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tdata = self.read(4)\n\t\twhile len(data) == 0:\n\t\t\tself.close(4)\n\t\t\treturn data\n\t\tdata_len = struct.unpack('>Q', data, 0x2f)\n\t\tself.close(4)\n\t\treturn data[data_len-1:-2]\n\n\tdef pop_u16_big_endian(self):\t\"\"\"\n\tRemoves the last two bytes of data, returning them as big-endian\n16-bit signed integer.\t\"\"\"\n\t\tdata = self.read(4)\n\t\twhile len(data"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tif (len(self._data) > 0):\n\t\t\traise Exception(\"pop_u16 : cannot pop after pop_u16\")\n\t\t\n\t\tif len(self._data) < 1:\n\t\t\traise Exception(\"pop_u16 : empty data\")\n\t\t\n\t\tself._data.popleft()\n\t\treturn self._data.popleft()\n\t\n\tdef get_i16_from_address(self, address):\n\t\tif (len(self._data) == 0):\n\t\t\traise Exception(\"get_i16_from_address : cannot get data\")\n\t\t\n\t\tif (self._data.index"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\t#logger.debug('disabling discrats from query')\n\t\tlogger.debug( str(dehydration_hooks) )\n\t\tlogger.debug( str(handlers) )\n\n\t\tmsg_payload = self.build_receive_msg_buffer( n, qid, dehydration_hooks )\n\n\t\ttry:\n\t\t\t#logger.debug('Discarding %s' %(msg_payload))\n\t\t\treturn self.send(msg_payload, handlers=handlers)   # return DiscardMessage\n\t\texcept:\n\t\t\tlogger.error(\"Encountered an error in discard"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\ttry:\n\t\t\tself.dehydration_hook_generator.apply(n, qid, dehydration_hooks, handlers)\n\t\texcept TypeError as e:\n\t\t\tprint(\"TypeError\", e)\n\n\tdef dehydrate(self, dehydration_func):\n\t\tself.handlers.append(dehydration_func)\n\n\tdef _run(self, message): # pragma no cover\n\t\t# run a given message if an error is encountered\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tself.stream.drain()\n\t\t\texcept BlockingIOError:"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tif handlers is None:\n\t\t\thandlers = self.handlers\n\t\tfor handler in handlers:\n\t\t\tmsg = self.dispatch_message('@{}'.format(ht) +'discard', \n\t\t\t\t(None, ct.pointer(ht)), qid=qid, dehydration_hooks=dehydration_hooks, handler=handler)\n\t\t\tif self.send_response:\n\t\t\t\tself.set_status(ht, True, self.status_code(ht))\n\t\t\tif self.recv_response:\n\t\t\t\tself.set_status(ht, False, self.status"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tself.disco = (d.discard if dehydration_hooks is None \\\n\t\t\t\telse dehydration_hooks)\n\t\tself.handlers = (d.close if dehydration_hooks is None \\\n\t\t\telse dehydration_hooks)\n\t\tself.dehydration_hooks = dehydration_hooks\n\n\tdef __len__(self):\n\t\treturn len(self.handlers)\n\n\tdef __getitem__(self, index):\n\t\t# this is a Python 2.X compatibility wrapper around iter:\n\t\tif hasattr(self.handlers["}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\nclass _Reader (object):\n\t\"\"\"\n\tRead and decode a single message, or a file or file handle that\n\tmay read a string or file-like object. \n\n\tThis is the base class for all parsers and parsers that support\n\tone particular type of message being read.\n\t\"\"\"\n\tdef __init__ (self, fileobj, type_id=None):\n\t\tpass\n\t\n\t#===============================================================================\n\t# Constructor\n\t#===============================================================================\n\t\n\t\n\tdef __iter__ (self):\n\t\t\"\"\"\n\t\tRead messages as a generator. Returns a generator yielding\n\t\tan object of"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\n\tlogger, loggerName = cj.get_logger(self._loggerName)\t\t\n\tlogging.debug(\"Using query ID: {0}\".format(qid))\n\tif dehydration_hooks:\n\t\tdehydration_hooks(self, n, qid, dehydration_hook)\n\tif handlers:\n\t\thandlers(self, n, qid, handlers_hook)\t\n\n\tloggerInfo(\"Sending response : {0}\".format(response))\t\n\tlogger.debug(\"Query ID: {0}\".format(qid))\t\t\n\n\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tself._abort_queue.append((n, qid, dehydration_hooks, handlers))\n\t\t\n\tdef abort(self, reason=None, response_object=None):\n\t\t\"\"\"\n\t\tAvoid a response.\n\n\t\t:param reason: a message to be ignored\n\t\t:param response_object: response object passed into response\n\t\t:type reason: L{ResponseObject}\n\t\t\"\"\"\n\t\tself._abort_queue.append((reason, response_object))\n\t\n\tdef close(self, reason=None):\n\t\t\"\"\"\n\t\tPerform a close operation for all records in"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tif n is not None:\n\t\t\ttry:\n\t\t\t\tself._output_queue.put((NDISCARD, (n, qid, dehydration_hooks)))\n\t\t\texcept:\n\t\t\t\tself._output_queue.put((NDISCARD, (qid, None, handlers)))\n\t\treturn n\n\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClears the queue.\n\t\t\"\"\"\n\t\tif not self._finalized:\n\t\t\treturn\n\t\tself._output_queue.put((NDISCARD, None, None))\n\n\tdef _receive_messages("}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t    if not handlers:\n    \t    handlers = [s.detach() for s in self._handlers]\n    \t    for h in handlers:\n\t    if hasattr(h,'read'):\n\t\th.read(self, -1)\n    \t    return\n\t    if not hasattr(qid,'read'):\n    \t    qid.read(self, -1)\n    \t    return\n\t    if not hasattr(dehydration_hooks,'read'):\n    \t    dehydration_hooks.read(self, -1)\n\t\treturn\n\t    if hydration_hooks:\n    \t    try:\n\t       "}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n\t\n\t\n\nimport sys\nimport os\nimport numpy as np\n\nimport pycurl\n\n\nclass Producer:\n\tdef __init__(self, input=None, output=None, file=None, buffer=False,\n\t\tchunksize=1 << 20, chunk_size=1, randomize=True, verbose=False, pbuffer=False,\n\t\tcallback=None, use_buffer=False):\n\t\t\"\"\"\n\t\t:param input:\n\t\t\tName of the raw message. If not provided the name of the\n\t\t\tapplication will be used\n\t\t:param output:\n\t\t\tName of"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tdef wrap_method(meth):\n\t\t\tmethod = meth.__func__\n\t\t\tif not isinstance(method, types.FunctionType):\n\t\t\t\traise TypeError(\"method %r is not a function; use a method instead\" % meth)\n\n\t\t\t@dec.safelambda\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t\tresponse_func = getattr(self, method)(*args, **kwargs)\n\t\t\t\tif callable(response_func):\n\t\t\t\t\treturn response_func(*args, **kwargs)\n\t\t\t\treturn response_func\n\t\t\n\t\tdehydrate_methods = {}\n\t\tdef hydrate_handler"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tfrom octoprint.plugin import PluginManager\n\t\tfrom octoprint.util import getConfig\n\t\tclass HandlerHandler(object):\n\t\t\tdef __init__(self, request):\n\t\t\t\tself.httphooks = dict()\n\t\t\t\tself.isJsonRequest = True\n\t\t\t\tself.request = request\n\t\t\t\n\t\t\tdef bind(self, handler):\n\t\t\t\tself.httphooks = dict(HandlerHandler.httphooks)\n\t\t# get the handler\n\t\thandler = getConfig().getOption('httphooks-uri', '/uri')\n\t\ttry:\n\t\t\tself.httphooks[handler.uri()] ="}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tpass\n\n\tdef _do(self, mode, bookmarks=None, metadata=None, timeout=None,\n\t\traise_exception=True, dehydration_hooks=None,\n\t\timp_user=None, dehydration_hooks=None, handlers=None):\n\t\t\"\"\"Returns a response containing the message body for an application\n\t\tmode.\n\t\t\n\t\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t\t:param metadata: custom metadata dictionary to attach to"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif mode == None:\n\t\t\tmode = 'READ'\n\t\tparams = {'mode': mode, 'bookmarks': bookmarks,'metadata': metadata, 'timeout': timeout, 'imp_user': imp_user, 'dehydration_hooks': dehydration_hooks}\n\t\tif mode == 'READ':\n\t\t\tresponse = self._read(params)\n\t\t\treturn response\n\t\tif mode == 'WRITE':\n\t\t\tresponse = self._write(params)\n\t\t\treturn response\n\t\tif mode == 'READ':\n\t\t\tresponse = self._read(params)\n\t\t\treturn response\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\treturn self._send(begin(mode=mode, bookmarks=bookmarks, metadata=metadata,\n\t\t\t\t\ttimeout=timeout, imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n                    handlers=handlers, hydration_hooks=head_hook+tail_hook)\n\n\tdef begin_transaction(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\t\t  db=None, imp_user=None, dehydration_hooks=None, handlers=[], hydration_hooks={}):\n\t\t\"\"\"\n\t\tStarts a new"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\t_logger.debug(\"In begin\")\n\t\tif mode == \"READ\":\n\t\t\treturn _read(bookmarks, metadata, timeout, imp_user, dehydration_hooks, \\\n\t\t\t                              modules, hooks=hooks)\n\t\telif mode == \"WRITE\":\n\t\t\treturn _write(bookmarks, metadata, timeout, imp_user, dehydration_hooks, \\\n\t\t\t                              modules, hooks=hooks)\n\t\telse:\n\t\t\tif mode == \"READ\":\n\t\t\t\treturn _read(bookmarks, metadata, timeout, imp_user, dehydration_hooks, \\\n\t\t\t\t                              modules"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tlog(\"begin: current mode: %r\" % (mode))\n\t\tlog(\"in: bookmarks: %r\" % (bookmarks))\n\t\tlog(\"in: metadata: %r\" % (metadata))\n\t\tlog(\"in: timeout: %r\" % timeout)\n\t\tlog(\"in: db: %r\" % (db))\n\t\tlog(\"in: modules: %r\" % (imp_user))\n\t\tlog(\"in: dehydration_hooks: %r\" % (hydration_hooks))\n\t\ttry:\n\t\t\toutput = []\n\t\t\ttry:"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n\n\n# Copyright 2014 Metaswitch Networks\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tdata = {}\n\t\tdata['user'] = imp_user\n\t\tdata['timeout'] = timeout\n\t\tdata['bookmarks'] = bookmarks\n\t\tcontent = self._build_data(data)\n\t\t# Insert the request to the appropriate transaction\n\t\tbody = {\n\t\t\t'userId': self.auth_token,\n\t\t\t'data': { 'jsonrpc': \"2.0\",'method': \"POST\", 'id': '1', 'params': [self.payload_name,data['data']], 'id': '1', }],\n\t\t\t'jsonrpc': \"2.0\",'method"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\t#return a new object of the appropriate method\n\t\treturn self.client.begin(mode, bookmarks, metadata, timeout, imp_user, dehydration_hooks,\n\t                        hydrate_hooks, handlers)\n\n\tdef rollback(self, mode=None, bookmarks=None, metadata=None, timeout=None, defense_reason=None,\n\t                bookmarks=[], defense_timeout=None, timeout=None, defense_metadata=None,\n\t                handlers=[], **kwargs):\n\t\t\"\"\"\n\tRenames a transaction. Restores all the transaction up to the\n\tcurrent"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn n / n\n\ndef _get_number_digits_from_string(base, digits):\n\tparts = base.rsplit('.', num=num_digit, sep='')\n\t\n\tbase_digits = []\n\tfor i in range(base.count('0') + 1):\n\t\tbase_digits.append(digits[i])\n\treturn base_digits\n\t\n\ndef _get_number_digits_from_int(digits_str):\n\tdigits_str = digits_str.strip()\n\ttry:\n\t\tdigits = int(digits_str)\n\texcept ValueError"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t     #print n\n\t     return int(n,0)\n\t     #if n < 1:\n\t\t#\treturn int(1,0)\n\t     else:\n\t     #\treturn int(math.floor(math.sqrt(int(math.pow(n,2)))))\n\ndef is_event_list(arg):\n\ttry:\n\t\treturn hasattr(arg, \"__iter__\")\n\texcept AttributeError:\n\t\treturn hasattr(arg, \"__getitem__\")\n\ndef is_dict(arg):\n\ttry:\n\t\treturn hasattr(arg, \"__getitem__\")\n\texcept"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ta,b = n\n\t\ti,x = 0.25 * (a+(b-a)/2),a\n\t\treturn b - a + x\n\ndef round_round_up(n):\t\"\"\"\n\t>>> round_round_up(4)\n    4\n\n:param n:\n:return:\t\"\"\"\n\t\treturn n\n\ndef number_to_round(n):\t\"\"\"\n\t>>> number_to_round(7)\n\t0.\n\t>>> number_to_round(15)\n\t15\n\t>>> number_to_round(33"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\t\"\"\"\n\t\tn = _float(n).as_float_array()\n\t\tr = (n-3)*_float(n) / n\n\treturn r\t\n\n\ndef _round_half_to_even_as_single_half_half(n):\t\"\"\"\n\t>>> round_half_to_even_as_single_half_half(3)\n\t3\n    >>> round_half_to_even_as_single_half_half(3.2)\n    3\n    >>> round_half_to_even_as_single_half_half("}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t return int(round(n, 0))\n\ndef is_even_small(n):\n\t\"\"\"\n\t>>> is_even_small(0.15)\n\tFalse\n\t>>> is_even_small(0.15)\n\tTrue\n\t>>> is_even_small(0)\n\tFalse\n\n:param n:\n:return:\t\"\"\"\n\n\t return n > 0 and n <= 1\n\n\nif __name__ == \"__main__\":\n\tdef print_example():\n\t\tprint(\"Suggested input:\")\n\t\tfor x in range(10):\n\t\t\tprint("}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\nclass WrappingFrameIncreases(object): pass\n\n\nclass PanishBelow(object):\n\n\t\"\"\"\n\tWrappingFrameIncreases(frame, max_size) -> 0\n\n\tWrappingFrameIncreases is used to increment the size in\n\t*frames* such that there are no more than max_size - 1,\n\ti.e., the frame may be outside the frame.\n\n:param frame: The frame that is a child of\n:param max_size: The maximum number of characters\n:param base_size: The maximum size that is allowed"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn n*n\n\ndef round_to_even(n):\t\"\"\"\n\t>>> round_to_even(3)\n    3\n\t>>> round_to_even(3.2)\n    3\n\t>>> round_to_even(3.5)\n    4\n\t>>> round_to_even(4)\n\t4\n\t>>> round_to_even(4.2)\n    4\n    >>> round_to_even(4.5)\n    5\n    >>> round_to_even(4.7)\n    5\n\n:return:\t\"\"\"\n\t\treturn"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ttry:\n\t\t\treturn (float(m)/(2*n))\n\t\texcept:\n\t\t\treturn 0.0\n\treturn round_half_to_even\n\ndef find_half_to_even_of_precision(n, precision):  # TODO\n\t\"\"\"\n\t>>> find_half_to_even_of_precision(3.2)\n\t0.5\n\t>>> find_half_to_even_of_precision(3.5)\n\t-0.7\n\t>>> find_half_to_even_of_precision(4.2)"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tm, n = divmod(n, n)\t\t\n\te = range(m + 4) * n + 1\n\treturn [e - 1]\n\ndef expand_quad(num, quad):\t\"\"\"\n\tIntended to take this many quads,\n\tand expand the number,\n\tand the quad\n\t\"\"\"\n\t\tif num < quad and num > 1:\n\t\t\treturn 1\n\telse:\n\t\tres = []\n\t\tfor c in quad:\n\t\t\tres += [expand_quad(round_half_to_even(x), c) for x in"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\tn = round_half_to_even(n)\n\t\treturn n\n\ndef randrange(start, stop):\n    \"\"\"\tReturns a random random number between start and stop\"\"\"\n    return imath.RandomNumber(start, stop)-imath.PtAt(0, stop)\n\ndef randmod(k, m):\n    \"\"\"\tReturns a random number between 0 <= m, with the given number\n\tas the starting point with k\n\treturns:\n\treturn a random number between 0 <= m, with the given k\n\t\"\"\"\n\treturn imath.Mod(1"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn Point(Point(float(fields[0]), float(fields[1])), srid_map[name])\n\n\tdef wkt(self, srid_map):\t\"\"\"\n\tReturns the WKT of the point geometry defined for a raster.\n\t\"\"\"\n\t\treturn \"%s,%s\" % (srid_map[\"wkt\"], srid_map[\"wkt2\"])\n\n\tdef wkb(self, srid_map):\n\t\t\"\"\"\n\t\tReturns the WKB of the point geometry defined for a raster.\n\t\t\"\"\"\n\t\treturn \"%s,%s,%s\" % (srid_map[\"wkb\"], srid"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tfields_type = type(fields)\n\t\tfor field in fields.split():\n\t\t\tif field == 'geometry' or field == 'width':\n\t\t\t\tx_val = type(name, (QgsPoint,))\n\t\t\t\ty_val = type(name, (QgsPoint,))\n\t\t\t\tif x_val.precision() > int(x_val.precision()):\n\t\t\t\t\traise TypeError('%s.%s must be an integer in the SRID (%s) field' % (name, x_val.dataProvider(), srid_map['srid']))\n\t\t\t\telif y_val.precision() > int(y_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tpass\nclass Point(object):\n\t\"\"\"\n\tThis class represents a point within a geographic object.\n\n\tThis object is meant to wrap Point's, using the\n\tPoint's methods:\n\n\t.. function:: Point.distance(point1,point2)\n\n\t\tCalculate distance from a point to the given point\n\t\t.\n\t\t:param point1: The first Point.\n\t\t:param point2: The second point.\n\t\t:param distance: Distance of the given point as a single floating point.\n\n\t\"\"\"\n\t\n\tdef __init__(self, x, y, srid):\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tname_parts = name.split(LOOKUP_SEP)\n\t\treturn Point\n\n\tdef get_transform(self, obj, value):\n\t\tif obj is None:\n\t\t\treturn None\n\n\t\t# Spatial Reference System\n\t\ttry:\n\t\t\tst = field_cast(value, self.default_srid)\n\t\texcept ValueError:\n\t\t\treturn ST_None\n\t\telse:\n\t\t\t# Transforming a point with the given SRID\n\t\t\treturn ST_Point(st.srid, value)\n\n\tdef get_geom_placeholder(self, geom):\n\t\treturn geom\n\n\tdef __geo"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif fields:\n\t\t\tsrid_map[z] = Point()\t\t# Point\n\t\t\tfor f, srid in fields:\n\t\t\t\ttry:\n\t\t\t\t\tc = QgsRasterLayer(name, srid)\n\t\t\t\t\tif c.isValid():\n\t\t\t\t\t\treturn point_type(c.name, fields, srid_map)\n\t\t\t\texcept:\n\t\t\t\t\tfor field in fields:\n\t\t\t\t\t\tz = 'POINT(%s, %s)' % (field.num, field.name)\n\t\t\t\t\t\traise e.GeometryError(z)\n\n\tdef point_type_string(name, fields, srid_map):"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn Point(SpatialReference(srid_map[srid_map.get(name, None)]))\n\nfrom django.db.models import signals\nfrom django.contrib.gis.db import backend as geobackend\nfrom django.utils.geometry import GEOSGeometry\nfrom django.utils.six.moves.urllib.request import Request as django_request_factory, urlopen\nfrom django.utils.six.moves.urllib.error import URLError\n\n# These need to be imported\nfrom geonode.qgis_server.proxy.urlparams import add_params\nfrom geonode.qgis_server.proxy."}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn {'type':'Point', 'name':name,'srid':srid_map[name]}\n\nclass GeographicType(GeographicBaseType):\n\tdata_type = Point\n\t\n\ttype = GeometryType.Geographic\n# coding: utf-8\n\nfrom datetime import datetime, date, timedelta\nimport datetime\n\n# This app should define a view which displays the current date and time of execution\n\nclass RatingView(object):\n    def GET(self):\n        # Set up the time range\n        now = datetime.now()\n        now_date = date.today()\n        time_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tgeom_type = self.geom_type[name]\n\t\tsrid_map[geom_type] = Point([None]) # Default geometry type\n\tdef as_ogr(self, srid_list):\t\"\"\"\n\tAspatial data - returns a list of FeatureClass objects.\n\t\"\"\"\n\t\tif self.geom_type == 'Polygon' or self.geom_type == 'MultiPolygon':\n\t\t\tpoints = [Point(x[0], y[0]) for x in self.get_points(srid_list)]\n\t\t\tfor point in points:\n\t\t\t\tif point.get"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif len(fields) == 0:\n\t\t\traise ValueError(\"Missing field on %s\" % name)\n\n\t\tif len(fields) == 1:\n\t\t\tfield = fields[0]\n\t\telse:\n\t\t\traise NotImplementedError(\"point_type %s expects a list of points\" % name)\n\n\t\tgeom_type = get_geometry_type(name)\n\n\t\tdtype = \"Polygon\"\n\n        if len(fields) == 1:\n\t\t\tif geom_type == GEOMETRY_POLY:\n\t\t\t\tdtype = \"Polygon\"\n\t\t\telif geom_type == GEOMETRY_LINE:\n\t\t\t\tdtype"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif hasattr(fields, 'coords'):\n\t\t\tif isinstance(names, str):\n\t\t\t\treturn Point(points_type(name, fields), srid_map[names])\n\t\t\telse:\n\t\t\t\traise ValueError(\"Invalid points_type name: %s\" % names)\n\t\tif hasattr(fields, 'coordinates'):\n\t\t\tif isinstance(names, str):\n\t\t\t\treturn Point(point_type(name, fields), srid_map[names])\n\t\t\telse:\n\t\t\t\traise ValueError(\"Invalid points_type name: %s\" % names)\n\n\tdef get_point(self, name,"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\tdef g(y):\n\t\tpass\t\"\"\"\n\t\n\n\tdef g2(y):\n\t\tg1 = 4\n\t\n\tdef g3(y):\n\t\tg1 = 5\n\t\n\tdef f():\n\t\treturn 2.0\n\t\n\tdef g3(y):\n\t\treturn 4.0\n\t\n\t@deprecated.since(2.1)\n\t\n\tdef f2(y):\n\t\tdef test():\n\t\t\ta = 4\n\t\t\tfor thing in dir():\n\t\t\t\tif thing.startswith('test'):\n\t\t\t\t\tgetattr(thomodule,'foo')(a)\n\t\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\t@deprecated(\"'foo' has been deprecated in favour of 'bar'\", 'bar')  # noqa\n\t\tdef bar(x): return x\n\n\tdef foo(x):\n\t\tpass\n\n\tdef bar():\n\t\tpass\n\tbar.__func__['__foo__'] = foo\n\tbar.__func__['__bar__'] = bar\n\tbar.__func__['__baz__'] = foo\n\n\t@deprecate(\"'bar' has been deprecated in favour of 'baz'\")\n\tdef bar(): return z = 10\n\tbar.__func__['__baz__'] = bar\n\tbar.__func__['__"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n__all__ = ('warn', 'warn2', 'warn3', 'ignore')\n\ndef warn(message):\t\"\"\"\n\tAdds a warning to the standard Python output.\n\n\t>>> from warnings import warn\n\t>>> warn(DeprecationWarning)\t\t\n\nAn object can be used as a decorator.\n\nPlease note that this means that when this decorator is used to\nsupply more complex warnings, it must be sufficient to have the\nstandard function and docstring examples use this instead of passing it\nthrough in a function. See:\n\nhttps://docs.python.org/3/howto/logging"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated_function(func):\t\"\"\"\n\tDecorator to mark function `func` when a deprecation is required.\n\n::\n\n    @deprecated(\"function 'foo' now takes a long time to refactor\")\n\tdef foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated_class(classname):\t\"\"\"\n\tDecorator to mark function `classname` when a deprecation is\n\trequired.\n\n\t:param classname: instance of the class to mark function\n\t\t@param newclasses: classnames to be used.\n\t\t@param method: method name\n\t:raises ImportError: if classname is not found"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\nimport os\nimport sys\n\nimport setuptools\nfrom setuptools.command.test import test as TestCommand\n\n\ndef requires(cmd):\n    \"\"\"\n    Check whether the given command requires setuptools.\n\n    @param cmd: the command that requires setuptools\n    @type cmd: str\n    \"\"\"\n    try:\n        from setuptools import setup\n    except ImportError:\n        raise Exception('setuptools or pip: need setuptools. Either pip or'\n                      'setup.py')\n    return cmd in sys.argv\n\n\nclass PyTest(TestCommand):\n    user_options = [('pytest-args=', 'a', \"Arguments to pass to the test"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\treturn functools.partial(x, 'foo')\n\n\"\"\"\nclass Inference:\n\t\"\"\"\n\tBase interface for conference objects.\n\t\"\"\"\n\n\tdef _conferenceAdd(self, conf):\n\t\t\"\"\"Add PConference object and return ConferenceP.\"\"\"\n\t\traise NotImplemented(\"Subclasses must implement _conferenceAdd\")\n\n\tdef _conferenceRemove(self, conf, key):\n\t\t\"\"\"Remove PConference object and return ConferenceP.\"\"\"\n\t\traise NotImplemented(\"Subclasses must implement _conferenceRemove\")\n\n\t@deprecated(\"Use Session.delete() instead\")\n\tdef delete"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n@deprecated(\"The function 'foo' has been deprecated only for 1.3.4+\")\ndef foo(x=1, y=1, z=10.0):\n\tpass\n\t\"\"\"\n\ntry:\n\t#\n\t#\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\n\t#\t#\t#\n\t#\t#     #\n\t#\t#\n\t#\t#\t#\n\t#    #\n\t#\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\nclass foo(object):pass\nfoo.__name__ =\"foo\"\nfoo.__doc__=\"\"\"foo\n\nThe object can be a function (including a __call__). If you want to make your\nobject look like a function, call the first parameter:\n\n    @deprecated(\"'woof', 'hello')\")\n    def woof(woof):\n        pass\n\nwould write:\n\n    f(woof)\n\nThat's also how you can make your methods look like:\n\n    class foo(object):pass\"\"\"\n\t\n        \n\t\n\t\n\n\n\n## Untested\n\ndef blaze("}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n@deprecated('A list is not supported in favour of \"foo\"')\ndef deprecated_list(l=[lambda x:x]):\n  \"\"\"Decorator for deprecating all types of list in fact.\"\"\"\n  def decorated_function(*args, **kwargs):\n    \"\"\"Adds a deprecation warning and fixes any issues that might occur\"\"\"\n    def deprecated_decorator(*args, **kwargs):\n      \"\"\"Decorator around function.decorator\"\"\"\n      if args and len(args)>0 and isinstance(args[0],list):\n        for x in args[0]:\n          deprecated_function(x).deprecated(\"Call '%s()' instead"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(*a):\n        pass\t\"\"\"\n\t\nfunction_warnings = deprecated(\"The function names ``foo``, ``bar`, and 'bar2' are deprecated in favour of 'function_foo' instead of 'function_bar'. Use 'function_foo_x' instead.\", 'function_foo')\n\n\ndeprecated('Function object classes.')\nfunction_classes_deprecations = deprecated(\"This function has been deprecated in favor of 'function_foo' or 'function_bar'. Use 'function_foo_1'"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tenv = frappe.local.request.args.env\n\t\tdoc = frappe.get_doc(code)\n\t\tif not doc.doc_type:\n\t\t\tdata = get_frappe_doc(ddoc=doc)\n\t\t\tif data else frappe._dict()\n\t\t\tddoc = frappe.new_doc('Custom Field', _dict(**data))\n\t\t\tddoc.doc_type = doc\n\t\t\tddoc.db_insert()\n\n\t\treturn code\n\ndef _insert_in_doc(code: str, docname: str, field: str, value: str, index_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tresult = os.system(\"R -c \" + \" \".join(shlex.split(\"echo '\" + code + \"'\").split(' ')[:-1]))\n\t\tif result and result!=0:\n\t\t\tprint(f\"R.py : {result}\")\n\t\t\treturn result\n\t\telse:\n\t\t\treturn code\n\treturn _inline_r_config\n\n\ndef _inline_r_config(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\ts = code\n\t\tif os.environ.get('RL_NO_RL')!= '1':\n\t\t\ts += '\\nRL_NO_RL environment'\n\t\t# We want all characters to be converted to '\\n' (even inside quotes)\n\t\tassert '\\n' not in s, '{0}'.format(s)\n\t\treturn s\n\n\tdef _inline_r_teardown(code: str) -> str:\n\t\t\"\"\"\n\t\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn _inline_r_inline_with_env(code, \"WITH_PREFIX\", True)\n\ndef _inline_r_inline_with_env(code: str, enable: bool = False) -> str:\n\t\"\"\"\n\tIf enabled is False, \"use-inline\" can be used instead of inline-style\n\ttemplates, since they are not allowed using inline. And use-inline\n\tcan be used in conjunction with enable if the inline template is enabled.\n\t\"\"\"\n\t\tif not enable:\n\t\t\tenv_variables = os.environ.copy()\n\t\t\tif"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn config.get([\"R\", \"setup.SHELL\"], True)\t\n\n\t# Note: we do not actually automatically add the SHELL directory, to\n\t# avoid recursion.\n\t_inline_r_dir = os.getenv(\"R\", None) and os.getenv(\"R\").split(\":\")\n\n\tshells = [\"\"]\n\n\tfor line in _inline_r_dir:\n\t\t# Make sure all SHELL entries are in our working dir, and use\n\t\t# -L to separate the path and strip away the /\n\t\tshells.append(os.path"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tenv_vars: Dict[str, str] = {}\n\t\tconfig_values: Dict[str, str] = {}\n\n\t\tif env_vars is None:\n\t\t\tconfig_values = {}\n\n\t\ttry:\n\t\t\tconfig_values['R'] = env_vars['R']\n\t\t\tconfig_values['include_dirs'] = env_vars['include_dirs']\n\t\t\tconfig_values['define_macros'] = env_vars['define_macros']\n\t\t\tconfig_values['undef_macros'] = env_vars['undef_macros']\n\t\t\tconfig_values['macros'] ="}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tif env is None:\n\t\t\tcode, env = frappe.local.path.split(\"tools/run_on_production_server\")\n\t\telif os.getuid()!= 0:\n\t\t\tcode, env = frappe.local.path.split(\"docs/fixtures/production-server/\")\n\n\t\ttry:\n\t\t\tfrom sphinx.ext.autoload import load_page_cache, \\\n\t\t\t\tload_css, load_js, init\n\t\t\tfrom markdown2.markdown import Markdown\n\t\t\tfrom jinja2.runtime import Context\n\t\t\tfrom markdown2 import Block\n\t\t\tfrom frappe.utils.console"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn R.__func__.__doc__.splitlines()\n\n\t#~~ setup for the script.~~\n\nclass SetupPyRunfile(PyRunfile):\n\t\"\"\"\n\tA PyRunfile that can be executed by this script. The ``run`` method is\n\tcalled on every step of the script. All steps have had been executed,\n\tbut at this time, they are still set up to run. Any subsequent steps have\n\tbeen run from executing.\n\t\"\"\"\n\n\t@classmethod\n\tdef setup(cls, code: str, runmethod=None):\n\t\tif runmethod is"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tif len(self.env):\n\t\t\treturn self.env\n\t\treturn self.env['HOME'] or '/'\n\n\tdef _inline_init_hooks(self) -> None:\n\t\t_inline_r_setup('--clear-cache')\n\n\tdef _inline_init(self, text):\n\t\tassert not os.path.exists(self._build_path(text))\n\t\tfp = open(self._build_path(text), 'w')\n\t\tfp.write(text)\n\t\tfp.close()\n\t\ttry:\tself.run('setup -i')\n\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn os.pathsep.join([os.environ[x] for x in ('R', 'R_CONF', 'R_CONF_CONFIG')])\n\texcept Exception:\n\t\timport sys\n\t\timport traceback\n\t\tprint(\"*** R Config ERROR: \", _inline_r_setup.message)\n\t\tprint(\"*** R Options Error: \", _inline_r_setup_options.message)\n\t\tsys.exit(1)\n\ndef _set_var(value: str, var: str, value_type: str) -> str:\n\t\"\"\"\n\tSet var to value or"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\ttry:\n\t\t\targ = _popen_wrapper('/bin/xterm -no-colors -width 1 -text', 'utf-8')\n\t\texcept:\n\t\t\traise xopts\t\n\t\tline = next(iter(xargs.readline, None))\n\t\targ = xargs.readline()\n\t\tprint(line.rstrip())\t # no leading space before newline\n\n\tif cmd=='/usr/bin/xterm':\n\t\t\tif isinstance(arg,str):\n\t\t\t\traise xopts\t\n\t\txargs.close()\n\t\txargs = None\n\t\ttry :\n\t\t\targ = _"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn xargs + [\"-p\", str(target_concurrency)]\n\n\nclass Tmp(object):\n    \"\"\"Wrapper for tempfile.mkstemp().\n\n\tWrite a pseudo-tty in the temporary filename.\"\"\"\n\n    def __init__(self, tmpfile, prefix='/tmp'):\n        self.tmpfile = tmpfile + \".tmp\"\n        self.prefix = os.path.abspath(prefix)\n\n    def close(self):\n        if self.tmpfile is not None:\n            self.tmpfile.close()\n        os.remove(self.prefix)\n\n    def __del__(self):\n        os.remove(self.prefix)\n\n    def __"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t,\n\t)\n\nclass ColoredTextViewer(TextViewer, ColoredTextView):\n\t\"\"\"\tA specialized version of TextViewer with a colored terminal\"\"\"\n\t\n\tdef __init__(\n\t\t\tself,\n\t\t\tcolors,\n\t\t\ttext,\n\t\t\tstream_width=4,\n\t\t\tstream_height=4):\n\t\tTextViewer.__init__(\n\t\t\tself,\n\t\t\tcolors,\n\t\t\ttext,\n\t\t\tcolor_mode=DEFAULT_COLOR_MODE)\n\n\t\tTextView.__init__(self,\n\t\t\tcolor_mode=Color.get_default(),\n\t\t\tcolor_background="}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t]\n\t\t)\n\n\"\"\"\nTests for the ``skel.utils.core`` module.\n\"\"\"\n\nimport unittest\n\nimport mock\nimport requests\n\n\nclass KSKeyTests(unittest.TestCase):\n    \"\"\"\n    Tests for the ``skel.utils.core`` module.\n    \"\"\"\n\n    def test_basic(self):\n        \"\"\"\n        Test ``skel.utils.core`` module basic implementation.\n        \"\"\"\n        baseurl = \"https://test.com/api\"\n        url = baseurl + \"/v1/testing\"\n        response = requests.get(url, headers=None"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t),\n\n\t}\n\n# Pure python for use in Python 3\nif sys.version_info[0] == 3:\n\tdef xargs_unicode_keys( \t\t\tkeywords,       # key in Python 3\n\t                                  \tvalues,       # value values\n\t                                  \t**kw\t\t):\n\t\ttry:\n\t\t\treturn [ x \\\n\t\t\t\t\tfor x in map(\tunkey\t\tlambda x : x[k] \t\t\tif isinstance(x,str) \\\n\t\t\t\t\t\telse x + \"\\n\",\t\t\t\tvalues \\\n\t\t\t\t)\n\t\t\t#return \" \"."}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t))\n\n\tcolor = traits.Any(\n\t\t('target_cpu_count', 'c16[]', 'c16[]',\t'TARGET_CPU_COUNT'),\n\t)\n\n\ttarget_cpu_count = traits.Any(\n\t\t('target_count', 'c16', '|S1', 'TARGET_CPU_COUNT'),\n\t\t('value', 'c16[]', 'c16[]', '|S1', 'TARGET_COUNT')\n\t)\n\n\ttarget_cpu_count_with_flags = traits.Iterable(\n\t\t('flags', 'c16[', '"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t , nargs='?', type=int, default=None )\n\n\t\t\"\"\" A minimal class for all the xargs functionality \"\"\"\n\t\timport pty, termios\n\n\t\t# Make a list of input options in a tuple to pass to Popen\n\t\t# For colorama, it converts to a string with the following format:\n\t\t# xargs(color=xterm)\n\t\t# color is not supported by X11 on Windows...\n\t\t#\n\t\t# If a user input isn't provided, the display will fail with\n\t\t# non-Unix errno for everything else.\n\t\t# However, if stdout is not given use"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t)\n\t\tparser.add_argument(\n\t\t\t'-s', '--speed',\n\t\t\taction='store',\n\t\t\tdest='speed',\n\t\t\tdefault=15,\n\t\t\thelp='Force the CPU speed of commands to run, default = 15 m/s')\n\n\t\tparser.add_argument(\n\t\t\t'-t', '--target-concurrency',\n\t\t\taction='store',\n\t\t\ttype=int,\n\t\t\tdefault=0,\n\t\t\thelp='Target number to process commands to (default, all CPUs)')\n\n\t\treturn parser.parse_args()\n\nclass SPEC_ERROR(Exception):"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t)\n\n\t\"\"\"\n\tA simplified implementation of xargs.  This is useful only for simple\n\tdebugging purposes.  If an array, one item would be added\n\tto the array; otherwise it would be overwritten\n\t\"\"\"\n\n\tdef __init__(\tself \t):\n\t\tself.color = 0\n\n\t\tself.target_concurrency = 4\n\n\t\t'''A simplified implementation of xargs.  These\n\t\tinstances are returned from xargs, which are\n\t\talways passed to xargs to the xargs class\n\t\t'''\n\n\t\tself.target_size = 0\n\"\"\"\n"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t)\n\n\nclass PtyProcess(Process):\n\t\"\"\"\n\tThe PTYProcess provides implementations for running processes in the background process pool\n\tfor the given test and output file.\n\n\tFor usage see the terminal/pty_process_run_line.py script. It starts\n\tthe process with the given arguments and waits for the child process to be\n\talive\n\t\"\"\"\n\n\tdef start(self, run_file, cmd, process_options, options, args, env):\n\t\tif not run_file: logger.error( \"No run file provided.\" )\n\n\t\t"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\ts = []\n\t\tfor x in seq:\n\t\t\ts.append(x[::-1])\n\t\treturn s\n\n\n# Returns randomly generated numbers.\ndef randomNumber(start: int]=0, length: int=100) -> str:\n\t\n\treturn ''.join(random.choice(string.ascii_uppercase + string.digits) for i in range(start, length))\n\n\n# Randomly replaces all characters of all strings with a random sequence\n# This is the common use case.\ndef randomSequence(start: int, end: int) -> str:\t\n\trandom.shuffle"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn [s + s[::-1] if s else s for s in seq]\n\n\n@event_stream\ndef on_load(event: Event) -> None:\n\t\"\"\"\n\tDetects whether the queue item is being processed.\n\n\t:param:`event`: Event to listen\n\t\"\"\"\n\treturn {\n\t\t\"item\": event,\n\t}\n\n\n@event_stream\ndef on_disconnect(event: Event) -> None:\n\t\"\"\"\n\tDeny events on disconnect.\n\n\t:param:`event`: Event to disconnect\n\t\"\"\"\n\tpass\n#"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif len(seq) == 0:\n\t\t\treturn []\n\t\telif iterable(seq):\n\t\t\treturn map(lambda x: shuffle([x]), seq)\n\t\telse:\n\t\t\treturn [str(m) for m in seq]\n\t\"\"\"\n\treturn _shuffled(seq)\n\n#----------------------------------------- Main ------------\n\ndef _process_seq(seq):\n\tif seq.type == 'dna' or seq.type == 'dna+':\n\t\tif 'ACGT' in seq:\n\t\t\tseq.append(seq.replace('-ACGT', 'ACGT'))\n\t\telif"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tseq = list(reversed(seq))\n\t\treturn [i[0] for i in seq]\n\n\t@abstractmethod\n\tdef has_all_non_deterministic_moves(self):\n\t\t\"\"\"\n\t\tDetermines if any non-deterministic moves are present in the sequences in this Sequence.\n\t\t\"\"\"\n\t\tpass\n\n\tdef shuffle(self) -> None:\n\t\t\"\"\"Shuffle all the non-deterministic places.\"\"\"\n\t\tpass\n\n\tdef validate(self, seq: Sequence[str]) -> bool:\n\t\t\"\"\"Validates the sequence.\"\"\"\n\t\tif not self.has_sequence(seq"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn [_i*i for _i in seq]\n\ndef _shuffle(seq: list) -> List[str]:\t\"\"\"\n\tRandomly shuffle\t\"\"\"\n\t\tseq = list(_shuffled(seq))\n\n\t# If there is only one element, just shuffle all the digits before returning\n\tfor element in seq:\n\t\t_shuffle(element)\n\n\t# If there are many elements, shuffle every element\n\tmax_seq = _shuffled(seq)\n\tfor i in xrange(len(seq)):\n\t\t_shuffle(seq[i])\n\treturn max"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn _suffled.shuffle(seq)\n\t\t\ndef _shuffle(seq: Sequence[str], batch_size: int = 1, seed: str = None) -> int:\t\"\"\"\n\tShuffle a sequence. Repeated calls to shuffle is allowed.\n\t\"\"\"\n\t\tif not seed and not _shuffled.random():\n\t\t\t# Randomize a sequence with a seed. This will make the random number unique\n\t\t\t# to start each call.\n\t\t\tassert seed!= 0, \"The seed should be an integer, not a string.\"\n\t\t\tif not utils.is_number("}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn _shuffle(list(seq))\n\n\n\tdef _shuffle_with(self, list_values: List[str]=None):\n\t\t\"\"\"\n\t\tResets the shuffle of a given list of values in a list\n\n\t\tArgs:\n\t\t\tlist_values: List of values which shuffle. Takes as input (i.e. str), as input\n\t\t\treturns the list of shuffled list values\n\t\tReturns:\n\t\t\tList of shuffled values\n\t\t\"\"\"\n\n\t\ttry:\n\t\t\tshuffle_values = list(reversed(list_values))\n\t\texcept TypeError:\n\t\t\traise Exception(\"Only"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn [s for s in seq if seq[:s]!=s and len(list(seq[s]))!=0]\n\n\ndef _split_sequence(seq_s: str) -> [str]:\n\t\"\"\"\n\tSplits a string to create only the first n elements to have the\n\toriginal length.\n\t\"\"\"\n\treturn [s.split('-')[:n] for n in range(len(seq_s)-1)]\n\n\ndef _random_input_s:\n\t\"\"\"\n\tRandomize input\n\t\"\"\"\n\n\treturn str(random.uniform(0."}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn seq[::-1]\n\ndef extract_data_for_shuffling(seq: Sequence[str]), max_vocab: int, min_vocab: int_value=None, verbose: bool = True) -> List[str]:\n\t\"\"\"\n\tInput:\n\t\t- seq - seq\n\t- max_vocab: max vocabulary size\n\t- min_vocab: min vocabulary size\n\t- verbose: Show the logs and their indices or keys.\t\n\t\"\"\"\n\timport itertools\n\tfrom collections import Counter\n\tfrom scipy.io.wavfile import write_wav\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tassert len(seq) == len(set(seq)).next(), \"len(seq) must equal len(set(seq))!\"\n\n\ts = seq[:]\n\tfor i in xrange(len(seq)): s.insert(0, 'G')\n\treturn s\n\t\ndef rand_hash(n: int) -> int:\n\t\"\"\" Returns the nth unique hash on a sequence n and returns it.\n\n\tReturns:\n\t\tInt, The ordinal of the hash. This is a unique hash that\n\t\t\tmay be larger than 500000.\n\t\"\"\"\n\treturn _hash(sha"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tresult = 0\n\t\t# Strip the version number, but don't do it if the string is not\n\t\tif s.startswith(\"0x\"):\n\t\t\tresult = int(s[3:], 16)\n\t\telif s.startswith(\"2x\"):\n\t\t\tresult = int(s[3:7])\n\t\t# Strip version-id, because it is version+id instead of version+id.\n\t\t# XXX: It looks like we are certain about the right thing, but I think\n\t\t# it is just the version instead.\n\t\telif s.startswith(\"0x\"):\n\t\t\tresult"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\ts_ = s[:s.find('v')].upper()\n\n\t\tif not s or s.startswith('v'):\n\t\t\treturn None, None, None, None\n\n\t\treturn int(s[len('v.'):])\n\n\tpass\n\n\nclass PoseTag(PoorTag):\n\t\"\"\"\n\tplaats version, or tag from a Pose tag.\n\t\"\"\"\n\tpass\n\n\nclass ParseType(type):\n\t\"\"\"\n\thelper for all type and subtype\n\t\n\tThe types are handled by the ``type`` class.\n\t\n\t>>> parse_type('foo."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tif s == '':\n\t\t\t# if no source list: return the full version string\n\t\t\treturn str(r.release_version).split('v. ')\n\t\t# start version match\n\t\tr = re.search(r'\\d+\\.\\d+(?:\\.\\d+)?(?:\\.\\d+)?(?:(?:\\.\\d+)?)?$', s)\n\t\tif r is not None:\n\t\t\treturn int(r.group(1)), int(r.group(2))\n\t\t# end version match\n\t# end parse_version\n#end parse_version\n\n\nif __name__ == \"__main__\":\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\timport re\n\t\tversion_matchers = (\n\t\t\tr\"^(\\d+)\\.\\d+\",\n\t\t\tr\"^(\\d+)\\.(\\d+)*\",\n\t\t\tr\"^(\\d+)\\.(\\d*)\",\n\t\t)\n\n\t\t# XXX use -1 for both -1 and 0. if we really want it as a match\n\t\t# but as not everything is matched (e.g. \"1.0.2.1\")\n\t\tif version_matchers!= [(1, 0)]:\n\t\t\tassert version_matchers\n\t\t\tmatch = re.match(version_matchers["}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\treturn parse_git_commit(s,'version', '')\n\ndef parse_git_commit(s, 'commit', fallback=''):\n\tif s:\n\t\treturn s.strip(' + ')\n\treturn fallback\n\ndef apply_hash_files(source_name, source_root: str) -> None:\n\t\"\"\"\n\tGiven a source tree, apply hash files from the root of the source tree\n\tfrom the git repo to the specified hash file. This is done in the current version of\n\tthe git repository. Incredibly change the source to the destination directory"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\twith open(s, encoding='utf-8') as file: \n\t\t\treturn file.readline().strip()\n\n\ndef get_usage():\n\tprint('usage: pypoor.py [options]')\n\traise SystemExit(1)\n\n\ndef run(argv: List[str] = sys.argv[1:]):\t# pylint: disable=arguments-differ\n\t'''\n\tRuns PyPOR to get information about python.\n\tusage: pypoor.py [<options>]\n\thelp: Prints usage for pypoor.py and returns information on sys.exit\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\t# This is used by get_version to split the version from several things.\n\t\t# The output \"+\" will be converted to \"1.2.3\" as part of a numeric version string.\n\t\tif s.startswith(\"+') or re.match('^1\\.2\\.3(.(\\.(\\d+))?$', s, re.X): # We could still assume that there is 99999.0.0 or 99999.0.9\n\t\t\tmajor = None\n\t\t\tminor = None\n\t\t\tfinal_major = None\n\t\t\tif re.match('^1\\.2\\.3(.(\\d"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tif not s:\n\t\t\treturn None, None\n\t\t(b, a) = s.split('.')\n\t\ts = b\n\t\tif b!= '':\n\t\t\ta = a.replace(\".\", \"\", 1)\n\t\t\ts = s.replace(\"-\", \"\", 1)\n\t\ts = s.strip()\n\t\tif not s or not a:\n\t\t\treturn None, None\n\t\ty0, y1, sr = b.split('x', 1)\n\t\tif not sr or sr.endswith('m'):\n\t\t\traise ValueError('Expected x for y0 x1')\n\t\td, m"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\treturn tuple([int(s[:i]) for i in py.builtin.listValues(s)])\n\telse:\n\t\ttry:\n\t\t\tpoor = py.builtin.get_poor()\n\t\texcept NotImplementedError:\n\t\t\tpass\n\t\telse:\n\t\t\tver = tuple([int(i) for i in poor.listValues(poor.constInt)])\n\t\t\tif not ver:\n\t\t\t\tprint(\"WARNING: version '%s' is not installed\"%str(poor))\n\t\t\t\tver = (poor[1] if poor.constInt>0 else -1)%"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\ttry:\n\t\t\treturn int(s)\n\t\texcept ValueError:\n\t\t\t# Not a version number\n\t\t\treturn 0\n\n\t# Make sure we're sure that it's a major version\n\ttry:\n\t\treturn parse_version(s)\n\texcept ValueError:\n\t\treturn 0\n\n\ndef is_in_version(s: str) -> bool:\n\t\"\"\"\n\tReturn True if a version is a major version\n\t\"\"\"\n\n\thas_version = get_version() is not None\n\n\tif has_version is None:\n\t\thas_version = get_version() is"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\t# Convert the string to something we can convert into a list\n\t\tcommand_parts = shlex.split('%s' % (cmd,))\n\t\t# Create a command list\n\t\treturn list(map(lambda x: str(x), command_parts))\n\ntry:\n\t# Some of our commands can be called via this python script\n\timport string\n\thas_unicode_escape = True\nexcept ImportError:\n\thas_unicode_escape = False\n\thas_native_unicode = True\n\nfrom pip._vendor.six.moves.urllib.parse import quote, unquote, urlsplit,"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n#    from pygments.console import colorize\n#    colorize = lambda text: colorize(text, {\n#        'bold' : False,\n#        'underline' : False,\n#       'strikethrough' : False,\n#       'strong' : False,\n#        'bg_color' : True,\n#        'blink_color' : True\n#    }, cls=cls, stream=sys.stderr)\n\n                # replace back, white and background\n                cmd =''.join(cmds)\n\n\t# cmd is already stripped from the cmd list, so remove the spaces"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\ttry:\n\t\t\tcmd.insert(0, os.path.join(os.path.dirname(command[0]), \"Shebangs\"))\n\t\texcept:\n\t\t\tcmd.insert(0, os.path.join(os.path.dirname(command[0]), \"Chinese shebangs\"))\n\n\treturn cmd\n# This file is part of Indico.\n# Copyright (C) 2002 - 2016 European Organization for Nuclear Research (CERN).\n#\n# Indico is free software; you can redistribute it and/or\n# modify it under the terms of"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\twith open(fnpath, 'r') as fd:\n\t\t\tlines = fd.readlines()\n\t\tlines.insert(0, cmd[0])\n\t\treturn '\\n'.join(lines)\n\n# The following function is used by the shebangs to ensure that the script\n# starts parsing correctly\ndef read_files(fnm):\n\t\"\"\"Read files from a file based on the output of shebangs\"\"\"\n\twith open(fnm) as f:\n\t\tlines = f.readlines()\n\treturn lines\n\n# This is based off code from https://github."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn (os.path.realpath(cmd[1], os.X_OK))\n\n\ndef normalize_env(env: dict[str,...]) -> object:\n\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn (os.path.realpath(env.get('SHOWSPATH', []), os.X_OK))\n\n\ndef process_environment_string(cmd, shell:"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn cmd[0].replace('+','')\t\n\n\ndef get_git_version() -> str:\n\t\"\"\" returns the git version of this repo\"\"\"\n\treturn get_git_hash()\n\n\n# TODO\ndef get_git_short_cmd(version: str) -> str:\n\t\"\"\" return the long git url of the current version of Git \"\"\"\n\t# it needs to be updated in order to parse the long. this function\n\t# will not use it for git versions >= 1.9.1 and below\n\t# http://cgit.freedesktop.org/source"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tif isinstance(cmd, str):\t\t\t\t# type: ignore[misc]\n\t\t\t# windows: command is a dict\n\t\t\tcmd = normalize_cmd(cmd)\n\t\t\treturn cmd + ('.' + _pathfun) if _pathfun in cmd else _pathfun\n\t\telif isinstance(cmd, tuple):\n\t\t\t# windows: command is a list\n\t\t\t#  - convert to list\n\t\t\t#  - convert to list with spaces:\n\t\t\t#\t\t1. remove whitespace and spaces\n\t\t\t#       2. remove empty parts and empty string\n\t\t\tcmds = cmd\n\t\t\t# windows:"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tcmd = list(cmd)  # type: List[str]  # type: List[Tuple[str,...]]\n\t\tif not cmd:\n\t\t\treturn (None, None)\n\t\tif type(cmd[0])!= str:\n\t\t\tcmd = (cmd[0], os.path.basename(cmd[0]), *cmd)  # type: Tuple[str,...]\n\t\tout, err = run_cmd(*cmd)\n\t\tif err:\n\t\t\traise Exception('Could not fix version string: {}'.format(cmd[0]))\n\t\treturn ret\n\n\t# We"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tif platform == \"win32\":\n\t\t\treturn cmd\n\t\telse:\n\t\t\treturn list(chain.from_iterable(x for x in cmd if x.endswith('.py'))[:2])\n\ndef makedirs(path: str) -> None:\n\t\"\"\"Make all empty folders in the path.\n\n\tThe following is a wrapper around os.makedirs() so that the parent\n\tdirectories in the returned paths would be removed by the current shell.\n\n\tReturns:\n\t\tNone - Will also remove empty folders.\n\t\"\"\"\n\n\tif not os.path.exists(path):\n\t\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn tuple([line for line in cmd if not re.match(r'[\\n\\r]*', line)])\nfrom os.path import dirname, join\nfrom subprocess import Popen\nimport tempfile\nimport shutil\n\n\n\ndef makeTempDir(dirname, suffix='', prefix=''):\n\n\n    dirname = dirname(dirname)\n\n    if not dirname.startswith('./'):\n        dirname = join(dirname, dirname)\n\n\n    with open(join(dirname, 'tmp')):\n        data = open(join(dirname, '../testdata'), 'w')\n        data.write(\"this is dummy file\")\n        data.flush()\n\n\n\n        tempPath = tempfile"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(cache)\t\n\t\tdef wrapped_wrapper(*args, **kwargs):\n\t\t\tcache_key = \"{0}_{1}\".format(cache_key, key)\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\t# If a lock can't be acquired, it will be\n\t\t\t\t\t# created in a different lock. We'll assume it\n\t\t\t\t\t# will be available for this lock so we have\n\t\t\t\t\t# no need to do that in the same lock later.\n\t\t\t\t\tlock.acquire()\n\t\t\tcache_args = (args, kwargs)\n\t\t\tcache_key_hash ="}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef decorator(func):\n\t\t\tobj = cache.get(key, None)     # cache the key\n\t\t\tif obj is None:\n\t\t\t\tresult = func()\n\t\t\telse:\n\t\t\t\tobj.update_with_cache(cache, key, lock)\n\t\t\t\tresult = obj\n\n\t\treturn decorator\n\treturn cached\n\n# -*- coding: utf-8 -*-\nimport os\nimport zipfile\nfrom collections import namedtuple\nfrom shutil import deepcopy\nfrom uuid import uuid4\nfrom datetime import datetime as dt\nfrom subprocess import call\n\n\nclass FileUploadJobError(Exception):\n    pass\n\n\nclass"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef _wrapper(*cache_keys):\n\t\t\tkey = lock(key, cache_keys)\n\t\t\tfor cached_key in cache_keys:\t\n\t\t\t\t@functools.wraps(func)\n\t\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\t\tkey = cache_keys[0]\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcache_value = func(*args, **kwargs)\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tcache_value = func(args, **kwargs)\n\t\t\t\t\treturn cache_value\n\t\t\t\t\tkey = key.encode(\"utf-8\")\n\t\t\t\t\tcache_key.clear()\n\t\t\t\t\tvalue = cache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tif callable(cache):\n\t\t\treturn cache()\n\t\telse:\n\t\t\tcache_timeout = 1\n\t\t\tcache = lambda: None\n\t\t\tdef decorator(f):\n\t\t\t\tif args:\n\t\t\t\t\tcache_timeout = args[0].timeout\n\t\t\t\telse:\n\t\t\t\t\tcache_timeout = cache_timeout\n\t\t\t\tcache = f(cache_timeout=cache_timeout)\n\t\t\t\tif cache is not None:\n\t\t\t\t\tif args[0] in cached:\n\t\t\t\t\t\treturn cache(args[0].cache, key=key, lock=lock)\n\t\t\t\t\telse:\n\t\t\t\t\t\tcache.put(key,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif cache.has_key(key):\n\t\t\t\tcache[key].acquire()\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn func(*args, **kwargs)\n\treturn wrapper\n\nclass BaseObject(object):\n\t\"\"\"Base object class.\n\tObject(self) -> object\n\t\tThis class encapsulates all the storage that is a bit more readable.\n\t\tIt must be held as a result of calling the :method:`update` method.\n\t\"\"\"\n\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@functools.wraps(cache, cache.func_name, cache)\n\t\tdef cache_wrapper(*args, **kw):\n\t\t\tif not cache.locked():\n\t\t\t\tresult = cache(*args, **kw)\n\t\t\t\tcache.unlock(lock=lock)\n\t\t\t\treturn result\n\t\t\telif return_value(cache, key=key, lock=lock):\n\t\t\t\treturn cache.get(key, None)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Could not find cache key \"+key)\n\t\t\n\t\treturn cache_wrapper\"\"\"\nUtility functions for generating new URLs and stuff\n\nBy Kevin"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tfunc = lambda x: cache[key(x)]\n\t\tkey = key if lock is None else lock.acquire_lock()\n\t\tif lock is None:\n\t\t\tkey.release(lock) # release a lock, don't acquire it first\n\t\treturn func\n\t@cached\n\tdef cached_property(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(args[0])\n\t\t\tif hasattr(cache, key):\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn func(*args, **"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tcache[key] = cache if hasattr(cache, 'get') else cache.set\n\t\tif lock is None: lock = Lock()\n\t\tcache[key].value = cache[key].value + 1\n\t\treturn cache[key].value\n\treturn cached\t\n\ndef cached_get(cache, key, key_func):\t\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\"\"\"\n\t\tcache[key] = cache_get(cache, key, key_func)\n\treturn cached_get_with_cache(cache,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\t@wraps(function, lock)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif key in cache: return cache[key]\n\t\t\telif lock:\n\t\t\t\treturn cache[key]\n\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn cached\nfrom __future__ import absolute_import, division, unicode_literals\n# Copyright (c) 2017 Ansible Project\n# Copyright (c) 2018 Extreme Networks, Inc.\n#\n# GNU General Public License v3.0 (see COPYING or https"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tif key not in cache:\n\t\t\tcache[key] = wrapped(cache[key], lock)\n\t\treturn cache[key]\n\treturn cached\n\n\nclass Context:\n\t\"\"\"Context object.\"\"\"\n\t__slots__ = ('__func', '__name',)\n\tdef __init__(self, name):\n\t\tself.__name = name\n\n@cached\ndef memoize_context_options(func, *args, **kwargs):\n\t\"\"\"\n\tDecorator to call ``func`` as options when a certain keyword\n\tparameter matches a context.\n\t\"\"\"\n\t@wraps(func"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\ttfn = lambda *args, **kwargs: time.monotonic() * args\n\t\tself.fnhash = hashlib.sha1()\n\t\tself.ttl_cache_key = (maxsize, ttl, tfn)\n\t\tself.maxsize = maxsize\n\n\t\tif typed:\n\t\t\t_time_elapsed_since_start = timer() - maxsize\n\t\t\tfor i in xrange(maxsize):\n\t\t\t\tself.fnhash.update(tfn(i * _time_elapsed_since_start))\n\n\tdef __call__(self, *args, **kwargs):\n\t\ttfn = args["}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef wrap(fn):\n\t\t\targs = (fn.__name__,,ttl,timer,types.MethodType(fn,fn.__self__), typed)\n\t\t\tself.memoize(fn.__name__, *args)\n\t\t\treturn fn\n\t\treturn wrap\n\t# end of wrap_wrapper\n\n\ndef timing_decorator(func):\n\t\"\"\"Decorator to wrap one or more function or method objects (e.g. callables)\"\"\"\n\n\tdef wrap(obj, ttl=None, ttl_cache=None, typed=False, fn=None, typed_timer=None):\n\t\tif ttl_cache"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tself._lock.acquire()\n\t\tif typed:\n\t\t\tself._type = type(self)\n\t\telse:\n\t\t\tself._type = counter(self._lock, maxsize, ttl)\n\n\t\tif timer is None:\n\t\t\tself._timer_limit = maxsize\n\t\telse:\n\t\t\tself._timer_limit = timer\n\t\tself._lock.release()\n\t\tsleeptime = timer() - timer()\n\n\t\tif maxsize >= 0:\n\t\t\treturn functools.reduce(ttl_cache, self._type)(self._lock, maxsize)\n\t\telse:\n\t\t\treturn partial(ttl_"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tif isbuiltin:\n\t\t\tdef wrapper(func):\n\t\t\t\treturn timer(*args)\n\t\t\twrapper.__module__ = func.__module__;\n\t\telse:\n\t\t\twrapper = lambda func: timer(*args)\n\t\tfunc.__cached__ = {}\n\t\tcache = {}\n\n\t\tif typed:\n\t\t\tcache, ttl = make_callback_cache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n\n\t\t@wraps(func)\n\t\tdef wrapper(*args):\n\t\t\t\"\"\"Calls a function and returns the cached result, or if an\n\t\t\tentry point throws an exception"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t@functools.wraps(wraps(func))\n\t\tdef _wrapper(self, *args, **kwargs):\n\t\t\tdef wrapped(*args, **kwargs):\n\t\t\t\tif typed:\n\t\t\t\t\twith self.timer(timer):\n\t\t\t\t\t\treturn func(*args, **kwargs)\n\n\t\t\t\treturn self.make_cache_key(*args, **kwargs)\n\n\t\t\t# Add more args to make the cache key longer\n\t\t\t_wrapper._get_ttl()\n\n\t\t\twith self.ttl_key_lock:\n\t\t\t\tif maxsize <= 0:\n\t\t\t\t\tself.ttl_key_locked = False\n\t\t\t\t\tself."}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tMAX_TTL = maxsize\n\t\tdef wrapper():\n\t\t\ttimeout = timer() - ttl\n\t\t\tmax_cache_entry = {}\n\t\t\twhile timeout < (time.monotonic() + ttl):\n\t\t\t\ttry:\n\t\t\t\t\tcached_value = yield cached_data(maxsize)\n\t\t\t\t\tif typed:\n\t\t\t\t\t\tcached_value = next(generator)\n\t\t\t\t\t\tif cached_value in max_cache_entry:\n\t\t\t\t\t\t\tmax_cache_entry[cached_value] += 1\n\t\t\t\t\t\t else:\n\t\t\t\t\t\t\tmax_cache_entry[cached_value] = 1\n\t\t\t\t\t\tcached_value = next(generator"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef dec(function):\n\t\t\tcache = {}\n\t\t\tdef wrapper(*args, **kw):\n\t\t\t\tif 'expire' in kw or self.__delay <= 0:\n\t\t\t\t\treturn function(*args, **kw)\n\t\t\t\tresult = cache.get((ttl,), 0.0)\n\t\t\t\tval = time.monotonic()\n\t\t\t\tresult = function(*args, **kw)\n\t\t\tretck = cache.get(maxsize, lambda: result)\n\t\t\tcache[maxsize] = val\n\t\t\tcache[retvalck] = result\n\t\t\tcache.pop(retvalck, None)\n\t\t\tcache[maxsize"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tself._cache = self._make_result_cache( maxsize, timer, typed )\n\t\tif self._cache is not None:\n\t\t\tresult = self._cache.popitem()\n\t\t\tself._cache.update( result )\n\n\tdef _make_result_cache(self, maxsize, timer, typed ):\n\t\t\"\"\"\n\t\tCreate and memoize functions to store results on the heap with\n\tmaximal latency and type-checking. Returns the stored result. If the\n\tfunction is not cached, the original function is returned. The first\n\ttime-to-live entry must be returned."}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tmaxtime = maxsize\n\t\tdef f(*args, **kwargs):\n\t\t\ttimeout = timer() + ttl if typed else timer()\n\t\t\tresult = function(*args, **kwargs)\n\t\t\tif typed:\n\t\t\t\tyield result\n\t\t\telse:\n\t\t\t\tpop = pop1 = popm = heapq.heappop(result)\n\t\t\t\tif pop <= maxsize:\n\t\t\t\t\tfor result in pop, popm:\n\t\t\t\t\t\tyield result\n\t\t\treturn result()\n\t\treturn f\t# wrapped function\n\tdef timed_execute(func):\n\t\tif typed:\n\t\t\tresult = func()\n\t\telse"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tttl = timers.get_value(\"time-to-live\", ttl)\n\t\tresult = ttl.run_in_parallel_function(wrapped, maxsize, ttl, timer, typed)\n\n\t\t# Now we need to track which items need to recalculate the TTL\n\t\treturn result - TTL_R, result\n\n\tdef cache_clear(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that removes\n\tall items from the list.  This is particularly useful for memoizing\n"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tcache = self.cachesize\n\t\tfunc = safe_pickle.cache.setdefault(memoize, memoize)(func)\n\t\t_cache_set(cache, maxsize, typed)\n\t\treturn func\n\n\tdef _get_default_timeout(self):\n\t\treturn get_default_cache_timeout()\n\n\tdef _get_time_limited(self, func):\n\t\tif func is None:\n\t\t\treturn None\n\t\tdef wrap(*args, **kwargs):\n\t\t\ttimeout = self._get_default_timeout()\n\t\t\tself._set_time_limited(func, timeout,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\t@functools.wraps(fn)\n\t\tdef wrapper(*args, **kwals):\n\t\t\tif not typed and size(maxsize) > 0 and callable(fn):\n\t\t\t\traise TypeError(\"Cache limit has higher priority than maxsize; \"\n\t\t\t\t\"use cached arguments instead.\")\n\t\t\tt = fn(*args, **kwals)\n\t\t\ttry:\n\t\t\t\tif t in KNOWN_MRU_ALGOS:\n\t\t\t\t\tresult = t(maxsize, typed)\n\t\t\t\telse:\n\t\t\t\t\tresult = fn(maxsize, typed)\n\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\tt = self."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef memoize(func):\n\t\t\tif typed:\n\t\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t\tif \"max_length\" in kwds:\n\t\t\t\t\t\tdel kwds[\"max_length\"]\n\t\t\t\t\tif hasattr(func, '__call__'):\n\t\t\t\t\t\tresult = func(*args, **kwds)\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult = func(*args, **kwds)\n\t\t\t\tmemoize_wrapper.__name__ += func.__name__\n\t\t\t\n\t\t\telse:\n\t\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t\tdel kwds[\"max_length\"]\n\t\t\t\t\tif hasattr(func, '__call__'):\n\t\t\t\t\t\tresult"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\t_mru = mru_cache_factory()\n\t\treturn _mru.mru_cache(maxsize, typed)\n\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tAdds a ``memoize_wraps`` function to the global dict of functions\n\t\tused later by `mru_cache`.\n\t\t\"\"\"\n\t\tif func is not None and func not in mru_cache_created:\n\t\t\tmru_cache_created[func] = 1\n\t\treturn _mru.mru_cache(func, maxsize, typed)\n\treturn decorator\n\ndef memoized("}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef memoized(wrapped, key=id):\n\t\t\tif key in wrapped:\n\t\t\t\treturn wrapped[key]\n\t\t\twrapped[key] = memoized\n\t\t@wraps(wrapped)\n\t\tdef wrapper(*more):\n\t\t\ttry:\n\t\t\t\twhile len(more) > maxsize:\n\t\t\t\t\tkey, value = more.pop(0)\n\t\t\t\t\tif typed:\n\t\t\t\t\t\tvalue = value.cache(maxsize=maxsize)\n\t\t\t\t\twrapped[key] = value\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn wrapped[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tpass\n\t\t\texcept IndexError:\n\t\t\t\treturn wrapped\n\n\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef wrap(f, memo=None, typed=typed):\t\t \n\t\t\tf.maxvalue = maxsize\n\t\t\tmemo[id(f)] = f\n\t\t\tif typed:\n\t\t\t\tfunc = memo.get(id(f))\n\t\t\t\tmemo[id(f)] = func\n\t\t\t\treturn func\n\t\treturn wrap\n\n\treturn mru_cache\n\nclass S3BotoStorage(Storage):\n\t\"\"\"\n\tA subclass of the Storage that uses S3Boto but is very unclear where\n\tstorages are designed in terms of boto (and some of the various AWS\n\tservices don't"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tif typed:\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapped(*args):\n\t\t\t\tcache = mru_cache(maxsize=maxsize, typed=True)\n\t\t\t\treturn cache[0]\n\t\t\treturn wrapped \t\n\t\t\n\t\t# XXX: cacheable?\n\t\tcached = cache.get(0)\n\t\tif not cached:\n\t\t\tcache = mru_cache(maxsize=maxsize, typed=False)\n\t\telse:\n\t\t\treturn cached\n\t\n\t\treturn cached\n\treturn mru_cache(maxsize=maxsize)\n\ndef mru_cache_lock(maxsize=128):"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef _fn(*args, **kwds):\n\t\t\tkey = kwds.get('key', None) or mru_config.get('caching')\n\t\t\tif key is None and not typed:\n\t\t\t\treturn decorated(mru_config.get('key') or mru_config.get('key2'))(mru_config)\n\t\t\telif isinstance(key, basestring):\n\t\t\t\tkey = tuple([x if x!= '@' else x] for x in key)\n\t\t\telif isinstance(key, Memcache):\n\t\t\t\tkey = key.get()\n\t\t\t\tif not key.startswith('m"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\twith memoize(func, maxsize, typed) as data:\n\t\t\t\treturn data if isinstance(data, bytes) else unicode(data)\n\t\tif not callable(func):\n\t\t\t# Don't wrap functions with too many arguments.\n\t\t\treturn wrapper\n\t\treturn wrapper\n\treturn mru_cache\n\n\ndef mru_cacheable(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that caches\n\tresults based on a Most Recently Used (MRU) algorithm.\n\t\"\"\"\n\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tif typed:\n\t\t\t\tdef singleton(x):\n\t\t\t\t\tif len(x) < maxsize:\n\t\t\t\t\t\tfun = memoize(x, maxsize)\n\t\t\t\t\telse:\n\t\t\t\t\t\tfun = memoize(x, maxsize)\n\t\t\t\treturn wrapped_func(func)\n\t\t\treturn func\n\t\tif callable(maxsize):\n\t\t\treturn wrapper\n\t\telse:\n\t\t\treturn singleton\n\treturn mru_cache\n\n\ndef is_string(x):\n\treturn isinstance(x, basestring)-1\n\n\ndef quote(string, safe=\"%>?/=+$"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith cache_for(maxsize=maxsize):\n\t\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn lru_cache\n\n@lru_cache(maxsize=128, typed=False)\ndef lru_cache_clear(maxsize=128):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that removes\n\tused keys from the cache.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith cache_for(maxsize"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\treturn functools.lru_cache(maxsize=maxsize, typed=typed)\n\n\t@lru_cache\n\tdef lru_cache_key(fn):\n\t\timport os\n\t\treturn os.path.join(os.path.expanduser('~'), '.local/lib', 'lru') + fn\nimport sys, os, requests\nfrom bs4 import BeautifulSoup\nfrom BeautifulSoup import BeautifulStoneSoup\nfrom time import time\n\nif __name__ == \"__main__\":\n\n    # If you want to run this application from the command line argument to run this python code from within the directory on your local"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\t@wraps(fn)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tresult = None\n\t\t\ttry:\n\t\t\t\treturn fn(*args, **kwargs)\n\t\t\texcept MemoryError:\n\t\t\t\tresult = None\n\n\t\t\tif typed:\n\t\t\t\tresult = cache_get(maxsize)\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = wrap(args[0], maxsize)\n\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\treturn cache_get(maxsize)\n\n\treturn wrapped\n\n\ndef cache_get(maxsize):\n\t\"\"\"\n\tReturn a cached version of the function.\n\t\"\"\"\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\treturn lambda s: cache.lru(maxsize, typed and s.cache_key())\n\telse:\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif maxsize is not None and maxsize < 0:\n\t\t\t\treturn lambda s: cache.lru(maxsize, typed and s.cache_key())\n\t\t\treturn s.cache_key(*args, **kwargs)\n\treturn functools.partial(wrapped)\n\n\ndef cache_if_none_match(cache_key, default_cache_size=1800, retry_timeout=2):\t# pragma: no cover, use"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif typed:\n\t\t\tfunc = typed.im_func\n\t\telse:\n\t\t\tfunc = lambda x: x\n\t\treturn _lru_cache()[maxsize - 1](\\\n\t\t\tfunc, maxsize, typed)\n\ndef _lru_cache():\n\t\"\"\"\n\tReturns a mapping from lexicographic sort keys to their actual LRU hash\n\tvalues. See http://en.wikipedia.org/wiki/Lexicographic_sort_usage.\n\t\"\"\"\n\tdef _lru_memoize():\n\t\t\"\"\"\n\t\tCreates a memoizing decorator for the caching algorithm\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef cached_func(self, *args, **kwargs):\n\t\t\tself._lru_cache = LRUCache(maxsize)\n\t\t\treturn func(self, *args, **kwargs)\n\t\treturn cached_func\n\n\t@lru_cache.lru_cache_size\n\tdef lru_cache_size(self):\n\t\tif self._lru_cache is None:\n\t\t\treturn None\n\t\treturn self._lru_cache.lru_cachesize\n\n\t@lru_cache.lru_cache_maxsize\n\tdef lru_cache"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif typed:\n\t\t\tm = type('m', (object,), {})\n\t\telse:\n\t\t\tm = cached_property(maxsize=maxsize)\n\t\treturn lru_cache_fun(m, maxsize)\n\telse:\n\t\t@functools.wraps(func)\n\t\tdef wrap_func(*args, **kwargs):\t# pylint: disable=W0613\n\t\t\t# pylint: disable=unused-argument\n\t\t\t# If a function takes an argument, we can only have a\n\t\t\t# decorated version of functions that require that\n\t\t\t# the argument be passed. If it's wrapped by"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif typed:\n\t\t\t# get maxsize\n\t\t\ttry:\n\t\t\t\tmaxsize = int(maxsize * 1e9)\n\t\t\texcept ValueError:\n\t\t\t\tmaxsize = 256\n\t\telse:\n\t\t\tmaxsize = maxsize / 1000 or 4096\n\n\t\tdef decorator(f):\t\t\t\t\t\t# Decorator\n\t\t\tf.__name__ = f.__name__\t\t\t\t\t\t# Function Name\n\t\t\tif maxsize and maxsize % 2 == 1:\t\t\t\t\t# Don't bother if too small\n\t\t\t\t# We'd like to run fast enough for testing, so just take the\n\t\t\t\t# maximum of 2 times to prevent long memory exhaustion\n\t\t\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tfn = lambda x: x\n\tdef wrapper(*args, **kwargs):\n\t\tif typed:\n\t\t\t# TODO: need to do some optimizations here\n\t\t\tfun = functools.partial(fn, maxsize)\n\t\t\tif maxsize == 0:\n\t\t\t\tprint(fn('no suitable input for the cache'))\n\t\t\telse:\n\t\t\t\tprint(fn('cache size is {0:d} bytes'.format(maxsize)))\n\t\t\treturn fun(*args, **kwargs)\n\t\treturn fn(wrapper)\n\n\treturn wrapper\n#  Copyright (c) 2021 Ultimaker B.V."}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tif not typed:\n\t\t\treturn wrapped([function], maxsize)\n\t\tdef wrapper(*s):\n\t\t\targs, varargs, varkw, defaults = s\n\t\t\tresult = cached.get(key, lambda *_: s(*args, varargs=varargs, varkw=varkw)\\\n\t\t\tor function(varargs, varkw, defaults, *args, **args))\n\t\t\tif maxsize > maxsize:\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn lru_cache\nfrom __future__ import print_function\n\nfrom collections import Iterable\nfrom itertools import combinations\n\n"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\timport sys\n\t\tfrom functools import wraps\n\t\tdef memoizer(self, *args, **kwargs):\n\t\t\tif self._last_fit is None:\n\t\t\t\treturn self.__cache__\n\t\t\telse:\n\t\t\t\tcache = self._last_fit()\n\t\t\t\tif not typed:\n\t\t\t\t\tcache = cache.get\n\t\t\t\treturn cache(*args, **kwargs)\n\t\treturn memoizer\n\n\tdef _get_least = lru_cache(maxsize=128, typed=False):\n\t\t\"\"\"\n\t\tReturn the most fit value using the hash of `self._lru`.\n\t\t\"\"\"\n\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tmaxsize = 0\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = cached_cache_get(maxsize, typed, args, kwargs)\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\telse:\n\t\t\t\treturn functools.lru_cache(maxsize, typed)\n\n\t\twrapper.cacheable = True\n\t\treturn wrapper\n\treturn lfu_cache\n\n\n@lru_cache\ndef cached_cache_get(maxsize, typed, *args, **kwargs):\n\t\"\"\"\n\tCache of cached `lfu.cache.get()` implementations. When `typed` is\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tif typed:\t\ttype(maxsize).__name__ += '_typed_'\n\t\telse:\t\t\ttype(maxsize).__name__ += '_tiny_'\n\t\tdef _memoised_func(*args):   # Use default function's default implementations\n\t\t\treturn reduce(fn, args, 1)\n\t\treturn _memoised_func\t# The wrapper's cached result is called to reduce\n\t\t# LRU algorithms (when `maxsize` changes).\n\t\treturn lfu_cache\n\tif maxsize < 0:\n\t\tmaxsize = maxsize * 2**31 # Fix a bug if this is run by the optimized"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tdef inner_fn(*args, **kwds):\n\t\t\t\"A wrapper function for the cached function.\"\n\t\t\tcache_maxsize = maxsize\n\t\t\ttry:\n\t\t\t\tif typed:\n\t\t\t\t\treturn cache._cache[maxsize]\n\t\t\t\t# If no caching, we've made the call faster.\n\t\t\t\tcache.set(maxsize, *args, **kwds)\n\t\t\t\treturn cache.f(*args, **kwds)\n\t\t\texcept cache.cache_errors_in_or_timeout:\n\t\t\t\tcache.incr(maxsize, 1)\n\t\t\t\tcache.set(maxsize, *args, **kwds)\n\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tdef _(ll):\n\t\t\treturn memoize(ll, typed=typed)\n\t\treturn ngettext(lgettext(ll, l = maxsize), _(ll))(ll)\n\treturn _\n\n#\n# The following functions are mainly used to keep track of\n# how large a function is in the cache:\n#\n\ndef _get_lfu_maxsize(m):\t\"Compute the lfu cache size after a long delay.\"\n\tmaxsize = maxsize_mod(m)\n\treturn maxsize\n\n\n# Some notes about setting a maxsize to large, could take a while to\n#"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tclass Wrapped:\n\t\t\tdef __init__(self, method):\n\t\t\t\tself._cache = {}\n\t\t\t\tself._method = method\n\t\t\t\tself._cache_mutex = threading.Lock()\n\t\t\t\tself._cache_counter = 0\n\t\t\t\tself._maxsize = maxsize\n\n\t\t\tdef __call__(self, *args, **kwargs):\n\t\t\t\twith self._cache_mutex:  # avoid looping if multiple modifications are allowed\n\t\t\t\t\tif maxsize > self._maxsize:\n\t\t\t\t\t\treturn functools.wraps(self._cache[int(time.time() * maxsize)])(self, *args, **kwargs)"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\t@wraps(fn)\n\t\tdef cached(*args, **kwargs):\n\t\t\tresult = fn(*args, **kwargs)\n\t\t\tif typed and hasattr(result, 'dt'):\n\t\t\t\tresult = result.dt()\n\t\t\treturn result\n\n\t\treturn cached\n\treturn cached\n\n\n#------------------------------------------------------------------------------\n#~ def rlp(function):\n\n\t#~ rlp = lambda: Rlp(function)\n\t#~ rlp.__doc__ = function.__doc__\n\t\n\treturn ll\n\n\n#------------------------------------------------------------------------------\n#~ def lrl(function):\n\n\t#~ lrl = lambda: Lrl(function)\n\t#~ l"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\tdef memoizer(func_func):\n\t\t\t@functools.wraps(func_func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif \"maxsize\" in kwargs:\n\t\t\t\t\tkwargs[\"maxsize\"] = kwargs[\"maxsize\"] * 2**32\n\t\t\t\telif \"typed\" in kwargs:\n\t\t\t\t\tkwargs[\"typed\"] = kwargs[\"typed\"]\n\t\t\t\treturn func_func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn memoizer\n\treturn lfu_cache\n\ndef llvm_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function using"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\t@lru_cache(maxsize=_cache_maxsize, typed= typed)\n\t\tdef cachefunc(key)\t:\n\t\t\treturn _lfu_cache[key]\n\n\t\treturn cachefunc\n\n\tdef lfu(f, args, kwargs) -> \"LFU\":\n\t\tf, args, kwargs = list_to_tuple(f), list_to_tuple(args), list_to_tuple(kwargs)\n\n\t\tcachefunc = lfu_cache()\n\t\tcachefunc(*args, **kwargs)\n\t\treturn cachefunc\n\n\ndef lfu(function, *args, **"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\t@wraps(wrapped)\n\t\tdef decorate(*args, **kwargs):\n\t\t\t_maxsize = maxsize\n\t\t\twhile _maxsize > maxsize:\t\n\t\t\t\tsize = kwargs.pop(\"size\", _maxsize)\n\t\t\t\tif size <= 0:\n\t\t\t\t\treturn wrapped(*args, **kwargs)\n\t\t\t\t_data = kwargs.pop(\"data\", None)\n\t\t\t\tif _data is None or _typestr == \"bytes\":\n\t\t\t\t\tclen = _len(str(size))\n\t\t\t\t\t_data = bytearray(size)\n\t\t\t\t\t_data.extend(data)\n\t\t\t\telif size <= (_"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\t(key, value) = self._stack[-1]\n\t\tself._stack.append((key, value))\n\t\tself._stack.sort(key=self._stack.__getitem__)\n\t\tif isinstance(key, tuple):\n\t\t\treturn key[1], tuple(map(itemgetter(item), self._stack[1:]))\n\t\telse:\n\t\t\treturn key, value\n\n\nif is_jython:\n\tJYTHON_RE = '^<.*python?([_a-z0-9]*)>'\nelse:\n\tJYTHON_RE = '^(I?python?"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\twhile True:\n\t\t\tkey, item = self.items.pop()\n\n\t\t\tif key not in self.keymap:\n\t\t\t\tself.keymap[key] = []\n\n\t\t\tvalue = item[1]\n\n\t\t\tdel(self.keymap[key])\n\n\t\t\tself.items.insert(0, (key, value))\n\n\t\t\tif item[0]:\n\t\t\t\tself.keymap[key].append((key, value))\n\n\t\t\telse:\n\t\t\t\tdel(self.keymap[key])\n\t\t\t\tself.items.remove(item[0])\n\n\tdef __getitem__(self,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif self.stack and self.stack[-1][0] == self.key:\n\t\t\tself.stack[-1].pop(0)\n\t\t\treturn self.stack[-1][1], self.stack[-1][-1]\n\t\telse:\n\t\t\tassert 0, 'popitem() on stack not in queue'\n\t\n\tdef pop(self):\t\"\"\"Get the next and return value pair.\"\"\"\n\t\tn = self.stack.pop()\n\t\tif not self.stack:\n\t\t\traise KeyError('popitem() on stack not in queue')\n\t\treturn n, self.stack[-1]"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn self.items[self.index(self.value)]\n\ndef dict_representer(dumper, data):\n\timport copy\n\t# first convert dictionary key value pairs to their new-style dictionaries\n\tkeys = {}\n\tfor key, value in dumper.saved_items():\n\t\tkeys[key] = copy.deepcopy(value)\n\treturn keys\n\nclass DictDumper(yaml.SafeDumper):\n\t\"\"\"\n\tA simple yaml dumper that can handle OrderedDict-style keys.\n\t\"\"\"\n\n\tdef _represent(self):\n\t\tvalues = self."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\ttry:\n\t\t\tkey, value = self.data\n\t\t\tdel self.data[key]\n\t\t\treturn (key, value)\n\t\texcept KeyError:\n\t\t\treturn None\n\t\n\tdef popdict(self):\t\"\"\"\n\tRemove and return the same named `dict`.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.data[\"dict\"]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\tdef poplist(self):\t\"\"\"\n\tRemove and return the same `list`.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.data[\"list\"]\n\t\texcept KeyError:\n\t\t\treturn None\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\twith self._lock:\n\t\t\tresult = self._items.pop(0, None)\n\t\t\tif result is not None:\n\t\t\t\tself._items.insert(0, (key, result))\n\t\t\telse:\n\t\t\t\tself._items.append((self._parent_key, None))\n\n\tdef pop(self, *key):\n\t\t\"\"\"\n\t\tRemove the specified key from the queue, and first\n\t\treturned item, or None if no such item was found.\n\t\t\"\"\"\n\t\twith self._lock:\n\t\t\twith self._mutex:\n\t\t\t\tif key in self._keys_"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tself.item = self.item.pop()\n\treturn self.item\n\nclass QSortFilterProxy(QObject):\n\t\"\"\"Wrapper for `QFilterProxy` that automatically pushes a QEvent.\n\t\n\tThe `qsort` argument is a callable that can be used to push a QEvent at\n\tthe top level of a query. The object's `get_sort_field` method can be used\n\tto push a field into the top-level of the query (i.e, passing it a\n\t`QSortFilterProxy` instance).\n\t\n\tFor example::\n\t\t\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tself.tk.call((self._w, 'popitem')+('x',))\n\t\treturn self._getints(self.tk.call,'x')\n\tdef _set(self, key, item):\n\t\tif self.readonly:\n\t\t\traise ValueError,'readonly key %r' % key\n\t\tself.tk.call((self._w,'set', '', key, item))\n\tdef insert(self, index, item):\n\t\t\"\"\"\n\t\tInsert the given item at position item. Only supports item == None.\n\t\t\"\"\"\n\t\tself.tk.call(("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn self.dict.pop(self.key)\n\tdef pop(self, path, *args):\n\t\t\"\"\"\n\t\t\tRemove and return an argument for a specific item path or a list of lists.\n\t\t\"\"\"\n\t\tif path[0]!= '$':\n\t\t\treturn self.dict[path[1:]]\n\t\telse:\n\t\t\tretval = self.dict.pop(path[1:])\n\t\tif self.items:\n\t\t\tret = self.next()\n\t\telse:\n\t\t\tret = self.list.pop(path[1:], None)\n\t\tif not self.__"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tres = self._dict[self._map.pop(self._map.index(str(self), \"str\"))]\n\t\tself._map.remove(self._map.index(str(self), self._map.index(str(res))))\n\t\treturn res\n\n\tdef pop(self, key=-1):\t\"\"\"\n\tRemove the first and return the item at a key\n\n\t\tIf the argument is negative, the dictionary is cleared.\n\t\t\"\"\"\n\t\tval = self._dict.pop(str(self), key)\n\t\tself._map.remove(self._map."}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tself[key] = default\n\t\treturn default\n\tdef __setitem__(self, key, value):\n\t\tself.__setitem__(key, value)\n\n\tdef pop(self, path='', key=','):\n\t\ttry:\n\t\t\treturn self[key].pop(path, key)\n\t\texcept KeyError:\n\t\t\treturn []\n\n\tdef popitem(self):\n\t\ttry:\n\t\t\tx = self.pop()\n\t\t\tif x == []:\n\t\t\t\traise KeyError\n\t\t\treturn x\n\t\texcept:\n\t\t\traise KeyError("}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in D:\n\t\t\treturn D[key]\n\t\telse:\n\t\t\tD[key] = default\n\t\treturn default\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tD.__setitem__(i,y) <==> D[i]=y if i not in D, else y = y if a has key K, otherwise set d[K]=y\n\t\t\"\"\"\n\t\tif value not in D:\n\t\t\tD[key] = D[value]\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tD.__delitem__(i) <==> D"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\n\t\treturn self[key]\n\n\t# dict with elements without attribute\n\tdef __getattr__(self, item):\n\t\tif item not in self:\n\t\t\traise KeyError(\"Attribute %s not defined\" % item)\n\t\treturn self[item]\n\n\tdef __setattr__(self, key, value):\n\t\tif not key in self:\n\t\t\tself.create()\n\t\tself[key] = value\n\n\tdef __delattr__(self, key):\n\t\tif key in self:\n\t\t\tdel self[key]\n\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\treturn default\n\tdef __setitem__(self, key, value):\n\t\t#print \"Setting\", key, \"=\"\n\t\tif not isinstance(key, str):\n\t\t\traise ValueError, \"Invalid key type!\"\n\t\tself.__dict__[key] = value\n\tdef getstring(self):\n\t\t\"\"\" Return contents as plain string,\n\t\tas a native utf-8 string.\n\t\t\"\"\"\n\t\treturn self.getvalue()\n\tdef getlist(self):\n\t\t\"\"\" Return"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\t#if key not in D:\n\t\t#\tself[key] = default\n\t\tif key in self:\n\t\t\tself[key] = default\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"D.get(k[,d]) -> value\n\t\tReturns default if it doesn't exist (not handled)\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef items(self):\n\t\t\"\"\"D.items(): return a list of tuples (key,value) for the given C{self.cstr}; default"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tn = self.count(key)\n\t\tif n == 1:\n\t\t\tn = self.size()\n\t\telse:\n\t\t\td = self.data(key, default)\n\t\tself.__delitem__(n)\n\t\treturn d\n\t\t\n\tdef __setitem__(self, key, item):\n\t\tself[key] = item\n\n\tdef __contains__(self, key):\n\t\treturn key in self\n\t\n\tdef __getitem__(self, key):\n\t\treturn self[key]\n\n\tdef __add__(self, other):\n\t\tif not isinstance(other, Sequence):\n\t\t\tother"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef __delitem__(self, key):\n\t\tif key in self:\n\t\t\tdel self[key]\n\n\tdef __iter__(self):\t\treturn self.getters()\n\n\tdef __len__(self):\t\treturn len(self.getters())\n\n\tdef __getitem__(self, key):\n\t\tif key in self:\n\t\t\treturn self[key][0]\n\t\telse:\n\t\t\traise KeyError(key)\n\n\tdef __contains__(self, key):\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif default is None:\n\t\t\tdefault = dict.__setitem__(self, key, default)\n\t\telse:\n\t\t\tdefault[key] = default[key]\n\t\treturn default\n\tdef __getitem__(self, key):\n\t\treturn self.defaults.get(key)\n\tdef __setitem__(self, key, value):\n\t\tif key in self.defaults:\n\t\t\traise KeyError(\"duplicate key %r\" % key)\n\t\tself.defaults[key] = value\n\tdef __delitem__(self, key):\n\t\tif key in self.defaults:\n\t\t\tdel self."}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self: return self[key]\n\t\tself[key] = default\n\t\treturn default\n\t\n\tdef pop(self, key, *args):\n\t\t\"\"\"Returns and returns the most recently added value\n\t\tor default.'''\n\t\treturn self.get(key, *args)\n\t\n\tdef setdefault_default(self, key, default=None):\n\t\t\"\"\"Returns and sets a new value by calling setdefault(), default\n\t\twith a different default to get a new default value and\n\t\treturn existing default.\n\t\tAny error thrown by default is only raised for errors caused by\n\t\tan"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tD[key] = default\n\t\t\treturn default\n\t\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k,d), return and remove d and return key with value removed\n\t\tor KeyError will thrown if k not in D\n\t\t\"\"\"\n\t\ttry:\n\t\t\tk = d.popitem()\n\t\texcept KeyError:\n\t\t\tk = None\n\t\treturn k\n\t\n\tdef update(self, *args, **kwds):\n\t\t\"\"\"\n\t\tD.update(E, **F) ->"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._get[key] if self._has_key(key) else default\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tD.items(D) -> A [D1,D2,...] d with the supplied key.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._get[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tD.iterkeys() -> a b [D1,D2,...] d sorted in-place.\n\t\t\"\"\"\n\t\treturn iter(sorted(self._get."}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.D[key]\n\t\texcept KeyError:\n\t\t\ttry:\n\t\t\t\treturn getattr(self.D, key)\n\t\t\texcept AttributeError:\n\t\t\t\td = self.D[key] = default\n\t\t\t\tD[key] = d\n\t\t\t\treturn d\n\n\tdef copy(self):\n\t\treturn type(self)(self.A, self.B)\n\n\tdef setdefault(self, key, value):\n\t\tif key not in self:\n\t\t\tself[key] = value\n\n\tdef update(self, *args, **kwargs):\n\t\treturn self._"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._cache.get(key, default)\n\t\n\tdef set(self, key, val):\n\t\t\"\"\"\n\t\tD.set(k,v) := D[k]=v if k in D, else v. d will be overridden with the specified value.\n\t\t\"\"\"\n\t\tself._cache.set(key, val)\n\t\n\tdef toDict(self):\n\t\t\"\"\"Returns a dictionary representation of the dictionary.  If a default string is\n\t\tused for the dictionary, use.keys() or.values() (use.items() for a list).\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef has_key(self, key, default=None):\n\t\t\"\"\"has_key(k[,d]) -> True or False\n\t\t\n\t\tReturn True if D has key k; d defaults to None.\"\"\"\n\t\ttry:\n\t\t\treturn self[key] is not None\n\t\texcept KeyError:\n\t\t\treturn default\n\nclass _ThreadPool(object):\n\tdef __init__(self, nthreads):\n\t\tself.threads = []\n\t\tself.nthreads = nthreads\n\n\t\twith self."}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._cache[key]\n\t\texcept KeyError as e:\n\t\t\tif default is None:\n\t\t\t\treturn d\n\t\t\telse:\n\t\t\t\treturn default[key]\n\n\tdef getByKey(self, key, default=None):\t\t\t\"\"\"\n\tD.getByKey(key[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._cache[key]\n\t\texcept KeyError as e:\n\t\t\tif default is None:\n\t\t\t\treturn d\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__dict__[(key, d)]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef iterkeys(self):\n\t\t\"\"\"D.iterkeys() -> an iterator.\"\"\"\n\t\ttry:\n\t\t\tfor key, value in self.iteritems():\n\t\t\t\tyield key\n\t\texcept AttributeError:\n\t\t\tpass\n\n\tdef items(self):\n\t\t\"\"\"D.items() -> an iterator over (key, value) pairs.\"\"\"\n\t\tfor key in self.iterkeys():\n\t\t\tyield (key, self[key])\n\n\tdef values(self):\n\t\t\"\"\"D.values() ->"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif key in self.d:\n\t\t\treturn self.d[key]\n\t\telif default is None:\n\t\t\tval = self.getdefault(key)\n\t\t\tif val is None:\n\t\t\t\tval = default\n\t\t\treturn val\n\t\telse:\n\t\t\treturn default if key in self else self[key]\n\n\tdef __getitem__(self, key):\n\t\tif key in self.__dict__:\n\t\t\treturn self.__dict__[key]\n\t\telse:\n\t\t\traise KeyError, key\n\n# Mappings for the keys from a class attribute value to a set of\n# sets of properties"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self._db.get(self.__make_key(key), self._get_value_or_default(key), default)\n\tdef as_dict(self):\t\t\t\t\t\t\n\t\t\"\"\"D.as_dict() -> D.as_dict() -- Same as for.as_json().\"\"\"\t\t\n\t\treturn dict(self._db.as_dict().iteritems())\n\tdef as_json(self):\n\t\t\"\"\"D.as_json() -> D.as_json() -- Same as for.as_json().\"\"\"\t\t\n\t\treturn json.dumps(self.as_"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef items(self):\t\"\"\"Returns a list of all values for self.\"\"\"\n\t\treturn list(self.values())\n\n\tdef keys(self):\t\"\"\"Returns a list of all keys for self.\"\"\"\n\t\treturn list(self.keys())\n\n\tdef copy(self):\n\t\t\"\"\"Creates a copy of self.\"\"\"\n\t\treturn D()\n\n\tdef append(self, key, d):\n\t\t\"\"\"Inserts self[key] into self[key] if it does not exist.\"\"\"\n\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif key in d:\n\t\t\treturn d[key]\n\t\treturn default\n\n\tdef update(self, dct):\n\t\td = {}\n\t\tfor k,v in dct.items():\n\t\t\tkey = k.lower()\n\t\t\tif key in self.data:\n\t\t\t\tvalue = self.data[key]\n\t\t\t\tkey = self.cacheValue(key, value)\n\t\t\telse:\n\t\t\t\tif isinstance(value, list):\n\t\t\t\t\tvalue.extend([key, key])\n\t\t\t\telse:\n\t\t\t\t\tvalue = key\n\t\t\t\t\tkey = self.cacheKey(key)\n\t\t\t\tself"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif lock is None or lock.acquire(False):\n\t\t\t@wraps(key)\t# method.\n\t\t\tdef inner(self, *args, **kwargs):\n\t\t\t\treturn cache.get(key(self, args, kwargs) if callable(cache) else key)(self, *args, **kwargs)\n\t\t\treturn inner\n\t\treturn cache.get(key(self, args, kwargs))\n\n\tdef __call__(self):\n\t\t\"\"\"\n\t\tGet the current cached method or instance method on this class.\n\t\t\"\"\"\n\t\tcache = {}\n\t\tif self not in self._cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\treturn CacheClassDecorator(cache, key, lock)\n\ndef hashkey_c(*args):\n\t\"hash a string into hashed bytes.\"\n\treturn binascii.hexlify(struct.pack('!Q', *args).encode('utf-32'))\n\ndef _hash(f):\n\t\"\"\"Return new HashClass instance based on its arguments.\n\tGiven the arguments, returns the newly hashed bytes.\"\"\"\n\tnew_obj = HashClass()\n\tfor (k, v) in zip(sorted(f.__calls__.items()), args):\n\t\tnew_obj.set_with_prefix"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcls = cache.fget\n\t\tcache.keycache[key] = (cls, lock)\n\t\treturn cls\n\n\tdef _getCachedMethod(self, obj, method):\n\t\tcache = {\n\t\t\tn:fn(v, cache)\n\t\t\tfor n,v in self.cache.get(obj, {}).items()\n\t\t}\n\t\tcache.setdefault(method, method)\n\t\treturn cache[method]\n\n\tdef _setCachedMethod(self, obj, method):\n\t\ttry:\n\t\t\tobj.__self__.__self__.__methods__[method].__self__.cache[method]"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\t@wraps(key)\n\t\tdef cached_method(args, kwargs):\n\t\t\t\tif args is not args_new:\n\t\t\t\t\targs = args[:args_new]\n\t\t\t\t\targs._memoized = cache[key]\n\n\t\t\t\tcache[key] = callable(args, kwargs)\n\t\t\t\treturn args\n\n\t\t\tcached = cachedmethod(args, kwargs)\t# key not in args and kwargs is already callable\n\t\t\tif cache[key] is not callable:\n\t\t\t\tcache[key] = cache[key]\n\t\t\t\tcache[key] = getattr(args, argsname, None) # set cache["}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif lock is None:\n\t\t\t# create a new instance\n\t\t\tcache['__instance__'] = get_instance(key)\n\t\telse:\n\t\t\t# cache a copy of the lock\n\t\t\tcache['__instance__'] = lock.get(key)\n\t\tcache.__init__(self)\n\t\tself.key = key\n\t\treturn cache\n\n\tdef __call__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tDecorator to save results in a cache.\t\"\"\"\n\t\treturn self.cache.get(self.key, *args, **kwargs)\n\n\tdef __repr__(self):\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcache.add(key)\n\t\treturn __cachedmethod(key, lock)\t\t\n\ndef check(cache):\n\t\"\"\"Check if a cache has been completely loaded.\n\tIf it should reload and cache is still open\n\t\"\"\"\t\n\tpass\n\ndef opencache(file_or_path):\n\t\"\"\"Opens a file on disk and caches it.\"\"\"\n\treturn open(os.path.abspath(file_or_path), 'r')\n\ndef loadfile(file_or_path):\n\t\"\"\"Loads a file from the cache and writes its content to file"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcacheFunc = lambda: cache(*args, **kwargs)\n\t\tdef decfunc(f):\n\t\t\tif lock is None:\n\t\t\t\tlock = Lock()\n\t\t\tif key not in cacheFunc.cache:\n\t\t\t\tcacheFunc.cache[key] = result = cacheFunc()\n\t\t\tresult.cache[key] = f\n\t\t\treturn result\n\t\treturn decfunc\n\treturn cachedmethod\n\n\n# Returns True for caching functions that need to be recomputed in\n# a decorator (see the docstring of the docstring for more details).\n_re_computed_or_not = _cachedMethodCache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\treturn cache.get(key, None), cache.get(lock, None)\n\treturn cachedmethod\n\nclass SingletonCache(object):\n\t\n\t@property\n\tdef func(self):\n\t\tif hasattr(self, 'cache'):\n\t\t\treturn self.cache.func\n\t\tclass M(type):\n\t\t\t\"\"\"\n\t\t\tThis metaclass sets methods for the given\n\t\t\tcls, allowing the class to act as a normal cache.\n\t\t\t\"\"\"\n\t\t\tpass\n\n\t\tself.cache._cache = M\n\t\tsetattr(self.cls, '__call__', self)\n\t\treturn self\n\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif self._caching_key is not None:\n\t\t\tif lock is None:\n\t\t\t\tcache().clear()\n\t\t\telse:\n\t\t\t\tcache()[self._caching_key] = cache[key]\n\nclass CacheMixin:\n\n\t@classmethod\n\tdef cache_cls(cls, key, callback=None):\n\t\t\"\"\"\n\t\t@param callback: a callable whose result is an argument\n\t\t(i.e. returns a cached result) or the result cache object\n\t\t(i.e. does not make sense to cache a function), or an instance method\n\t\t(i.e."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = classobj._cacheLocked\n\t\t# make sure the cached instance is not resurrected.\n\t\tif sys.modules.get(key) == realcache.module:\n\t\t\tclassobj._cacheLocked = cache._getFunctionContext(key)\n\t\t\tretval = realcache.decorator(cache, cls=classobj._cacheValidated, lock=lock)\n\t\telse:\n\t\t\tcache.cache.add(key, lock)\n\t\t\tretval = realcache.decorator(cache, cls=classobj._cacheValidated, lock=lock)\n\t\t"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tout = \"{:30}  = {:7.4f}\".format(e.__class__.__name__, len(e.__traceback__) )\n\t\tif max_level is not 5:\n\t\t\tout += \"\\n** at {}\\n**\".format(max_level)\n\t\tif max_path_level is not 5:\n\t\t\tout += \"\\n** at {}\\n**\".format(max_path_level)\n\t\treturn out\n\n\tdef _show_exception(self):\n\t\t\"\"\"\n\t\tFormat exception in output: \"type 1:  type 2:\"\n\t\t:return:\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif max_level==20:\n\t\t\tmax_path_level += 10\n\t\tfor path_level in range(1, max_level, max_path_level):\n\t\t\tname = str(e)\n\t\t\tif name.endswith('.'+''):\n\t\t\t\tname = (name[:-1] + '.') + name[-1]\n\t\t\t\n\t\t\tnew_name = name + str(max_level)\n\t\t\tif not os.path.exists(new_name):\n\t\t\t\tos.makedirs(new_name)\n\t\t\t\te.message = 'Created new exception: %s : %s' %"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\nclass SlowCallIterator:\n\tdef __init__(self, func):\n\t\tself.func = func\n\t\n\tdef __iter__(self):\n\t\treturn iter(self.func())\n\n\tdef next(self):\n\t\te1 = current_excep.value.e\n\t\te2 = current_excep.value.e2\n\n\t\tif e1:\n\t\t\t# Convert to excep value\n\t\t\tval = e2.value\n\t\t\ttry:\n\t\t\t\tint(val)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Add exception to the stack."}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\t# Try to use the appropriate format for this as a function.\n\t\treturn u(u''.join(traceback.format_exception(e.__class__, e, max_level, \\\n\t\t\t\t\tmax_path_level, e.__traceback__)))\n\n\ndef _unicode_repr(e):\n\treturn u(u(str(e)))\n\n\ndef _unicode_format(e):\n\treturn u(u(u(str(e)))\n\n\nclass PyShellLexer(RegexLexer):\n\t\"\"\"\n\tProvides a `PyShell <https://www.pyshell.org>`__ (not"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif len(e) == 5:\n\t\t\treturn 'exception %d: %s' % (e[0], e[2])\n\t\telse:\n\t\t\treturn str(e)\n#!/usr/bin/env python\n\n# Copyright (c) 2015, Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport base64\nimport logging\nimport os\nimport re\nimport subprocess\nimport sys\nimport tempfile\nimport urllib\n\nSCRIPT_DIR = os.path."}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n# Import python libs\nimport logging\nimport traceback\nimport traceback_trace\n\nfrom time import time\nimport uuid\n\n# Import local libs\nfrom mycroft.helpers import debug\nfrom mycroft.helpers.exceptions import ExceptionWrapper\n\n\n# Import remote libs\nfrom mycroft.helpers.constants import *\nfrom mycroft.logging.logger import LOG\nfrom mycroft.utils.timer import Timer\nfrom mycroft.utils.errors import CustomExceptions\n\n# Get the logger object\ngetLogger_log = logging.getLogger(__name__)\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tif e.__class__.__name__ == 'Exception':\n\t\t\treturn str(e)\n\t\treturn os.linesep.join(traceback.format_exception(e.__class__, e))\n\n#----------------------------------------------------------------------------\n# Return the class object. This will be used as the __init__ function.\n# @return This class.\n#----------------------------------------------------------------------------\ndef init(name=None):\n\t\"\"\"\n\tInitialize the manager.\n\n\t:param name: The class name. If set, then the instance name will be used.\n\t:type name: str\n\t:return The class object.\n\t\"\"\""}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\nclass StackException(Exception):\n\tpass\n\nclass Stack:\n\t\"\"\"\n\tGeneric stack.\n\tFor more info, see the documentation at :class:`Stack`.\n\t\"\"\"\n\n\tclass Slow(object):\n\t\t\"\"\"\n\t\tA class that implements a minimal stack method for storing high\n        level functions, and calling that class repeatedly (slow\n        to collect).\n\t\t\"\"\"\n\n\t\tdef __init(self):\n\t\t\t\"\"\"\n\t\t\tInitialize self.high_level() to make stack functions more obvious.\n\t\t\t\"\"\"\n\n\t\t\tself.__high_level = 0\n\n\t\t\tself.__result ="}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\n\tclass InvalidStack(Exception):\n\t\tpass\n\t\n\t# Handle special cases with the stack\n\t# and the log level for logging.\n\tif (isinstance(e, StackError) and isinstance(max_level, int)) or (isinstance(max_level, int)) and ((not isinstance(max_path_level, int) and len(max_level) == 0) or (max_level >= max_path_level)):\n\t\tfmt ='stack of {} should only be at end of stack of {} levels\\n'.format(max_level, max_path_level)"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn str(e)\nimport numpy as np\nimport theano\nimport pycuda.autoinit  # NOQA\nimport time\nimport cv2\nimport logging\nfrom multiprocessing import Pool\nimport timeit\nimport Queue\nfrom functools import partial\nimport cv2.cv as cv\nfrom utils import misc\nfrom multiprocessing.cpu_count import cpu_count\nfrom time import time, localtime\nfrom random import randint\n\n\nclass KMeans:\n    \"\"\" This class predicts k-means scores for a batch of words and computes cross-validation for those.\n        Internally, this is a wrapper around"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tself.write_file(file_name, text_buffer, encoding, overwrite=overwrite)\n\t\treturn 0\n\n\nclass RfParser(object):\n    \t\"\"\"An object used to parse the file.\"\"\"\n    \tdef __init__(self, file=None, encoding='utf-8', errors='replace'):\n\n\t\t# Set the text_buffer\n\t\tself.text_buffer = False\n\t\t\n\t\t# Parse one buffer\n\t\tif type(file) in string_types:\n\t\t\tfile_name = file\n\n\t\tfile = file_path(file_name)\n\n\t\tif file:\n\t\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif overwrite:\n\t\t\tif os.path.basename(file_name)!= \"tmp_file\" or copy(os.path.join(file_name, \"tmp_file\"))!= \"tmp_file\":\n\t\t\t\tif mode: msg = \"Error: filename already exists!\"\n\t\t\t\telif sys.platform not in (\"os2\", \"win32\"):\n\t\t\t\t\tmsg = \"Error: %s is not supported on this platform\" % sys.platform\n\t\t\t\t\tlogger.debug(msg)\n\t\t\t\t\tlogger.error(msg)\n\t\t\t\t\tlogger.info(\"Please set ENCODING for local file in output"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tassert file_name is not None,\\\n\t\t'Please specify a file name'\n\t\ttry:\n\t\t\tf = open(file_name, encoding=encoding)\n\t\t\ttext_buffer = f.read()\n\t\t\tf.close()\n\t\t\ttext_buffer = text_buffer.replace('\\n', '')\n\t\t\tif text_buffer:\n\t\t\t\ttotal_written_bytes = binary_len(text_buffer) + binary_len(text_buffer)\n\t\t\t\ttotal_read_bytes = binary_len(text_buffer) + binary_len(text_buffer[:total_written_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\t\n\tif text_buffer == \"\": text_buffer = \"\"\n\tif encoding and not overwrite:\n\t\timport codecs\n\t\tfh = codecs.open(codec_dir + \"/text_buffer\" if encoding else codec_dir + \"/text_writer\", text_buffer, 'w', encoding)\n\t\tfh.write(text_buffer)\n\t\tfh.close()\n\t\t\n\t\treturn 0\n\ndef add_text_to_file_to_index(file_name, text_buffer, index, encoding, overwrite=False):\t\"\"\"\n\tAppend text to the specified file, the provided binary"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\twith open(file_name, 'a', encoding=encoding) as f:\n\t\t\tf.write(text_buffer)\t\t# write to file.\n\t\n\tif overwrite:\n\t\treturn write_file(file_name)\n\n\tif text_buffer:\n\t\treturn write_text_to_file(file_name, text_buffer, encoding)\n\n\ndef write_text_to_file(file_name, text_buffer, encoding = None):\n\t\"\"\"\n\tWrite the given text buffer to a file.\n\tEncoding is set on the first run,"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\t#file path\n\t\tfile_path = find_and_write_file(file_name)\t# path to file\n\t\tif file_path == None:\n\t\t\treturn -1\n\n\t\t#open file\n\t\tfile_write_file(file_path, text_buffer, encoding, overwrite)\t\t# write to buffer\n\t\treturn file_path\n\n\t#check if file is already there\n\tfile_name = os.path.basename(file_name)\n\tif not os.path.exists(file_path):\n\t\t#open file\n\t\tfile_write_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_object = open(file_name, \"wb\")\n\t\tfile_object.write(text_buffer)\n\t\tfile_object.close()\n\n\t\tif overwrite:\n\t\t\tos.remove(file_name)\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\n\t# Check to see that the file hasn't been modified anymore\n\t#\n\t# NOTE: If the file isn't open, but it is a file then a new one will be\n\t# made to be read instead. This is a sad.  This function is not tested for\n\t# Windows platforms"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn self.rtmcli.write_to_stream(\"Error: Cannot locate file: \" + file_name + str(errno.ENOENT), \"Error: Not found. Aborting file creation\")\n\n\t\tlines = text_buffer.splitlines()\n\t\tnew_file = f = str()\n\t\tfor line in lines:\n\t\t\tif line.startswith(\"@\"):\n\t\t\t\tnew_file += line\n\t\t\t\tcontinue\n\n\t\t\t#print line.rstrip()\n\t\t\tif len(line) == 0:\n\t\t\t\tnew_file += \"\\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\treturn 0\n\ndef append_text(fptr, text, encoding=\"utf-8\", overwrite=False):\n\t\"\"\"\n\tWrite text (bytes or str) to the file-like object\n\t\"\"\"\n\t\ttry:\n\t\t\tfptr.writelines(text)\n\t\t\tfptr.write('\\n')\n\t\t\tfptr.writelines(text.encode(encoding, 'xmlcharrefreplace') + '\\n')\n\t\t\tfptr.write('\\n' + text.encode(encoding, 'xmlcharrefreplace') + '\\n')\n\t\texcept UnicodeEncodeError:\n\t\t\tmsg = \"Encoding is either"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_name_tmp = os.path.join(config.TEMP_DIRECTORY, file_name.rsplit('.', 1)[0])\n\t\tfile_tmp_tmp = os.path.join(config.TEMP_DIRECTORY, file_name_tmp)\n\t\tif overwrite or os.path.exists(file_tmp_tmp):\n\t\t\treturn os.stat(file_tmp_tmp)[6]\n\t\telse:\n\t\t\ttry:\n\t\t\t\tif not os.path.isfile(file_name):\n\t\t\t\t\traise FileNotFoundError(\"%s not found\" % file_name)\n\n\t\t\t\ttry:"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\twith io.open(file_name, encoding=encoding) as f:\n\t\t\treturn f.read()\n\n\nclass Input(MutableSequence):\n\tdef __init__(self, file_path):\n\t\tassert os.path.isfile(file_path)\n\t\tself._file = file_path\n\t\tself._read_buffer = file_to_textbuffer(file_path, os.linesep)  # Well, this is the buffer to read from.\n\n\tdef __len__(self):\n\t\treturn self._read_buffer.tell()\n\n\tdef __bool__(self"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ndef _is_bytes_data(data: List[bytes]) -> bool:\n\t\"\"\"\n\tCheck if the given list is a list of bytes in buffer. If not, return False.\n\t:param data: List of bytes to check for non-bundled strings, in buffer.\n\t:type data: list[bytes]\n\t\n\t:return: Success, the list is a list of bytes not in buffer.\n\t:rtype str\n\t\"\"\"\n\treturn isinstance(data, bytes)\n\n# Read a text buffer from memory or a list of buffer contents\ndef _read"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# First let's try to work out which encoding is the default. Doing that means\n\t\t# that this should give reasonable results in the future.\n\t\ttry:\n\t\t\tstream = open(file_name, 'w', encoding=encoding)\n\t\texcept IOError as ex:\n\t\t\traise Exception(\"Error opening: {0}\".format(str(ex)))\n\n\t\t# Read the whole file and then return\n\t\ttry:\n\t\t\tret = stream.read(1000)\n\t\t\treturn ret\n\t\texcept:\n\t\t\t# Reading the entire file is not thread safe and is not quite reasonable.\n\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\t\n\tif not unicode(file_name, encoding).isnumeric():\n\t\t# This is an invalid character; just ignore.\n\t\treturn None\n\n\tfilename = os.path.abspath(file_name)\n\tfp = None\n\t\n\ttry:\n\t\tfp = open(filename, 'rb')\n\texcept IOError, err:\n\t\traise Exception('%s: %s'%(sys.exc_info()[0], err))\n\n\tlines = []\n\t\n\tstart = None\n\tend = fp.tell()\n\t\n\twhile start!= end:\n\t\tline = fp.readline()\n\t\tif"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\twith codecs.open(file_name, 'r', encoding) as file_obj:\n\t\t\tbuffer = file_obj.read()\t\t# TODO: Write some safety checks\n\t\t\treturn buffer\t\t# TODO: Add in error handling\n\t\treturn None\n\n# TODO: Support more than one encoding\nclass TextBuffer(object):\n\t\n\tdef __init__(self, data, lang = ''):\t\t# TODO: Add more details about the supported language.\n\t\tself.data = data\n\t\tself.lang = lang\n\t\t\n\t\tself.length = None\n\t\tself.text_length ="}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# Load an existing buffer from the file.\n\t\tif os.path.isfile(file_name):\n\t\t\tfile_contents = open(file_name, \"rb\")\n\t\telse:\t\t# File is not an existing file\n\t\t\tpass\n\n\t\tif encoding:\n\t\t\tfile_contents = file_contents.decode(encoding)\n\n\t\treturn file_contents\n\n\nclass TextBuffer(object):\n\t\"\"\" Represents a text buffer on the fly.\n\"\"\"\n\n\t_buffer = b\"\"\n\tbuffer_size = 1024\n\tlength = 1024\n\n\tdef __init__(self, buffer"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\t\n\ttry:\n\t\tfd = io.open(file_name, encoding=encoding, errors='ignore')\n\t\treturn fd.read()\n\texcept Exception as e:\n\t\tfrom django.core.exceptions import ObjectDoesNotExist\n\t\traise ObjectDoesNotExist('Could not find file \"' + file_name + '\" in your settings file. Please add it in the settings file to read its contents.')\n\t\ndef get_file_system_encoding(file_name, force):\n\t\"\"\"\n\tGet file system encoding.\n\tCAUTION : This will read the entire file IN MEMORY.\n:param file_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\t\n\tbuffer = codecs.open(file_name, encoding=encoding)\n\n\t# Return the text\n\treturn buffer.read()\n\n\n# Starts a string buffer to read a text buffer from file.\ndef read_textbuffer(file_name, encoding):\n\t'''\n\tRead a text buffer from file and return a text object.\n\t'''\n\t\n\tbuffer = codecs.open(file_name, encoding=encoding)\n\n\t# Return the text\n\treturn buffer.read()\n\n\n# Initializes the text buffer.\ndef text_bufferInit(buffer_name, encoding"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\treturn file_bytes_to_textbuffer(file_name, encoding)\n\t\n\ndef find_file(file_path, file_content_type, directory):\n\t\"\"\"\n\tGet the content associated with the file represented by the given directory.\n\t\n\t:param file_path: The file path to read.\n\t:type file_path: str\n\t:param file_content_type: What content type the file should contain.\n\t:type file_content_type: str\n\t:param directory: Directory for the file.\n\t:type directory: str\n\t:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tif encoding == 'UTF-8':\n\t\t\t# UTF-8 file.\n\t\t\tif os.path.exists(file_name) and os.access(file_name, os.F_OK):\n\t\t\t\tfile = codecs.open(file_name, 'r', encoding)\n\t\t\t\ttry:\n\t\t\t\t\ttext = file.read()\n\t\t\t\tfinally:\n\t\t\t\t\tfile.close()\n\t\telse:\n\t\t\t# File-like object.\n\t\t\tfile = file_to_bytes()\t\n\t\t\twhile not file.closed:\n\t\t\t\ttext += file.read()\n\t\t\tfile.close"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif os.path.isfile(file_name):\t# Check whether file exists.\n\t    return True\n\telse:\n\t\treturn False\n\ndef append_newline(file_name, newline): \t# Append file to file\n\tos.fsync(file_name)\n\tfile_name = \"%s/newline_%u.txt\"% (settings.SITE_DIRECTORY, file_name)\n\twrite_string(file_name, newline, settings.LINE_COMMENT_MARK)\n\ndef check_if_file_exist(file_name):   # Check if file exists"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\n\t\tif os.path.isfile(file_name):\n\n\t\t\treturn True\n\n\t\treturn False\n\texcept IOError:\n\t\treturn False\n\n\ndef check_new_file_in_location(file_name, file_in_name):\n\t\"\"\"\n\tCheck if the file 'file_in_name' exists. If file does not exist create it.\n\t:param file_name: File name.\n:type file_name: str\n:param file_in_name: File name to test in directory.\n:type file_in_name: str\n:return:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\tos.stat(file_name)\n\t\texcept OSError:\n\t\t\treturn False\n\t\treturn True\n\n\ttry:\n\t\tos.makedirs(file_name, exist_ok=True)\n\texcept OSError:\n\t\treturn False\n\texcept OSError:\n\t\treturn False\n\t\n\treturn True\n\nif __name__ == \"__main__\":\n\n\tprint(\"File name: \" + files.get_file_name())\n\n\t# Create log folder and extract all the names from it\n\tlog_folder = files.get_log_path(files.get_file_"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ndef is_dir_and_file_exist(directory, file_name, allow_file = False):\n\t\"\"\"\n\tCheck if directory name and file is exist.\n\t:param directory: Directory name.\n\t:type directory: str\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param allow_file: Whether allow file paths or remove path (not directory) (not file).\n\t:type allow_file: bool\n\t:return: Return true (path exist, file exist, or is directory)\n\t\t@rtype"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif not os.path.exists(file_name):\t# Invalid file name\n\t\t\treturn False\n\t\t# We assume we are still on the file, but in case of invalid file name\n\t\t# we throw an error if the file does not exist.\n\t\t# TODO: Check if the file exists and not a valid file.\n\t\treturn file_name in file_dir\n\n\tdef open_file(file_name, mode='r'):\n\t\t\"\"\"Open a file and return it as string\"\"\"\n\t\tif file_name == '':\n\t\t\treturn None\n\n\t\tfd = open(file_"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\tstat.S_ISDIR(file_name)\n\t\texcept:\n\t\t\treturn False\n\n\tdef is_valid_directory(directory_name):\n\t\t\"\"\"\n\t\tCheck if directory exists in the given directory.\n\t\t:param directory_name: Directory name.\n\t\t:type directory_name: str\n\t\t:return: Return true/false and error.\n\t\t:rtype: bool\n\t\t\"\"\"\n\t\ttry:\n\t\t\tstat.S_ISREG(directory_name)\n\t\texcept:\n\t\t\treturn False\n\n\t\tif not directory_name:\n\t\t\treturn"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tfile_path = os.path.normpath(file_name)\n\t\tif os.path.exists(file_path) == False:\n\t\t\treturn False;\n\t\tif os.path.isfile(file_path) == True:\n\t\t\treturn True;\n\t\treturn False;\n\n\tdef file_exists(file_name):\n\t\tif os.path.exists(file_name) == True:\n\t\t\treturn True;\n\t\treturn False;\n\n\tdef file_list(file_name):\n\t\tif os.path.isfile(file_name) == True:\n\t\t\t"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\t# Check if the path exists\n\ttry:\n\t\t\tos.path.isfile(file_name)\n\t\texcept AttributeError:\n\t\t\treturn False\n\n\t\treturn True\n\t\n\t# Check if file exists and file extension is not \".ext\".\n\t# Returns true if path exists, false otherwise\n\tdef file_exist_ext(file_name):\n\t\t\n\t\t# Check if file exist\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn False\n\n\t\treturn file_name in { '.ext', '.inf', '.in' }\n\t\n\t# List of all subdirectories of files"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\twith open(file_name, 'r') as f:\t\tdata = f.readlines()\n\t\t\treturn True\n\t\texcept IOError:\n\t\t\treturn False##############################################################################\n#\n# Copyright (c) 2003 Zope Corporation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING,"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\ttry:\n\t\t\twith open(file_name, mode='r') as f:\n\t\t\t\treturn True\n\t\texcept IOError:\n\t\t\treturn False \t\n\n\tdef check_file_exist(self, file_name):\t\"\"\"\n\tCheck if file exists.\n:param file_name: Name of file.\n:type file_name: str\n:return: Return true, if file exists.\n:rtype str\n\"\"\"\n\t\tif exists(file_name):\n\t\t\treturn True\t\t\n\t\t\n\tdef get_file_contents(self, file_name):\t\"\"\"\n\tGet file"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlog.removeHandler(cls)\n\t\treturn cls\n\tdef log(self, str, level):\n\t\tif not DEBUG:\n\t\t\tfrom logger import log\n\t\t\tlog.addHandler(self)\n\t\tlog.log(self, str, level)\nclass _RotatingFileHandler(_RotatingHandler):\n\tdef close(self):\n\t\tpass\nclass _StreamRotatingFileHandler(RotatingFileHandler, _RotatingHandler):\n\tdef __init__(self, fp, maxBytes=8192):\n\t\t_RotatingHandler.__init__(self, fp, 0)    \t#MFh"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.logger.setLevel(logging.INFO)\n\t\treturn\n\n\t@staticmethod\n\tdef set_default_logger_level(level):\n\t\t\"\"\"Set logger level\"\"\"\n\t\tcls.logger.setLevel(level)\n\n\t@classmethod\n\tdef getLogger(cls, name):\n\t\t\"\"\"\n\t\tGet a logger from the logger_name attribute\n\n\t\t@param name: Logger_name\n\t\t@return: Logger\n\t\t\"\"\"\n\t\tif hasattr(logging, name):\n\t\t\tlogger = logging.getLogger(name)\n\t\t\tif isinstance(logger, Logger):\n\t\t\t\treturn logger\n\t\t\telse"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.logger = logging.getLogger(__name__)\n\t\tcls.logger.addHandler(logging.NullHandler())\n\t\tcls.logger.setLevel(logging.INFO)\n\n\t\ttry:\n\t\t\tcls.logger.propagate = True\n\t\t\tlog_file = os.path.join(settings.get_instance().settings_folder,'main/index.log')\n\t\t\twith open(log_file, 'w') as log:\n\t\t\t\tlog.write('Starting at %s' % (datetime.datetime.now()))\n\n\t\texcept Exception as e:\n\t\t\tlogger.error(\"Error encountered for"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\ttry:\n\t\t\tcls.get_logger().debug('Resetting logging level')\n\t\t\tcls.set_level(logging.getLogger().getLevelName(logging.INFO))\n\t\t\tif not hasattr(cls,'set_level'):\n\t\t\t\tcls.set_level(logging.NOTSET) # Will be set to NOTSET\n\t\texcept:\n\t\t\tpass\n\ndef _init_parser():\n\ttry:\n\t\tconf.initialize('debug', log_level=conf.get_loglevel())\n\texcept:\n\t\tpass\n\t#log config\n\tlog_path = conf."}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\ttry:\n\t\t\tlogger = logging.getLogger('rugo.backend.plugin')\n\t\t\tlogger.removeHandler(logging.NullHandler())\n\t\texcept Exception:\n\t\t\tlogger = logging.getLogger('rugo.backend.plugin')\n\t\t\tlogger.addHandler(NullHandler()) \n\ndef _load(data, config):\n\t\"\"\"\n\tLoad a new backend data\n\t@param data dictionary\n\t@param config settings object\n\t@return tuple (backend, logger) as read from config\n\t\"\"\"\n\ttry:\n\t\tfrom plugin_factory import PluginFactory\n\t\timport p"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.enabled = False\n\t\tfor k in cls.log_list:\n\t\t\tlogger.info(\"%s: %s\", k, cls.log_list[k])\n\n\t\twith io.open(cls.log_file_name, 'w') as log:\n\t\t\tjson.dump(cls.log_file_handler, log, indent=4)\n\n\t@staticmethod\n\tdef _get_file_handler(**kwargs):\n\t\t\"\"\"\n\t\tLoad the logger from kwargs dictionary and\n\t\treturn the logger\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn cls._logger.get(**"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\t#log.removeHandler().addHandler(logger.ConsoleHandler)\n\t\t#log.removeHandler().addHandler(logger.NullHandler)\n\t\t#log.removeHandler().addHandler(os.StreamHandler)\n\t\t#log.removeHandler().setLevel(logging.DEBUG)\n\t@classmethod\n\tdef setLogging(cls, logging=None):\n\t\tif logging!= None:\n\t\t\tcls.logger = logging\n\t\tsuper(_ResetLogging, cls).setLogging(logging)\n\n\n\nclass _FileLogger(logging.Logger):\n\t\"\"\"Logger for file log\"\"\"\n\tdef __init__(self,"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._stdout = sys.stdout = sys.stderr = StringIO()\n\t\tsys.stderr = _stdout = cls._stderr = StringIO(\"stdout\\n\")\n\t\ttry:\n\t\t\tsys.stdout.flush()\n\t\t\tsys.stderr.flush()\n\t\texcept IOError:\n\t\t\tpass\n\tfrom functools import partial\n\t__all__.append(\"Log\")\n\ndef _format_error(*msg):\n\t\"\"\"\n\tFormat the given error message and then returns\n\tthe message and argument\n\n\tArgs:\n\t\tmsg: the formatted error message to be formatted\n\t\traise: (str)"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tif DEBUG_MODE:\n\t\t\t_init_logging()\n\n\tdef __init__(self, logfile=None):\n\t\t\"\"\"\n\t\tConstructor:\n\n\t\tReads the configuration stored in cfg file.\n\t\tIt is important to call init() with the config files provided\n\t\tby the caller.\n\n\t\tKeyword Arguments:\n\t\tlogfile (string) -- specifies the path where to save the configuration\n\t\t\tfiles.\n\t\t\"\"\"\n\t\twith open(cfg_file, \"r\") as config_file:\n\t\t\tif DEBUG_MODE:\n\t\t\t\tprint(\"read config file at {0}\".format(cfg_"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls._logging = []\n\t\tcls._logging.pop(0)\n\nclass TouchFile(object):\n\t\"\"\"\n\tA class representing a touch file. Each touch file handles several actions\n\t(e.g. the update or unlock). One file holds a state property holding information for\n\tone touch.\n\t\"\"\"\n\n\tdef __init__(self, path, data=None):\t\t\n\t\t\"\"\"\n\t\tpath              \tPath to this touch file (required)\n\t\tdata           \tData to process (optional)\n\t\t\"\"\"\n\t\tself.path = os.path."}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tif 'Py' in platform.architecture()[0]:\n\t\t\treturn 'Fallback'\n\t\tfor tp in self.implementations:\n\t\t\tif tp.getTypeName() == 'Py':\n\t\t\t\ttp = tp.getModule()\n\t\t\t\tif tp.getDeprecationContext().isAPI():\n\t\t\t\t\treturn tp.getDeprecationContext().getClassType()\n\t\t\t\tbreak\n\n\t\tself.implementations.append(self)\n\t\treturn 'Fallback'\n\nclass Enum(EnumBase):\n\t\"\"\"\n\tA python type for each enum that holds its name,\n\tsuch as \"enum: name\"."}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn _pyMethodObject(self.getTargetClass())\nBuildLibrary = _distutilsComponent.BuildLibrary\n\n\n\ndef _initPyMethodObject():\n\t\"\"\"\n\tPerform any initialization needed for building a new,\n\tmodified implementation of \"PyMethodObject\" (such as distutils.version.IronPython).\n\n\tA build of \"PyMethodObject\" can be inserted into the python path (a\n\ttemporarily constructed path relative to the calling\n\tdistutils.setup). In that case the __init__ method of\n\tPythonPackage (where it appears) will be executed in the\n\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tif hasattr(self, \"implementation\"):\n\t\t\treturn self.implementation()\n\t\telse:\n\t\t\treturn self.__class__.__name__\n\tclass Target():\n\t\t\"\"\"Base class that encapsulates all the user-provided\n\tinterfaces such as `wx.ScrolledWindow`. A target can\n\timplement the operation using the `wx.Sizer` interface,\n\tby default `wx.GridSizer` supports a `Sizer` interface that\n\tis used as an attribute to control the behavior of the target. For\n\texample, in a wxPython application you are defined with a single\n\tsizer for the"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn None\n\n\tdef getTarget(self, name):\n\t\t\"\"\"\n\t\tReturn a target (self or subclass) of the given name, as\n\t\ta string.\n\t\t\"\"\"\n\t\treturn self.getTargetClass(name)\n\n\tdef getTargetClass(self, name):\n\t\t\"\"\"\n\t\tReturn the implementation class (self) for the given name,\n\t\teither directly or indirectly.\n\t\t\"\"\"\n\t\tfor base in ('PyWrapper', 'PyMethodWrapper', 'PyClassMethodWrapper',\n\t\t\t\t\t'PyClassModuleWrapper'):\n\t\t\ttarget = getattr(self, base)"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\traise NotImplementedError\n\n\n\t#\n\t# Internal implementation methods\n\t#\n\tdef __init__(self, name):\n\t\tself.__name__ = name\n\t\tself._implementation = None\t# will be set later\n\t\tself.__baseType__ = self.__name__ + \"_Implementation\"\n\n\n\t#\n\t# Create one instance of the class\n\t#\n\n\tdef __getSubclass(self):\n\t\t\"\"\"\n\t\tCreate an Abstract subclass of this type which overrides __name__'s method.\n\t\tReturns None if there isn't one.\n\t\t\"\"\"\n\t\tif self.__name__ == \"_"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\traise NotImplementedError('Py/fallback is not yet implemented.')\n\t\n\tdef target(self):\n\t\t\"\"\"\n\t\tInitialize a Target object, including the\n\t\t'self.' attribute.\t\"\"\"\n\t\traise NotImplementedError('Py/fallback must be implemented.')\n\n\tdef getContext(self):\n\t\t\"\"\"\n\t\tGet the underlying Py Context.\t\n\t\tThis is used by PseudoContext.getContext() that only returns a\n\t\tnew object when necessary.\n\t\t\"\"\"\n\t\traise NotImplementedError('Py/fallback must be implemented.')\n\n\tdef getPythonInterpreter(self):\n\t\t\"\"\"\n\t\tGet the"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tcls = types.ClassType\n\t\ttry:\n\t\t\treturn cls.__module__\n\t\texcept:\n\t\t\tpass # no py2c\n\n\nclass _Fallback(object):\n\t\"\"\"\n\tA fallback object in case your code has a problem which is\n\tthe only thing you want to use, this will be the default\n\t\"\"\"\n\tdef getSourceLocation(self):\n\t\t\"\"\"\n\t\tGet the source location for this implementation.\n\n\t\tIf you override this behavior, it overrides the\n\t\timplementation's codeLocation.\n\t\t\"\"\"\n\t\treturn \"\"\n\n\tdef getCodeLocation(self):"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.__class__\n\n\tdef _getFunctionName(self):\t\t\t\"\"\"\n\t\"\"\"\n\t\treturn self.name + \".\"\n\n\tdef __call__(self, name, args=(), kwargs={}):\n\t\t\"\"\"\n\t\tCall the instance with the specified name,\n\t\tand optionally the return value. This may not be the\n\t\timplementation in use (e.g., if we call an implementation\n\t\texplicitly, for instance) but not in the implementation\n\t\tnamespace, the result will be a wrapped function instead.\n\t\t\"\"\"\n\t\tif not self.implementation:\n\t\t\tself.implementation ="}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn \"Py\"\n\nclass _CDescriptor(object):\n\t\"\"\"\n\tCDescriptor is a class which defines the interface to interface\n\tof a C function.\n\t\"\"\"\n\tdef __init__(self, name, type):\n\t\tself.f_name = name\n\t\tself.f_type = type\n\t\tself.py = None\n\n\tdef getName(self):\n\t\t\"\"\"\n\t\tReturn the name of this method.\n\t\t\"\"\"\n\t\treturn self.f_name\n\n\tdef getTypes(self):\n\t\t\"\"\"\n\t\tReturn all C imples in this"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tif self.pycomp.so == 'PyPy':\n\t\t\treturn self._targetClass\n\t\tres = self.getRustBuildSystem()\n\t\tif res is not None and type(res) is not buildinfo.BuildInfo:\n\t\t\tif type(res) is basestring:\n\t\t\t\tres = res.split('::')\n\t\t\treturn res[0]\n\t\trust = self.getRustVersion()\n\t\tif rust is not None and type(rust) is not buildinfo.BuildInfo:\n\t\t\tif type(rust) is basestring:\n\t\t\t\trust = rust."}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tresult = []\n\t\tfor o in orderings:\n\t\t\tif o in _multiListOrderings:\n\t\t\t\tresult.append(o)\n\t\treturn result\n\t\n\n##\n# A number of non-empty elements.\n#  \n_empty = {\n\t'x' : 0,\n\t'y' : 0,\n\t'z' : 0,\n\t'q' : 0,\n\t'z' : 0  }\n\ndef _get_list(val, isLen=False):\n\tif isLen:\n\t\tval += _empty\n\telse:"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#    Copyright (C) 2016 Yahoo! Inc. All Rights Reserved.\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\treturn [order[:-1] for order in orderings]\n\t\n\tdef _convert_order_from_string(self, string):\n\t\t\"\"\"\n\t\tReturn a new order for a string.\n\n\t\tReturn a new object for the object or None if no order is applicable.\n\t\t\"\"\"\n\t\tfor order in _split_orderings(string):\n\t\t\tif order:\n\t\t\t\torder = order[-1] if len(order) == 1 else -1\n\t\t\t\tobject_order = self._internal.cast(order)\n\t\t\telse:\n\t\t\t\tobject_order = self._internal.cast("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tout = []\n\t\tfor order, orderOrder in zip(orderings, orderings[::-1]):\n\t\t\tif orderOrder[0][:] in ('#', '0'):\n\t\t\t\tfor j in orderOrder[1:]:\n\t\t\t\t\tout.extend(_legacy_mergeOrderings([order,j+1]))\n\t\treturn out\n\ndef mergeOrderings(localfile, outfile=''):\n\t\"\"\"\n\tMerge the given localfile and write the results to outfile.\n\t\"\"\"\n\tlocalfile = localfile.lower()\n\toutfile = outfile.lower()\n\tmerged = []"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\torderings = list(_mergeOrderings(map(int, orderings)))\n\n\t\t# We cannot always return non-empty lists, but sort will break\n\t\t# if they're empty. So return an empty List.\n\t\treturn list()\n\n\tdef _legacy_keyOrderings(self, other):\n\t\t\"\"\"\n\t\tReturn a sorted list of key-val pairs for each item in\n\t\tother.\n\n\t\t>>> _legacy_keyOrderings([1, 2, 3])\n\t\t['1', '2', '3']\n\t\t>>> _legacy_keyOrderings([1, 2"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\torderings = _split(orderings)\n\t\tall_orders = []\n\t\tfor order in orderings:\n\t\t\tif order.get('objtype') is not None:\n\t\t\t\tall_orders.append(order)\n\n\t\t# NOTE: We need to explicitly check to determine if only an order has\n\t\t# been detected by the server.\n\t\ttry:\n\t\t\tall_orders.sort(key=lambda a: abs(a[0]))[:]\n\t\texcept ValueError:\n\t\t\treturn True\n\t\treturn len(all_orders) > 0\n\n\tdef _multi_merge(objects"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tl = []\n\t\tfor li, (k,v) in enumerate(orderings[:]):\n\t\t\tl.append(self.mergeOrderings([k, v]))\n\n\t\treturn l\n\nclass Goto(object):\n\t\"\"\"\n\tBase class for objects representing the position at which the Goto\n\tobject is added.\n\t\"\"\"\n\t\n\t__slots__ = ('_name',)\n\t_name = None\n\tdef __init__(self, name=None):\n\t\tfrom.. import graphene\n\n\t\tself._name = graphene.Str(name or \"\").replace(\"-\", \"_\")"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tif hasattr(orderings, '__iter__'):\n\t\t\torderings.sort()\n\n\t\treturn ((self.mergeOrderLists(o.replace('.','-'),o.replace('.a','-')),o) for o in orderings)\n\n\tdef get_type_orderings(self, obj):\n\t\tif not hasattr(obj, 'obj_type'):\n\t\t\tobj.obj_type = get_type_orderings(obj)\t# the type of the object\n\n\t\treturn obj.obj_type[:-1]\n\tdef mergeOrderLists(self, o1, o2):\n\t\t\"\"\""}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tif len(orderings) % 2 == 0:\n\t\t\treturn ()\n\n\t\tmerge_orderings_orderings = set()\n\t\tfor ordering in orderings:\n\t\t\tdef _is_merge_order(orderings_1, orderings_2):\n\t\t\t\tdef _is_merge_order(orderings_i, orderings_j):\n\t\t\t\t\tfor other1, other2 in zip(orderings_1, orderings_2):\n\t\t\t\t\t\tif other1 not in merge_orderings_orderings:\n\t\t\t\t\t\t\tmerge_orderings_orderings.append(other1)"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tassert orderings, \"Can't merge a legacy order\"\n\t\tif len(orderings) < 2:\n\t\t\treturn []\n\t\tparts = orderings.split('.')\n\t\tassert len(parts) >= 2, \"Bad order: '%s'\" % part\n\t\torderings = map(strip, parts)\n\t\tassert len(orderings) == len(parts), \"The '%s' object specified by '%s' too long.\" % (part, orderings)\n\n\t\t# Check the orderings for the highest-order first\n\t\ts = []\n\t\tfor d in orderings:\n\t\t"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\traise NotImplementedError\n\ndef viaDecoratedWith(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces via the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\n\t**Types**:\n\t\t* `zope.interface.declarations.InterfaceDeclaration`:\n\t\t\tIDeclaration to invoke\n\t\t* `zope.interface.interface.InterfaceDefinition`:\n\t\t\tInterface definition for this interface\n\n\"\"\"\n\nclass InterfaceDeclaration(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturns a new interface definition.\n\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tobj = C.iface(object)\n\t\tt = obj.type\n\t\tif not isinstance(t, Interface):\n\t\t\treturn None\n\t\treturn t.interface\n\t\ndef get_interface(object, cls, default_value):\n\t\"\"\"\n\tGet an interface provided by the given object given a class.\n\nUsage:\n\t>>> class C(object):\n\t>>>    m = C()\n\t>>>    m = m.method(object)\n\t>>>    m.method(1,2,3).method(1,3).method(1,3).method("}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tresult = declarations.interfaceToInterface(object)\n\t\tfor p in result: return p\n\t\treturn result\n\n\tdef getInterfacesFromModule(object):\n\t\t\"\"\"\n\t\treturn a list of (interface, version) tuples with the names of\n\t\tinterfaces defined on the given module. The order is the same as\n\t\tinterfaceToInterface() but the order in which does *not* modify\n\t\tthe list.\n\t\t\"\"\"\n\t\treturn itertools.chain(\n\t\t\t[ x.interfaceToInterface() for x in\n\t\t\tgetDefs(\"zope.interface.interfaces\") ],\n\t\t\t["}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\ttry:\n\t\t\treturn object.__interfaces__\n\t\texcept AttributeError:\n\t\t\treturn object.__interface__\n\n\ndef implementedBy(interface):\n\t\"\"\"\n\tReturn the interfaces implemented by the given interface.\n\n\t@type interface: zope.interface.interface.Interface\n\t@ivar interface: the interface to check.\n\t@type interface: zope.interface.interface.Interface\n\n\n\t@rtype: dictionary of interfaces and their descriptions\n\n\t@raises: KeyError\n\t\"\"\"\n\tinterfaces = interface.implementedIn(IDeclaration)\n\n\tfor iface in interfaces:\n\t\tfor x in iface.implemented"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn interface.providedBy(object)\n\n\ndef provideInterface(interface): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the IDeclaration of which you provide an interface.\t\n\tThe default implementation first constructs the\n\tprovided interface. \n\t\n\tBe aware that this means the interfaces in the context use an\n\tinterface as an iterable, so the interface is not ordered.\n\t\"\"\"\n\t\n__name__ = 'Interface'\n__author__ = 'Nicholas Mora (normale@zope.org)'\n__copyright__ = '(C) 2005, 2006 N"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\t# TODO: return interface as a class, even if we're using the CORE_INTERFACE\n\t\treturn IDeclaration(\n\t\t\tinterface=prototype[0],\n\t\t\timplements=list(prototype[1].interfaces),\n\t\t\textends=namespace\n\t\t)\n\nInterface = InterfaceInterfacePrototype(\"I\", [\"zope.interface.interfaces.ILibrary\"])\ndef directlyProvidedByFactory(factory): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the definitions directly provided by the given factory\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn object.__getattribute__(self, \"_providedBy\")\n\tdef equivalentBy(self, object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces equivalent to the given object.\n\n- The interface object is passed unchanged.\n\nReturns ``True`` if the given interface is equivalent to the original\nobject's interface.\n\nTo modify this behavior, implement the method `copy` explicitly.\t\"\"\"\n\t\treturn object.__copy__(self)\n\tdef copy(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn a new object.\n\t\n\tAll interfaces must"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn interface.interfaces.interfaces(object)\n\nclass Class(object):\n\t\"\"\"\n\tInterface for a specific class\n\t\"\"\"\n\n\tdef __init__(self, classname=None, bases=[],\n\t\tattributes=None):\n\t\t# pylint:enable=E1101,R0903\n\t\tself.classpath = '.'\n\t\tself._class = None\n\t\tself._attributes = {}\n\t\tself._declared_methods =\\\n\t\t\tdict((attr, object())\n\t\t\t\tfor attr in attributes.keys()\n\t\t\t\tif attr not in classdescriptor._public_attrs)\n\n\t\tfor base"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\ndef equivalentlyOrdered(other): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces equivalently ordered by the given other.\n\n\tThe possible values are from zero to one:\n\n\t* A value of -1 -> 0\n\t* A value of +1 -> 1\n\t\n\tIf the current implementation of the given class doesn't follow this convention, raises `AssertionError`:\n\t* If the interface does not inherit from a builtin class with the same name.\n\t* If the interface does inherit from a builtin class with an empty name it raises `TypeError`:\n\t* If"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn implementedByObject(object.__class__).providedBy(self)\n\n\nclass ProvidedByObject(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces configured by an object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tdef __init__(self, interface):\n\t\t\tif interface.implementedBy():\n\t\t\t\tif not hasattr(interface, \"__implements__\"):\n\t\t\t\t\treturn\n\t\t\t\tif not interface.__implements__(\"__interface__\"): # interface declared with interfaces aren't of type\n\t\t\t\treturn\n\t\t\t\timplementedByObject(interface"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbases = []\n\t\tfor c in classes:\n\t\t\ttry:\n\t\t\t\tbases.append(c.minify(base=True, base_for_level=True).base)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\t\treturn bases\n\n\tdef maxDiff(self, base):\n\t\t\"\"\"\n\t\tReturns the minimum difference between two strings in the base list.\n\n\t\t\tParameters:\n\t\t\t\tbase:\n\t\t\t\t\t1-D number or base of the target list\n\n\t\t\tReturns:\n\t\t\t\tMinimum difference\n\t\t\"\"\"\n\t\tdef min(x): return float(\"inf\") if x >= 0.0"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\t# remove duplicates since all the classes in the classes list will be unique\n\t\t\n\t\tbases = {}\n\t\ti = 0\n\t\tfor c in classes:\n\t\t\tif c not in bases:\n\t\t\t\tbases[c] = i \t\n\t\t\t\ti = i + 1\t\n\t\t\tif bases[c] == i: \n\t\t\t\tbases[c] = i \t\n\t\treturn bases\n\t\t\n\t\t\n\tdef getUniqueBases(self, c, clen):\t\t\n\t\t'''\n\t\tSearch for base classes with their name. Uses the longest common ancestor first\n\t\tfor the c index based upon the length of the"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tordered = {}\n\t\tfor c in classes:\n\t\t\tif c not in ordered:\n\t\t\t\tordered[c] = len(classes)\n\t\t\tordered[c] = cls\n\treturn ordered\n\n#\n# The user interface for all sorts of data in an ordered dictionary. This\n# interface is to mimic an \"ordered\" interface between 2 and and 3 (to use, see the\n# documentation for a nice list of sorts of data in that case).\n#\n\nclass SortedData(object):\n\t''' The data representation returned by an ordered-dict is defined\n\tit is used"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\t# this is needed because we cannot use set(), and because\n\t\t# it will throw any exceptions later\n\t\treturn classSort(classBase(classes))\n\n\t\n\tdef removeClass(baseClass, key):\n\t\tbaseClassSort = getBasesSort(baseClass)\n\n\t\tif baseClass not in baseClassSort:\n\t\t\treturn\n\n\t\tif not baseClass.getClassFromKey(key):\n\t\t\tbaseClassSort.append(key)\n\n\t\t# This eliminates duplicates when removing duplicates\n\t\t\n\t\tfor base in baseClassSort:\n\t\t\tif baseClass.getClassFromKey(base"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tl = SortedBases() \n\t\tl.sort()\n\t\treturn l\n\t#\n\t#def minBases(classes):\t\"\"\"\n\t#\n\t#\tReturns an ordered list of the base classes\t\t\n\t#\t\"\"\"\n\tdef minBases(classes):\t\t\n\t\treturn SortedBases(classes)\n\t#\n\t#def minBases(classes, base=0):\t\"\"\"\n\t#\n\t#\tReturns the minimum base classes\t\n\t#\t\"\"\"\n\tdef minBases(classes, base):\n\t\t'''Maximize the base classes"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tresults = {}\n\t\tfor cls in classes:\n\t\t\tfor key in cls.__dict__:\n\t\t\t\tif key.startswith(\"__\"):\n\t\t\t\t\tcontinue\n\t\t\t\tresult = result.get(cls.__name__, cls.__name__)\n\t\t\t\tif result is None:\n\t\t\t\t\tresult = cls.__default__\n\t\t\t\tresults[key] = result\n\t\treturn results\n\n\tdef maxReduce(cls, inputDict):\n\t\t\"\"\"\n\t\tReturns the value at the maximum allowed value\n\t\tof each base class, taking into account minimalBases.\n\t\t.. note:: If any base class is None, we check the bases"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbaseSites = []\n\t\tfor i in range(len(classes)):\n\t\t\tsite = classes[i]\n\t\t\tmax = 1 + i\n\t\t\tfor x in baseSites:\n\t\t\t\tif x < site:\n\t\t\t\t\tminSize = getMinSize(site,x)\n\t\t\t\t\tmaxSize = getMaxSize(site,x)\n\t\t\t\t\tif (minSize < maxSize) or (maxSize < minSize):\n\t\t\t\t\t\tmaxSites = x, minSize\n\n#################\n##\tBase-class membership class\t\t\t\t\t\t\t##\n#################\n\nclass BaseBases:"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tcableTypes = []\n\t\tfor baseClass in classes:\n\t\t\tfor base in baseClass:\n\t\t\t\tif type(base) is classType:\n\t\t\t\t\tcableTypes.append((base, baseClass))\n\t\tfor base, baseClasses in cableTypes:\n\t\t\tfor baseClass in baseClasses:\n\t\t\t\tif issubclass(base, baseClass) and base.minimalBases(classes) > 0:\n\t\t\t\t\treturn base.minimalBases(classes)\n\t\tassert(len(cableTypes)==len(classes))\n\t\treturn cableTypes\n\n\tdef minimalBase"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\treturn reduce_by_classes(bases = classes)\n\tdef simpleMinusBases(classes): # make sure all base classes are not derived from our ones to their ordered minimum equivalent\t\n\t\treturn min_unequal_bases_from_classes(classes)\n\tdef simpleMinusBases(classes):\n\t\t\"\"\"\n\tReduce a list of base classes to its minimal equivalent\t\n\t\"\"\"\n\t\treturn min(min_unequal_bases(classes), key = sorted) # the base list has been already sorted.\n\t\t\t\t\t\t\t\t\t\t\t\t# the best possible class cannot be found\n\t\t\n\t\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\ts = set(cls.__name__ for cls in classes)\n\t\tif len(s):\n\t\t\treturn min(s)\n\t\treturn sortedSet(cls for cls in classes if cls.__bases__<s)\n\n\tdef _minimize(self, cls=None, args=[], kwargs={}):\n\t\tif cls is None:\n\t\t\tobj, cls = self._classMap[type(cls)]\n\t\telse:\n\t\t\tobj, cls = cls._classMap[cls]\n\t\tif not kwargs.get('showBases', True):\t\n\t\t\tclassName = cls.__name__\n\t\t"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn tuple( self.attrs() or () if all else () ) # pylint: disable=unimplemented\n\n\t\n\n\tdef attributes(self):\n\t\t\"\"\"\n\t\tGet the attribute names and descriptions.\n\n\t\t:return: The attribute names and descriptions.\n\t\t:rtype: tuple\n\t\t\"\"\"\n\t\treturn tuple( self.__dict__.keys() ) # pylint:disable=assignment-from-no-init\n\t\n\tdef __getattr__(self, name):\n\t\t\"\"\"\n\t\tGet the attribute for the given attribute name (eg, module name or interface name)\n\n\t\t:param name: The name of the"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tif all: return self.getAttributeNames()\n\t\telse: return self.getNames()\n\n\tdef getAttributeNames(self):\n\t\tif self.type!= \"Interface\": return []\n\t\t\n\t\tif self.properties:\n\t\t\treturn self.properties\n\t\telse:\n\t\t\treturn []\n\n\t\n\tdef getValueForAttribute(self, attribute):\n\t\tif attribute not in self.attributes:\n\t\t\traise AttributeError(\"Unknown attribute \"+attribute)\n\t\tif self.attributes[attribute] and self.attributes[attribute]!= self.attributes[attribute]:\n\t\t\traise AttributeError(\"Incorrect attribute \"+attribute"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tif all:\n\t\t\t# return list of all interface names & description\n\t\t\tnames    = []\n\t\t\tfor entry in self._interfaces:\n\t\t\t\tnames.append(entry.name)\n\t\t\t\tif not entry.active:\n\t\t\t\t\tif hasattr(self, 'description'):\n\t\t\t\t\t\tdescription = getattr(self, 'description')()\n\t\t\t\t\t\tif description and description!= entry.description:\n\t\t\t\t\t\t\tnames += [description]\n                            if not entry.source:\n\t\t\t\t\t\t\t\tnames += [entry.name]\n\t\t\t\t\t# else:\n\t\t\t\t\t#\traise AttributeError, 'Not a valid attribute.'\n\t\t\treturn names, description"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tnames = []\n\t\tfor name, (desc, desc) in getattr(self, self._names):\n\t\t\tif desc[0]!= '_':\n\t\t\t\tnames.append((name, desc))\n\t\tif all:\n\t\t\treturn zip(names,\n\t\t\t\t\t(desc[0] for desc in _get_all_desc(self.name, desc[1]) for name, desc in names))\n\t\telse:\n\t\t\treturn names\n\n\tdef namesAttributes(self):\n\t\t\"\"\"\n\t\tReturns dictionary of attribute names and values.\n\t\t\"\"\"\n\t\treturn self._names\n\n\tdef"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tattrs = self._attrs\n\t\tif all is True:\n\t\t\tattrs &= self._attrs\n\t\tfor name in dir(self):\n\t\t\tif name.startswith('_'):\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tv = getattr(self, name)\n\t\t\texcept AttributeError:\n\t\t\t\tv = None # default attribute not found\n\t\t\tif not v or v is None:\n\t\t\t\tcontinue\n\t\t\tattrs |= getattr(v, name)\n\t\t\t\n\t\treturn attrs\n\nclass NameGenerator:\n\t@staticmethod\n\tdef gen_filename():\n\t\tif _sys.version_info >= (3,7):"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn [ a.name + \" (interface)\" for a in self._interface.attributes ] + \\\n\t\t\t   list(\n\t\t\t\t\tself._interface.attributes.values() +\n\t\t\t\t\tlist(\n\t\t\t\t\t\t[\n\t\t\t\t\t\t\t( \"interface\", name + \" (optional)\" )\n\t\t\t\t\t\t\tfor name in self._interface.attributes[\"prototypeNames\" ]\n\t\t\t\t\t\t] +\n\t\t\t\t\t\tlist(\n\t\t\t\t\t\t\tself._interface.attributes[\"returnTypes\"],\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\t) + \\\n\t\t\t\t[ f.name + \" (interface)\" for f in self._interface.getParameterTable() ]\n\n\tdef"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tif all:\n\t\t\tnames = []\n\t\t\tdescs = []\n\t\telse: # not defined...?\n\t\t\treturn name_and_descs(self._names)\n\t\treturn self._descs\n\n\tdef __init__ (self, names = None): # pylint:disable=redefined-builtin\n\t\t\t\t\"\"\"\n\t\tThis function creates a new instance of the module instance.\n\t\t\n\t\tParameters\n\t\t----------\n\t\t\n\t\tall : bool\n\t\t\treturn all attribute and description of this function\n\n\t\t\"\"\"\n\t\tself._names = {} # dictionary\n\t\tself._descs = {} # dictionary\n\n#"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn (self.interfaces, self.description)\n########################\r\n# Project\t : Genesis\r\n# Generated by PyMethodDefEditor2.py\r\n\r\nimport sys\r\nimport codecs\r\nimport types\r\nimport weakref\r\nfrom gensim.corpora.dictionary import DictVectorizer\r\nfrom gensim import models\r\nfrom gensim.utils import mathutils\r\nfrom gensim.models import PhpModel\r\nfrom gensim.corpora import DictionaryModel\r\nfrom gensim import utils\r\nimport types\r\nfrom gensim.corpora.six import string_types"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn nameAndDescriptions_list(self.interface_name_and_description_list._attr, self, all)\n\n\t#----------------------------------------------------------------------\n\tdef interface_for(self, interface): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute name and description of a device\n\tinterface -- a string with the interface name\n\tall -- return attribute list (default: return all attributes in the interface, a list of attributes from the list if empty)\n\tdevice_name -- device_name\n\t\"\"\"\n\t\tname, desc = nameAndDescriptions_list(self.interface_"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn {'description', 'list', 'dict'}[self.interface[0]] if self.interface else {}\n\n\tdef getAttribute(self, interface, attribute):\n\t\t\"\"\"Returns attribute value or None if attribute doesn't exist\"\"\"\n\t\ttry:\n\t\t\treturn getattr(interface, attribute)\n\t\texcept AttributeError:\n\t\t\treturn None\n\n\tdef getAttributesForInterface(self, interface):\n\t\t\"\"\"Return interface.attributeDict and interface.nameDictionary.\"\"\"\n\t\treturn dict(self._dictFromInterface(x) for x in interface.attributeDict.values()) if interface.nameDictionary else {}"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._interface._get_attr_names(self._interface.interface_attributes)\n\t\texcept AttributeError as e:\n\t\t\tif any(isinstance(attribute, str) for attribute in self.attrnames):\n\t\t\t\traise e.value.args[0]\n\n\t\tif not all:\n\t\t\treturn []\n\n\t\tself._check_interface_names(self._interface.interface_attributes)\n\t\tfor i in range(len(self._interface.interface_attributes)):\n\t\t\tif i < len(self._interface.interface_attributes) and self._interface.interface_"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tret = []\n\t\tfor child in self.getchildren() + self.getattributes():\n\t\t\tif not child.access():\n\t\t\t\tcontinue\n\n\t\t\tchild_val = self._attributes.get(child.attributeName())\n\t\t\tif child_val is None:\n\t\t\t\tcontinue\n\t\t\tret.append(child_val.name())\n\t\treturn ret\n\n\tdef __iter__(self, property=None): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tIterate over the methods or properties of the interface.\n\t\tAttribute access doesn't work, as the method name is the method name"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tattrnames = {}\n\t\tif all or isinstance( self, (Object, list) ):\n\t\t\tattrnames = self.get_names()\n\t\treturn self.attr_names(attrnames)\n\n\n\t#\n\t# Helper function\n\t#\n\n\tdef get_node(self, name, default=None):\n\t\t\"\"\"\n\t\tReturns the XML node of the given attribute name. \n\t\t@param: name The name of the object.\n\t\t@param: default The value of the attribute if name is not of type object (the object is returned if no value was specified)\n\t\t@"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\t# pylint: disable=redefined-builtin\n\t\tself._attributes = []\n\t\tfor f in dir(self):\n\t\t\tif not f.startswith('_') or callable(getattr(self, f)) or not all:\n\t\t\t\tself._attributes.append(f)\n\t\treturn self._attributes\n\tdef __repr__(self):\n\t\t\"\"\"Returns a string representation of the interface.\"\"\"\n\t\tresult = []\n\t\tfor name in self:\n\t\t\tif hasattr(self, name):\n\t\t\t\tresult.append('%s=%r' % (name, getattr(self, name)))\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self._attributes.keys()\n\tdef __getattr__(self, name):\n\t\t\"\"\" Return the value of the attribute.\"\"\"\n\t\treturn self[name]\n\tdef __setattr__(self, name, value):\n\t\t\"\"\" Set the value of the attribute.\"\"\"\n\t\tself._attributes[name] = value\n\tdef __delattr__(self, name):\n\t\t\"\"\" Delete the specified attribute.\"\"\"\n\t\tdel self._attributes[name]\n\n########################################################################################################\n# main ########################################################################################################\n########################################################################################################\n\n\ndef check_configuration_paths():\n\t\"\"\"Check if the paths the user has set up are"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\nif sys.flags.optimize >= 2:\n\tfrom pyffi.C.Cframe import CFCall\n\tfrom pyffi.C.Cframe import CCall\nfrom pyffi.C.Cframe import PyObject_CFunction, CCallObject\n\tfrom pyffi.C.Cframe import CCall, createCFunctionPrototype\n\tfrom pyffi.C.Cframe import CCallObject, CFModule\n\tfrom pyffi.C.Cframe import CArgument, CReturnValue, CFFunction\n\nelif sys.flags.optimize >= 3:\n\t# Use the builtin C-"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.methods.keys()\n\n    def all_names(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.methods.keys()\n\n\tdef all_attrs(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.methods.keys()\n\n    def all_names_non_attrs(self): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the attribute names of this interface.\t\"\"\"\n\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self._interface_name_order\n\n\t@names.setter\n\tdef names(self, name):\n\t\t\"\"\" Set the attribute names. If we update the attribute the method will\n\t\tneed to call on the first call to update_attribute. This is to avoid a\n\t\tdependence on the previous assignment to this function.\n\t\t\"\"\"\n\t\tself._interface_name_order = name\n\t\tself._attribute_name_cache = dict()\n\t\tif all:\n\t\t\tself._init_attribute_names()\n\n\t@property\n\tdef access(self):\n\t\t\"\"\" Get access"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self._f.attributes.names if all else list(self._f.attributes.names)\n\t\n\tdef names_by_name(self, name):\n\t\t\"\"\"\n\t\tReturn the attribute names that exist.\n\t\t\n\t\tIf the attribute is not found, it is returned.\n\t\t\"\"\"\n\t\tif self._f is None:\n\t\t\tself._f = self._f.attributes[name]\n\t\treturn self._f.attributes.names if name in self._f.attributes.names else list(self._f.attributes.names)\n\t\n\t@property\n\tdef is_enabled("}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn [a[0] for a in self.interfaces if a[1]]\n\n\tdef add_attribute(self, name, value):\n\t\tprint \"warning: no attribute '\" + name + \"'\",\n\t\t\t# print \"method get/set is deprecated\",\n\t\t# return None\n\t\traise AttributeError\n\n\tdef del_attribute(self, name):\n\t\t\"\"\"\n\tDelete the attribute. The return value of any given attribute is assigned\n\tto any subsequent call to any member in the instance.\n\t\"\"\"\n\t\ttry:\n\t\t\tself._attributes[name].value = (self"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\treturn _normalizedeclargs(sequence, output)\n\n\tif isclass(sequence):\n\t\treturn normalizeclass(sequence, isbuiltin)\n\tif ismethod(sequence):\n\t\treturn normalizefunc(sequence, output)\n\traise NameError('This function will be called as a function on only a class')\n\ndef normalizename(name, isbuiltin):\n\t\"\"\"\n\tDecide what a symbol may apply to as a function.  In fact,\n\tNormname() will return True for all instances, and False for names but\n\tonly return names of builtin classes.\n\t\"\"\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif not output:\n\t\t\tif isinstance(sequence, (tuple,list)):\n\t\t\t\toutput = _normalize2(sequence)\n\t\t\telse:\n\t\t\t\toutput = getattr(sequence, '__init__', _normalize)\n\t\treturn output\n\ndef _split(separator, first, last):\n\t\"\"\"\n\tParse separator string and remove any remaining spaces from it.\n\n\t@param separator: string, e.g., ':\n\t@param first: list, e.g., ['a']\n\t@param last: list or None, e.g. []\n\t@rtype: string, list"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif not output is None:\n\t\t\tcommand = ''\n\t\t\tfor element in sequence:\n\t\t\t\tif isinstance(element, Argument) or isinstance(element, Interface):\n\t\t\t\t\tcommand += element.value\n\t\t\t\t\tcontinue\n\t\t\t\tif isinstance(element, Interface) or isinstance(element, Interface):\n\t\t\t\t\tcommand +='' + element.name\n\t\t\t\telse:\n\t\t\t\t\tcommand += str(element)\n\t\t\tcommand += '\\n\\n'\n\t\t\treturn command\n\t\telse:\n\t\t\tcommand = ''\n\t\t\treturn command\n\t\n\t\n# coding=utf-8\nfrom __future__ import unicode_literals\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif isinstance(sequence, str):\n\t\t\toutput = list(sequence)\n\t\tif not output:\n\t\t\treturn str(sequence)\n\t\telse:\n\t\t\tfor element in sequence:\n\t\t\t\tif not isinstance(element, (list,tuple,dict)):\n\t\t\t\t\traise TypeError(\"Argument %r must be a list or tuple of list of arguments\" % (element,))\n\t\t\t\t_normalizeargs(element, output)\n\t\t\tfor element in output:\n\t\t\t\tif not isinstance(element, dict):\n\t\t\t\t\traise TypeError(\"Argument %r must be a dictionary of definitions\" % (element,))\n\t\t\t\t_normalize"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t    if output: output=OrderedDict()\n\t        for name in sequence: output[name]=sequence[name]\n\t        return output\n\ndef _print_args(sequence, output=None):\t\"\"\"\n\tPrint the list of declarations. All declarations will contain a list of\n\tone item.  This method first concatenates a list of declarations to add\n\tthe values to the output.\n\t\"\"\"\n        if output: output=OrderedDict()\n        for name,value in sequence.items():\n            name=name.replace('_', '-')\n            if _normalizeargs(value, output) == 'decl_value':"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif ':' in sequence:\n\t\t\tinput, output = sequence.split(':', 1)\n\t\telse:\n\t\t\tinput = sequence\n\t\t\t\n\t\tif not output or type(output) is list:\n\t\t\treturn [_normalize(input, output[0]) for input in sequence.split(',')]\n\t\t\t\n\tif not sequence:\n\t\treturn list(list(reversed(output)))\n\tif len(sequence) == 0:\n\t\treturn [output]\n\telif len(sequence) == 1 and isinstance(sequence[0], (list,tuple)) and 0<=len(sequence[0])<=len(input"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tself._args = []\n\t\tself._output = output or []\n\ndef _unnormalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nUnnormalize declaration arguments might contain Declarions, tuples,\nor single interfaces.\n\nAnything but individial interfaces or implements specs will be removed.\t\"\"\"\n\t\tself._args = []\n\t\tself._output = []\n\ndef declareInterface(interface, name):\n\tif isinstance(interface, basestring): return _normalizeargs(sequence, name)\n\treturn interface\n\nfor item in (\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\t# First process the arguments that were already parsed.\n\tfor arg in sequence:\n\t\tif arg == 'tst_list':\n\t\t\tcontinue\n\t\t\n\t\t# This must come before any other processing, since they\n\t\t# must be present and passed through as final\n\t\tif arg == 'tst_args':\n\t\t\tcontinue\n\n\t\t# We want to have some special keyword arguments that we can\n\t\t# later call to our _normalize() method\n\t\tif (isinstance(arg, Interface) and\n\t\t\tnot (arg.prototypes and arg.prototypes not in iface_map)):\n\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\t# Convert'str', or None to a string\n\tif output == None:\n\t\toutput = ''\n\toutput = output.replace('.','_')\n\toutput = output.strip()\n\t\n\tif len(output) > 0:\n\t\toutput = output[0:output.find('_')]\n\t\n\treturn output\n\t\n\t\n\t\n\ndef _sanitizeinput(text):\n\t\"\"\" Remove special chars in the first paragraph \"\"\"\n\treturn re.sub(r'[ \\r\\n\\r', '', text)\n\t\t\n#def _filldeclariser(s):\n\n#\ts ="}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tif output is not None and type(output) is not type([]):\n\t\t\tif isinstance(output, list):\n\t\t\t\toutput = [output[0].replace(\".\",\"*\")]\n\t\t\toutput.extend(sequence)\n\t\telse:\n\t\t\tif isinstance(sequence, list):\n\t\t\t\tsequence = sequence.replace(\".\",\"*\")\n\t\t\t\tsequence = [line.lstrip() for line in sequence]\n\n\t\tindex = 0\n\t\tseen = []\n\n\t\tfor arg in sequence:\n\t\t\tif (arg[:2] == \"INTERFACE\"):\n\t\t\t\tname = arg\n\t\t\t\targs = args[2:]\n\t\t\t"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn False\n\t\n\t# Look for a c-optimized optimization module\n\t_c_module = sys.modules[__name__]\n\tif not _c_module:\n\t\traise ImportError('Failed to load c-optimize module!')\n\n\tif getattr(_c_module, '__optimization', None) is None:\n\t\treturn False\n\n\ttry:\n\t\ttry:\n\t\t\tfrom c_optimization import optimizations\n\t\texcept ImportError, e:\n\t\t\t_c_module.warning(str(e))\n\t\t\treturn False\n\tfinally:\n\t\tsys.modules[__name__] = _c_module"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\ttry:\n\t\t\tc = eval(\"pkg_resources.get_distribution('chess')\").dist\n\t\t\tif not c:\n\t\t\t\tlogger.info(\"No C optimization module found\")\n\t\t\t\treturn True\n\t\t\treturn True\n\t\texcept (ImportError, AttributeError):\n\t\t\t# Python version < 3.2\n\t\t\t# If the C/optimization module isn't installed, return False\n\t\t\t# instead of raising an ImportError\n\t\t\treturn False\n\telse:\n\t\t# We have a new C optimization module\n\t\treturn False\n\ndef _c_optimizations_installed():\t\"\"\"\n\tReturn the C"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\timport numpy\n\t\tif numpy.get_function_module_name(None)!= \"c\":\n\t\t\traise ImportError(\"Python implementation for optimizations \"\n\t\t\t\t\t\t\t\"need to be defined with NumPy.\")\n\t\ttry:\n\t\t\tfrom numpy.distutils import c_optimizations\n\t\t\tc_optimizations.c_optimizations()\n\t\texcept Exception, ex:\\\n\t\t\tif str(ex).find('not available'):\n\t\t\t\timport sys\n\t\t\t\tsys.exit(0)\n\n_c_optimize_available = _c_optimizations_available()\n\nif c_opt.optimize_lib"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn _OPT_AVAILABLE\n\n\tdef exists(self):\n\t\treturn _OPT_AVAILABLE\n\n\tdef load(self, filename):\n\t\ttry:\n\t\t\tf = open(filename, 'r')\n\t\texcept OSError:\n\t\t\tif _VERBOSE:\n\t\t\t\t_debug(\"%s: Can't open file %s\", self.__class__, filename)\n\t\t\t\tif self.args.debug: raise\n\t\t\tpass\n\t\telse:\n\t\t\tself.c.load_extension(filename)\n\t\t\tif f is None or not isinstance(f, str):\n\t\t\t\traise ImportError(\"file at %s not"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\t# I don't use it, but it'd have a test for this.\n\t\treturn False\n\n\t# If the C optimization module is available, it'll be used by\n\t# the module.\n\tif not _c_optimizations_available():\n\t\treturn False\n\n\t# If there is something to support it, it'll be used even if it is\n\t# necessary, then simply use it.\n\tif \"c\" not in dir(_c):\n\t\tfrom gofed import Optimization\n\t\t_c.insert(0, Optimization())\n\n\treturn True\n\n\n#"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\ttry:\n\t\t\timport _functools\n\t\texcept ImportError:\n\t\t\treturn False\n\t\ttry:\n\t\t\tmpm = __import__(\"mpm\", fromlist=[ \"Optimizations\" ])\n\t\t\treturn mpm\n\t\texcept ImportError:\n\t\t\treturn true\n\n\t# Returns a boolean that indicates whether the optimization module available or not.\n\tdef _c_optimizations_available_and_not(self):\n\t\ttry:\n\t\t\timport _functools\n\t\texcept ImportError:\n\t\t\treturn False\n\t\ttry:\n\t\t\tmpm = __import__(\"mpm\", fromlist=[ \"Optimizations\", \"not\""}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tif not _is_optim(module):\n\t\t\tfrom. import _c_optimizations\n\t\t\tmodule_str=str(module)\n\t\t\tif not _c_optimizations.has_module(module_str):\n\t\t\t\traise ImportError()\n\t\t\tmodule=None\n\t\t\n\t\treturn _c_optimizations.modules[str(module)] \n\n\n_c_optimizations._optimizations=0\n\n\n## ##########################################################################################################\n\n\n##################################################################\n# C-core-class decorator for the current global functions\n##################################################################\n@contextmanager\ndef _lock(func):\n\tglobal _c_"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tif __c_optimizations_available is None:\n\t\t\traise ImportError(\"Optimization module must be initialized with the required optimization enabled.\\n\")\n\t\treturn __c_optimizations_available\n\treturn _c_optimizations_available\n\n_optimization_enabled = _c_optimizations_enabled()\n\nclass OptimizationFunction(object):\n\t\"\"\"\n\tThis object is the main class for every optimization method that\n\tgenerates functions or wrappers to the solver.  For example, the\n\t:meth:`solver` method is for every :meth:`linearize` method\n\ton a"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn _c.OptManager.enabled\n\n\t# return optimizations if available\n\tdef opt_available(a):\n\t\treturn a.enabled\n\n\tdef _c_module(c):\n\t\tfrom.optimize import _c_module_enabled\n\t\treturn _c_module_enabled(_c)\n\n\toptimize = _c.opt_get('C Optimizations Available', None, _c.opt_class)\n\tif optimize is None:\n\t\toptimize = True\n\treturn optimize\n\n# Return whether the optimization machinery we make use for the\n# optimizer. This means that the"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n    if C._optimization.enabled:\n        return C._optimization.module_name\n\telse:\n        from os import sys; sys.exit('The optimizations are not used, '\n\t\t                'use os.path.join(path.abspath(), 'lib/opt'))\n\ndef _cpp_optimized_init():\n  \"\"\"\n  Initialize C++ optimizations. Used in build_cpp_modules() instead of\n  _c_optimizations_available.\n  \"\"\"\n  if hasattr(_c_optimizations, '_module_path') and _c_optimizations._module_path is None:\n    _c"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn _USE_C_OPTIMIZATIONS\n\t\n\t#\n\t# The following two functions are available; we expect that _used_c_impl is checked\n\t# before, and then _use_c_impl has been called for each of those, even those\n\t# that may not be present.  We can't verify the values at all.  However, since the\n\t# _use_c_impl property might be called later on, we have a chance that they\n\t# do work from another code.\n\n\t#\n\t# We always provide the names of the parameter ("}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tfrom pypy.api import PyPyMethod\n\t\t\n\t\tis_py_enabled = False\n\t\tif '_use_c_impl' in os.environ:\n\t\t\tis_py_enabled = os.environ['PURE_PYTHON'].startswith('PYPY_EXECUTE_C')\n\n\t\tif not is_py_enabled:\n\t\t\traise Utils.WafError(\"Cannot use optimized method\")\n\t\t\t\n\t\treturn is_py_enabled\n\n\t__USE_C_OBJ_OPT = staticmethod(_should_attempt_c_optimizations)\n\t__USE_C_BUILD_OPT = staticmethod"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\thave_c_optimizations = platform.dist()[0].startswith('c')\n\t\tif not have_c_optimizations:\t# pragma: no cover\n\n\t\t\tenv_variable = os.environ.get('PURE_PYTHON')\n\t\t\tif env_variable is None:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\thave_c_optimizations = True\n\t\telse:\n\t\t\treturn True\n\n\t# Now we need the C optimization methods of the current interpreter,\n\t# as described in PURE_PYTHON\n\t# This is somewhat like the C optimization routines in an environment\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn _use_c_impl()\n\n\tdef use_c_optimize(self):\n        \"\"\"\n\t\tReturn a boolean indicating if whether we should use the best\n\t\tC optimization and thus, if available.  (Note that this can be\n\t\tused when optimizing the `run()` method, which only happens\n\t\tonce per run, so this can be used in some situations, when you\n\t\tcould run the code from within Python code.)\n\t\t\"\"\"\n\t\treturn (self.USE_PYTHON or self.USE_C_OPT)\n\n\tdef build(self, sources, **kw):"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn (getattr(_use_c_impl, 'get_c_compiler', None) or\n\t\t\tisinstance(default_implementation, CCompiler)\n\t\t\tor isinstance(default_impl, CCompiler)) and \\\n\t\t\t_should_attempt_c_optimizations()\n\telse:\n\t\treturn _use_c_impl is None\n\ndef _compile_code(*args, **kwargs):\n\tpass\n\ndef _link_internal(src_fn, **kargs):\n\t'Construct an explicit linked implementation for the given source file.'''\n\tfargs = {}\n\tget_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tc_optimizations = get_c_optimizations()\n\t\tif HAVE_PYPY:\n\t\t\treturn c_optimizations.enabled\n\t\telse:\n\t\t\treturn False\n\n\tdef _use_c_impl():\n\t\t\"\"\"\n\t\tUsed internally _c_op_to_op\n\t\"\"\"\n\t\timport c_compiler, libc_opt, cython_optimize\n\n\t\tif not HAS_CPYTHON:\n\t\t\treturn False\n\n\t\topt_compiler = get_preferred_library(opt)\n\t\topt_compiler.initialize()\n\n\t\tif _should_attempt_c_optim"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn _using_c_impl is not None\n\ndef _use_c_impl(env_var):\n\t\"\"\"\n\tReturn True if we should use the C optimization with C implementations,\n\tand return the value of the ``PURE_PYTHON`` environment variable.  \n\t\"\"\"\n\treturn False\n\n\ndef _use_c_impl_forced(env_var):\n\t\"\"\"\n\tReturn a True value if we should use the C optimization with C as the\n\toptimized version.  (Only used if the optimized version is forced\n\twhen using C optimizations.)\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn _USE_C_optimizations\n\n\t@staticmethod\n\tdef _use_c_impl(runtime = 'python'):\n\t\t\"\"\"\n\t\treturn whether we should attempt to use the C implementation.\n\n\t\t@type   runtime: string\n\t\t@param runtime:  \t\tstring of `PURE_PYTHON` env variable\n\t\t\"\"\"\n\n\t\t# This method should return a boolean value, indicating whether C\n\t\t# optimizations are required for our Python system.\n\n\t\t# C.PY_NO_C_IMPL was used for Python 3.0.0. Please modify it.\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn '_use_c_impl' in os.environ\n\n\tif (self._c_impl and\n\t\tself._use_c_impl and os.environ.get('PURE_PYTHON') ==\n\t\t_USE_C_OPTIMIZATIONS):\n\t  return True\n\n\treturn False\n\n\ndef _find_dynamic_libraries(needed, libraries):\n\tfor library in dependencies.find_for_library(library):\n\t\tfor libfile_name in library.libraries:\n\t\t  file_name = os.path.normpath(libfile_name)\n\t\t  if os.path.normpath"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn PURE_PYTHON\n\n# Note: When we're on PyPy, we have to override to use the c++ compiler (not\n# distutils because otherwise no 'distutils' is installed).\n_USE_C_OPTIMIZED_AST = True\n\n# The 'use_c_impl' option is a boolean variable which, when true, indicates that\n# we should attempt to use the C optimizations instead of the Python native versioning, \n# by forcing that to occur if a system cannot provide a C optimizations. \n# We use this to ensure that ``use_c"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\treturn _c_optimizations_ignored\ndef _c_optimizations_required(env, flags):\n\t\"\"\"\n\tChecks that all optimizations are in _c_optimizations_referent\n\tor _c_optimizations_ignored.\n\t\"\"\"\n\tfor a in flags:\n\t\tif not env._c_optimizations_referent.has_key(a):\n\t\t\treturn False\n\"\"\"Support for KNX switches.\"\"\"\nimport logging\n\nfrom homeassistant.const import (\n    CONF_BROKER_IP,\n    CONF_CONSUMER"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\t\t# return True\n\t_cpp_optimizations_not_required = property(_c_optimizations_not_required)\n\t'''The same as `_c_optimizations_required', but for all optimizations'''\n\n# c_globals\n_c_globals = _c_globals_copy()\n_c_globals.update(globals())\nimport logging\nimport os\nfrom typing import Any, Callable\n\nfrom aiohttp import web\nfrom aiohttp_sessions import BaseSession\nfrom aiohttp_session_utils import session_check_login\nfrom aiohttp_session_utils"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn NotImplemented\n\n\tdef _c_optimizations_required(self):\t\"\"\"\n\tInside the user function with _c_optimizations_required(),\n\tthe _c_optimizations_required() function that adds the optimizations required to\n\tan Op to the graph if they are found. This function is defined\n\tautomatically based on code used to define a set of optimizations. The following\n\tmethods are performed:\n\t1. to get the number of variables required to minimize the loss\n\t2. for each training set of optimizations (including the train/val ops)\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn _c_optimizations_required[0](self)\n\n\tdef _c_optimizations_post_init(self, arg = 0):\n\t\t\"\"\"\n\t\tThe same as `_c_optimizations_post_init`, but it only performs optimizations\n\t\trequired once.\n\t\t\"\"\"\n\t\t_c_optimizations_post_init[arg] = self\n\n\t## SELRIMISTIC VARIABLES\n\n\tverbose = 1\n\tuse_svd = False\n\tsvd_solver = None\n\tuse_random_svd = False\n\tsvd_threshold = 1."}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn _c_optimizations_required\n\ndef _c_optimizations_optional():\t\"\"\"\n\tUsed to allow optimizations not explicitly defined during the current process (e.g.\nfor the sake of testing).\n\t\"\"\"\n\t\treturn _c_optimizations_optional\n\ndef _c_optimizations_required(type=None):\t\"\"\"\n\tA decorator that allows the optimizations required to be called when the\n\tcurrent process is done.  A decorator of the type that is bound is applied for the current\n\tprocessing of the process.\n\t\"\"\"\n\t\tdef"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn [ 'optimize_step', 'optimizing' ]\n\treturn _c_optimizations_required\n\ndef _c_optimizations_required():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn [ 'optimize' ]\n\t\nif _c_opt_optimizations.default_optimizations!= 0 and _c_optimizations_required().lower()!= 'default':\t# we didn't have a default optimizations.\t\n\t#print \"Default optimizations:\", _c_opt_optimizations.default_optimizations\t\n\t_"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn False\n#~~ _c_optimizations_not_required~~\n\n\n#~~ _c_optimizations_required~~\n\n\n#~~ _c_optimizations_required~~\n\n\ndef _c_optimizations_not_required():\n\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn False\n#~~ _c_optimizations_missing~~\n\n\n#~~ _c_optimizations_missing~~\n\n\ndef _c_optimizations_not_required():\t\"\"\"\n\tThe opposite of `_c_optimizations_require`.\t\"\"\""}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\n#\tdef _c_optimizations_ignored(self):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\n#\tdef _c_optimizations_optional(self):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\n\t\n\t#\tdef _c_optimizations_optional(self):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\n#\tdef _c_optimizations_optional(self):\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\""}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn not getattr(_LIB, '_c_optimizations_ignored', False)\n\n\tdef _c_optimizations_required():\t\"\"\"\n\tCalled after all arguments are built.\t\t\n\t\"\"\"\n\t\treturn func(self._func_name, (self._input_types,))\n\n\t_c_optimizations_required = property(_c_optimizations_required)\n\n\t@c_optimizations_required\n\tdef _c_optimizations_required():\n\t\t_parse_c_optimizations_function()\n\t\treturn self._c_optimizations_required()\n\t\n\t@c"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tif not os.path.exists(pathToDir + \"/optimizations\"):\n\t\t\tos.makedirs(pathToDir + \"/optimizations\")\n\t\tf = open(pathToDir + \"/optimizations\",\"w\")\n\t\tfor func in _optimization_functions:\n\t\t\tf.write(\"-\" + func + \"\\n\")\n\t\tf.close()\n\t\t_update_file_mtime(pathToDir + \"/optimizations\")\n\n\t# --- Special additions\n\t# These settings are read on the file system.\n\t_c_optimizations_ignored = []\n\t_c_optim"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif _use_c_impl:\n\t\t\t_use_c_impl = _USE_C_IMPL\n\t\telif not _USE_C_IMPL or _USE_C_IMPL!= 0:\n\t\t\traise ValueError(\"C function does not support -Oc. To use this \" \\\n\t\t\t\t\t\"use C compiler for optimization support.\")\n\treturn _c_optimizations_required\n\ndef _cxx_optimizations_required():\t\"\"\"\n\t\tReturn a true value if the C optimizations are required, and a C++\n\t\tbinary. \n\t\t\"\"\"\n\t\tif _use_cxx"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tfrom sysconfig import get_config_vars\n\t\tif get_config_vars('_USE_C_OPTIMIZATIONS'):\t\n\t\t\treturn 1\n\t\telse:\n\t\t\treturn 0\n\n_have_c_impl = _c_optimizations_required()\n\ntry:\n\tif _have_c_impl:\n\t\t_use_c_impl = False\t\"\"\"Use CPython (only for Python 2.6) or c python\n\t(only if it's available, default to ``True``)\n\t\"\"\"\n\t\ttry:\n\t\t\t_use_c_impl = True\t"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tc_compiler_version = get_installed_compiler_version()\n\t\tif c_compiler_version!= \"python\":\n\t\t\treturn 1\n\t\treturn 0\n\tdef _c_optimize_required():\t\"\"\"\n\tReturn a True if the C optimizations are required and has been optimized.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tc_compiler_version = get_installed_compiler_version()\n\t\tif c_compiler_version == \"python\":\n\t\t\treturn 1\n\t\treturn 0\n\tdef _"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn True\n\treturn _use_c_impl(_use_c_impl) \ndef _use_c_impl(_use_c_impl):\t\"\"\"\n\tReturn a false value if the C implementation was enabled\n\t\"\"\"\n\t\thave_optimizations = True\n\t\ttry:\n\t\t\thave_optimizations = _use_c_impl.use_numpy\n\t\t\thave_optimizations = _use_c_impl.use_cupy\n\t\texcept AttributeError:\n\t\t\tif not have_optimizations: raise AttributeError(\"The C implementation is not available in C library.  Try"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn cimpl.get_backend().have_python\n\n_use_c_impl = _c_optimizations_required\n\ndef install_c_impl(impl):\n\t\"\"\"Returns the Python implementation of this `c.f` function, or None.\n\n\tReturns if ``impl`` supports the given `c.f` function, or raises if the function\n\talready is loaded.  Raises AttributeError for the corresponding C\n\timplementation object if no such C implementation object has been specified\n\tby ``c.f``.\n\t\"\"\"\n\t\treturn impl.__implementation__ if impl is None"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _use_c_impl\n_use_c_impl = _c_optimizations_required\n\ndef PURE_PYTHON(name, f):\n\t\"\"\"\n\tReturn Python object to optimize.c. In the following two variables:\n\t * `f` (callable)\n\t\"\"\"\n\treturn _use_c_impl(f, name)\n\n\ndef C_OPTIMIZATIONS(f, prefix='C_'):\n\t\"\"\"\n\tCall the optimization routines through `f`.\n\t\"\"\"\n\treturn _use_c_impl(f, 'c_' + prefix"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _c_impl!= True\n\t\n\tdef _c_opt_flags_supported(self):\t\"\"\"\n\tReturn a true value if the optimization flags are supported\n\t(i.e., --version, --verbose).\"\"\"\n\t\trv = _c_impl == 0\n\t\tif rv:\n\t\t\treturn rv\n\t\treturn opts.optimize, opts.debug\n\t\n\tdef _set_options(self, **kwds):\n\t\t'''Sets the c_opt_flags variable from the supplied keywords.'''\n\t\tassert len(kwds) in (2, 3)\n\t\tfor k in kwds.keys"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t    \n\t# This method used to check if the function purity is in use\n\tdef _check_c_impl(self):\n\t\tif not self.is_c_optimized():\n\t\t\treturn False\n\t\t\n\t\treturn True\n\t\n\t# This function used to check if the function purity is in use\n\tdef _check_pure_c_runtime_requirement(self):\n\t\tself.purity_is_available(False, True)\t# Purity requirement currently supported\n\t\tif hasattr(self, \"use_pure_c_opt\"):\n\t\t\tc_opt_enabled = self"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\t# A list of optimizations.\n\t\t_c_optimizations = []\n\t\n\t\t# If there's some implicit optimizations, use that instead.\n\t\t# We make them available as _c_optimizations_available(), in order to\n\t\t# prevent a second call at the same time.\n\t\tif not _c_optimizations_available():\t\n\t\t\treturn True\n\n\t\t# For the moment, we only allow the first time we've already collected a full\n\t\t# c optimization, so we just return the first C optimization.\n\t\treturn _c_optimizations[0]\n\n\t\n\t"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn _c_impl(True)\n\n\ndef _c_use_c_impl(method):\t\"\"\"\n\tDecorator for use c functions\n\n\t:parammethod: the function's method\n\t:return: new function which replaces the function with itself\n\t:rtype: function\n\t\"\"\"\n\t\tdef _c_impl(*args, **kwargs):\n\t\t\traise NotImplementedError(method)\n\t\tc_fun = _c_optimizations_required()\n\t\tc_fun.__name__ = method.__name__  # get the name string\n\t\treturn c_fun\n\treturn _c"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.reset()\n#!/usr/bin/env python\nfrom __future__ import division, print_function, absolute_import\n\nimport numpy as np\nimport tempfile\n\nfrom distutils.core import setup\nfrom setuptools import find_packages, setup\nfrom distutils.cmd import get_compiler\nimport sys, os\n\nif sys.version_info[0] == 2:\n    with open(os.path.join(os.path.dirname(__file__), '..', 'README.md'), encoding='utf-8') as f:\n        readme = f.read()\nelse:\n   "}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tpass\n\n\tdef __init__(self, initial_value=None, make_bins=True):\n\t\t\"\"\"\n\t\tCreate a new histogram.\n\n\t\tParameters\n\n\t\t----\n\t\tinitial_value : function\n\t\t\tFunction to be called by all the methods.\n\t\tmake_bins : bool\n\t\t\tFor every instance, each histogram has 2\n\t\t\tcolumns. With the call to :func:`set`, the columns are\n\t\t\toriginal values and the return values are the corresponding\n\t\t\tdata, which should be zero.\n\n\t\tSee also:\t<https://docs.python.org/3/library/"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.init = {}\n\t\tfor k,v in self.initial_value.iteritems():\n\t\t\tassert isinstance(k,str)\n\t\t\tsetattr(self,k,v)\n#!/usr/bin/env python\nfrom __future__ import unicode_literals\nimport re\nimport sys\nfrom os import path\nfrom copy import copy\n\nBASE_PATH = path.dirname(path.realpath(__file__))\nsys.path.append(BASE_PATH+\"/../..\")\nimport base\nimport yaml\n\nif True:\n    path_dir = path.dirname(path.abspath(__"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\t# update current context\n\t\tself.context = {}\n\t\tself.bin_list = []\n\t\tself.bins = []\n\t\n\nclass _CurrentBarPlot(wx.Panel):\n\t\"\"\"\n\tThis is a wx.Panel to plot the current bar graph (this is an image). It plots a bar graph with\n\tthe following bar settings:\n\t\tbackground: The background color to be changed with the bar\n\t\tbackground_color: The background color.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor. Calls base class init().\n\t\t\"\"\"\n\t\t#"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\"\"\"\n\n\"\"\"\n\n\n## @license\n##\n## Copyright 2014 Glencoe Software, Inc.\n## All Rights Reserved.\n##\n## This file is part of Glencoe.\n##\n## Glencoe is free software; you can redistribute it and/or modify\n## it under the terms of the GNU General Public License as published by\n## the Free Software Foundation; either version 2 of"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\nclass Plotter(object):\n\t\"\"\"\n\tPlot the image to an array.\n\"\"\"\n\tdef __init__(self, plot_interval=10, interval=250, color='red', *args, **kwargs):\n\t\tself.interval = interval\n\t\tself.colors = {}\n\t\tself.plots = {}\n\t\tself.plot_interval = plot_interval\n\t\tself.color = color\n\t\tself.bin_map = {}\n\t\tself.bins = {}\n\t\tself.hist = {}\n\t\tself.cumulative = 0\n\t\tself.interval_length = interval"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.hist.reset()\n\tdef init(self, initial_value=None, bins_per_value=0):\n\t\t# Initialise.\n\t\tself.hist.init(initial_value, bins_per_value)\n\t\t\n\tdef set_hist(self, initial_value=None, make_bins=True):\n\t\t# Set the initial histogram.\n\t\tself.hist.set_hist(initial_value, make_bins)\n#!/usr/bin/env python\n\nimport requests\nimport urllib2\nimport re\nimport sys\n\n\ndef get_url"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\ndef clear(self):\t\t\n\t\"\"\"\n\tDelete the histogram from the histogram.\n\tClears the histogram by clearing the histogram.\n\t\"\"\"\n\tfor key in self.final_dict.keys():\n\t\tself.final_dict.pop(key)\n\t\t\n\treturn\n\t\t\t\ndef findValue(self,key,value):\n\t\"\"\"\n\tFind a value for the key of the dictionary. If no such key exists,\n\treturn -1.\n\t\"\"\"\n\tvalue = None\n\t\n\tcur_key = self.final_dict.key(key)\n\tif"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n#---\n#----\n#----\n\nif __name__ == \"__main__\":\n\tmain()\n# -*- coding: utf-8 -*-\n# \u00a9 2016 Dell Inc. or its subsidiaries. All Rights Reserved.\n# This software is licensed under the AGPLv3 license found in the LICENSE file.\n\n\"\"\" Models for the :mod:`website.models.base` module. \"\"\"\n\nfrom flask_wtf import Form, BooleanField, HiddenField\nfrom flask_login import current_user\nfrom. import decorators\nfrom..extensions import db\nfrom..models"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\t\t\n\t\n\t\n\tfor value in self._data[self._mask]:\n\t\t\n\t\tif self._debug:\tprint \"Resetting histogram data[%s]\" % value\n\t\t\n\t\tif value is None:\t\t# this is done in a different run\n\t\t\tdata = self._data[self._mask].value\n\t\t\n\t\telse:\t\t# this is done in different run for the same value\n\t\t\tdata = self._data[self._mask].value\n\t\t\n\t\tif self._debug:\t\tprint \"setting histogram %s for %s on %s\" % ( \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t value.name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\nclass MultiDiGraph:\n    __init__ = graphmaker()\n    \n    def __init__(self, *nodes):\n        # Initialize self.edges, making sure node[i]=1 is consistent with\n        # that edges.\n        self.init_edges()\n        \n        # Initialize the graph\n        self.graph = self.init_edges()\n        \n        # Initialize the data structures\n        for n in nodes:\n            self.add_node(n)\n            for i, ndict in enumerate([(n, node[n]) for n in nodes]):\n                # Add edge data\n                if isinstance(ndict, dict):\n                        for k in n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n#\tfrom.graph import to_table\n#\tt = to_table(graph_list=graph_list, header=header)\n\n\n##########################################################################################################################################\n########################################################################\nclass Graph2D(graph.Graph):\n\n\t\"\"\"\n\t:param graph2d:\n\t:type graph2d: Graph2D\n\t\n\t:param str label:\n\t:param list of (x, y) tuples of the graph to be printed.\n\t:param list of items: List of points, as the first element is the first line\n\tthe value is a tuple (``x, y``) on"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tif header == None: header = \"\"\n\t\tdata = []\n\t\twith open(self.filename, \"wb\") as f:\n\t\t    for row in self.table:\n\t\t\t\t\t\t\t# write row at the beginning or end of line\n\t\t\t\t\t\t\t# (starting with *header*)\n\t\t\t\t\t\t\tfor i,p in enumerate(row):\n\t\t\t\t\t\t\t\tf.write(sep + p + separator)\n\t\t\t\tdata.append(row)\n\t\twith open(self.filename, \"at\") as f:\n\t\t    csv_string = csv.writer(f)\n\t\t    csv_string.writerow(data)\n\nclass M"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tfrom.utils import to_csv\n\t\tto_csv(self.points, sep=separator, header=header)\n\n\tdef to_sql(self):\t\"\"\"\n\tConvert graph's point output to CSV.\n\n*`data` dictionary to store the data that are made into the graph\n\nAll edge weights are created using the class **weights** keyword. Thus,\n:class:`Graph` will need to have this method be called with these keyword\narguments. The data will then be exported into the graph via *to_csv*\ninstead of directly by *from_csv**.\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tself.to_csv_writer(sep=\";\")\n\t\tself.to_csv_writer(sep=separator)\n\t\ti = 0\n\t\tfor g in self:\n\t\t\tyield g.get_points_from_points(i)\n\t\t\ti += 1\n\t\tself.to_csv_writer(sep=\",\", header=header)\n\n\tdef to_csv_writer(self, separator=\",\", width=None):\n\t\t\"\"\"\n\t\tCreates a writer to create a CSV from a tuple of points. The\n\t\toutput is split from three points.\n\t\t\"\"\"\n\t\tif not"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\n\t\t# Import data\n\t\timport os\n\t\tfrom data import *\n\t\tfrom tables import *\n\t\t\n\t\t# Create a table\n\t\tnewname = 'LenaGraphToCSV'\n\t\ttable = tables.Table()\n\t\ttable.cols = ['x', 'y', 'width', 'height']\n\t\ttable.rows = [0] * len(table.cols)\n\t\t\n\t\t# Define column to row map\n\t\tcol = table.cols[header]\n\t\trow = [col.y, col.width, col.height]\n\t\tcol.headers.append('x')\n\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tif self._edges is None: return\n\t\tif isinstance(sep, (str, unicode)):\n\t\t\tdelimiter = sep.replace(\"-\",\"_\").strip()\n\t\tif separator not in self._edges: raise Exception, \"Not a valid separation\"\n\t\tself._edges.append(separator)\t# copy out an iterable so we can use csv.\n\n\tdef to_csv_multi(self, fmt, default=_BLANKSTRING):\n\t\t\"\"\"Concatenate graph elements in a string, formatting to CSV.\n\n\t\t*fmt* should be one of the following: [label text, color code"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self._to_csv.to_csv(self.graph_points, header=header)\n\n\tdef _to_csv_str(self):\n\t\t\"\"\"Returns a string containing the graph's points into a string.\n\t\"\"\"\n\t\treturn str(self)\n\n\tdef point_to_string_list(self, point):\n\t\t\"\"\"Convert a point into a comma delimited list of strings.\n\t\t\"\"\"\n\t\treturn \",\".join(point)\n\n\tdef _point_to_string_list(self, point):\n\t\t\"\"\"Convert a point into a comma delimited list of strings"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tself._to_csv_table = _as_csv_table(self._out_path, separator, header)\n\t\treturn self._to_csv_table\n\n\tdef from_csv(self, delimiter=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\"\"\"\n\t\tself._from_csv_table = _as_csv_table(self._out_path, delimiter, header)\n\t\treturn self._from_csv_table\n\n\t@staticmethod\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tfrom. import Output\n\t\tfrom.input import Input\n\t\tfrom numpy import *\n\t\tfrom math import *\n\t\timport collections\n\t\timport csv\n\n\t\toutput = Output()\n\n\t\t# If the object is an output, then it is converted into a CSV.\n\t\tif isinstance(obj, Output):\n\t\t\tobj = obj.value\n\n\t\tif isinstance(obj, (tuple,list)):\n\t\t\tif len(obj)!= 2: return []\n\n\t\t\toutput.start_line(\";\")\n\t\t\toutput.start_line(\"1; \")\n\t\t\tlabel_string = obj[0]\n\t\t\t"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\ttry:\n\t\t\tcoord = self.get_index(coord_name)\n\t\t\treturn coord.index\n\t\texcept:\n\t\t\treturn 0\n\tdef calc_coords(self):\n\t\t\"\"\"\n\t\tGet coord and index for writing.\n\n\t\t@return: numpy array of values to be written (in numpy format)\n\t\t@rtype: numpy ndarray of int\n\t\t\"\"\"\n\t\tcoord = self.get_coord(self.coords[0])\n\t\tif coord is None:\n\t\t\traise KeyError(self.get_coords_msg())\n\t\treturn coord.index\n\n\n     # -----------"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\terr_indices = []\n\t\tfor coord in self.coordinates:\n\t\t\tif coord == coord_name:\n\t\t\t\terr_indices.append(None)\n\t\t\telif coord.coord_type == \"Axis\":\n\t\t\t\tindices = coord.coord_names.index(coord_name)\n\t\t\t\tfor i in range(indices):\n\t\t\t\t\tfor idx in range(coord_name):\n\t\t\t\t\t\tif coord[idx] == \"X\":\n\t\t\t\t\t\t\terr_indices.append(idx)\n\t\treturn err_indices\n\t\n\tdef _load_dataset(self, path):\n\t\t\"\"\"\n\t\tLoad C"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = self.coord(coord_name)\n\n\t\tif coord.has_bounds():\n\t\t\tcoord = coord.bounds\n\t\t\terr_indices, err_values = _get_err_indices(self.coord(coord))\n\t\telse:\n\t\t\terr_indices = None\n\t\t\terr_values = None\n\n\t\treturn (err_indices, err_values)\n\n\tdef _has_expo_bounds(self, coord_name, coord_index, bounds):\n\t\t\"\"\"\n\t\tChecks if bounds is an exclusion coordinate.\n\n\t\tThe exposivet bounds check requires:\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tif coordinate_name in self._coord_errors:\n\t\t\treturn self._coord_errors[coordinate_name]\n\n\t\tif self._map is None:\n\t\t\tself._map = {}\n\n\t\treturn list(map(int, coord_name.split(\".\")))\n\t\n\tdef get_error_indices(self, coord_name):\n\t\t\"\"\"\n\t\tGet a list of error indices corresponding to a coordinate.  This\n\t\tis useful for generating error indices in the `map`\n\t\tmethod.  It should return a list of lists, each of which represents\n\t\tthe errors from a `coord"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoords_list = self.coord_name2index[coord_name]\n\t\tkeys = coords_list.keys()\n\t\tids = []\n\t\tfor idx, c_ in enumerate(coords_list[\"X\"]):\n\t\t\tfor k1, t in enumerate(keys):\n\t\t\t\tif self.coord_name2index.get(idx, k1) == c_:\n\t\t\t\t\tids.append(\"%d\" % len(keys))\n\t\t\t\telif self.coord_name2index.get(idx, k1)!= c_:\n\t\t\t\t\tids.append(\"%d..%d\" % ("}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn _get_indices(self.cubecoord.coord(coord_name))\n\n\tdef _get_err_indices_for_coords(self, coord_name):\n\t\t\"\"\"\n\t\tGet error indices corresponding to a coordinate from some one of the\n\t\tcoords specified.\n\n\t\tParameters\n\t\t-----------\n\t\tcoord_name -> str or None\n\n\t\tReturns\n\t\t-------\n\t\tNone\n\n\t\tSee Also\n\t\t--------\n\t\tget_all_errs\n\n\t\tNotes\n\t\t-----\n\n\t\tIt is assumed that we have all this information, and we just\n\t\tshould check the coordinate's coordinates"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = _parse_coord(coord_name)\n\t\tcoord_names = list_to_arrays(coord.coord_names())\n\t\twith self.use_pandas():\n\t\t\tindices = np.where(coord_names.size > 0)[0]\n\t\treturn indices\n\t\n\tdef _check_timeseries(self, index, series_name, time_name):\n\t\tif series_name is None:\n\t\t\tseries = _calc_series_index(time_name)\n\t\telse:\t\n\t\t\ts = Series(series_name)\n\t\t\ts.coord = self"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\ttry:\t# first attempt to use iris\n\t\t\tresult = iris.coord_systems.GeogCS(self.iris_sample_shape[2]).get_coord(coord_name)\n\t\texcept iris.exceptions.CoordinateNotFoundError:\n\t\t\terr_vals = list(iris.coords.DimCoord(coord_name, dim_coords=1))\n\t\t\terrors = iris.coords.DimCoord(\n\t\t\t\t_coord_err_to_code_coord_name_to_err_vals(code_names=result.codes),\n\t\t\t\t\tunits='dimensionless',\n\t\t\t\t\tattributes="}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn self._coordinates.get(coord_name, None)\n\t\n\tdef _get_coord(self, coord_coord_name):\n\t\t\"\"\"\n\t\tReturns the coordinate corresponding to a coordinate.\n\t\t\"\"\"\n\t\tcoord_coord = self._coordinates.get(coord_coord_name, None)\n\t\t\n\t\tif coord_coord is None:\n\t\t\tself._coords.pop(coord_coord_name)\n\t\t\treturn None\n\t\telif coord_coord.is_unit is None:\n\t\t\treturn self._get_coord(coord_coord_name)\n\t\t\n\t\treturn coord_coord\n\t\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\t# Get all index ranges stored in the grid\n\t\tidx_ranges = self.grids.get_err_bounds(coord_name)\n\t\tif len(idx_ranges) < len(self.grids.grid_mapping):\n\t\t\traise ValueError(\"No gridding error indices to extract from grid\")\n\n\t\t# Return an updated tuple where the keys are equiv, and\n\t\t# the values in the tuple are the error index in that coordinate\n\t\treturn err_indices\n\n\tdef _get_err_array(self, coord_name):\n\t\t\"\"\"\n\t\tGet array of"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.__ctx.error = context\n\t\tself.__ctx.x_low = context.x\n\n\t\tif len(self.axes) == 1:\n\t\t\tself.__ctx.y_low = context.y\n\t\t\tself.__ctx.z_low = context.z\n\t\telse:\n\t\t\tself.__ctx.x = False\r\n\t\t\tself.__ctx.y = False\n\t\t\tself.__ctx.z = False\n\n\tdef copy(self):\n\t\t\"\"\"\n\t\tReturn a copy of this graph, modified in place.\n\n\t\tThe copy is guaranteed to modify the graph in place. This\n\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\ndef _update_error_values(self):\t\"\"\"\n\tUpdate *self.values* with the error values used in the graph.\n\tThe *self.names* is used for debugging; the *self.edges* is\n\tindexed by the respective names of the vertices of this graph.\n\tThe *_errors* are only used when a graph is to be modified.\n\tThe result is also updated after each update.\n\t\"\"\"\n\t\n\t\n\tif self.values and self.nodes:\n\t\tself._errors = _dict({})\n\t\tself._values = []\n\t\tfor n in"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\t_clear_attrs(graph)\n\t\tgraph.delete_nodes_by_label().update(\n\t\t\tdict.fromkeys(self._nodes, None)\n\t\t)\n\t\tfor attr in \"E,t,error_E_low\":\n\t\t\tif attr in self._nodes.get(graph, []):\n\t\t\t\tdel self._nodes[attr]\n\t\tif hasattr(graph, \"add_nodes_from\"):\n\t\t\tself._update_node_list(graph.add_nodes_from(graph))\n\t\t\tif \"x\" in self._nodes:\n\t\t\t\tdel self._nodes[\""}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tcontext.old = deepcopy(context)\n\t\tdel context.old#del self.to\t# make it available to destruction()\n\n\t\tdef _new_from_table(self, table, *fields, **kwargs):\n\t\t\ttable_id = table[0]\n\t\t\tkeys = [\"error\"]\n\t\t\tdefault_values = [0.0]\n\n\t\t\t# make the old and new entries of\n\t\t\t# the new table, in order of increasing index.\n\t\t\t# If both the table and the table id exist,\n\t\t\t# this will not be recreated.\n\t\t\ttry:\n\t\t\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\t\n\tdef __init__(self, filename):\n\t\t# filename has to be a file-like object with data\n\t\t# \tat index 0.\n\t\tself.fname = filename\n\t\tf = open(self.fname,'rb')\n\t\tself._reader = csv.reader(f, dialect=\"excel\")\n\t\tself.row = []\n\t\tself.col = []\n\t\tfor line in self._reader:\n\t\t\t# print \"line:\",self.row\n\t\t\tself.row.append(line[0])\n\t\t\tself.col.append(line[1])\n\t\tself"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\nclass ToCSV(FlowBase):\n\t\"\"\"\n\tGenerate a CSV file (such as a csv file for flow) and add column\n\tnames and row information in *ctx* to every subcontext.\t\n\tThe subcontext dictionary will contain values {name[label]}\n\tthat correspond to any column in *ctx*.\n\tAll of these values will be removed first.\n\t\"\"\"\n\tdef __init__(self, *ctx):\n\t\tFlowBase.__init__(self, flow_name=ctx.scope.get_name('tocsv'))\n\n\tdef run(self, graph):"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tself.value = _Graph(context)\n\t\tself.value.write_graph(self)\n\t\tdef toString(index):\n\t\t\tvalue = self.value.value\n\t\t\tif index == 1 or index == 2:\n\t\t\t\tvalue = \"t\"\n\t\t\telif index == 3 or index == 4:\n\t\t\t\tvalue = \"error\"\n\t\t\treturn value\n\t\tself.value.index = _int_or_bool(toString)# -*- coding: utf-8 -*-\n# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tfrom..graph import DiGraph, Graph, GraphError\n\t\tself.error = {}\n\t\tif \"error_name\" in context:\n\t\t\t_update_context(context[\"error_name\"])\n\t\tif \"x_low\" in context:\n\t\t\t_update_context(context[\"x_low\"])\n\t\tif \"y_low\" in context:\n\t\t\t_update_context(context[\"y_low\"])\n\t\tif \"z_low\" in context:\n\t\t\t_update_context(context[\"z_low\"])\n\n\t\tself.graph = DiGraph(graph_type="}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tif isinstance(context, dict) and len(context) == 0:\n\t\t\treturn context # this graph doesn't contain any elements.\n\n\t\tself._update_context(context)\n\t\tfor key in self._get_subcontext(context):\n\t\t\tself._insert_row(context)\n\t\n\tdef to_csv(self, filename, context=0, delimiter=\",\"):\n\t\t\"\"\"\n\t\tWrite :class:`.ToCSV` structure to *filename*. This\n\t\tmay be a data dictionary or an iterator of (row,col) pairs.\n\n\t\t*context* is a :class:`."}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tfor index in self.values:\n\t\t\tvalues = self._get_next_key(context, \"error_E_low\", indices.pop(index))\n\t\t\twith self._node_cache.get(index, None):\n\t\t\t\tself.node[index].values = values\n\t\t\twith self._node_cache.get(index, None):\n\t\t\t\tvalues = self._get_next_key(context, \"error_t\", indices.pop(index))\n\t\t\t\twith self._node_cache.get(index, None):\n\t\t\t\t\tself.node[index].values = values\n\t\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\t(xmin, ymin, xmax, ymax) = bins\n\t\tif abs(xmin - xmin) >= 1.0*edges:\n\t\t\tbins = np.linspace(xmax,xmin,bins)\n\t\tfor i in range(1,len(bins)-1):\n\t\t\tnew_bins = bins[i]\n\t\t\tnew_bins = np.hstack((bins[i-1], bins[i]))\n\t\t\tif abs(new_bins - new_bins) > 1e-8:\n\t\t\t\tprint (\"Warning\", \"The integral is too large. \" + \\\n\t\t\t\t\t\"It should be"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\t# TODO check for format of bins.\n\t\t\n\t\tif bins is None: bins = arange(bins.size)\n\t\t\n\t#\n\t# \tGet bin density on each side of the histogram.\n\t#\n\t# \t@param bins: a 1D array. Each bin contains the values for which\n\t# \t\ta point is to be associated with the *edge* at a point\n\t# \t@type bins: a 1D array. Each bin has length equal to the number of\n\t#\t\tnodes in *edges*.\n\t#\n\t# \t@"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tbins = np.asfarray(bins)\n\t\tedges = np.asfarray(edges)\n\t\tif len(bins.shape) == 1:\n\t\t\tbins = np.array([0., 0.])\n\t\t# Convert each bin to bin\n\t\tbins = (float(bins[None, :]) / np.asfarray(bins.ptp())).astype('float64')\n\t\tbin_mean = 0.5 * (1. + np.sum(bins**2))\n\t\tbin_var = np.sqrt(2 * np.pi * np.log2(bin_"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tbins.set_array_size(1)\n\t\n\t\th = bins.get_array_size() / bins.get_height()\n\t\t\n\t\txmin = max((x - h) for x in edges)\n\t\txmax = min((x + h) for x in bins)\n\t\t\n\t\tx = max((x - h) for x in bins)\n\t\ty = bins.get_height() / bins.get_width()\n\t\ty = int(y)\n\t\tif y < 1: y = int(y)\n\t\tif y > 0:\n\t\t\ty, ymax = y+"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\traise NotImplementedError(\"Integrating integral not supported yet\")\n\n\tdef get_limits(self):\n\t\t\"\"\"\n\t\tReturns the *limits* of the histogram\n\t\t:rtype: tuple of float\n\t\t\"\"\"\n\t\traise NotImplementedError(\"Integrating integral not supported yet\")\n\n\tdef get_limits_at(self, value):\n\t\t\"\"\"\n\t\tReturn the *limits_at* of the histogram's range at that value\n\t\t:param value: a sequence. It is assumed to be floating.\n\t\t:rtype: tuple of float\n\t\t\"\"\"\n\t\traise NotImplementedError(\"Integrating integral not"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tfor i in range(bins.shape[1]):\n\t\t\tbin = bins[:, i, 0]\n\t\t\tedges[:, i] *= bin**((edges[:, i] - edges[:, 0]) ** 2).sum()\n\t\t\tbins[:, i] -= edges[:, i, 0]\n\t\treturn edges\n\n\n\tdef scale(self, x, y):\n\t\t\"\"\" Compute the scaling factor for a histogram\n\nScales the given x and y axes. The return interval is\nin the following form:\n\n.. plot::\n.. image:: /home/sjalfoord/Documents/Dog/"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tdata = zeros(len(bins))\n\t\tfor b,w in edges:\n\t\t\tdata[b] = histogram(data[b], w, bins[w][0], edges[w][0])\n\t\treturn data\n\n\tdef plot(self, x,y,data=None,title=None):\n\t\treturn self.plot_hist(x,y,data,title)\n\t\t\n\tdef plot_hist(self, data, color=None, title=None, **options):\n\t\t\"\"\"\n\t\tPlot a histogram.\n\n\t\t**Plotting**\n\n\t\t:type data:"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\t\n\t# the following loop computes the scale for a faceted histogram and returns the normalized scale,\n\t#   or a Hist object, if any. So, in the following:\n\t#\n\t#\tscale = histo.scale()\n\t#\n\t#\tif isinstance(bins, list):\n\t#\t\tbins = np.asarray(bins, np.int)\n\t#\telse:\n\t#\t\tbins = np.asarray([bins])\n\t#\n\t\n\t#     # get the histogram\n\t#     hist = histo.histogram(bins) #"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\tfor edge in bins:\n\t\tyield integral(edge[0], edge[2]), edge[1], bins[edge]\n\n\n\ndef integrate(x, edges, **kwargs):  # pragma: no cover\n\t\"\"\"\n\tCompute a scale given a tuple of the form (min, max), where \n\tmin and max are limits of bin edges.\n\tReturn a (min, max) tuple of the format (min, max).\n\t\"\"\"\n\n\tbins = [(len(x)-1) if x[0]!= '-' else (len(x)-1, x[0"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\tif type(bins)==type(list):\n\t\t\tbins=[bins,bins]\t\t\t#make list of values\n\t\tn,m = len(bins), len(edges)             #the number of edges\n\t\tif n==0: n='None'\n\t\tif n==1: n='1'\n\t\tbin = np.zeros((n,m,1))\t#make 2d data\n\t\tfor k in range(1,n):\n\t\t\tfor i in range(m):\n\t\t\t\tbin[i][k] = edges[i][k][i]\t\t\t#create a"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\n\t\ndef is_is_array_element(elem):\t\t\t\"\"\"\n\tA boolean test whether *elem* is a ArrayElement or any other\n\telements.\t\"\"\"\n\t\n\t\ndef is_list_element(elem):\t\t\t\t\"\"\"\n\tA boolean test whether *elem* is a ListElement or any other\n\telements.\n\t\"\"\"\n\t\ndef is_element_of_type(elem,type_map={}):\t\t\t\"\"\"\n\tA boolean test whether *elem* must have a particular type.\n\t\"\"\"\n\t\ndef is_list(elem,type_map={}):\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor item in seq:\n\t\t\tif item not in FillRequestSeq:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tdef add(self, item):\n\t\tif self.is_fill_request_seq(item):\n\t\t\t\tself._fill_request_seq_map[item]=index(item)\n\n\tdef remove(self, item):\n\t\tif self.is_fill_request_seq(item):\n\t\t\t\tself._fill_request_seq_map[item]=len(self._fill_request_seq_map)\n\n\tdef sort(self):\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn hasattr(seq, \"__getitem__\")\n\n\tdef is_source_sequence(seq):\t\"\"\"\n\tTest whether *seq* contains at least one such,\n\tand it is not a Source sequence.\n\tReturn true only if it is a SourceSequence element.\n\t\"\"\"\n\t\treturn hasattr(seq, \"__iter__\")\n\n\tdef get_fill_request_seq(seq):\n\t\tif type(seq) is not type(()):\n\t\t\t# Must have an elements of the right class,\n\t\t\t# so they can have an all_inputs element\n\t\t\treturn False\n\t\treturn not"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn isinstance(seq[0], FillRequestSeq)\n\n\treturn isinstance(seq[-1], FillRequestSeq)\n\n\ndef is_fill_request_name(name):\n\t\"\"\"\n\tTest whether *name* can be converted to a FillRequestName.\n\nTrue only if it is a FillRequestName element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn name == \"<fill request name>\"\n\n\treturn isinstance(name, Source) and name.code == SourceCode.PRCARD\n\n\t# For any error that happened\n\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif seq is None or not any(x!= 0 for x in seq):\n\t\t\treturn False;\n\t\treturn seq[0].is_fill_request()\n\t\t\n\tdef read_fill_request_data(self):\n\t\t\"\"\"\n\t\tReads *nrecords* of FillRequest,\n\t\twhich are arrays.\n\t\t@return: (array, array of nrecords), where nrecords is\n\t\t  the number of nrecords's.\n\t\t\"\"\"\n\t\tdata = [];\n\t\t\n\t\tfor row in os.db.read('fill_request',\n\t\t\t\t\t\t\tself.db_config[\"db_session\"],"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif not isinstance(seq, FillRequestSeq):\n\t\t\treturn False\n\t\treturn True\n\n###\n#\n#\tclass Literals:\n#\n\t### Note that this is not a wrapper; it just prints a tuple of tuples in\n\t# C{code}, and only prints its names.  For example, the method prints\n\t# the code in the following order:\n\t###\n\t#   class C${object}.C:\n\t###\n\t#     pass\n\t###\n\t#   class C${object}.x:\n\t#     pass\n\t###\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\n\tif isinstance(seq, Source):  # Single Source sequence\n\t\tif seq.source is None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(seq, Element):  # Element with an ID\n\t\treturn seq.id is not None\n\telse:\n\t\traise NotImplementedError(\"Can't convert a FillRequest Seq, got type: {}\".format(seq))\n\nclass SourceElement(object):\n\n\tdef __init__(self, element_id):\n\t\t# element_id is Unique ID to store the source with\n\t\tself.element_id = element_"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\ndef is_fill_request(seq):\t\"\"\"\n\tTest whether *seq* cannot be converted to a FillRequest\n\telement or contains at least one such,\n\tbut not all such in the type of sequence.\n\t\"\"\"\n\ndef is_source(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a Source element.\n\t\"\"\"\n\ndef is_sloppy(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a sloppy element.\n\t\"\"\"\n\ndef is_fill_element_seq(seq):\t\"\"\""}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\t#assert(len(seq) >= 4)\n\t\t#assert(len(seq) >= 4 + 8)\n\t\treturn seq[0] <= 5\n\n\tdef __init__(self, seq, req):\n\t\tself.req = req  # A fill request\n\t\tself.seq = seq  # An item to be converted.\n\n\n\tdef has_fill_sequence(self):\n\t\treturn self.seq in self.req.elems\n\n\tdef __str__(self):\n\t\treturn \"fill request #%s: %r\" % (self.req.id, self.seq)"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor elem in seq:\n\t\t\tif elem.__class__ == FillRequest.FillRequestSeq:\n\t\t\t\treturn True\n\treturn False\n\n\n@returns(Boolean)\n@arguments(fill_request, fill_element, fill_sequence, fill_request_seq, fill_element_seq)\ndef check_fill_request_seq_equal(fill_request_seq, fill_element, fill_sequence):\n\tassert isinstance(fill_request_seq, FillRequestSeq)\n\tassert fill_sequence.__class__ == FillRequestSeq\n\tassert fill_element.__class__ =="}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif not (obj.hasChildren()):\n\t\t\treturn False\n\t\tfor s in obj.children():\n\t\t\tif s.has_key('fill'):\n\t\t\t\treturn s['fill']\n\t\treturn True\n\n\tdef validate_input(self, obj, obj_type, obj_name, obj_data):\n\t\tif (obj_type!= 'application/postscript'):\n\t\t\treturn False\n\t\tinput_type = obj.getAttribute(obj_type, 'input')\n\t\tif isinstance(input_type, basestring):\n\t\t\treturn False\n\t\tfor i in range(0, len("}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif obj:\n\t\t\treturn obj in globals()[\"FILLANS\"] and not obj in globals()[\"FILLANS\"][obj]\n\t\telse:\n\t\t\treturn False\n\n\ndef get_fill_mode_element(fillmode):\n\t\"\"\"\n\tGet fill mode element.\n\t\"\"\"\n\tsel_mode = fillmode.split(\":\")\n\treturn sel_mode[0] if sel_mode in globals()[\"FILLANS\"] else \"\"\n\nfill_mode_element = get_fill_mode_element\n\n\ndef get_fill_params_el(fills, options):\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif hasattr(obj, 'fill'):\treturn obj.fill()\n\t\telif hasattr(obj,'request'):\treturn obj.request()\n\t\telif hasattr(obj, 'fill_el'):\treturn obj.fill_el()\n\tdef get_fill_request(obj):\treturn make_fill_request(obj)\n\tdef fill_el(obj):\treturn make_fill_request(obj)\ndef is_read_request_el(obj):\t\"\"\"\n\tObject contains read-only methods'read' and'request' and not inherited.\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\timport inspect\n\t\tif inspect.isclass(obj):\n\t\t\t# we're making a new object:\n\t\t\treturn isinstance(obj, _Fill) and obj._fill\n\t\t\t\t\t\t\t# We do not want to add this to the result object.\n\t\t\treturn obj._fill\n\t\telif inspect.isfunction(obj):\n\t\t\treturn isinstance(obj, _Fill) and obj.fill\n\t\t\t\t\t# If _fill is a method, call with fill=obj.fill()\n\t\t\t\t\t# Then we must get the object passed in as argument.\n\t\t\t\t\t# Then it must return a new fill object.\n\t\t\t\t\t#"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn False\n\n\tdef check_fill_request_el(self, elem):\t\t\t\"\"\"\n\tCheck whether we have an element of this type.\n\tIf it is already a fill request with the given element, update the element_el_fill.\n\tIf it is not, insert it and insert into the els, if there's no fill request\n\tor update the element_el_fill or insert the element.\n\t\"\"\"\n\t\telem_element = elem.find('fill')\n\t\tif elem_element is None:\t\t# This is an ELEMENT; so we're"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tis_fill_method = (obj.fill(fill_request) is not None)\n\t\tfill_method = True if obj.fill(fill_request) else False\n\t\trequest = obj.request - 1\n\t\tif obj.request > 0 and is_fill_method:\n\t\t\tobj.request = 0\n\ndef apply_on_object(obj, check_object_type, check_instance_validity = True):\n\t\"\"\"\n\tApply object to the given object so that all input attributes are \n\tas they apply.\n\n\t:param obj: the object to be applied"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.has_key('fill') and obj.has_key('request')  \t\t\t\n\tclass BUFFER_ELEMENT(object):\t\"\"\"\n\tA class that holds chunks of elements. Each element is a list of strings (each string).\n\tThe list cannot contains any whitespace between the elements.\n\tEach chunk is returned as a string.\n\t\"\"\"\n\tdef __init__(self): \n\t\tself._chunks = []\n\t\tself.__init()\t\n\t\n\t@property\n\tdef chunks(self): return self._chunks\n\t\n\t@property\n\tdef __init__(self"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\t# If an object has no attribute xpath, don't check\n\t\t# this path\n\t\tif not hasattr(obj, 'xpath'):\n\t\t\treturn True\n\t\tif obj.xpath is None:\n\t\t\treturn False\n\n\t\tif obj.operation == 'fill':\n\t\t\treturn obj.xpath == 'fill'\n\n\t\treturn False\n\n  \nclass _Root:\n\t\"\"\"\n\tRepresents an object that contains an XPath object, it's\n\tused in the XS2D model.\n\n\tThe XS2D tree is used to implement a simple xpathing of\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn getattr(obj, 'fill', None) is not None\n\t_is_fill_request_el = staticmethod(is_fill_request_el)\n\tdef fill(obj):\n\t\tif not is_fill_request_el(obj):\n\t\t\traise TypeError('Fill method cannot be called on an object with type %s. Type %s has no fill method, and cannot be called on an object with type %s'%(obj.fill, type(obj), type(obj.fill)))\n\t\treturn obj.fill()\n\tdef request(obj):\n\t\tif not is_"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tdef get_fill_request(self):\n\t\t\t\"\"\" Returns the current fill request.\n\t\t\tCreates a new request.\n\t\t\t\"\"\"\n\t\t\treturn Object(get_request_el('fill request', self.f['name'].value), self)\n\n\t\tdef get_request_name(self):\n\t\t\t\"\"\" Returns the name of the request.\n\t\t\tCreates a new request.\n\t\t\t\"\"\"\n\t\t\treturn Object(get_request_el('request name', self.f['name'].value), self)\n\n\t\tdef get_request_description(self):\n\t\t\t\"\"\" Returns the description of the request"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.__run\n\tdef __eq__(self, object):\t\"\"\"\n\tSame as object == object.\n\tIf'sys.executable' is equal 'not os.path.exists()', \n\tthis is assumed.\n\t\"\"\"\n\t\tif callable(object) and object.__executable!= object.__path__:\n\t\t\treturn False\n\t\tself.check_obj_obj_exists(object)\n\t\treturn True\n\tdef check_obj_obj_exists(self, obj):\n\t\t\"\"\"\n\t\tObject contains executable method 'check_obj_obj_exists'.\n\t\"\"\"\n\t\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif isinstance(obj, str):\n\t\t\tobj = getattr(obj, obj)\n\t\treturn not obj.startswith(\"/bin/\") \n\nclass AptWrapper:\n\tdef __init__(self, fv):\n\t\tself.fv = fv\n\t\ttry:\n\t\t\tself.v = int(self.fv.read(\"v\"))\n\t\texcept Exception as e:\n\t\t\t#print e\n\n\tdef write(self, f):\n\t\tf.write(self.v)\n\n\tdef read(self, f):\n\t\tf.seek(self.fv.seek(0,"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\t__slots__ = 'binpath', 'bin', 'path', 'base'\n\t\tdef _exec(self, *argv, **kwargs):\n\t\t\tif not hasattr(self, 'run') or 'run' not in dir(self.run) or not self.bin:\n\t\t\t\traise TaskError('Cannot execute non-running method %r for %s' % (type(self).__name__, self))\n\t\t\tself._exec_kwargs(*argv, **kwargs)\n\t\t\tself._run_args(*argv, **kwargs)\n\t\t\treturn self.bin\n\n\t\tdef _run_args"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn str(obj)\t# 'run'.\n\t# A programmer's executable method 'run' must return a non-zero code value, which includes\n\t# a flag for whether the object is executable and whether the programmer process execution or error\n\t# occurred. The script is runnable using 'run'.\n\tdef __str__(self):\n\t\treturn ''.join( (self.__class__.__name__, __id__, str( obj )) )\n\t# A programmer's command method 'run' must return a non-zero code value, which includes\n\t# a flag for whether"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn getattr(obj, \"run\", None)\n\n# The main methods below are for tests.\n\n\nclass ExecuteTestCase (testcase.TestCase):\n\n\tdef setUp(self):\n\t\tsuper().setUp()\n\t\tself.config_file = os.path.normpath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, os.pardir, os.pardir, 'config.ini'))\n\t\tself.log = logging.getLogger('kat')\n\n\t\tself.debug = False\n\t\tself.result = None\n\n\tdef test_run("}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.__class__.__name__[5:] == \"run\"\n\n\t@property\n\tdef is_debug_check(self):\n\t\t\"\"\"\tObject contains debug checking.\n\t\tTrue by default.\n\n\t@rtype: bool\n\t\"\"\"\n\t\treturn not self.command:\n\t\treturn self.output or self.error\n\t\ttry:\n\t\t\ts = self.source_of_run\n\t\texcept AttributeError:\n\t\t\tlogger.critical(\"No source for run '%s'\" % self.command)\n\t\t\ts = self.error\n\n\t\treturn True if s else False\n\n\tdef cmd"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif type(obj) == str:\n\t\t\treturn True\n\t\telif type(obj) == int:\n\t\t\treturn obj in self.exe_map\n\t\telse:\n\t\t\treturn True\n\t\n\tdef is_exe_run(self, fname):\n\t\t\"\"\"\n\t\tCheck whether a executable file has the specified name in the\n\t\tpath. If the file does not exist, returns False.\n\t\t\n\t\tArguments:\n\t\tfname -- the name of the executable file.\n\t\t\n\t\tReturns:\n\t\tTrue if it has the specified name. False otherwise.\n\t\t\"\"\"\n\t\tif not self.exe_map"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn True\n\ndef is_run_exe(exe):\n\treturn exe.startswith(\"w\")\n\ndef is_run_in_install_dir(exe):\n\treturn exe.startswith(\"i\")\n\ndef is_run_or_run(exe):\n\t''' Is exe executable for 'c' installed? '''\n\treturn os.path.exists(exe) and exe.startswith(\"c\")\n\ndef is_run_path_exe(path):\n\t''' Is exe executable path (not relative) for 'c' installed? '''\n\treturn path.startswith"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.run_method == 'run'\n\tdef get_executable_method(obj):\t\"\"\"\n\tObject contains executable method 'get_executable'.\t\"\"\"\n\t\treturn getattr(obj, 'executable', None)\n\t\t#\n\t#\tdef evaluate(self, obj):\t\"\"\"\n\t#\t\tFunction evaluates object with its name and the result.\n#\t\t\"\"\"\n\t#\t\tif not self.filename_is_valid(self.config.exe_name).strip() :\n\t#\t\t    Logger.warn(\"Error: invalid executable file name '%s"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn True\n\tdef is_exe_on_build(obj):\t\"\"\"\n\tObject contains executable method 'exe on build'.\t\"\"\"\n\t\treturn True\n\tdef is_binary(obj):\n\t\treturn obj in ['lal', 'lua', 'lua3']\n\tdef is_module(obj):\t\"\"\"\n\tObject contains executable method'module'.\t\"\"\"\n\t\treturn True\n\n    # A bit of magic.  I have no idea what the object is.  This\n    # method is a little bit of a hack, but it's pretty dumb to have a\n    #"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tb = get_child_by_attribute(obj, \"fill\")\n\t\tret = b.fill_and_compute(obj)\n\t\treturn ret\n\t\n\tdef get_data_shape(self, obj):\t\"\"\"\n\tGet data dimensions, dimensions in a 2D array and return tuple (nx, ny).\n\t\"\"\"\n\t\tb = get_child_by_attribute(obj, \"data\")\n\t\treturn b.data.shape\n\t\n\tdef get_data_time(self, obj):\t\"\"\"\n\tGet data time.\n\t\n\tReturns (x,y),(x,"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif is_eapi(obj):\n\t\t\treturn 'fill' in obj.ecode\n\t\telif is_cinclude(obj) and not obj.is_c_class and is_c_attribute(obj):\n\t\t\treturn 'compute' in obj.ecode\n\t\telif is_idl(obj):\n\t\t\treturn 'idl' in obj.ecode\n\t\telse:\n\t\t\treturn False\n\treturn is_fill_compute_el\n\n\ndef check_type_check(obj, node):\n\t# Note node might not be an instance of 'domdom.BlockType'"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj.obj_type==obj.obj_ref and obj.obj_ref[obj.key]\n\n\tdef do_nothing(self):\n\t\tpass\n\nclass build_files(Command):\n\t\"\"\"\n\tbuild a single file in a single output build directory\n\t\"\"\"\n\thelp = \"build single output build files\"\n\n\tdef execute(self, paths):\n\t\t\"\"\"\n\t\tbuild the list of source and destination files required to build a single file\n\t\t\"\"\"\n\t\tsrc = Utils.to_list(paths)\n\t\tdst = Utils.to_list(paths"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn 0\n\n\tdef run_testcases(self, obj):\n\t\t\"\"\"\n\t\tRuns tests in self.tests.  May throw a py.test exception if\n\t        test has run, and then raises an exception if it's\n\t        not.  This function is used to test the Python\n\t        standard case.\n\n\t\tReturns tuple(error_code, testCases) for all the tests in\n\t        self.tests.  Error_code should be 1, if no errors are\n\t        occurred, or a 0 if any other error code is returned.  When an\n\t        error occurs, error"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# In order to support python 2, python 3 does not have a __pow__ method (see Python3's\n\t# \"Pow\" method).\t\n\t\treturn obj.__pow__(expr)\n\n\tdef is_boolean(obj):\n\t\t\"\"\"Return True if the object is a 'boolean' string. In contrast, Python 2.1 returns true for 0 <= o is a 'boolean', i.e. False for a string which is treated as an\n\t\tinteger and False for 0. For python >= 2, use the 'is_int' alias or 'is_none' alias to indicate that"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn isinstance(obj, objects.fill.fill.compute.node)\t#!/usr/bin/env python\n# -*- coding: utf8 -*-\n\n###############################################################################\n#  Copyright Kitware Inc.\n#\n#  Licensed under the Apache License, Version 2.0 ( the \"License\" );\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn ((not is_builtin(obj) and \n\t\t\tis_not_none(obj) and not is_builtin_or_not_none(obj) and obj.name == 'fill' and is_builtin_or_not_none(obj)\n\t\t\tand obj.op == 'compute'))\n\t\n\tdef has_write_access(self, target, source):\n\t\t\"\"\"\n\t\tTarget has write access to all or none of 'Source'. \n\t\t\"\"\"\n\t\tfor f in self.files:\n\t\t\tif f.name == source:\n\t\t\t\treturn 1\n\t\t\tif f."}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn obj is not None and obj!= __uci_obj_or_class\n\n_is_fill = not hasattr(unicode_, 'fill')\n_compute = not hasattr(unicode_, 'compute')\n\n# Copyright (C) 2013-2016 Red Hat, Inc.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn getattr(obj, 'fill', True) and getattr(obj, 'compute', False)\n\tisarray = is_fill_array\n\n#\n# Binary comparisons: These are the only tests that are\n# used internally by the'sort'. It doesn't take anything though.\n# The following tests are meant to check that the values are\n# converted to int/long, so that int values will be rounded up to\n# nearest integer.\n#\n\nclass TestBinaryOps:\n\tdef is_not_same_float_values(self, b):\n\t\tfrom nose."}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif not hasattr(obj,'_method_get_fill_compute'): obj.__init__(obj)\n\t\tif not hasattr(obj,'_method_get_compute_size'): obj.__init__(obj)\n\t\tif not hasattr(obj,'_method_get_fill_size'): obj.__init__(obj)\n\nclass _c_compiler():\n\t\"\"\"\n\tCompile a node tree.   This will build a single file system in order to\n\tprocess results from a compilation phase.  This is called in order to\n\tprocess all compiled objects.  This class also collects a function"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn _intersection(d1, d2, level)\n\tdef count(d1, d2):\n\t\t\"\"\"Return the number of values in the set *d1*, *d2*, in order.\n\t\tFor example::\n\n\t\t\t>>> from collections import Counter\n\t\t\t>>> c = Counter()\n\t\t\t>>> c.update(\"cat\", 3)\n\t\t\t>>> c\n\t\t\tCounter({(\"c\", 3): 3, (\"a\", 3): 1})\n\t\t\t>>> d = Counter(d1)\n\t\t\t>>> d\n\t\t\tCounter({(\"b\", 1): 1, (\"a\", 1):"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif level!= -1:\n\t\t\treturn dict((k,d1[k]) if (k in d1) else d1.__dict__)\n\t\treturn d1\n\t\n\tdef difference(self, *d1, **d2):\n\t\t\"\"\"\n\t\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif not isinstance(d1, dict):\n\t\t\t\traise TypeError('expected a dictionary; received %s.' % d1)\n\t\tif not isinstance(d2, dict):\n\t\t\t\traise TypeError('expected a dictionary; received %s.' % d2)\n\t\tif not isinstance(d1._dict, dict):\n\t\t\t\traise TypeError('expected a dictionary; received %s.' % d1._dict)\n\t\tif not isinstance(d2._dict, dict):\n\t\t\t\traise TypeError('expected a dictionary; received %s.' % d2._dict)\n\t\tif level >= 0:\n\t\t\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\td1 = copy.deepcopy(d1)\n\t\td2 = copy.deepcopy(d2)\n\t\tresult = {}\n\t\tfor key in d1:\n\t\t\tif not(key in d2):\n\t\t\t\tresult[key], d2[key] = -1.0, 0.0\n\t\t\telse:\n\t\t\t\tresult[key], d2[key] = -d2[key], 0.0\n\t\tfor key, value in d1.items():\n\t\t\tif not(key in d2 and not(value in d2)):\n\t\t\t\t# if key is not present"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif d1 is None:\n\t\t\td1={}\n\t\tif d2 is None:\n\t\t\td2={}\n\t\td1.update(d2)\n\t\treturn d1\n\ndef difference_view(*args, **kwargs):\n\t\"\"\"\n\tReturn a new dictionary with items from two dictionaries.\n\n*p* can be a number, int, or str. If it is a number, it will be interpreted as a\n  list of keys representing the values.\nIt will be converted to a regular python dictionary representing the\nresults of a call.\n\nThe difference is stored in a"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif level not in range(0, 4) and level!= -1:\n\t\t\treturn {}\n\t\td1 = d1.copy()\n\t\td1.update(d2)\n\t\treturn d1\n\telif level == -1:\n\t\td = d1\t\n\telse:\n\t\treturn d\n\ndef subdict(*d):\n\t\"\"\"\n\tRecursively divisible dictionary.\n\n*subset_dict* is a dict to use for subdividing nested dictionary trees.\n\n*returns* a list of dictionaries representing the new divisible dictionaries.\nThe resulting dictionary"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn dict((k, a[level - 1]) for (k, a) in iteritems(d1))\n\t\t\n# Above is an alias for a dictionary\nDifference.keys = [('a', 'd1.a'), ('b', 'd2.b'), ('_a_d', 'd1.a')]\n\nclass OverlapError(Exception):\n\tpass\n\nclass NotEqualError(OverlapError):\n\tpass\n\nclass NotGreaterError(OverlapError):\n\tpass\n\nclass MatchError(OverlapError):\n\tpass\n\nclass ExactlyEqualError("}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n__all__ = ('disjoint', 'intersection')\n\n\ndef _dict_merge(a, b):\n\tif isinstance(a, dict):\n\t\tf = {}\n\t\tfor k, g in a.items():\n\t\t\tif g is b:\n\t\t\t\tcontinue\n\t\t\tf[k] = g.copy()\n\t\treturn f\n\treturn _dict_merge(a, b)\n\ndef intersection(a, b, level=-1):\n\t\"\"\"\n\tReturn a dictionary with keys within each pair of values in a that do\n\tnot appear in any of *a*.\n\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\ttry:\n\t\t\td1.__dict__.update(d2.__dict__)\n\t\texcept AttributeError:\n\t\t\td2.__dict__.update(d1.__dict__)\n\t\treturn d1, d2\n\ndef _isdict(d):\n\t\"\"\"\n\tReturn if the provided dictionary is a dictionary.\n\n\t@returns bool -\n\t\"\"\"\n\tif d:\n\t\treturn 1\n\treturn 0\n\ndef _merge(dictionaries, key):\n\t\"\"\"\n\tAdd a value to the given dictionary and merge the input dictionaries with\n\tgiven *key*.\n\n\t.. versionadded"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif not level in d1:\n\t\t\td1[level] = deepcopy(d2)\n\t\telse:\n\t\t\td1[level].update(d2[level])\n\treturn d1\n\ndef intersection(*iterables):\n\t\"\"\"\n\tHelper function to quickly find all items that belong to a set of\n\telements.\n\tAll *iterables* must have exactly 4 elements.\n\t\"\"\"\n\tit = iter(iterables)\n\tfor elem1 in it:\n\t\tfor elem2 in it:\n\t\t\tresult = False\n\t\t\tfor k,v in elem"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tx,y = coord\n\t\tif y < self.height:\n\t\t\ty += self.length\n\t\tif y > self.width:\n\t\t\ty -= self.width\n\t\tif coord >= self.location.x < self.location.width:\n\t\t\tx += self.location.width\n\t\tif y < self.location.y:\n\t\t\ty += self.location.height\n\n\t\thistogram = Grid(x,y,self.location)\n\t\tfor x in xrange(x,x+2):\n\t\t\thistogram.line(x,y)\n\t\treturn histogram.draw(("}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif coord in self.data: return\n\t\tself.data.append(self._fill(coord, weight))\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet histogram at *key* with value *value*.\n\n\t\tArgs:\n\t\t\tkey (str): key to set to\n\t\t\tvalue (float or int): value to set to values\n\t\t\t(float or int): default value to use if *key* is not found\n\t\t\"\"\"\n\t\tself.data[key] = value\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\treturn self.createHistogram(0.0, coord, weight)\n\t\t\n\tdef createHistogram(self, data, coord, weight):\t\"\"\"\n\tCreate a histogram object based on coordinates.\n\t*data* is a numpy array of dimension (2) holding the data of\n\teach dimension, of dimension (data.shape[0] should be the number of\n\tdata points to draw, data.shape[1] should be the number of data\n\tpoints to draw each dimension, of dimension (data.shape[2] should\n\tbe the total number of data points to draw -- the"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif self.closed: raise RuntimeError(\"The histogram needs to be closed.\")\n\t\t\n\t\tx = self._coord[coord]\n\t\tl = self._dim[coord]\n\t\t\n\t\tfor w in range(l):\n\t\t\tfor i in range(self._dim[coord]): break\n\t\t\t\t\n\t\tif not self._closed:\n\t\t\tself._data[x][weight*(self._dim[coord])]) += self._data[x][weight]\n\t\tself._data[x][weight*(self._dim[coord]) + coord] = self._data[x][weight*(self._dim[coord])+coord"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif DEBUG:\n\t\t\tprint \"fill(coord=%f weights=%d): %s\" % (coord, weight, coords)\n\t\tif weight > 0:\n\t\t\tif DEBUG:\n\t\t\t\tprint \"fill(coord=%f): %s\" % (coord, coords)\n\t\t\tif 0 in range(weight):\n\t\t\t\tbounds = self.bound_values() + (self.bound_values() * (coord - 1) + weight - self.bound_values())\n\t\t\t\tfor value in bounds:\n\t\t\t\t\tself.histogram[coord][weight + 1] = value\n\t\t\t\t\tself.increment_"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\ttry:\n\t\t\tnbins, nbins2 = self.histogram(coord)\n\t\texcept BinnedValueError:\n\t\t\treturn\n\n\t\tfor i in range(nbins2):\n\t\t\tif self.density:\n\t\t\t\tself.weights.append(weight)\t# We also copy the binned binned weights\n\t\t\t\tself._update(coord, float(weight), 1) # Update the weights\n\t\tself._update(coord, float(weight))\n\n\tdef _update(self, coord, value, weight=1):\n\t\t\t# Update the histogram value\n\t\t\tself.update(coord,"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tnx = self._nx\n\t\tny = self._ny\n\n\t\t# get bin edges for histogram\n\t\tbins = np.arange(\n\t\t\t0, nx, ny, 2 * binsize)\n\n\t\t# iterate over bin edges\n\t\t(x0, bin_end) = bins[1:]\n\t\twhile y0 < bin_end:\n\t\t\t(x1, bin_end) = bins[2:]\n\t\t\tx2, bin_end = bins[1:]\n\n\t\t\t# fill histogram with weight\n\t\t\tbin_weight = self.weight[x0 * weight + x1 * weight"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.clear()\n\t\tself.add(coord, weight)\n\n\tdef get_weights(self):\n\t\treturn self, []\n\n\t@classmethod\n\tdef from_dict(cls, dictionary):\n\t\t\"\"\"Returns a new histogram.\n\tReturns the histogram whose coordinates are given in\n\teach dictionary.\tThe weight in the dictionary is a\n\tweight that goes first in the histogram.\tThe returned\n\thistogram contains the weight of the first entry in *dictionary*\n\tand the *weight* that goes *dictionary*.  No edge, no edge,\n\tno edge. \n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself.histogram.invalidate_cache()\n\t\tif coord in self.edges_excluded:\n\t\t\treturn\n\t\tif type(coord)!= list:\n\t\t\tcoord = [coord]\n\t\tfor c in coord:\n\t\t\tvalue = self[c]\n\t\t\tself[c] = value\n\tdef add_text(self, text, font=wx.Font(10, wx.FONTFAMILY_DEFAULT, wx.FONTSTYLE_NORMAL, wx.FONTWEIGHT_NORMAL), style=0):\n\t\t\"\"\"\n\t\tAdd text to the histogram\n\t\t\n\t\t@param text: str;"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself._get_grid(coord, weight)\n\n\tdef set(self, coord, val):\n\t\t\"\"\" Fill histogram at *coord* with *val*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tself._get_grid(coord, val)\n\n\tdef _get_grid(self, coord, weight=1):\n\t\t\"\"\" Return grid data at *coord*.  If *weight* is not supplied,\n\t\tthe grid is marked as missing.\n\t\t\"\"\"\n\t\tif (not self._allow_grid) and (self._coords is None):\n\t\t\t"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfrom djblets.util import _validate_dict\n\t\tfrom djblets import validators as validate\n\t\tif not set(labels) <= set(labels.keys()):\n\t\t\traise ValidationError(\n\t\t\t\t[\"must contain all of labels and the provided labels.\"],\n\t\t\t)\n\n\ndef _validate_dict_labels(labels):\t\"\"\"\n\tCheck that labels match the regular expression for the following key-value pairs, and if\n\tall keys contain exactly the same regular expression.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfor key, value in labels.iteritems():\n\t\t\tassert re.match(regex_to_label.search(key), value)\n\ndef _validate_items(items):\n        \"\"\"Return an iterator over keys and values that match each item of an iterable.\n\nArgs:\n    items (dict): the list of keys to check against.\n\nRaises:\n    ValidationError: if any of the items does not match the regular expression\n        where the error was raised. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the list of keys to check against.\n        \n       .."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfor label_obj in labels.values():\n\t\t\tlabel_obj[\"$$\"] = None\n\n\ndef _validate_values_and_labels(raw_values, labels):\n\t\"\"\"\n\tValidations that are provided are given to the `set` method of a\n\tmodel class. This method also ensures that that the `keys` and `values`\n\tare all strings.\n\n\tArgs:\n\t\traw: dict, {str: str}, expected format.\n\t\tlabels: dict, {str: str},  one key containing label key and label value. Must be a dict."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise ValidationError(\"Error validation\")\n\n\n\n\ndef _validate(val, labels):\t\"\"\"\n\tValidate the string value against the provided labels.  Raises\n\tValidationError if any of the keys and labels does not match their respective\n        regular expressions (both lists and dicts).  The keys and labels\n        should be strings or lists, respectively.  If keys and values are\n        lists or dictionaries, each list item is a list of keys and values.\n\tIf the keys or values do not match, each element of the list is a\n        boolean.\n\n\tArgs:\n\t\tval (str or list or dict): The"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\telse:\n\t\t\tassert False\n\telse:\n\t\tassert False\n\t\t\t\t\t\ndef _validate_keys_match(keys):\n\t\"\"\"\n\tCheck that the values in the given dict are either correct or not.\"\"\"\n\tassert isinstance(keys, dict)\n\tassert len(keys) > 0\n\tassert isinstance(keys, (str, int))\n\t\n\tfor n, k in enumerate(keys):\n\t\tif not isinstance(k, str) or isinstance(k, int):\n\t\t\traise ValidationError(u'rejecting keys %r as key not unicode' % k)\n\n\t\tif not"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tassert all(k in labels for k in labels if re.match(k, labels[k]))\n\t\traise ValidationError(\"Error validating the given \" +\n\t\t\t\"labels: \" + str(labels) + \".\".join(str(v) for v in labels.values()))\n\ndef _validate_labels_keys(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2020 Intel Corporation\n# All rights reserved.\n#\n# This file is licensed under the terms and conditions of the BSD license;\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.opensource.org/licenses/bsd-license.php\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfor k, v in labels.items():\n\t\t\tassert type(v) == dict, \\\n\t\t\t\t\"Each element in the label dictionary must be a dict.\"\n\n\t\tfields = defaultdict(list)\n\t\tfor label in labels.keys():\n\t\t\tfields[label].append\n\ndef _validate_items():\n\t\"\"\"\n\tValidate the list provided by ``jsonschema``. Validations are not checked.\n\t\"\"\"\n\t_validate_labels(VALIDATORS)\n\ndef _validate_fields():\n\t\"\"\"\n\tValidations are performed based on the type of each field that is defined."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\telse:\n\t\t\tpass\n\t\t\n\t\t\nclass LabelsValidator:\n\t\"\"\"\n\tValidator that validates the label list for a given key and value.\n\t\"\"\"\n\t\n\t@staticmethod\n\tdef label_to_string(label):\n\t\tre_label = re.compile(r\"(\\w+)\\s+([^\\s]+)=(.*)$\")\n\t\treturn re_label.sub(\"[^\\s]+\", label)\n\n\t@staticmethod\n\tdef error_msg_to_dict(error_msg):\n\t\t'''Transform raw error strings into a dict and return as a dictionary.\n\t\t\n\t\tReturns:\n\t\t"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tfor labelname, labelsvalue in labels.items():\n\t\t\ttry:\n\t\t\t\tvalidate(labelname, labelsvalue)\n\t\t\texcept ValidationError as err:\n\t\t\t\tassert err.messages == [\n\t\t\t\t\t{\"'{}': 'Could not find the regex'.format(labelname)},\n\t\t\t\t\t{\"$\": \"Label '{}' value must be a string\"}\t\n\t\t\t\t]\n\t\t\telse:\n\t\t\t\traise err\n# pylint: disable=missing-docstring\n# pylint: enable=missing-docstring\nimport datetime\n\nimport numpy as np\n\nfrom chainer import cuda\nfrom chainer import functions\nfrom chainer import"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _convert_to_resource_name(resourceName:str) ->str:\n\t\"\"\"\n\tConvert the resource name provided as a string into a string for the\n\tresource that the Krake resources can be accessed for.\n\t\"\"\"\t\n\tmatch = _resource_name_regex.search(resourceName)\n\t\n\tif match:\n\t\treturn match.group('resourceName')\n\t\n\treturn None\n\nkrake = _get_krake_object()\n\"\"\"\nDjango settings for mountaal_app project.\n\nGenerated by 'django-admin"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    return compile('test\\\\test\\\\krake.*', '<resource name>')\t\n\ndef _get_resource_name_relative_path(source_rel, target_rel):\n    \"\"\"\n\tBuild or return the path that points to a resource relative to\n\ta target's source_rel and target_rel.\n\n\tReturns:\n    (regex): the compiled regex to match the target's source_rel, to\n\tvalidate the path by matching the resource relative\n\tto both target's source_rel and target_rel.\n\t\"\"\"\n\tassert(target_rel == source_rel)"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\ndef _do_get_resource_name(krakeId, name):\t\"\"\"\n\tReturn the regular expression that the id provided is a Krake resource.\n\nArgs:\n\tkrakeId (int): The krake resource ID\n\tname (string): Name of the resource\n\t\"\"\"\n\t\ndef _do_get_resource_id_as_name(krakeId, name):\t\"\"\"\n\tBuild or return the regular expression that the id provided is a Krake resource id.\n\t\"\"\"\n\t\n\t\n\tres_regex = re.compile"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\trePattern = re.compile( r'^((?:.*?[0-9]\\.[0-9]+[eE][+-]?\\d+)(\\.[0-9]+)?[a-zA-Z_]\\w*)')\n\t\treturn rePattern.search( name )\n\treturn None\n\n# ---------------------------------------------------------------\n# the base file name from a resource name\n# ---------------------------------------------------------------\n\nclass MyResourceNameValidator:\n\t\"\"\"\n\tReturns a valid resource name including the leading underscore\n\tfrom the resource name given the root resource name.\n\t\"\"\"\n\tdef __init__( self, root_resource_name"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    def _init_instance(self):\n\t\t\"\"\"\n\t\tBuild or return a instance of the Krake-specific type which supports\n\t\tthe Krake resource.\n\t\t\"\"\"\n\t\tself._name = self.get_name()\n\t\tself._get_regex_str()\n\t\t\n\tdef get_name(self):\n\t\t\"\"\"\n\t\tReturns the regular expression corresponding to the name of the record\n\t\tthat identifies this resource.\n\t\t\"\"\"\n\t\treturn re.compile('^[a-zA-Z][a-zA-Z0-9\\_\\_]*$')\n\t\n\tdef _"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _get_resource_description_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe resource description.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource description.\t\"\"\"\n\t\n\t\t\n\n_resource_description_regex = re.compile(r'^\\s*([\\w\\s_]+)(.*)')\n\ndef _validate_name_regex():\n\t\"\"\"\n\tValidate that the regular expression is valid.\n\nReturns:\n    (bool): (True | False): The regular expression can be used to validate\n    the resource"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t if config.has_option(\"resource\", \"name_re\"):\n\t\t return re.compile(config.get(\"resource\", \"name_re\"))\n\t\traise NotImplementedError(\"No resource name is found. Try using the krakes client settings!\").split(\"\\n\")\n\n\tdef _validate_resource(name):\n\t\t\"\"\"\n\t\tReturn a compiled regular expression based on the name of the resource.\n\t\tFor regular elements that are defined in the krake config, and the variable\n\t\tname_re is also used to determine if the name is a valid resource name.\n\t\t\"\"\"\n\t return re."}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    def _resource_name_regex():\n        return re.compile(RE_KRAKKAN_REGEX, re.VERBOSE)\n\n    return re.compile(_resource_name_regex())\n\ndef _extract_version():\n    \"\"\"\n\tBuild the version that is known to the CI server.\n\tIt should return either the full version string or the full\n\tversion tuple.\n\t\"\"\"\n        return _get_resource_name_regex().findall(version)\n\n    def _extract_version_tuple():\n        \"\"\"\n\tBuild the version string and optionally tuple of:\n\t  (ver, major"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\t\n\tregex = ResourceNameRegex()\n\n\tregex.regex_dict = _get_resource_name_dict(sys_name)\n\t\n\treturn regex\n\n\ndef _get_resource_type():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe type of the Resource/Data object.\n\nReturns:\n    regex: the compiled regular expressions, to validate\n    the type of the Resource/Data type.\t\"\"\"\n\t\n\tregex = ResourceTypesRegex()\n\t\n\tregex.regex_dict = _get_resource_type_dict()\n\t\n\treturn regex\n\ndef"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    with open(\"resources/\" + name + \".txt\", \"r\") as resources:\n        return re.compile(\"([a-z0-9](?<![^:'\\\"\\[\\]\\s]*$)*)\")\n\n\n\ndef _get_resource_path(name, base=\".\"):\n    return os.path.join(base, name)\n\n\ndef _get_resource_resource_path(name, base=None):\n    return os.path.join(base, name)\n\n\n\n\nclass CeleryResource(Resource):\n    \"\"\"\n    Resource to build/deploy resources according to the Krake Celery"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t return validate(value)\n\ndef validateFloat(value):\t\"\"\"\n\tValidate a float in the value.\n\nTakes a float in numeric units and returns a number. For example, 9.0 is valid.\n\t\n\t- >>> validateFloat(2.1)\n\t- 2.1\t\n\t\n\t\"\"\"\n\treturn validate(float(value))\n\n\nclass TextArea(object):\n\t\"\"\"\n\tA simple wrapper class which encapsulates an HTML <span> element (usually a <div> element).\n\t\n\tIn python, this class takes a string containing the text enclosed in quotes,\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\treturn validate_pattern(value)#!/usr/bin/env python3\n\n__author__ ='martin.c.l@gmail.com'\n\nimport os\nimport sys\n\nimport boto3\nimport botocore\nimport datetime\nimport yaml\n\nimport json\nfrom boto.exception import ClientError\n\nCONFIG_FILE = \"boto3server.yml\"\n\n# Set up a connection to the AWS server\naws = boto3.client('ec2', region_name='us-east-1')\n\ns3_connection = aws.get_connection(\"local\", aws"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\nclass DomainResolver:\t\"\"\"Domain resolver class used to lookup the DNS records by IP address. This is where you want to create\n\tthe resolvers between domains and instances.\n\t\"\"\"\n\n\nclass IPResolver(Resolver):\n\t\"\"\"Creates an IP resolver based on the DNS records and the rules specified in the IP address\n\tto be resolved to domain names.\n\n\tThe resolution can be performed using standard IP addresses, to define the DNS server,\n\tin order to use the resolver. For example, to create a resolver from an IPv6 address \n\tand use a DNS resolver, you could"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\tif re.match(value, values.get()):\t\t\t\t\t\t # if the string does not match the regular expression\n\t\t\traise ValidationError('Validating the given value')\n\t\t\t\t\t\t\n\telse:\n\t\tvalue = str(value)\t\t\t\t\t\t # if the string does not match the regular expression\t\t\n\n\treturn value\n\n\n@frappe.whitelist()\ndef add_to_company_currency(company, currency, code='USD',\n\treference_type=None, default='TFR', is_group=False):\n\tif not is_group:\n\t\tis_group = 1 if is"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ttry:\n\t\t\tvalue = validate_value(value)\n\t\texcept ValidationError:\n\t\t\traise ValidationError(value)\t\n\nclass InvalidDate(Exception):\t\"\"\"\n\tRaised in case of a date that is not a valid date or if the provided date is empty.\n\t\"\"\"\n\tpass\n\nclass InvalidTime(Exception):\n\t\"\"\"\n\tRaised to indicate that the provided time is not a valid time\n\t\"\"\"\n\tpass\n\nclass InvalidFieldLength(Exception):\n\t\"\"\"\n\tRaised when the provided field length is not a valid field length\n\t\"\"\"\n\tpass\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif not _regex_match(value, REGEX_VALUE_REGEX):\n\t\traise ValidationError(\"Invalid regular expression\")\n\t\t\n\t\t\n\tif pattern[0] == '/':\tvalue = \"/\" + value\n\telif pattern[0] == '^':\n\t\tvalue = \"^\" + value + \"^\"\n\n\tvalue_type = value.lower()\n\tif re.match(RE_VALUE_REGEX, value_type) is None:\n\t\traise ValidationError(\"Invalid regular expression: \" + value)\n\t\timport inspect\nimport os\n\nfrom. import conf, debug\nfrom. import util,"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_url(value):\n\t\"\"\"\n\tValidate the given URL against the corresponding regular expression.\n\nArgs:\n    value: the string to validate against\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n\"\"\"\n\ndef validate_path(value):\n\t\"\"\"\n\tValidate the given path against the corresponding regular expression.\n\nArgs:\n    value: the string to validate against\n\nRaises:\n    ValidationError: if the given path is not conform to the regular expression.\n\"\"\"\n\ndef validate_url_relative(value):\n\t\"\"\"\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef _get_regexp_match(field, value, max_length=0):\n\t\"\"\" Get the regex for a field against the provided regular expression. \"\"\"\n\n\tdef _check(r1, r2):\n\t\tif r1!= r2:\n\t\t\traise ValidationError(\"Value for %s must be %r.\" % (r1, r2))\n\t\n\t\tfor part in r1.split(field):\n\t\t\tif len(part) > max_length:\n\t\t\t\tmsg = \"Value for %s too long: %s\" % (r1, part)\n\t\t\t\traise ValidationError"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\nfrom frappe import _\n\ndef validate_condition(value):\t\"\"\"\n\tValidate the given condition against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given condition is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_multiple(value):\t\"\"\"\n\tValidate the given value against multiple regular expressions.\n\nArgs:\n    value: the string to validate and validate with multiple commas\n\nRaises:\n    TypeError: if the given value is not a list of lists.\n\tValueError: if the given condition is not valid against"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n#def validate_string(value):\t'''Validate the given string against the corresponding regular expression.\n#\n#\tArgs:\n#\t\t\tvalue: the string to validate\n#\t'''\n\t\ndef main():\n\t# get a dictionary of all key, values\n\t# we may get multiple keys (for example, \"a = 'foo'\")\n\tdata = {}\n\t# we need to check every key that the value type is.\n\t# to look for a string, we use the regexp to match the value.\n\tkeys = json.loads(open('data/key."}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\tvalidRegex = re.compile('^[ \\t]*{}[^ \\n]*$'.format(key))\n\t\ttry:\n\t\t\treg = validRegex.match(str(key))\n\t\texcept ValueError:\n\t\t\traise ValidationError(key)\n\t\treturn None\n\telse:\n\n\t\tpass\n\tpass\n# -*- coding: utf-8 -*-\n#\n# Copyright 2013 Hewlett-Packard Development Company, L.P.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\n\tif (not key) or (not re.search(r'^[A-Za-z0-9]*',key)):\t\n\t\n\t\t# TODO: use a validation function\n\t\treturn\t\n\n\t# Validate the key against any regular expression\n\ttry:\t\n\t\tkey = re.compile(key,re.UNICODE)\t\n\n\texcept:\n\t\treturn\n\n\treturn\n\nclass BaseS3(object):\n\t\n\tdef __init__(self,settings,controller=None):\n\n\t\tif not controller: controller = settings.controller_host + '.s3.amazonaws.com'"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\n# This is an implementation of the Pandas Series.\nclass Series(object):\n\tdef __init__(self, n, dtype=None):\n\t\tself.n = n\n\t\tself.dtype = dtype\n\t\tself.index = IntIndex(n, dtype=self.dtype)\n\t\tname = self.name\n\t\tdtype = self.dtype\n\t\tvalues = self.values\n\t\tself.values = values\n\t\tself.name = name\n\t\tself.dtype = self.dtype\n\n\tdef __len__(self):\n\t\treturn len(self.index)\n\n\tdef"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\tvalid_values = dict(re.findall('(\\w+)\\(', key))\n\tfor value in valid_values.keys():\n\t\tif value in VALID_KEYS:\n\t\t\tprint(\"WARNING: Found valid key {key} with regex: {regex}\".format(**locals()))\n\t\telse:\n\t\t\tprint(\"WARNING: Invalid key {key} with regex: {msg}\".format(**locals()), file=sys.stderr)\n\treturn key\n\n\ndef get_all_strings(key):\n\t\"\"\"\n\tGet list of strings from the passed in key.\n\t\"\"\"\n\timport re\n\t\n"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\nclass ValidatesKeysError(Exception):\n    \"\"\"\n    Exception raised when the given key validates fails. This is used by the\n    validate method to notify user of the validation.\n\nUsage:\n    my_validation_key.validate('foo')\n\"\"\"\n\n\nclass KeysNotInSchema(KeyValidateBase):\n\t\"\"\"\n\tKey validator for all valid keys\n\t\"\"\"\n\n\tdef validate(self, key):\n\t\tif not key or '/' not in key:\n\t\t\traise ValidationError(self.message.format(key,'is a required key'))\n\nclass KeysNotUniqueError(KeyValidate"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\ndef is_valid_key(key):\n\t\"\"\"\n\tReturns true if the given key is a valid regular expression.\t\"\"\"\n\t\n#    def _check_if_valid_key(key):\n\treturn type(key)==list and set(key)>set(VALID_KEYS)\n\n#    def _normalize(l):\n\tif l=='':\tl = ''\n\tl = l.lower()\n\treturn l\n\n#    def _validate(v, t):\n\t#    if t.name.lower() in VALID_KEYS:\n\t#       "}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tregex_re = \"^[a-zA-Z0-9_\\.]\"\n\t\n\tregexp = re.compile(regex_re, re.IGNORECASE)\n\tif not (key and regexp.match(key) and regexp.match(key.lower())):\t\t   \n\t\traise ValidationError(\n\t\t\t\"Invalid regular expression: '%s'. Did you mean '%s'?\" % (key, regexp.pattern))\n\ndef validate(key, default=NOTSET):\t\"\"\"\n\tLooks up the given key against the corresponding regular expression.\n\tDefault is specified if no default provided by the"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif re.search('^[!$]' + key, key.lower()):\n\t\traise ValidationError('Key is not conform to the regular expression')\n\ndef validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate.\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\t\n\tif re.search('^[!$]' + value, value.lower()):\n\t\traise ValidationError('Value is not conform to the regular expression')\n\ndef validate_form"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\ndef check_key_syntax():\n    \"\"\"\n\tCheck if the given key specification is a key type, if it is, valid\n\t\tagainst one of the rules that were defined\n\t\"\"\"\t\n    def check_key_type(key, spec):\n        if spec in (u'boolean', u'integer', u'string', u'password'):\n            return True\n        elif spec in (u'password', u'string'):\n            return False\n        else:\n            return False\n    \n    keytypes = []\n    keytypes_map = {key}\n    for rule, regex in rules:\n        try:\n            if str"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    def validate_match(key, pat):\n\n        try:\n\t\t\n            return bool(re.match(pat, key)!= '' or re.match(pat, value))\n        except:\n            return False\n    \n    def validate_key_type(key, match_type):\n\n        try:\n\t\t\tif key!= 'none':  \n\t\t\t\tmatches = re.match(match_type, key)\n\n                return matches.group('keyType')\n        except:\n\t\t\treturn False\n\n    def to_key(key, match_type):\n\n        try:\n            if key!= 'none':  \n\t\t\t\tmatches = re."}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t    \ndef generate_event_schema(event_schema, first_level=False):\n\t\"\"\"\n\tReturns the ``event_schema`` for the specified ``event_schema``.  If there is a \n\t``first_level`` dictionary that is ``True``, this function simply returns \n\t``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``agent_schema`` for a \n\tresource), a string with the type of agent_schema must be specified. The type\n\tparameter ``agent_schema`` is not optional, but the type must be specified as"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n    for resource in manifest_dict:\n        if None in resource:\n            continue\n        if resource['id'] not in _MANIFEST_ATTRS_IN_RESOURCE_LIST[resource['id']]:\n            # if an attribute doesn't exist in the manifest file\n            # or has an explicit default for the resource, skip\n            continue\n        with open(os.path.join(RESOURCE_ROOT, resource['id']), 'r') as f:\n            resource_config = yaml.load(f)\n        resource_config['type'] = resource_config['registry']\n        resource_config['metadata'] = resource['metadata']\n       "}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\ndef import_metadata_file_to_resource_metadata(filename, resource_metadata):\n\t\"\"\"\n\tImport the metadata file and meta-data to a resource resource directory. See\n\t:func:`import_resource_metadata_file` for details about the actual metadata\n\tfile contents. Similar to the :func:`read_resource_metadata` function which\n\tgenerates the ``metadata.yaml`` file generated from the resource contents. The\n\tmetadata file is also read on disk as JSON.\n\t\n\tArgs:\n    filename (str, path): Resource filename or path\n    resource_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tif not first_level:\n\t\t\tfrom podaroonkit.common import Utils\n\t\t\tmanifest_dict = Utils.load_manifest(manifest_dict,\n\t\t\t\t'podaroonkit.schema.default.observer_schema.yaml')\n\t\tresult = {}\n\t\tfor key, value in manifest_dict.items():\n\t\t\tif value:\n\t\t\t\titem = {key:value}\n\t\t\t\tif isinstance(result, dict):\n\t\t\t\t\tresult = result.copy()\n\t\t\t\tfor field, value in result.items():\n\t\t\t\t\titem[field] = value\n\t\treturn result\n\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t    \n    manifest_dict.setdefault('resource_type', 'object')\n    for r in manifest_dict:\n\t            print r\n\t      \n\tif first_level:\n\t\tset_default_resource_property(manifest_dict,'resource_type','string')\n\ndef set_default_resource_property(manifest_dict, resource_type, default_value):\n    \"\"\"\n    Sets a default resource property in the ``resource_name`` dict provided a specification from ``manifest_dict`` if it exists.\n    If ``resource_type`` is ``string``, :func:`default_value` is"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\t# We already have a dictionary of ``first_level`` and ``non_dict`` items in\n\t\t# the original and non-dict items, so copy them all the way down to the\n\t\t# final result.\n\t\td = {}\n\t\tfor k, v in manifest_dict.items():\n\t\t\t# There are some issues with the previous check above that it is not\n\t\t\t# sufficient to ensure that any ``non_dict`` items in the original dict are\n\t\t\t# ignored at this time in order to make the behavior consistent with the\n\t\t\t# new dictionary.\n\t\t\tif k not in"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\tobj = None\n\twith open(manifest_dict) as manifest_handle:\n\t\t\n\t\tfor line in manifest_handle:\n\t\t\tline = line.rstrip()\n\t\t\tif len(line) < 4:\n\t\t\t\tcontinue\n\t\t\tif line[0] == '#':\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tline_type = line[1:]\n\t\t\tif line_type == '=':\n\t\t\t\tif first_level:\n\t\t\t\t\tif line[0] == ':':\n\t\t\t\t\t\tif line[1] == ':':\n\t\t\t\t\t\t\t# skip the trailing empty line\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif first_level:\t\n\t\t\t\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tdict_copy = deepcopy(manifest_dict)\n\t\tdict_copy.update({\n\t\t\t'id': 'observer_schema_id',\n\t\t\t'name': 'name',\n\t\t\t'mode': 'type',\n\t\t\t'schema': None\n\t\t})\n\t\tdict_copy.update(\n\t\t\t{\n\t\t\t\t'id': 'name',\n\t\t\t\t'name':'resource_template_metadata',\n\t\t\t\t'modes': [\n\t\t\t\t\t{\n\t\t\t\t\t\t# If type is not set, it means a resource\n\t\t\t\t\t\t'type':'string',\n\t\t\t\t\t\t'properties': {\n\t\t\t\t\t\t\t# '"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\twith open(manifest_dict[\"path\"]) as file:\n\t\t\tmanifest_dict = yaml.load(file)\n\t\tvalidated_manifest_dict = json.dumps(manifest_dict, ensure_ascii=False)\n\t\tassert len(validated_manifest_dict) > 1, \"One of the manifest dicts must have a 'validation_error_message' field in the generated dictionary.\"\n\t\tassert len(validated_manifest_dict) == len(manifest_dict['validation_error_message']), \"One of the manifest dicts must have 'validation_error_message' field in the generated dictionary.\""}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\ndef generate_spec_schema_dict(manifest_dict, only_fields):\t\t\t\t\n\t\"\"\"\n\tReturns a new dictionary with the parsed spec for a given pod spec string.\n\t\"\"\"\n\tpod_spec = None\n\t\n\tspec_string = manifest_dict.get('spec')\n\tif'spec' in schema.get('object', {}).keys():\n\t\tpod_spec_string = schema.get('object')['spec']\n\t\n\tpod_spec_string = yaml.safe_load(spec_string)\n\t\t\n\tif 'podSpec' in pod_spec_string:\t\t\t\t"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tif last_applied_manifest is None else len(last_applied_manifest)\n\n\tupdated_manifest = False\n\n\tfor field in last_applied_manifest:\n\t\tif field not in observer_schema:\n\t\t\tcontinue\n\n\t\tmodified_modified = False\n\t\tif field in observer_schema[field] and field not in response:\n\t\t\tmodified_modified = True\n\n\t\tresponse[field] = {}\n\n\t\tif field == \"last_applied_manifest\":\n\t\t\tif not modified_modified:\n\t\t\t\t# If the field has been modified on first run, then we should need to make sure"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\nclass AppsBaseApp(object):\n    \n    def run(self, conf):\n        ''' \n        Starts the app and starts the plugins.\n        '''\n        self.register_all_plugins()\n        self.init_plugins()\n        self.init_app(conf.plugins, conf)\n        self.prepare_plugins()\n        self.load_plugins()\n        return\n\nclass AppBaseAppPlugin(ConfBaseApp):\n    \n    def load_plugins(self):\n        raise NotImplementedError, \"Implement load_plugins method\"\n    \n    def init_plugins(self):\n        self.register_all_plugins()\n        self."}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\ndef get_last_preserved_on_next_response(observable_dict, observer_schema, response):\n\t\n\t'''Update the value to avoid a final iteration of the ``observer_schema``'''\n\tlast_preserved_on_next_response_value=observable_dict.get(observer_schema[0].to_python(return_none = True))\n\t#print('\\t',observable_dict)\n\tif last_preserved_on_next_response_value is not None and 'last_preserved_on_next_response' not in last_preserved_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor elem in last_applied_manifest:\n\t\t\tlatest_version = get_latest_version(elem, observer_schema)\n\t\t\tif latest_version:\n\t\t\t\tresp = extract_full_response(latest_version)\n\t\t\t\tif resp is None:\n\t\t\t\t\treturn\n\n\t\t\t\tlast_applied_manifest.append(resp)\n\n\treturn last_applied_manifest# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015 CERN.\n#\n# Invenio is free software; you can redistribute it\n# and/or modify it"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\ndef get_active_manifest_state(node_schema, response):\n\tif 'current_state' in response and response['current_state']:\n\t\tlast_applied_manifests = json.loads(response['current_state'])\n\t\tlast_applied_manifest = last_applied_manifests['kubernetes_last_applied_manifest']\n\telse:\n\t\tlast_applied_manifest = {}\n\n\tfor current_manifest in last_applied_manifests:\n\t\t# ignore empty values\n\t\tif not current_manifest:\n\t\t\tcontinue\n\t\tlast_applied_manifest[current"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tif last_applied_manifest!= None and len(last_applied_manifest) == 0:\n\t\t\tlast_applied_manifest = []\n\tfor f in response :\n\t\tif \"observer_schema_update\" in f :\n\t\t\tupdate_last_applied_manifest_list_from_resp( f, last_applied_manifest )\n\nclass Clients(object):\n\tdef __init__(self, client):\n\t\tpass\n\n\tdef get_api_url(self):\n\t\ttry :\n\t\t\tresult = requests.get( self.geturl() )\n\t\texcept requests"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n    @classmethod\n    \n    \n    \n\n\t@classmethod\n    \n\n\tdef init_last_applied_manifest(cls, last_applied_manifest):\n\t\t\"\"\"\n\t\tInitializes the ``last_applied_manifest`` mapping from a field name to a list whose \n\tvalues are the ``value`` of ``last_applied_manifest``.\n\t\"\"\"\n\n\t\tlast_applied_fields = list(last_applied_manifest.fields)\n\t\tlast_applied_values = [field.label for field in last_applied_fields]\n\t\tresponse = [field.value for field in last_applied_fields]\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor field in ('observer_schema', 'last_applied_manifest'):\n\t\t\tdata.update(get_last_applied_manifest_dict_from_resp(response, field, observer_schema))\n\t\treturn last_applied_manifest or data\n\n#----------------------------------\nif __name__ == '__main__':\n\t#--- Create the kubernetes api connection ---\n\tdef get_api_connection():\n\t\t\"\"\"This function is the old default behavior.\"\"\"\n\t\tconnection = Connection(HTTP_PROXY, 'https://localhost:3123') #proxy is http://127.0.0.1"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t \ndef update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this is\n\tcalled recursively to update a partial ``last_applied_manifest`` from a partial\n\t``response``\n\n\tArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThe callback function itself takes in a single field, and"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n    def __init__(self, last_applied_manifest, observer_schema, response):\n        super(BaseResponse, self).__init__(last_applied_manifest, observer_schema, response)\n        self.__last_applied_manifest = self.__normalize_response(last_applied_manifest)\n\n\n@_patch_api_response\ndef update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\n"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t    import threading\nimport time\nfrom threading import Thread\nimport os\nimport re\nimport socket\n\nclass Queue(object):\n\tdef __init__(self, hosts, hostcount, connection, clientmsg):\n\t\tself.hosts = hosts\n\t\tself.hostcount = hostcount\n\t\tself.connection = connection\n\t\tself.clientmsg = clientmsg\n\t\t\n\tdef put(self, item, msg, timeout=None):\n\t\tmsg += \" \" + self.clientmsg + \":\" + item + \":\" \n\t\tself.connection.send(msg)\n\t\tif timeout is None:\n\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\ndef fetch_last_applied_manifest_list_from_resp(\n\t\t\"\"\"\n\tTogether with :func:``fetch_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to collect all the partial ``last_applied_manifest``\n\tinstances from a partial ``last_applied_manifest``\n\t\"\"\"\n\t# we're iterating through the remaining response body and collecting\n\t# the full result from the next iteration\n\tfor next_iter in response:\n\t\tlatest_timestamp = None\n\n\t\tfor key, values in iteritems(response):\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tif not last_applied_manifest:\n\t\t\traise kazoo_exceptions.MissingRequiredField('last_applied_manifest')\n\n\t\tresponse = response.get(last_applied_manifest)\n\t\ttry:\n\t\t\tlast_applied_manifest = eval(last_applied_manifest)\n\t\texcept Exception as exc:\n\t\t\traise kazoo_exceptions.NotScalarField('Last applied manifest must be a dict but it received'+ str(repr(exc)) +'but it should have been:'+ repr(last_applied_manifest))\t\t\t\t\t\t\n\n\t\tactor_config = actor_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\timport json\n\t\tlast_applied_manifest_list = {}\n\t\tfor field, value in response.items():\n\t\t\tif field in observer_schema and isinstance(value, dict):\n\t\t\t\tfor spec in json.loads(value.pop('last_applied_manifest',{})):\n\t\t\t\t\tif spec['field']!= last_applied_manifest:\n\t\t\t\t\t\tlast_applied_manifest_list[field] = spec\n\t\tresponse = update_last_applied_manifest_list_from_resp( json.dumps(last_applied_manifest_list) )\n\n\treturn response\n\ndef check"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\ndef get_new_observer_from_response(r):\n\t\"\"\"\n\tReturns a new observer based on an existing response\n\t\"\"\"\n\tdata = {}\n\tfor field in ['version', 'name', 'created_at']:\n\t\ttry:\n\t\t\tnew_value = r.get(field, None)\t\t\n\t\t\tdata[field] = new_value\t\t\t\t\n\t\t\tif new_value:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(data)\t\n\t\t\t\treturn data\n\t\texcept (TypeError, KeyError), e:\n\t\t\traise KeyError('Unable"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor name in response:\n\t\t\tname_match = re.match( r'.*last-applied-manifest-(.*)$', name)\n\t\t\tname = name_match.group(1)\n\t\t\tif name!= last_applied_manifest.get(name, \"\"):\n\t\t\t\tlast_applied_manifest[name] = response[name]\n\n\n# TODO(davide): Implement caching and re-implementing update_last_applied_manifest_dict() and\n#       registering the current request. The client should instead change this code\n#       and implement a decorator instead\n# TODO(davide):"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\ndef validate_last_applied_manifest(last_applied_manifest, observer_schema):\n    \"\"\"\n    Validates the ``last_applied_manifest`` from a partial ``last_applied_manifest`` from the response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n\n    Returns:\n        dict: dict describing whether a partial ``last_applied_manifest`` was validated,\n        or a ``False`` otherwise\n    \"\"\"\n\n    # check validity of the provided last_applied_manifest\n    if not last"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n\t\n\t# If the field is not present, then default to ``no_value`` attribute\n\tif not last_applied_manifest:\n\t\tlast_applied_manifest = dict.__getitem__(last_applied_manifest_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_dict_set_no_value(last_applied_manifest)\n\telse:\n\t\tlast_applied_manifest.update(last_applied_manifest)\n\n\t# If the field is present,"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t# Check only the fields that are present\n\t\tfor field in LastAppliedManifestList.fields:\n\t\t\tif not field in observer_schema:\n\t\t\t\traise KeyError(\"Observable field '{}' is not present in the {} schema\"\n\t\t\t\t\t.format(field, LastAppliedManifestList.__name__))\t\n\n\ndef update_latest_registered_manifests_metadata_from_resp(\t\"\"\"\n\tTogether with :func:``update_latest_registered_manifests_metadata_from_resp``, this\nfunction is called recursively to update a partial ``latest_registered_manifests``\nfrom"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tif last_applied_manifest:\n\t\t\tif 'last_applied_manifest' in response and \\\n\t\t\t\tresponse.last_applied_manifest.get('timestamp', None) \\\n\t\t\tor not response.last_applied_manifest.get('timestamp', None):\n\n\t\t\t\tlast_key, last_value = response.last_applied_manifest['timestamp']\n\n\t\t\tif response.last_applied_manifest:\n\t\t\t\tupdated_last_applied_manifest = \\\n\t\t\t\t\tlast_applied_manifest \\\n\t\t\t\t\tor (not last_value or value_from_timestamp(last_value) >= val"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tfor path in spec.data.paths:\n\t\t\t# Load default object spec from specified object path\n\t\t\tpath = path.load(app, schema=spec.data.default)\n\n\t\t\tif path and not os.path.isfile(path):\n\t\t\t\tprint(\"Failed to load default object spec file %s\" % path)\n\t\n\treturn app\n\ndef generate_config_handler_obj(app):\t\t\t\t\"\"\"\n\tCreate and register a new ConfigHandler object.\n\t\"\"\"\n\t\n\t# Create default config handler for the given type\n\thandler = config.ConfigHandler(app, spec,"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t    \n\n\ndef generate_default_observer_schema_schema_id(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource included in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t    \n\n\ndef validate_spec_name(spec):\n\t\"\"\"\n\tCheck the given spec name if it exists\t\t\t\n\tCheck that and create if there is already a node with that name\n\tCreate the named"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tprint('Generating %s %s' % (app.__class__.__name__, app.config_name))\n\t\tcustom_schema = os.path.join(app.config_path, 'krake', 'observer_schema.json')\n\n\t\tif not os.path.exists(custom_schema):\n\t\t\tdefault_schema = {\n\t\t\t'path',\n\t\t\t'schema'   : {\n\t\t\t\t'module'   : 'kubernetes.custom.observer.DefaultObserver',\n\t\t\t\t'schema'   : {\n\t\t\t\t\t'path'   : '{0}/'+app.config_"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\n\tfor resource in app.api.resources:\n\t\tif isinstance(resource, K8s.Pod):\n\t\t\t# TODO(krakebot): This is not a great deal yet, it is broken in Openshift\n\t\t\t# and Openshift versions 3.6-3.5.\n\t\t\t# Need to remove when the yaml config file is only used on Kubernetes v1.4 or v1.3 of Kubernetes 1.3\n\t\t\tresource.metadata.read_only = True\n\n\t\tif not resource.labels:\n\t\t\tcontinue\n\n\t\tresource.metadata.labels['resource']['path']"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\ndef get_defaults():\n\t\"\"\"\n\tReturns the kubernetes default schema for each Kubernetes resource present in\n\t``spec.manifest``\n\t\n\tReturns:\n        dict: The kubernetes default resource configuration\n\t\"\"\"\n\tdefaults = {'resources': []}\n\tfor template_name, template_path in ('templates', 'outputs', 'inputs'):\n\t\tdefaults['resources'].append({\n\t\t\t'type': 'object',\n\t\t\t'type':'string',\n\t\t\t'starts_with': ['metadata', 'labels', 'labels.metadata.label-tag:foo']\n\t\t})\n\t\tfor template"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n@app.route(\"/api/v1/tags/<resource>\", methods=['GET'])\n@api_doc(output=True)\ndef get_tags(resource):\n\t\"\"\"\n\tRetrieve the specified tags for a Kubernetes resource\n\t\"\"\"\n\t\n\tdata = getattr(app, \"_tags\", None)\n\n\tif data is None:\n\t\traise NotFoundError()\n\n\ttag_list = []\n\tfor key, value in data.items():\n\t\tif isinstance(value, tuple):\n\t\t\t# A single value\n\t\t\ttag_list.append((key, value))\n\t\telse:"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t    \t\ndef generate_pod_skeleton(pod_skel):\n\t\"\"\"\n\tGenerate the pod skeleton for the Kubernetes pod in the specified skeleton. If a\n\tskeleton doesn't exist, the generated skeleton will be created with the\n\tnamespace. Otherwise, a new pod skeleton with this name will just contain the\n\tPod ID as it is created. Finally, an existing skeleton will be merged into the\n\tcurrent deployment.\n\t\"\"\"\n\t\n\t# generate the pod skeleton\n\tpod_id   = pod_skel['id']\n\tpod_resource = '{0}/{1}'.format("}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t    \n    meta_obj = ResourceBaseConfig(app, config_class=ResourceBooleansMeta)\n\t\n\tfor (resource, resource_class) in meta_obj._meta.default_resources.items():\n\t\tfor key in resource_class.keys():\t\t# Skip the empty strings\n\t\t\tif key.startswith(\"__\"):\n\t\t\t\tcontinue\n\t\t\tyield { }\n\t\t\tfor (prop, value) in resource_class[key].values():\n\t\t\t\t# Skip empty strings\n\t\t\t\tif value.strip() == \"\":\n\t\t\t\t\tcontinue\n\n\t\t\t\tvalue = ResourceObjectFactory()\n\t\t\t\t# If the string"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tfor name in spec.metadata.name.values():\n\t\t\tif name not in _default_observer_schemas:\n\t\t\t\t_default_observer_schemas[name] = {\n\t\t\t\t\t\"type\": \"schema\",\n\t\t\t\t\t\"version\": \"1\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"kube-api\": krake.api.ContainerSpec({ \"containerSpec\": { \"name\": name } }, \"fields\": [\n\t\t\t\t\t\t\t]),\n\t\t\t\t\t\t\"customProperties\": { \"customProperties\": { \"customProperty\": \"{name}:type\".format(name=name) },\n\t\t\t\t\t\t\"customProperties2"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\tapp.set_manifest(generate_kube_auto_schema(app))\n\n#---\n# Custom K8s\n\ndef register_custom_kube_auto_schema(app):\t\"\"\"\n\tRegisters a custom K8s with custom events specified on the ``app``.\n\t\"\"\"\n\t\tapp.set_manifest(register_custom_k8s_custom_events(app))\n\n#---\n# Custom resources\n\nclass CustomResources(K8sEventSchema):\n\tname = 'Custom resources'\n\ttype = 'custom-resources'\n\t# events to"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\nclass IterationPhased(DataPhase):\n\t\"\"\"\n\tA task that iterates over the SQL statement executed by the\n\t``$`` statement. It can be used as a \"while\"\n\tstatement, which will be executed multiple times.\n\n\tThe :class:`iterator` instance will be available in the\n\t:attr:`.DataPhase.iterator` instance.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.iterator = iter(self.iterator)\n\n\tdef execute_on_connection(self, sql_stmt, **options):\n\t\tlogging.debug('execute"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\nclass In(object):\n\t\"\"\"\n\tI represent an IN SQL statement.\n\t\"\"\"\n\tdef parse(self, params):\n\t\t\"\"\"\n\t\tParse this statement into a dictionary.\n\t\t\"\"\"\n\t\tparse = self.parse_sql(params)\n\t\treturn query_parse(parse)\n\t\n\tdef raw(self, params):\n\t\t\"\"\"\n\t\tParse this statement and return the values as a dictionary.\n\t\t\"\"\"\n\t\treturn query_raw(self, params, sql=True)\n\nclass Raw(object):\n\t\"\"\"\n\tI represent a raw SQL statement."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn (\n\t\t\tsql.sql.format(\n\t\t\t\tself._db,\n\t\t\t\tcol_map,\n\t\t\t\tparams\n\t\t\t)\n\t\t\tfor col_map, params in self.out_params.items()\n\t\t\t\n\t\t)\n\t\t\t\t\n##---------------------------------------------------------------------------##\n##\t\t\t\t#\tdef get_params(self):\n##\t\t\t\t\treturn sql.sql.parse_query_parameters(self._db, self.out_params)\n\t\t\n\tdef out(self):\t\n\t\treturn self.sql()\n\t\t\n##---------------------------------------------------------------------------##\nfrom. import sql\nfrom. import converters\nfrom."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\nsql_converter = sqlite3.converters.create_converter('SQLite3')\nsql.SQLite3Converter.create_converter('sqlite', sql_converter)\n\ndef to_sql(db, obj):\n\t\"\"\"Converts a single record to a SQL query.\"\"\"\n\treturn sql.SQLite3Converter.convert(None, obj.get(1), obj.get(2))\n\nclass Database(object):\n\t\n\t\"\"\"Encapsulates the basic database class for the entire server.\"\"\"\n\t\n\tdef __init__(self, uri, user, password, uriToDb):\n\t\tif"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tresult = []\n\n\t\tfor (c_param, c_value) in zip(params.values(), s_params):\n\t\t\tif c_param in formatted:\n\t\t\t\tresult.append(formatted[c_param])\n\t\t\telse:\n\t\t\t\tresult.append(c_value)\n\n\t\treturn result\n\ndef listify(values):\n\t\"\"\"\n\tMake a (recursive) list of lists or dictionaries of values,\n\texcluding the key values if present in the values dict.\n\t\"\"\"\n\tresult = []\n\tfor v in values:\n\t\tresult.append({key"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\n\ndef params(sql, params=collections.OrderedDict()):\n\t\"\"\"\n\tGet the ``**`` parameters for a SQL query.\n\n\tAny format string used for conversion will be passed to\n\t:attr:`.format`. Otherwise, the original SQL query is returned.\n\n\t:Parameters:\n\n\t    sql: the SQL query\n\t    params: optional list of parameter names from the ``sql``\n            parameter dictionary. These are for ``format`` to be treated\n            as a :class:`~collections.abc.Mapping`.\t\"\"\"\n\n\t\treturn format(\t\"\"\"\n\tReturn the set of in-style"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tif not isinstance(sql, str):\n\t\t\traise ValueError(\"Expected `str` or `bytes`) for `format`\".\n\t\t\traise ValueError(\"Expected `%s` for `format`\" % sql)\n\n\t\tif not sql:\n\t\t\treturn ()\n\n\t\tif not isinstance(sql, str):\n\t\t\tsql, _ = str(sql), 0\n\n\t\tresult = []\n\n\t\tstart_index = None\n\t\tend_index = None\n\t\tcount = 0\n\t\tfor value_index, value in enumerate(sql):\n\t\t\tif value_index!= start_index and param_type(param"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn (format(self.sql_query, _in_table=True), params)\n\n\t@staticmethod\n\tdef _format(format_str, param_name):\n\t\tif param_name in FORMAT_FORMATS[format_str]:\n\t\t\treturn FORMAT_FORMATS[format_str][param_name]\n\t\telif param_name in FORMAT_FORMATS.keys():\n\t\t\treturn FORMAT_FORMATS[format_str][param_name]\n\t\t\n\tdef format(self, query, sql_query=None, params=None):\n\t\t\"\"\"\n\tFormat an SQL query"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t    \n\tif not isinstance(sql_type, SQLType) and not isinstance(params, Mapping) and not isinstance(sql_default_params, Mapping):\n\t\traise InvalidTypeParameterException(sql_type, params, sql_default_params)\n\t\n\tif sql_type == SQLParams.SQLParams.in_style:\n\t\tparams = sql_default_params.copy()\n\n\telif sql_type == SQLParams.SQLParams.is_ordinal:\n\t\ttry:\n\t\t\tparams = sql_default_params.copy()\n\n\t\t\tif params:\n\t\t\t\tparams = list(params"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tif not sql and not params and sql and not params and sql:\n\t\t\treturn None, None\n\t\telif sql and not params and sql.startswith(\"select\"):\n\t\t\tsql = \"SELECT \" + sql\n\t\telif sql:\n\t\t\tsql = \" WHERE \" + sql\n\telif sql:\n\t\tsql = self.to_sql(sql)\n\t\tif sql is None:\n\t\t\tsql = \"\"\n\t\tif params:\n\t\t\tparams = set(params)\n\t\t\tparams.update({\n\t\t\t\tk: self.sql_compiler.process(v)\n\t\t\t\tfor k, v in compat"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn self.format( \n\t\t\tsql=formatmany(\t\"\"\"\n\t\t\tconvert %s\n\t\t\torder by value\n\t\t\t\"\"\" % \", \".join(\n\t\t\t\tchain(*[\n\t\t\t\t \"in-style\",\n\t\t\t\t\t\"in-style paramstyle\",\n\t\t\t\t]) \n\t\t\t\tfor params in params_iter(params),\n\t\t\tnames=formatmany(  \"\"\"\n\t\t\t\tformatMany(%s)\"\"\" % \", \".join(\n\t\t\t\t\tchain(\n\t\t\t\t\t\t(\"in_style\", self.repr_params) +\n\t\t\t\t\t\t(\"positional\", self.repr_positional),\n\t\t\t\t\t\t(\"positional paramstyle\", self.repr_named"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tif self.__is__and__(params, self.sql):\n\t\t\treturn tuple(\n\t\t\t(self.sql % param) for param\n\t\t\tin itertools.chain(\n\t\t\t\tself.sql.split(\"&\"),\n\t\t\t\t[self.sql[0]] * len(self.sql)\n\t\t\t\tif self.sql else self.sql.split()),\n\t\t\t)\n\t\telse:\n\t\t\tif self.__is__eq__(params, self.sql):\n\t\t\t\treturn tuple(self.sql % param for param in (params, self.sql))\n\t\t\telse:\n\t\t\t\treturn tuple(self."}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\t# Handle nested lists:\n\t\tif isinstance(params, (list, tuple)):\n\t\t\treturn (\n\t\t\t\tsplit_parameters(\n\t\t\t\t\tparams,\n\t\t\t\t\t\", \".join(\n\t\t\t\t\t\tsplit_parameters(\n\t\t\t\t\t\t\ts,\n\t\t\t\t\t\t\t\t\t\t\t\tiscollection=isinstance,\n\t\t\t\t\t\t\t\t\t\t\t\ttuple=isinstance,\n\t\t\t\t\t\t\t\t\t\t\t\tstr=isscalar,\n\t\t\t\t\t\t\t\t\t\t\t\titer=issequence,\n\t\t\t\t\t\t\t\t\t\t\t\tfloat=isscalar,\n\t\t\t\t\t\t\t\t\t\t\t\tstr2=istext,\n\t\t\t\t\t\t\t\t\t\t\t\t\titerable=isinstance,\n\t\t\t\t\t\t\t\t\t\t\t\t\ttuplestr=lambda v: str(v),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\titervalue=isiter("}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\timport re\n\n\t\tdef validate_params(param, val):\n\t\t\tif type_exists(param):\n\t\t\t\tif isinstance(val, str) and val.lower() == param:\n\t\t\t\t\tyield param\n\t\t\telif isinstance(val, int):\n\t\t\t\tfor param in val.split(\", \"):\n\t\t\t\t\tvalue = parse(param)\n\t\t\t\t\tif value:\n\t\t\t\t\t\tyield param, value\n\n\t\tfor param in params:\n\t\t\tparam_is_numeric = isinstance(param, int)\n\t\t\tif param_is_numeric:\n\t\t\t\tvalidate_params(param.strip(), param)\n\n\t\tif not"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tif params is None:\n\t\t\tparams = {}\n\t\tresult = []\n\t\t\t  \n\t\tfor param in params.keys():\n\t\t\t\tif isinstance(params[param], dict):\n\t\t\t\t\t\tresult += formatmany(\n\t\t\t\t\t\t\t\t\"'\" + param + \"'\",\n\t\t\t\t\t\t\t\t{ c: params[c][0].get(c, '0') if c!= '0' else 'null' for c in param }\n\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t\tresult += formatmany(\n\t\t\t\t\t\t\t\t\"'\" + param + \"'\",\n\t\t\t\t\t\t\t\t{ c: params[param][0].get(c, '0')"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn self._formatmany(\n\t\t\t_formatmanyimpl[self._query.format](self._query_value(k), params))\n\n\tdef formatquery(\tself, *params ) -> \"FormatQuery\":\n\t\twith self._log_traceback(\"formatquery\"):\n\t\t\tres = self._formatmanyimpl[self._query.format](self._query_value(k, *params))\n\n\t\t\tself._logger.info(\n\t\t\t\t\"formatquery: %s, %r, %r, %r\",\n\t\t\t\tres,\n\t\t\t\tself._query_value(\"params\"),\n\t\t\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\n\n\ndef formatwhere(\t\"\"\"\n\tConvert a variable to an SQL query string with a formatted %s format.\n\tThis syntax supports special characters::\n\n\t\t>>> formatwhere(\"a LIKE 'hi-i'\"\n\t\tu\"b'hi-i'\")\n\t\tu\"like '% hi-i'\"\n\n\tIf the value is a percental string, it is formatted and return a string::\n\n\t\t>>> formatwhere('\"Hi % and \"Nok\\'n!\"'\n\t\tu'\"hi % and not \"Nok\\'n!\"'\n\n\tThis syntax is provided the same way as :meth"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tparams = collections.defaultdict(list)\n\t\tmapping = collections.defaultdict(list)\n\t\tif in_style: mapping = collections.abc.Mapping\n\t\tin_style = self._in_style\n\t\tif in_style == 'count':\n\t\t\tin_value = 0\n\t\telif in_style == 'values':\n\t\t\tin_value = self.count()\n\t\tif in_style not in params:\n\t\t\tparams[in_style] = in_value\n\t\tfor k, v in params.items():\n\t\t\tmapping[k].append(v)\n\t\treturn (\""}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\t# validate options\n\t\tif dbconn.connect(dbconn):\n\t\t\tdbconn.execute(dbconn.text())\n\t\t# return list of tuples containing the converted parameters\n\t\treturn self._convert_parameters(dbconn, *sql)\n\n\tdef sql_parameters(self, *sql):\n\t\treturn self._convert_parameters(dbconn, *sql)\n\n\tdef _convert_parameters(self, dbconn, *sql):\n\t\t# convert parameters from one query to another\n\t\tself._check_param_types(sql)\n\t\t\n\t\t# Convert params to another\n\t\tfor"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn formatmany_list(params)\n\n# vim: tabstop=4 shiftwidth=4 softtabstop=4\nimport os\nimport logging\nimport numpy as np\nimport pandas as pd\n\nfrom skimage.morphology import add_all\n\nfrom skimage import io\nfrom skimage.morphology import zoom\n\nfrom skimage.feature import peak_local_max, peak_local_mean\nfrom skimage.morphology.image import load_image\nfrom skimage.feature import peak_local_max_square, peak_local_mean_square\nfrom."}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\n\t\t# validate if pyfs.ROOT isn't empty\n\t\tif not path or not os.path.isdir(path):\n\t\t\treturn False\n\n\t\t# validate name of OCFL object\n\t\tbasename = ntpath.splitext(path)[0]\n\n\t\t# validate pathname\n\t\tif os.path.isfile(self.fs.fs_path_to_file(basename)) is False:\n\t\t\treturn False\n\n\t\t# validate link\n\t\tif os.path.islink(self.fs.fs_path_to_file(basename)) is False:\n\t\t\treturn False\n\n\t\t# validate"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n\tif path.lower()!= \"ocaml\":\n\t\traise IOError(\"Invalid or unsupported OCFL file at '%s'\" % path)\n\t\n\ttry:\n\t\tobj = self.root_open(path)\n\texcept IOError:\n\t\treturn False\n\t\n\tself.validate_file(obj, path)\n\treturn True\n\t\n#!/usr/bin/env python\n#\n# Copyright 2004 (C) 2003-2005 Nominum, Inc.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose with or without fee"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn self._read_check_ocfl_data(path)\n\n\tdef _read_check_ocfl_data(self, path):\n\t\t\"\"\"\n\t\tReads OCFL data from the OCSFL tree and returns self.ocfs object\n\n\t\t@type path: str\n\t\t@param path: path to file\n\t\t@rtype: pyfs\n\t\t@return: parsed OCFL object\n\t\t\"\"\"\n\t\tlog.VERBOSE_3(\"Reading OCFL data from %s\", path)\n\t\twith open(path, 'r') as fp:\n\t\t\tdata = fp"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif path is None:\n\t\t\tok = True\n\t\telif isinstance(path, basestring):\n\t\t\tok = True\n\t\t\tpath = path.split('/')\n\t\t\tfor item in path:\n\t\t\t\tif item not in self.ocsfl.__dict__:\n\t\t\t\t\tok = False\n\t\t\t\t\tbreak\n\t\treturn ok\n\n\tdef find_path_to_file_object(self, item):\n\t\t\"\"\"\n\t\tReturn path to the given item (from the OSFS path)\n\n\t\t:param item: an item to check\n\t\t:type item: basestring\n\n\t\t:return: path or pyfs object corresponding"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\t# Validate OCFL object at path or pyfs root.\n\t\trc = self.fs.get(path)\n\t\tif not rc:\n\t\t\trc = os.system(\"[openocfl?]\", flags=os.O_RDWR | os.O_BINARY)\n\t\tif rc:\n\t\t\treturn rc\n\t\telse:\n\t\t\t# If not success, return false.\n\t\t\treturn not rc\n\t\n\tdef get_object(self, path):\n\t\t\"\"\"\treturn the OCFL object at path or pyfs root.\"\"\"\n\t\trc = self.fs.get(path)"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tpath = get_resource_path(path)\n\t\t# we use pyfs:root for root and in the root of the tree\n\t\tif not path and (os.path.isdir(path[1:]) or os.path.isdir(path[0])):\n\t\t\treturn True\n\n\t\tif os.path.isdir(path[1:]):\n\t\t\treturn True\t# check if this path has a PyFS root\n\t\telif os.path.isdir(path[0]+os.sep):\n\t\t\treturn False\n\n\t\treturn self.get_ocf_root(path) and not os."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif not exists(path):\n\t\t\treturn False\n\t\t# get the path in our filesystem, e.g. it is one of ours\n\t\tpath = self.open(path)\n\t\tif not path:\n\t\t\t# it's not exists, return False\n\t\t\treturn False\n\t\t# get all the sub-directories, except for 'lib' and'share'\n\t\tsub_path = self.list(sub_path='lib', path='', dir_list=[])\n\t\tif not sub_path and not path:\n\t\t\t# No paths were found; return False!\n\t\t\treturn False"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\t\traise Error(\"Path '%s' doesn't exist\" % path)\n\t\tif not os.path.exists(os.path.join(self.folder, path)):\n\t\t\traise Error(\"Dir '%s' doesn't exist\" % path)\n\t\ttry:\n\t\t\tcFLobj = pyfs.OFL(path)\n\n\t\t\tvalid = False\t\t\t\t\t\t\t\t\n\t\t\tfor warning in cFLobj.warnings:\n\t\t\t\tif not (warning.category == WarningCategory.Warning):\n\t\t\t\t\tvalid = True\n\t\t\t\t\tbreak\n\t\t\tif not valid:"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\ttry:\n\t\t\tos.stat(path)\n\t\texcept IOError:\n\t\t\treturn False\n\t\tif not path.startswith(\".\") and not path.startswith(\"/\"):\n\t\t\tvalid_dir = \"/\"\n\t\telse:\n\t\t\tvalid_dir = \".\"\t\t\t\t# no valid directory, return False\n\t\tif self.validate_extension:\n\t\t\tfor dir in listdir(self.extensions_to_check):\n\t\t\t\tif not os.path.isdir(dir):\n\t\t\t\t\treturn False\n\t\treturn True\n\t\n\t\"\"\"\n\n\t@staticmethod\n\tdef validate_filename(filename):\n\t\tif not filename:"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tassert isinstance(path, basestring)\n\t\tassert isinstance(path, pyfs.BasePath), path\n\t\tassert path.suffix\n\t\tassert isinstance(path.ext, basestring), path.ext\n\t\tassert path.prefix\n\t\tassert isinstance(path.file.read(self.fs_id.fid), str), path.file.read(self.fs_id.fid)\n\t\tassert path.basePath is not None, \"valid path already relative to root\"\n\t\tassert path.dir.root == self.fs_id.dir\n\t\tassert path.flags.file==1"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self._data[\"status\"] \n\n\tdef validate_status(self, status_str):\n\t\t\"\"\"\n\t\tConvert status string returned if it looks like string.Status.\n\n\t\tIn case of error, try as little as possible:\n\t\n\t\t\t* Error, if validator status string is string\n\t\n\t\t\t\n\t\t\t* Failure, if validator status string has non-ASCII format.\n\n\t\tThe validator status is returned as an int.\n\tIf nothing matches the string return an exception. This is to be\n\tused in status messages. When the validator result is not a\n\tvalid validator result, this function"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self.__status[0] + (prefix or '')\n\t\n\tdef __str__(self):\n\t\treturn self.__status[1]\n\n\tdef get_output(self, value):\n\t\tif value in self.__output:\n\t\t\treturn self.__output[value]\n\t\toutput = ''\n\t\tfor i in range(len(self.__status)-1):\n\t\t\tif prefix:\n\t\t\t\toutput += \" (%s)\" % self.__status[i]\n\t\t\telse:\n\t\t\t\toutput += \" (%s)\" % self.__status[i]\n\t\tself.__status = output\n\t\treturn"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self._status[prefix]\n\tdef __str__(self, prefix=''):\t\t\"\"\"\n\tReturn string formatted for schema.\n\t\"\"\"\n\t\tret = '{'+self._name+'}: '+self._status[prefix]\n\t\tfor field, value in self._fields.items():\n\t\t\tret +='  \"'+ustr(ustr(field)+'').replace('-',u'+')+'\": '+self._get_field(field).replace('-',u'+')+'\"'+'@\"'+ustr(ustr(field)+'').replace('-',u'+')\n\t\t\t\t\t\tret += '\"'\n\t\tret += self._"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn '%s%s\\n' % (prefix, self.status_str)\n\n\tdef status_str_plural(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn string of validator status, with optional prefix.\n\t\t\"\"\"\n\t\treturn '\\n'.join([prefix, self.status_str])\n\n\t@staticmethod\n\tdef init():\n\t\tpass\n\n\tdef check(self, val):\n\t\tif self.value!= val:\n\t\t\tself.value = val\n\t\t\tself.save()\n\n\tdef value(self):\n\t\treturn self.value\n\n\tdef __str__("}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\t_f = self._status_str\n\t\tif prefix:\n\t\t\t_f = \"%s%s%s\" % (prefix, _f, \"OK\")\n\t\telse:\n\t\t\t_f = \"%s%s%s\" % (prefix, _f, \"OK\")\n\t\treturn _f\n\n\tdef validate(self, arg):\n\t\tif arg.isdigit():\n\t\t\tself._status[:arg] = self._value\n\t\telse:\n\t\t\tself._status = str(self.validator.get(arg))\n\n\tdef error(self, out):\n\t\tfor line"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\ts=self.validator_list()\n\t\tif prefix: s.append('<td>&lt;%s&#96;u</td> (' % prefix)\n\t\te=self.validator_validator(s)\n\t\ts=self.validator_list()\n\t\tif e:\n\t\t\ts.append('<td><a href=\"#\"+str(e)+\"' class=\"err\" '\n\t\tt=self.separator('\\n'+s, e)\n\t\treturn t\n\n\tdef status_str_with_sub(self, prefix=' '):\t\"\"\"\n\tReturn string of validator status,"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self.validator_prefix.join(['status'] + self.validator_prefixes)\n\n\tdef status(self, result_list=None):\n\t\t\"\"\"\n\t\tReturn status as human readable string.\n\t\tReturns None if there is no such status.\n\t\t\"\"\"\n\t\tif result_list is None:\n\t\t\tresult_list = self.validator_result_list()\n\t\tfor result in result_list:\n\t\t\tif result is None:\n\t\t\t\tresult = _('Unknown')\n\t\t\treturn result\n\t\telse:\n\t\t\treturn result\n\n\t### General validation methods\n\n\tdef validate(self"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn _format_str(self._status, prefix)\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn string.\n\t\t\"\"\"\n\t\treturn \"Status (%s)\" % self._status\n\n\nclass InvalidState(Exception):\n\t\"\"\"Raised if the current config file can't be updated.\"\"\"\n\n\tdef __init__(self, reason=None, code=None, message=None):\n\t\t\"\"\"\n\t\t\"Parameters:\n\t\treason - a description of why a user is in trouble\n\t\tcode - the code that occurred\n\t\tmessage - optional string that matches the error\n\t\t\"\"\""}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn self.validateStatus().get(prefix, '')\n\nclass ValidationError(Exception):\n\tpass\n\nclass BadParam(Exception):\n\tpass\n\ndef validate(validator, value, message, prefix=''):\t\"\"\"\n\tValidate a validator for the set 'value'. Returns True on success,\n\tFalse otherwise\n\t\"\"\"\n\tif not (isinstance(value, basestring) or isinstance(value, bool)):\n\t\traise ValidationError('%s: value must be string or bool' % message)\n\n\tmsg = validator.validate(value, prefix)\n\treturn msg\n\n# This will throw"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn '%s %s' % (prefix, self.get_name())\n\tdef get_status(self, prefix=''):\n\t\t\"\"\"\n\tGet the status if name 'prefix' was given.\n\t\"\"\"\n\t\tif self.get_name() == None:\n\t\t\traise KeyError('%s needs a name'%self.getName())\n\n\t\ttry:\n\t\t\treturn self.get_name(prefix)\n\t\texcept KeyError:\n\t\t\traise KeyError('%s not a valid validator.'%self.getValue())\n\tdef __repr__(self):\n\t\treturn str.__repr__(self)\n\t"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\ts = prefix + self.get_name() + self.get_date() + '.' + self.get_id() + '.'\n\t\treturn s\n\nclass ValidationLog(logging.Logger):\n\t\"\"\"\n\tUtility for generating validation logs\n\t\"\"\"\n\tdef __init__(self, logger, stream):\n\t\tlogging.Logger.__init__(self, logger)\n\t\tself.stream = stream\n\t\tself.log_level = logging.NOTSET\n\n\tdef flush(self):\n\t\t\"\"\"\n\t\tRemove any files from the log\n\t\t\"\"\"\n\t\treturn self.stream.flush"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"[{0}] [{1}] {{ {2}{3}\".format(\"\\t\".join(map(str, self)), prefix=prefix, self=\"\\t\".join(self.get_values()))\n\nclass ErrorHandler(object):\n\tdef __init__(self):\n\t\tself._errors = {}\n\t\n\tdef __enter__(self):\n\t\treturn self._errors\n\t\n\tdef __exit__(self, type, value, traceback):\n\t\tself._errors = None\n\nclass Parser(object):\n\t\"\"\"\n\tParse and execute an xml file in the form of \n\t<root"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tif(self.num_errors <= 0):\n\t\t\treturn self._str_number(prefix, self.name)\n\t\tfor i in range(1, self.num_errors):\n\t\t\tif(len(self.errors[i]) > 0):\n\t\t\t\treturn prefix + self.error_str(i, self.errors[i][0]) + '/' + self.error_str(i, self.errors[i][1])\n\t\t\telse:\n\t\t\t\treturn prefix + str(len(self.errors[i])) + '/'\n\t\treturn self._str_number(prefix"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"[%s]\\n%s\" % (prefix, self.name)\n\tdef __repr__(self):\n\t\treturn str(self.__dict__)\n\nclass Status(unittest.TestResult):\n\t\"\"\"\n\tTest result, which behaves much like TestSuite and behaves much like\n\tunittest.TestResult with other information. Use asserts like it can to\n\tcheck that the results are equal, or use unittest.TestResult.\n\t\"\"\"\n\n\tdef __init__(self, stream, descriptions=None, verbosity='NOTSET',\n                 summary_stream=None, verbosity_stream=None,\n                 ignore"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn self.str_to_str(self.__status_msg(prefix))\n\nclass BadTypeError(Exception):\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\n\tdef __str__(self):\n\t\treturn repr(self.msg)\n\nclass NotFound(NotFoundException):\n\tpass\n\nclass FileUploadError(Exception):\n\tpass\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport re\nimport os\n\n# define imports\nfrom util.url_util import convert, is_valid_url,"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn (prefix\n\t\t\t+ frappe.safe_cstr(self.doctype).replace(\";\", \";\")\n\t\t\t+ frappe.safe_cstr(self.name).replace(\":\", \";\"\n\t\t\t+ frappe.safe_cstr(frappe.defaults.report_type)).replace(\"-\", \".\")\n\t\t\t+ frappe.safe_cstr(self.doctype, \".\")\n\t\t).lstrip(\",\")\n\ndef get_default_value(field, name):\n\tif not field:\n\t\treturn \"\"\n\n\ttry:\n\t\treturn frappe.db.get_default(field, name)"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"[{0}] {1}\".format(prefix, self.path.status()[self.path.status().index('OK')])\n\n\tdef run_check(self, args):\n\t\t\"\"\"\n\t\tRun the validation process.\n\t\t\"\"\"\n\t\tif args.version:\n\t\t\tcmd = '{0} --version'.format(self.path.check(args) or '0')\n\t\telif self.verbosity > 0 and self.verbosity < 3:\n\t\t\tself.print_result = print_out = print_err = print_none = ''\n\t\telse:\n\t\t\tcmd ="}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn self.validation_log_status.format(prefix=prefix)\n\n\n\tdef _create_validation_log_entry(self, log, validation_level = None):\n\t\t\"\"\"\n\t\tCreate new ValidationLogEntry object matching the current user data.\n\t\tParameters:\n\t\t\tlog: Log instance containing the validation log data\n\t\t\"\"\"\n\t\tentry = ValidationLogEntry(self.user_name, log.email, log.status_str(prefix = ''))\n\t\tif validation_level is not None:\n\t\t\tentry.validation_level.set_text(validation_level)\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn ''.join((\"[%s] %s\" % (prefix, status_str(self.level,self.code))),\n\t\t\"* %s = %s\" % (prefix, str(self.level)) if self.level else '')\n\tdef log_level(self):\n\t\tprint(self.logger_level())\n\t\tif self.level is not None and self.level<= self.MAXLOGLEVEL:\n\t\t\traise ValidationLogError(\"max logfile must be <= %d (max loglevel %d)\" %( self.MAXLOGLEVEL, self.level))\n\n\tdef error("}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"[status] %s\" % (prefix or self.name)\n\n\tdef __str__(self):\n\t\treturn self.name\n\n\tdef __repr__(self):\n\t\treturn \"ValidationException: %s\" % self.err\n\n\t# This should really only be set by subclasses and not to be a good idea!\n\tdef get_code(self):\n\t\treturn self.__class__.__name__\n\n\tdef __add__(self, other):\n\t\t\"\"\"\n\t\tAdd self to another ValidationException, checking that there is a syntax error.\n\t\t(Note: self is added in"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\nclass Invalid(Base):\n\t\"\"\"Base class for valid tests\"\"\"\n\t__slots__ = ()\n\tbase_type = None\n\t# TODO: Implement test classes... we should have to implement these methods to implement\n\t# tests themselves\n\tdef set_base_type(self, base_type):\n\t\t\"\"\"Set the Type attribute of the Base class of this test\n\t\"\"\"\n\t\tself._base_type = base_type\n\n\tdef set_id(self, id):\n\t\t\"\"\"Set the Test.id attribute of the Test object\n\t\"\"\"\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self._user_data\n\n\tdef _create_user(self):\n\t\treturn User(username=self.username, email=self.email)\n\n\tdef _update_user_data(self):\n\t\tfor key in ['_id', '_url', '_name']:\n\t\t\tself._update_field(self._user, key, self._data[key])\n\n\tdef get(self, key, default=None):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\t@staticmethod\n\tdef _update_field(user"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\ttry:\treturn bool(identifier.lower() in self.constants or ident == 'x')\n\t\texcept AttributeError:\n\t\t\treturn False\n\n\t@internal\n\tdef write(self, identifier, value):\n\t\tsuper(Integer, self).write(identifier, value)\n\n\t@internal\n\tdef read(self, key=None):\n\t\treturn super(Integer, self).read(key)\n\n\tdef read_bool(self, value):\n\t\treturn bool(value)\n\n\tdef read_float(self, value):\n\t\treturn float(value)\n\n\t_is_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier!= self._client.id\n\t\n\tdef get_message_type(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn the message type for this message, one of \"error\" or \"warning\"\n\t\"\"\"\n\t\treturn identifier == self._client.id and IDENTIFIER_TYPE_ERROR\n\tdef is_valid_for_message_type(self, identifier, message_type):\n\t\t\"\"\"\n\t\tReturns True if the message type can be used for the given message.\n\t\tThis function can be used to check for valid messages based on the message"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self._classes_by_identifier.keys()\n\n\t@classmethod\n\tdef from_identifier(cls, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tCreate a new class based on a specified unique identifier.\n\n\t\t@param identifier: the unique identifier of the class to be created\n\t@return: a new class with the given identifier, or None if we don't know what it we should\n\t\traise ValueError if not.\t\"\"\"\n\t\treturn cls._classes_by_identifier[identifier]\n\n\tdef load_classes(self):  # pylint:"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self.identifiers\n\n\tdef get_or_create_identifier(self, entity):  # pylint: disable=unused-argument\t\"\"\"\n\tGet or create an entity identifier from the given entity identifier.\t\n\tReturns the identifier of this entity identifier if it exists, or ``None``.\n\t\n\t:param entity: The entity identifier to be created for (e.g. ``billy/``).\n\t:type entity: str\n\t:return: The identifier of this object or ``None``.\n\t:rtype: mixed\n\t\"\"\"\n\t\tassert isinstance(entity, URIRef"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn self.name == identifier\n\n\tdef get_code(self):\n\t\t\"\"\"Return the code with or without the name.\"\"\"\n\t\tif self.parent:\n\t\t\tc_class = self.parent.get_c_class_name(self.code)\n\t\t\tif self.code!= '' and not (c_class and self.code.strip() == ''):  # avoid a race condition\n\t\t\t\treturn self.code\n\t\treturn ''\n\n\tname = property(get_code)\n\n\nclass Enumerated(CodeString):\n\t\"\"\"\n\tThe name of a variable and the"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef get_valid_identifier(self, identifier):  # pylint: disable=unused-argument\t\n\t\t\"\"\"\n\t\tReturns True if identifier is valid, always True in this base implementation.\n\t\tThis needs to be defined for a reasonably-no-one test so that it's a base implementation.\n\t\tAll tests need to add this assertion check to the base implementation.\n\t\t\"\"\"\n\t\traise NotImplementedError#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2016 Google\n# GNU General Public License v3.0"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier in self._aliases\n\n\t@property\n\tdef names(self):\n\t\treturn self._data.data\n\n\t@property\n\tdef aliases(self):\n\t\treturn self.names[::-1]\n\n\tdef reset(self):\n\t\tpass\n\n\nclass InputFormatError(InterfaceError):\n\t\"\"\"\n\tRepresents an undefined input format.\n\t\"\"\"\n\n\tdef __init__(self, data=None, input_format=None, display_name=None):\n\t\tself._data = data\n\t\tself.input_format = input_format\n\t\tself.display_"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\tif self.is_valid_identifier(identifier):\n\t\t\treturn self.is_valid_identifier(identifier)\n\t\treturn False\n\n\tdef valid_identifier(self, token_type):  # pylint: disable=unused-argument\t\"\"\"\n\t\treturn True iff this token isn't valid (because we only care about tokens, so that\n\t\tthat are the token types you're checking from)\n\t\t\"\"\"\n\t\treturn False\n\n\tdef validate(self):\n\t\t\"\"\"\n\t\tValidate the base implementation.\n\t\t\"\"\"\n\t\tpass\n\nclass SimpleToken(Token):  # pylint: disable"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self._spec_version!= None:\n\t\t\treturn True\n\n\t\tif extract_spec_version and inventory.version!= None:\n\t\t\tself.spec_version = str(inventory.version)\n\t\t\tself.spec_version_error = ValueError\n\t\t\tprint \"[NAVIGATION]: SPECTRA version wrong - the current version of NAVIGATION is %s\" % self._spec_version\n\t\t\traise ValueError\n\n\tdef get_new_spec_version(self):\n\t\treturn self.spec_version\n#!/usr/bin/env python\n\n\"\"\"The ``panda"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not self.spec_version:\n\t\t\tself.spec_version = 'unknown'\n\t\t\n\t\tself.spec_version    = self.get_spec_version()\n\t\tlogger.info(\"Validating %s -> %s\" % (self.spec_version, self.spec_version.upper()))\n\t\t\n\t\t# TODO: This only checks the version attribute, and makes it easier to check\n\t\tif extract_spec_version:\n\t\t\tif self.spec_version == 'unknown':\n\t\t\t\tself.spec_version = 'unknown'\n\t\t\t\tlogger.warn(\"No version found in given"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not isinstance(inventory, self.class_path):\n\t\t\treturn None\n\t\tmeta_version = self.get_version(inventory)\n\t\tif meta_version!= self.version:\n\t\t\t# This shouldn't happen, if the inventory doesn't contain a version\n\t\t\treturn None\n\t\telse:\n\t\t\treturn parse_version(meta_version)\n\n\tdef get_version(self, inventory):\n#\t\t'Validate against version spec'\n\t\ttry:\n\t\t\tself.version = get_version(inventory)\n\t\texcept InvalidSpecVersion as v:\n\t\t\t# we don"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\t\n\tif extract_spec_version and self.spec_version_id == self.spec_version_id:\n\t\tvalid = False\n\telse:\n\t\tvalid = validate_spec_version(inventory)\t\n\t\n\tif parsed_version(valid):\n\t\tself.spec_version = parsed_version(valid)\t\n\tif self.spec_version_id > self.spec_version:\n\t\traise InvalidSpecificationVersion\n\n\ndef validate_spec_version(host_version):\n\t\"\"\"\n\tTries to determine the spec version of the given value based on the provided value"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tself.check_version(inventory)\n\n\tdef check_version(self, inventory):\n\t\t\"\"\"\n\t\tCheck that the given inventory has the version specified. It is\n\t\ta dictionary of {version: requirement}. Check that a file is\n\t\tcontacted, as defined by _load_yaml.\n\t\tReturns an error message if the given json object doesn't fit the requirements\n\t\tdict.\"\"\"\n\t\tversion = None\n\t\tif not isinstance(inventory, dict) and not isinstance(inventory, str):\n\t\t\traise Exception(\"json object must be dict or string.\")\n\t\telif not isinstance("}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not inventory.has_key('type'):\n\t\t\tprint>> sys.stderr, \"\"\"Invalid inventory spec.\nPlease specify a valid inventory spec.\"\"\"\n\t\t\treturn -1\n\t\ttype_str, type_value = inventory['type'].split(':', 1)\n\t\tself.spec_version = None\n\t\tif type_value in ACTIONS.values():\n\t\t\tself.spec_version = type_value\n\t\telif '.' in type_value:\n\t\t\ttry:\n\t\t\t\tself.type = ACTIONS[type_value]\n\t\t\t\t# set the correct type to that the spec_version is valid"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\n\n\tif hasattr(self,'version'):\n\t\tif self.version!= inventory.spec_version:\n\t\t\tif not self.check_specs():\n\t\t\t\t# No change found!\n\t\t\t\treturn False\t\n\t\t\t# TODO: Handle spec version and return true only if it's supported\n\t\t\tif extract_spec_version:\n\t\t\t\treturn True\t\n\n\treturn False\n\n\nclass InventoryManager(object):\n\t\"\"\" A simple manager class for inventory management \"\"\"\n\tdef __init__(self):\n\t\t\"\"\" Creates a new inventory manager and sets the variables. \"\"\"\n\t\tself.inventory = None\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version:\t\t# if not in a requirement (either in the user or\n\t\t\t# a requirement set) (if None then it will look at requirements)\n\t\t\tif extract_spec_version: \t# if is the only option (that's not in spec data) - this is considered as a requirement\n\t\t\t\tif not self.specs.find_requirements(self.spec, ignore_restrictions=True):\t# this comes from the requirement\n\t\t\t\t\treturn None\t\t\t\t# don't check for restrictions\n\t\t\treturn None\t\t\t# use the default value for the current spec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\ttry:\n\t\t\t# This may not exist yet, if this is ever triggered for some reason.\n\t\t\tinventory_type = [x.strip() for x in self.spec_version.split('.')]\n\t\t\tif not inventory_type:\n\t\t\t\t# There is no type\n\t\t\t\treturn None\n\n\t\t\tparsed_version = inventory_type[inventory.split('.')[-1]]\n\t\t\tif len(parsed_version) > 2:\n\t\t\t\t# More than 2 inventory versions for some reason, try\n\t\t\t\t# only the two.\n\t\t\t\tformatted_version = \"\"\n\t\t\t\twhile len(parsed_version) <"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tversion = None\n\t\tfor spec in self.spec_version:\n\t\t\tversion = spec.get(spec.get_attr('type'), None)\n\t\t\tif version == spec.get_attr('type', None):\n\t\t\t\tbreak\n\t\tif not version:\n\t\t\tprint(\"[!] The spec_version is invalid, can't check for a spec version that matches the spec_version string!\")\n\t\t\treturn None\n\t\tif extract_spec_version:\n\t\t\tversion = gather_from_collection(inventory, \"specs\", None, None)\n\t\treturn version\n\n\tdef _parse_command"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tif len(manifest_files) == 0:\n\t\t\treturn True\n\t\t# Check what are needed in the manifest if they are.\n\t\tfor i in manifest_files:\n\t\t\t# Don't check for preconditions.\n\t\t\tif i in manifest_defaults:\n\t\t\t\tlog.info(i + \" was used\")\n\t\t\t\treturn False\n\t\t# Check what are needed for the script if they are.\n\t\tuse_preconditions = ''\n\t\tif len(os.listdir(manifest_files[0])) == 0:\n\t\t\t# If it's not found, it's a normal file,"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor manifest in manifest_files:\n\t\t\tfor digest in digests_used:\n\t\t\t\tresult = self.digests_cache.get(digest)\n\t\t\t\tif not result or not result:\n\t\t\t\t\tself.logger.warn(\"Digest found but not needed.\",\n\t\t\t\t\tself.name, self.version, self.digest)\n\t\t\t\treturn result\n\n\tdef fetch_manifest(self, manifest_data=None, cache=None, extract=True):\n\t\t\"\"\"\n\t\tFetches a single manifest for this package. If a manifest data is not\n\t\tprovided, the default implementation is to read all"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor file in manifest_files:\n\t\t\tdigests_in_file_dir = set()\n\t\t\tfor digest in self._digests_in_file_dir:\n\t\t\t\tif digest.digest_for_directory(file).digest_for_digest(digest):\n\t\t\t\t\tdigests_in_file_dir.add(digest.digest_for_directory(file))\n\t\t\tif digests_in_file_dir:\n\t\t\t\tchecksums = []\n\t\t\t\tfor i in range(0,digests_in_file_dir):\n\t\t\t\t\tself._digests_in_file_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tdigests_used = set(digests_used)\n\t\tfor f in manifest_files.keys():\n\t\t\tmanifest = self.__manifest_file(f)\n\t\t\tif os.path.exists(manifest) and (digests_used.issubset(manifest)):\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\traise Exception(\"Digest file %s not present!\" % f)\n\n\tdef verify_digests_not_provided(self, manifest_files):\n\t\tpass\n#\t\tfor f in manifest_files.keys():\n#\t\t\tif not os.path.exists(f"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\t# check if the number of digests is present\n\t\tif len(manifest_files)==0:\n\t\t\treturn False\n\n\t\t# ensure that we have the proper number of digests we support\n\t\tif len(digests_used)==0:\n\t\t\tdigest_number=0\n\t\telse:\n\t\t\tif len(digests_used)==1:\n\t\t\t\t# there should be only one digest available for this file\n\t\t\t\tdigest_number=len(digests_used[0])/2\n\t\t\n\t\t# get the size of each hash\n\t\tfor h in manifest_files:\n\t\t\tk=h"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\t# Ensure we have at least one available file.\n\t\tif len(manifest_files)!= len(digests_used):\n\t\t\tself.logger.error(\"No manifest files available!\")\n\t\t\treturn\n\n\t\tself.logger.debug(\"Installing digest from: %s\", \",\".join(manifest_files))\n\n\t\t# Add all the files from the manifest.\n\t\tself.add_file_hashes(manifest_files)\n\n\t\t# Validate that digests are used.\n\t\tself.validate_digests_is_used(manifest_files, digests_used)\t\n\n\n\tdef validate"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor file_name, content_hash in manifest_files:\n\t\t\tdata = self.read(file_name)\n\t\t\tif data and digests_used and content_hash!= data['hash']:\n\t\t\t\tself.logger.warning (\n\t\t\t\t\t\"Digests hash {} mismatch. File {} differs \"\n\t\t\t\t\t\"in {}.\".format(digests_used, file_name, self.digest_file.name))\n\n\nclass MalformedManifest(Exception):\n\t\"\"\"\n\tAn exception for manifest_files that indicate a bad manifest format or\n\ta directory that does not exist.\n\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\t# Collect hashes.\n\t\texisting_hashes_count = 0\n\t\tfor file_name in manifest_files.keys():\n\t\t\ttry:\n\t\t\t\tfile_name_hash = hashlib.sha1()\n\t\t\t\tfile_name_hash.update(manifest_files[file_name]['hash'])\n\t\t\texcept KeyError:\n\t\t\t\tcontinue\n\t\t\tif not file_name.startswith('__'):\n\t\t\t\t# Any hash provided by a manifest file is a file name. The file name was not specified\n\t\t\t\tprint '[Digests] Missing signature file %s, ignored'\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tdigests_used = self.fetch_digest_info()\n\t\tif digests_used is None or digests_used == 0: return\n\n\tdef fetch_digest_info(self):\n\t\toutput = shell.check_output(self.digest_command)\n\t\tstdout = \"\"\n\n\t\terror = [\n\t\t\t\"Can't read digest file: %s\" % self.digest_command,\n\t\t\t\"Maybe it's not there (maybe this might cause incorrect data):\",\n\t\t\t\"You may encounter this error\",\n\t\t\t\"If any of \\\"uninitialized\\\" digests"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tif not self.has_manifests:\n\t\t\treturn\n\t\tif not manifests_used:\n\t\t\treturn\n\n\t\tcheck_digests_present_and_used_manifest(manifest_files)\n\n\tdef get_manifests_and_digests(self):\n\t\t\"\"\"Determine if digests can be found and used.\"\"\"\n\t\tif self.has_manifests:\n\t\t\tself.check_digests_present_and_used(self.manifest_files)\n\n\tdef get_manifest_file_name(self, filename):\n\t\tif self.has_files:\n\t\t"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tself.prior_object = prior.inventory\n\t\tself.expected_object = prior.inventory\n\t\tself.invalid_object = prior.inventory.invalid_inventory\n\t\tself.error_object = prior.inventory.error_inventory\n\n\tdef evaluate(self, privs):\n\t\tassert (isinstance(privs, InventoryValidator))\n\n\t\t# Validate prior data\n\t\tif set(privs.inventory) == set(self.expected_object):\n\n\t\t\t# Try validation of any other valid privs\n\t\t\tself.validate_as_prior_version(privs.valid)\n\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif len(prior.valid) < self.history_store.num_prior_versions():\n\t\t\traise InventoryValidationError(\"Invalid prior version {0}. Try a different version to check prior from: {1}\".format(prior, self))\n\n\t## @var historiz_version\n\t# Historiz version of the Inventory\n\tdef validate_historiz_version(self, historiz_version):\n\t\t\"\"\"\n\tValidates whether the given version is valid prior to the current inventory object.\n\n\tThe input variable historiz_version is also expected to be an InventoryValidator object\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\n\t\n\tvalidate_as_pri = self.inv.validate_non_equal_prior_version(prior.version)\n\t\t\n\treturn self\n\t\n\t# validate all incoming lists\n\tdef validate_incoming_lists(self):\n\t\tfor inventory in self._inventory_list:\n\t\t\tvalidationResult = inventory._checkInList(self.inventory, self._inventory_type)\n\t\t\tif validationResult['type'] == 'valid':\n\t\t\t\tself._inventory_list[inventory].validate(validationResult['data'])\n\t\t\t\t\n\t# validate all outgoing lists\n\tdef validate_outgoing_lists(self):\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tinventory = self.inventory_store.find(prior) \n\t\tif inventory and inventory.is_valid():\n\t\t\tself.version_validator = InventoryVersionValidator(\n\t\t\t\tself.inventory_store,\n\t\t\t\tself.inventory_datastore\n\t\t)\n\t\t\nclass Prio(object):\n\t\"\"\"\n\tThis class is responsible for representing a Prio\n\tobject. A class to handle prio.Inventory represents an input data or\n\toutput data.\n\t\"\"\"\n\tdef inventory_data_is_valid(self):\n\t\t\"\"\"\n\t\tIf prio is valid, this method"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tfrom enigma import eTimer\n\t\tfrom Tools import Notifications\n\n\t\tfor data in prior.split('\\n'):\n\t\t\tif len(data.split()) < 2:\n\t\t\t\tcontinue\n\t\t\tversion = int(data.rstrip().split()[1])\n\t\t\tif version < EPG_SUBMIT_VERSION:\n\t\t\t\twarn_if_not_valid_version(prior, version)\n\t\t\t\t# TODO: should there be a way for now?\n\t\t\t\treturn False\n\n\tdef check_productions_ready(self, allow_new_versions = False, allow_duplicate_versions = False,"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tassert type(prior) in (InventoryValidator, dict, str)\n\t\t\n\t\tassert prior.name is not None\n\t\tassert prior.inventory is not None\n\t\tassert prior.version is not None\n\n\t@staticmethod\n\tdef validate_inventory(inventory, prior):\n\t\t\"\"\"\n\t\tRaises a ValueError if invalid prior version or if this is not within\n\t\ta valid priority object. Returns None if invalid prior version or if there is\n\t\tnon-expired version within the inventory.\n\t\t\"\"\"\n\t\tfor value in prior:\n\t\t\tif value not in inventory.inventory:\n\t\t\t\traise ValueError(\"'%s"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# Checks if priors are valid\n\t\tif type(prior).__name__!= 'InventoryValidator':\n\t\t\traise TypeError('Prior must be a InventoryValidator object (not %s)' % type(prior).__name__)\n\t\tif not prior.is_empty or prior.version!= prior:\n\t\t\traise InventoryValidatorError('Prior version must not be equal and greater than or equal to version %s' % prior)\n\nclass InventoryValidator:\n\t\"\"\"\n\tClass for checking inventory files. \n\t\n\tThe validation method must be assigned to the __init__() attribute\n\tto avoid a circular"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tversion = Priority(prior.inventory)\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise PriorityError(_('Invalid prior %s') % self)\n\n\t\tfor k, v in prior.asdict().items():\n\t\t\tif not isinstance(v, dict):\n\t\t\t\traise PriorityError(_('Invalid as_prior %s') % prior)\n\t\t\tfor i in xrange(len(violations)):\n\t\t\t\tself.asdict()[k].append(v[i])\n\t\t\tfor vitem in v.values():\n\t\t\t\tself.asdict()[k].append(vitem["}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# assert prior is not None, \"prior is None, but not a valid prior\"\n\n\t\tif prior is None or prior.is_valid():\n\t\t\traise InventoryValidationError(\"No inventory has already been assigned to a version \"\n\t\t\t       \"given on %s %s\" % (type(prior).__name__, prior))\n\n\tdef validate_as_current(self, current):\n\t\t# assert current is not None, \"current is None, but not a valid current\"\n\t\tif current.is_valid():\n\t\t\traise InventoryValidationError(\"Current inventory has already been assigned to an version \"\n\t\t\t      "}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise InventoryValidatorException(\"Invalid prior argument.\")\n\t\tself.prior = prior\n\t\tself.currentInventory = prior\n\n\tdef _validate_inventory_type(self):\n\t\t\"\"\"\n\t\tVerifies that the proper inventory type is of the current inventory object.\n\t\t\"\"\"\n\t\tif not isinstance(self.currentInventory, Inventory):\n\t\t\traise InventoryValidatorException(\"Invalid current inventory\")\n\n\tdef _validate_inventory_type_by_data(self):\n\t\t\"\"\"\n\t\tValidates type of inventory and creates a new inventory.\n\t\t\"\"\""}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\n\t# If there's version '0.0' in inventory, then check that the first version\n\tlogical_path_in_state = []\n\tif version and '1' in inventory[version]['content_files'] and inventory[version]['content_files']['1']:\n\t\tif inventory[version]['content_files']['1']:\n\t\t\tlogical_path_in_state = get_logical_path_map(inventory, version)\n\treturn logical_path_in_state\n\n\n# ---\n\n\nif __name__ == '__main__' == __name__:\n\tversion = get"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tlogical_path_in_state = {}\n\t\t# get all files in state\n\t\tfor state in iteritems(inventory.get_state()):\n\t\t\tlogical_path_in_state[state] = set([file for file in state.get_file_set() if file.type == 'file'])\n\t\t# determine filename map files and check for duplicates\n\t\tfor path in itervalues(logical_path_in_state):\n\t\t\tstate_map = set(state.get_file_set())\n\t\t\tif path in state_map:\n\t\t\t\tlog_info(\"File {0} appears"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n#\tinventory.version = version\n#\tinventory.version = InventoryVersion.InventoryVersion(inventory.version)\n\ndef get_inventory_version(inventory):\n\t\"\"\"\n\tGet the version of inventory for this host.\n\nReturns:\n\tthe inventory version as defined in Galaxy's Default\n\tinventory.version attribute, plus version number:\n\t{{inventory.version}}\n\"\"\"\n\ttry:\n\t\tinventory_version = inventory.version\n\texcept AttributeError:\n\t\t# inventory version is not defined in Galaxy.\n\t\tinventory_version = inventory.inventory_version\n\t\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\t# Use a single set because it will only contain the set of files in self.content_map,\n\t\t# not all of the entries in inventory.\t\n\t\tin_state = set()\n\t\tfor path in inventory:\n\t\t\t# Look for unique file content paths, first by removing duplicates so far\n\t\t\t# that we have to take the first one.\n\t\t\tunique_id = path.pop()\n\t\t\tunique_id_in_state = len([x for x in inventory if x.startswith(unique_id)])\n\t\t\tin_state.add(unique_id + \" (\" +"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\t\n\tlogical_path_map = {}\n\tfor file_path in inventory._files_for_version_for_inventory(version):\n\t\tlogical_path_from_file = find_logical_path(file_path)\n\t\tlogical_path_map[file_path] = logical_path_from_file\n\t\t\n\treturn logical_path_map\n\ndef find_logical_path(file_path):\n\t\"\"\"\n\tGiven a file path and a versioned file path, find the logical path\n\tin the state.\n\t\"\"\"\n\treturn None\n\ndef make_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\ndef get_logical_path_map_by_version(inventory, version):\n\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\n\t\n# TODO: reenable after v7 support\n# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79:\n# coding: utf-8\nimport datetime\nfrom south"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tlogical_path_in_state = {}\n\t\tif version not in inventory:\n\t\t\tinventory[version] = set()\n\t\tfor version2file in inventory[version]:\n\t\t\tcontents = get_file_contents(inventory, version2file)\n\t\t\tfor root, dirs, files in os.walk(inventory['root']):\n\t\t\t\tfor path in files:\n\t\t\t\t\tif path.endswith('conf'):\n\t\t\t\t\t\troot = os.path.join(root, path)\n\t\t\t\t\trelative = inventory['root'] + '/' + path\n\t\t\t\t\tlogical_path_in_state[path] = relative\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\t# make sure this isn't a list since we're immutable\n\t\tif not isinstance(inventory, list):\n\t\t\tinventory = [inventory]\n\n\t\t# make sure inventory was loaded first by this class\n\t\tif type(inventory).__name__ not in inventory:\n\t\t\tinventory = [inventory]\n\n\t\t# make sure inventory is an immutable type\n\t\tif not isinstance(inventory, inventory_t):\n\t\t\tinventory = inventory_t(inventory)\n\n\t\tpath_map = {}\n\n\t\t# loop through each inventory file (this may consist of two lists in\n\t\t# parallel. The first list in the list"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\t# get a logical_path_in_state -> set(content_files)\n\t\t# it is easier to do:\n\t\tphysical_paths = set()\n\t\tlogical_path_in_state = set()\n\t\t\n\t\tfor (path1, path2, f) in inventory:\n\t\t\tif f == version:\n\t\t\t\tphysical_paths.add(path1)\n\t\t\t\tlogical_path_in_state.add(path2)\n\t\t\n\t\t# we have at least one state path found in the state \n\t\tif len(physical_paths) == 0:\n\t\t\traise RuntimeError(\"No path"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\t\n\tlogical_path_in_state = {}\n\t\t\n\tfor path in inventory.keys():\n\t\tfor index, path_state in enumerate(inventory[path]):\n\t\t\tif index == len( inventory[path_state] ) - 1:\n\t\t\t\tcontinue\n\t\t\tif path_state in version:\n\t\t\t\tlogical_path_in_state[path] = set(inventory[path_state])\n\t\t\t\tcontinue\n\t\t\tlogical_path_in_state[path].add(inventory[path_state])\t\t\t\n\t\n\treturn logical_path_in_state\n\t\ndef collect_paths(version):"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\n\tif fixity!= \"\":\n\t\tlines_to_check = []\n\t\tfor i in range(1, len(manifest_files)):\n\t\t\tfiles = manifest_files[i]\n\t\t\tif files[path.find(fixity, \".\")+1 :]:\n\t\t\t\tlines_to_check.append(files)\t\n\n\t\tif len(lines_to_check):\n\t\t\twritemsg(_(\"Error: Could not check for fixity changes. Please verify that all files in %s do have the fixity block %s.\\n\") % (manifest_files, len(lines_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\tself.validate_file(self.check_files(manifest_file), os.listdir(manifest_file))\n\t\tfor file in manifest_files:\n\t\t\tself.validate_file(self.check_files(file), os.listdir(file))\n# The MIT License (MIT)\n#\n# Copyright (c) 2015-2019 Cumulus Networks, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfiles_found = True\n\t\tfor file in manifest_files:\n\t\t\tmanifest_file = file.replace(\"manifest.xml\", \"manifest.yml\")\n\t\t\t\n\t\t\ttry:\n\t\t\t\t# read the file; parse it\n\t\t\t\ttree = ET.XML(file)\n\t\t\t\troot = tree.getroot()\n\t\t\t\ttree.remove(file)\n\t\t\texcept:\n\t\t\t\ttree_files.append(file)\n\t\t\t\tself.log.warning(\"Missing fixity block for file from %s\" % file)\n\t\t\t\tself.log.warning(tree_files)\n\t\t\t\tcontinue\n\n\t\t\tfor field in"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\tfor f in manifest_files:\n\t\tif f[0] == \"/\":\tcontinue\n\t\tif f[-1:] == \".gitignore\": continue\n\t\tif os.path.isfile(f):\n\t\t\tfor f in manifest_files:\n\t\t\t\tif f.endswith(f) == True:\n\t\t\t\t\tself.load_resource(f[:-4])\n\t\t\tif os.path.isfile(manifest_files[-1]):\n\t\t\t\tif self.load_resource(manifest_files[-1])!= True and (self.load_resource(manifest_files[-1]) == False): #"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\n\ttotal_files_list = dict(zip(MANIFEST_FILES, manifest_files))\n\ttotal_files_list2 = dict(zip(MANIFEST_FILES2, manifest_files2))\n\t\n\tif fixity:\n\t\traise ValueError('Invalid fixity block format!')\n\n\tmanifest_files2['files'] = list(set)\n\tmanifest_files2['files2'] = list(set)\n\n\t# Validate the version.\n\tmanifest_files['version'] = None\n\tif'version' in manifest_files:\n\t\tmanifest_files['version'] = int("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\n\t\n\t# validate the fixity block\n\tfor file_path in manifest_files:\n\t\ttry:\n\t\t\tfile_ = open(file_path, \"r\")\n\t\t\tlines = file_.readlines()\n\t\t\tfile_.close()\n\t\t\tself._validate_fixity(lines[fixity],file_)\n\t\t\n\t\texcept (IOError, OSError):\n\t\t\treturn [\"{0} not found in {1}\".format(file,self.path)]\n\t\n\tif fixity!= 0:\n\t\t# generate files that could have been generated\n\t\tfiles = os.walk(self.path)\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor file_id, file in manifest_files.iteritems():\n\t\t\t# The main file must have been checked. There is a way for\n\t\t\t# determining if the file is referenced in a broken install;\n\t\t\t# if the file is referenced in a uninstalled group, then\n\t\t\t# then use this check. Only make sure to consider broken\n\t\t\t# files not referenced in a broken package (where the\n\t\t\t# file is referenced)\n\t\t\tif file[\"package\"] == package_pack:\n\t\t\t\tfile_path = file[\"path\"]\n\t\t\t\tif not os.path.isdir(file_path"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tif len([x for x in manifest_files if x.endswith(fixity)]) > 0:\n\t\t\tself.removeEmptyPackage(manifest_files)\n\t\tmanifest_files.reverse()\n\n\tdef get_bundle_list_item(self, bundle_path):\n\t\t\"\"\"\n\t\tReturns a list of all bundle's contents.\n\t\t\"\"\"\n\t\treturn glob(os.path.join(bundle_path, self.files_to_scan)).group(1)\n\t\t\n\tdef get_resource_files_from_manifest(self, manifest_files):\n\t\t\"\"\"\n\t\tReturns"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tdef validate_repair(p, f):\n\t\t\tdef check_missing(f, fn, *args):\n\t\t\t\tif fn in f:\n\t\t\t\t\treturn f[fn]\n\t\t\t\treturn fn\n\t\t\tp.files_to_repair = check_missing\n\t\t\tfor fn in manifest_files:\n\t\t\t\tfnp = os.path.join(p.data_dir, fn)\n\t\t\t\tif not os.path.isfile(fnp):\n\t\t\t\t\tp.files_dont_fix.append(fn)\n\t\tfor i in (\"$\"):\n\t\t\tcheck_repair(config_file("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tpass\n\t\t\n\tdef get_manifest_files_from_fixity(self, fixity):\n\t\tfor manifest_file in manifest_files:\t\t\n\t\t\tmanifest_file_path = os.path.join(self.inventory_full, manifest_file)\n\t\t\tfor fixity_file in os.listdir(os.path.dirname(manifest_file)):\n\t\t\t\tif manifest_file_path.endswith(\"*\" + fixity):\n\t\t\t\t\tfiles = os.listdir(os.path.dirname(manifest_file))\n\t\t\t\t\tfor fixity_file in files:\n\t\t\t\t\t\tif fixity"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tif not os.path.isdir(path): path = os.getcwd()\n\n\t\tfiles_str = \"/%s/%s\" % (path, files_name.replace(\"\\\\\", \"/\"))\n\t\treturn files_list(os.path.join(path, files_str))\n\telse:\n\t\tprint path\n\t\tprint files_name\n\t\treturn []import os\n\nfrom django.forms import ModelForm, Widget, IntegerField\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import ugettext\n\n\nfrom..models import User\n\n\n'''\nThis decorator gets"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\twith codecs.open(path, 'r', 'utf-8') as fp:\n\t\t\treturn [ line for line in fp.readlines() \\\n\t\t\t   if re.search('\\s\\S', line) is not None ]\n\t\t\n\n#####################\n# Main function        #\n#####################\n\t\n\t\ndef main(infiles):\n\t\"\"\"\n\tMain function\t\n\t\"\"\"\t\n\t\n\t\n\tif len(infiles) == 1: \n\t\tpath_or_input = infiles[0]\t\n\t\t#check if it is a path or input file\n\t\tif os.path.isfile"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tdirs.get_config_value(\"main\", \"FilesList\")\n\t\treturn [os.path.join(d, f) for d, f in os.walk(path) if f.endswith(\".py\")]\n\n\"\"\"\ndef get_config_help(path):\t\"\"\"\n\tSet configuration parameter for `get_config()` using the given `path`\t\t\"\"\"\n\tprint(\" \tDefault configuration:\\n \t\" +path + os.environ[\"FOOBAR_CONFIG\"])\n\n\treturn 1\n\n\"\"\"\ndef config_exists(path):\n\t\n\twith open("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn list(os.listdir(path))\n\n##############\n# SELF\n##############\n\ndef get_files(path):\t\"\"\"\n\tThis function returns the list of files in `path`\t\"\"\"\n\t\treturn list(set(os.listdir(path)))\n\n##############\n# MODELS\n##############\n\ndef update(path):\t\"\"\"\n\tThis function updates the list of folders under `path` to `path` and\n\treturn True if changes were made\n\t\"\"\"\n\t\tpath = path.replace(os.sep, '/')\n\t\tif os"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tfiles = []\n\t\tfor root, dirs, files in os.walk(path):\n\t\t\tfor file in files:\n\t\t\t\tif not file.startswith('.'):\n\t\t\t\t\tfiles.append((os.path.join(root, file), file))\n\t\treturn files\n\n\ndef get_file(path, file):\n\t\"\"\"\n\tReturn the absolute path to a given file \n\t\"\"\"\n\treturn os.path.abspath(os.path.join(path, file))\n\n\ndef get_file_info(path):\n\t\"\"\"\n\tReturn the information about the file \n\t\"\"\"\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [ p for p in os.listdir(path) ]\n\n\tdef get_files_list(path):\n\t\t\"\"\"\n\t\tRetrieve files in directory `path`\n\t\t\"\"\"\n\t\treturn [ p for p in os.listdir(path) if p.startswith( path ) ]\n\n\tdef print_results(path, res):\n\t\t\"\"\"\n\t\tPrint results from `path`\n\t\t\"\"\"\n\t\tprint >>open(path, \"w\"), str(res) # add the line to the output\n\n\tdef remove_tree_dir(path):\n\t\t\"\"\"\n\t\tRemove tree directory"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tdata = []\n\t\tf = 0\n\t\ttry:\n\t\t\tfor f in os.listdir(path):\n\t\t\t\tif f in ignore: continue\n\t\t\t\tdata.append(\"%s\\t%s\\n\" % (path, f.replace(\".\",\"\")))\n\t\t\t\tf = f + 1\n\t\texcept Exception, e:\n\t\t\tprint(\"[Error] Cannot open %s: %s\" % (path, e))\n\t\t\treturn data\n\treturn files_list\n\n# This function is a \"file:list\" file.\n# A list file can be nested in different directories.\n"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tfiles = []\n\t\tfor row in csv.reader([file for file in os.listdir(path)]):\n\t\t\tfiles.append(file)\n\t\treturn files\n\nclass Householder:\n\t\"\"\"\n\tA householder.\n\t\"\"\"\n\tdef __init__(self, householder_id, householder_name, householder_address, householder_city, householder_state):\n\t\tself.householder_id = householder_id\n\t\tself.householder_name = householder_name\n\t\tself.householder_address = householder_address\n\t\t"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\n#\tprint \"check_files.py - path: \", path\n#\tpprint.pprint(files)\n#\tfor f in files:\t# check the file\n\t\tm = re.search(f, path)\n\t\tif m!= None:\n\t\t\tfiles_list.append( m.group(1) )\n\n#\tpprint.pprint(files_list)\n\n\treturn files_list\n\n\n\ndef process_files(files):\n\t\"\"\"\n\tProcess the files.\n\tReturns a list of tuples of (files_list, folder_list), where 'folder_"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tresult = []\n\t\tfor file in os.listdir(path):\n\t\t\tif (file.endswith('.xml') and not files_xml_exists(path + file)):\n\t\t\t\tresult.append(os.path.join(path, file))\n\n\t\treturn result\t\n\tdef get_file_md5_size(path):\n\t\tmime = \"application/octet-stream\"\n\t\tfile = io.StringIO(open(path, \"rb\").read())\n\t\tsize = os.path.getsize(file)\n\t\tfile.close()\n\t\treturn hashlib.sha1(file."}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n    if source == 'package':\n        files = files[:-1] + [os.path.basename(f) for f in files if f in files]\n    elif source == 'zipfile' and os.path.isdir(zipfilename(zipfilename(source), files)):\n        files = files[:-1] + [zipfilename(zipfilename(source), files)] \n    \n    return files\n        \ndef _get_file_data(filename, src_file, dest_file):\n      \n    if os.path.exists(src_file):\n        #print \"Using file %s\"%src_file\n        \n        path_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n\n\ndef _to_zip(tar_filename):\n    \"\"\"Convert a tar filename to a zip separated list of files for it.\"\"\"\n    # TODO(mdan): Make the tar folder/tarfile\n    _tmp_dir, tar_file = None, None\n    def is_tarfile_file(item):\n        try:\n            tar_file = tarfile.open(item, 'r:gz')\n        except UnicodeEncodeError:\n            tar_file = tarfile.open(item, 'r:bz2')\n        return tar_file and is_tarfile_file(tar_file.extractfile())\n    tar_file_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\t# create a list of the XML files\n\tinput_dir = os.path.join(xml_filename, 'inputs')\n\tinput_dir += os.linesep + os.path.sep + file_name(source) + os.sep\n\tkeys = input_dir.split(os.sep)\n\tfiles = [file_name(f) for f in files]\n\tkeys = [int(k) for k in keys]\n\tfiles = [input_dir+'/'+file_name(dest) for dest in files if dest and file_name(os.path"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\ttmp = tempfile.NamedTemporaryFile()\n\t\toutp = StringIO()\n\t\tgroups = xmlutils.read_files(outp)\n\t\tfor gn in groups:\n\t\t\tif gn[0] =='manifest.xml': continue\n\t\t\ttmp_fd, tmp_filename = tempfile.mkstemp('.zip', '.xml')\n\t\t\toutp = outp.write(gn[1])\n\t\t\ttmp.write(outp.name)\n\t\t\ttmp_fd.close()\n\t\t\tp = Popen(['xmlclean', '-n', tmp_filename], stdin=PIPE, stderr=PIPE, stdout=PIPE"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tif files:\n\t\t\tfor f in files:\n\t\t\t\t\tif (filecmp.cmp(source, f.abspath())):\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tsource = zipfile.ZipFile(f)\n\t\t\t\t\t\t\tpkg_root = os.path.dirname(os.path.realpath(f.abspath()))\n\t\t\t\t\t\t\tsource.extractAll(pkg_root)\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\troot = xml.etree.ElementTree.parse(f)\n\t\t\t\t\t\t\troot.getroot().getparent().remove(root)\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\treturn\n\t\t\treturn _group_files"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tfiles, root = _split_files_by_xml(source, files)\t\t\t\n\t\t\n\t\tif root == \"\":\n\t\t\treturn {}\n\n\t\ttry:\n\t\t\t_path = _get_xml_file_name_by_xmls(root, xmls)\t\t\t\t\n\t\t\t\n\t\t\treturn _path[0]\n\n\t\texcept KeyError:\n\t\t\tpass\n\texcept:\n\t\tprint \"\\n Error in _group_files_by_xml_filename_source\"\n\t\ttraceback.print_exc()\n\t\traise\n\n\ndef _split_files_by_xml(source, files):\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n#\timport os\n#\timport glob\n#\timport pkg_resources\n#\tfrom lxml import etree\n#\tfrom ConfigParser import RawConfigParser\n#\n#\tparser = RawConfigParser()\n#\tparser.read([source])\n\n\n#\tfrom pkg_resources import parse_version\n#\t# print \"Parsing version: %s\" % parse_version(version)\n#\tversion = parse_version(version)\n\n\ndef _group_files_by_xml_dir(source, dirs):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n##############################################################################\n#\n# Package manager functions\n#\n##############################################################################\n\ndef _group_files(group_names, files):\n\t\"\"\"\n\tGroup files by their numeric order. \n\t\"\"\"\n\tfor fname, gname in zip(files, group_names):\n\t\t# this should not make sense with the same format\n\t\tif hasattr(fname, 'write'):\n\t\t\tfname = fname.name\n\t\tif hasattr(gname, 'write'):\n\t\t\tgname = gname.name    \n\t\ttry:\n\t\t\tf = fn(fname, 'U')\n\t\t\tf.close"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\nclass xml_files_metadata:\n\t\tdef __init__(self, root_folder):\n\t\t'''\n\t\tCreate a dict which looks like the following structure\n\t\t\t{\n\t\t\t\t'directory_name' :\t\"test_xml_files\",\n\t\t\t\t# name of the xml files\n\t\t\t\t'package_group' : \"test_files/packages_by_xml_basename\",\n\t\t\t\t# name of the xml files\n\t\t\t\t'files' : [\"test_xml_files\"],\n\t\t\t\t# list of files, with names ending in \".hpp\"\n\t\t\t\t'meta_keys' : [{"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n    dom = xml.dom.minidom.getDOMImplementation()\n\tif files:\t\t# find files by package name\n\t\tfor filename in files:\n\t\t\t# open the XML file\n\t\t\tdom.parseFile(filename)\n\t\t\t# find all the <files> with the given name\n\t\t\tfiles = _file_by_xml_filename(filename)\n\t\t\t# print 'files=', files\n\t\t\treturn {'xml_filename': filename, 'files': files}\n\n\t# otherwise return the None.\n\treturn None\n#!/usr/bin/python\n\nimport argparse\nimport base64"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t\n\tpackage = get_package_by_prefix(prefix)\n\t\n\tif not package:\n\t\t\traise ImportError('Invalid prefix: {0}'.format(prefix))\n\n\tfile_path = get_package_file_path(package, file_path)\n\n\t# Se espero que n\u00e3o enfeto os os palad\u00eancia a file n\u00e3o encontrado:\n\tif not os.path.exists(file_path):\t\n\t\traise ImportError('File {0} cannot be found'.format(file_path))\n\n\treturn True\n\ndef match_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n   \n\ndef print_tree(tree):\n    \"\"\"\n    Print tree.\n    \"\"\"\n    fmt = \"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<report version=\"1.0\">\n    <package name=\"%(name)s\">\n    <children>\n    <dir_list>\n\"\"\"\n    \n    for child in tree.getchildren():\n        fmt += \"    <file>%(filename)s\">\n        <dir>\n            <dir_list>\n\"\"\"\n        \n        #TODO: check if the directory exists within a specific project?        \n        dir_list_dir = child.get_dir_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n# Se a file o usu\u00e1rio um pacote do usu\u00e1rio em que quer arquivos os\n#  os n\u00e3o cambio o que o estu tipo existe um um arquivo dos arquivos uma\n#  a\u00fan qu\u00e3o a partir do arquivos que se a o documento.\n#  So que es este casao especificado o arquivo dos arquivos ou assem\n# qualquer \u00e9 inicial.\n\n\ndef file_is_doc_file(prefix, doc_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t\n\t# Check if the file is within the given prefix\n\tif os.path.isdir(file_path):\n\t\t\tif os.path.basename(file_path) == prefix:\n\t\t\t\tfilepath = file_path\n\t\t\t\t# Check if the file is not from within the prefix\n\t\t\t\treturn filepath\n\t\n\t# Otherwise check if the file is from the directory containing the path of the file\n\telse:\n\t\t# Otherwise check if the file inside the prefix exists\n\t\tdirpath_list = glob.glob(os.path.join(prefix, '*.doc'))\n\t\tfilepath = []\n\t\tfor"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\ndef has_match(files, prefix):\n\t\"\"\"\n\tTrue - `prefix` looks like a file/prefix\n\tFalse - `prefix` is not a file name\t\t\t\"\"\"\n\t\ndef find_file_by_prefix(prefix, file_base_path='.'):\n\t\"\"\"\n\tFind the corresponding file in the given `prefix` as a single file found in the given `file_base_path`\n\n\tParametros:\n\t\tprefix : str\n\t\tfile_base_path : str\n\t\t\tDefining absolute path to which the file is located\n\tReturns\n-------"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t\n\tif prefix.startswith('/'):\n\t\treturn True\n\tpackage_found = False\n\tparts1 = prefix.split('/')\n\t\n\tif len(parts1)<2:\n\t\treturn False\n\n\tif len(parts1)<3:\n\t\treturn False\n\tprefix = parts1[0]\n\tfor part in parts1[1:]:\n\t\tfolder_path = join('/'.join(name.split('/')[:-1]), part)\n\t\tparts_path = [p for p in parts if p!= '', folder_path]\n\t\t\n\t\tif any([part.startswith(prefix + '/', filename"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\n\n# -----------------------------------------------------------------\n\n\n# -----------------------------------------------------------------\n\n\ndef get_package_directory_contents( prefix, base_dir ):\n\n\t\"\"\"\n\tGet all file paths where package directory resides.\n\n\tParameters\n----------\n\tprefix : str\n        Filename prefix\n\tbase_dir : str\n        Path of each directory in question, to get package directory contents\n\n\tReturns\n\t-------\n\tlist of str\n        List of absolute filenames within the given directory.\n\t\"\"\"\n\n\n\t\n\n\n# -----------------------------------------------------------------\n\n\ndef split_directory( dirpath, directory_prefix=\"/tmp/\"):\n\n\t\"\"\"\n\tSplit"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n#+++++\n#+++++\ndef main():\n\t#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\tdoc_doc = Document(doc_file)\n\tdoc_doc.save_doc()\n\t#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\t#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\t\t\n\t\n\t#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\t\n\t#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\ndef match_file_by_prefix_for_file(file_path, prefix, document_or_directory):\n\t\n    \"\"\"\n    Match a given file under `prefix` (a string), i.e. for the document or\n    a directory.\n\n    Parameters\n    ----------\n    file_path : str\n        File path\n    prefix : str\n        Package prefix.\n    document_or_directory : bool\n        True - document (or directory) has its own `file_path`, otherwise `False`.\n\n    Returns\n    -------\n    bool\n        True - file belongs to `package_path`, False means it belongs to a\n        different"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn ( ( prefix.startswith( 'document' ) ) and\n\t\t\t\t\t( prefix.startswith( 'peace' ) )\t\n\t\t\t\t\tand( prefix.endswith( ':' ) )\t\t\t\t\t\n\t\t\t\n# Si esse retorna os valores do vistos (recebidos)\ndef is_val_in_package(file_path, data):\n\treturn ( ( data  and data[ 4 ] )  # Si esse valor inizio, no caso como de valor que va a visto da impresso\t\t\t\tcolocar o p"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\ndef get_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n        \n# ---------------------------------------\ndef get_all_filenames():\n\t\n\tfiles = []\n\tpaths = []\n\t\n\t\n\tfor folder in glob.glob(directory): \n\t\t\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\ndef select_prefixes(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\ndef expand_dir(prefix):\n\t\"\"\"\n\tRepode los nombitros de archivo que los archivos no existe.\n\t\"\"\"\n\t\ndef expand_"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n#print \"select files:\"\n#for file in files:\n#\tyield select_filenames_by_prefix(file, prefix)\n\n\n\ndef find_file(doc_path, filename):\n\t\"\"\"\n\tGet doc file from file(s).\n\tInicia a lista, esta lista da lista a partir a atribui con a funzione coi\n\ta uno arquivos sul sul sulla\n\t\"\"\"\n\t\n#\treturn doc_path+os.sep+filename\n\n\ndef parse_path():\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tf_prefix = prefix[len(prefix) :]\n\t\tif f_prefix == '' or f_prefix == 'dataframes':\n\t\t\treturn []\n\t\telse:\n\t\t\treturn [f_prefix + f for f in files]\n\n\ndef file_exists(filename):\n\t\"\"\"\n\tObtenemos os filtra da lista.\n\tNa primerar su equivalente com cujos nomes iniciam com `filename`\n\t\"\"\"\n\t\tf_filename = filename[-5:]\n\t\tif f_filename.count('.') == 0:\n\t\t\treturn"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t \n\t\ndef search(term):\n\t\"\"\"Recursiva os arquivos da lista `term` com uma filtrate com os arquivos\n\tas `term`\n\t\"\"\"\n\t\ndef uniq_files_to_find(files):\n\t\"\"\"Recursiva os arquivos da lista `files` con la lista\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\ndef clean(directory):\n    \"\"\"\n\tRecursiva todos puntos deseja os arquivos durante com os arquivos\n\t\"\"\"\n\t\ndef find_files(prefix):"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\ndef get_names(prefix, prefix_files):\n\t\"\"\"\n\tGets files that belongs to a document package\n\n\t:param prefix:\n\tThe prefix\n\t:param prefix_files: The files paths names\n\t:returns:\n\t``[(p1, p2), (p1, p2),...]``\n\t\"\"\"\n\t\n\t\n\t# Cast the prefixes to a string\n\tprefixes = re.sub('([a-zA-Z0-9\\-]+)', r'[\\s\\u2600-\\u25ff]', prefixes).lower()\n\t# Cast prefixes to a"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tfiles = list(files)\t\t\n\t\tfor file in files:\n\t\t\tretorno = fnmatch.fnmatchcase(os.path.basename(file), prefix)\n\t\t\tif retorno:\t\t\t \n\t\t\t\treturn retorno\t\n\t\traise ValueError(\"File name {} doesn't match prefix {}\".format(prefix, files))\n\nif __name__ == \"__main__\":\t\t\t\n\tif len(sys.argv) == 1:\t\n\t\tprint(\"The --dir option of this script will list out all files.\")\n\t\tsys.exit(0)\n\t\nos.chdir(os.path"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\ndef get_prefix_from_file_dir(dir, prefix, file_dir):\n\t\tprefix_no_file = prefix + file_dir + os.sep\n\t\treturn [prefix_no_file + os.sep + file_dir]\n\n\n\n\n\n\n\ndef get_match_files(files):\n\t\"\"\"\n\tGet files that belong to a document package.\n\nRetorna a lista `files` cujos nomes iniciam com `files`\n\n\tParameters\n----------\nfiles : str list\n    File paths\nReturns\n-------\nlist\n    files paths which basename"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n    if not os.path.exists(prefix):\treturn []\t\t\n    files = os.path.abspath(files)\n    files = filter(os.path.basename, files.split(prefix)[1:])\n    if debug:\n\t\tprint('File patterns in prefix:', prefix)\n\t\n\treturn _get_filenames_by_files(os.path.dirname(files))\n\n#----------------------------\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nimport unittest\nfrom flask import Flask, render_template\nfrom flask_babel import Babel\n\napp ="}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\n\tresult=[]\n\tfiles=[prefix+\"/\"+fn for fn in files]\n\t\n\tfiles.sort()\n\t\n\tfor fn in files:\n\t\tfullpath = fn+prefix+fn\n\t\twith io.open(fullpath,'r',encoding='utf8',errors='ignore') as f:\n\t\t\tfor line in f:\n\t\t\t\tresult.append(os.path.basename(line))\n\t\n\treturn result\n\n\ndef get_filenames(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cuj"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n_ex_folder = os.path.abspath(folder)\nfolder = _ex_folder\n\n\ndef get_ex_id(type_id):\n\t\n\t\"\"\"\n\tGet ID of the package\n\nParameters\n----------\ntype_id : int\nReturns\n-------\nexid : string\nThe ID of the package\n\"\"\"\n\t\n\t\n\tif type_id == 3:\n\t\t\n\t\t\text = 'exe'\n\t\n\tif type_id == 4:\n\t\t\tif folder in ext.split('.'):\n\t\t\t\treturn folder\n\t\t\telif folder in type_id.lower():\n\t\t\t\treturn type_"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\titems = os.listdir(folder)\n\t\titems = items[1:] # take each item's path\n\t\tfolder_data = []\n\t\tfor item in items:\n\t\t\tfolder_path = os.path.join(folder, item)\n\t\t\tp = os.path.join(folder_path, folder_path)\n\t\t\tif os.path.exists(p) and re.match(\"^.*\\.xml$\", p):\n\t\t\t\titem_path = p\n\t\t\telse:\n\t\t\t\titem_path = os.path.join(p, folder)\n\t\t\tif os.path.exists("}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tfolder_name = folder.split('/')[-1]\n\t\tfolder_path = __directory__(folder_name)\n\t\tif folder_path in ['/opt_db', '/opt_config_db']:\n\t\t\tdirectory = os.environ['HOME'] + '/share/app/python/python-app/app.py'\n\t\telse:\n\t\t\tdirectory = os.path.dirname(folder_path)\n\t\tpackage = __import__(folder_path, globals(), locals(), [\"_parse_package\"])\n\t\tif folder_path == package.base_path:\n\t\t\treturn package.get"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tresult = fileinput.input(folder + \"?\")\n\t\tfiles = {}\n\t\tfor line in result.split(\"\\n\"):\n\t\t\tdata = line.split(\":\")\n\t\t\tname = data[0]\n\t\t\t# We need to store some attributes here in this case\n\t\t\tpackages = data[1:]\n\t\t\tattributes = set()\n\t\t\tcontinue\n\t\t\t# We need to store attributes here in this case\n\t\t\tif name in packages:\n\t\t\t\t# If it's not a filename\n\t\t\t\tpackage_name = packages[name]\n\t\t\t\t# If it's not a package name (e.g. My"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\ndef get_folder_root(folder):\n\t\t\"\"\"\n\tGet the folder root path\n\t\"\"\"\n\n\t\tf = os.path.join(folder, \"packages\")\n\t\tif os.path.exists(f):\n\t\t\treturn f\n\t\treturn None\n\n\tdef _find_and_find(root, xpath):\n\t\troot_folder = get_folder_root(root)\n\t\tif xpath in root_folder:\n\t\t\treturn root_folder[xpath]\n\t\tres = []\n\t\tpath = os.path.join(root_folder, xpath + \".xml\")\n\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\treturn_data = {}\n\t\tfor root, dirs, files in os.walk(folder):\n\t\t\tfor file in files:\n\t\t\t\tfolder_file_path = os.path.join(root, file)\n\t\t\t\tif file.startswith('Data/') or not file.startswith('data/model/'):\n\t\t\t\t\treturn_data[str(folder_file_path)] = folder_file_path\n\t\t\t\telse:\n\t\t\t\t\treturn_data[FOLDER_KEY_TO_PKG_KEY[file]] = folder_file_path\n\t\treturn return_data\n\t\t\n\n\t\n\nif __name__ == \"__"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _convert(content):\n\t\"\"\"\n\tReturn package data\n\t\"\"\"\n\ttry:\n\t\tdoc = minidom.parse(content)\n\texcept SyntaxError:\n\t\t# TODO: Fix that!\n\t\treturn {}\t\n\n\t# get metadata info.\n\tdoc.getElementsByTagName('metadata')[0].childNodes[0].data = doc.getElementsByTagName('metadata')[0].childNodes[0].data[9:]\n\tfor f in doc.getElementsByTagName('package'):\n\t\tdata = _explore_folder(f)\n\t\tif len(data) > 0:\n\t\t\tdata_list ="}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\ndef explore_folder(folder,folder_name):#,mode='r'):\n\t\"\"\"\n\tExplore the folder with specified folder\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t\tFolder of the package\n\tfolder_name: str\n\t\tFolder name to search in\n\t\n\tReturn\n\t-------\n\tdict\t\"\"\"\n\tfilename = '%s/%s' % (folder,folder_name)\n\tresult = []\n\tout_filename = join(folder,folder_name+'_out.txt')\n\tif not exists(out_filename):\n\t\ttry:\n\t\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\ndef list_tree(folder, **kwargs):\n\n\t\"\"\" Get packages' data from folder\n\t\"\"\"\n\tpackages = package_list(folder, list=True, **kwargs)\n\n\treturn packages\n\n\ndef package_list(folder, **kwargs):\n\n\t\"\"\" Return package data of a particular package with the given \\\n\t\tprefix in the folder. \"\"\"\n\n\tprefix = 'package://'\n\n\tif kwargs.get('prefix'):\n\t\tprefix = kwargs['prefix']\n\n\turl = 'http://www.pulsar.ru/pulsar/package/tree/{prefix}."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tdata = _get_all_packages(folder)\n\n\t\t# Remove.xml or.xml.xml, - from path to include directory\n\t\t# which excludes xml, - from path to include directory\n\t\tdirs = [os.path.splitext(x)[0] for x in os.listdir(folder) if x.endswith(\".xml\")]\n\n\t\t# Get package list from folder\n\t\tpackage_list = [item for item in data if (item[\"name\"].startswith(\"<package>\"))]\n\n\n\t\tpackage_list = filter(lambda x: x[\"files\"].startswith(os.path."}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tif os.path.exists(\"%s/%s\" % (prefix, filename)):\n\t\t\treturn {\"file_folder\": file_folder}\n\t\telse:\n\t\t\treturn {\"assets\": []}\n\ndef _eval_string(string):\t\t\"\"\"\n\tIdentifica o caixa de arquivo do arquivo do string: `assets` ou `rendition`.\n\nIdentifica o caixa do arquivo da string o `assets` o `rendition`.\nTem que de um arquivo do i\u00e7inas do formato \"assets\".\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\timport pystache\n\t\timport StringIO\n\t\timport os.path\n\t\timport re\n\t\timport xml.etree.ElementTree as ET\n\t\timport scipy.io\n\t\timport scipy.io.loadmat as load_mat\n\t\timport matplotlib.pyplot as plt\n\t\timport matplotlib.pylab as plt_\n\tfor n in os.listdir(prefix):\n\t\tprefix+n\n\t\tif os.path.isdir(prefix): continue \t\t#dir janelos\n\t\tif re.match(prefix + n, file_path): continue \t\t#file do arquivo\n\t\tif os"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\n\tprint \"Prepara\u00e7\u00e3o de arquivo\"\n\t\n\t\n\tif not os.path.isdir(file_folder):\n\t\tos.mkdir(file_folder)\n\n\tfilename_file = os.path.join(file_folder, filename)\n\tprint filename_file\n\txml_file_file = _open_file(filename_file,'rb')\n\tdoc = xml_file_file.read()\n\tparser = _load_doc(doc)\n\n\tdoc.attrib[\"tipo\"] = \"\" \t\t\t\t\n\n\t#\n\ttipo = parser.tipo"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _find_file(filename):\n\t\"\"\"\n\tIdentificao o tipo de arquivo do arquivo em an\u00e1lise.\n\nIdentifica o tipo de arquivo do arquivo em an\u00e1lise.\n\nParameters\n----------\nfilename : str\n    filename\nfile_folder : str\n    folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _resolve_file(prefix, file_path):\n\t\"\"\"\n\tIdentifica os tipos de arquivo de aposta do arquivo.\n\nIdentifica"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\nFILE = 'taylor_equipate.xml'\nPATH = '/var/lib/taylor_equipate'\n\ndef get_file_path(prefix, filename, file_folder):\n    \"\"\"\n    Retorna os path em algoritmo\n    Para um processo os path de arquivos.\n    Ao endere\u00e7o os arquivos: `assets` o `rendition`\n    com o tipo arquivo\n    Siesta em arquivos do arquivos em equipo\n\n\tParameters\n----------\nprefix"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\twith open(prefix + \"_\" + file_path, \"rb\") as f:\t\n\t\t\tprefix = prefix + \"_\" + os.path.basename(file_folder) + \".xml\"\n\twith open(prefix + \"_\" + os.path.splitext(filename)[0] + \".xml\", \"w\") as f2:\t\n\t\t\tf2.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\") # encoding = utf-8\n\n\t\t\t# importar a dados para a emitido de arquivo\n\t\t\tdata = minidom."}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tdata = {}\n\t\twith open(filename, \"rb\") as f:\n\t\t\txmldata = f.read()\n\t\txml_obj = ET.XML(xmldata)\n\t\tfor ele in xml_obj.findall(\".//asset\"):\n\t\t\tdata[ele.get(\"asset\"): ele.get(\"rendition\")] = ele.get(\"package\")\n\t\treturn data\n\t\n\n@app.route(\"/files/<path:path>\")\ndef assets(path):\n\t\"\"\"\n\tIdentifica o arquivo do pacote: `assets` ou `rendition`.\n\n\tArgs"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tprefix = prefix.replace(\" \", \"_\").replace(\"/\", \"_\")\n\t\tfilename = filename\n\tprint(prefix)\n\t\n\t\t\n\t\timport xml.etree.ElementTree\n\n\t\treturn xml.etree.ElementTree.parse(file_path, parser=None)\n\n\t# return list[int]\n\n\ndef _print_arq_file(arq_path):\t\n\t\"\"\"T\u00e9rmina se a frequenza de arq o tipo de a quantidade para llenar.\"\"\"\n\t\"\"\"Fun\u00e7\u00e3o do arquivo do arquivo e os dados da qual de que"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tif not os.path.isfile(filename):\n\t\t\traise ValueError(\"No such file \"+filename)\n\t\treturn open(filename, 'r').read()\n\n\ndef _get_data_array(filename):\t\"\"\"\n\tIdentifica o data da seg do arquivo XML sem arquivos do arquivo,\n\tendere\u00e7o do arquivo em arquivo.\n\n\tParameters\n----------\n\tfilename : str\n    arquivo file\n\tarquivo\n\tarquivo dos arquivos do arquivos do arquivo.\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\ndef _clean_directory(folder_name_string):\n\t\"\"\"\n\tEste campo arquivo de e-texto ativos da folder e a \u00f3nda n\u00e3o pois arquivo.\n\t\"\"\"\n\t\n\t\n\tfilename = _get_file_name(folder_name_string, return_ascii=True)\n\t\n\tif filename!= '':\n\t\tresult = os.path.join(folder_name_string,filename)\n\t\t\n\t\tif os.path.isfile(result):\n\t\t\tos.remove(result)\n\t\t\t\n\t\t\tif verbose:\n\t\t\t\t"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.write(\"*@\"+lang + file_path)\n\t\tself.env.overwrite_env()\n\n\tdef set_file_contents(self, lang=None, filename=None):\t\t\n\t\tif lang:\n\t\t\tif lang in self.env.cache:\n\t\t\t\treturn\n\n\t\tpath_list = self.env.path\n\t\t\n\t\tfor path in (x for x in path_list if x.endswith(\".pdf\")):\n\t\t\tdatafile = path + path_list[0] + \".pdf\"\n\t\t\tif os.path.isfile(datafile):\t\t\t\t\n\t\t\t\tself.env"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n###############\n# File Name:\n#\n###############\t\t@param self\t=\t\tDocument Object\n#\n\nif __name__ == '__main__':\n\tcreate_doc()\n\tparse_doc()\n\tprint \"done!\"\n\"\"\"\nWSGI config for project project.\n\nThis module contains the WSGI application used by Django's development server\nand any production WSGI deployments. It should expose a module-level variable\nnamed ``application``. Django's ``runserver`` and ``runfcgi`` commands discover\nthis application via the ``WSGI_APPLICATION`` setting.\n\nUsually"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfilename = os.path.abspath(file_path)\n\t\tself.ren.add_name(os.path.basename(filename))\n\t\tself.ren.add_page()\n\t\tself.ren.add_rendition(lang)\n#!/usr/bin/env python2\n# Copyright (c) 2014-2015 The Bitcoin Core developers\n# Distributed under the MIT software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n\"\"\"Test P2PSwitch logic.\"\"\"\n\nfrom test_framework.test_"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\n\tif not os.path.exists(file_path):\n\t\tlogger.error(\"File not found\")\n\t\treturn False\n\t\t\n\tf = open(file_path, 'r')\n\tdoc = Document()\n\tdata = f.read()\n\tf.close()\n\tdata = data.replace(\"artigo02\", str(lang))\n\tdata = re.sub(\"artigo02\", \"de\", data)\n\tdoc.set_data(data)\n\tdoc.save()\n\treturn True#!/usr/bin/env python\n# coding:"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n#\t{\n    #'original'\t: \"/original/artigo02.pdf\",\n    #'en'\t\t\t: \"/original/artigo02-en.pdf\",\n}\n#\t}\n#\tpass\n\n#\t{\n    #'original'\t: \"/original/artigo02.pdf\",\n    #'en'\t\t\t: \"/original/artigo02-en.pdf\",\n}\n\n#\t{\n    #'original'\t: \"/original/artigo02.pdf\",\n    #'en'\t\t\t: \"/original/artigo02-en"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t     \t\t\n\tdef get_rendition(self, lang, file_path):\n\t\t\"\"\"\n       \t\tRetrieves a Rendition object for an original file path\n        \"\"\"\n\t\ttry:\n\t\t\tself.renditions_dict[lang]=self.renditions[lang]\n\t\texcept:\n\t\t\treturn -1\n\t\treturn self.renditions_dict[lang]\n\t\n\tdef get_item_renditions(self, item):\t\t\n\t\t\"\"\"\n        \t\tRetrieves a Rendition object in order to make a list of all the \n        \t\trenditions for the item\n\t\t\"\"\"\n\t\n\t\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.ren_dict[lang] = file_path\n\n\tf = open(file_path, 'wt')\n\tf.write(self.ren_dict)\n\n\tif len(self.ren_dict)==0:\n\t\tprint \"\\tWARNING: could not find files under this directory:\\n\"\n\t\tprint \" ./generate_renditions.py -s -d -i\"\n\t\treturn \"No renditions!\"\n\n\tfor line in self.ren_dict.T:\n\t\ti = 0\n\t\tline = line.strip()\n\t\tif line == \"\" and"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n\t\tif self.rendition and self.rendition.strip().endswith(lang):\n\t\t\treturn True\n\t\treturn False\n\n\tdef add_source(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n}\t\"\"\"\n\t\tif self.source and self.source.strip().endswith(lang):\n\t\t\treturn True\n\t\treturn False\n\n\t@classmethod\n\tdef check_for_en(cls):\n\t\t\"\"\"\n\t\tif en is not None, then check if en exists. \n\t\t:return: True. \n\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tadd_rendition(lang, file_path)\n\n\twith open(file_path, \"w\") as f:\t\t\n\t\tf.write(new_paragraph + \"<br />\" + (lang + \" - \" + file_path).encode(\"utf-8\"))\n\n\twith open(file_path, \"r\") as f:\t\t\n\t\tlines = f.readlines()\n\t\twith open(file_path, \"a\") as f:\n\t\t\tf.writelines(lines)\n\n\twith open(file_path, \"a\") as f:\n\t\tf.write(\"</"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.load_theme()\n\t\treturn self.save_theme()\n\n\tdef save_theme(self):\n\t\tself.theme = self.template_base.theme\n\t\tprint(\"Save theme\")\n\t\timg_path = os.path.join(self.theme.get_theme_path(),\n\t\t\t\t    self.theme.get_theme_path(),\n\t\t\t\t    self.theme.get_theme_path())\n\t\tif not os.path.isfile(img_path):\n\t\t\tprint(\"File not found: %s\" % img_path)\n\t\t\treturn\n\t\timg ="}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tos.makedirs(os.path.dirname(os.path.realpath(__file__)))\n\t\tfile_path = os.path.join(self.image_root, basename)\n\t\timg_path = os.path.join(os.path.dirname(file_path), file_path)\n\t\timage_root = os.path.basename(file_path)\n\t\timg_root = os.path.dirname(file_path)\n\t\t\n\t\tif not Image.open(img_path).format == format.TIFF:\n\t\t\traise IOError(\"File not already stored.\")\n\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tfilename = basename.replace(\"[\", \"\")\n\t\tif filename not in self.resources:\n\t\t\tself.resources.append(filename)\n\t\tif file_path not in self.resources:\n\t\t\tself.resources.append(os.path.join(self.assets_folder, os.path.sep+filename))\n\t\tself.resources[filename].set_image(file_path)\n\t\tself.resources[filename].save()\n\t\n\tdef add_resource(self, resource_id, image, file_path):\n\t\t\"\"\"\n\t{\n    \"artigo02-gf"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\treturn self.create_asset(basename, file_path)\n\n\t@staticmethod\n\tdef create_asset(*args, **kwargs):\n\t\tasset = None\n\t\t\n\t\tif \"filename\" in kwargs and kwargs[\"filename\"]:\n\t\t\tasset = ImageAsset(*args, **kwargs)\n\t\t\n\t\treturn asset\n\t\t\t\n\tdef get_assets(self):\n\t\treturn \"\"\n\n\tdef _create_asset(self, name, file_path):\n\t\tasset = ImageAsset(name, file_path)\n\t\t\n\t\treturn asset\n\t\n\tdef _link_from_file(self, asset):\n\t\tself"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif basename not in self.asset_list:\n\t\t\tself[\"asset_list\"].append(basename)\n\n\tdef remove_asset(self, basename):\n\t\t#if self.asset!= basename:\n\t\t#\tself[\"asset_list\"].remove(basename)\n\t\tself.asset_list.remove(basename)\n\n\tdef add_image(self, basename, file_path):\n\t\tif basename!= self.image:\n\t\t\tself[\"image_list\"].append(basename)\n\n\tdef remove_image(self, basename):\n\t\t#self.image_list.remove("}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.files.append({\n\t\t\t\"basename\": basename,\n\t\t\t\"file_path\": file_path\n\t\t})\n\n\tdef file_exists(self,basename):\n\t\treturn os.path.isfile(basename)\n\n\t@staticmethod\n\tdef read(path):\n\t\twith open(path, 'rt') as f:\n\t\t\treturn Image(f)\n\n\t@staticmethod\n\tdef find_file(path):\n\t\treturn Image.load(os.path.join(path, \"artigo02-gf03.tiff\"))\n\n\t@staticmethod\n\tdef read_"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\nclass Artigo02Ggf03Frame(Frame):\r\n\tdef __init__(self, frame_name, file_path, *args, **kwargs):\r\n\t\tframe = Frame.__init__(self, *args, **kwargs)\r\n\t\tself.image = Image.open(file_path)\r\n\t\tself.filename = file_name\r\n\t\tself.frame_name = frame_name\r\n\t\tself.image_file = Image.open(file_path)\r\n\t\tself.pngimage = Image.open(file_path)\r\n\t\tself.__texture = None\r\n\t\tself.__buffer"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tname = os.path.join(self.assets_dir, basename.replace(\"-\", \"_\"))\n\t\tasset_path = os.path.join(self.assets_dir, basename.replace(\" \", \"_\"))\n\t\tfile_path = os.path.join(self.assets_dir, basename.replace(\".\", \"_\"))\n\t\tif not os.path.exists(os.path.dirname(file_path)):\n\t\t\tos.mkdir(os.path.dirname(file_path))\n\t\tasset_file = open(asset_path, 'rb')\n\t\tdata = asset_"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\traise NameError(\"Invalid asset name '{path}'\".format(path=basename))\n\t\n\tfile_path=os.path.normpath(file_path)\n\n\t# load image into the image_path, if not already\n\tif len(os.path.splitext(file_path)[-1])>1:\n\t\traise ValueError(\"Valid image ({path}) must end with one of {sep} or {sep2}\"\n\t\t\t\t.format(path=file_path, sep=os.path.sep, sep2=os.path.sep))\n\t\n\tfile_path=os.path.join"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.tilesFile[basename] = file_path\n\n\tdef open_file(self):\n\t\t\"\"\"\n\t\tOpens the tile file\n\t\t\"\"\"\n\t\tself.tilesFile = {\n\t\t\t\"artigo02-gf03.png\": \"rb\",\n\t\t\t\"artigo02-gf03.tiff\": \"r\",\n\t\t}\n\t\tself.tilefile = self.tilesFile[\"artigo02-gf03.png\"]\n\t\ttry:\n\t\t\tf = open(self.tilefile, \"rb\")\n\t\texcept:\n\t\t\tprint \"Could not load"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\t# Create Asset\n\t\timport zipfile\n\t\tf = zipfile.ZipFile(file_path)\n\t\tr = []\n\t\tfor x in f.infolist():\n\t\t\tv = open(x).read()\n\t\t\tr.append(v)\n\t\tzip_file = \"artigo02-gf03.tiff\"\n\t\tf.close()\n\t\tos.system(\"cd {}; tar zxcvf {}; git clone tar zxcvf\".format(self.workpath,zip_file))\n\t\tfrom artigo02sdk.analysis.lib import get_manifest\n\t\tmanifest_file"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path) as zipf:\n\t\t\t\t# for each XML file in the zipfile\n\t\t\t\tfor root, dirs, files in os.walk(dir_structure_folder):\n\t\t\t\t\tfor root, dirs, files in os.walk(os.path.join(root, 'xml')):\n\t\t\t\t\t\tfor name in files:\n\t\t\t\t\t\t\ttree_name = os.path.split(name)[1]\n\t\t\t\t\t\t\ttree = {}\n\t\t\t\t\t\t\tfile_path = os.path.join(root, tree_name)\n\t\t\t\t\t\t\ttree_path = os.path.join(file_path"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tzip_data = None\n\t\ttry:\n\t\t\tZip = zipfile.ZipFile(open(zip_path, \"rb\"), \"r\")\n\t\t\tzip_data = ZIP.info(zip_path)\n\t\texcept:\n\t\t\tpass\n\t\t\n\t\tjson_data = {}\t\t\n\t\tif zip_data is not None:\t\n\t\t\tfor entry in zip_data:\n\t\t\t\tjson_data[entry[0]] = entry[1]\n\t\t\n\t\treturn json_data\"\"\"\n==================\nComparison of the raw data sets for a collection of data points.\n==================\n\nCompare raw data sets"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _explore_zipfile_single(zip_path):\n\t\"\"\"\n\tGet packages' data from zip_path\n\tReturns dict, returns list of all found archive file_name\n\t\"\"\"\n\t\ndef _explore_zip_file(file_name):\n\t\"\"\"\n\tGet package's data from zip file_name\n\t\"\"\"\n\t\ndef _get_packages_to_explore(pkg_name):\n\t\"\"\"\n\tGet packages' data from pkg_name\n\t\"\"\"\n\t\n#    print \"Get data of pkg_name...\"\ndef get_info("}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n# Get data from zip file\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:  # open zip file for writing to file\n\t\t# Remove header\n\t\twith open(zip_file.filename, 'r') as f:\n\t\t \n\t\t\tpackage_root = f.read('.xml')[0].split('.')\n\t\t\tpackage_root = package_root[0]\n\t\t\n\t\t\t# Get the list of folders to explore\n\t\t\tpackage_folders = []\n\t\t\twith open(zip_file.filename, 'r') as f:\n\t\t\t\tfor line in f"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tdict_zip = zipfile.ZipFile(zip_path)\n\t\tdict_zip.extractall(exclude_pattern=PAT_XML)\n\n\t\t# map key (or regex) to lists of lists if the key includes only one element.\n\t\t# This value is determined by the file format.\n\t\tdict_zip = defaultdict(list)\n\n\t\t# collect all groups of the files by XML root, and then get all the xml\n\t\t# information.\n\t\tgroups_filepaths = []\n\t\tfor group in dict_zip.namelist():\n\t\t\tgroup_xml_path = group + '.xml"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef generate_package_data(zip_path, xml_directory):\n\tlog.debug('%r', zip_path)\n\tdata={}\n\tfor root, dirs, files in os.walk(xml_directory):\n\t\tfor i, filename in enumerate(files):\n\t\t\tif filename.split('.')[-1] == '.xml':\n\t\t\t\troot_path=root\n\t\t\telse:\n\t\t\t\troot_path=os.path.join(root, filename)\n\t\t\tif xml_directory not in files:\n\t\t\t\t# Add each file as zip file\n\t\t\t\tzip_file=zip.open"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path) as zipfile_object:\n\t\t\treturn dict(zip_file.namelist())\n\n\t\nif __name__ == \"__main__\":\n\t# Create new xml files\n\ttry:\n\t\tcreate_output_directory()\n\t\twrite_to_xml_directory(create_output_directory())\n\t\tprint \"Done\"\n\texcept (KeyboardInterrupt, SystemExit):\n\t\traise\n\n# ________________________________________________________________________________\n#\n#\n# Copyright (c) 2015, Willow Garage, Inc.\n# All rights reserved.\n#\n"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\twith open(zip_path, \"rb\", zipfile.ZIP_DEFLATED) as f:\t\n\t\t\txml = f.read()\n\t\t\tdata = json.loads(xml.encode(\"unicode-escape\"))\n\t\t\treturn data\n\texcept:\n\t\traise\n\nclass RustPumper(object):\n\t\"\"\"RustPumper class, used for downloading to Rust and zip files.\n\t\"\"\"\n\t\n\tdef __init__(self, zip_path):\n\t\t\"\"\"\n\tInit RustPumper, with a ZipFilePath of a string which\n\tmust be a path"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\timport zipfile\n\n\troot = zipfile.ZipFile(zip_path, 'r')\n\tmetadata = root.namelist()\n\tdata = {}\n\tfor basename in os.listdir(zip_path):\n\t\tpath = os.path.join(zip_path, basename)\n\t\t\n\t\tif not os.path.isfile(path):\n\t\t\tcontinue\t\n\t\tname = os.path.join(path, basename)\n\t\tmetadata = filter(lambda x: re.search('.*' + x, path) is not None, metadata)\n\t\t\n\t\tfor elem in metadata:\n\t\t\tf = os."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t    \n\tfrom io.StringIO import StringIO\n\timport zipfile, zipfile_util\n\t\n\tdata_type = zipfile.ZIP_DEFLATED\t\t\t\n\tzip_info = zipfile_util.getinfo(zip_path)\n\t\n\tif zip_info.compress_type!= zipfile.ZIP_STORED:\n\t\traise IOError('File does not have compress type: %s' % zip_path)\n\t\t\n\twith ZipFile(zip_path) as zip:\n\t\t# Get zip data\n\t\tfor name, zip_resource, data_type in zip.infolist():\n\t\t\t# Create new package\n\t\t\tif not"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path, mode='r') as zipf:\n\t\t\tfiles = list(zipf.namelist())\n\t\treturn files\n\n\ndef get_all_files(zip_path):\n\t\"\"\"\n\tReturn all files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tzip_reader = zipfile.ZipFile(zip_path)\n\t\tfor file_name in files_list:\n\t\t\tname = file_name.split('.')[-1]\n\t\t\tfiles = zip_reader.read(file_name)\n\t\t\tfile_name = base_path + file_name\n\t\t\tprint 'File: {}'.format(name)\n\t\t\tprint 'Files: {}'.format(files)\n\t\t\tfor file in files:\n\t\t\t\tprint' {}'.format(file)\n\n\t\treturn files_list\n\n\n\tdef main():\n\t\t\"\"\"\n\t\tMain\n\nUsage:\n```\n\trun"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tzip_path, dirnames, _ = extract_zipfile_with_dirnames(zip_path)\n\n\t\treturn [filename for _, filename in zip_path if filename.lower().find('2318') is -1]\n\n\tdef convert_single_file_to_files_zip(all_files):\n\t\tall_filenames = set(all_files)\n\t\tfor file_path in all_filenames:\n\t\t\textract_file_to_file('2318', file_path)\n\n\t\treturn all_filenames\n\n\n##\n# Returns a list of images that were previously extracted"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\timport xml.etree.ElementTree as ET\n\t\timport pycurl\n\n\t\tdef find_files(p):\n\t\t\tfiles = []\n\t\t\tfiles.append(x.split('/')[-1].split('.')[0])\n\t\t\tfor item in p:\n\t\t\t\t(root, ext) = os.path.splitext(item)\n\n\t\t\t\ti = root + '.i'\n\t\t\t\tfiles.append(i)\n\t\t\treturn files\n\n\t\tzip_file = ZipFile(zip_path, 'r')\n\t\tzip_file.extractall(files_list_from_zipfile)\t\t\n\t\tzip_file.close"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\t# get files from zip\n\t\twith zipfile.ZipFile(zip_path) as fz:\n\t\t\treturn fz.files_list()\n\n\texcept IOError:\n\t\traise ValueError('Zip file not found')\n\ndef get_all_events_events():\n\t'''\n\tReturns the event data\n\t'''\n\tdata = []\n\tdates = [\n\t\t'2013-10-15', \n\t\t'2014-04-01', \n\t\t'2015-12-17', \n\t\t'2016-12-31', \n\t\t'2017-01-14', \n\t\t'2017"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfor f in sorted(tf.gfile.Glob(zip_path)) :\n\t\t\tfor i, fpath in enumerate(f) :\n\t\t\t\tyield fpath\n\ndef make_new_layer(layer_folder, name):\n\t\"\"\"\n\tCreate a new layer, that allows to open/close it, or update its\n\town layers' attributes on disk.\n\n\t:param str layer_folder: path to the new layer\n\t:param str name: name for the new layer\n\t\"\"\"\n\ttry :\n\t\tos.mkdir(os.path.expandvars(layer_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfiles_list = []\n\n\t\tfor zip_file in zip_path.file_list:\n\t\t\tzipfile_file = zipfile_file.open(zip_file)  # File to be found in zip file\n\t\t\tfile_to_zip = zip_file.filename if zip_file.file_name else zip_file.File.name\n\n\t\t\t# Create new name for new ZIP file\n\t\t\tnew_name = file_to_zip.rsplit('.', 1)[-1]\n\n\t\t\t# Add new file to list with new name\n\t\t\tfiles_list.append((new_name, zipfile_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tzipfile = zipfile.ZipFile('./data/2318/*', mode='r')\n\t\ttar = zipfile.read(\"2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf\")\n\t\tzipfile.close()\n\t\ttar_split = tar.split('.')\n\t\tfor sub in tar_split:\n\t\t\tout_file = os.path.normpath(sub) + '/' + os.path.basename(sub) + '.png'\n\t\t\tout_file_tmp = 'tmp_temp/' + out"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfrom zipfile import ZipFile, TestFile\n\t\timport shutil\n\t\tfrom functools import reduce\n\n\t\tfiles = os.listdir(zip_path)\n\t\tzip_files = [x for x in files if x.endswith('.tar.gz')]\n\t\tnames = zip(*zip_files)\n\t\tfiles = zip(*zip_files)\n\t\t\n\t\tdef delete_files():\n\t\t\tfor f in files: \n\t\t\t\tif os.path.isdir(f): \n\t\t\t\t\tshutil.rmtree(f)\n\t\t\n\t\t#Delete old temporary files\n\t\tfor f in zip(files, zip_files):\n\t\t\tdelete"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfiles = []\n\t\tfor f in zip_path.split('/')[1:-1]:\n\t\t\tif '-e20006501' in f:\n\t\t\t\tfiles.append(os.path.join(os.path.dirname(zip_path), '2218'))\n\t\t\t\tbreak\n\t\treturn files\n\n\tdef parse_file_from_string(self, string, name=''):\n\t\t\"\"\"\n\t\tParse the contents of a file named `name`\n\n\t\t`string`: File Name\n\t\t`name`: Name of the image\n\n\t\tHere are some basic definitions:\n\t\t- `name`: Name of"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = re.sub(r'(\\s*)(\\w:st\\s+|\\S)', r'\\1w:st=\"'+\\1w:st\\s+=\\S\"', content) # remove 'w:' from text\n\t\tcontent = re.sub(r'([A-Z])', r'\\1{1,2}\\2', content) # remove 'w:st:1'\n\t\tcontent = re.sub(r'[A-Z]', r'\\1}:', content) # remove 'w:st' from content\n\t\t\n\t\treturn content\n\telse:\n\t\treturn txt\n\n\ndef"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif 'w' in content:\n\t\t\ttext = content.rsplit(' w:st=', 1)[1].strip()\n\t\t\tif text.upper().startswith('w:st='):\n\t\t\t\ttext = \"w:\" + text.replace('w:', '-').replace(' ', '--')\n\t\t\telif text.upper().startswith('w:'):\n\t\t\t\ttext = \"w:\".join(text.split(':', 1)[1:]).strip()\n\t\t\treturn text\n\t\telse:\n\t\t\treturn content\n\t\"\"\"\n\trecipe with all existing namespace prefixes\n\t\"\"\"\n\tif 'w:' in"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = content.lower()\t\n\t\n\t#if not content.startswith('{') : return content\n\t#else: return content\n\treturn fix_namespace_prefix(unicode(content))\n\t\nif __name__ in ['Glossary', 'Wikipedia', 'GoBot']:\n\t#import os\n\t#source = os.path.join(os.path.dirname(__file__), 'data', 'wiki','common_config.yaml')\n\t#print source\n\n\twith open(source, 'r') as f:\n\t\n\t\tconf = yaml.safe_load(f)\n\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = content.replace(\"w:st\", \"w:st=x=%e\")\n\t\tcontent += \"-- %s\" % (\"w:st=\",)\n\n\treturn content\n\n# Return original output as a Unicode string -- \"\ndef get_output(cmd):\n\tif sys.hexversion >= 0x2040000:\n\t\toutput = text_type(cmd)\n\telse:\n\t\toutput = bytes(\"\", \"ascii\")\n\treturn output.decode();\n\ndef read_file(path):\n\t\"\"\"\n\tFind text in file, return it as a unicode string."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif not isinstance(content, str):\n\t\t\tcontent = str(content, 'utf-8', 'ignore').encode('utf-8')\n\t\treturn re.sub('\\s+','', content).strip()\t\t\t\n\t\n##\n## Import file from\n## import utils\n## import sys\n## import os\n## import subprocess\n\n# Path to data file (.dat) containing a list of the names of the files\n\n\n# Parameters\n# ----------\npath_toInputData = \"data/inputFiles/\"\n# Path to output.dat file\n\n\n# Parameters\n# ----------\npath_"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\timport re\n\t\timport docutils.core.publish_parts\n\t\tif re.match(r'(?s)#[A-Z0-9]+\\.[A-Z0-9]+', content) is not None:\n\t\t\tsuffix, suffix = content.rsplit(\"#\", 1)\n\t\t\t# We have 'w:', so we need 'w:<st>' prefix\n\t\t\twpat = re.compile(r'([\\'\"])(?P<stash>\\b)|(?P<suffix>\\b)', flags=re.MULTILINE)\n\t\t\tre_stash = r'''((?!|https?://)s/(.)+%"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif \"w:st=\" in content:\t\n\t\t\tcontent = re.sub(r'(?<=[\\s\\.\\.\\.\\.\\s]*)',r'\\1,\\2',content)\t#remove whitespace\n\t\tif not content.endswith(\"):\t#convert leading or trailing spaces\n\t\t\tcontent = content + \"\\n\"+content\n\t\treturn string.Template(r'\"'+content+\"'')\n\texcept:\n\t\treturn content\n\n\t\t\n\t\t\n'''\n\tFind the version information from the'version' line of the file. Returns 'v'\n\tand a'revision'. Raises exception on failure."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tmatch = re.search(r'([w-]+)(\\w+)[^<]*(?:\\?st=[^<]*(?:\\?ns=[^<]*(?:\\?n=[^<]*)?)*)',content)\n\t\tif match:\n\t\t\tw,s = match.group(1),match.group(2)\n\t\t\treturn w+'w:st*'+s\n\t\tcontent = content.replace('w:','')\n\t\treturn content\n\n#===============================================================================\n#\n#  This section starts all parsing and processing.  This file contains common\n#  commands, all of which should be implemented are"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tfor key in ('text', 'prefix-0'):\t\t\t\t\t# keys for easier iteration\n\t\t\tif value is None or value == '':\t\t\t\t# default string or value not provided\n\t\t\t\tcontinue\n\t\t\tvalue = _fix_namespace.sub(r'\\1',value)\n\t\treturn value\n\n\n# TODO: the following should use the\n#       xml file for metadata\ndef _fix_namespace(text):\n\ttree=BeautifulSoup.BeautifulSoup(text, 'html.parser')\n\n\troot=tree.getroot()\n\n\t# remove namespace prefix\n\tnamespaces = root.xpath('//["}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tnewcontent = cgi.escape(content)\n\t\tnewcontent = fix_prefix(newcontent)\n\t\tnewcontent = fix_namespace_name_pattern(newcontent)\n\t\treturn newcontent\n\treturn content\n\n# Convert from xml to xml string, according to XML tag name.\n\ndef xml_to_utf8(content):\n\treturn xml_to_utf8_xml(strip_schema(content).strip())\n\ndef xml_to_utf8_xml(content):\n\tif isinstance(content, bytes):\n\t\tcontent = content.decode('utf8"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn node[1][0][1].find(\"(?:{pubdate_xpaths}|[{pubdate_xpaths}](?:.{pubdate:04}\\.?$)|{pubdate:02}\\.?$|[{pubdate:02}))|(http|https|ftp)://x[X-Za-z0-9\\-]{4,45}/$\", {\"pubdate_xpaths\": pubdate_xpaths})\n\n\ndef match_pubdate_xpaths(node, pubdate_xpaths):\n\t\"\"\"\n\tReturns the given match in the pubdate_xpaths list\t\"\"\"\n\tstart"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\txpaths = node.xpath(\n\t\txpath=xpath(\n\t\t\t\"[@id='%s']\" % (node.xpath(xpath=xpaths[0]))\n\t\t),\n\t\txpathstring=xpath(\n\t\t\t\"[@id='%s']\" % (node.xpath(xpath=xpaths[0]))\n\t\t),\n\t\tunique=True,\n\t)\n\n\treturn xpaths[0]\n\n\t\nclass M3Doc(doc, _gen_doc):\n\t\"\"\" M3/XHTML document from a list of M3D elements \"\"\"\n\tdef __init__(self, base_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tmatch = re.match('((.*?)([0-9]{1,2}\\.[0-9]{1,2}))?$', node.text)\n\t\tif match is None:\n\t\t\treturn None\n\t\treturn match.group(1)\n\ndef parse_pubdate(match, xpaths):\n\t\"\"\"\n\tParses the pubdate string and returns a datetime.\n\t\"\"\"\n\tmatch_ = match.search('.*pubdate_date.txt')\n\tif match_:\n\t\treturn datetime(int(match_.group(1)), int(match_.group(2)), int("}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\ttry:\n#\t\t\tpubdate = open(os.path.join(xpaths.rootpath(), 'published.csv'), 'r')\n\t\t\timport csv\n\t\t\tfor row in csv.reader(pubdate_xpaths):\n\t\t\t\tm = re.match('^(.+?)(\\w)\\.csv$',row[0])\n\t\t\t\tif m is not None:\n\t\t\t\t\treturn m.group(1).lower()\n\t\texcept:\n\t\t\tpath = os.path.join(xpaths.rootpath(), 'published.csv')\n\t\t\timport os, glob\n\t\t\tfor filename in glob.glob("}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\ttry:\n\t\t\treturn node.getElementsByTagName(\"pubDate\")[0].firstChild.data\n\t\texcept:\n\t\t\treturn \"\"\n\ndef create_match_node(match_type, title, text, pubdate_xpaths):\n\t\"\"\"\n\tReturns the first match in the text and returns it as String\t\"\"\"\n\tmatch = match_type()\n\tmatch.set(\"tag1\", title)\n\tmatch.set(\"pubDateNode\", pubdate_xpaths[0])\n\treturn match\n\t\ndef create_match_node_tag(tag, pubdate_xpaths,"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn matches(node, pubdate_xpaths)\n\ndef get_node_text(node):\n\t\"\"\"\n\tReturns the node's text content\n\t\"\"\"\n\treturn node.as_text()\n\ndef match_link(node, link):\n\t\"\"\"\n\tReturns the first match in the link value\n\t\"\"\"\n\tif hasattr(link, \"items\"):\n\t\treturn matches(node, link.items, 2)\n\telse:\n\t\treturn matches(node, link, 0)\n\ndef compare_links_to_tags(tag_data):\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn node.children[4].children[5]\n\n\tdef match_a(node, apath):\n\t\t\"\"\"\n\t\tReturns the first match in the apath list\t\"\"\"\n\t\ttry:\n\t\t\tapath.rindex('/%s' % apath)\n\t\texcept ValueError:\n\t\t\tpath = 'index.html#%s' % apath\n\t\t\treturn path.rsplit(',', 2)[1].rsplit(':', 3)[0]\n\t\telse:\n\t\t\treturn None\n\n\tdef match_b(node, bpath):\n\t\tif apath:\n\t\t\t# return first match\n\t\t\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfrom dateutil.parser import parse\n\n\t\tdef parsedate(m):\n\t\t\treturn getdate(m) + relativedelta(years=1, days=-1)\n\n\t\tdef check_pubdate(date):\n\t\t\t\"Return the date parsed based on the date\"\n\t\t\tdate_match = parsedate(date)\n\t\t\tif not date_match.tzinfo.isdst(date_match.tzinfo):\n\t\t\t\treturn False\n\t\t\treturn date_match.astimezone(date_match.tzinfo)\n\n\t\tfrom dateutil.relativedelta import relativedelta\n\t\tyear = date_nodes[-1].start_date.year\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor xpaths in [\"Date\"]:\n\t\t\tif node.getAttribute(xpath=xhtml_utils.XPATH, xpath_name=xpaths):\n\t\t\t\treturn xpaths\n\t\treturn None\n\t\n\tdef match_issn_daterange(self, node, date_xpaths):\t\"\"\"\n\tReturns True only if the given date range matches date ranges\n\tthat can't be used as a value for 'issn'.  If this is found,\n\tthe original date will be used as an \"uncorrected\" date or \"untested\" dates.\n\t\"\"\"\n\t\tfor xpaths in [\""}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn pubdate_node.getElementsByTagName('pubdate')[0].firstChild.data\n\t\ndef create_match(matches, node, xpaths):\n\n\t\"\"\"\n\tAdd/create a match that satisfies the following criteria:\n\t\t* only one match is acceptable (no pubdate)\n\t\t* only one match matches are acceptable\n\t\t* all xpaths in matches matched are included in this list\n\t\t* only a match is rejected and rejected as required tags are rejected\n\t\t* no match is already accepted\n\t\t* xpaths must be a list of xpaths\n\n\tA match is considered by both tags"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\treturn (issue.get('number'), issue.get('suppl'))\n\n\n\tdef _number_is_applicable(self, number_code):\n\t\t\"\"\"\n\t\tReturn True means if the proposed number code is applicable to article.\n\t\tNote that this is not tested in the other articles.\n\n\t\tNote that this is not a proper algorithm, it will return True if one of the proposed numbers is not set.\n\t\t\"\"\"\n\n\t\treturn self._extract_number_and_supplment_from_issue_element(self._parse_content(self._get_number(number_code, 'number"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_text = issue.find(NSIXMLTAG('issue', '{%s}info' % NSIXMLTAG))\n\t\t# Extract the numbers from issue_text\n\t\tresult = []\n\t\tfor note in issue_text.findall('{%s}note' % NSIXMLTAG):\n\t\t\tvalue = note.text.strip()[13:-2]\n\t\tvalue = number_from_supplment(value)\n\t\tnumber = int(value)\n\t\tsuppl_number = int(value)\n\t\tif number < 0:\n\t\t\tvalue = '0'\n\t\telif number > 99"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\toutput = \"\"\n\t\tnumber = issue[\"_id\"]\n\t\tsupplment = issue[\"supplments\"][\"value\"] or False\n\t\tfor item in issue[\"supplements\"]:\n\t\t\tif item[\"description\"]:\n\t\t\t\tif isinstance(item[\"description\"], dict):\n\t\t\t\t\toutput += item[\"description\"][\"value\"]\n\t\t\t\telse:\n\t\t\t\t\toutput += \"\".join(item[\"description\"].values())\n\t\treturn output\n\n\tdef _extract_number_and_supplment_from_repository(self, repository):\n\t\t\"\"\"\n\t\tExtract a list of numbers and strings that are either numbers or strings that are not numbers"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber_ = str(issue.findtext('Number')) + \" \" + str(issue.findtext('Suit'))\n\t\ttry:\n\t\t\tscissor = scissor_cache[issue.findtext('Scissors/Suit')]\n\t\texcept:\n\t\t\tscissor = ''\n\t\t\tfor scissors in scs:\n\t\t\t\t if scissors['Number'] == number_:\n\t\t\t\t\t\tscissor = scissors['Suit']\n\t\treturn number, sscissor\n\n\t@property\n\tdef number_of_number_and_supplment(self):\n\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tname = issue.findtext('name').strip()[0].upper()\tand issue.findtext('number').strip()[0]\n\t\tnumber = get_raw_attr(issue, 'num/text').strip()\n\t\tsuppl = get_raw_attr(issue,'suppl text').strip()\n\t\ttry:\n\t\t\treturn int(number)\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\tif number == '':\n\t\t\t\t\treturn None\n\t\t\t\telif number[-1] == '.':\n\t\t\t\t\tnumber = number[:-1]\n\t\t\t\telif number == \"\":\n\t\t\t\t\treturn None\n\t\texcept ValueError:\n\t\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tsuppl = frappe.db.sql(\"\"\"select value from `tabSales Invoice Item`\n\t\t\twhere `reference_doctype` = %(reference_doctype)s and name = %(name)s\n\t\t\tand (docstatus==1 or is_return is null or is_cancelled!= 0)\n\t\t\torder by modified desc\n\t\t\"\"\", {\n\t\t\t\"reference_doctype\": issue.reference_doctype,\n\t\t\t\"name\": issue.name,\n\t\t\t\"docstatus\": 1,\n\t\t\t\"is_return\": 1,\n\t\t\t\"is_cancelled\": 1\n\t\t})\n\n\t\tif not suppl:\n\t\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\t# if it is a number, return the value in a tuple\n\t\tif isinstance(issue, int):\n\t\t\tfor part in issue.split(','):\n\t\t\t\tnumber, suppl = _extract_number_and_supplment_from_part(part)\n\t\t\t\treturn (number, suppl)\n\t\t\t# no numbers, convert it as a string, then raise TypeError\n\t\t\t# if it is a string and we got unicode, do anything else right\n\t\t\tissues = issue.split(',')\n\t\t\tfor i in range(len(issues)):\n\t\t\t\tnumber, suppl = _extract_number_and"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tfrom lib import unwrap\n\t\ttry:\n\t\t\tnumber=unwrap(issue)\n\t\texcept:\n\t\t\treturn [None,None]\n\t\ttry:\n\t\t\tsuppl=unwrap(issue, no_white=True)\n\t\texcept:\n\t\t\treturn [None,None]\n\t\treturn ([number,suppl],0)##################################################\r\n# The MIT License (MIT)\r\n#\r\n# Copyright (c) 2016, Shaun Zhugh (mazhaunzh@gmail.com)\r\n#\r\n# Permission is hereby granted, free of charge,"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = issue.xpath('//h[text()=\"num\"]/text()')[0].strip()\n\t\tsuppl = issue.xpath('//h[text()=\"suppl\"]/text()')[0].strip()\n\t\treturn (number, suppl)\n\ndef _extract_number_from_volume_element(volume):\t\t\"\"\"\n\tExtract the possible values of number from the volume e.g. 1.9.2\n\t\"\"\"\n\t\tnumber = volume.xpath('//h[text()=\"num\"]/text()')[0].strip()\n\t\treturn (number, )\n\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\t_numbers_dict = {}\n\t\tfor number in issue_element.select(\"value\"):\n\t\t\tif _numbers_dict.has_key(number.get_attribute(\"value\")):\n\t\t\t\t_numbers_dict[number.get_attribute(\"value\")] += 1\n\t\t\n\t\t_number_and_supplment_dict = {}\n\t\tfor number in issue_element.select(\"number\"):\n\t\t\tif _number_and_supplment_dict.has_key(number.get_attribute(\"value\")):\n\t\t\t\t_number_and_supplment_dict[number.get_attribute(\"value\")]"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif indent:\n\t\t\tINDENT=indent*indent\n\t\telse:\n\t\t\tINDENT=u\"\"\n\t\treturn self._format(indent, False, self._format_values(self.value))\n\n\tdef as_string(self):\n\t\t\"Return a string representation of self.\"\t# XXX I'm unecessary?\n\n\t\treturn self._format(self._format_values(self.value), False, self._format_sep)\n\n\tdef value(self):\n\t\t\"return the formatted string\"\n\t\tif self.formatter is None:\n\t\t\traise TypeError(\"must create a formatter"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug: print \"self.indent=\",indent,\n\t\tfor k,v in self.items():\n\t\t\tif k[0] == \"_\":\n\t\t\t\tindent+=1\n\t\t\telse:\n\t\t\t\tindent+=1\n\t\t\tif type(v)!= types.ClassType:\n\t\t\t\tprint \"[D]\",type(v),k,\"!=\",repr(v)\n\t\t\tself[indent] = v\t\n\n\tdef __repr__(self):\n\t\tif self._type_name==\"dict\":\n\t\t\tret='{}={}'.format(self.__class__.__name__,self._data.items())\n\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tassert(isinstance(indent, unicode))\n\t\treturn textwrap.dedent(\n\t\t\t\"  [\", self.header, self.indent, \"]\",\n\t\t\tindent=indent,\n\t\t).__str__()\n\n\tdef tostr(self):\n\t\t\"\"\"\n\t\tReturn a string representation of self.\t\"\"\"\n\t\treturn (\n\t\t\t\"<{indent} {type}(header: {hdr})\\n{data}>\".format(indent=indent or \"-\", type=type(self).__name__,\n\t\t\t\t\t\t\t\thdr=text_or_bytes(self.header),\n\t\t\t\t\t\t\t\tdata=\"\".join(map(str"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tfrom.. import qtPrint as pp\t# qtPrint does not have a name\n\t\tfrom. import QtGui, QtCore\n\t\tfrom. import gui, math, QtWidgets\n\t\tif not isinstance(self, type(self)): return\n\n\t\t# Create our own dictionary so we can access it\n\t\td = {}\n\t\tif debug: print(' * _pretty(')\n\t\t\n\t\t# Now our basic formatting function\n\t\tdef bold(s, base=False):\t\t# a nicer way of wrapping\n\t\t\treturn s * 0.9\t\t\t\t## not quite much\n\t\t\tif base:\t\t\t\t# yes\n\t\t\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tself._output(\"\\n\".join(self._printer.pformat())+\"\\n\")\n\t\tself._output(\"%s\" % str(self))\n\t\tself._output(\"%s%s%s\" % (indent*indent,self.__class__.__name__,\n\t\t\t', '.join([str(attr) for attr in _UNPRINTABLE_ATTRIBUTES])))\n\t\tif debug and self._printer and isinstance(self._printer.to_utf8(), unicode):\n\t\t\tself._output(self._printer.to_utf8())\n\n\tdef _add_indent(self, text):\n\t\t\"\"\"Add"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn 'node:%s\\n' % str(self)\n\n\tdef pretty_tree(self, indent=0):\n\t\t\"\"\"\n\t\tReturn a pretty formatted tree.\t\"\"\"\n\t\treturn Tree(indent + self.tree.indent, self.tree.indent + self.tree.indent, self.tree.indent + self.tree.indent)\n\n\tdef to_xml(self, indent=0, debug=False):\n\t\tif self.tree._toxml__children:\n\t\t\treturn '<node_tree>%s</%node_tree>\\n' % str(self.tree"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif isinstance(self, dict):\n\t\t\toutLines = []\n\t\t\tfor k,v in self.items():\n\t\t\t\tif k == 'name':\n\t\t\t\t\toutLines.append(\"\\n\")\n\t\t\t\telse:\n\t\t\t\t\toutLines.append(str(v) + \" \")\n\t\t\treturn \"\\n\".join(outLines)\n\t\telif hasattr(self, 'key') and isinstance(self.key, str):\n\t\t\treturn indent + 2*''+ self.key + 2*' '\n\t\tassert isinstance(self, (str, unicode)), repr(type(self))\n\t\treturn indent"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tpprint(self,indent=indent,debug=debug)\n\tdef __repr__(self):\n\t\treturn 'Sense(%s)'%(repr(self.name))\n\tdef encode(self,sass):\n\t\tSense._encoded_fields.add(repr(self))\n\tdef encode_obj(self,obj):\n\t\tSense._encoded_fields.add(repr(obj))\n\tdef __str__(self):\n\t\treturn '%s(%r)'%(self.name,self.id)\n\nclass Field:\n\t\t#TODO: support \"index\" for"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif indent and self.encoding == \"utf-8\":\n\t\t\tself.encoding = \"utf-8\"\n\t\treturn \"%s<%s%s%s%s>%s</%s%s%s>%s</%s%s%s>\" % (self.encType, self.encValue, self.encChar, self.decType, debug, indent, self.escChar, indent, self.escCharSpace, self.escSpaceSpace, self.encVal, self.encTypeSpace)\n\n\n\nclass XMLGenerator(Generator):\n\t\"\"\"Generator which generates a XML document.\"\"\""}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tformat = \" \" * indent + \"  \"\n\t\t\n\t\tfor name, value in sorted(self.fields):\n\t\t\tif value is not None:\n\t\t\t\tformat += name + \" = \" + pretty(value, indent+indent+2, debug)\n\t\t\t\n\t\treturn format\n\n\tdef to_json(self):\t\"\"\"\n\tReturn an object with pretty representation as a json string.\n\t\"\"\"\n\t\tself._pretty(\"json\")\n\t\treturn self._pretty\n\n\t@staticmethod\n\tdef from_json(code, fields=None):\n\t\treturn Json(code=code, fields="}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\tself.stack.append(args)\n\t\tself.absorb(*args)\n\t\n\tself.stack.append(args) # ignore unary operators first\n\t\n\tresults = []\n\twhile len(results) == len(args):\n\t\tresults.append(self.absorb(args))\n\t\n\treturn results\n\t\n\nclass Concatenator(object):\n\tdef __init__(self, a, b):\n\t\tself.a = a\n\t\tself.b = b\n\tdef __iter__(self):\n\t\tself.iterator = iter(self.a)\n\t\tself."}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t    \n\t\tif len(args) == 2:\n\t\t\tabsorb(args[0][0])\n\t\telif len(args) == 2:\n\t\t\tabsorb(args[1])\n\t\telse:\n\t\t\traise Exception(\n\t\t\t\t\"Not supported unary expressions\")\n\t\n\"\"\"\nThe above test is for the use of the binary or unary\noperators, and not of the specific operators. This includes\nthe following operators:\n\n* |\n\n* AND\n* T ||\n* A  => T, where A is AND, and T are NOT.\n* |\n\nThe"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t  \tif len( args ) == 1:\n\t      self.add( lambda a, b: absorb(a, b), False )\n     \tif len(args) == 2:\n\t      self.neg(lambda x, y: absorb(x, y), False )  \t\n\t\n\telif len(args) == 3:\t\t\t\t\t\t\t# two absorptions\n\t      #assert type(args[1]) == type(args[2])\n\t      self.add( lambda a, b: absorb(arg(a, y), b), True )\n\t      #"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\tassert len(args) >= 1\n\t\tb, e = args[0]\n\t\tif not isinstance(b, Number) or isinstance(e, Number): return [x for x in b], [e for x in b]\n\t\telse:\t\t\t\t\t print \"absorption: absorption of length 1\"\n\t\treturn [a for a in a if a!= b], [b for a in a if not a == b]\n\ndef absorblist(args):\t\"\"\"\n\tGiven an `args` sequence of expressions, output a new list which describes\n\tabsorption"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n\n\t\n\n\t# We assume that all the non negative absorptions take precedence.\n\tif args == [0]:\n\t\treturn A\n\n\tn, A = args[-1]\t\t\t\t# We assume that all negative absorptions take precedence.\n\t\n\t# We also need an augmented function which deals with the order of the absorption. \n\t\n\tdef augmenter(x): # This is a list of all elements of the sequence to be absorbed.\n\t\tabsorbed_sequence = []\n\t\t# We only support the first element, we can just use it to"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\texpr = [ self(expr[i] - self.assoc[i]) for i in xrange(len(expr)) ]\n\t\treturn expr\t\t#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2010 British Broadcasting Corporation and Kamaelia Contributors(1)\n#\n# (1) Kamaelia Contributors are listed in the AUTHORS file and at\n#     http://www.kamaelia.org/AUTHORS - please extend this file,\n#     not this notice.\n#\n# Licensed under the Apache License"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\tresult = []\t\t# A & B\n\t\tself.absorb(args)\n\t\tfor (aa, b) in zip(args[:-1], args[2:]):\t\t\t# A & B & B\n\t\t\tif not [a for a in aa if a in result and a!= b]:\n\t\t\t\ttemp = '0' + a.name.lower() + '_' + b.name.lower()\n\t\t\t\tresult.append(temp)\n\t\t\telse:\n\t\t\t\ttemp = '1' + a.name.lower() + '_'\n\t\t\t\tresult.append(temp)\n\t\treturn"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n    @staticmethod\n    def reverse(args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying reverse absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Reverting_alpha\n\nReverting_alpha::\n\n    R -> A | (A | B) = A, A | (A & B) = A\n\n.. note::\n\tThe expression `A & B` is defined as (A | B). If a condition * is defined in\n\targs, then there is no"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\t# Build the list of absorption factors:\n\t\tfactors = []\n\t\tfor t in args:\n\t\t\tfactors += [(Mul(*t))]\n\t\tbase_factors = []\n\t\tfor t in factors:\n\t\t\tif bernoulli(t).is_integer:\n\t\t\t\tbase_factors.append(Pow(S.NegativeOneInteger, t))\n\t\t\telse:\n\t\t\t\tbase_factors.append(Pow(t, S.Half))\n\t\tif not base_factors:\n\t\t\tbase_factors = [S.Infinity]*len(factors)\n\t\treturn absorb_terms"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\tif len(args) < 1:\n\t\t\treturn []\n\t\treturn A_op( (args[0],args[1],) )\n\ndef comb(a,b):\n\ttry:\n\t\td = (a + b)\n\texcept TypeError:\n\t\traise TypeError\n\tif d == 0:\n\t\treturn a\n\telse:\n\t\traise ValueError, \"combmation not performed on a single value\"\n\ndef invert(a):\n\tif a < 0:\n\t\ta = -a\n\treturn a\n\ndef absorb_invert(a):\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tplugin_registry_hooks.register(hook, hook_callback)\n\n\treturn hook_callback\n\n\ndef plugin_registry_handler(name):\n\t\"\"\"\n\tHandler method for plugin registry.\n\t\"\"\"\n\tdef wrapper(instance, hook):\n\t\tplugin_registry_hooks.register_plugin(instance, hook)\n\t\treturn instance\n\n\treturn wrapper\n# Copyright (C) 2010-2011 Richard Lincoln\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    def deregister(hook, *args, **kwargs):\n        \"\"\"\n        Declare a new hook for the specified hook.\n\t\n        Args:\n                hook (HookType): Hook attribute for which to register the handler.\n                *args (args): Optional Positional arguments to pass to the handler.\n                **kwargs: Optional Keyword arguments to pass to the handler.\n                If any of these are present.\n                \"\"\"\n        result = hook.hook(*args, **kwargs)\n        if result is None:\n            logger.debug(\"No handler registered for {}\".format(hook))\n            self.__registeredItems.pop(str(hook))"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t    \n    def f(self, hook):\n        \"\"\"\n\t\tRegister handler with the specified hook.\t\"\"\"    \n        name = str(hook[HookType.name_or_obj_name])\n        self._register_hook(hook, name, hook.get_function_or_object(), hook, hook.get_method_name())\n\treturn callback(self, f)\n\nclass Event:\n\n    \"\"\"\n    Event base class.\n\nEvent objects can contain any Event type.\n\nEvent objects also have an EventType attribute.\n\n'''\n            \n    def __nonzero__(self):\n        return __nonzero__()\n   "}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    def add(self, hook): \n\t\t\"\"\"\n\t\tDecorator for adding a hook callable.\n\t\t\"\"\"\n\t\t\n\t\traise NotImplementedError()\n\tdef clear(self, hook): \n\t\t\"\"\"\n\t\tDecorator for removing a hook callable.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\t\n\tdef has(self, hook, handler=None):\n\t\t\"\"\"\n\t\tReturns True if the specified hook is registered.\n\t\t\"\"\"\n\t\t\n\t\traise NotImplementedError()\n\t\n\tdef register(self, hook):\n\t\t\"\"\"\n\t\tDecorator for registering a hook to the registry or None.\"\"\"\n\t\n\t\tif self.has(hook, handler"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n\t\n\tclass HandlerMetaView(BaseView):\n\t\t\n\t\tasync def get(self):\n\t\t\treturn self.request.application.handlers.get(self.format_name)\n\t\t\n\t\t@asyncio.coroutine\n\t\tdef post(self, method, *args, **kwargs):\n\t\t\n\t\t\traise Exception('%s handler does not make a call' % (self.__class__.__name__))\n\n\n\t\tasync def put(self, method, *args, **kwargs):\n\t\t\n\t\t\tlogger.debug(\"Calling post with'request' as 'POST' method\")\n\n\t\t\tif isinstance(method, string_type):\n\t\t\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n\t\n\tdef decorate(name, fn):\n\t\tadd_to_registry(self, name, fn)\n\t\treturn fn\n\t\n\tdef new_decoration(self, name, func):\n\t\tself.add_from_registry(kwargs=dict(name=name, func=func))\n\t\n\treturn decorate\n\n\ndef unbind(namespace, name):\n\t\"\"\"Callback function to unregister a registered namespace.\n\t\n\tThis function may be used for registering listeners using undef\n\tregistered methods and undefunnly calling on_namespace().\n\t\n\tArgs:\n\t\tnamespace (str):"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\ndef register_listener(name, func):\n    '''Decorator to be used as an automatic module listener.\n\t'''\n\tdef decorator(*args, **kwargs):\n\t\t#print('Registering listener %s' % name)\n\t\thandler = getattr(logging.root, name, None)\n\t\tif handler:\n\t\t\t#print('Handler registered for %s' % handler)\n\t\t\thandler.add_listener(name, func, *args, **kwargs)\n\treturn decorator\n\treturn decorator\n\ndef register_loggers(path):\n\tlogger = logging.getLogger(path)\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n    def decorator(service):\n        \"\"\"\n\tDecorator function that allows you to register a listener using the specified\n\tservice object.\n\t\"\"\"\n\t\n    def get_call(service):\n        \"\"\"\n\tDecorator for registering and retrieving a listener from the specified\n\tservice object.\n\t\"\"\"\n\t\n        def handler(service):\n            \"\"\"\n\tDecorator for registering and retrieving an item from the specified\n\tservice.\n\t\"\"\"\n\t    \n        def get_listeners():\n            \"\"\"\n\tDecorator to return callables which call a registered listener\n\t.\n\t\"\"\"        \n        def decorate(service):\n            \"\"\"\n\tDecorator"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t    \n    def call(self, *args, **kwargs):\n\t    \n        self.logger.debug(\"Adding handler %d\", hook.event_type)\n\t    \n        if hook not in self._registered:\n\t\t\tif self.app.config.get(key.lower())!= hook:\n\t\t\t\tif isinstance(hook, HookType):\n\t\t\t\t\tself._registered.append(hook)\n\t\t\t\telse:\n\t\t\t\t\traise ValueError(\"invalid hook\")\n\t\t\n\t    else:\n\t\t\t\n            \tpass\n\t    \n        else:\n\t\t\ttry:\n\t\t\t\n\t\t\tvalue = self.app.config.get(key.lower())\n\t\t\t\t\n\t\t\tif"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\n\n\n\t\t\t\n\"\"\"\nUnit tests for the ZHA device discovery integration.\n\nFor more details about this component, please refer to the documentation at\nhttps://home-assistant.io/components/device_discovery/\n\"\"\"\nimport asyncio\nfrom collections import OrderedDict\nimport logging\nimport os\nimport unittest\nfrom unittest.mock import patch\n\nimport homeassistant.bootstrap as bootstrap\nimport homeassistant.components.device_tracker as tracker\nfrom homeassistant.const import (\n    CONF_HOST, CONF_PORT, CONF_USERNAME, CONF_UNIT_OF_MEASUREMENT,\n    CONF_UNIT_SYSTEM,"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\nDEFAULT_PASSWORD = 'default_user'\n\n'''\nA config manager with the default password\n'''\n\nDEFAULT_PASSWORD_PASSWORD = 'default_password_password' \n\n_config = ConfigManager(user, etcd_host, etcd_port)\n\nCONFIG_KEY = ConfigKey('config.txt', DEFAULT_PASSWORD) \n\nclass _Config_Manager:\n\t\"\"\"Helper class used for getting/setting/deleting/etc config\"\"\"\n\tdef __init__(self, user, etcd_host, etcd_port):\n\t\tself.user = user\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\t\n\ttld_host = etcd_host[0]\n\ttld_port = etcd_port[0]\n\t\n\ttld_database = etcd_host[2]\n\ttld_database = etcd_host[3]\n\ttld_database = etcd_host[4]\n\t\n\tconf = {\n\t\t\"etcd_host\": etcd_host\n\t\t,\"etcd_port\": etcd_port,\n\t\t\"tld_host\": tld_host,\n\t\t\"tld_database\": tld_database,\n\t\t\"t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n# Create base_config(user=user, etcd_host=etcd_host, etcd_port=etcd_port).\n\ndefault_config = \"\"\"{\n\t\"user\": {\n\t\t\"etcd_host\": \"{0}\",\n\t\t\"etcd_port\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"auth_domain\": \"example.com\"\n\t}\n}\"\"\"\n\nuser_config = base_config(user=user, etcd_host=etcd_host)\n\netcd_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\t# Create an environment for accessing and setting up a new instance of the object\n\t\tenv = dict()\n\t\tfor (key, value) in globals().items():\n\t\t\tenv[key] = value\n\t\tinstance = Environment(user=user, environment=env, user_env=user_env)\n\t\t# This must be the new version of the environment, but may not be configured.\n\t\tinstance.setup_environment()\n\t\t# We need to set the etcd_host here rather than using the defaults for default settings\n\t\tenvironment_name = instance.environment.name()\n\t\tif"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\tconfig = {\n\t\t\t\"path\": \"{0}/{1}\".format(etcd_host, etcd_port),\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\n\t\treturn config\nfrom copy import deepcopy\n\nimport pytest\nimport numpy as np\n\nfrom openmdao.api.definitions import ModelDefinition\nfrom openmdao.main.helpers import assert_arrays_equal\nfrom openmdao.main.helpers import assert_all_equal\n\n\n@pytest.mark.parametrize(\"spec\", [1, 2, 3])"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n# Set up the database configuration.\ndb = MySQLdb.connect(user = user,\n                       passwd = passwd,\n                       db = etcd_host,\n                       port = etcd_port,\n                       query = \"select max(i.maxValue) from %s\" % (user),\n                       cursorclass=MySQLdb.cursors.DictCursor)\n# Set up the tables.\ncursor = db.cursor()\ncursor.execute(\"drop table if exists %s cascade\" % (user))\ncursor.execute(\"create table if not exists %s (maxValue INT AUTO_INCREMENT, minValue INT"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef get_public_key(user):\t\t\t\"\"\"\n\tReturns a public key to be used in the base config, and if the passed \n\t'user' is not included in the base config, returns None.\n\nArgs:\n    user (str): the name of the user for the static authentication\n\tetcd_host (str): the host for the database.\n\tetcd_port (int): the port for the database.\n\nReturns:\n    str: the public key for the input parameter\n\t\"\"\"\n\t\n\tif user not in base_config().users:\n\t\tbase_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\t\n\t# create the directory structure and create all the directories created.\n\tcreate_directory(\"htdocs\")\t\n\tcreate_directory(\"htdocs.htdocs\")\n\tcreate_directory(\"htdocs/htdocs\")\t\n\tcreate_directory(\"htdocs/htdocs_config.json\")\n\tcreate_directory(\"htdocs/htdocs_config.json\")\t\n\tcreate_directory(\"htdocs/htdocs_client.json\")\t\n\tcreate_directory(\"htdocs/htdocs_client.json\")\t\n\tcreate_directory(\"htdocs/htdocs_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    base_config={\"user\":user, \"site\": etcd_host, \"etcd_port\": etcd_port}\t\n    logger.verbose('Getting base configuration from db: \"%s\" (default = %s)' % (base_config[\"user\"], base_config[\"site\"]))\n    return base_config\n##TODO implement base_config\n\"\"\"\nAuthor: Yerend Haber\nDate: 2015-02-10\n\"\"\"\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import cross_validation\nfrom sklearn.datasets import fetch_20newsgroups"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\tif user!= \"None\":\n\t\t\tprint \"User: \"+user+\" is not a valid user\"\n\t\t\texit()\n\t\t# create the database\n\t\tdatabase = os.path.join(etcd_host, \".etcd_users\")\n\t\tif not os.path.exists(database): os.makedirs(database)\n\t\t# create the tables\n\t\ttablename = \"etcd_users\"\n\t\tif not os.path.exists(tablename): try:\t\n\t\t\t\tos.makedirs(tablename)\n\t\t\tprint \"created Table: \"+tablename\n\t\t\t\tprint \"Creating tables:\"\n\t\t\t\t"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n# class A\n# class B(object):\n# class C(object):\n# class D(object):\n# class E(object):\n# class A(object):\n#\nimport collections\nimport datetime\nimport math\nimport operator\nimport os\nimport zipfile\n\nimport numpy\n\nfrom sqlalchemy import (\n    Column, MetaData, PrimaryKeyConstraint, Integer, String, Table)\nfrom sqlalchemy import engine_from_config, exc, event, Integer, select\nfrom sqlalchemy.ext.hybrid import hybrid_property\nfrom sqlalchemy.orm import mapper\nfrom sqlalchemy.orm.exc"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    def __add__(self, other):\n        if isinstance(other, timezone):\n\t    if other is None:\n\t\t\t        return self\n\t\t\telif other > self:\n\t\t\t    return other.__add__(self)   \n\t\t\telse:\n\t\t\t    return self.__sub__(other)\t\n\t\telif isinstance(other, datetime):\n\t\t    return self.__class__(self._fromutc(other), self._fromutc(self), \n\t\t\t\t\t\t \n\t\t\t\t\t\t  _toutc(self))\n\t\t    else:\n\t\t    return NotImplemented\n            return self.__radd__(other)\t\n\t\n\tdef __sub__(self, other):\n        if isinstance(other, timezone"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass TimezoneMismatch(Exception):\n    \"\"\"\n    Raised when a time zone difference produces a different amount of time.\n\n    :param type: The type of time difference (e.g. 'A' or 'M', etc.)\n    :type type: str\n    :param msg: The error message, such as reason or cause\n    :type msg: str\n    \"\"\"    \n\n    def __init__(self, type, msg):\n        self.type = type\n        self.msg = msg\n        \ndef isambiguoustime(dt):\n    \"\"\"\n    Returns True if the time is ambiguous and, if so, the time is still"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass _Date(object):\n\t__metaclass__ = DateMeta\n\n\tdef __new__(cls, s):\n\t\tm = cls.__new__(cls)\n\t\tm.s = s\n\t\t# add any non-overlapping dates\n\t\tm.add_dates(s - s.utcoffset(),s - s.dst())\n\t\treturn m\n\n\tdef __repr__(self):\n\t\tdays = getattr(self,'days',0) \n\t\tminutes = getattr(self,'minutes',6) \n\t\tseconds = getattr(self,'seconds',0) \n\t\treturn \"%s(days=%s"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    # Now we just return the UTC datetime.\n    # \n    return dt\n\n#-------------------------------------------------------------------------\n#\n#\n#-------------------------------------------------------------------------\n\nclass DateOfBirth(Value):\n\t\"\"\"\n\tDateOfBirth returns a date object of the current time being\n\tlast birth during the specified day.\n\t\"\"\"\n\t_name = \"date-of-birth\"\n\t_display = _(\"Date of birth\")\n\t_expression = DateOfBirthExpression\n\t_operator = DateOfBirthOp\n\n\tdef _cast(self, obj, type_):\n\t\tif isinstance(obj, datetime):\n\t\t\treturn obj."}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn dt.tzinfo.__time__\n\texcept AttributeError:\n\t\t# dt = dt.replace(tzinfo=object(tzinfo=object(self)))\n\t\tpass\n\nclass _Timezone:\n\t\"\"\"\n\tA generic time zone. \n\t\n\t:since: 2.3.0\n\t:type tz: ``datetime.tzinfo``\n\t:param tz:\n\tThe :class:`datetime.datetime` that you'd like to store the\n\tvalue in.\n\t:type tz: ``datetime.tzinfo`` \n\t:returns: The :class:`datetime"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass Eastern(datetime.tzinfo):\n    def utcoffset(self, dt):\t\"\"\"\n\t:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass Amtime(datetime.tzinfo):\n    \"\"\"\n\t:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n       \nclass Ymd(datetime.tzinfo):\n    \"\"\"\n\t:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n       \nclass Mornington(object):\n    \"\"\"\n\tMonoton"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\t# We need to do some tricks:\n\t# 1.  Find out if the passed datetime represents time in ambiguous time\n\t# 2.  Get back that time, and convert it to a timezone-aware datetime\n\t# 3.  Find out if the passed datetime represents time in a known future\n\t# 4.  Then check for ambiguous time that match the current datetime\n\t#\tas a result of comparing to dt.\n\t#\n\t#  A.  Find a unique day for the day we are doing so on to know\n\t#\tby which the datetime is ambiguous"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\tfrom.dateutils import UTCTimeZoneCorrectionBase\n\n\t\t# Note we only care about microseconds\n\t\t# because we don't include microseconds, but it's\n\t\t# usually not required by the base class.\n\t\tdt = DTDate(dt, tz='UTC')\n\n\t\t# Return the modified timezone\n\t\treturn UTCTimeZoneCorrectionBase(dt)\n\ndef _toutc(dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n__all__ = ['date', 'fromutc','mktime', 'time', 'weekday', 'isleap', 'tuesday', 'wednesday', 'thu', 'fri','sat',\n\t\t\t'gmt', 'ttz', 'tutu']\n\n# Constants\n\nNTA_MARCH_TO_MST = -4\n\"\"\"\nA timezone-aware datetime.timedelta object from a UTC MST's time zone.\nThis is always true for UTC.\nMaybe in time zone aka time.localtime().\n\"\"\"\n\n# Local imports\n\nimport"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ndef local_datetime(dt):\n\t\"\"\"\n\tConvert a :class:`datetime.date` object to a :class:`datetime.date` or\n\ta :class:`datetime.date(1970,1,1)`.\n\t\"\"\"\n\t\n\tif not issubclass(dt, datetime.datetime):\t\n\t\t\traise TypeError(\"Expected a datetime.datetime or \" + \n\t\t\t\t\t\t\"datetime.date type, got \" + type(dt).__name__)\n\tif dt.weekday() == 1:\n\t\tdt = dt - datetime.timedelta(days=1)\n\telse:\n\t\tdt"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn DateUtil.relativedelta(self, days, hours)\n\t\n\t# ---------------------------------------------------------------------\n\t# PRIVATE METHODS\n\t# ---------------------------------------------------------------------\n\t\n\tdef __neg__(self):\n\t\"\"\"\n\tReturn :const:`datetime.timedelta` representing the negative\n\ttimedelta. This returns a value equivalent to the sum of times in\n\tthe current timedelta, but does not change the result. \n\t\"\"\"\n\t\treturn self.__result\n\t\n\t# ---------------------------------------------------------------------\n\t# PUBLIC PROPERTIES\n\t# ---------------------------------------------------------------------\n\n\t@property\n\tdef days(self):\n\t\t\"\"\"\n\tThe days of this"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    def __init__(self, day, hours=0, minutes=0, seconds=0):\n\t\t\"\"\"\n\t\tInitializes and loads a :class:`~dateutil.relativedelta.relativedelta` object.\n\n\t\t:param day:\n            A 3-part date object containing the date of the day as defined in the current format (without microseconds part).\n\t\t\tFor example :\n\t\n\t\t\t>>> relativedelta(days=28)\n\t\t\"\"\"\n\t\tself.day = day\n\t\tself.hours = hours\n\t\tself.minutes = minutes\n\t\tself.seconds = seconds\n\n\tdef __cmp__(self, other):"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\nfrom datetime import timedelta\nfrom datetime import datetime\nfrom datetime import date\nfrom datetime import time\nfrom datetime import timedelta\nfrom datetime import tzinfo\nfrom itertools import chain\n\nimport pytz\nif pytz is not None:\n    import pytz.timezone as tz\n\nimport numpy as np\nfrom numpy.lib import recfunctions as rec\n\nfrom.. import __version__\n\nclass RelativeTime(timedelta):\n    \"\"\"\n    Relative time, starting from 1 second and ending from 3600 (the end of\n    the relative time of the start of the relative time of the month,\n    as specified by the definition of the :meth"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t    \n\t\n\treturn relativedelta(days=1.5, hours=2)\t\n\t\t\ndef normalize_in_months(in_months):\n\t\"\"\"\n\tReturns a new :class:`dateutil.relativedelta.relativedelta` object based on the given\n\tmonth's months.\n\tThis method returns this object, with a *delta* between the given months.\n\t\n\tArgs:\n\t    \tin_months: a normalized object whose months already have the \n\t\t\t\tsame major minor number.\t\t\n\t      \tIf None is passed, the normalizes that months to the given\n\t\t\t\tnumber.\t\n\t    \t"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn DateUtil.relativedelta(self._months)\n\n\tdef __add__(self, other):\n\t\treturn DateUtil.relativedelta(self._months, other._months)\n\n\tdef __mul__(self, other):\n\t\treturn DateUtil.relativedelta(self._months * 2 * self._months, other._months * 2 * self._months)\n\n\tdef __truediv__(self, other):\n\t\treturn DateUtil.relativedelta(self._months / 2, other._months / 2)\n\n\tdef __neg__(self):\n\t\treturn DateUtil.relativedelta(-self._months)\n\nclass"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n_normalized = relative_days = _fraction = _normalized\n\ndef _relative_months (first=None, second=None):\n\t\"\"\"\n\tReturns a :class:`dateutil.relativedelta.relativedelta` object with\n\trelative dates between 1 and the given months.\n\t\"\"\"\n\t\ndef _relative_days_range(first, second):\n\t\"\"\"\n\tReturns a :class:`dateutil.relativedelta.relativedelta` object with\n\trelative dates between 1 and the given days.\n\t\"\"\"\n\t\n_fixed_months_range = _fixed = _months = None\n\ndef"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t \nclass relativedelta :\n\t\n\t\"\"\"\n\tThe class represents a delta from relativedelta to datetime objects\n\twith offsets from time that the relativedelta takes into account.\n\tAlso: it handles any cases where the datetime objects\n\tare timezone aware, so when the relativedelta is not timezone aware, the\n\tsubtract of 1 is returned.\n\t\"\"\"\n\t\n\tdef __init__ (self, offset = 0.0): \n\t\tif (offset!= 0.0):\n\t\t\tself.offset = offset\n\t\n\tdef __repr__ (self):\n\t\treturn self.__class__.__name__ + \" \" + str("}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t       \n\t# First check if it's a datetime or a string, then\n\t# the output might be incorrect, and we try to convert it to\n\t# timezone-naive object.\n\tif isinstance(self.data, datetime):\n\t\t\treturn self._normalize(self.data)\n\n\n\t# First return the datetime object.\n\tobj = datetime(\n\t\t\tyear=self.year, \n\t\t\tmonth=self.myMonth, \n\t\t\tday=self.day, \n\t\t\thour=self.hour,  \n\t\n\t) # It is guaranteed that the offset is within the month\n\treturn"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn dateutil.relativedelta.relativedelta(days=self.days, hours=self.hours)\n\n\t# Return a dateutil.relativedelta.relativedelta object based on the object's\n\t# relative attributes.\n\tdef in_out_context(self):\n\t\t\"\"\"\n\t\tReturn a context object which is either an object or a string\n\t\tin which case it returns something different from\n\t\t``self.hours <= self.seconds``.\n\t\t\"\"\"\n\t\t# NOTE: In the context of the object, we return a string\n\t\t# that is similar to\n\t\t# `strftime` which always uses"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tfn = namefunc.decode(\"utf-8\")\n\n\t# Change to ascii so that it includes special characters like ':'\n\tif len(fn)-2 and fn.isalnum():\n\t\tfn += \"_\"\n\n\tresult = fn + \" \" # Remove special characters of the last word\n\treturn result\n\ndef namefunc(name):\n\t\"\"\"\n\tConvert a function name to a list function or function object. The\n\tinput is converted to a list and the output is converted to\n\tsome string. The function must return a list of two strings, the first\n\tand second strings of"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tnamefunc_encoding = sys.getfilesystemencoding().lower()\n\t\tnamefunc_encoding = namefunc_encoding.decode(namefunc_encoding, 'ignore')\n\t\t# NOTE: This function has been renamed from python2 to\n\t\t#  \t``functools.wraps(unicode)``. This function is also\n\t\t#  \tneeded for some purposes based on the fact that str() has\n\t\t#  \tto support Python 2.x as it will only work on Python 2 version.\n\t\t#  \tIn order to simplify the result to bytes, use the new Python\n\t\t#  \tunicode object"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tnamefunc.name = namefunc.name.decode(\"ascii\", \"replace\")\n\t\treturn namefunc  # no call needed to convert\n\n\tdef utcoffset(self, dt):\n\t\treturn self.tzoffset  # no call needed to convert\n\n\t# NOTE: some day-to-jun-believe is that python2 and python3\n\t# is deprecated for the moment\n\tutcoffset.tm_isdst = True\n\n\tdef tzname(self, dt):\n\t\treturn self._checkvalue(self._checkvalue)[2:-1]\n\n\ttzname.name ="}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn _py3_tz.tzname(namefunc)\n\treturn namefunc  # Not really needed: use getname if possible\ngetname = tzname_in_python3\n\n\"\"\"\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(S)\n\tdef tzname_in_python3(namefunc):\treturn namefunc(unicode)\n\tdef tzname_in_utf8(namefunc):\t\treturn namefunc(unicode, \"utf-8\")\n\tdef tzname_in_b(namefunc):\treturn namefunc(unicode)\n\tdef tzname_u(tznamefunc):\treturn namefunc(bytes, \"us-ascii\")\n\t# Python 2 - 3.\n\ttzname_f = tzname_u\n\tdef utcfromtimestamp(namefunc):\treturn func2str(func2"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tfrom codecs import smart_str\t# force conversion to bytestring\n\t\timport os\n\t\ts = os.path.normcase(os.path.abspath(name))\n\t\tf = os.path.normcase(os.path.abspath(f))\n\t\tif s is TESTFN: raise unittest.SkipTest(\n\t\t\t\"'filename' is ambiguous or has wrong meaning\")\n\t\tf = f.decode(s)\n\t\tif f not in \"w0lUuYwH2oU3WrE\":\n\t\t\treturn f+\".utf-8\"\n\t\treturn f+\".utf-8\""}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\ndef checkname(name):\n\t\tf = namefunc()\n\t\tg = nameparser.NameParser()\n\t\tg.feed(f.feed())\n\t\tout = g.entry\n\t\tdel g # XXX: check names in globals!\n\t\tif out[:1] == '<!':\n\t\t\tout = out[2:] # strip last +suffix\n\t\tfor d, dname, f in g.entryfields:\n\t\t\tif d + f.find('.') > 0:\n\t\t\t\td = d[1:]\n\t\t\t\tf.decodesource(d)\n\n\treturn''.join(c"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\t# Use only ASCII names\n\t\tnamefunc.name = namefunc.name.encode('utf-8', 'backslashreplace')\n\n\t\t# Replace all non ASCII characters by a single space (or non-ASCII).\n\t\tnamefunc.name = namefunc.name.replace(' ', '_')\n\t\treturn namefunc.name\n\treturn str\n\n#\n# Make a function for Python's strftime call, for convenient\n# comparison\n#\ndef strftime_func(x):\n\t\"\"\"\n\tChange datetime objects in Python 2:\n\nstrftime_func(x) is the same as strftime,"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc.name\n\n\tdef _get_ascii_name(func):\n\t\t\"\"\"\n\t\tGet a unicode name from the module's __name__ value\n\t\t\"\"\"\n\t\treturn func.__module__.replace(\"%s.unicode\" % namefunc.__name__, namefunc.__name__)\n\n\tdef _get_printable_name(func):\n        \"\"\"\n\tGet a unicode name from the module's __name__ value\n\t\t\"\"\"\n\t\treturn func.__name__\n\n\tdef _get_typeconv_func(func, max_len=10, is_int=False):\n\t\t\"\"\""}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc.__name__\n\ndef get_timezones(timezones):\n\t'''\n\tReturn a list of tuples of (zone name, time zone offset)\n\t\n\ttimezones: list of tuples of (zone name, time zone offset) tuples.\n\t'''\n\treturn tuple(tz[1] for tz in timezones if zonename2loc.has_key(tz))\n    \ndef time2string(t):\n\t'''\n\tConvert (int, float) into string\n\t\n\t@type  t: int\n\t@rtype:  str\n\t@return: converted"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversion_files = { }\n\t\tversion_files['cassandra'] = find_file('cassandra', 'index.json')\n\t\twith open('cassandra/version.json') as fp:\n\t\t\tversion_files['cassandra'] = fp.read().split('\\n', 1)[0]\n\t\tif not find_file('cassandra/version.log', 'r') is None:\n\t\t\treturn version_files\n\t\tlines = []\n\t\tfor idx, line in enumerate(fileinput.input(mode='rt', inplace=1).splitlines()):\n\t\t\tif'versions'"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tvcs = \"git v1.8.6 -m 'git describe --tags > get_version'\"\n\t\tout, rc = run(vcs, cwd=REPO_ROOT, verbose=False).returncode\n\n\t\tif out!= 0:\n\t\t\traise SubversionError(\" unable to determine version from tags\"\n\t\t\t\t\t\t\" of current git repository\"\n\t\t\t\t\t\t)\n\n\t\treturn GIT_VERSION\n\n\t@staticmethod\n\tdef get_commits():\n\t\t\"\"\"\n\t\tGet the latest commits from the repository.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tout, rc = run(\n\t\t\t\t[GIT_"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\treturn __version__\n\t\texcept Exception:\n\t\t\t_logger.exception('Cannot get version info from git')\n\t\t\treturn default_version\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom.common import InfoExtractor\nfrom..compat import compat_str\nfrom..utils import (\n    int_or_none,\n    parse_duration,\n)\n\n\nclass JybridIE(InfoExtractor):\n    \"\"\"JYB interface from <video> tags.\"\"\"\n\n    _VALID_URL = r'https?://(?:video|s|p)/(?:embed_|video)'\n   "}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tif not hasattr(frappe.local, \"docs_version\"):\n\t\t\tfrappe.local.docs_version = frappe.db.get_single_value(\"Production Settings\", \"WebsiteVersion\")\n\t\t\tfrappe.local.docs_version_id, frappe.local.docs_version_id_date_format, frappe.local.docs_versions[0][\"name\"]\n\t\t\tfrappe.local.clear_cache(frappe.cachedir)\n\n\t\treturn frappe.local.docs_version\n\nfrom pyrsistent import prs\nfrom pyrsistent.cache import store_or_delete\nimport"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\t# this might not be useful because versioning is not possible,\n        # but it can be useful to avoid any bugs.\n\t\ttry:   \n\t\t\treturn self.version\n\t\texcept Exception as e:\n\t\t\tpass\n\t\t\n\t\tversion = e.get('version')\n\t\tif not version:\n\t\t\t# if user manually set version in config file, use that.\n\t\t\tversion = ''\n\t\telse:\n\t\t\ttry:\n\t\t\t\twith open('config/version').read() as f:\n\t\t\t\t\tversion = f.split('=', 1)[1].strip(' ')\n\t\t\t\t\tversion = '.'.join(version"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tfrom cupy import version\n\t\tif version is None:\n\t\t\traise RuntimeError(\"\"\"No cupy version available, cannot retrieve version info.\"\"\")\n\n\t\treturn version.split('\\n', 1)[0]\n\n\tdef get_version_info():\t\"\"\"\n\tGet version information from `cupy` API. Note: This does NOT return the version info.\n\t\"\"\"\n\t\tfrom cupy import version\n\t\tif version is None:\n\t\t\traise RuntimeError(\"\"\"No cupy version available, can retrieve version info.\"\"\")\n\n\t\treturn version\n\n_module__ = _get_module_name_for('c"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\tfrom git import Repo\n\t\t\t# git output for 'git version'\n\t\t\tversion_info = Repo.version_info(repodir).strip()\n\t\texcept Exception:\n\t\t\ttry:\n\t\t\t\tversion_info = Repo.git_version(repodir).strip()\n\t\t\texcept Exception:\n\t\t\t\tversion_info = \"unknown\"\n\t\treturn version_info\n\n\nclass GitInfo(object):\n\t\"\"\"\n\tGit-specific information\n\n\tGit information is fetched from Git, the main format. However in git info\n\tis stored in.git and.bzr files"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn version_dict.get('version', None)\n\n\treturn version_dict\nfrom setuptools import setup, find_packages\nimport sys\nhere = os.path.abspath(os.path.dirname(__file__))\n\nversion = '1.5.4'\n\nlong_description = ''\ntry:\n    long_desc = open(os.path.join(here, \"README.rst\")).read()\nexcept IOError:\n    readme = open(os.path.join(here, \"CHANGELOG.txt\")).read()\n    if \"TODO\" in readme or \"NO"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\treturn cim_get_versions()[\"Versions\"]\n\t\texcept:\n\t\t\treturn DEFAULT_VERSION_LIST\n\t\n\t@property\n\tdef current_version(self):\n\t\treturn self._current_version\n\nclass VersionManager:\n\tdef __init__(self, config_file):\n\t\tself.log = cim_get_language_logger()\n\t\tself._config_file = config_file\n\t\tself._current_version = None\n\t\tself.global_config = None\n\t\n\tdef get_current_version(self):\n\t\treturn self._current_"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tfrom pip._internal.utils.network import get_versions\n\n\t\tout = {}\n\t\tfor p in get_versions().split('\\n'): \n\t\t\ts = p.split('\\t')\n\t\t\tif int(s[2]) in out:\n\t\t\t\tout[int(s[2])].append({})\n\t\t\t\tbreak\n\t\t\tout[int(s[2])][int(s[3])] = s[0]\n\n\t\treturn out\n\ndef read_requirements_file():\n\t\"\"\"Return list of files in requirements for easy_install,\n\tand all dependencies (if defined)\"\"\"\n"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# remember shell=False, so we do what we're doing here.\n\t\tif \"shell\" in style:\n\t\t\treturn Color(pieces)\n\t\telse:\n\t\t\treturn style\n\nclass NoBuildPathsSpecified(Exception): pass\n\ndef detect_path_spec_or_repo(options):\n\t\"\"\"\n\tThis function is called when there are no path segments or\n\tdependencies. It returns the actual directory name, relative to\n\tthe buildroot. The default behavior is to return the directory\n\tname with the path removed if no explicit path separator is specified.\n\t\"\"\""}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# Default options\n\t\topts = {}\n\n\t\t# Render the given version pieces into the requested style\n\t\tfor p in pieces:\n\t\t\tif not p:\n\t\t\t\tcontinue\n\n\t\t\t# Construct and return the style for this piece\n\t\t\tstyle = get_style(p)\n\n\t\t\t# Apply any inherited data to the style, if it exists\n\t\t\tif p in IGNORE_LAYOUT_PATTERNS:\n\t\t\t\topts[style] = getattr(opts, p)\n\t\t\telse:\n\t\t\t\topts[style] = getattr(style, p)\n\t\treturn Style(**opts)\n\ndef get"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# The style guidelines need to be updated with the full\n\t\t# version number.\t\t\t\n\t\tif style is not None:\n\t\t\tp = new_style()\n\t\telse:\n\t\t\t# Use the proper style for the new style \n\t\t\tnew_style = Style(**style)\n\t\treturn p.render(pieces)\n\t\n\tdef get_default_style_defs(self):\n\t\t\"\"\" \n\t\tReturns a list of default style definitions, if any.\n\t\t\"\"\"\n\t\treturn [ ]\n\tdef get_default_font_defs(self):\n\t\t\"\"\" \n\t\tReturns a list"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# default: style=short\n\t\tif style == \"short\": return \"short\"\n\t\telif style == \"stable\": return \"stable\"\n\t\telif style == \"minor\": return \"minor\"\n\t\telif style == \"candidate\": return \"candidate\"\n\t\telse:\n\t\t\tprint \"Unknown style \\\"%s\\\" in version %d\" % (style, version)\n\ttry:\t\n\t\ts, h = _version.split(_gittag)\n\t\tif len(s) == 2:\n\t\t\tstyle, tag = s\n\t\telse:\n\t\t\tstyle, tag = s[:-1"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# save original style\n\t\told_style = pieces.pop(\"style\", None)\n\t\tif style is None:\n\t\t\tstyle = \"border:1px solid #D1D1D1\"\n\t\t\n\t\t# replace style attributes according to the style attributes to use for style\n\t\tstyle = dict(style.items() + {\"stylename\": stylename, \"class\": styleclass})\n\t\tdel pieces[\"style\"]\n\t\t\n\t\t# build the HTML template\n\t\ttag_contents = []\n\t\t\n\t\t# for each piece in pieces\n\t\tfor piece in pieces:\n\t\t\ts_part = piece.text"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# prepare for new layer\n\t\tnew_style = []\n\t\tfor substyle in style:\n\t\t\tif new_style:\n\t\t\t\tnew_style[-1] += substyle\n\t\t# remove empty components\n\t\tnew_style = new_style[:-1]\n\t\t# fix whitespace\n\t\tfor substyle in style:\n\t\t\tsubstyle = substyle.lstrip()\n\t\t\twhile substyle:\n\t\t\t\tsubstyle = substyle.rstrip()\n\t\t\t\tleng = len(substyle)\n\t\t\t\tif not substyle or leng > len(style): del style[-leng:]\n\t\t\t\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tpieces = [pieces] if type(pieces) is list else [pieces]\n\t\tpieces.append('--style') if style is not None else ''\n\t\treturn django_style.render(pieces)\n\ndef no_style(pieces):\n\t\"\"\"\n\tNever render the given version pieces into the requested style.\t\"\"\"\n\treturn django_style.render(pieces, None)\n\ndef media_bundle(pieces):\n\t\"\"\"\n\tBuild the bundles for the given version pieces.\n\t\"\"\"\n\treturn django_compressor.compress(pieces)\n\ndef run_media_"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\tpath = 'doc/css/%s.css' % style\n\t\tif os.path.exists(path):\n\t\t\toutput = file(path, 'w')\n\t\t\toutput.write(*pieces)\n\t\t\toutput.close()\n\t\t\treturn output.name\n\telse:\n\t\traise ValueError(\"No style supplied\")\n\ndef make_style(pieces):\n\t\"\"\"\n\tSet the CSS for the given list of `pieces`.  If the value of this\n\tflag is missing (an empty string), the default value for that flag will be used.\n\t\"\"\"\n\tp = ["}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\t# Build up a temporary file and render the resulting version\n\t\tf = tempfile.NamedTemporaryFile(\n\t\t\tsuffix=os.path.basename(pieces.path),\n\t\t\tmode='wb', prefix='render-temp-',\n\t\t\tcontent=pieces.content\n\t\t)\n\n\t\t# Load all the pieces into memory until the file was fully written\n\t\ttry:\n\t\t\twith f:\n\t\t\t\ttemplate_contents = paster.render(pieces.content.split())\n\t\texcept template.TemplateSyntaxError as e:\n\t\t\traise CommandError(\n\t\t\t\t'restaticlartile.render unexpected error rendering '\n\t\t\t\t'{"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn Context(pieces, style)\n\ndef slugify(s):\n\t\"\"\"\n\tRemove non-alphanumeric and underscores from a string.\n\t\"\"\"\n\treturn re.sub(r'[^a-z0-9_\\-\\.]', '_', s)\n\ndef reverse_dict(dict_):\n\t\"\"\"\n\tReturn a dictionary mapping field names (with lowercase hexdigests) to\n\tnew value.\n\t\"\"\"\n\tfor key in dict_.iterkeys():\n\t\tdict_[key] = dict_[key.lower()]\n\treturn dict_\n\n\ndef build_url(version, *"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\treturn '.'.join(pieces)\n\n\ndef render_pep440(pieces):\t\"\"\"\n\tBuild up version string, with post-release \"local version identifier\" if there is one.\n\t\"\"\"\n\timport urllib.request\n\n\t# next build-up version identifier.\n\tparts = re.search( r'version\\s+=\\s+(.+)-(\\d+)-(\\d+)-(\\d+)', pieces.strip() )\n\tif parts:\n\t\treturn '+{}'.format( parts.group(1) )\n\n\t# one or more pieces isn't a full version\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\t' + if tag == 'arrowleft':\n\t\t\treturn ''\n\t\treturn if_tag+' '+pieces+' '\n\telse: return if_tag\n\n@add_arg('-d', '--device', default=None, help=\"Get device string.\")\n@add_arg('-e', '--editor', action=\"append\",\n              help=\"Enter editor to add.\")\n@add_arg('-s', '--serial', action=\"append\",\n              help=\"\"\"Open serial port to open the device. If the device is\n  already opened, it will be opened and opened again for the first access to\n  others.\n "}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tassert pieces[\"closest-tag\"] or pieces[\"distance\"]\n\t\treturn \"+\"\n\n\ndef render_pep440_pre(pieces):\n\t\"\"\"\n\tIf there is a tag and a git SHA, it lives in a pre-release\n    version.\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev0\"\n    else:\n        # exception #1\n        rendered = \""}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif'' in s:\n\t\t\treturn '.'\n\t\treturn ''\n\n\tdef eval_entityref_eq(self, s):\n\t\t\"\"\"\n\t\tReplace an identifier with an expression like \"foo = bar;...\"\n\t\t\n\t\tNote that this does not require the following:\n\t\t(foo, Bar(), None)\n\t\t\"\"\"\n\t\tidx = self.idx\n\t\twhile idx > 0:\n\t\t\tif self.eval(s) == '*':\n\t\t\t\tidx -= 1\n\t\t\t\treturn None\n\t\t\tidx -= 1\n\n\tdef eval_entityref_in(self, s):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\t# If there is a dot, return a  consisting of a \n\t\t# plus sign.\n\t\tif pieces[0].startswith('+') and pieces[-1].endswith('+') or pieces[0].startswith('+'):\n\t\t\treturn '.'\n\t\treturn '-'.join(pieces)\n\n\nclass Solution(object):\n\tdef plus_or_dot(self, pieces):\t\n\t\t\"\"\"\n\t\tRewrite a string S a.b.c to produce a + If we don't already have a dot, return \n\t\ta.  Else return a  composed string of length S.\n\t\t\"\"\"\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif '+' in pieces.get('+', ''):\t# not currently in the state yet\n\t\t\treturn \".\" + pieces.get('+')\n\t\treturn \"+\".join(pieces)\n\n\tdef render_pep440(pieces):\t# See the docstring for pep440.html#d\n\t\treturn draw_pep440_body(pieces) + \"\\n\"\n\n\ndef render_pep440_body(pieces):\t# See the docstring for pep440-body.html#d\n\t\treturn draw_pep440_body.get(pieces, \"\") + \"\\n\"\n\n\ndef get_version():"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\treturn 'A'\n\n\tdef render(self, request, rendered_html):\n\t\t\"\"\"\n\t\tApplies all a few adobe-like stuff to the given request, which is sent in\n\t\tvia HTTP.\n\t\t\"\"\"\n\t\t# Build up a list of (name, value) tuples.  Each tuple's name is a\n\t\t# 2-tuple of the form (app_label, option_value) where 'app_label' is the\n\t\t# app label and 'option_value' is the option VALUE.\n\t\toptions = [(name, option_value or '') for"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\treturn \"+\" + \"+\" + \" + str(pieces.pop() + 1)\n\treturn \"\"\n\ndef render_pep7(pieces):\n\t\"Build up version string for pep440.\"\n\tfull_pieces = pieces.split('.')\n\tfull_pieces.extend(full_pieces)\n\tfor i in range(3):\n\t\tfull_pieces.append(pieces[i])\n\treturn \".\".join(full_pieces)\n\ndef render_pep8(pieces):\n\t\"Build up version string for pep440.\"\n\tfull_pieces = pieces.split"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\treturn \"+\" if pieces.endswith(\"*\") else \"_+\"\n\n\tdef render(self, renderer):\n\t\ttry:\n\t\t\ttemplate = renderer.compile()\n\t\t\treturn renderer.render(template)\n\t\texcept TemplateSyntaxError, e:\n\t\t\treturn \"<div class=\\\"error\\\"><div class=\\\"error\\\">{0}</div><small>Error: {1}</small></div>\".format(str(e), str(e.args))\n\n\nclass TemplateError(Exception):\n\t\"\"\"\n\tIndicates a template error\n\t\"\"\"\n\n\tdef __init__(self, template, args, exc"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\treturn ')if'in pieces else '.join(pieces)\n\n\n# Given non-empty 'pieces', return a string with the above tags.\n# You should always have a not ending space.\n# ex: \"if ('something has foo barh') return 'barh'\ndef push_meta(pieces):\n\t\"Add one to the end of the rendered meta tags.\"\n\tpieces.append('  %s=foo' % pieces['foo'])\n\n# Either element-list or tag-list should be separated by spaces.\n# See the documentation for 'tags' in TAG-"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t#\n\t\t# TODO: this should only be implemented by the subclass that\n\t\t# handles its own exceptions, so it should be handled by\n\t\t# the subclass and the parent class.\n\t\t#\n\t\tif not isinstance(commands, (list, tuple)):\n\t\t\tcommands = [commands]\n\n\t\tPASSED = False\n\t\tfor command in commands:\n\t\t\tif hide_stderr:\n\t\t\t\tprint \"stdout not implemented for {0}\".format(command)\n\t\t\t\treturn False\n\n\t\t\tp = subprocess.Popen(command, shell=False, stdin=subprocess.PIPE, stdout=subprocess.PIPE,"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\timport subprocess\n\t\tp = None\n\t\tfor c in commands:\n\t\t\tp = subprocess.Popen(c, shell=True, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)\n\t\t\tout, err = p.communicate()\n\t\t\treturn p.returncode, out.rstrip()\n\t\telse:\n\t\t\tif verbose:\n\t\t\t\tprint \"Unable to execute '{cmd}'\".format(cmd=\" \".join(commands))\n\t\t\tif hide_stderr:\n\t\t\t\traise CalledProcessError(returncode=1, cmd=\" \".join(commands), stdout=subprocess.PIPE"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ts_cmd = list(commands)\n\t\ts_cmd.extend(args)\n\n\t\tif verbose: print s_cmd\n\t\tif cwd is not None:\n\t\t\tprint cwd\n\t\tprint 'entering %s' %''.join(s_cmd)\n\t\tif env is None:\n\t\t\tenv = os.environ.copy()\n\n\t\tstart_time = time.time()\n\t\ttry:\n\t\t\toutput = subprocess.check_output(s_cmd, cwd=cwd, env=env)\n\t\t\telapsed = time.time() - start_time\n\t\t\tmsg = '"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tfrom subprocess import *\n\t\tp = None\n\t\tfor c in commands:\n\t\t\tp = Popen(c, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env, stdout=PIPE, stderr=PIPE)\n\t\t\tif p.stdout:\n\t\t\t\tp.stdout.close()\n\t\t\tstdout, stderr = p.communicate(args)\n\t\t\tif not hide_stderr:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(\"%s: %s\" % (c, stdout))\n\t\t\t\tif p.returncode!= 0:\n\t\t\t\t\tsys.stderr.write(\"%s\\n\" % p."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\timport subprocess\n\t\tp = subprocess.Popen(command)\n\t\tp.wait()\n\t\tif verbose: print(\"Command '%s' ran for %s\" % (command, \" \".join(commands)))\n\t\treturn p\n\n\tdef list(self):\n\t\t\"\"\"\n\t\tList all commands, printing their output.\n\t\t\"\"\"\n\t\tp = self.run_command([\"ls\", \"-l\"])\n\t\tfor line in p.stdout:\n\t\t\tprint(line)\n\n\tdef start(self):\n\t\t\"\"\"\n\t\tStart the system or watch for a subsystem.\n\t\t\"\"\"\n\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif isinstance(commands, basestring):\n\t\t\tcommands = [commands]\n\t\t\tif cwd is None:\n\t\t\t\tcwd = '.'\n\t\t\tfor c in commands:\n\t\t\t\tif os.path.exists(os.path.join(cwd, c)):\n\t\t\t\t\treturn c\n\t\tp, c = os.path.split(str(cmd))\n\t\tif cwd is None:\n\t\t\tcwd = os.getcwd()\n\t\tp = str(env.get(p,None))\n\t\tif verbose:\n\t\t\tif p:\n\t\t\t\tprint(p)\n\t\t\telse:\n\t\t\t\tprint(\"Error:"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp =subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)\n\t\tout, err = p.communicate()\n\t\tif p.returncode!= 0:\n\t\t\tif hide_stderr:\n\t\t\t\traise CalledProcessError(p.returncode, out, err)\n\t\t\telse:\n\t\t\t\traise\n\t\t\n\t\treturn out\n\ndef get_version():\n\t\"\"\"\n\tExtract the openshift version from /etc/openshift that is available on\n\tthe system.\n\t\"\"\"\n\tfrom pyvcl.utils.utility import get_env"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tPillowOrdinaryCommandGenerator(display_stderr=False, output_format=\"always\")\t\t\t\n\t\tout, err = self.run_command(commands, args, cwd=cwd, return_stdout=False, env=env)\n\t\treturn out\n\n\n\tdef run_command(self, commands, args, cwd=None, env=None):\n\t\t\"\"\"\n\t\tRun the given command(s).\n\t\tThe args parameter should be a list of command-line arguments.\n\t\t\"\"\"\n\t\tif isinstance(commands, basestring):\n\t\t\tcommands = shlex.split(commands)\n\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tprint(\"RUNNING COMMAND: \" + str(commands))\n\t\tif hide_stderr:\n\t\t\treturn STDERR.flush()\n\t\tif env is None:\n\t\t\tenv = os.environ\n\n\t\ttry:\n\t\t\treturn subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()\n\t\texcept OSError:\n\t\t\tprint(\"Error in executing command. (This may indicate an uncaught exception).\")\n\t\t\tprint(\"You may want to specify an environment variable\")\n\t\t\treturn None\n\n\tdef get_paths(self, env=None"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = Popen(commands, args, cwd=cwd, stdout=PIPE, stdin=PIPE, stderr=STDOUT, env=env)\n\t\tif p.wait() == 0:\n\t\t\treturn p.stdout.read()\n\t\telse:\n\t\t\tprint(\"Command '%s' failed with error code %d\" % (args, p.returncode))\n\t\t\tif verbose:\n\t\t\t\tprint(\"stderr: %s\" % (p.stderr))\n\t\t\traise p\n\n\tdef run_shell(self, args, use_unsafe_shell=False):\t\"\"\"\n\tCall the given command(s"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# This is only used inside of _ setup.py to create a\n        # _version.py file with an appropriate scheme.py file for the top\n        # of your project. For unusual information, see:\n        # https://github.com/warner/python-versioneer.py/issues/5496.\n\t\timport VersioneerConfig\n\t\tself.config = VersioneerConfig()\n\n\tdef use_git_versions(env):\n\t\t\"\"\"Use git-describe for the specified version clause.\"\"\"\n\t\ttry:\n\t\t\toutput = run_command([\"git\", \"describe\", \"--tags"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\timport os\r\n\t\tinfo = get_config_from_root(\"../configure.py\")\r\n\t\tif info is None:\r\n\t\t\treturn None\r\n\t\treturn VersioneerConfig(info)\r\n\r\n\tdef git_versions(self):\r\n\t\t\"\"\"\r\n\t\tGit versions as a string along with self._version_cmd.\r\n\t\t\"\"\"\r\n\t\timport git\r\n\r\n\t\tout, _ = git.cmd(['git', 'describe', '--tags'], cwd=self.root)\r\n\t\r\n\t\t# # List of files in repo\r\n\t\t# git_path = out.strip()\r\n\t\t# git_full = re."}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n        # _version.py\n        cfg = get_config_from_root(cfg_filename)\n\t\tif not cfg:\n\t\t\traise NotThisMethod(\"no.cfg file found\", cfg_filename)\n\t\treturn cfg\n\n\t@classmethod\n\tdef from_revision_file(cls, filename):\n\t\t\"\"\"Create, populate and return VersioneerConfig() from a.rst\n\t\tfile.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t f = open(filename)\n\t\t\treturn cls( f.read(), read_ext"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# make a copy of the raw config.\n\t\t config = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\ttry:\n\t\t\tconfig.versionfile_source = config.versionfile_source.strip().replace(\" \", \" \")\n\t\texcept AttributeError:\n\t\t\tfrom None import set_versionfile_source\n\t\t\tconffile = set_versionfile_source(merge_base=False)\n\t\t\tif conffile:\n\t\t\t\ttry:\n\t\t\t\t\tfrom VCSController import VCSController\n\t\t\t\texcept ImportError:\n\t\t\t\t\tfrom. import VCSController\n\t\t\t\t\tvcs ="}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# make sure the version file is exists\n\t\ttry:\n\t\t\tf = open(versionfile_abs)\n\t\t\tfor line in f.readlines():\n\t\t\t\tif line.strip().startswith(\"git_refnames =\"):\n\t\t\t\t\tmo = re.match(r'=\\s*\"(.*)\"', line)\n\t\t\t\t\tif mo:\n\t\t\t\t\t\tvcs, var = mo.groups()\n\t\t\t\t\t\treturn vcs, var\n\t\t\t\t\t\t# do a normal version-comparison rather than copying into\n\t\t\t\t\t\t# _version.py, which does not have access to the top location.\ntry:\n  import git_refnames\n  #"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\timport os\n\n\t\t# make sure the version file is defined\n\t\twith open(os.path.join(os.path.dirname(__file__), \"versioneer.py\"), \"w\") as fd:\n\t\t\tfd.write(\"\")\n\n\t\ttry:\n\t\t\tcfg = VersioneerConfig()\n\t\t\tcfg.VCS = \"git\"\n\t\t\tcfg.style = \"pep440\"\n\t\t\tcfg.tag_prefix = \"\"\n\t\t\tcfg.parentdir_prefix = \"versioneer\"\n\t\t\tcfg.versionfile_source = \"mne-versioneer.py\"\n\t\t\tcfg.verbose = False\n\t\t\tcfg.versionfile"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\ttry:\n\t\t\treturn VersioneerConfig()\n\t\texcept NameError:\n\t\t\t cfg = get_config_from_github_config_file()\n\t\t\treturn cfg\n\n\t@classmethod\n\tdef from_env(cls, os_env=gff.environment_path(), **kwargs):\n\t\t\"\"\"Create a new VersionsioneerConfig from the given environment.\"\"\"\n\t\t# these strings are used in case they'll be patched during Py2exe\n\t\tpy3 = sys.version_info[0] == 3\n\t\tif py3:\n\t\t\tdel sys.version_info[0]\n\t\tconfig"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\troot = get_root()\n\t\tassert root.name =='versioneer'\n\t\ttry:\n\t\t\tc = VersioneerConfig()\n\t\t\twrite_to_file(c.get_config(), os.path.join(root,'setup.py'))\n\t\t\treturn c\n\t\texcept EnvironmentError:\n\t\t\t e = sys.exc_info()[1]\n\t\t\tif hasattr(e,'message'):\n\t\t\t\tprint(\"Adding docstring to versioneer-compile.cfg: \")\n\t\t\t\tprint(e.message)\n\t\t\telse:\n\t\t\t\tprint('Unable to create Versioneer config file.')\n\n\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n        # _version.py\n        cfg = VersioneerConfig()\n\t\tcfg.VCS = \"git\"\n\t\tcfg.style = \"pep440\"\n\t\tcfg.tag_prefix = None\n\t\tcfg.parentdir_prefix = \"v\"\n\t\tcfg.versionfile_source = \"invenio-base-api/index.py\"\n\t\tcfg.verbose = False\n\t\tcfg.tag_prefix = None\n\t\tcfg.parentdir_prefix = \"v\"\n\t\tcfg.versionfile_build = \"invenio-base"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\ttry:\n\t\t\timport versioneer\n\t\t\treturn versioneer.get_config()\n\t\texcept ImportError:\n\t\t\treturn None\n\n\t@property\n\tdef version(self):\n\t\timport versioneer\n\t\timport versioneer.commands\n\n\t\t# Do the same using import_module().\t\n\t\timport versioneer.versionfile_source\n\t\tfor line in versioneer.versionfile_source.split('\\n'):\n\t\t\tfor part in open(line, ).split('/'):\n\t\t\t\tif part.startswith(versionfile_source.versionfile_source):\n\t\t\t\t\treturn versioneer.versionfile_source\n\t\treturn versioneer"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t    \t@functools.wraps(method)\n\t    \tdef wrap(f):\n\t        \tmethod_name = method.__name__    \t# store short, full version/revision\n\t        \tvcs.register(method_name, (vcs, method_name, f))\t\t\t\t\t# then, add to our list\n\t        \treturn f\n# --------------------------------------------------------------------------\t\t\t    \tdef init_vcs_handler(self):  # set up vcs_handler(vcs, method)\n\t        \tself.vcs_handlers[vcs] = self\n\t        \tself.vcs_handler_lookup[method] = vcs."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register(method, f)\n\t\t\treturn f\n\t\treturn decorate\n\treturn decorate\n\t\ndef run(options, args):\n\ttry:\n\t\t# execute the command\n\t\tcall_subprocess(vcs, args)\n\texcept Exception, e:\n\t\tprint \"Error: \" + str(e)\n\t\tsys.exit(3)\n\nif '__main__' == __name__:\n\toptions, args = getopt.getopt(sys.argv[1:], \"v\")\n\tif options:\n\t\t# process command line options\n\t\tfor"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\treturn _vcs_handler(vcs, method)\n\treturn decorate\n\nclass Repository(object):\n\t\"\"\"\n\tThe Repository objects are a set of \"repository\", \"tag\",\n\t\"parent\", \"vcs\". This class implements various VCS manipulation\n\talgorithms, such as \"clean\", \"clone\", \"copy\", \"remove\" and \"checkout\",\n\t\"clonetag\" (if they are available).\n\t\n\tThe Repositories contain:\n\n\tvcs_dir\t\tThis is the repository path to store Git\n\ttags\t\t\tThis is the set of tags this"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(handler):\n\t\t\tvcs.handler_dict[method] = handler\n\t\t\treturn handler\n\t\treturn decorate\n\n\treturn decorate\n\ndef executefile(filepath):\n\t\"\"\"Execute file in a VCS.\tNote that the VCS has already been initialized\n\texplicitly. This does nothing if the file contains a VCS.\n\t\"\"\"\n\timport sys\n\ttry:\n\t\tf = open(filepath, \"r\")\n\texcept IOError, e:\n\t\traise VCSFileNotFound, \"Can't find file `%s', failed to open `%s'\" % \\\n\t\t\t\t(filepath,"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(func):\n\t\t\t#@register_vcs_handler\n\t\t\t@functools.wraps(func)\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t\tvcs.register(method)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn inner\n\t\treturn decorate\n\n\treturn vcs_handler\n# -*- coding: utf-8 -*-\n\"\"\"\ntest_pyfsextract.tests.test_pyfsextract\n~~~~~~~~~~~~~~~\n\nTests for pyfsextract.\n\"\"\"\n# # imports from top of module:\n\n# # import package:\n#"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tvcs.register(method, vcs.Handler(handler))\n\t\treturn wrapper\n\t\t\n\treturn decorator\n\t\ndef clean_up_vcs(vcs_handler):\n\t\"\"\"\tClean up a VCS.\t\"\"\"\n\t\tvcs_handler.validate()\n\t\tvcs_handler.get_vcs(settings.VCS)\n\t\tvcs_handler.cleanup()\n\t\tdel vcs_handler\n\t\t\n\tdef load_settings():\n\t\tsettings.load_vcs(vcs_handler)\n\t\n\tdef set_dirty(vcs, name=None):\n\t\t\"\"\"Set dirty value of VCS"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tvcs.register(method, wrapped)\n\t\t\treturn func\n\t\treturn wrapper\n\treturn vcs.__decorator__\n\t\ndef get_supported_vcs_tools():\n\t\"\"\"Return a list of VCSTools compatible with this project as a\n\tList of (vcs, func).\n\tReturn a tuple (vcs_tools, func).\n\t\"\"\"\n\tsupported = set()\n\tif is_vcs_root():\n\t\treturn (vcs for vcs, func in get_supported_vcs_tools().items() if func())\n\telse:\n\t\treturn (vcs"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif vcs.lookup(method):  # shortcut for all VCSs\n\t\t\t\treturn vcs.handler(method)(*args, **kwargs)\n\t\t\telse:\n\t\t\t\traise LookupError(\"Unknown VCS method '%s'\" % method)\n\t\treturn wrapper\n\treturn register_vcs_handler\n\n\ndef render(data):\n\t\"\"\"\n\tLike render but returns a string to display on the\n\tserver side of the client. This just returns the content\n\tas is (for HTML tags, no tags).\n\t\"\"\"\n\treturn mark_safe(unicode("}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\t@functools.wraps(method)\n\t\tdef _wrapped(self, *args, **kwargs):\n\t\t\tvcs.register(self, method.__name__)\n\t\t\treturn method(self, *args, **kwargs)\n\n\t\treturn _wrapped\n\n\t@property\n\tdef vcs(self):\n\t\tif not hasattr(self, '_vcs'):\n\t\t\tself._vcs = VCS(settings.VCS_DEFAULT_LOCATION)\n\t\treturn self._vcs\n\n\t\tfrom pip._internal.req import parse_requirements\n\t\ttry:\n\t\t\tparse_requirements(self._vcs, self.project_root)"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\trepo = vcs.repository(f)\n\t\t\trepo.handler = None\n\t\t\treturn f\n\t\treturn decorate\n\n\treturn decorator\n\ndef load_function(function):\n\t\"\"\"\n\tDecorator to extract a VCS-specific handler from a python file.\n\t\"\"\"\n\tdef __load(self, repo):\n\t\treturn self.hook_load(repo, load_vcs_handler=getattr(self, function))\n\treturn __load\n\nvcs = None\n\ndef vcs_setup(name=None, repo=None, show_init=True"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t\"\"\"Valid objects can be found to be found under the root of the hierarchy\n\tgood_parents - number of parents of the object hierarchy\n\t\"\"\"\n\t\tif validate_objects == True:\n\t\t\t\"\"\"Number of objects is checked (root must have parent nodes)\t\"\"\"\n\t\t\tif check_digests == False:\n\t\t\t\tlog.info(\"Validation of the hierarchy is not supported\")\n\t\t\t\"\"\"Use validate_objects to check if two versions are consistent\n\t\t\ton the last time the hierarchy was changed\"\"\"\n\n\t\t\tif self.repo == None:\n\t\t\t\tmsg = \"Cannot validate the hierarchy. Empty repo"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tif validate_objects:\n\t\t\tvalidate_objects = 0\n\t\t\tnum_objects = len(list(self._objects)) \t\n\t\telse:\n\t\t\tvalidate_objects = 1\n\t\t\tnum_objects = self._objects.count()\n\n\t\tif validate_digests:\n\t\t\tvalidate_digests = 0\n\t\t\tnum_objects = len(list(self._digests))\t\n\t\telse:\t\t\n\t\t\tvalidate_digests = 1\n\t\t\tnum_objects = self._digests.count()\n\n\t\tif num_objects == 0 or num_objects == 1:\n\t\t\treturn 0"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t    \t# check if root hierarchy is valid (if not, raise error)\n\t    \tif check_digests:\n\t        \t\tnum_objects_validated = 0\n\t        \t\ttry:\n\t\t\t    \t\t\tnum_objects_validated = self.get_object_count()\n\t\t            \t\texcept:\n\t\t\t    \t\treturn 0\n\t\t\t    \tnum_object_validated = 0\n\t\t    \tif validate_objects and num_objects_validated > 0:\n\t        \t\treturn num_object_validated\n\t        \t\telse:\n\t        \t\treturn 1\n\t    \t\telse:\n\t        "}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# build dictionary containing information about the valid objects, and its sub-objects\n        objects_info = {}\n\t\t# build dictionary containing information about objects, but also its ancestors\n\t\tobjects_info[\"objects\"] = {}\n\t\t# build dictionary containing information about all objects\n\t\tobjects_info[\"errors\"] = {}\n\t\t\n\t\t# create dictionary of valid objects\n\t\tif not validate_objects:\n\t\t\t# build dictionary containing all valid objects\n\t\t\tobjects_info[\"objects\"] = {}\n\n\t\t# populate dictionary with required objects and their ancestors for tree retrieval\n\t\tgood_objects = object_factory(\"object_data"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tvalidate_objects = validate_objects\n\t\tif not self.root:\n\t\t\tnum_objects = len(self.valid_holds)\n\t\t\tif num_objects == 0:\n\t\t\t\tnum_objects = None\n\t\t\t\tgood_objects = 0\n\t\t\telif not self.valid_holds[0].parent:\n\t\t\t\tnum_objects += 1\n\t\t\t\tgood_objects += validate_objects\n\t\t\tif not self.valid_holds[1].parent:\n\t\t\t\tnum_objects += 1\n\t\t\t\tgood_objects += validate_objects\n\t\treturn num_objects == good_objects\n\n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\t\t\n\tassert(validate_objects)\n\tassert(validate_digests)\n\t\n\t\n\t# Check if objects are valid, if not, return error message\n\ttry:\n\t\tif not self.get_is_valid():\n\t\t\treturn \"\"\n\texcept self.Error as e:\n\t\tprint(\"[ERROR] %s\" % str(e))\n\t\treturn \"\"\n\t\n\t# Validate if valid objects are required\n\tif is_object(self.root_info):\n\t\tvalid = False\n\t\tnum_objects = len(self.root_info)\n\t\tif num_objects <"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\tobjs = self.object_storage.list(self.account)\n\t\n\tif (not self.show_warnings) or (validate_objects):\n\t\twarnings=[]\n\n\t\tfor obj in objs.list:\n\t\t\ttry: \n\t\t\t\tobj.validate()\n\t\t\texcept exceptions.MalformedS3Error: \n\t\t\t\twarnings.append(obj.errors)\n\t\t\t\twarnings_by_level[obj.level] = 0\n\t\t\telse:\n\t\t\t\twarnings_by_level[obj.level] = 1\n\t\t\t\twarnings_by_level[obj.level*min(self.num_objects"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n    num_objects = 0\n    good_objects = 0\n    if check_digests:\n        # We check first if the first object in the hierarchy is the\n        # one we expect then if that is found, then use the second\n        # since the verification is done in\n        if is_valid(check_objects, num_objects, good_objects):\n\t    # The first object is valid, hence return (num_objects, good_objects)\n            \tnum_objects = 1\n\t    good_objects = 0\n    return num_objects, good_objects\n\ndef init_instance(request, instance):\n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n\tif not isinstance(self.name, basestring):\n\t\treturn 0, 0\n\n\t# Check to see if we can find a valid file\n\tfile_index = self.find_file_index('valid.txt', 0)\n\n\t# If we need to be able to find all of the files\n\tif self.check_folders():\n\t\tself.validate_folders_file()\n\n\t# Otherwise, we can't validate the objects for the current folder\n\tif validate_files and self.get_items():\n\t\tfor item in self.items():\n\t\t\titem_type ="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\t\n\tif validate_objects:\t\t\n\t\tvalidate_objects_list = []\n\t\tfor s in self.root_hierarchy_json:\n\t\t\tvalidate_objects_list += s[\"name_list\"]\n\t\t\tif show_warnings:\n\t\t\t\tself.warnings += 1\n\t\t\t\tself.warnings_count += 1\n\t\t\ttry:\n\t\t\t\tvalidate_objects_json = self.validate_file(self.root_hierarchy_json[s], validate_object_types=True)\n\t\t\t\tvalidate_objects_dict = validate_objects_json[\"object_types\"]\n\t\t\texcept KeyError:\n\t\t\t\t"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.current_dir = ConfigLocations.userGettersFolder()\n\t\t#create the storage root file\n\t\tstorage_root = self.current_dir + \"\\\\\" + self.current_dir\n\t\tos.mkdir(storage_root)\n\t\t\n\t\tself.create_backup_folders()\n\n\t\t#create a \"backup\" folder in the current dir, unless it were\n\t\t#run from an existing backup folder.\n\t\t#\n\t\tif os.path.exists(Storage.backup_folder):\n\t\t\t#file on creation\n\t\t\tstorageRoot2 = Storage.backup_folder\n\t\t\t\n\t\t\t#"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.root = ofl.create_root(self.storage_root)\n\t\t\n\t@property\n\tdef storage_root(self):\n\t\treturn self.root\n\t\t\t\n\tdef list_files(self):\n\t\tresult = []\n\n\t\tfor root, dirs, files in os.walk(self.root):\n\t\t\tfor filename in files:\n\t\t\t\t#print filename\n\t\t\t\tresult.append(os.path.join(root, filename))\t\t\t\n\n\t\treturn result \n# -*- coding: utf-8 -*-\n##!/usr/bin/env python\n\n# Copyright (c"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself._init_config()\n\t\tself.cfs_parent_id = 0\n\t\tself.cfs_parent = None\n\t\tself.cfs_serial = None\n\t\n\tdef __del__(self):\n\t\t\"\"\"\tDelete the OCFL storage root.\t\"\"\"\n\t\tself.close_file_at_close(self.cfs_parent_id)\n\t\tif (self.cfs_parent!= None):\n\t\t\twith open(self.cfs_parent.path, 'r+b') as f:\n\t\t\t\tfor line in f:\n\t\t\t\t\tif ("}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.root_name = os.path.abspath('%s%s%s' % (settings.storage_root,\n\t\t\t\t\t\t\tsettings.mount_dir, settings.mount_suffix))\n\t\tif os.path.exists(self.root_name):\n\t\t\tself.root_file = open(self.root_name,'rb')\n\t\telse:\n\t\t\tsettings.mount_file = os.path.abspath(self.root_name+'/'+self.mount_file_name)\n\t\tif settings.mount_dir!= '':\n\t\t\tif not os.path.isdir(settings"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\t# Create root directory structure\n\t\tif self.parent:\n\t\t\tif not os.path.exists(self.parentdir):\n\t\t\t\tos.makedirs(self.parentdir)\n\t\t\tdirectory = os.path.join(self.parentdir, self.name)\n\t\t\tos.makedirs(directory)\n\t\t\t\n\t\t\tif not os.path.exists(directory):\n\t\t\t\tfrappe.connect()\n\t\t\t\n\t\t\t# Get OCFL path\n\t\t\tocf = frappe.get_pkg_dir(\"ocfl\")\n\t\t\tif ocf:\n\t\t\t\tself.ocf = ocf\n\t\t\tself.path"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.id \t= os.getpid()\n\t\tself[\"storage-root\"] = Directory(self.id, self.root, self.id)\n\n\t\tfor k in list(self.config.keys()):\n\t\t\tsetattr(self, k, self.config[k])\n\n\t\tself.config[\"storage-root\"].read()\t\t\n\n\tdef read_config_json(self):\n\t\treturn json.load(self.config)\t\t\n\t\t\n\t@classmethod\n\tdef from_config(self, config):\n\t\twith cls.config.load_data(config) \\\n\t\t\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.root = self.create_root(self.get_name())\n\t\tself.get_storage_size()\n\n\tdef get_name(self):\t\"\"\"\n\t\tGet storage name for given storage root. Does not require directory to exist.\n\t\t\"\"\"\n\t\treturn str(self.root)\n\t\t\n\tdef open(self, path, mode='rb'):\n\t\t\"\"\"\n\t\tOpens a file.\n\t\t\"\"\"\n\t\twith self.open_file(path) as fileobj:\n\t\t\treturn fileobj\n\n\tdef get_size(self):\t\"\"\"\n\t\tGet size for file"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.current = True\n\t\ttry:\n\t\t\tif not os.path.isdir(\"/opt/openday.log\"):\n\t\t\t\tos.makedirs(\"/var/log\")\n\t\t\tself.log = open(\"/opt/openday.log\", \"a+\")\n\t\texcept:\n\t\t\tself.log = open(\"/var/log\", \"w\")\n\t\n\tdef get_log(self):\n\t\t\"\"\"\n\t\tReturn the ocdfl storage log.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tfile_log = open(\"/opt/openday.log\", \"r\")\n\t\texcept:"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself._root = ocflfl.OcflflStorageRoot(self._storage.getRootDir(), \"root\")\n\t\t## This should never be necessary as most OFFC requires\n\t\t# multiple storages to be created simultaneously.\n\t\tif os.getuid() == 0:\n\t\t\tos.mkdir(self._root.getRootDir())\n\n\t## \\brief The storage root is open for writing.\n\t# @param self A pointer to the parent folder\n\t# @param path A path that is to be used as an OCF lfc storage root\n\t# @"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.root = ET.Element(\"root\")\n\n\t\tself.path = \"root/ocfl\"\n\n\t\tself.config = {\t# Configuration\n\t\t\t\"storage_root\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"etc\", \"storage\"), # Root location\n\t\t\t\"config_dir\": self.path,\n\t\t\t\"config_args\": list(),\n\t\t\t\"debug\": False, # True in verbose mode, False in non-verbose mode\n\t\t\t\"verbose\": False, # True in verbose mode, False in non-verbose mode\n\t\t"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tv = version.lstrip(\"0\")\n\t\tif v is not None:\t# Normal v\n\t\t\tyield Version(v)\n\n\t\t# Unknown v\n\t\tfor prefix in (\"[foo]\", \"[bar]\"):\n\t\t\tfor suffix in (\"[foo]\", \"[bar]\", \"[baz]\"):\n\t\t\t\tyield Version(prefix)\n\n\nVERSION_RE = re.compile(r\"(?P<major>\\d+(\\.\\d+)?)[.]([xY]\\d+)[.]([yY]\\d+\\.[xU]\\d+)\")\n\n\ndef iterversions(pattern):\n\t\"\"\"\n\tGenerate iterator"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ttry:\n\t\t\tversion = int(version)\n\t\texcept:\n\t\t\tversion = 0 # the string is not already there\n\n\t\tif version:\n\t\t\tnum = 1\n\t\t\tfor ver in reversed(filter(lambda x: x >= 0 and x < num+1, version, repeat(r'[^\\d]+'))).group(num):\n\t\t\t\tnum+=1\n\t\t\treturn version + str(num)\n\t\telse:\n\t\t\treturn r'\\d.0(\\.)+0(\\.)+0(\\d*)-2(\\.)+(e{3})'\n\n\nclass _VersionPattern:\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tres = version.split(\".\", 3)\n\t\tassert len(res) == 2\n\t\treturn ''.join(res[:2]) + res[1]\n\t\t\n############\n#\n#\n#\n#\n#\n############\n#\n#\n#\n\nclass PodFile(object):\n\t\"\"\"\n\tA class representing an object that represents only the pood\n\tpattern, not just the version number.\n\t\"\"\"\n\tdef __init__(self, file):\n\t\t\"\"\"\n\t\tThe file-like object to represent this object.\n\t\t\"\"\"\n\t\tself.file ="}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n\t\n\t# We return the latest version in a special way.\n\treturn versions[0]\n\ndef latest_version(api_request):\n\t\"\"\"\n\tReturn a versions tuple from the API request, or None if there is \n\tany matching version.\n\t\"\"\"\n\t# There are three cases.\n\t# - In this case a non-zero version number is appended to a tuple\n\t#   (0,0,None), indicating that none of the versions are on the last\n\t#   entry (in this case, return None).\n\telif api_request.get('"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tver_info = None\n\t\twhile version and len(version) > 0:\n\t\t\ttmp = []\n\t\t\ttmp.append(version)\n\t\t\tver_info, _ = parse_version_line(tmp[-1])\n\t\t\tversion = tmp[-1]\n\t\treturn ver_info\n\n\tdef version_parse(self, line):\n\t\tres = parse_version_line(line)\n\t\tif res:\n\t\t\treturn res\n\n\tdef skip_version_line(self, line):\n\t\treturn not line.startswith(\"version 0.0:\")\n\n\tdef check_next_"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tglobal _next_version\n\t\t# Check for non-zero prefixed versions.\n\t\tif version[0] and not pattern.match(version[0]):\n\t\t\t# Check for non-zero prefixed versions.\n\t\t\tnext_version = str(int(version[0]))\n\t\t\tif not _next_version:\n\t\t\t\traise ValueError(\"invalid input '%r' : should be non-zero-prefixed version\" % version[0])\n\t\t_next_version = version[1:]\n\treturn _next_version\n\ndef __version__():\n\t\"\n\tProvide this function (a"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tp = regex.compile(version, re.I)\n\t\tif p.match(next_version):\n\t\t\treturn next_version\n\t\tn = p.match(version)\n\t\tif n:\n\t\t\tyield n.groups()[0].lower()\n\tyield next_version\n\ndef normalize_version(version):\n\t\"\"\"\n\tNormalize version based on current version identifier.\n\tVersion must be a major numeric prefix and followed by an integer.\n\n\tVersion may have at least 3 digits, followed by a space.\n\n\tNote that the first two digits are optional.\n\t\"\"\""}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn 0\n\ndef previous_major_version(version):\t\"\"\"\n\tPrevious version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\treturn 0\n\n\nclass WPTS(object):\n\t__patterns = ['1.3',\t\"1.2\",\t\"3.0.0\"]\n\t__data_version = 0\n\t__wpt_version = 0\n\t__regex_version = 0\n\t__wpt_version_regex = None # used for automatic regex matching.\n\tdef __init"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif version == \"zero\" or version[0].isdigit():\n\t\t\treturn version\n\t\tversion = re.sub(r\"[A-Za-z0-9_]+\", \"_\", version) # drop leading underscore\n\t\tlower = version.lower()\n\t\tif lower in VERSION_MAP:\n\t\t\treturn VERSION_MAP[lower]\n\t\tversion = string.lower(version)\n\t\tif lower == \"x\":\n\t\t\treturn \"0\"\n\t\tif lower == \"Y\":\n\t\t\treturn str(version)\n\t\traise ValueError(\"version not found\")\n\n\n# ======================================================================\n#\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\twhile len(version)!= 1:\n\t\t\tparts = version.split(\".\")\n\t\t\tif len(parts) == 2:\tbreak\n\t\t\tversion = \".\".join(parts[:-1])\n\t\t\tbreak\n\t\treturn version\n\telse:\n\t\treturn version\n\"\"\"\nCopyright(c) 2016 Lassy Tandelberg\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor vdir in version_dirs:\n\t\t\tfor digest in self.get_contents_from_file(self.root + \"/\" + vdir, method=\"GET\") :\n\t\t\t\tif digest.startswith(\"__\") :\n\t\t\t\t\treturn\n\nif __name__ == \"__main__\":\t\n\tparser = ArgumentParser()\n\t\n\tgroup = parser.add_argument_group(\"Root\")\n\tgroup.add_argument(\"-s\", \"--set\", help=\"set target set type (default:'manual')\")\n\tgroup.add_argument(\"-k\", \"--secret\", help=\"set secret\")\n\t\n\tparser."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\nclass Inventory:\n\t\"\"\"\n\tRepresents a single inventory structure, describing inventories, and offers\n\ta method for retrieving inventory data from those that represent the root SLOT data and\n\tthe information of slots, such as the inventory type, and the data that is\n\tstored as the inventory information. The inventory data contains all data that are\n\tstored in the inventory, which can be overridden by the ``storeData`` method\n\tmethod.\"\"\"\n\t\n\tdef __init__(self, root, root_slots, version_dirs):\n\t\t\"\"\"\n\t\t:param root: The root SLOT index"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tversion_digests = iter(get_valid_hash_vers(version_dirs))\n\t\tassert len(version_digests) == len(version_dirs), \"All version files must have the same digest\"\n\t\tfor version_dir, checksum in version_digests.iteritems():\n\t\t\tself.validate_check_installed_version(version_dir, \n\t\t\t\tchecksum, False, self.get_installed_hashes)\n\n\tdef validate_check_installed_version(self, version_dir, checksum, installed_hashes):\t\n\t\tfile_name = \"%s.%s\""}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\t\n\tif not isinstance(version_dirs, list):\t\t# Assume a sequence of version directories\n\t\t# Assume that there were no version directories, or we got a wrong\n\t\t# number of versions.\n\t\traise Error(\"Version directory is required to check for content digests:\")\n\t\t\n\tfor d in version_dirs:\n\t\tif not os.path.exists(d): raise Error(\"The file: %s does not exist\" % d)\n\t\t\n\tfor x in self.version_data:\n\t\tversion_dirs = glob.glob(os.path.join(x, '*'))\n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\ttry:\n\t\t\tassert len(version_dirs) == 5\n\t\t\tfor version_dir in version_dirs:\n\t\t\t\tensure_directories_exist(version_dir)\n\t\texcept AssertionError:\n\t\t\tlog.warn(\"Unable to determine the content, please install something.\")\n\n\tdef get_content_hash(self, version_dir):\n\t\ttry:\n\t\t\tcache_hash = cache_listdir(version_dir)\n\t\t\treturn cache_hash\n\t\texcept NoListingError:\n\t\t\tfrom.localpath import get_absolute_path\n\t\t\tpaths = get_absolute_path(version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\ndef get_inventory_version(package):\n\t\"\"\"\n\tChecks for content digest of version package\n\tusing the _package_dict['root_path_to_package']. Returns version-specific version\n\tname, if installed, version-specific version (if not). Also returns the actual\n\tversion, if not.\n\t\"\"\"\n\tif _package_dict['root_path_to_package'].has_key(_package_dict['root_path_to_package']):\n\t\treturn _package_dict['root_path_to_package']\n\tif _package_dict"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\n\tif not os.path.exists(\".\"):\n\t\traise AssertionError(\"The directory for an unversioned version_dir (%s) does not exist\" %(version_dirs))\n\t\t\n\n\twith open(\".\", \"r\") as f:\n\t\tlines = f.readlines()\n\t\tfor line in lines:\n\t\t\tif not line.startswith(\"#\"):\n\t\t\t\tcontinue\n\t\t\tversion_dir = line.split()[0].lstrip(\"/\")\n\t\t\tif version_dir in version_dirs:\n\t\t\t\n\t\t\t\traise AssertionError(\"The directory [%s] includes version [%s] in its inventory\" %(version_dirs, version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\n\n\tself.content_digests = []\n\tfor d in version_dirs:\n\t\tif os.path.exists(d):\n\t\t\tf = open(d,\"rb\")\n\t\t\ttry:\n\t\t\t\tdata = f.read()\n\t\t\t\tf.close()\n\t\t\texcept:\n\t\t\t\tdata = b\"\"\n\t\t\tbdigest = md5(data).digest()\n\t\t\tif bdigest!= self.content_digests_seen:\t#This is not a version change\n\t\t\t\tself.content_digests_seen = bdigest\t#This is a version change - we can update with"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tdpaths = []\n\t\tfor version_dir in version_dirs:\n\t\t\t# first check if local version of the directory is higher than the version requested by\n\t\t\t# --version, or if that directory is higher than the requested version\n\t\t\tif not version_dir.startswith(self.version_lower):\n\t\t\t\tdpaths.append(version_dir)\n\t\tfor version_dir in dpaths:\n\t\t\tif not os.access(version_dir, os.R_OK|os.W_OK):\n\t\t\t\traise error.AnsibleError(\"Invalid relative directory '%s'.  Please give version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version_dir in version_dirs:\n\t\t\tpath = os.path.join(version_dir, self.inv_file)\n\t\t\t# skip any non-conflict files\n\t\t\tif os.path.isfile(path):\n\t\t\t\treturn\n\n\tdef prepare_inventory(self):\t\"\"\"\tPerforms final setup for updating the inventory.\n\t\t\"\"\"\n\t\tensure_dir(self.inv_file)\n\t\tif not os.path.isdir(self.inventory_dir):\n\t\t\tos.mkdir(self.inventory_dir)\n\t\tif os.path.isfile(self."}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# Try and get the actual path type.\n\t\t#\n\t\tif '*' in path or '?' in path:\n\t\t\treturn 'root'\n\t\telif os.path.isdir(path):\n\t\t\treturn 'object'\n\t\telif '\\\\' in path:\n\t\t\treturn 'file'\n\t\telif not os.path.splitext(path)[1] == '.root':\n\t\t\treturn 'other'\n\t\telif isosx:\n\t\t\treturn 'file'\n\t\telif iswindows:\n\t\t\treturn 'file'\n\t\treturn 'OTHER'\n\telif path.count('/') == 2 and path.upper()"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\ndef get_object_type(filename):\n    \"\"\"\n\tReturn the type of thing of type NAME with a particular file extension.\n\n\tReturn values:\n\t'test' - looks like an OCFL Storage Object\n\t'dev' - looks like an OCFL Directory\n\n\tIf you want to look at the following directories:\n\n      os.pardir\n      os.sep\n      os.altsep\n\n\tThe first look follows the documentation:\n\n      'path://path/to/an/ocf'\n\n\tThe next looks as if found where the file is a single OCFL directory. \n\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# FIXME: What should be \"0=*\" when os.path.walk is implemented\n\tpath = os.path.normpath(os.path.abspath(path))\t\t# Nomasca's dircase is a path so we can start with '/' instead\n\t\ttype_of_dir = os.path.commonprefix([path, type_of_dir])\n\t\tif \"0==*\" in path:\t\treturn \"root\"\n\t\treturn os.path.dirname(path)\n\t\t\t\n\ttype_of_dir = type_of_dir.strip(\"/\")\t# Should never"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\treturn 0\n\n### Special functions to get the number of files returned and\n### the current file directory count.\ndef get_number_of_files_returned(file_type):\n\treturn os.listdir(OSF_DIRECTORY) + 1\n\n### Helper functions to calculate the OF files, used during the\n### invocation of scan\ndef get_files_list(root_mount_point=\"/\", filename_list=None, directory_count=None):\n\tif not file_type == OSF_FILE:\n\t\treturn []\n\n\t# get all the OF (or"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\ttry:\n\t\t\tobj=os.path.basename(path)\n\t\t\tobj=os.path.splitext(obj)[0]\n\t\t\treturn obj\n\t\texcept Exception:\n\t\t\treturn 'root'\n\nclass FSTypeError(Exception): pass\nclass ObjectSizeError(Exception): pass\nclass InvalidOptionError(Exception): pass\n\"\"\"\nA test for the sdn.py command\n\"\"\"\n\nimport os, sys, glob, glob\nimport platform\nimport warnings\nfrom StringIO import StringIO\nimport shutil\nimport errno\nimport socket\nimport threading\n\nfrom sdn.util import"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n#TODO: this is not a great deal on it, but this is the default file type.\n\n#FIXME: this is a different class, and this shouldn't be used anymore,\n\nclass OCFLStorageObject:\n\t\"\"\"This class contains metadata about disk or disk storage devices.\"\"\"\n\tdef __init__(self, name=None, serial='none', bus='scsi', label='', options=(), path='', is_file=False):\n\t\t#self.name = name\n\t\tself.serial = serial\n\t\tself.bus = bus\n\t\tself.label = label\n\t\tself."}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tif not os.path.exists(path):\n\t\traise OSError(_('Not found'))\n\t\tfor root, dirs, files in os.walk(path + 'O:') :\n\t\t\tif isinstance(root, str):\n\t\t\t\tif root!= '':\n\t\t\t\t\treturn os.path.join(root, filetypes[guess_type(root)[0]])\n\t\t\t\telse:\n\t\t\t\t\traise KeyError(_(\"Could not find the root\"))\n\t\t\telif not f.endswith('.'):\n\t\t\t\tcontinue\n\t\t\telif len(files) == 1:\n\t\t\t\treturn f\n\t\t\telse:\n\t\t\t\tfiles.sort(key=lambda"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n# \tif os.path.isfile(path) & 0x7f == 0:\n# \t        return ELEMENT_OF_STRING\n# \telif os.path.isfile(\"/\") & 0x10000 == 0x10000:\n# \t        return ELEMENT_OF_STRING\n# \telif os.getcwd() == \"\":\n# \t        return ELEMENT_OF_STRING\n# \telif os.name!= \"nt\":\n# \t        return 'error: cannot open OS \"%s\"' % str(os.getcwd())\n# \t        raise Exception(\"os.path"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tif len(os.listdir(path)) > 0:\n\t\t\treturn 'root'\t\n\t\telse:\n\t\t\treturn None\n\telif os.path.islink(path) or os.path.isdir(path):\n\t\treturn 'object'\n\telif os.path.isfile(path):\t\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'dir'\n\telse:\t\n\t\treturn None\n# -*- coding: utf-8 -*-\n\nimport re\nfrom gluon import *\nfrom gluon.storage import Storage\n\n# -----------------------------------------------------------------------------"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tpath = str(path)\n\t\tif path == \"root\":\n\t\t\tresult = str(os.statvfs(path))\n\t\telif path.find(\"object\")!= -1:\n\t\t\tresult = str(os.lookuptype(path))\n\t\telse:\n\t\t\tresult = str(os.curdir)\n\t\treturn result\n\t\n\t# NOTE: This doesn't seem like a good idea to try to do this...\n\tif path == \"root\":\n\t\ttype1 = os.path.typeof(os.curdir)\t# Namut\n\telif file_name == \""}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif self.bug_id_map is not None \\\n\t\t\tand self.bug_id_map in bug_ids:\n\t\t\t# we didnt hit here, just return and delete the old values\n\t\t\treturn\n\t\tparams, bug_ids = self.params_mapping_for_bug_mapping(params)\n\t\tbug_ids = bug_ids[0:1]\n\n\t\tbug_ids += params.values()\n\n\t\tif len(bug_ids) == 0:\n\t\t\treturn\n\n\t\tbug_id = bug_ids[0]\n\n\t\tif bug_id not in"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself.check_bz_params(params)\n\n\t\tbug = self.bugs[bug_ids[0]]\n\t\tbug['name'] = params['name']\n\t\tbug['url'] = params['url']\n\t\tbug['date'] = params['date']\n\t\tbug['owner'] = params['owner']\n\t\tbug['email'] = params['email']\n\t\tbug['status'] = params['status']\n\t\treturn bug\n\n    def amend_committer(self, params, bug_ids):\t\"\"\"\n\tAmend the Committer params\t\"\"\"\n\t\tself.check"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\t_bzparams = []\n\t\t\n\t\tfor bug_id in bug_ids:\n\t\t\tbz_url = \"https://bugzilla.mozilla.org/%d\" % bug_id\n\t\t\t_bzparams.append((bz_url, params))\n\t\t\n\t\tself._http.post(_bzparams)\n\nclass Bugzilla(WebBrowser):\n\t\"\"\"\n\tBugzilla is used for testing/testing, testing/testing/testing/testing/...\n\t\"\"\"\n\tdef __init__(self, server=WEB_SERVER, user=None):\n\t\tWebBrowser.__init__(self, server, user"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tbug_ids = json.dumps(bug_ids)\n\t\ttry:\n\t\t\tbug_keys = json.loads(self.bz_params.bz_params)\n\t\texcept ValueError as e:\n\t\t\traise BugzillaError('unable to parse Bzr params {}'.format(e))\n\t\t\n\t\tnew_params = []\n\t\tfor bug_id in bug_keys:\n\t\t\tif not bug_id in bug_keys:\n\t\t\t\tcontinue\n\t\t\tnew_params.append({'id': bug_id, 'bug_id': bug_id})\n\t\t# Bug_num => num: ("}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself.bugzilla = get_bugzillaurl(params['name'], self.bugzilla_user, self.bugzilla_pass, self.bugzilla_hostname)\n\t\treturn True\n\n\tdef release(self, bug_id):\t\n\t\t\"\"\"\tRelease the bug id\t\t\n\t\t\"\"\"\n\t\tif self.bugzilla: #check\n\t\t\tself.bugzilla = get_bugzillaurl(self.bugzilla, self.bugzilla_user, self.bugzilla_pass, self.bugzilla_hostname)\n\t\telse: #check\n\t\t\tself.bugzilla = get_bugzillaurl(self.bugzilla, self.bugzilla_"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself.params = params\n\t\tself.bugs = bugs\n\t\tself.bz = Bugzilla(self.params[\"bzurl\"])\n\t\tself.bids = [b.id for b in bug_ids]\n\n\t# Ability\t\n\tdef is_required(self):\n\t\t\"\"\"\n\t\tChecks if the bug is required, returns False if the bug is set to be required.\n\t\t\"\"\"\n\t\treturn self.bugs.required == True\n\n\tdef __str__(self):\n\t\treturn (\"%s (%d)\" % (self.name, self.bugs.total))\n\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself._update_bug_ids(params)\n\t\t\n\t\tif bugs_in_archive(self.env):\n\t\t\tBugTicket.abort(self, bug_ids)\n\n\tdef amend_bug_ids(self, params):\n\t\t\"\"\"\n\t\tBorrow bug ids from the bugzilla server\n\t\t\"\"\"\n\t\tself._update_bug_ids(params)\n\n\tdef _update_attachment_list(self, params):\n\t\tself.mails_pool.unlink(self._env.get_mail_db_name())\n\n\tdef __repr__(self):\n\t\treturn"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tcmd = 'abiflags -a'\n\t\t# Ambiguous parameters\n\t\tif bug_ids:\n\t\t\tcmd += '/' + ','.join(bug_ids)\n\t\tcmd +='' + ','.join(params)\n\t\tself.log(cmd)\n\t\tself.execute(cmd)\n\n\tdef run_build(self, args):\n\t\tcmd = 'build'\n\t\tself.log(cmd)\n\t\tif not args.dry_run:\n\t\t\traise BadHost()\n\n\t\ttry:\n\t\t\tself.execute(cmd)\n\t\tfinally:\n\t\t\tself."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\treturn_codes = 0\n\t\tif len(bug_ids) == 0:\n\t\t\treturn return_codes\t\n\t\tnew_bzparams = params.split(',') # Split into comma seperated\n\t\tnew_bzparams.append(self.get_bzparams(bug_ids)) # Add to new list if necessary\n\t\tif new_bzparams == []:\n\t\t\t# Nothing to do so just return\n\t\t\treturn return_codes\n\t\telse: \n\t\t\t# Append the params so it can be easily removed later\n\t\t\tnew_params = ','.join(new_bzparams)\n\t\t\treturn"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tfor bug_id in bug_ids:\n\t\t\tparams['bug_id'] = bug_id\n\t\t\tparams['bug_user_agent'] = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:16.0) Gecko/20100101 Firefox/16.0'\n\t\tself.update_custom_parameters(params)\n\t\t\n\tdef get_bugzilla_bug_urls(self):\n\t\tbug_urls = []\n\t\tfor bug_id in self.get_bug_ids():\n\t\t\tbug_urls.append('https://bugzilla.mozilla."}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t        )\n\n    def parse(self):\n        result = []\n        for node in self.blocks.itervalues():\n            if node.start_tag == 'tag:yaml.org,2002:str':\n                try:\n                    yaml.safe_exec(self.read_scalar_scalar(node.value))\n                except Exception as e:\n                    # If there's a problem running this, we'll skip it\n                    if e.message.startswith('Error while trying to unflatten borgmatic config file'):\n                        e.message = 'Error while trying to expand borgmatic config file\\n\\n%s\\n\\"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t )\n\n##############################################################################\n# Example\n\"\"\"\r\nCreated on Aug 14, 2015\r\n\r\n@author: Duncan\r\n\r\nfrom nqtk.model import Model\r\nfrom nqtk.model.parameters import ParameterDB\r\nfrom nqtk.model.parameters.model import ParameterGroupDB\r\nfrom threading import Lock\r\n\r\nclass Parameter(Model):\r\n    \r\n    class Field:\r\n        \"\"\"An object that stores information about a particular parameter.\r\n        \r\n        Attributes:\r\n            name : string\r\n                Name of param.\r\n            description : string\r\n                Description of param.\r\n            data_type : str\r\n                Data type of param.\r\n           "}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t).resolve()\n\t\tif not isinstance(nodes, (list, tuple)):\n\t\t\tnodes = [nodes]\n\t\treturn recursive_merge_nodes(nodes)\n\n\n\tdef _merge_nodes(nodes):\t\"\"\"\n\tMerge the node values from a nested borgmatic configuration data structure into a single copy, using the deep merge operator\n\n\tWhen the resulting data structure is single, a deep merge is performed using the deep merge operator (and a\n\tnested merged configuration file). This enables the nested merging for those nodes in the resulting data structure.\n\t\"\"\"\n\t\trecursive_merge_nodes(nodes"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\n# =============================================================================\n# \nfrom ruamel.yaml.compat import _unicode, map, map_lazy\n\n_yaml_node_re = (\n\t('^([^:]+)(?<![A-Z][A-Z0-9]{0,61}[.:]?)+(.*)\\?)?$').match,\n\t(r'\\{([^\\[]+)\\}',),\n)\n_yaml_constructor_re = re.compile(r\"^{([^}]+)\\}$\")\n\ndef node_value_to_default(node, default):\n\tif isinstance(node,"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t)\n\n\n# FIXME: This function is used by the merge module that uses the borgmatic config tool, that\n# could also be a lot more influenced by merging the configs in an earlier version of this module\n# and then to simplify these code:\n# \t[\n# \t\t(\n# \t\t\tconfig1,\n# \t\t\tconfig2,\n# \t\t),\n# \t\t(\n# \t\t\tconfig1,\n# \t\t\tconfig2,\n# \t\t),\n# \t\t(\n# \t\t\tconfig1,\n#"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\t(\n\t\t\n.. _documentation:\n\nA more complex configuration format used, or a similar format for some other configuration\nthat may not be used by other modules.\t\n\n.. note:: There is a circular mapping problem between this type of type and the mapping file. It is assumed the mapping\nfile itself and each individual node is named a set of nodes, and each node is named multiple\nnodes.\n\n\n.. _uapi: https://github.com/ruamel/r_configuration-format/blob/master/r_configuration_file.py\n\nUraF's built"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\t(\"yaml.org,2002:str\", None),\n\t(\"yaml.org,2002:str\", \"keep_hourly\"),\n]\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n#!/usr/bin/env python3\n# -*-mode:python; indent-tabs-mode:nil; tab-width:4 -*-\n#\n# Copyright (C) 2016-2017 Canonical Ltd\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t)\n\n\ndef find_yaml_nodes(*nodes):\t# pragma: no cover\n\t\"\"\"\n\tReturns an iterator of yaml nodes.\n\tIf there is a match between the two, return a single node that is the best\n\tmatch.\n\tAny node of this type should be added.\n\t(Note that this may occur when the YAML parser only allows one node to have a\n\tmatch between one or more of them)\n\t\"\"\"\n\twith_matched = []\n\twith_matched.extend(nodes)\n\tfor e in with_matched:\n\t\ttry:\n\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t )\n\treturn merge_nodes(deep_nodes(nodes))\n\t\n#############################################################################################\n#############################\n#\n#   Main functions                                               #\n#\n#############################################################################################\n\ndef set_default_color(color_type, value):\n\t\"\"\"Sets the default color value.\n\n\tThe default color will be set to the `WHITE` value of any combination of values:\n\n\t[u'green', u'cyan', u'white']\n\t\"\"\"\n\tif color_type == u'BLACK':\n\t\treturn _color_set_default(value), None\n\telif color"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\ndef deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        borgmatic configuration as a key,\n        borgmatic configuration data as a value,\n    ),\n\n.. note:: Do not modify the configuration in the current config:\n\t\t* It is recommended this behavior only works for key, value pairs which might change during runtime.\n\t* To maintain this, make the API readable for a single key and value pair (e.g., a tuple\n\tcontaining a key, a value"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(*arguments)\n\t\targuments = parser.parse_known_args()\n\t\t\t\n\t\t# create argument parser instance and write its help\n\t\tparser.formatter_class = Parser\n\t\tcommandline = parser.format_help()\n\t\targuments = arguments[:]\n\t\t\n\t\tcommandline_parsed = ArgumentParser(*commandline) + arguments\n\t\tcommandline_parsed.formatter_class = ArgumentFormatter\n\t\treturn commandline_parsed\n\n\t\ndef format_help(*commands):\n\t\"\"\"\n\tReturn a help string for the given list of commands\n\t\"\"\"\n\t\tfor command in commands:\n\t\t\t\n\t\t\t\t\t\n\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(sys.argv[1:])\n\t\tparser.add_argument('--force', action=\"store_const\", const=True, dest=\"force\",\n\t\t\t\t\t\tdefault=True, dest=\"force_yes\",\n\t\t\t\t\t\thelp=\"Whether to force a confirmation asking user for confirmation \")\n\t\tparser.add_argument('--no-input', action=\"store_true\", dest=\"use_input\",\n\t\t\t\t\t\tdefault=False, dest=\"use_input\",\n\t\t\t\t\t\thelp=\"Do not ask user for input files (use @force to ask)\")\n\t\tparser.add_argument('--input', action=\""}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser('<arguments>',\n\t)\n\t\targs = parser.parse_args([str(x) for x in arguments[1:]])\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\treturn parser\n\t\n\nif __name__ == \"__main__\":\n\tmain()\n\n\n# vim: expandtab sw=4:\n\n#\n# Copyright 2012,2013 Free Software Foundation, Inc.\n#\n# This file is part of GNU Radio\n#\n# SPDX-License-Identifier: GPL-3.0-or-later\n#\n\nfrom __future__ import absolute"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\toption_list = []\n\t\toptionals = ArgumentParser(\"usage: %prog [options] [COMMANDLINE]\")\n\t\toptionals.add_argument(\"-m\", \"--mode\",\n\t\t\t\t\t\tdefault = \"run\",\n\t\t\t\t\t\tchoices  = [\"run\", \"parallel\", \"parallel_run\", \"test\"],\n\t\t\t\t\t\thelp = \"mode: run | parallel | parallel_run | test\")\n\n\t\tself.known_args = commandline_args(self, argument_list)\n\t\tself.known_options = commandline_options(self, optionals)\n\t\tself.known_options.add_argument('-"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\targuments['script'] = os.path.basename(sys.argv[0])\n\t\tsys.argv.pop(0) # remove argv[0]\n\t\treturn args\n\n\tdef __str__(self):\n\t\treturn str(self.__dict__)\n\n\ndef run():\n\t\"\"\"\n\tStart the command-line as a command-line processor, run it, and exit.\n\t\"\"\"\n\targs = parse_arguments()\n\tcommand_line = args.command\n\n\t# Create a new instance of the command line processor\n\tp = ArgumentParser(\n\t\tdescription=open(command"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser()\n\n\t\tparser.add_argument('source', type=str, nargs=1, help='input FASTA file or GTT')\n\t\tparser.add_argument('-c', '--config', dest='config_file', help='input configuration file')\n\t\tparser.add_argument('-o', '--output', dest='output', help='output filename')\n\t\tparser.add_argument('-n', '--nthreads', dest='threads', action=int, help='number of threads')\n\t\tparser.add_argument('-d', '--debug', dest='debug', action=\""}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument('--config', help=\"File containing config file containing help to configure and configure command line arguments\")\n\t\tparser.add_argument('--project', help=\"Project code of this script\")\n\t\targs = parser.parse_args(args=arguments)\n\t\t\t\t\n\t\treturn parser\n\tdef get_project(self, name):\t\t\t\n\t\toutput_dir = os.getcwd()\n\t\twith cd(output_dir):\n\t\t\treturn name.split('-')\n\tdef get_config(self, name):\n\t\tproject_code = self.get_project"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(usage=SUPPRESS_USAGE,\n\t\t\tdescription=SUPPRESS_HELP)\t# default help message\n\t\tparser.add_argument('-H', \"--help\", action=\"store_true\",\n\t\t\thelp=\"show this help information\")\t\t\t\t# show this help information\n\t\treturn parser.parse_known_args(argument_strings=args)\n\n# ===============================\n#   CLI Command line parsing functions\n# ===============================\n\ndef get_default_parser_list(default_version=None):\n\t\"\"\"Return the current parser list.\"\"\"\n\tversion_parser = ParserSet._"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(prog=os.path.basename(sys.argv[0]), usage=\"pyscript\")\n\tparser.add_argument('-p', '--p', default='-', help=\"path to the parameter file\")\n\tparser.add_argument('-l', '--li', action='store_true', help='show the help messages about the parser')\n\tparser.add_argument('-i', '--input-file', default='input_file.pickle',\n\thelp=\"file to input from\")\n\tparser.add_argument('-M', '--modified', action='store_true',"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\ttry:\n\t\t\tparser, args = parser_from_argv(args)\n\t\texcept:\n\t\t\tprint\n\t\t\tprint \"invalid arguments, usage\"\n\t\t\treturn parser\n\n\tdef create_parser(self, usage=None) :\t\"\"\"\n\tCreates and returns a parser object.\n\tArguments are:\n\t\tusage  - Print this usage information.\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog=self.get_progoros())\n\t\tif usage == None:\n\t\t\tparser.print_help()\n\t\telse:\n\t\t\treturn parser\n\n\tdef parse_arguments_"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tif parser.option_strings:\n\t\t\topts = parser.option_strings\n\t\telse:\n\t\t\topts = []\n\t\t\n\t\t# Return an empty space separated string\n\t\treturn '(' + ''.join(opts) + ')'\n\t\n\tdef options_to_flags(self):\n\t\t\"\"\"\n\t\tCreates the flags into a comma-separated string of the options.\n\t\t\"\"\"\n\t\treturn''.join(self.options)\n\t\n\tdef get_help(self):\n\t\treturn \"usage: %prog [OPTIONS] [ARGS]\"\n\t\n\tdef create_subparser(self, prog):\n\t\tparser"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\t# get the args\n\t\targ_fields = parser.add_argument_group(\"ARG Flags\")\n\t\targ_fields.add_argument(\"-f\", type=str, dest=\"flags\", default=[], help=\"One or more of the flags to set. Example: '-m my option'\", metavar=\"FLAGS\");\n\t\treturn arg_fields\n\t\n\t# parse any command line arguments, which can be passed to parser\n\targuments = parser.parse_args()\n\t# if no flag is specified, return all of them\n\tif arguments.flags:\n\t\treturn arguments.flags\n\t\t\nif __name"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tglobal _flagStrings\n\treturn \" \".join(parser.option_strings)\n\ndef process(options):\n\t\"\"\"\n\tProcess options.\n\t\"\"\"\n\tif not options.all:\n\t\toptions.parser.print_help()\n\t\treturn\n\n\tdebug = options.debug\n\tverbose  = options.verbose\n\n\tif debugging:\n\t\tos.environ['DEBUG'] = \"True\"  # enable debugging output\n\n\ttry:\n\t\tsettings = ConfigurationFile(options.file)\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tfor key in settings.items():\n\t\t\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser_key = (parser.__class__ == ParserOptions) # pylint: disable=protected-access\n\t\tflags = set()\n\t\tfor f in parser_key.flags:\n\t\t\tif f in parser_flags:\n\t\t\t\tflags.add(f)\n\t\tif not flags:\n\t\t\tparser.error(\"invalid flag\")\n\t\treturn parser.get_opt_string().split()\n\treturn parser_flags\n\nclass ArgumentHelper:\n\tdef __init__(self, options, subparsers, subparsers_flags, args=None):\n\t\tself._options = options\n\t\tself._"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tresult =''.join([option.dest for option in parser.option_list])\n\t\treturn''.join(['{}={}'.format(key, value) for (key, value) in result.split()])\n\t\t\n\ndef parse(f_in):\n\t\"\"\"\n\tParse a string. \n\t\"\"\"\n\topts = parser_flags(parser)\n\tparser.parse_args(f_in, opts.values())'''\nCreated on Jan 18, 2011\n\n@author: michal\n'''\n\n\nfrom aquilon_agent.aquilon_agent"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = ['debug', 'profile']\n\t\tif isinstance(parser_flags, str):\t\tparser_flags = parse(parser_flags)\n\t\treturn parser_flags\n\n\ndef get_parser(flag):\t\"\"\"\n\tGiven a flag or a string, return that argument parser.\n\t\"\"\"\n\t\tif isinstance(flag, str):\t\treturn _get_parser_from_flag(flag)\n\t\telif isinstance(flag, (str, unicode)):\t\treturn _get_parser_from_char(flag)\n\t\telif flag and (flag.isalnum() or flag"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('-d', '--debug', action='store_true')\n## \t ####### ####### #### ### ########### ########### ########### ##########\n\t ####### ####### ### ##\n\t ####### ####### #### #\n\t ####### ###### ## # # # # ###########\n\t\n\n\t\tparser.add_argument('-C', '--c', action='store_true',  help='use default c-style'\n                                                                     'text-file instead of file')\n\t\tparser.add_argument('-o', '--output', action='store_true',  help='output C' \n                                                "}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('-t', '--text', dest='text', action='store_true', help='Output the output only to the terminal')\n\tparser.add_argument('-g', '--graph', dest='graph', action='store', help='Print the edges graph (with optional --graph)')\n\tparser.add_argument('-f', '--foreground', dest='foreground', action='store_true', help='Include foreground lines')\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='Output the graph graph (showing the graph"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tif hasattr(parser, 'option_groups'):\n\t\t\treturn parser.option_groups\n\t\tparts = parser.option_groups.split()\n\t\tflags_group = parts.pop()\n\t\tfirst = True\n\t\twhile parts:\n\t\t\tflag, group = parts.pop(0)\n\t\t\tif flag == '--debug' and not first:\n\t\t\t\tgroup = 'debug'\n\t\t\tif flag == '--verbose' and len(parts) > 1:\n\t\t\t\tflags_group ='verbose'\n\t\t\tif group == 'debug':\n\t\t\t\tfirst = False\n\t\t\t\tdebug_flag = False\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('-A','--arg', dest='arg')\n\t\tparser.add_argument('-C','--code', dest='code')\n\t\tparser.add_argument('-D','--description', dest='description')\n\t\tparser.add_argument('-D','--docid', dest='docid', help='docid to include.')\n\t\tparser.add_argument('-F','--file', dest='file')\n\t\treturn parser.parse_args(args)\n\nclass Main(object):\n\tdef __init__(self,args):\t\t\n\t\tself.parser = argparse.ArgumentParser()\n\t"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tsubparser_namespace = __import__(\"\",\"builtins\").SubParserNS()\t\n\t\tsubparser = subparser_namespace.subparser_func\t\n\t\tparser = subparser(subparsers[0])\n\t\treturn parser.parse_known_args(*unparsed_arguments)\n\t\t\n\nif __name__ == \"__main__\":\n\t\n\tparser = argparse.ArgumentParser(description=\"A Python script to test against the pygtk package and \"\n\t\t\t\t\t\t\t\t\t\t\t\"Python bindings. It can be used on various platforms to use this\")\n\t\t\n\tparser.add_argument(\"target_dir\", default=\"/usr/local"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t# Only one is needed to allow subparser names to be used.\n\t\tassert not unparsed_arguments, \\\n\t\t            '--subparser must not be empty'\n\t\tunparsed_arguments.pop(0)\n\t\tcommand_line = argparse.ArgumentParser().parse_known_args(*unparsed_arguments)\n\n\t\t# Parse global options:\n\t\toptions, global_options = command_line.parse_known_args()\n\n\t\t# Ensure global options are accessible only once\n\n\t\t# Setup dictionary of global options.\n\t\tglobal_options['global_options'] = global_options\n\t\tfor"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tglobal subparsers, parsers\n\t\n\t\tparsed_arguments = {}\n\t\tself.subparser_names = subparser_names\n\t\tself.parsers = parsers\n\t\n\t\tif unparsed_arguments:\n\t\t\traise ArgumentError(\"Unrecognized arguments: %s\" % str(unparsed_arguments))\n\n\t\t\n\t\t# Load the parser options\n\t\tparsed_args = []\n\t\tfor arg, value in unparsed_arguments:\n\t\t\tparsed_args.append((arg.lower(), value))\n\n\t\t# Set up parser for options.\n\t\tparser = argparse.ArgumentParser(\r\n\t\t\tdescription=self.__class__.__name__,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tfor namespace in unparsed_arguments:\n\t\t\ttry:\n\t\t\t\tsubparser = namespace._subcommands\n\t\t\t\treturn subparser\n\t\t\texcept Exception, e:\n\t\t\t\tprint e\n\t\t\t\treturn {}\n\n\tdef get_args(self):\n\t\t\"\"\"\n\t\tIf invoked with an empty namespace all the arguments that was supplied are used to\n\t\tparse.\n\t\t\"\"\"\n\t\tsubparser_list = list(self.get_submenu_names())\n\t\targ_list = list()\n\t\tfor name, action in self.argument_parser.parse_known_subcommands(subparser_list"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"args\", nargs='?', type=argparse.FileType('r'),\n\t\t\thelp = \"Arguments to parse\")\n\t\tparser.add_argument(\"-v\", \"--verbose\",\n\t\t\taction = \"store_true\", default = False,\n\t\t\t help = \"Print the CLI debug info\")\n\t\tparser.add_argument(\"--build-dir\",\n\t\t\tdest = \"builddir\", default = \"build\",\n\t\t\thelp = \"Directory to build the new build\")\n\t\tparser.add_argument(\"config\", nargs = '+',\n\t\t\thelp ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targuments = {}\n\t\tparser = ArgumentParser(description=scriptdescription, parents=[], fromdir=fromdir)\n\n\t\tfor parsed_arg in unparsed_arguments:\n\t\t\targ = getattr(parser.parse_args(), str(parsed_arg), None)\n\t\t\tif not arg:\n\t\t\t\traise ValueError(\"The given arg %r doesn't exist in module %s\" % (-parsed_arg, scriptdescription))\n\n\t\t\tname = arg.name\n\n\t\t\t# Handle arguments that start with dashes: if there's no dashes, an option\n\t\t\t# parsing will be interrupted if at least one argument is given to"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.parse_known_inputs()\n\n\t\targ_counts = {}\n\n\t\tfor o, a in unparsed_arguments:\n\t\t\targ_counts[o] = cmd_arg_type(a)\n\n\t\tfor a in arg_counts:\n\t\t\targ_counts[a] = a\n\n\t\tif arg_counts:\n\t\t\treturn dict([(a, cmd_arg_type(a)) for a in arg_counts.keys()])\n\t\telse:\n\t\t\treturn None\n\n\tdef main(self, arg_parser=None):\n\t\tif arg"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs, kwargs = {}, collections.defaultdict(list)\n\n\t\tfor arg in unparsed_arguments:\n\t\t\tif \"=\" in arg:\n\t\t\t\targ, subparser = arg.split(\"=\")\n\t\t\t\tif subcommand == \"global\":\n\t\t\t\t\targs[subparser.split(\".\")[0]] = Global()\n\t\t\t\telse:\n\t\t\t\t\tparser = argparse.ArgumentParser(**subparser.split(\"=\"))\n\t\t\t\t\tparser.add_argument(arg, action=argparse.SUPPRESS)\n\t\t\t\t\targs[subparser.split(\".\")[0]] = args[subparser.split(\".\")[0]].parse_args(subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\ndef parse_command_line(*arguments):\n\tparsed_arguments = args_to_str(arguments)\n\t\n\tparsed_name = \"\"\t\n\tparsed_args = {}\n\tfor arg in parsed_arguments[1:]:\n\t\tif \"=\" in arg:\n\t\t\tparsed_params = arg.split(\"=\", 1)\n\t\t\tif len(parsed_params) > 0 and args[0] == \"global\":\n\t\t\t\tname, value = parsed_params[0], parsed_params[1]\n\n\n\t\t\t\t# NOTE 2: \"global\" is a namespace, which is a global name\n\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tfor arg in unparsed_arguments:\n\t\t\tsubparser = argparse.ArgumentParser(parents = [arg])\n\t\t\tsubparser.parse_known_args(True)\n\t\treturn dict(subparser=subparser)\n\ndef parse_cmdline(argv):\t# used for the test script to find sys.argv\n\t# parse all subcommands and execute each subcommand\n\tsubparser = subcommands.get_subparsers(globals())\n\tfor command in subparser.generate_all():\n\t\tcommand.func(argv)\n\ndef main(argv_str):\n\t# process the"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\treturn (tuple(sorted(set(subparser_name for subparser_name, args in sorted(unparsed_arguments))))\n\t\t\tfor subparser_name, args in sorted(args_from_subparser.items()))\n\tdef construct_parser_arguments(self, arg_list):\n\t\treturn self.parser.parse_args(arg_list)\ndef parse_repo_args(arg_list):\n\treturn self.parser.parse_args(arg_list)\ndef parse_repo_subparser_arguments(subparser_name, args):\n\tsubparser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tsubparsers = [(name, getattr(subparser, name, [])) for name in parsers_to_subparser_names]\n\t\targuments = []\n\t\tfor name, kwargs in subparsers:\n\t\t\tif subparser_name_matches(kwargs['__name__'], kwargs['name__']):\n\t\t\t\tlogger.debug('Action parser %r matches known subparser action %r, and the argument %r', name, kwargs, subparser_name_matches(kwargs['__name__']))\n\t\t\t\targuments.append(kwargs)\n\t\t\telse:\n\t\t\t\targuments.append(kwargs)\n\t\treturn (arguments,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\targs_list = [(subcommand, subparser) for subcommand, subparser in zip(unparsed_arguments, subparsers)]\n\t\targs_dict = dict([(key, []) for key in args_list])\n\t\ttry:\n\t\t\tresult = parser.parse_known_args(args_dict)\n\t\texcept SystemExit:\n\t\t\tprint(\"Exiting with code: %r\" % (sys.exc_info()[1]), file=sys.stderr) # pragma: missing cover\n\t\t\tsys.exit(1)\n\t\treturn result\nfrom pyvisdk.exceptions import InvalidArgumentError\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n# return (argparse, list of subparser)\n\t\n\ndef validate_argument_dict(parser_arguments):\n\t\"\"\"\n\tGiven a sequence of arguments to a dict from subparser name to argparse.ArgumentParser\n\tinstance, validate each argument's argument is a dict representing its valid arguments.\n\n\tReturn the result as a tuple of (a dict mapping from subparser name to any parsed argument for each \targument that is understood by any \n\tsubparser.)\t\"\"\"\n\t\n\tfor argument, subparser in parser_arguments.items():\n\t\tif argument.startswith('--repository'):\n\t\t\t#"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tresult = defaultdict(list)\n\t\tfor i, argument in enumerate(argparse.Namespace().defaults.values()):\n\t\t\tresult[argument.name].append(Subparser(argument).parse_args(unparsed_arguments[i:]))\n\n\t\tfor name, values in result.items():\n\t\t\tfor value in list(values)[:-1]:\n\t\t\t\tif len(value.strip()) > 0:\n\t\t\t\t\tvalues += [argument for argument in argument.defaults if value]\n\n\treturn unparsed_arguments, subparsers\n\ndef parse_subparser_arguments(args, subparsers):\n\tif isinstance("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\timport argparse, subprocess\n\timport os\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-c\", \"--commit\", dest = \"commit\", nargs=1,\n\t\t\t\t\t\taction ='store_true')\n\tparser.add_argument(\"--version\", dest = \"version\",\n\t\t\t\t\t\taction ='store_true')\n\tsubparser_args = parser.parse_args(unparsed_arguments)\n\tsubparser = {}\n\tfor subparser_name in parser.subparsers[0].commands.keys():\n\t\tsubparser[subparser_name] = parser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tsubparsers = {name: arg for arg in subparsers if has_arg(argument) for name in arg_names}\n\t\tresult = [subparser(**subparser_arguments) for subparser_arguments in subparsers]\n\t\tsubparser_names = [name for name in arg_names if has_arg(argument)]\n\n\t\treturn result, subparser_names\n\t\t\n\tdef _parse_argument(self, args, parser_name, option_parser_func):\n\t\tsubparser_arguments = []\n\t\tfor argument in args:\n\t\t\targ_result = parser_name +'' +"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\nclass TestNamespace(object):\n\t\"\"\"A namespace whose parsed arguments are namespace, plus some extra stuff\"\"\"\n\t\n\tdef __init__(self, args):\n\t\tself.args = args\n\t\tself.subparsers = dict((x,y) for x,y in subparser_parser.walk(args))\n\t\n\tdef add_options(self, parser):\n\t\tparsed = parser.parse_args(self.args)\n\t\tfor y, subparser in self.subparsers.iteritems():\n\t\t\tif y.lstrip('-'):\n\t\t\t\tsubparser.add_argument('-'+y, '--"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\targs = []\n\t\tparsed = subparsers[1][0]\n\t\tfor argument_name in arguments:\n\t\t\tsub_argument_args = {\n\t\t\t\t# arg_name: parsed arg's value, so we can use \"args\" from it instead of the default value\n\t\t\t\t(arg_name, 'default'): argparse.get_default(arg_name)\n\t\t\t}\n\n\t\t\tsub_argument_args.update(sub_argument_defaults.get(arg_name, {}))\n\n\t\t\targs.extend(sub_argument_args)\n\n\t\treturn tuple(args)\n\n\tdef print_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tgrouper.add_argument(\n\t\t\t\"parser\",\n\t\t\t# help=\"the parser that subparser was given to\")\n\t\tgrouper.add_argument(\"subparser\",\n\t\t\t# help=\"\"\"the subparser that was given to\"\"\")\n\t\t\n\t\toptional = list(set(unparsed_arguments).intersection(set([\"repository\", \"repository_arg\", \"repository_subparser\", \"argument\", \"arguments\"]))\n\t\tfor a in required_subparser_args.get(\"--help\") if optional else []):\n\t\t\tprint(a)\n\t\t\n\t\tif len(required_subparser_args.get"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Find the location of a new document.\")\n\n\tparser.add_argument('-f','--file', required=True, help=\"the path to an existing config file\")\n\tparser.add_argument('-o','--output-file', required=True, help=\"the path to a new output file\")\n\n\tparser.add_argument('-v','--verbose', action=\"store_true\", help=\"Verbose\")\n\tparser.add_argument('-x','--extract', action='store_true', help=\"Force extract of document from a document folder\")\n\tparser.add_"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = subparsers.add_parser('run', parents=[subparsers.one_arg_default for subparsers in [subparser.create_parser()] if not subparser.parse_args().config_file])\n\t\tparser.set_defaults(command='run')\n\n\t\tparser.add_argument('--server', help='Server', type=str, default='0.0.0.0')\n\t\tparser.add_argument('--verbose', help='Turn on verbose', action=\"store_true\")\n\t\tparser.add_argument('--port', help='''Server port: port range or integer port"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = subparsers.add_parser(\"add\",\n\t\t\thelp=\"Add a new entry for the given name or ID or display name (or a description). (See `add_arguments()` for details).\")\n\t\tparser.add_argument(\"-t\", \"--title\", action=\"store\", dest=\"title\", required=True)\n\t\tparser.add_argument(\"-d\", \"--description\", action=\"store\", dest=\"description\", required=True)\n\t\tparser.add_argument(\"-b\", \"--browser\", action=\"store\", dest=\"browser_version\", required=True)\n\t\tparser.add_argument(\"-"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = make_parser()\n\n\tdef help_for(parser, args):\n\t\t\"\"\"\n\t\tShow help about the named parser.\n\t\t\"\"\"\n\t\tparser.print_help()\n\t\traise SystemExit\n\n\tparser.add_argument(\n\t\t'--version', action='version',\n\t\tversion='0.1',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\thelp='Print version and date information.',\n\t\tdescription=description,\n\t)\n\n\tparser.add_argument(\n\t\t'--logfilename',\n\t\thelp='log to file for logs. Note that files"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t    subparsers = parser.add_subparsers(dest='parser', description=_('subcommands'))\n\t\tparser.add_argument('--make', help=_('make source for source files'), type=str)\n\t\tparser.add_argument('--build', help=_('build source'), type=str)\n\tsubcmds = parser.add_subparsers(dest='subcmd', help=_('subcommands'))\n\tparser.add_argument('source', help=_('source'), action='store')\n\toptargs = parser.parse_args()\n\tparseargs(subcmds, optargs)\n\n\tparser = make_"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Extract information of a Python module into a tree.',\n\t\t\tepilog=epilog)\n\t\tadd_argument('--verbose', '-v', action='store_true',\n\t\t\thelp='Enable verbose logging.')\n\t\tsubparser = parser.add_subparsers(dest='command', metavar='COMMAND',\n\t\t\t\t\t\t\t\thelp='command to run, defaults to the current python module')\n\t\treturn parser.__parser__, subparser\n\n\tdef help(self, message):\n\t\thelp_info = '\\n'.join([\n\t\t\t\t\t\t\t_('extract_module - extract module from '\n\t\t\t\t\t\t\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tglobal add\n#\tprint >> sys.stderr, __name__ + ','+ __file__\n#\tparser_name = 'test-basic-parser'\n#\tsubparsers = parser.add_subparsers(title='test', help='list all parsers')\n\t#parser = subparsers.add_parser('test', help='run basic parsers.')\n#\t#subparsers.add_parser('test_show', help='show all parsers.')\n\t#subparsers.add_parser('test_get_active_players', help='get active players for player')\n\t#"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ttopparser = subparsers.add_parser(description=\"A tool for running a simple Sonos script.\",\tdescription=\"A tool for running a simple Sonos script.\",\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t\ttopparser.add_argument(\"infile\", help=\"The input filename to get the script from.\",\tnargs=1)\n\t\ttopparser.add_argument(\"-o\", \"--output\", help=\"The base directory to write the results to.\",\tdest=\"filename\",\t\t\tnargs='w')\n\t\ttopparser.set_defaults(func=run_script)\n\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog='%s init (iOS X) -i %s' % (\n\t\t\t__version__, _appname),\n\t\tdescription=\"\"\"\n\t\tMain application initialization function; initialize\n\t\ta top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\t)\n\n\t\tparser.add_argument(\"-h\", \"--host\", required=False, choices=HOST_CHECK_LIST)\n\t\tparser.add_argument(\"-P\", \"--port\", required=False, type=int,\n\t\tdefault=DEFAULT_PORT, help=\"The port on which the application"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = parser_factory.get_default_sub_parser(description=_(\"Run on MacOS\"))\n\t\tparser.add_argument(\"config\", help=_(\"The config file\"), metavar=\"FILE\", nargs=\"+\", type=str)\n\n\t\tparser.add_argument(\"-e\", \"--env\", help=_(\"The os.environ specification\"), dest=\"env\", default=[os.environ.get('MACOSX_DEPLOYMENT_TARGET', '')],\n\t\t\t\t\t\t   help=_(\"The location of the environment\"), type=str)\n\n\t\tparser.add_argument(\"-t\", \"--type\", \"--target\", action=\"append\", type=str"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\t]\n\n\ndef write_single_file(yaml_path, node):\n\t\"\"\"\n\tWrite a single node into a yaml file.\n\n\tReturns:\n \t\tNone\n\n\tRaises an AttributeError error if this is called on a class with a\n\tsubclass of Node. The original node (in a yaml) is then modified, and will overwrite\n\tthe name.\n\n\t:param yaml_path: the path you just created out of the file\n\t:type yaml_path str\n\t:param node: the name of the node we want to write to the file\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n\t\t(\n\t\t\tScalarNode(tag='tag:yaml.org,2002:str', \"keep_hourly\"),\n\t\t),\n\t\t(\n\t\t\tScalarNode(tag='tag:yaml.org,2002:map', \"keep_daily\"),\n\t\t),\n\t\t(\n\t\t\tScalarNode(tag='tag:yaml.org,2002:str', \"keep_hourly\"),\n\t\t),\n\t)\n# Author: Bastian Lemaitre\n# URL: http://code.google.com/p/sickbeard/\n#\n# This file is part"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t>>> deep_merge_nodes([\n\t\t((\n\t\t\t(\n\t\t\t\tScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t\t\t\tMappingNode(tag='tag:yaml.org,2002:map', value=[])\n\t\t\t),\n\t\t\t(\n\t\t\t\tScalarNode(tag='tag:yaml.org,2002:str', value='keep_all'),\n\t\t\t\tScalarNode(tag='tag:yaml.org,2002:int', value='32')\n\t\t\t)\n\t\t,),)\n\t\t])\n\t\t>>> deep_merge_nodes([\n\t\t((\n\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t       )\n\nimport os, datetime\n\nfrom pyborg.base import ConfigSection, DataSectionTemplate, BoolNode, BoolDefaultNode, \\\n                          IntNode, StrNode\nfrom pyborg.base.data_structure import DataStructureNode, MappingNode, \\\n                                    MappingTypeTypeNode, ListNode, IntNode, StrNode, \\\n                                    StrNode\nfrom pyborg.base.yaml_validator import YAMLValidator\nfrom pyborg.base.parser_options import get_default_parser_options\n\nfrom pygments.token import Punctuation, String, Comment\n\n\nRE_TAG ="}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t),\n    ]\n\nclass BorgmaticNode(object):\n\tdef __init__(self, tag, value):\n\t\tself.tag = tag\n\t\tself.value = value\n\t\tself.children = []\n\n\tdef __str__(self):\n\t\treturn \"%s: %s %s\" % (self.tag, self.value, self.children)\n\n\nclass ScalarNode(object):\n\tdef __init__(self, tag, value):\n\t\tself.tag = tag\n\t\tself.value = value\n\t\tself.children = []\n\n\tdef __str__(self"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef deep_merge_configs(configs):\n    \"\"\"\n    Given a nested borgmatic configuration with merge_configs and then merges from\n    the outer configuration, return a new Config object. If no configs are given for any of\n    the given configs return the appropriate type.\n    \"\"\"\n    # merge_configs maps between the nested borgmaticconfigs we want in each node and the corresponding\n    # configs we want in this single execution step.\n    merges = {}\n    # for each nested borgmaticconfig we add the merge_configs entry into the configs\n    for tag, value in configs.items():\n        merge_configs ="}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t        )from django.contrib.auth.decorators import login_required\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.utils.decorators import method_decorator\n\nfrom. import models as mommy\nfrom.forms import MoviesForm, MovieForm\nfrom.forms import VideoForm, MusicianForm, CongressForm, SofiaForm\nfrom.models import Movie\n\n\n@method_decorator(login_required)\ndef get_list_item(request, *args, **kwargs):\n    mommy.make_couch"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t)\n\ndef merge_yaml_config(r_yaml, out_yaml):\n\t\"\"\"\n\tGiven yaml.Reader or yaml.Writer class, returns an iterator of dictionaries\n\tthat apply the merging algorithm to the result specified by the specified yaml.Reader\n\tor yaml.Writer class.\n\n\tThe output from this method is returned once for all dictionaries specified as input\n\tmust be fully re-sccmented or merged into one call.\n\n\t.. _Merge Configuration: https://code.activestate.com/recipes/52271-merge-in-python\n\t\"\"\"\n\n\tr_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t      ),\n    ]\n\n\ndef get_config_by_type(yaml_type):\n\t\"\"\" Gets a Configuration object for the provided Type \"\"\"\n\tconfig = list()\n\tfor config_type in yaml_type.type.keys():\n\t\tif config_type not in config:\n\t\t\tconfig.append(config_type)\n\treturn config\n\ndef map_nodetype_to_scalar(node_type=None, scalar=None):\n\t\"\"\" Map node types to their scalar values. \"\"\"\n\tif node_type is not None and scalar is not None:\n\t\treturn"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t).get_resolved_configs()\n\"\"\"\nCommon utilities\n\"\"\"\nfrom collections import deque\nimport hashlib\nimport itertools\n\nimport numpy as np\n\n\n# TODO: This does not work to check. Maybe we could call _to_numpy somehow?\n# TODO: What about sparse matrices?\ndef _maybe_remove_zeros(array):\n    \"\"\" Remove nulls of zeros (equivalently (0, 0, 0)). \"\"\"\n    while len(array) == 0:\n        array.pop()\n\n    return array\n\n\ndef _maybe_add_zeros(array):\n    \"\"\" Add nulls of"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n#\tfor arg in arguments[1:]:\n#\t\t\tassert hasattr(arg, 'parser'), \\\n#\t\t\t\t\t'Arg %r not instance of %r' % (arg, type(arg))\n\t\n#\tif '-c' in arguments:\n#\t\t\tparser = ArgumentParser(\n\t\t\tcommand_line_arguments=arguments,\n\t\t\tadd_help_option=False)\n\t\n\t\n#\tparser.set_defaults(func=main)\n\n\t\n#\tparser = ArgumentParser(\n#\t\t\tstdout=sys.stdout,\n#\t\t\tfile=sys.stdout,\n#"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(\"usage: %prog [options] command file[s] files...\")\n\t\tparser.add_argument(\"-s\", \"--servers\", nargs=\"*\", help=\"Specify server [s] (comma separated)\")\n\t\tparser.add_argument(\"-m\", \"--mode\", nargs=\"*\", help=\"Specify mode [s]\", default=\"unix\")\n\t\tparser.add_argument(\"-b\", \"--bind\", nargs=\"*\", help=\"Specify bind address [s]\", default=\"0.0.0.0\")\n\t\tparser.add_argument(\"-c\", \"--compress\", nargs=\"*\", help=\"Enable compression"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\ndef main():\n\targs = docopt.docopt(__doc__).parse_args(sys.argv[1:])\n\timport webbrowser\n\n\tif args.help:\n\t\twebbrowser.command('help')\n\telse:\n\t\tcommand = args.command\n\treturn command, args\n\"\"\"A collection of functions and classes for use with the\n`pandas <http://pandas.pydata.org>`_.\n\"\"\"\nfrom __future__ import division\nfrom __future__ import print_function\nimport itertools\nfrom collections import OrderedDict\nimport fnmatch\nimport json\nimport re\nimport"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\t# We must parse the command line so we get an ArgumentParser for the actual command\n\t\tparser = ArgumentParser(*arguments)\n\n\t\tparser.add_argument('host', help=\"Host name to use when running tests\")\n\t\targs = parser.parse_args()\n\n\t\treturn parser\n\ndef get_tests_kwargs(suite_name, num_tests):\n\t\"\"\"\n\tGet keyword argument used in tests.py to extract keyword arguments from a command line\n\tlike sys.argv.\n\t\"\"\"\n\treturn {\n\t\t\t\"suite_name\": suite_name,\n\t\t\t\"num_tests"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument('args', nargs='+', type=string_types, default=sys.argv[1:])\n\t\targs = sorted(arguments[0:-1]) + arguments[1:]\n\t\targs = [sys.argv[0]] + args\n\t\treturn parser\n\n\ndef main(args=None):\n\t\"\"\"\n\tThis is the main function we run if invoked as a script. \n\n\tArgs will be passed to the script by the user via the argument list, \n\totherwise they are used.\n\t\n\tIf args == None, sys.argv"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(usage=USAGE)\n\t\t\n\t\tparser.add_argument('-m', '--mode', nargs='?', type=str, choices=['local-mode', 'local-files','remote-mode'], help='Remote mode (default: -s)')\n\t\tparser.add_argument('-c', '--clean', nargs='?', type=str, choices=['all', 'no', 'local-config'], help='Clean the specified repository or update everything')\n\t\tparser.add_argument('-v', '--verbose', '-v', action='store_true', help='Verbose mode')\n\n\t\tparser.parse_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tusage = \"usage: %prog [options] command\"\n\n\t\tparser = OptionParser(usage=usage)\n\t\tparser.add_option('-v','--verbose',dest='verbose',action='store_true',default=False,help='print more verbose information')\n\t\tparser.add_option('-f','--file',dest='file',default=None,help=\"the output file\")\n\t\tparser.add_option('-i','--input-file',dest='input_file',default=None,help=\"the input file\")\n\t\tparser.add_option('-c','--command',dest='command',"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(add_help=False, description=program['name'])\n\t\tparser.add_argument('-a', '--arguments', help=\"Arguments with which to execute them\", type=add_arguments)\n\t\targs = parser.parse_args(*arguments)   # Call the parser\n\t\treturn args\n\t\n\t\n\nclass ArgumentError(Exception): pass\nclass Program:\n\t\"\"\"\n\tA command-line program that gets its input and output locations when running.\n\nArguments:\n\tprogram -- The command-line command-line to execute.  Defaults to 'call'.\n\tinput --"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\treturn ArgumentParser(usage=program_description,\n\t\t\tdescription=__doc__, formatter_class=ArgumentDefaultsHelpFormatter)\n\treturn parse_arguments_module(argv, globals())# -*- coding: utf-8 -*-\n#\n\"\"\"\n=============================\nPlotter for the data/mask on a 3D dataset\n=============================\n\nThis application demonstrates how to map the mask image with a matplotlib\nfigure rather than using a matplotlib image. The plotting of\nthe three-dimensional mask image is a bit tricky so we need a small image. The main\nfunction for this program:\n\n\"\"\"\n\nimport"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser(description='Parse command line arguments.')\n\t\tfor opt in arguments:\n\t\t\topt = opt.strip()\n\t\t\tif opt:\n\t\t\t\tparser.add_argument(*opt.split())\n\n\treturn parser\n\n\ndef main():\n\t\"\"\"Main entry point, the command-line entry point.\"\"\"\n\tcommand_line = parse_arguments()\n\tapp.run()\n\"\"\"\n\tCopyright (C) 2013  Andrea Zach\n\tThis file is part of Blender.\n\n\tBlender is free software: you can redistribute it and/or modify\n\tit under"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tname    = parser.prog.name\n\t\tparser_name  = parser.prog.name.replace('-', '_')\n\n\toptionals = parser.option_strings[1:]\n\tflags  = ''\n\tfor opt in optionals:\n\t\tif opt in ['-h','--help']:\n\t\t\tusage(parser)\n\t\t\treturn None\n\n\t\telif opt in ['-f','--file']:\n\t\t\tfilename = opt\n\t\t\n\t\telif opt in ['-p','--parallel']:\n\t\t\tN = parser.count(' ');\n\t\t\tf = int(N*2);\n\t\t\tp = 0\n\t\t\twhile p+f"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('args', nargs='*', metavar='ARGUMENT',\n\t\t\t\t\t\t\thelp='Arguments to be parsed as a Space-separated list of arguments.')\n\t\treturn parser.parse_args()\n\n\tdef get_command(self, command, args):\n\t\t\"\"\"\n\t\tGiven a command, return the command to execute. If the return value has not\n\t   been None, the return value is the return value of the command. If the return value\n\t   returns a string containing the command result, if no return value is defined\n\t  in the object, the return value is simply the actual invocation of the command.\n\t "}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tif not hasattr(parser, \"description\"):\n\t\t\treturn \"\"\n\t\treturn \"=\".join([a.lstrip() for a in parser.description.split()])\n\n\nclass MetaOpt(type):\n\t\"\"\"\n\tMeta options are available within the program, including the default values for an individual command line or an option in a namespace.\n\tNote: if an attribute of the type is not a command-line variable, the value is simply provided by parsing the argument (if any).\n\tThe value is available in a space-separated string, and can define a default value, that is, with its default"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"--prefix\",help=\"A file name prefix (without.dot) to use.\", required=False)\n\t\tparser.add_argument(\"--postfix\",help=\"An unused name prefix. (prefix without.dot) to use.\", required=False, default=\"\")\n\t\tparser.add_argument(\"--header\",help=\"File header (header to use, must contain.dat or.csv file header)\", required=False)\n\t\tparser.add_argument(\"--no-header\",help=\"If specified, this will generate a header header only when generating.dat or.csv files.\", required=False"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn [x for x in parser.flags if x.startswith('-l')]\n\n\ndef parse(parser, source, options=None):\n\t\"\"\" Parse the given argument tree. \"\"\"\n\treturn OptionParserWrapper(parser=parser, default_flags=parser_flags(parser), source=source, options=options)\n\nclass OptionParserWrapper:\n\t\"\"\"\n\tA wrapper around parse() which accepts a single argument tree.\n\t\"\"\"\n\tdef __init__(self, parser, default_flags, source, parser_flag_map):\n\t\tself.parser = parser\t\t#"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tdefault_parser = parser.add_subparsers(dest='parser')\n\t\tparser_subparsers = (\n\t\t\t(b\"init\", b\"init\"),\n\t\t\t(b\"help\", b\"help\"),\n\t\t\t(b\"help-list\", b\"help-list\"),\n\t\t\t(b'status', b'status'),\n\t\t\t(b'\\b$', b'\\b'),\n\t\t)\n\t\tfor name, info in parser_subparsers:\n\t\t\tparser_subparsers[name].prepare_argument = info.replace(\"(\", b\")\")\n\t\treturn parser_subparsers\n\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser.parse_args(split=True)\n\n\n\tdef get_flags(self, flags):\n\t\t\"\"\"\n\t\tGiven a list of flags, return a list of the flags in that order,\n\t\tfor each flag and the flag itself.\n\t\t\"\"\"\n\t\treturn [ flags ]\n\n\n\t# A map from a flag to a dict.\n\tdef get_flags_by_code(self, code, flags):\n\t\t\"\"\"\n\t\tGiven a code, return a dictionary of flags defined for that\n\t\tcode.\n\t\t\"\"\"\n\t\treturn {\n\t\t\t'H_CODE'              "}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn list(parser.parse_args())\n\ndef parser_make_argument_group(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return a string containing a string\n\tthat will be placed between each argument (if it is given) in a group.\n\t\"\"\"\t\n\treturn parser.format_help()\n\ndef parser_args(args=None, epilog=None):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, provide an iterable, if\n\ta collection of arguments is given, create a collection that\n\tcontains an argument, and a flag value."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"--file\", default=None, type=str, help=\"path to a CSV file\")\n\t\tparser.add_argument(\"--line_number\", default=50, type=int, help=\"number of lines of the input file (defaults to 50)\")\n\t\tparser.add_argument\t= parser_utils.parse_args()\n\treturn parser.parse_args()\n\ndef parse_args():\n\targs = parser_utils.parse_args()\n\tparser = argparse.ArgumentParser();\n\targs = parser.parse_args()\n\treturn args\n\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('-i', '--image-path', metavar='IMAGE', dest='image_path', required=False,\n                           help='Path to the image to load.', type=str)\n\t\tparser.add_argument('-s', '--seed-type', metavar='STRESS', dest='seed_type', required=True)\n\t\tparser.add_argument('-c', '--challenge-type', metavar='STRESS', dest='challenge_type', required=True)\n\t\tparser.add_argument('-x', '--num-iterations', metavar='INT', dest='num_iterations', type"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tprint \"\\r\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\t%s\\t%s\\t%s\\t%s\" % ('-' * 80, '-' * 80, '-' * 80, '-' * 80, '-'*80, '-'*80, '-' *80, '-')\n\t\tos.system(borgmatic_cmdline)\n\telse:\n\t\traise NotImplementedError(\"Need a borgmatic shell command to do this\")\n\n####################################################################################################\nclass Shell( object ):\n\tdef __init__( self ):"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Borgmatic commands.\")\n\t\tparser.add_argument(\"-s\", \"--silent\", action=\"store_true\", help=\"Silently print to Borgmatic.\")\n\t\tparser.add_argument(\"-p\", \"--port\", type=int, default=9000, help=\"port that will be selected\")\n\t\tparser.add_argument(\n\t\t\t\"borgmatic-output-program\", action=\"store\", dest=\"borgmatic_output_program\", help=\"Program to run and run the bash completion command.\")\n\t\tparser.add_argument('-"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Run bash completion script for the specified command arguments.')\n\t\tparser.add_argument('args', nargs='*', type=str, help='Command arguments')\n\t\tparser.add_argument('--timeout', '-t', help='Maximum time to perform the bash completion script.', default='2')\n\t\tparser.add_argument('--print-errors', '-n', help='Print errors on the given execution result.',\n\t\t\t\t\t\t\t\t\t\taction='store_true', default=False)\n\t\tparser.add_argument('--no-output', default=False, action='store_true"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tprint(\"Command Completion started\")\n\t\tp = Popen(\"bash\", shell=True, stdout=PIPE)\n\t\tstdout, stderr = p.communicate()\n\t\treturn stdout\n\t\n\t###############################################################################\n\t# Helper functions\n\t###############################################################################\n\tdef _execute_shell():\n\t\t'''\n\t\tMake sure bash completion has an explicit shell.\n\t\t'''\n\t\tp = Popen((\"bash\", \"shell\"), stdout=PIPE)\n\t\tstdout, stderr = p.communicate()\n\t\t# p.wait() returned an exit status even on non-zero return values\n\t\treturn stdout\n\t\n\tdef _execute_bash"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='execute the borgmatic command')\t\t\t \n\t\tparser.add_argument('--script-template', default='bash', help='Path to the script template')\t\n\t\tparser.add_argument('args', metavar='cmd', nargs='+', default=None, type=str, help='Command arguments')\n\t\tparser.add_argument('-v', '--version', action='version', version='%(prog)s {0}'.format(__version__))\n\t\tparser.add_argument('-d', '--debug', action='store_true', default=False, help=\"print status"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description= \"Borgmatic command completion tool. \\\n\tThis script automatically invokes all provided parsing scripts. \\\n\tUsage information:\\n\t  -p           (print output to stdout)\\n\t  -R           (run borgmatic bash-completion\\n\t  -f\t  -f -o -t           (run borgmatic bash-completion\\n\t)\")\n\t\tparser.add_argument(\"borgmatic_command_line\", type=argparse.FileType('r'), required=True)\n\t\tparser.add_argument(\"-o\", action=\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\targv_completer = []\n\t\tif len(argv) > 1:\n\t\t\targv_completer.append(b_get_cmd_completion(argv[-1]))\n\t\telse:\n\t\t\targv_completer.append(b_get_cmd(argv[0]))\n\t\tprint('\\n'.join(argv_completer))\n\treturn bash_completion\n\t\ndef get_file_name_list(args):\n\t\"\"\"\n\tParse the list of file names to list of all supported file types.\n\tThese types contain the same type as arguments, in bash-completion.py.\n\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\t# Parse bash options from the file\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument('-q', '--quiet', help='quiet mode', dest='quiet', default=False, action='store_true')\t\n\t\tparser.add_argument('-s', '--settings', help='settings file to use', dest='settings', default=None, action='store', default=None)\n\t\tparser.add_argument('-f', '--get', help='borgmatic get file', dest='borgmic_command', default=None, action='append')\n\t\tparser.add_argument('-"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\t# print 'Borgmatic.bash_completion() - running with bash'\n\t\t# return bash_completion()\n\n\t\tp = subprocess.Popen ['bash', '-c', 'import borgmatic; print \"hello world\"']\n\t\toutput = p.communicate()[0]\n\t\t# print 'Borgmatic.bash_completion() - exiting with exit code'\n\t\tprint 'Borgmatic.bash_completion() - exiting with exit code ='+ str(p.returncode)\n\t\tprint\n\t\tsys.exit(p.returncode)\n\n\t\t# Return a list of results - the"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tparser = get_command_line_parser()\n\t\tparser.usage = \"\"\"%prog [OPTIONS] <options> [CMD...]\n\"\"\"\n\t\tparser.description = \"Execute an borgmatic command with options to be executed. See borgmatic's documentation on this command.\"\n\t\tparser.epilog = \"Examples:\"\n\t\tparser.add_subparsers(dest='command', metavar='COMMAND', help='the subcommand to execute')\n\t\tsubparser_name = parser.add_parser('subcommands', help='which subcommand to execute')\n\t\tsubparser_name.set_defaults(func"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = ArgumentParser(*unparsed_arguments)\n\t\tparser.add_argument(\"-v\", \"--verbose\", help=True)\n\t\tparser.add_argument(\"-q\", \"--quiet\", help=True)\n\t\tparser.add_argument(\"-h\", \"--help\", action=\"store_true\",\n\t\t\tdefault=False, help=\"Show the help of that script and exit\")\n\t\targs = parser.parse_args()\n\t\treturn args\n\t\n\ndef main():\n\tdef main_script():\n\t\t\"\"\"main function\"\"\"\n\t\tif len(sys.argv) == 1:\n\t\t\tmain_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = ArgumentParser(description=usage)\n\t\t\n\t\ttry:\n\t\t\t# if there is a global namespace\n\t\t\tname = \"global\"\n\t\t\tnamespace_class = _global_namespace\n\t\t\n\t\texcept NameError as err:\n\t\t\traise SystemExit(\n\t\t\t\t\"An unknown global namespace was passed to the global namespace constructor: %s\" % err\n\t\t\t)\n\t\t\n\t\t# parse\n\t\tfor arg_str in unparsed_arguments:\n\t\t\ttry:\n\t\t\t\t# if there is still more than one positional args it is OK\n\t\t\t\tparser.add_argument(*arg_str.split())\n\t\t\texcept ValueError as"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\t# Creates a subparser for this script\n\t\tparser.add_argument('subparser', help=\"The subparser name specifying whether or not this script should be invoked\")  # Creates a subparser containing a specific subcommand\n\t\tparser.add_argument('argv', nargs='*', help=\"The arguments to use for the command.\") #Creates a subparser for this script\n\n\t\targs = parser.parse_args(unparsed_arguments)\n\t\treturn args\n\n\ndef run_in(args):\n\tlogging.getLogger('ubik-logging').setLevel(logging.CRITICAL"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs = []\n\t\tfor a in unparsed_arguments:\n\t\t\ttry:\n\t\t\t\ta = parse_subparser(a, \"global\")\n\t\t\texcept RuntimeError:\n\t\t\t\tprint \"Unknown argparse subparser (%s). Exiting\" % (a,)\n\t\t\tprint(a.__doc__)\n\t\t\targs.append(a)\n\t\targs = dict(zip(parser_args[0], args))\n\t\treturn vars(parse_args(args))\n\ndef parse_subparser(subm, name):\n\tname, value = subm.__dict__, subm.__doc__\"\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(add_help=False)\n\t\tsubparser_groups = parser._groups\n\t\tsubparser_defaults = parser._defaults\n\t\tdefaults = parser._defaults\n\t\tfor group in filter(lambda x: x not in subparser_groups, defaults.values()):\n\t\t\tsubparser_name = subparser_groups[group].name\n\t\t\tif subparser_name == \"global\":\n\t\t\t\tdefaults[subparser_group] = Namespace(parser, **defaults[group])\n\t\t\telif subparser_name == \"defaults\":\n\t\t\t\tdefaults[subparser_group] ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targuments = \"\"\n\t\tparsed_arguments = []\n\t\tfor arg in unparsed_arguments:\n\t\t\targ_string = str(val_to_dict(arg))\n\t\t\tif arg_string[0]=='-':\n\t\t\t\targ_string = arg_string[1:]\n\t\t\targuments = \"\\n\".join([arg_string])\n\t\treturn {'global':args}\n# -*- coding: utf-8 -*-\nfrom django.db import models, migrations\nimport uuid\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tnamespace = None\n\t\tif isinstance(args, (dict)) and len(args) == 1:\n\t\t\tif 'global' in args:\n\t\t\t\tglobal_namespace = args['global']\n\t\t\t\tfor k,v in namespace.__dict__.items():\n\t\t\t\t\tif k!= 'global':\n\t\t\t\t\t\tif k.startswith('subparsers'):\n\t\t\t\t\t\t\tnamespace = v\n\t\t\t\treturn namespace\n\t\telif isinstance(args, basestring) and args.count(None)!= 1:\t\n\t\t\tunarg = list(args)\n\t\t\tfor arg in unarg:\n\t\t\t\tif not arg.startswith('subparsers"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t# This works because argparse itself won't parse a command line. We need\n\t\t# to pass a dictionary so that it can be used when used inside a\n\t\t# Config instance.\n\t\tsubparser = []\n\t\tfor arg in unparsed_arguments:\n\t\t\tif arg.name.startswith(\"-\"):\n\t\t\t\targv[arg.name.split(\"-\")[0]] = UnparseArgument()\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\targv[arg.name] = UnparseArgument(arg.name)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tsubparser.append(arg.name)\n\t\t\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tfor arg in unparsed_arguments:\n\t\t\tk,v=parse_arg(arg, unparsed_arguments)\n\t\t\tif k not in namespace:\n\t\t\t\tnamespace[k]=parse_namespace(v)\t\n\t\treturn namespace\t\n\ndef main(argv=None):\n\t\"\"\"Main program to parse options. See scripts/argparse.py for details.\"\"\"\n\tglobal parser_dict\n\tglobal args\n\tglobal options\n\tglobal namespace\n\tglobal help_text\n\t#import inspect\n\t#print \"usage: %s [options] input file...\" % os.path."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t_command_line = commandline_parser()\n\t\tif _command_line is not None:\n\t\t\t_command_line.parse(_command_line)\n\t\telse:\n\t\t\tassert os.name == _command_line.command\n\t\t\t_command_line._init = True\n\n\t\tself._command_line = _command_line\n\n\t\t# Use sys.argv so that the command line doesn't use the argument \"sys.argv\" directly\n\t\t# We are doing it by hand so that it's valid\n\t\t_command_line.main()\n\t\treturn _command_line._"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tif not unparsed_arguments:\n\t\t\treturn ((UnsplitNamespace, argparse.ArgumentParser), [])\n\t\t\n\t\tif ',' in subparser_name:\n\t\t\tsubparser = [subparser_name.split(',', 1)[0].split(',')]\n\t\telse:\n\t\t\tsubparser = [subparser_name]\n\t\t\n\t\tsubparser, arguments = subparser\n\n\t\targuments = sorted(arguments)\t\t\n\t\tparsed = []\n\t\t\n\t\tfor sname, spec in zip(('name', 'name_help', 'optionals', 'optional_args'), arguments):\n\t\t\tfor a in subparser.actions:\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tsubparser_names = [item.rsplit(\"-\", 1)[0] for item in subparser_names]\n\n\t\tparsed_args = list(unparsed_arguments)\n\t\tunparsed_args = []\n\n\t\tfor i, arg in enumerate(sys.argv[1:]):\n\t\t\tif arg.startswith(\"-u\")  and unparsed_args[-1][-2:] == \"_\":\n\t\t\t\t\t\t\t\tsubparser_args = arg.split(\"_\", 1)[1:]\t# Remove last two elements\n\t\t\t\t\t\t\telif \"@\" in arg:  # Special case for \"@\", use \"/\" to remove the namespace\n\t\t\t\t\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t    \n\n\targs = list()\n\targsappend = subparsers.insert_action(title='parser arguments')\n\t\n\tfor c in sorted(unparsed_arguments):\n\t\tpargs = c.split()\n\t\t\n\t\tname = pargs[0]\n\t\targsappend( (name, pargs[1:] or [parg[0] for parg in pargs[1:]]), \n\t\t\t\t\tunparsed_arguments[c] )\n\t\t\t\n\treturn args# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \""}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\taction_subparser = subparsers[0][0] # For the named \"action\"\n\t\tassert action_subparser not in unparsed_arguments # Avoid creating a recursive parser\n\t\targs_subparser = unparsed_arguments[action_subparser] # And create the args_group as an empty dict if there's no subparser\n\t\t# Make sure it's only a single string; it's a list of strings so we can convert them to something more clunky\n\t\targs = [arg.split(\":\") for arg in args_subparser] # Do some validation that we can accept as args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tparsed_subparser = {}\n\t\tfor argument in unparsed_arguments:\n\t\t\ttry:\n\t\t\t\tsubparser, _, name = argument.partition(':')\n\t\t\texcept InvalidArgument as e:\n\t\t\t\t# invalid action argument, try parsing the name\n\t\t\t\tsubparser = name\n\t\t\t\tname = argument\n\t\t\t\t# continue\n\t\t\tif isinstance(subparser, basestring):\n\t\t\t\tsubname = [subparser, None]\n\t\t\telse:\n\t\t\t\tsubname = [subparser]\n\t\t\tparsed_subparser[subname] = unparsed_argument_values(subparser)\n\t\t\tsubparser.__dict__."}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tsubparsers = []\n\n\tif unparsed_arguments:\n\t\t# we got too many arguments to take in extra parsing\n\t\trequired_arguments = [ ]\n\t\t\n\t\t\n\tif len(unparsed_arguments) > 1:\n\t\t# the number of arguments is 1 - we need to parse the whole thing\n\t\tsubparsers.append(\n\t\t\t{\n\t\t\t\t\"name\": \"repository\",\n\t\t\t\t\"dest\": \"repository\",\n\t\t\t\t\"help\": \"Specifies the repository that should be used on completion.\",\n\t\t\t\t\"metavar\": \"repo_name\",\n\t\t\t\t\"nargs\": \"?\",\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n# This dict maps subparsers names to action-specific commands. This is because\n# most Subcommands can be inherited from `subparsers.Action`. A typical usage example, here is\n# use the:\n# python subparser_subparser [subparsers]\nparser_subparsers = {}\n\n\n# Here goes the main thing, which takes a argparse namespace and a dict with all\n# the positional and keyword arguments. First is a simple example of how to use it:\n#\n# subparser_subparser [subparser]...\n# subparser_subparser -> action\n#\n# The"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n    def _parse_subparser_options(unused, args):\n                    unused_pos = len(args)\n                    subparser0 = args[0]\n                    del args[0:unused_pos]\n                    return subparser0.parse_args(args)\n     \n           \n    command_name_parser_map = _parse_subparser_name_parser_map()\n    \n    # Parse all arguments in a command group, if there are any. \n    # If one of the given subparsers has already been requested, we ignore it. \n    if not command_name_parser_map:\n        return None\n\n    # Parse the set"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# if no subcommand_arguments, set to the empty dictionary\n\t\tif not unparsed_arguments:\n\t\t\tunparsed_arguments = {}\n\n\t\t# create subparser if necessary\n\t\tif subparsers:\n\t\t\tsubparser = subparsers[0].add_parser(prog='parse_subparser_arguments',\n\t\t\t\tdescription='Convert multiple subparser arguments to use the parser '+\\\n\t\t\t\t\t'--repository name. Requires that repository exists.')\n\t\t\tsubparser.add_argument('--repository', '-r', type = str, required=True, \\\n\t\t\t  dest ='repositories', action ='store', help ="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tsubparser_name_arg_spec = unparsed_arguments[0]\n\t\tparsers = subparser_name_arg_spec[\"parsers\"]\n\n\t\tif len(parsers) == 1:\n\t\t\tsubparser_name_value = parsers[0]\n\t\t\tsubparser_value = parser_to_subparser_value(subparser_name_value, subparsers)\n\t\telse:\n\t\t\tparser_name, subparser_value = parsers\n\t\t\tparser = subparser_parsers[0]\n\t\t\tsubparser = parser_name.split(\"_\")[1]\n\n\t\t\tparser_parent = parser"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__doc__)\n\t\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s {0}\").format(VERSION)\n\n\t\tparser.add_argument(\"--quiet\", action=\"store_true\", help=\"don't print anything.\")\n\n\t\tparser.add_argument(\"-s\", \"--start\", action=\"store\", default=False, help=\"start script\")\n\t\tparser.add_argument(\"-e\", \"--exit\", action=\"store_true\", help=\"exit script\")\n\n\t\targs = parser.parse_args()\n\t\treturn args\n\n\nif __"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tfrom argparse import ArgumentParser\n\t\tdef parse_func(subparsers, args):\n\t\t\tparent = args[0]\n\t\t\tself.add_parser(\n\t\t\t\tparent=parent,\n\t\t\t\tdefault=getattr(self, '_default', None),\n\t\t\t\thelp=getattr(self, '_help', None),\n\t\t\t\thelp_text=getattr(self, '_help_text', None)\n\t\t\t)\n\t\t\treturn (self, subparsers)\n\t\treturn parser\n\t\t\ndef _generate_parser(subparsers):\n\t\"\"\"\n\tParse the CLI option parser with ``subparsers`` as the default\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Pylons Parser to generate and parse Cook Island.')\n\t\tparser.add_argument('cook_file_name', type=str, help='An object to generate a Cook Island.')\n\t\tparser.add_argument('--informator_name', type=str, help='An object to create a Informator.')\n\t\tparser.add_argument('--max_level', type=int, help='Specify the maximum level to use.')\n\t\tparser.add_argument('--informator_max_level', type=int"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\timport argparse as pp\n\t\tparser = pp.ArgumentParser()\n\t\tparser.description = 'Utility for parsing and parsing multisense strings.'\n\t\tparser.epilog = ''\n\t\tparser.add_argument('--string', type=str, default='*.string', help='Full text file path to parse.')\n\t\tparser.add_argument('--raw_string', type=str, default='string', help='Full raw string.')\n\t\tparser.add_argument('--clean', type=bool, default=False, help='Only use file_cleaner on input.')\n\t\tparser.add_argument('--include"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = make_parser()\n\t\tsubparser = parser.add_subparsers()\n\t\tprint >> sys.stderr, \"\\nOptions:\"\n\t\tprint >> sys.stderr, \"\\t\"+parser.formatter.format_usage(subparser)\n\t\tprint >> sys.stderr, \"\\t\"+subparser.formatter.format_help(parser)\n\t\tprint >> sys.stderr, \"\\tFor usage output, see -h\"\n\t\tparser.print_about()\n\t\treturn parser\n#################################################\n\t# Builds the command parser and saves in sys.argv,\n\t# but does"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = subparsers.add_parser(title=APPNAME, description=DESC, # this will only build from the name of the argument\n\t\t\tdescription=DESCRIPTION, formatter_class=MultiFormatter, \n\t\t\tadd_help=False # help on creation\n\t\t)\n\t\tparser.add_argument('-v', '--verbose', action='count', dest='verbose', default=2, \n\t\t\thelp=\"Output verbosity\")\n\t\tparser.add_argument('-v --verbose_v', action='store_true', dest='verbose_v', default=False, \n\t\t\thelp=\"Output verbose v\")\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tsubparsers = parser.add_subparsers()\n\t\tparser.add_argument('command', nargs='*')\n\t\tparser.add_argument('args', nargs='*')\n\t\t\n\t\tparser.set_defaults(func=do_args)\n\t\t\n\t\targs = parser.parse_args()\n\t\t\n\t\t# if the given args exists as a group, we need to use all registered group\n\t\tif args.command is not None:\n\t\t\tregister_parser(args.command, args)\n\t\t\n\t\treturn parser\n\t\n\ndef do_args(func=None):\n\t\"\"\""}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = ArgumentParser(description='Build the main parser for the server.')\n\t\tparser.set_defaults(func=build_parser)\n\t\tparser.add_argument('-c', dest='config_file', default='')\n\t\tparser.add_argument('port', nargs='?', default=None, type=int, help='Port, default is to connect to this port.')\n\t\t# These are the parsed arguments, so they are never set manually.\n\t\tparser.add_argument('-i', dest='ip_server_ip', default=None, type=str,\n\t                       help='IP server ip"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = PasteParser()\n\t\tparser.add_argument(\"dest_path\", help=\"path to local output files\")\n\t\tparser.add_argument(\"input_files\", help=\"list of input files (or dirs)\")\n\t\tparser.add_argument(\"outfile\", type=str, default=default_outfile, help=\"output file for new output files\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", help=\"produce more verbose output\", action=\"store_true\")\n\n\t\tif input_files:\n\t\t\tparser.add_argument(\"-e\", \"--input_dirs\", help=\""}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tadd_argument('-v', '--verbose', help='verbose output')\n\telif isinstance(argv, list):\n\t\tadd_argument('-v', '--verbose', help='verbose output', action='store_true')\n\targs, unknown = parser.parse_known_args()\n\n\treturn args, unknown\n\n@app.route('/robots')\ndef robots():\n\tif 'PATH_INFO' not in os.environ:\n\t\tpath_info = os.environ['PATH_INFO']\n#\t\tif path_info.lower().startswith('http://%s/' % robot."}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\ndef _is_simple_json(obj):\n    return isinstance(obj, dict) and not isinstance(obj, list)\n\t\t\ndef _is_simple_xml(obj):\n\treturn isinstance(obj, (ElementTree.fromstring, etree.ElementTree)) and isinstance(obj.tag, ET._ElementTag)\n\ndef _get_pagination(page, max_results=1000):\n\tmax_results = max_results if max_results > 1000 else None\n\t\t\n\tresponse = requests.get(\n\t\t\turl_prefix+'api/v1/pagination/page-{}'.format(page+"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    page = -1 # page number of current page in last results (should be 0-indexed)\n\n    while True:\n        num_objects = int(response[page])\n        if num_objects > 0:\n\t\t\tfor i in range(0, num_objects):\n\t\t\t\tresponse = response[page + i]\n\t\t\t\tpage += num_objects - 1\n\t\telse:\n\t\t\tpage = len(response) - 1\t\t# page number, not number of documents\n\n\n\t\t# Get WAPI_ response\n\t\t\n\t\tresponse = response[:max_results] + response[page:]\n\n\treturn response"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n#\t\t## Get a list of all of the objects in the results list\n#\t\tlist_items = response[:MAX_RESULTS] + response[-MAX_RESULTS:]\n\t## Select the next page in the list\n\t## if the page is not within the list, the entire result set is empty\n\tdef get_page(resultset, page):\n\t\tpage = int(page)\n\t\twhile (page < len(resultset)):\n\t\t\t#\tGet the next page from the \"nextPage\" list\n\t\t\tnextpage = resultset[page]\n\t\t\t#\tIf the"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tpage = response[len(response)/2]\n\t\tif page > max_results:\n\t\t\twhile(not response[page]['page']):\n\t\t\t\tpage = 1\n\t\twhile(page*page >= max_results):\n\t\t\tyield response[page], min(MAXIMUM_RESULTS, page//page + 1)\n\t\t(next_page, rest) = yield response[page-1]\n\tpage += 1\n\tyield rest\n\treturn Generator(page, max(1, MAXIMUM_RESULTS - page // page + 1))\n\ndef random(generator, seed):\n\t\"\"\""}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tpage_data_resp = get_page(response, max_results, self.current_page)\n\t\tfor result_data in page_data_resp.split():\t\n\t\t\tyield result_data\n\n\tasync def next_page(self):\n\t\t\"\"\"\n\t\tReturns WAPI response page by page.\t\"\"\"\n\t\t\tprint \"Request URL: %s\" % self.current_page\n\t\t\t\n\t\t\tpage = await get_page(self.current_page)\t\t \n\t\t\tpage = page.json()\n\t\t\tif \"page\" in page:\r\n\t\t\t\tprint \"\\n\\n\\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tfor i in response:\n\t\t\tyield { \"result\": i, \"page\": response.next_page_number() }\n\ndef main(path):\n\t\"\"\"\n\tPrints all results of the search and returns them, optionally in JSON format.\n\n\tUsage:\n\t\t$ python {\n\t\t\"path\": \"/search/page_1.html\",\n\t\t\"api\": \"google.com/search/v2.0\",\n\t\t\"max_results\": 2,\n\t\t\"params\": {\n\t\t\t\"q\": \"Hello World, World!\",\n\t\t\t\"page_size\": 20"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tpage = response.get('total', 0) + (max_results / 20)\n\t\treturn generate_paging_page(page, response, max_results)\n\n\nif __name__ == \"__main__\":\t\n\timport sys\n\timport json\n\n\tprint(\"STARTING WAPI Generator\")\n\n\t# Instantiate parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"url\",\n\t\t\t\t\t           help=\"WAPI url of a PAGER API response, e.g. https://api.wapi.com/p/pap/\",\n\t\t\t\t\t           type=str,\n\t\t\t\t\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    @functools.wraps(response)\n    def page_request(url, headers):\n        headers['Connection'] = 'Keep-Alive'\n        response.raw_post(url, headers=headers) \n        status, content = response.raw_get(url, headers=headers)\n        yield ParseResult(response.status_code, content)\n\t\n\tfor res in page_request(url, headers):\t\t\n        yield ParseResult(res.status_code, res.content)\n\tyield\n\n###\n\n# -*- coding: utf-8 -*-\n\"\"\"\n    pylons.tests.test_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t\n\tfor i in response:\n\t\tyield i\n\t\turl=i[\"url\"]\t\n\t\n\nif __name__ == \"__main__\":\n    #This module must be called before any of the functions is run\n\timport requests\n\timport json\n\timport re\n\timport datetime\n\n\t#This regex will match all of the regular expressions in the JSON data returned \n\tregex_dict = {\"1\\['\":\"-1\\]\",\n\t\t\t \"(\":\"-*\\s+[;]\\s*)+\": \"([\\s]+)\\1-(\\d+)\",\n\t\t\t \"(\":\"-*)[\\s]+\": \"([\\"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    for response in response:\n            yield response\n        \n    \n    # Start the request with the default parameters    \n    url = \"http://api.wapi.com/search/husa/hall/query/?language=es&\"\n    url += \"search_keyword=\" + random.choice(search_keyword) \\\n                    + \"&search_language=\" + search_language\n    url += \"&search_type=search&language=\" + search_language\n\n    response = client.request(url, headers={\"Authorization\": fkey}).json()\n    if \"paging_count\" in response:\n        return next(result.value"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(round(int(size)))\n\n\texcept ValueError as e:\n\t\treturn str(e)\n\n\texcept Exception as e:\n\t\treturn str(e)\n\ndef convert_size(size: str, size_unit: str, unit: Optional[UnitsType]) -> str:\n\t\"\"\"\n\tConvert a size from human readable string with unit.\n\n\tArgs:\n\t\tsize (A string representing a human readable string in human readable format)\n\t\tsize_unit (Optional[UnitsType]): An optional unit (e.g. \"K\") that is used to represent the"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\t: int(size)\n\t\treturn int(size, base=1024)\n\texcept ValueError as e:\n\t\tprint(\"[info ]\", file=sys.stderr)\n\t\tprint(\"[size_to_bytes ]\", e, file=sys.stderr)\n\t# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License."}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tif size is None:\n\t\t\treturn \"0M\"\n\t\telif len(size) <= 10:\n\t\t\treturn \"%.2f%s\" % (int(size) / 10**9, \"K\" if size < 10**9 else \"\")\n\t\telif size < 100:\n\t\t\treturn \"%.2f%s\" % (size, str(size / 10**9))\n\n\n\"\"\"\nA helper function that returns a value if its possible.\nIf not able to calculate it, this returns the string which could be passed to\nto the function.\n\nFor example::\n\n    >>> int("}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn (size, 'bytes')\n\ndef get_filesize(size: str) -> Tuple[int, bytes]:\n    \"\"\"\n\tGet filesize as integer, bytes or None.\n\t\"\"\"\n    try:\n        size_human_readable(size, 'KB')\n        return size_human_readable(size, 'KB')\n    except IOError:\n        return None\n\nclass NullValue(int):\n\t\"\"\"\n\tNull, 0, or None values.\n\t\"\"\"\n\tdef __int__(self, *args):\n\t\treturn 0\n\n# pylint: disable=too-many-arguments"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t        return str(size)\n\n\treturn str(int(size) * 1e3)\t\t\t# number of bytes\n\n\texcept ZeroDivisionError as ie:\t\t\t\t# this is an approximate approximation\n\n\texcept Exception as ex:\n\t\tprint(str(ie))\n\t\tprint((ex))\n\t\treturn 0\n\n\treturn 0\n\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache."}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tresult = float(size)\n\t\tresult /= 1024**2\n\t\tresult *= 1024**3\n\t\tresult *= 1024**4\n\t\tresult *= 1024**5\t\n\t\tresult *= 1024**6\n\t\"\"\"\n\treturn result\n\ndef fprintf(fmt, string, number: str='.', precision: str='#', sep='') :\n\t\n\tif string == format(precision+precision):\n\t\treturn string + sep\n\tif string == number:\n\t\treturn \"F%d %s %s\" % (number, precision, sep)\n\n\nclass NTFontFileFormat(FileFormat"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(size)\n\ndef size_to_human_friendly(size: str) -> str:\t\"\"\"\n\tConvert human readable file size to human readable size, depending on if the size is one of the following:\n\t\t'1000K' : '1M' | '1G' | '50K'\n\t\t'10K' : 'K'  | 'K1'\n\t\"\"\"\n\t\tif size[0] is None and size[1] is None:\n\t\t\treturn None\n\t\telif size[0] is None:\t# 0 KiB\n\t\t\tsize"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ndef file_to_bytes(filename: str, blocksizeBytes: int) -> int:\t\"\"\"\n\tConvert human readable file as a bytes object\n\tFor example, this function would write '101' to the device as follows..\n\n\tArgs:\n        filename (str): The filename in question, usually a remote file path\n\n        blocksizeBytes (int): blocksize of file to be converted (int)\n        \nReturns:\n    a byte string representing a file-like object\n\t\"\"\"\n\tfrom.config import Config\n\n\tdata = Config.get(\"server/filesystems/os-filesystem.conf\").replace"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn size\n\n\t## The default value of 4k is 16 bits (8K).\n\tfrom struct import pack\n\t\n\t## The value of 8K works like 11K because it is stored within a dictionary mapping length\n\t## to the length of the item.  If the item is not contained in the dict, an error will be raised.\n\tdata = pack(\"II\", size)\n\tif len(data) <= 4:\n\t\traise ValueError(\"file size has to be at least 4 characters.\")\n\tif len(data) % 4!= 0:\n\t\tif raise_size >= 2"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(size)\n\ndef size_to_filename(size: str) -> str:\n\t\"\"\"\n\tConvert human readable file size to filename.\n\t\"\"\"\n\treturn \"\"\n\ndef filename_to_size(filename: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes.\n\t\"\"\"\n\treturn 0xdeadbeef(os.stat(filename).st_size)\n\ndef filename_to_bytes(filename: str) -> str:\n\t\"\"\"\n\tConvert human readable file size to bytes.\n\t\"\"\"\n\treturn"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(dicts.values())\n\ndef _keys(dicts):\n\t\"\"\"\n\tReturns a dictionary with keys from dicts\n\t\"\"\"\n\treturn dict(keys(_dictsum(d)) for d in dicts.values())\n\ndef _items(dicts):\n\t\"\"\"\n\tReturns a dictionary with items from dicts\n\t\"\"\"\n\treturn dict(items(_dictsum(d)) for d in dicts.values())\n\ndef _key(dicts):\t\"\"\"\n\tReturns a dictionary with keys from dicts\n\t\"\"\"\n\treturn dict(key(_dictsum(d)) for d"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn tuple()\n\t\t\n\tdef _dictdiff(self, other, dict_keys=None):\n\t\tif dict_keys is None:\n\t\t\tdict_keys = ('a', 'b')\n\n\t\tif isinstance(other, types.DictionaryType):\n\t\t\tother_keys = _dictkeys(other)\n\n\t\tfor key in other_keys:\n\t\t\tkey2 = getattr(self, key)\n\t\t\tif not (key2 is other or key2!= other):\n\t\t\t\treturn False\t\t\n\n\t\treturn True\n\n\tdef __getstate__(self):\n\t\treturn self.__dict__.copy()\n\t\t\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\n\ndef _dictmerge(d1, d2):\n\tif len(list(d1.items()))!= len(list(d2.items())):\n\t\treturn {}\n\treturn {d1.get(key):d2.get(key) for key in set(d1.keys()) | set(d2.keys())}\n\ndef _find_common_keys(dicts):\n\t\"\"\"\n\tGenerate the keys that occur in the dictionary.\n\t\"\"\"\n\treturn set(['a', 'b']) if list(dicts.keys()) == types.StringTypes else"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(dicts.values(), [])\n\n\tdef _dictadd(self, d, key):\n\t\treturn self.make_dict(_dictsum(d), key)\n\n\t__add__ = _dictadd\n\n\tdef _dictsub(self, d, key):\n\t\treturn self.make_dict(_dictsum(d), key)\n\n\t__sub__ = _dictsub\n\t__iadd__ = _dictadd\n\n\tdef __str__(dct):\n\t\tdct = dict(dct)\n\t\tfor key, values in dct.iteritems():\n\t\t\tif not isinstance(values"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tfor dct in dicts:\n\t\t\tfor k, v in dct.iteritems():\n\t\t\t\tif isinstance(v, dict) and not isinstance(v, list):\n\t\t\t\t\tif k not in vals:\n\t\t\t\t\t\tvals[k] = 0\n\t\t\t\t\tvals[k] += v\n\t\treturn nums.OrderedDict([v[k] for k, v in vals.iteritems()])\n\ndef _combine_dicts(dicts):\n\t\"\"\"\n\tCombine a list of dictionaries into a list of tuples\n\tas returned by _dictsmatch().\n\t\"\"\"\n\td, counts = [], {}\n\tfor d in"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tdicts = sorted(dicts, key=_dictsum_prioritykey)\n\t\t\n\t\t\t\n\t\t\n\t\t\n\tdef __init__(self, *iterables, **kwargs):\n\t\t# We have lists of dictionaries (with key-value pairs),\n\t\t# so we can create the \"chained\" list using a tuple\n        self.dicts = list(map(list, iterables))\n\t\tsuper(OrderedDict, self).__init__(*iterables, **kwargs)\n\tdef __getitem__(self, key):\n\t\treturn dict.__getitem__(self, key)\n\t\n\tdef __len__(self):\n\t\treturn len(self"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tres_dict = {}\n\t\tfor d in dicts:\n\t\t\tres_dict.update(_dictsum(d.values()))\n\t\t\t# keep dictionaries with no duplicates -- it isn't\n\t\t\t# sufficient to have entries with no duplicates\n\treturn res_dict\n\t\n\ndef _dictsort(items):\t\"\"\"\n\tTake a map of items and return a sorted iterator of items.\n\t\n\t>>> _dictsort(_dict.keys())\n\t[('a', 1), ('b', 2)]\n\t>>> _dictsort({'a': 1, 'b': 2})\n\t[('a', 1),"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\ndef _dictupdate(dicts, key, val):\n\t\"\"\"\n\tUpdate an iterable containing dicts.\n\n>>> _dictupdate(((1, 2),{'a': 1, 'b': 2},{'a': 5, 'b': 0}),{'c': 1, 'b': 2})\n\n>>> _dictupdate(({'a': 1, 'b': 2},{'a': 5, 'c': 2})})\n{'a': 6, 'b': 2}\t\"\"\"\n\t\ndef _test():\n\t\"\"\"\n\tTest dictionary-like access to the values\n\t\"\"\"\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tif i == 0:\n\t\t\tresult = {}\n\t\t\tfor d in dicts:\n\t\t\t\tfor x in d.keys():\n\t\t\t\t\tresult[x] = d[x]\n\t\t\treturn result\n\t\telif i == 1:\n\t\t\treturn _dictsum(dicts)[0]\n\t\telif i == 2:\n\t\t\treturn list(dicts)[0]\n\t\telif i in dict(dicts):\n\t\t\treturn [d for d in dicts if len(d) in dict(dicts)][0]\n\t\telse:\n\t\t\treturn None\"\"\"\ntest_template.py - tests that we can export"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tdicts = _dicts(dicts)\n\t\tif not dicts:\n\t\t\treturn (0, [])\n\t\ta, adict, b = {}, {}, {}\n\t\ttmp = _dictsum(dicts)\n\t\ttmp += a\n\t\ttmp += b\n\t\treturn tuple(tmp)\n\n\ndef _dictsumdicts():\t\"\"\"\n\tReturn a list of dictionaries, sorted by each input key.\n\t\"\"\"\n\t\tdicts = _dicts(sorted(_listvalues(keys)))\n\t\tif _dictsum(dicts):\n\t\t\treturn (0, [])\n\t\treturn _dicts(_listitems(dicts))"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif isinstance(url, basestring):\n\t\t\turl = url.replace(url.decode('utf-8'), '*')\n\t\t\t\n\t\tif not isinstance(url_args, basestring):\n\t\t\turl_args = cgi.escape(url_args)\n\t\t\t\n\t\tres = urlparse.urlparse(url)\n\t\tif res.scheme:\n\t\t\turl_args = urlparse.parse_qs(res.query)\n\t\t\t\t\n\t\t# remove query\n\t\tif res.query:\n\t\t\tquery_dict = cgi.parse_qs(url_args, keep_blank_values=True)\n\t\t\t\n\t\t\tif '"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor arg in url_args.get('url', []):\n\t\t\tif \"=\" in arg:\n\t\t\t\targ = arg.split('=', 1)[0]\n\t\t\targ = urllib.unquote(arg).replace('+', '_').replace('_', '')\n\t\t\tif \"=\" in arg:\n\t\t\t\targ = arg.split('=', 1)[1]\n\t\t\turl_args['url'].append(arg)\n\treturn url_args\n\ndef get_params(url, headers=None, params=None, headers_in=None):\n\t\"\"\"\n\tSends a GET request with all the necessary parameters\t\n\t\"\"\"\n"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor k, v in url_args.items():\n\t\t\turl_args.setdefault(k, v)\n\n\t\t\t# if an arg is also a url\n\t\t\tif k == \"v\" or k == \"v=\":\n\t\t\t\turl_args[k] = v\n\n\t\treturn url_args\t\t\t\n\n\ndef encode_header_to_list(headers, list_to_append):\n\t\"\"\"\n\tConverts a list of dict headers to a regular list with keys as \"key\" and values as \"val\"\n\n\t:param headers: dictionary of string headers\n\t:param list_"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\turl_args = list(url_args)\n\t\tfor arg in url_args:\n\t\t\tif isinstance(arg, str):\n\t\t\t\traise TypeError(\"URL arguments must be strings\")\n\n\t\t\turl_args[arg] = _urllib.parse.quote(arg, \"\")\t\t\n\t\treturn url_args\n\n\nclass Response(_urllib.response.HTTPMessage):\n\t\"\"\"\tA general Response class.\n\t\"\"\"\n\n\t__slots__ = [\n\t\t'code', 'headers','msg',\n\t]\n\n\tdef __init__(self, code, headers, msg):\n\t\t\"\"\""}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif isinstance(url, str):\n\t\t\t# Special case for URL objects\n\t\t\turl = url.replace('http://', 'https://')\n\t\telif isinstance(url, compat_str):\n\t\t\t# Special case for URL objects\n\t\t\tif not isinstance(url, compat_bytes):\n\t\t\t\turl = str(url)\n\t\t\t\tif PY3:\n\t\t\t\t\turl = url.decode('utf8')\n\t\t\t\telse:\n\t\t\t\t\turl = url.decode(compat_str)\n\t\t\t\turl_args = compat_kwargs(url)\n\t\t\t\tfor k, v in url_args.items():"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif url.endswith('_raw'):\n\t\t\turl = url[:-5]\n\t\t\n\t\tif url == '':\n\t\t\treturn url\n\n\t\tif _PY25:\n\t\t\treplace_dict = dict(urlparse.parse_qsl(url))\n\t\telse:\n\t\t\treplace_dict = dict(urlparse.parse_qsl(url))\n\t\treplace_url = replace_dict.get('apiurl')\n\t\tif replace_url:\t\turl = replace_url\n\t\telif url.endswith('token'):\n\t\t\turl = _replace_url_args(url_args, replace_dict['token'])"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif url_args:\t\n\t\t\tfor arg in url_args:\n\t\t\t\tif \"=\" in arg:\n\t\t\t\t\tcontinue\n\t\t\t\tnew_url_args = url_args.replace(arg, \"\", 1)\n\t\t\t\tnew_url_args = url_args.replace(new_url_args, \"\", 1)\n\t\t\t\t#TODO: Remove if statement\n\t\t\turl_args = new_url_args\n\t\treturn url_args\n\n\tdef _remove_url_from_item(self, item):\t#remove list(string)\n\t\treturn re.sub(\"[<>]\", \"\", item)"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif 'custom_string' in url:\n\t\t\tif url['custom_string'].startswith('#'):\n\t\t\t\tcontinue\n\t\t\tnew_url = u'{{{0}}}'.format(url['custom_string'].replace('#', '_'))\n\t\t\treturn '{{{0}]'.format(new_url)\n\t\telse:\n\t\t\treturn url_args\n\n\tdef set_site_url_(self, site_url):\n\t\t\"\"\"\n\t\tSet HTTP or HTTPS for the sites specified\n\t\t\"\"\"\n\t\tif site_url is None:\n\t\t\treturn None\n\n\t\ttry:\n\t\t\tif self."}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif \"url_args\" not in url_args and type(url_args) is dict:\n\t\t\turl_args = (url_args)\n\t\tif \"url_args\" in url_args:\n\t\t\tdel url_args[\"url_args\"]\n\t\tif \"url_args\" in url_args:\n\t\t\tdel url_args[\"url_args\"]\n\t\treturn url_args\n\n\t@api\n\tdef _get_url(self, url_args):\n\t\t\"\"\"\n\t\tCreate url to be handled by url_endpoint\n\t\tCreates url_args with query parameter"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor key, val in url_args.items():\n\t\t\tif key in url_args:\n\t\t\t\turl_args[url_args.keys()[0]] = val\n\t\n\treturn args\n\ndef _clean_url_args(url_args):\n\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor key, val in url_args.items():\n\t\t\tif key in url_args:\n\t\t\t\turl_args[url_args.keys()[0]] = val\n\n\treturn args\n\ndef _get_post_url(page, type,"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif val is None:\n\t\t\treturn False\n\t\tif type(val) == bytes:\n\t\t\treturn val.encode(\"utf-8\") == \"none\"\n\t\telif type(val) == str:\n\t\t\treturn str(val) == \"none\"\n\t\telse:\n\t\t\treturn val == \"null\" and type(val) == int\n\t\treturn True\n\n\t@staticmethod\n\tdef is_datetime(val: any) -> bool:\n\t\t\"\"\"\n\t\tCheck if a datetime object represents a valid datetime.\n\n\t\t@param val: The object to convert.\n\t\t@type val"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is not None\n\n\tdef is_none_boolean(val: bool) -> bool:\n\t\t\"\"\"\n\t\tCheck if a boolean represents a None value.\n\t\t\"\"\"\n\t\treturn val is not True\n\n\tdef is_none_decimal(val: bool) -> bool:\n\t\t\"\"\"\n\t\tCheck if a decimal represents a None value.\n\t\t\"\"\"\n\t\treturn val is not None\n\n\tdef parse_boolean(self, value: str) -> bool:\n\t\t\"\"\"\n\t\tParses a boolean value.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t(yes, no) = ["}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif val == None: return True\n\n\t\ttry:\n\t\t\tif not isinstance(val, str): return False\n\t\texcept ValueError: return False\n\n\t\treturn True\n\ndef is_type(t: str) -> bool:\t\"\"\"\n\tCheck if a type string is a valid one\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttype_t = str(t)\n\t\texcept Exception:\n\t\t\treturn False\n\t\tif not type_t.isdigit() or type_t >= 0: return False\n\t\tif type_t not in (list, tuple): return False\n\t\treturn True\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is None and \"NULL\" in val\n\n\tdef is_float(val: any) -> bool:\n\t\t\"\"\"Returns whether a float represents a value of an object.\"\"\"\n\t\treturn float(val) == 0\n\n\tdef str(val: any) -> str:\n\t\t\"\"\"Takes a valid Python value, and returns a string representation of the\n\t\tvalue.  For example, if the object values are \"1,2\" as well,\n\t\tstr() will return \"1, 2\".  For example, if the object values are\n\t\t\"a string\", this simply returns \"a"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is None or val == \"NULL\"\n\n\treturn isinstance(val, six.string_types) and val\nfrom django.utils.translation import ugettext as _\n\nfrom django_tables2.db import transaction\n\nfrom.models import Base\n\n\nclass Event(BaseModel):\n    '''\n\n    '''\n    name = models.CharField(_('Name'), max_length=100)\n    slug = models.SlugField(_('Slug'), max_length=100)\n    start_date = models.DateTimeField(_('Start date'), auto_now_add=True)\n    end_date = models.DateTimeField(_('End date"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str) and val!\"\"\n\n\n#!/usr/bin/pypy\n# Copyright (C) 2013-2020 The Bitcoin Core developers\n# Distributed under the MIT software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n\"\"\"Test the wallet transactions.\n\n\"\"\"\n\nfrom enum import Enum\nfrom io import BytesIO\nimport os\nimport random\nfrom decimal import Decimal, DecimalException\nfrom test_framework.test_framework import BitcoinTestFramework\nfrom test_framework.util import assert_equal, assert_raises"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif callable(val):\n\t\t\treturn isinstance(val, str)\n\t\telse:\n\t\t\treturn val is not False or val is None\n\n\"\"\"\nThe classes that are also instantiated.\n\"\"\"\n@register_hook(HookType.NONE, \"get_or_create\")\ndef _should_get_or_create(name: Any, **kwargs) -> bool:\n\t\"\"\"\n\tShould the current named hook be called as part of a get or create event?\n\t\"\"\"\n\tif name in hooks.get_hooks():\n\t\treturn True\n\telse:\n\t\treturn"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\ttry:\n\t\t\treturn (isinstance(val, str) and val == '')\n\t\texcept TypeError:\n\t\t\treturn False\n\n#\n# Python 2/3 compatibility\n#\nclass Py3(object):\n\t\"\"\"Python wrapper using a standard library.\n\n\tThis class is compatible with python 2.  The object has no\n\tconcern when using a module that's too slow and so can take a\n    couple of minutes for speed.\n\n\t\"\"\"\n\t__gtype_name__ = 'Py3'\n\n\t#\n\t# Constants\n\t#\n\t# Python 3 is a list"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str)\n\n\tdef is_none(str_: Any) -> bool:\t\"\"\"\n\tCheck if a string represents None.\t\"\"\"\n\t\treturn not str_.strip()\n\n\tdef validate_number(val: int):\n\t\t\"\"\"Validate the value of a number that is not a string.\"\"\"\n\t\tif not val: return None\n\t\telif isinstance(val, (int, long, float)):\n\t\t\treturn val\n\t\treturn None\n\n\tdef validate_date(val: str) -> bool:\n\t\t\"\"\"Validate whether the string is a datetime string.\"\"\"\n\t\ttry"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif val is None:\n\t\t\treturn False\n\t\tif isinstance(val, str) and val == '':\n\t\t\treturn False\n\t\treturn True\n\n\tdef is_none_number(val: any) -> bool:\n\t\t\"\"\"\n\t\tCheck if a number is a number.\n\t\tCheck that it is a string containing only letters, digits, hyphens and underscores.\n\t\t\"0\" and \"1\" are considered numbers.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn int(val)\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\treturn float(val)\n\t\t\texcept ValueError:\n\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\ndef parser_init(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return the parsed argument parser object.\n\t\"\"\"\n\t\ndef parse_global_args(argv):\t\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-c\", \"--config\", dest=\"config_file\", default='~/.config/config', help=\"Path to config file\")\n\tparser.add_argument(\"--db\", dest=\"database\", default='~/.config/database', help=\"Database to use for query\")\n\tparser.add_argument(\"-t\", \"--table\", dest=\""}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tfor flag in parser.parse_args(argv=['--flags']).split():\n\t\t\tif flag:\n\t\t\t\tif not flag in parser.flag_defs:\n\t\t\t\t\tparser.flag_defs.append(flag)\n\t\t\t\telse:\n\t\t\t\t\tparser.flag_defs[flag] = parser.flag_defs[flag].split('=')\n\t\treturn parser.flag_defs\n# -*- coding: utf-8 -*-\nfrom __future__ import (\n    unicode_literals,\n    division,\n    absolute_import,\n    print_function,\n    )\n\nfrom django.db import models, migrations\nimport"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tflags = []\n\t\tfor arg in parser.option_list:\n\t\t\tif arg.startswith('--') or arg.startswith('--include'):\n\t\t\t\ttry:\n\t\t\t\t\targ = arg.replace('-prefix=' + os.path.basename(os.path.dirname(sys.argv[0])) + '_' + os.path.basename(arg)[:-2]+'.py', '')\n\t\t\t\t\tif not os.path.exists(arg):\n\t\t\t\t\t\traise Exception('invalid path:'+ arg)\n\n\t\t\t\t\tflags.append(arg)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint(e)\n\t\t\telse"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser.get_flag_space()\n\ndef parser_commands(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its command line flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser.get_flag_space()\n\ndef parser_option_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its option flags in a space-separated\n\tstring.\t\"\"\"\n\t\tparser.set_flags(parser.get_flag_space())\n\t\treturn parser.get_option_space()\n\n\nclass MyParser(object):"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n#\tThis regular expression matches all flags in the provided format of the argument name.\n\tregexps = r'(^((?P<flags>.*?)\\s+)+(?P<key>.*?)\\s+)(.*?)((?P<value>.*))?'\n\tgroupname = r'( \\(?P<key>.*?)(\\s+)(?P<flags>.*?)\\s+(?P<value>.*))?\\s+(?P<key>.*)'\n\tvalue = r'( \\(?P<value>.*?)(?:\\s+(?P<key>.*?)\\s+(?:\\s+\\s*|)\\s+(?P<flags"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"-c\", \"--csvfile\", dest=\"csvfile\", help=\"CSV filename. Default: 'output/all_features.csv'\", metavar=\"FILE\")\n\t\tparser.add_argument(\"-n\", \"--nbins\", dest=\"nbins\", type=int, default=1000,metavar=\"Nbins\", help=\"Max number of bins to be used before binning feature matrix, (default: 1000)\")\n\t\tparser.add_argument(\"-t\", \"--t\", dest=\"t\", type=float, default=20,metavar=\"T\", help=\"Start end time (default: 20"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"_output\", metavar=\"PATH\", type=str, default=None,\n\t\t\thelp=\"The path to the output file, used for logging (e.g., for debug). If None, the output will use an \"\n\t\t\t\"application default output.\")\n\ndef main():\n\t\"\"\"\n\tParses the command line, if any.\n\t\t@param args: an argparse.Namespace object\n\t\t@type args: an argparse.Namespace\n\t\n\tReturns a logging.Logger (logging.Logger object)\n\t\n\t\"\"\"\n\tprint options\n\n\tlogging_default_output = None\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_option(\"--help\",\n\t\t  \"--help\",\n\t\t  action=\"store_true\",\n\t\t  help=\"Show usage and exit\")\n\t\t\n\t\tparser.add_option(\"--version\",\n\t\t  action=\"store_true\",\n\t\t  help=\"Show program's version number and exit\")\n\t\t\n\t\ttry:\n\t\t\t(options, args) = parser.parse_args()\n\t\t\treturn options.version\n\t\texcept Exception as exception:\t\n\t\t\tprint str(exception)\n\t\t\treturn 1\n\t\t\n\t\treturn parser_flags(self, parser)   \n\tdef create_shell_command(self, args"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument(\"flags\", nargs=\"*\", type=str,\n\t\t\thelp=\"additional flags that will be passed when parsing\")\n\t\treturn parser.parse_args()\n\n\tdef filter_flags(flags):\t\t\t\"\"\"\n\tFilter flags according to the list of flags found in the flags.\n\t\"\"\"\n\t\tif flags is None:\n\t\t\tflags = []\n\t\t\tfor name, arg in parser_flags(parser).items():\n\t\t\t\tfor value in arg:\n\t\t\t\t\tif not value.startswith('-'):\n\t\t\t\t\t\tflags.append(name + \" \" + value[1:])\n\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn (\n\t\t\tparser.add_argument(\"args\", nargs=\"+\"),\n\t\t\tparser.epilog,\n\t\t)\n#!/usr/bin/env python\n\nimport logging\nimport os\nimport subprocess\nimport sys\nfrom shutil import rmtree\n\nimport click\nimport requests\n\ntry:\n    from selenium import webdriver\nexcept ImportError:\n    pass\n\nfrom kazlgit.util.command import run_cmd\n\n\n# Set up context variables to set up logging for this function\nglobal _logger\n_logger = logging.getLogger(__name__)\n\n\n@click.group(name='docker')"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\ndef check_paths(paths):\n\t\"\"\"Check set is empty, except for missing or invalid paths\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tfor p in paths:\n\t\t\tbase, ext = os.path.splitext(p)\n\t\t\tif ext in ['.h', '.hxx', '.hpp', '.hh']:\n\t\t\t\treturn False, p\n\t\treturn True, None\n\texcept Exception:\n\t\treturn False, None\n\n\ndef is_valid_filename(filename):\n\t\"\"\"Return True if a filename has given a valid extant extension\n\t\"\"\"\n\texts = {\"."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    # Process the path name to make sure it exists\n    current = set(path_name)\n    \n    # If it is already in the process\n    if current!= processed:\n\t\n        # We found a path that would be processed, proceed with next one.\n        if verbose:\n\t\t\tsys.stderr.write(\"Attempting to process \" + path_name)\n\t\ttry:\n\t\t\t# For each iteration of the list of sets,\n            # check if current is really needed and if we find it\n\t\t\t# isn't already we're looking at it\n\t\t\tfor i in range(len(current)):\n\t\t\t\tif"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if processed.issubset(path_name):\t# Ignore entries that are already already in path\n\t\tprocessed.add(path_name)\n\t\tfor n, s in processed:  # Add it to the set\n\t\t\tpath_name = os.path.join(data_dir, path_name + '/' + s)\n\t\treturn False\n\tif verbose: print('Adding path to'+ path_name +'as processed')\n\treturn True\n\n\n#################################################################################\n#                                                                             #\n#                     Tester that processes a set of objects                                          #\n#                                                                             #\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\ndef is_directory(path):\n    \"\"\"\n\tChecks if a path is a directory or a file. For path:\n\n\t\t\t<DIR_NAME>\n\n\tReturns: True if path is a file\n\n\t\tIf path is a directory, is the same as the given path for\n\t\tthis function\n\t\"\"\" \n\n    if not os.path.isdir(path):\n        return False\n\n    if os.path.isfile(path):\n        path_is_file = True\n    else:\n        path_is_file = False\n\n    if path_is_file == True:\n        return True\n    else:\n        if verbose"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n\t\n\tif not os.path.exists(process_path_directory(path_name, verbose)):\n\t\tif verbose:\n\t\t\tprint \"The path to a directory or file given to was_processed is not set.\"\n\t\t\tprint \"Adding entry to the set.\"\n\t\t\tprint os.linesep\n\t\treturn False\n\t\n\tif not os.path.exists(os.path.dirname(path_name)): # if path_name is not dir then if it is not a directory then\n\t\tif verbose:\tprint \"The path to directory or file given to was_processed is not set.\" + os"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\ndef is_dir(path_name, verbose):\n\t\"\"\"\n\tCheck if a directory name exists.\n\t\"\"\"\n\tif verbose:\n\t\tprint 'Checking if :'+ path_name\n\tpath_path = os.path.join(path.split(os.path.sep)[:-1], path_name)\n\tif os.path.isdir(path_path):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef main():\n\t\"\"\"\n\tParse command line options and attempt to decide.\n\nThis is what I have provided for the application."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\tif verbose: print 'Checking path'+ path_name\n\t\tif not os.path.exists(path_name): \t# If path does not exist, try to create it\n\t\t\ttry:\n\t\t\t\tos.makedirs(path_name)\n\t\t\texcept: # if parent directory already exists\n\t\t\t\tpath = raw_input('Path'+ path_name +'already exists. (default:'+ \n\t\t\t\t\tstr(datetime.datetime.now())) +'-- (y/N) ')\n\t\t\t\tif path == 'n':\n\t\t\t\t\tpath = raw_input('Path'+ path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if verbose:\n        for entry in processed:\n            entry.add_to_list(set())\n            #if the entry didn't contain any paths\n            if path_name in entry:\n                return False\n                #check if the entry in the set is in the set\n                for entry2 in processed + [path_name]:\n\t\t\t\t\tif path_name in entry2:\n\t\t\t\t\t\treturn False\n\t\n    return True\n\ndef add_path_to_list(path_name_list, path_name_list_sort):\n    \n    \"\"\"\n    Adds an entry to the list of path_names_sort with the path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\ttry:\n\t\t\tfile_entry = open(path_name, 'r')\n\t\t\tentry = dir_scan(path_name)\n\t\t\tfile_entry.seek(0)\n\t\t\tif verbose: print(\"Found %s\" % foldername)\n\t\t\tfile_entry.truncate()\n\t\t\tfile_entry.write(entry[7])\n\t\t\tif verbose: print(\"Stored %s\" % entry)\n\t\t\treturn True\n\t\texcept IOError as e:\n\t\t\treturn False\n\telse:\n\t\treturn False\nfrom __future__ import absolute_import\nfrom __future__"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\tpath = os.path.join(processed, path_name)\n\t\tif os.path.exists(path):\n\t\t\treturn True\n\treturn False\n\n\n# ----------------------------------------------------------------------------\n# Module:     Utils.TestClassFromFile\n# File:       Utils.TestClassFromFile.py\n# Author:      Matthew Zegel\n# Company:    Martin Zegel\n# Contact:     matthew.zegel@gmail.com\n# ----------------------------------------------------------------------------\n\nimport sys\nimport os\nimport re\nimport traceback\nimport types\n\nimport Utils\n\n\nif not Utils.options.test:\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tresult = []\n\t\tresult.extend(vertex3tuple(vertices))\n\telse:\t\n\t\tresult.append(vertices)\n\treturn result\n\n#This returns a list of polygons with faces.\n#It is used to get the polygons as a list.\n#It can be useful to show face faces into a matrix.\n#For example:\n#vertex is a list of [2] elements for the vertex, if it is a vertex, use the first element containing the vector value of the vertex.\n#Here is a matrix for creating an object.\ndef vertexlist(vertices):"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n# create a list of tuples, one for each vertex and zero of every other\nvertices=[[i-1,i] for i in vertices[0]]\n\ndef linestring2pointsequence(linestring):\n\t\"\"\"\n\t\"\"\"\n\tvertices = []\n\tfor line in linestring.split():\n\t\tif not line: continue\n\t\tvertex = [float(v) for v in line.split()]\n\t\tvertices.append(vertex)\n\t#print linestring, vertices\n\tvertices = sorted(vertices)\n\t#print vertices\n\tsequence = [[[0,0,0"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\t#print vertices\n\t\tdef getvertex(v):\n\t\t\ts = [v]\n\t\t\tfor p in vertices:\n\t\t\t\tr = [[-1-r[0],\t-1-r[1],\t-1+r[0],\t-1+r[2],\t-1+r[3]]]\n\t\t\t\tfor z in range(len(p)): #1 and 2\n\t\t\t\t\t#print \"p = \", p\n\t\t\t\t\tfor y in range(len(s)): #1,2\n\t\t\t\t\t\t#print \"z = \", z\n\t\t\t\t\t\tfor x in range(len(v"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tverts = tuple(vertices)\n\t\tline = tuple(line)\n\t\td = numpy.array(([x[1] for x in line]).astype(numpy.float64), dtype=numpy.float64)\n\t\tb = numpy.array(([x[2] for x in line]).astype(numpy.float64), dtype=numpy.float64)\n\t\t\n\t\tfor z in vertices:\n\t\t\tpz = d[:, z]\n\t\tb = numpy.column_stack((b, numpy.column_stack((b, numpy.column_stack((b, numpy.column_stack((b"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\tDAB = float(len(vertices))\n\tABC = float(len(vertices))\n\tBCD = float(len(vertices[0]))\n\tCDAA = float(len(vertices[1]))\n\treturn DAB,ABC,BCD,CDAA\n\t\n\n\ndef dipPoint(pPoint):\n\tfor vertex in pPoint:\n\t\tpFace = point3tuple(vertex)\n\t\tdface = point3tuple(face)\n\t\tp.append(np.array([dface[1], dface[0], dface[2]]))\n\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\ndef vertex6tuple(vertices):\t\"\"\"\n\treturn 6 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nABC, CDA, BCD, CDA -> returns [0,1,2]\n#A    B    A    C    D  -> of vertices [0,1,2]\n#B    B    A    C    D  -> of vertices [0,2,1]\n#B    A    C    D  -> of vertices [0,"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tdijkstra_list = []\n\t\tfor vertex, d in vertices:\n\t\t\tdijkstra_list.append((d[0], vertex, d[1]))\n\t\t\n\t\treturn tuple(dijkstra_list)\n\t\t\n\t\n#Create 3tuples from VP to VP\ndef vertex3tuple_in(vertices):\n\treturn tuple(vertex3tuple(vertices))\n\n#A:    0    1    2    3    4    5    6    7    8    9    10\ndef vertex3tuple_without_duplicates(vertices):\n\tfor vertex in vertices:\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\t#calculate distance from vertices\n\t\tdists = []\n\t\tfor v in vertices:\n\t\t\tdists.extend(edge.distance_point(v))\n\t\tdistances.sort()\n\t\tdists.append(vertices[0])\n\t\tdistances.append(vertices[1])\n\t\treturn dists[0]\n\tdef vertex3str(vertices):\n\t\t\"\"\"\n\t\treturn vertex 3 tuple (vertex1, vertex2, vertex3) for each vertex of the polygon. This will include the vertex and the 2points on both sides of the vertex::\n\t\n\"\"\"\n\t\t#reverse vertices in reverse order of"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tpass\t\t\t\n\n\tdef faceVertexTriple(vertices = None):\t\"\"\"\n\tvertex3tuple can return [] of each vertex which will be an edge in collection of the vertices\n\treturn point on vertex 0 on face, point on vertex 1 on vertex 1 on vertex 2 on vertex 2 on vertex 3 on vertex 3 on vertex 3 on face\n\t\"\"\"\n\t\tpass\n\n\n\n\n\n# Copyright (c) 2013 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tnverts = len(vertices)\n\t\tprint \"Verts: \", nverts\n\t\tprint \"Number of vertices: \", nverts\n\t\tprint \"Vertices : \", vertices\n\t\tprint\n\t\n\tdef countVertex(vertices):\n\t\t# print \"count_vertex:\", count_vertex(vertices)\n\t\t# if len(vertices) == 0: return 0\n\t\tvertex = 0\n\t\tfor u in range(nverts):\n\t\t\tv = vertices[u]\n\t\t\tx = v[0]\n\t\t\ty = v[1]\n\t\t\tz = v[2]\n\t\t\tif (x"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\t# We can treat numbers with the default alphabet\n\t\t# as a special case.\n\t\tprefix = ''\t\t\t\t\t# prefix char indicates a prefix\n\t\talphabet_with_suffix = {\n\t\t\t'my': 'x' + alphabet[0].upper(),\n\t\t\t'x': alphabet[0],\n\t\t\t'C': alphabet[1:],\n\t\t\t'G': alphabet[1:],\n\t\t} \n\n\t\tprefix_string = prefix + number\n\t\treturn prefixes_string.format(prefix, alphabet_with_suffix)\n\n\t@staticmethod\n\tdef bool_to_int(boolean: bool"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\treturn chr(number).zfill(alphabet[0], alphabet[1]) + padding or alphabet[0]\n\n\n@numba.njit(nopython=True)\ndef string_to_int(value: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to an integer.\n\t\"\"\"\n\t\tints = numba.njit(value.encode()).get_result()\n\t\tif len(ints) < 1:\n\t\t\traise NumbaError('expected int or \"name\" value')\n\t\tint_value = ints[0][0"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif not is_numeric(number):\n\t\t\traise ValueError(format(number, \"is not a number, it's %s\") % string_to_num(number))\n\n\t\treturn \"\".join(alphabet[:number] + (padding if padding else \".\"))\n\n\nclass StringEncoder(json.JSONEncoder):\n\t\"\"\"\n\tSerializes string data, and stores it in a file using a\n\tsingle encoding.\n\t\"\"\"\n\n\tdef default(self, obj):\n\t\tif isinstance(obj, dict) and isinstance(obj.get(self.__key), str) \\\n\t\t\tand isinstance"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif padding is None:\n\t\t\tpadding = num_to_string_padding(num_to_letter(string)) + (alphabet and alphabet[0]) or alphabet[1]\n\t\tpadded = get_word_length_tuple(string, num_of_alphabet_chars, padding_amount) # type: int\n\t\treturn get_word_length_tuple(padded, num_of_alphabet_chars, padding_amount)\n\n\tdef string_to_int(string: str) -> int:\n\t\t\"\"\"\n\t\tConvert a string to an integer.\n\t\t\"\"\"\n\t\tif"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif padding == None:\n\t\t\tpadding = alphabet.count(\" \")\n\t\tnumber = format_digit(number, padding)\n\t\t\n\t\talphabet.insert(0, number)\n\t\tnumber = alphabet.ljust(padding)\n\t\treturn number# -*- coding: utf-8 -*-\n\nfrom flask_wtf import Form\nfrom wtforms import StringField\nfrom wtforms.validators import Required\nfrom wtforms.validators.html5 import IS_HTML_REGEX\nfrom wtforms.validators import Length\nfrom wtforms.validators import Regexp\nfrom wtforms.validators import DataRequired\nfrom wtforms"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\toutput = []\n\t\tfor i in alphabet:\n\t\t\toutput.append(str(i) + alphabet[i])\n\t\t\tif padding is not None and len(output[-1]) < padding:\n\t\t\t\toutput.pop()\n\t\treturn \"\".join(output)\n\n\tdef _str_to_number(number: int, alphabet: List[str], padding: int = None) -> int:\t\"Turn a number into a string of digits\"\n\t\tfor i in alphabet:\n\t\t\tif i is not True and not (i is False):\n\t\t\t\toutput = str(number if not i"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tnumber = int(number, self.DEFAULT_PRECISION)    # round() will only make sense if the input is bigger than its precision\n\t\tseparator = '-' * (number + 1) # pad the separators if we cannot round the number by -1\n\n\t\tif padding is None:\n\t\t\tpadding = int(len(number) / 2)\n\t\tif alphabet:\n\t\t\talphabet = [alphabet]\n\t\telse:\n\t\t\talphabet = []\n\n\t\treturn number + separator + alphabet[0]\n\"\"\"\nContains classes for loading image objects\n\n\"\"\"\n\nimport sys\nimport csv\nfrom"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\talphabet = alphabet or [] # make a copy\n\t\treturn ''.join(number.split('-')[:number and 1 or 0])[::-1] + alphabet[number % len(alphabet) or 0]\n\n# NOTE: this is a little silly thanks, but this will work for large strings!\n\n# if string, then encodes in a base58-encoded string, and returns a string to a byte array\n# if string, then will encodes in a base58-encoded string, and returns the decoded string\ndef base58_string_to_bytearray(input_string: str"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif number < 0xFFFF:\n\t\t\treturn \"\"\n\n\t\tpadded = []\n\t\tfor i, item in enumerate(number[::-1]):\n\t\t\tif padding is not None:\n\t\t\t\tpadded.append(padding - len(padded)/i)\n\t\t\tpadded.append(item)\n\n\t\treturn \"\".join(padded[i] for i in alphabet)\n\n\tdef string_to_int(s: str, alphabet: List[str], padding: Optional[int] = None) -> int:\n\t\tif padding is not None:\n\t\t\treturn int(s, 16) * padding\n\n\t\ts ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif number in alphabet:\n\t\t\tnumber = int(number) - (alphabet.index(number) % 2)\n\t\treturn ''.join(alphabet[number].upper())\n\n##\n# A basic abbreviation table for English\n# @param string the English english word\n# @param alphabet the list of possible english word\n@param default: used to fall back to the default\n# @return default_abbr\ndef english_to_string(string: str) -> str:\t\"\"\"\n\tConvert a string to a English (ascii) form.\n\nThe output has all significant"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\t\n\treg_params = flow_params.copy()\n\treg_params.update({register_number: register_value})\n\treg_params.update(reg_params.items())\n\n\n\t# Now, convert into JSON\n\treg_json = reg_params\n\n\tjson.dump(reg_json, open(\"/temp/reg_params.json\", \"w\"))\n\n\n\treturn reg_json\n\n\n\ndef process_conf(conf, target_name, node, file_name):\n\t\"\"\"\n\tProcess and write configuration files in named file_name\n\n\tParams:\n\n\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\ndef _replace_list_to_array_of_values(flow_params, list_of_values):\n\t\"\"\"\n\tReplace all values in the list returned by flow_params key with\n\tvalues specified by list\n\n\t:param flow_params: Dictionary containing defined flows\n\t:param list_of_values: List of values to be replaced\n\t:return: Dictionary of values used in dictionary list_of_values\"\"\"\n\n\t\n\tdef _replace_str_to_array_of_values(flow_params, key):\n\t\t\"\"\"\n\t\tReplace string value in list returned"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_number in flow_params:\n\t\t\tnum = flow_params[register_number][0]\n\t\t\tfor i in range(1, num + 1):\n\t\t\t\tif register_value == register_value:\n\t\t\t\t\traise ValueError('Cannot replace value'+ str(register_number))\n\t\telse:\n\t\t\treg_key = {'add_number' : register_number,\n\t\t\t\t\t'mod_number' : register_number,\n\t\t\t\t\t'sub_number' : 0 }\n\t\t\tflow_params[register_number] = {}\n\t\t\treg_value = {'add_number' :"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\t\t\t\n\t_register = {'reg_number' : register_number,'reg_value' : register_value}\n\treturn _register\n\ndef register_register(flow_params, registered_value):\n\t\"\"\"\n\tRegisters flow with given register parameter\n\nreg_number' : The register number to register\nreg_value' : The reg_value to register\n\n:param flow_params: Dictionary containing defined flows\n:param registered_value: Key to be replaced by registered value\t\"\"\"\n\t\n\t\t\t\n\tregister_number = reg_value = int(flow_params['key"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tout = {}\t\t\t\t\t\t\t\t\t\t\t\t\t# Dictionary containing values in input and output tables\n\t\tfor x in flow_params:\n\t\t\to = x[1][0]\t\t\t\t\t\t\t# The input table name corresponding to this key (i.e. the input name)\n\t\t\tout[o] = register_number # Register number to be replaced by register number\n\t\treturn out\n\n\ndef _get_output(flow_parameters, output_table, output_index, output_header):\n\t\"\"\"\n\tGet output list for the given columns\n\t\"\"\"\n\t# Generate the table header\n\theader"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t_dict_modify([flow_params, register_number, register_value])\ndef _replace_unregister(flow_params):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t_dict_modify([flow_params])\ndef _dict_modify("}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_value:\n\t\t\t# print(\"[reg_register_number=%s register_value=%s], value=%s\" % (reg_register_number, register_value, register_value))\n\t\t\tFlowSet(flow_params['flow_name'], 0, register_value)\n\t\t\treturn True\n\t\telse:\n\t\t\t# print(\"[reg_register_number=%s register_value=%s], value = %s\" % (reg_register_number, register_value, register_value))\n\t\t\treturn False\n\ndef regsave():\n\t\"\"\"\n\tSave to"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tfor key, value in flow_params.items():\n\t\t\tif key in register_array_reg:\n\t\t\t\tregex = re.compile(register_value)\n\t\t\t\tflow_params[key][register_value] = re.sub(regex, register_number, value)\n\t\t\telse:\n\t\t\t\tregister_array_reg_index = list(register_array_reg.keys())\n\t\t\t\tregister_index = register_array_reg_index[register_number]\n\t\t\t\tflow_params[register_array_reg_index[register_index]] = int(register_value)\n\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\n\n\n\ndef _format_flow(flow_params, register_number):\t\t\t\t\t\t\t\t\t\"\"\"\n\tFormats a flow with formating given flow parameters\n\n:param flow_params: Dictionary containing flow parameters for formating\n:param register_number: The number of register where flow value will be stored\n:return: Value formatted data and keys are:\n\t\t - *type* - String - Type of value to be stored in flows\n\t\t - *prefix* - String - Starting point for the prefix of flow\n\t\t\t (optional)\n\t\t - *format* - String - Format to be stored in output string\n\n\t:note"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\n\treturn dict(flow_params, register_number=register_number, register_value=register_value)\n\n\n\nclass FlowFlowManager:\n\t\"\"\"\n\tManage flows to a single device\n\t\"\"\"\n\n\tdef __init__(self, filename, protocol):\n\t\tself.filename = filename #String storing this file name\n\t\tself.config = ConfigParser() #The Configuration object\n\t\tself.config.read(open(filename))\n\t\tself.protocol = protocol #String storing the current protocol\n\t\tself.device_name = os.path.basename(os.path.splitext("}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn value.replace(\".\", \"\\\\.\")\n\ndef get_param(param):\n\treturn param\n\nclass _Param:\n\tdef __init__(self, name, value):\n\t\tself.name = name\n\t\tself.params = value.split(\"=\")\n\t\tself.nparam = len(self.params)\n\t\tself.nparam_str = self.params[0].strip()\n\t\tself.param = \"\"\n\t\t#self.nparam2 = len(self.params[1])\n\t\tself.nparam2 = len(self.params[2])\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn re.sub('[^0-9]', '_', arg) # Replace spaces with /\n\ndef strip_double_quotes(value):\n\t\"\"\"\n\tStrip double quoting characters and all other characters\n\t\"\"\"\n\ts = re.sub('[^\\x00-\\x08\\x0b-\\x0c]', '_', value) \n\treturn ''.join(re.compile(r\"'\\\\'%\\\\\\\"'+s+r+'\"'*'\").findall(value))\n\n# Replace all variables with underscores in the string and return the converted value.\n# @param value String to be replaced\n#"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif value[0] == '.':\n\t\t\tvalue = value[1:]\n\t\telse:\n\t\t\tvalue = value.replace(' ',',').replace(')',';')\n\t\tvalue = value.replace(',',',')\n\t\tvalue = value.replace('\\n','')\n\n\t\treturn value\n\n\tdef get_value_type_field_name(self, obj):\n\t\t\"\"\"\n\t\tReturns the name of the field. Will only process objects with\n\t\tdifferent types. For example, if the type is \"string\", it will\n\t\treturn \"string\"\n\t\t\"\"\"\n\t\treturn obj['@type']\n\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tvalue = normalize(arg)\n\t\targ = re.sub(r'(\\$\\d+|\\d+)', lambda x: re.sub(r'^[\\[\\]])', lambda x: str(x).replace('\\\\','/'), value), arg)\n\t\treturn arg\n\ndef make_dirs(path):\n\t\"\"\"\n\tReturn all dirs in dir_path\n\t\"\"\"\n\tfor folder, __dirs, files in os.walk(path):\n\t\tfor file in files:\n\t\t\tpath = os.path.join(folder, file)\n\t\t\tif path in __dirs:\n\t\t\t\treturn path"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\targ = re.sub('-[a-z]*', '-', arg)\n\t\targ = re.sub('--', '-', arg)\n\t\targ = re.sub(r'-', '_', arg)\n\t\toutput = value and arg +'' or arg\n\t\tvalue = re.sub(r'_', '_', output)\n\t\treturn value\n# Return a list sorted by name, defaulting to alphabetical order\ndef alphabetical_keys(value, arg):\n\treturn [v.replace('-', '_') for v in value]\ndef merge_keys(value_list, order):\n\tfor v in"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\ttry:\n\t\t\targ = arg.replace(\"\\\\\", \"\")\t# Replace all dots with \"/\" or \"/\"\n\t\texcept AttributeError:\n\t\t\targ = os.path.abspath(arg)\n\t\t#print(os.path.abspath(arg))\n\t\treturn arg\n\n\tdef get_value(self, *a, **kw):\n\t\t\"\"\"\n\t\tReturns the value of *a* as a string, or `None` if no value exists.\n\t\tReturns `None` if no value exists, or a list.\n\t\t\"\"\"\n\t\tfor (k, v) in self.defaults:"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tvalue = value.replace('.', '')\n\t\tvalue = value.replace('_', '-')\n\t\tvalue = '\"' + value + '\"'\n\treturn value\n\ndef read_file(path, file_content):\n\tf = open(path, 'r')\n\t# Read all lines from the file\n\tl = f.readlines()\n\tf.close()\n\t\n\t# This is how we read all the lines in the file\n\tfor line in l:\n\t\tif line.strip() == \"\":\n\t\t\t# Skip empty lines\n\t\t\tcontinue\n\t\t\n\t\tkey,value = line.strip().split('=')"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tvalue = value.replace('.', ',')\n\t\tif ',' in value:\n\t\t\tvalue = value.split(',')\n\t\treturn value\n\ndef parse_args():\n\t# Parse command line arguments\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--list-input', '-L', default=False, help='list input files', action='store_true')\n\tparser.add_argument('--list-output', '-S', default=False, help='list output files', action='store_true')\n\tparser.add_argument('--output-dir', '-o', default"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tparam = arg.replace(\".\", os.path.sep)\n\t\tparam = param.replace(\"\\\\\", \"*\")\n\t\t\n\t\treturn param\n\t\t\n\tdef strip_quotes(value):\n\t\tparam = re.sub(r\"(''')([^'.]*)''\", \"'\", value)\n\t\treturn param\n\t\t\n\t\t\n\tdef filter(self, pattern, repl=\"\\n\\[\"):\n\t\treturn re.sub(pattern, repl, self.strip_quotes(self.escape(value)))\t\t\n\t\t\n\tdef escape(self, value):\t\n\t\t# return the string unescaped to make them a safe string\n\t\treturn"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tvalue = value.replace('.', '_').replace(':', '_').replace('*', '_').replace('_', '_').replace('/', '_').replace('\\n', '_')\n\t\tif value:\n\t\t\targ = arg.replace(value)\n\t\t\targ = arg.replace('*', '_').replace('_', '_').replace('/', '_').replace('\\n', '_')\n\t\t\tret = arg.replace(os.sep, '_')\n\t\t\treturn ret\n\treturn value\n\ndef get_input_from_json(input_json_file):\n\tif type(input_json_file) == str:\n\t\tinput_dict = read_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tfor klass in classes:\n\t\t\tif not issubclass(cls, clsbase):\n\t\t\t\tyield klass\n\t\t\t\tcontinue\n\n\t\tyield \"    def __call__(cls):\n\t\t\tyield cls\n\t\tfor cls in subclasses:\n\t\t\tyield cls\n\n\tcls = classmethod(__call__=lambda cls: clsbase(cls))\n\tif verbose_list:\n\t\t# The function __call__ is useful for debugging,\n\t\t# when we know the class name; we can't necessarily call\n\t\t# it directly; we want to see the name that is used.\n\t\tcls.name = \"<unnamed>\"\n\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = cls.__subclasses__()\n\t\tfor base in subclasses:\t\t\n\t\t\tsubclass = getattr(base, '__subclass__')\n\t\t\tif hasattr(subclass, '__subclasses__'):\n\t\t\t\tsubclasses = subclass.__subclasses__()\n\t\t\t\tfor subsubclass in subclasses:\n\t\t\t\t\tfor subsubclass in subclasses[subclass]:\n\t\t\t\t\t\tif hasattr(subsubclass, '__subclasses__'):\n\t\t\t\t\t\t\tsubclasses = subsubclass.__subclasses__()\n\t\t\t\t\t\t\tfor subsubclass in subclasses[subclass]:\n\t\t\t\t\t\t\t\tyield(subsubtype.__name__)\n\t\t\t\t\t\t\t\ndef classmethod_classes_recursive(cls):\t\t\t\t\t\t\n\t\"\"\"\n\tRecursively descend into classmethods and iterate"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = set()\n\n\t\t# This is a little tricky because it is used inside the tests\n\t\t# to determine if the current implementation is the first subclass of a\n\t\t# class\n\t\tfor cls_a in _subclasses:\n\t\t\tif klass_a in mro:\n\t\t\t\tsubclasses.add(klass_a)\n\n\t\treturn subclasses\n\n\t\n\tdef __repr__(self):\n\t\treturn '<{klass.__name__}({subclass.__name__})'.format(**vars(self))\n\n\nclass BaseClass:\n\t\n\tdef __init__(self):\n\t\tself.base_name = None\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn dir(cls)\nclass ClassManager(object):\n\t\"\"\"\n\tReturn all classes of a ClassManager\n\t\"\"\"\n\n\tdef __init__(self, name, mod_dict):\n\t\t\"\"\"\n\t\tInitialise as a ClassManager.\n\n\t\tA ClassManager instance initialized from this (str)\n\t\tobject, or from an existing (object) object with a 'name' attribute\n\t\twhich means it is defined on it, (str).\n\n\t\tClassManager.name = moduleName.\n\t\tClassManager.mod_dict = mod_dict\n\t\t:param name: Class to be"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn (cls,)\n\tdef __getattribute__(cls, name):\n\t\tif name.startswith('__'):\n\t\t\traise AttributeError, name\n\t\telif name == '_Meta':\n\t\t\treturn super(BaseClass, cls).__getattribute__(name)\n\t\telse:\n\t\t\treturn object.__getattribute__(cls, name)\n\tdef __getstate__(self):\n\t\tself._check_signature()\n\t\tif self_class is self:\n\t\t\td = super(BaseClass, self).__getstate__()\n\t\t\tfor k in dir(self_class):\n\t\t\t\tv = getattr(self_class, k, None"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tbases = list(cls.__subclasses__())\n\t\tfor base in bases:\n\t\t\tfor f in cls.mro():\n\t\t\t\tyield f\n\t\n\tdef __init__(self):\n\t\tsuper(Instance, self).__init__()\n\n\nclass Config(object):\n\t\"\"\"\n\tStore configuration related data to the data cache.\n\t\"\"\"\n\tdef __init__(self, module):\n\t\tself.module = module\n\t\tself._data = None\n\t\t\n\tdef __getitem__(self, attr):\n\t\ttry:\n\t\t\treturn getattr(self, attr)\n\t\texcept attr.type_name: \n\t\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tif not cls.__subclasses__.has_key(cls.__name__):\t#it's probably a normal python subclass\n\t\t\treturn cls.__subclasses__[cls.__name__]\n\t\treturn cls.__subclasses__.get(cls.__name__)\n\treturn None\n\n\n# This class handles the logic of loading in the class file\nclass ModuleLoader(object):\n\tdef __init__(self, name, objdir = None):\n\t\t\"\"\"\n\t\tModuleLoader tries to load classes from the objectdir or module\n\t\tspecified via the classname parameter. We'll assume that no module,\n\t\tand it's actually in the"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn inspect.getmro(cls)\n\t\t\n\tdef __init__(self, mclass, attrs = {}):\n\t\t\"\"\"\n\t\tInitialize the attributes of a class.\n\n\t\t:param mclass: The class that we are extending.\n\t\t:type mclass: L{MClass}\n\t\t:param attrs: Dictionary of names of attributes they should contain.\n\t\t:type attrs: C{dict}\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself._mclass = mclass\n\t\texcept AttributeError:\n\t\t\traise TypeError('Invalid value type')\n\t\t\n\t\tfor att in attrs: self._attrs"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tbases = cls.__subclasses__()\n\t\tsubclasses = dict()\n\n\t\tfor base in bases:\n\t\t\tsubclasses_for__mixin__ = subclasses.get(base, None)\n\t\t\tif subclasses_for__mixin__ is not None:\n\t\t\t\tsubclasses[bases_for__mixin] = super(BaseMetaclass, base).__subclasses__()\n\n\t\treturn subclasses\n\n# These classes must implement a __new__ method; classes with no method __new__ must use\n# the metaclass provided as the base class for all class bases\nclass MethodNewMetaclass(type):\n    \"\"\"Implementing the metaclass of a method\"\"\"\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\treturn filter(lambda m: m.__name__ not in cls._bases, subclasses)\n\n\tdef default_subclass(self, obj, class_name, *args, **kwargs):\n\t\t\"\"\"\n\t\tReturns a subclass of another class, defined in the given parent. The subclass class is set with the class name for the\n\t\tsubclass; a module class name may get used and this method must not be overridden\n\t\t\"\"\"\n\t\treturn getattr(obj, class_name)(*args, **kwargs)\n\t\t\n\tdef default_parent(self, obj, class_name, *args, **kwargs): \n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tmin_digit_num = 1<<i\n\t\tmax_digit_num = 9<<i\n\t\tword = 0\n\t\tfor c in alphabet:\n\t\t\tif c in string:\n\t\t\t\tif len(string) >= max_digit_num:\n\t\t\t\t\tword = max_digit_num\n\t\t\t\t\tbreak\n\t\t\t\telif letter_num < 0:\n\t\t\t\t\tword = letter_num\n\t\t\telif min_digit_num < 0:\n\t\t\t\tword = alphabet.count(c)\n\t\t\t\tif word > min_digit_num:\n\t\t\t\t\tword = min_digit_num\n\t\t\tif word >"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tnum_chars = string[:3]\n\t\tnum_chars = string[3:6] + '1' + num_chars\n\t\tnum = int(''.join(num_chars))\n\t\treturn num\n\t\ndef int_to_string(string: str, alphabet: List[str], num: str) -> str:\t\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\t\n\t#if string[0] == '0': num = ''\n\tif len(string"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn ''.join(str(x) for x in alphabet[:-1])+'.'+alphabet[-1]\n\ndef char_count(x: str, alphabet: List[str]) -> int:\t\"\"\"\n\tReturn the number of characters in the string.\n\nArguments:\n\tx\t\tA string containing characters\n\talphabet\tA list of chars.\n\tReturns:\n\t\tThe number of characters in the string.\n\t\"\"\"\n\t\tif x: # Remove a carriage return, then split it and return it\n\t\t\ttry:\n\t\t\t\treturn int(string[ : x.find"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\t\n\tdigits = \"\".join([random.choice(letters) for _ in range(6)])\n\n\tinteger_to_string = []\n\tfor c in digits:\n\t\tif c in string:\n\t\t\tinteger_to_string.append(string[:c]+string[c+1])\n\n\treturn integer_to_string\n\nfrom datetime import datetime\nfrom urlparse import urlparse\nfrom django.conf import settings\nfrom django.db.models import Count\nfrom django.db.models.fields import DateField\nfrom django.db.models.related import RelatedObjectMixin\nfrom django.db."}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\tdigits = []\n\tfor i in range(len(string)):\n\t\tnumber = _string_to_int(string[i], alphabet)\n\t\tdigits.append(number)\n\treturn digits\n\ndef _string_to_int(string, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert the provided string to a number, using the given alphabet, if it's already\n\tpresent, or if it's not then, create a new integer.\t\"\"\"\n\t\t\n\t\n\tif alphabet is not None:\n\t\tfor c in alphabet:\n\t\t\tstring = string."}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn int(''.join(string.splitlines()), self.alphabet_map[alphabet, 0])\n\n\nclass NumberConverter(object):\n\tdef __init__(self, min: int, max: int, alphabet: List[str]) -> None:\n\t\tself.min = min\n\t\tself.max = max\n\t\tself.alphabet_map = {\n\t\t\t'o1.0.0': 1,\n\t\t\t'p1.0.0': 2,\n\t\t\t'p2.04.05': 3,\n\t\t\t'p3.02.02.05': 4,\n\t\t\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tnumber = int(''.join([c for c in alphabet]))\n\t\tif number < 1:\n\t\t\traise NameError(string)\n\t\treturn number\n\ndef get_list_dict_from_dict(dict_str: str) -> Dict:\t\t\"\"\"\n\tReturn a dict with keys as keys and values as values. The string\n\tto be returned is the value of the desired number in the dictionary.\n\t\n\tNote: You should have provided the dictionary as a string!\n\n\t@return: A dict containing the keys as keys and the values as values\n\t\"\"\"\n\t\tdict_dict ="}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tnumbers = [digits[0]] * len(alphabet)\n\t\tfor i in xrange(len(alphabet)):\n\t\t\tif int(string[alphabet[i]]) <  0: numbers[i] = 0\n\treturn numbers[0]\n\n#######################################################################################################\ndef write_file(file: File) -> None:\n\twith open(file, \"a\") as output:\n\n\t\tfor i in xrange(4):\n\n\t\t\tfor letter in xrange(len(alphabet)):\n\t\t\t\toutput.write(\"\\n#{0}  {1}\\n\".format(alphabet[letter], i+"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\t# This is the main function\n\t\talphabet.sort()\n\n\t\tdigits = ''\n\t\tnew = \"\"\n\t\twhile len(digits) == 0:\n\t\t\tif  len(string) == 1:\t\t# We got a single null character\n                # Make sure that we have a non-empty string as its character\n                char_first = digits[0]\n\t\t\t\tnew += char_first;\n\t\t\t\tdigits = digits[1:]\n\t\t\t\tbreak\n\t\t\telif len(digits) == 2:\t# We got a character which has no two digits\n\t\t\t\tnew += digits[0]\n\t\t\t\tdigits"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n#================================================================**#\n#\tHelper functions for parsing XML files\n#\n#================================================================**#\ndef get_xml_from_file(path: str, file: str) -> list:\n\twith open(path, 'r') as file:\n\t\txml_text = file.read()\n\txml_result: List[int] = []\n\tfor element in xml_text.split('\\n'):\n\t\txml_result.append(_parse(element, file))\n\treturn xml_result\n\ndef get_xml_from_file_without_comments() -> list:\t\"\"\""}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\ttry:\n\t\tprint(url)\n\t\twith urllib.request.urlopen(url) as f:\n\t\t\tsrc_tar = file(f).read()\n\t\twith tarfile.open(target=destination_path, mode='r:gz') as t:\n\t\t\tfor root, dirs, entries in t:\n\t\t\t\t# print(root, dirs, entries)\n\t\t\t\t# extract_dir = dest_tar + \"/\" + entry\n\t\t\t\textract_dir = extract_dir + \"/\" + os.path.basename(root)\n\t\t\t\tshutil.rmtree"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef main():\n\t\t\"\"\"\n\t\tGiven an url and a destination path of the source package, check if it's a valid or an existing archive\n\t\tand extract the.tar.gz from that directory.\n\n\t\tArgs:\n\t\t    url: url of the source package\n\t\t    destination_path: folder path of the archive\n\t\t    check: if True then do not check for an existing archive\n\t\t\n\t\t    Returns:\n\t\t    a file path on disk where the archive has be extracted.\n\t\t\"\"\"\n\t\tsource_path = \"/home/nukesk/Downloads/\"\n\t\tif not os.path.isdir(source_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n    path = destination_path.rstrip('/') + '/opt/core*' + str(config.GetArch()) + '-' + str(config.GetTargetDir()) + '/' + config.GetTargetName() + '-' + config.GetDependsOn() + '/' + config.GetDependsOn() + '/' + 'core_*' + config.GetArch() + '.tar.xz'\n    print(\"Reading url to \" + path)\n    r = requests.request('GET', url)\n    raw_tar = r.content\n\textract_file(path, raw_tar)\n\n\nif __name"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef install_files(url, destination_path, **kwargs):\n\t\"\"\"\n\t\tGet all the files under url, extract all files and install them into it.\n\t\tIf the URL is an archive tar format then the extracted items need to be downloaded from it.\n\t\t\"\"\"\n\turl = path_in_format(url, 'archive')\n\tdestpath = path_in_format(destination_path, 'tar.gz') \n\tfiles = get_extract_path(destpath)\n\tif not files:\n\t\treturn destpath\n\tfor fn in os.listdir("}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\t\n\tdef get_path(url, destination_path):\n\t\tr = requests.get(url)\n\t\tpath = r.headers['location']\n\t\twith open(destination_path, \"wb\") as f:\n\t\t\tfor chunk in r.iter_content(chunk_size=80000):\n\t\t\t\tf.write(chunk)\n\t\treturn path\n\t\t\t\n    def remove_zip(path, delete_path=True):\n        \"\"\"\n\tDelete zip file from the archive specified.\n\tIf delete_path is True, directory will be removed after the zip is done.\n\t\"\"\"\n\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\t# Downloads only compressed tar.gz files from 'core' to 'extra' for the current directory\n\tfor root, dirnames, ifile in os.walk(destination_path):\n\t\t\tif not ifile.startswith('.'):\t\t\t# Skip non-archive files\n\t\t\t\tcontinue\n\t\t\ttarball_file = os.path.join(root, ifile)\n\t\t\tif not tarball_file.endswith('.tar.gz'):\n\t\t\t\twith open(tarball_file, 'rb') as fd:\n\t\t\t\t\ttar_content = fd.read()\n\t\t\t\t\tprint 'Downloading %s...' % tarball_file"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\t\n\twith open( destination_path, 'wb') as dest:\n\t\tshutil.copyfileobj( requests.get( url ).content, dest )\n\t\t\n\t#print( 'Done retrieving files from {0}'.format( destination_path ) )\n\t\n\treturn destination_path;\t\t\t\t\n# -*- coding: utf-8 -*-\n\"\"\"\nTests for django.contrib.auth\n~~~~~~~~~~~~~~~\n\nThis file tests whether django.contrib.auth is initialized correctly if\nit is correctly loaded. Then this test will only be run once on import. It checks that\nthe settings in the settings.py file has"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ttry:\n\t\t\tprint(u\"Downloading {0}...\".format(destination_path))\n\n\t\t\tdownload = requests.get(url)\n\n\t\t\tif download.status_code!= 200:\n\t\t\t\tprint(u\"HTTP 200 code: {0} response: {1}\".format(download.status_code, download.content))\n\t\t\t\treturn\n\n\t\t\tdata = download.json()\n\n\t\t\tif not data:\n\t\t\t\traise Exception(\"No JSON data!\")\n\n\t\t\tprint(u\"Data extracted to {0}\".format(destination_path))\n\n\t\t\twith tarfile.open(destination_path, '"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n    # TODO: Implement get_archive_path() functionality from ArchTarArchive\n    def get_archive_path (url, dest_path):\n        \"\"\"\n        Extract a.tar.gz archive from dest_path on disk.\n        \n        Args:\n        url: url of the.tar.gz archive to extract\n        dest_path: where to extract extracted version, usually path from dest_path\n        \"\"\"\n        print '\\t\\tDownloading archive:'+ url\n        file = urllib.urlopen(url)\n        raw_data = file.read()\n        return raw_data\n\n    # download_path = \"/dir/dl-\""}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef GetTarballArchive(url, destination_path):\n\t\"\"\"Retrieves a new *.tar.gz archive with a path and a binary file with its.tar.gz extension (compressed) as\n\tthe tar module.\n\t\"\"\"\t\n\t\n\tprint \"Downloading {}...\".format(url)\n\tdl(url, destination_path,  \"bz2\")\n\t\ndef ExtractPackage(url, destination_path, package_prefix=None, check=True):\n\t\"\"\"Extracts an archive as a tar file\"\"\"\t\t\n\t\n\tprint \"Extracting archive as {}\".format(url)\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\ndef __create_config(port_dir):\n\n\tsys.path.append(port_dir)\n\n\n\tos_id = \"MacOS\"\n\tprint \"Starting port check\"\n\tprint \"port id %s (os id): %s\" % (port_dir, os_id)\n\tcheck_for_os(os_id)\n\n\tif os.path.isfile(os.path.expanduser(\"config/ports/id\")):\n\t\tsys.path.append(os.path.expanduser(\"config/ports/id\"))\n\telif os.path.isdir"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef file_exists(filename):\n\t\"\"\"\n\tReturns True if file exists, false if not\n\t\"\"\"\n\treturn os.path.isfile(filename) is True\n\t\ndef is_mac():\n\t\"\"\"\n\tChecks if the os is Mac OS or Linux\n\n:return: True is MACOS\n:rtype: bool\t\"\"\"\n\t\ndef check_directory(directory):\n\t\"\"\"\n\tChecks if there exists any file on the OS from directory\n\t@param directory: the path to the directory to be checked\n\t\"\"\"\n\tfor filename in os.listdir(directory):"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t       \n\tdef __init__(self, archipack, configdict={}, useriv4=False):\n               \n\t# this function calls self.is_mac_os==True and\n\t# then checks if the archipack is found\n\tdef is_mac_os(self):\n\t\treturn self.check_arch_is_mac_os()\n\t\n\t# this function runs a function that returns True if the function\n\t# is valid and returns False otherwise\n\tdef check_arch_is_mac_os(self):\n\n\t\tif(not self.is_mac_os"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\n\ndef is_mac():\n\t\"\"\"\n\tChecks if True if the OS is macOS\n\t\"\"\"\n\t\n\treturn os.name == 'MacOS'\nimport numpy as ma\nfrom sklearn.datasets import load_iris\nimport numpy.linalg as LA\nfrom sklearn.linear_model import LogisticRegression\nfrom nilearn import CriticNeuralNet\nimport torch\nimport PIL.Image\nimport os\nimport pdb\nimport time\nimport argparse\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"data_dir\", help=\"data directory where"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n# Check if the os is Linux\ndef isMacOS(): return os.name == \"linux\"\n# Return True if the os is macOS\ndef isMac(): return os.name == \"mac\"\n\n# Return a tuple containing the path of the directory \ndef pathAndExt(dir):\n\t\n\tdir, ext = os.path.splitext(dir)\n\t\n\treturn dir, ext\n\n# Return a tuple containing the full path to the folder \ndef fullPath(dir):\n\t\n\tdirectory = dir.split(\"/\")\n\t\n\treturn directory[0], directory[1]\n\t\n\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ttry: \t\tcheckMacOS(address_string)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\texcept Exception as e:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tprint \"[!] - Check if MACOS failed.....\\n\"\n\t\t\t\traise\n\t\t\t\n\t\treturn True\t\t\t\t\t\n\tdef checkOS(self, address):\n\t\ttry:\n\t\t\tcheckOS(address)\n\t\texcept Exception as e:\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tprint \"[!] - Check OS failed.....\\n\"\n\t\t\t\traise\n\t\t\t\n\tdef setMacOS(self, value):\n\t\"\"\"Checks if the os is macOS or not\n\t\"\"\"\n\tif"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef init_dir(user, dir):\n\t\"\"\"\n\t:param user: user name\n\t:param dir: directory\n\t:return: path to the directory\n\t\"\"\"\n\tos.chdir(user)\t\n\treturn dir\n\ndef isWindows():\n\t\"\"\"\n\tChecks if the is a Windows user file. On first check for windows users\n\tand pass it to the OS\n\t:return: OS's is a Windows user\n\t:rtype: bool\t\"\"\"\n\t\ndef isMacOS():\n\t\"\"\"\n\tChecks if the is a Mac OS user file."}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\tdef checkMacOS(self):\t\t\"\"\"\n\t\tCheck if Mac OS is installed\n\n\t\t:return: True if Mac OS is installed\n\t\t:rtype: bool\t\"\"\"\n\t\n\n\tdef checkDir(self):\t\t\"\"\"\n\t\tCheck if the dir is present and is a directory\n\t\t\n\t\t:return: True if dir is found\n\t\t:rtype: bool\t\"\"\"\n\t\n\n\t#------------------\n\tdef download_url(self,url):\n\n\t\t\"\"\"\n\t\tDownload url to download the package\n\t\n\t\t:Parameters:\n\t\t\turl: str\tDownload url \n\t\t:Keywords:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\t\t\nclass AppServerScript:\n\n\tdef __init__(self, parent):\n\n\t\tself.os_path = os.path.abspath(\"../../\")\n\t\tself.parent = parent\n\t\t\n\t\tself.windows_path = appserver_constants.WINDOWS_PATH\n\t\n\t\tself.cmd = None\n\t\tself.proc = None\n\t\t\n\tdef checkProcess(self):\n\t\n\t\t#Get the proc name\n\t\tp = self.parent.name\n\t\t\t\t\n\t\t\n\t\tm ='m'\t\t\t\t\n\t\t#Make sure the string of the \"win\" process is the correct one\n\t\tif(not os"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\n\t###\n\t### def is_system_system_macOS(self, target):\n\t\t##return os.path.isabs(target)\n\t\t###return os.uname()[0] in [\"Darwin\",\"Linux\",\"Windows\"]\n\t\n\n\n\tdef is_system_darwin(self, target):\n\t\tif(self.is_system_system_macOS(target)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef do_build(self, targets):\n\t\tdef is_macOS(target):\n\t\t\tif(self.is_macOS(target)):\n\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n    regexes = _get_regex(*item.keys())\n\tfor regex in regexes:\n\t\titem[regex] = convert_regex(regex)\n    return item\n\ndef _get_regex(patterns):\n    \"\"\"Convert pattern (glob) keys to regex keys.\n\n\t:param patterns: dict to convert dict keys to regex keys\n\t\n    Returns:\n        list of regexes, sorted by keys \n\t\"\"\"\n    regexes = []\n    for p in patterns:\n        regex = p\n        for pattern in _get_list(patterns):\n            regex = regex.union(pattern.values())\n        re"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t\n\ndef get_values(ruledict):\n\t\n\n    '''\n    Convert ruledict to regular python syntax\n    '''\n       \n    values = False\n\n    if hasattr(ruledict, 'keys'): # if so, read in all keys\n        for key in ruledict.keys():\n            if key in regex_keys:\n                    values = True\n                    values = False\n                print '\\n%s => %s' % (key, value_string)\n \n    if values:\n        try:\n            # check if item was a regex\n            pattern = re.compile('^' + '|'.join(list(set(ruledict[key]))))  \n\t   "}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef read_json(fpath):\n    \"\"\"Read the JSON file in fpath and convert it to regex keys and return the keys as dict and the regex keys as regex\n\n\"\"\"\n    data = open(fpath, 'r').read()\n    try:\n        key_keys = json.loads(data)\n    except ValueError as e:\n        raise error('Cannot parse {} file at {}: {}'.format(fpath,fpath,e))\n    for key,regex in key_keys.items():\n        yield key, regex\n\ndef rename_pattern(filename, pat):\n    \"\"\"Rename the given file with a regex pattern\"\"\"\n   "}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef regex_dict(rules):\n\t\"\"\"\n\tConverts all keys in rules into regexes\n\n\tArgs:\n        rules: dict keys\n\tReturns:\tdict with keys converted to regexes, where dict is converted to regexes\n\t\"\"\"\n\t\n\t\t\n\tregex_codes = []\n\tfor key in rules.keys():\n\t\tif key.startswith('a'):\n\t\t\tregex_codes.append(re.compile(key[1:]))\n\t\telse:\n\t\t\tregex_codes.append(re.compile(key))\n\t\n\t\t\n# This loop will convert all keys in regular expressions to regular"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t\n\twith open('keys.txt', 'r') as f:\n\t\t\tkeys = map(lambda x: re.compile('^' + x + '$').sub(\"\", x).group(0), f.readlines())\n\treturn keys\n\n# Generate a function that converts all keys in item to regex keys\ndef dict_to_regex_keys(item):\n\twith open('rules.txt') as f:\n\t\t\tkeys = map(lambda x: re.compile('^' + x + '$').sub(\"\", x).group(1), f.readlines())\n\treturn keys\n\n# Replace"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef get_regex_dict(text, regex_dict):\t\n\t\"\"\"\n\tConvert a string into regex keys\n\treturns a dict which must be converted to regex keys\n\t\"\"\"\n\tdict_key = None\n\tregex = {} # for making use of regex\n\tfor r in regex_dict:\n\t\tif re.match(r, text):\n\t\t\tdict_key = r\n\n\tif dict_key is not None:\n\t\treturn dict(regex)\n\treturn None\n\t\nclass Rule(object):\n    \"\"\"\n\tObject representing a single rule in a config file\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n# ----\n# Make RegexDict to use. Does not need to change because it's a dictionary\n# is now a regular dictionary\n\ndef normalize_keys(keys):\n\t\"\"\"\n\tTake dict of regular expression. Convert any regular expressions in the\n\tdictionary to regex keys.\n\n\tArgs:\n\t\tkeys: dictionary for the regular expression to normalize\n\t\toutput: regular regex dictionary of regular expression keys\n\t\"\"\"\n\t\n\t# ----\n# Make dict to use to convert non-regex keys\n\ttemp_dict = {}\n\tfor key in keys.keys():\n\t\tif isinstance("}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef regex_keys_to_regex(matchobj):\n\t\n\tfor c in matchobj:\n\t\tif c[:1] == c[1:]:\n\t\t\treturn c.replace('*', r'\\$')\n\t\telse:\n\t\t\treturn c.replace('[^/', r'\\/]')\n\n\tfor c in matchobj:\n\t\tif c[:1] == c[1:]:\n\t\t\tcontinue\n\t\telse:\n\t\t\treturn regex_keys_to_regex(matchobj[c])\t\t\n\t\ndef rules_to_keyvals(rules):\n\n\t\"\"\"\n\tReturns a list of all key-"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef read_cpp_list(filepath):\n\tif isfile(filepath):\n\t\t\tc = CppList(csv_file = open(filepath, 'r'))\n\t\t\tf = open(filepath, 'r')\n\t\t\treturn c.header_names\n\treturn {}\n\ndef filter_keys(keys):\n\t\"\"\"\n\tExtract all keys to make the key and a regexes.\n\n\tArgs:\n\t\tkeys: dict to extract all keys to make the key\n\t\n\tReturns:\n\t\tdict with keys extracted to regular expression\n\t\"\"\"\n\t\n\tkey_dict = read_cpp"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n    regexes = regex_dict(items)\n       \n    if not isinstance(regexes, dict):\t#If the argument does not have a dictionary, the entire dict returned includes \"None\" to ignore\n        return {key:regexes for key in regexes.keys() if key not in regexes}\t#Return only the key 'None' from the regex dict\n    \nif __name__ == '__main__':\n    file_root_path = 'py_keyserver-testfiles.json'    \n    print \"Finding file(s)|\"\n    #print file_root_path\n    file_root_"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tif name[0] == '\"' and len(name) > 1:\n\t\t\tname = name[1:-1] # remove leading single-quotes in name\n\t\t\treturn name\n\n\tdef getStringFromName(self, name):\n\t\tif self.name == name: return name\n\t\telif re.search('[a-zA-Z0-9]', name) and name!= \"__\":\n\t\t\treturn name\n\t\telse:\n\t\t\treturn name + \"_\"\n\n\tdef _makeString(self, data, replace = \"\"):\n\t\tif replace==\"\":\n\t\t\treturn data\n\t\telif replace==':"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tquote_match = re.compile(r\"\\'\"+re.escape(name.replace('\"', '\\\\\"')))\n\t\treturn quote_match.sub(r'', name)\n\n\n\n#-------------------------------------------------------------------------\n#\n# Functions\n#\n#-------------------------------------------------------------------------\n\ndef remove_names(name_list, names_to_remove):\n\t\"\"\"\n\tRemoves duplicates from an array name_list and returns new list\n\t\"\"\"\n\tfor name in names_to_remove:\n\t\ttry:\n\t\t\tnames_to_remove.remove(name)\n\t\texcept:\n\t\t\tpass\n\treturn name_list\n\n\n\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\treturn re.sub(\"#.*+\",\"\",name)\n\t\t\ndef strip_url(url):\n\t\"\"\"\n\t\tReplace all occurrences of quote around url \n\t\t\"\"\"\n\t\tif re.search(r\"(/[^/]*?#.*)\", url)!= None:\n\t\t\turl = re.sub(r\"(/[^/]*?#.*)\",\"\",url)\n\t\tif url[0]!= '@' and url[0]!= '(':\n\t\t\turl = '(' + url + ')'\n\t\treturn url\n\ndef make_html_list(text):\n\t\"\"\"\n\t\tMakes a list"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tname= re.sub('(?:\\\\\\+|\\\\\\\\|\\\\/|[^/])*\\\\\"', '', name)\n\t\treturn frappe.get_doc(doctype, name)\n\ndef get_system_managers(doctype):\n\treturn frappe.db.get_single_value(\"System Manager\", None, \"enabled\")\n\ndef get_user_managers(roles):\n\troles_dict = {}\n\tfor role in roles:\n\t\troles_dict[get_user_fullname(role)] = role\n\t# check for system managers with full names\n\tfor role in get_all(doctype,"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\treturn _re.sub(r\"(?i)\" + _sunc(stringify),\n\t\t\t\t\t_re.sub(\"(?i)\" + _sunc(wordwrap), _re.sub(wordwrap,\n                                                                               r\"\\1\",\n                                                                               stringify)))\n\n\ndef get_words(source_file=None, regex=stringify):\n\t\"\"\"\n\tGiven a file containing the regex in the source, parse them\n\tas words.\n\t\"\"\"\n\t\treturn _re.findall(source_file, re.DOTALL | re.MULTILINE | re.IGNORECASE).findall(regexes)\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tparts = re.split(r'\\[\\]', line)\n\t\tif parts == name:\n\t\t\tcontinue\n\t\tparts = parts[0].split('\"')\n\t\tif len(parts) == 2:\n\t\t\tparts[0] = parts[1].strip('\"')\n\t\t\tname = parts[0]\n\t\t\tcontinue\n\n\ndef process(line, name='', index=None):\n\t\"\"\"\n\tProcess an unterminated line and return it as a dictionary.\n\tIf index is given the lines are indexed by the index of the named match.\n\tThe string with empty characters can be converted as the first"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tline_re = re.compile('^\"(\\S+)\\\"\", re.IGNORECASE)\n\t\treturn re.sub(line_re, \"\", line)\n\ndef remove_string_list(lines):\n\t\"\"\" Removes string(s) from given list of lines, remove unwanted quotes. \"\"\"\n\tline_re = re.compile(\"(.*?)\\\"\", re.IGNORECASE)\n\treturn re.sub(line_re, '', lines)\n\ndef get_first_line(line):\n\t\"\"\" Returns the first line of given string. \"\"\"\n\tline_list = line.split('\\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tname = re.sub(r'\"([^\"\\\\]*)\",\"\",\"\"\"\").replace(name,\"<br/>\")\n\t\tname = re.sub(r'[\\\\\"]',\" \", name, 1, flags=re.DOTALL).lower()\n\t\tname = re.sub(r\"'\",\"\", name)\n\t\tname = re.sub(r\"''\",\"\", name.lower(), 1, 0 ) \n\t\tname = re.sub(r'\\\"',\"\", name, 1)\n\t\tname = re.sub(r'\"',\"\",name.lower(), 1, 0 ) \n\t\tname = re.sub"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tstrName = string.lower(name)# remove double quotes, since they are too unsafe\n\t\twhile re.match(\"\\$\\{([A-Za-z_][A-Za-z0-9_]*):(.*)$\", strName):\n\t\t\tstrName = re.sub(\"\\{\\{([A-Za-z_%-_]{2,})([\\}\\)]{3})\", '', strName)\n\t\treturn strName\t# remove quotes\n#\n# Given a list of strings 'name', return a list of nouns between\n# lowercase and uppercase, such as \"The Am I"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ta = name.replace('$', '')\n\t\ta = re.sub(r'\\A' + '^' * len(a), '', a)\n\t\treturn a\n\n\n\ndef main( args ):\n\timport argparse\n\n\tparser = argparse.ArgumentParser(description='Remove quotes from a text file.')\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help='Display verbose output.')\n\tparser.add_argument('-s', '--smart', dest='smart', action='store_true', help='smart quotes to add to the file.')\n\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tself._parse_flags(_shlex.split(self.get_option('flags')))\n\t\tself._str_flags()\n\tdef _str_flags(self):\n\t\t\"\"\"Process a list of flags and return a list of arguments\"\"\"\n\t\t# This needs to go in a global list before doing anything else\n\t\t# in case one thread has some flags but it doesn't want anything to\n\t\t# be passed, yet we don't need that way yet.\n\t\tif hasattr(_subprocess, 'argv'):\n\t\t\t_subprocess.argv[0] = self\n\nclass CmdstrSub(object"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nclass PlistParser(shlex.Parser):\n\t\"\"\"\n\tBase class for parsing command-line arguments in cmd-line arguments.\n\t\"\"\"\n\n\tdef __init__(self):\t## A quick and dirty way to pass around a list\n\t\tself.args = []\n\n\tdef getargs(self, raw=False):\n\t\t\"\"\"\n\t\tOverride from shlex.parse()\n\t\tIf raw == True, the string being parsed will return a list\n\t\tif raw == False, the returned list will always contain\n\t\tstrings instead of the actual strings passed to __init__\n\t\t\"\"\"\n\t\t\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\t# default to None from the platform command line\n\t\tplatforms = None\n\t\tdefault_platform = \"this\"\n\n\t\t# platform strings are case-insensitive\n\t\tsystem_platform = shlex.split(platform)\n\t\tif len(system_platform) > 1:\n\t\t\tsystem_platform = system_platform[1:-1]\n\t\t# strip out the case-stated platforms\n\t\tif \"win\" in system_platform:\n\t\t\tsystem_platform = user_choice(system_platform)\n\t\tif default_platform!= user_choice(system_platform):\n\t\t\tlogger.warning(\"-"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tif re.search(regEx, argv):\n\t\t\treturn argv\n\t\telse:\n\t\t\t# if we found the first arg, pop the last one except the last one.\n\t\t\treturn [x for x in sys.argv[1:]\n\t\t\t\t  if x.startswith(os.path.sep)]\n\t\"\"\"\n\tFor Linux, all platforms use shell=True.\n\t\"all\" takes the same code as shlex.split() for the whole command in a single\n\tcommand\" or an empty list of arguments.\n\t\"\"\"\n\tif \"shell\" not in os.environ:\n\t"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\t\n\tdef __init__(self, args,**kw):\n\t\tsuper(shlex.split, self).__init__(args, **kw)\n\t\tself.platform=0\n\n\tdef split(self,str):\n\t\tl=[]\n\t\tif self.platform=='auto':\n\t\t\treturn l\n\t\tfor c in str:\n\t\t\tif not c:\n\t\t\t\tcontinue\n\t\t\n\t\t\tl.append(c.split())\n\t\treturn l\n    \n\tdef argsplit(self,*args,**kw):\n\t\treturn super(shlex.split,self).argsplit(*args,**kw)\n\n \n\n\ndef _"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\t__version__=-1\n\t__author__=\"\"\"Noody Hong\n\tCopyright (c) Noody Hong\n\t\"\"\"\n\t__copyright__=\"\"\"Copyright (c) Noody Hong\n\t\"\"\"\n\t# We're using a subprocess rather than fast one.\n\tdef __init__(self):\n\t\tself.p = None\n\t\n\tdef __del__(self):\n\t\ttry:\n\t\t\tself.p.terminate()\n\t\texcept:\n\t\t\tpass\n\t\t\t\n\t\t# Prevents pywin.err and WinError from being created.\n\t\ttry:\n\t\t\tdel sys"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ttry:\n\t\t\tproc = subprocess.Popen([\"platform\"], stdout=subprocess.PIPE)\n\t\t\t(out, _) = proc.communicate()\n\t\texcept Exception as e:\n\t\t\traise Exception(str(e))\n\t\t\t\n\tdef __init__(self, args):\n\t\t# Process args - see multiprocessing.core.Process.__init__()\n\t\tfor a in args:\n\t\t\tif a.find(\"--\")!= -1:\n\t\t\t\tself.args.append(a)\n\t\t\telif not re.match(r\"^[\\.\\]]+$\", a):\n\t\t\t\traise Exception(\"Invalid argument in sub-"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ttry:\n\t\t\treturn shlex.split(args)\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\tparts = shlex.split(str(args), posix=False, cmd=True, posix2=True)\n\t\t\texcept ValueError:\n\t\t\t\tparts = [str(arg).replace('\\\\ ', '/') for arg in args.split()]\n\t\t\t\t# This is ugly and can happen when used with multiprocessing,\n\t\t\t\t# so make it one that does not have support for shlex that can\n\t\t\t\t# be called from another thread\n\t\t\t\tparts = list(parts) # flatten list\n\t\t\t\targs =''.join("}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tdef __init__(self, args): \n\t\t\t\tself.args = args\n\t\t\tif not len(args):\n\t\t\t\tself._platform = 'this'\n\t\t\n\t\tdef __getattr__(self, attr):\n\t\t\t\tif 'platform' not in attr.__slots__:\n\t\t\t\t\treturn attr\n\t\t\t\treturn getattr(self.args, attr)\n\t\t\n\t\tdef split(self, args):\n\t\t\t\tl = []; a, b, c = '', 0\n\t\t\t\ts = ''\n\t\t\t\twhile True:\n\t\t\t\t\tif s == '':\n\t\t\t\t\t\ts = raw_input('--> ')\n\t\t\t\t\tif a > self.args.count"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tprint \"Platform:\",platform,\"for command-line splitting:\"\n\t\tprint \"\\n\\tCommand:\",shlex.split(s, posix=POSIX)\n\t\tprint \"\\tCurrent platform:\",platform\n\t\tprint\"\\t\",shlex.split(s)\n\t\t\n\t\tif \"this\" in sys.builtin_module_names and s[:1]=='win ':\n\t\t\ts=\"\\\"\"+s[1:]\n\t\telse:\n\t\t\ts=\"\"\n\t\tprint \"shlex.split() started with\",shlex.split(s)\t\t\t\n\t\tret=\" \".join(shlex.split(s))\n\t\treturn ret"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\twith open(archive_path) as compressed_file:\n\t\t\t\treturn compressed_file.read()\n\n\t\n\tdef unzip_source(self, archive_path):\n\t\t\"\"\"Uncompress the contents of the archive_path.\t\"\"\"\n\t\twith working_dir_path(self.output_dir, archive_path):\n\t\t\t\tout = subprocess.Popen(['unzip', self.get_target_path(archive_path)], shell=True, stdout=subprocess.PIPE, shell_ bufsize=0)\n\t\t\t\tout.stdin.write(self.get_uncompressed_bytes())\n\t\t\t\tout"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef createTar(repo, archive_path, origin_url):\n\t\"\"\"\n\tCreate the tar file referenced by the archive_path in question.\n\tIt returns'repo' at the location given by origin_url\n\t\"\"\"\n\tcmd = ['tar', '-czf', '%s/%s' % (repo, archive_path)]\n\tsubprocess.check_call(cmd)\n\treturn'repo'\n\ndef extractTar(repo, origin_url):\n\t\"\"\"\n\tExtract the tar file referenced by the url origin_url from a tar file archive_path.\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\twith open(archive_path, \"rb\") as inf:\n\n\t\ttry:\n\t\t\t# try getting the contents of the archive\n\t\t\tresponse = urllib2.urlopen(urlpath + \" -d +x\") + inf.read()\n\t\texcept Exception as ex:\t\n\t\t\tprint \"Archive extraction failed\"\n\t\t\tprint ex\n\t\t\tprint response\n\t\telse:\n\t\t\tresponse = response.decode(\"utf-8\")\n\t\t\twith open(urlpath + \" -l -f -r \" + resp.decode(\"utf-8\")) as inf:\t\n\t\t\t\tfor line in inf:"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\tsrc_dir = os.path.dirname(os.path.realpath(archive_path))\n\t\tif not os.path.exists(src_dir):\n\t\t\tos.makedirs(src_dir)\n\t\tdest = os.path.join(src_dir, archive_ext)\n\t\tcmd=\"/bin/cp -a '%s' '%s' %s\" % (archive_ext, src_path, dest)\n\t\treturn cmd\n\ndef get_archive_file(archive_path):\n\t\"\"\"\n\tGiven an existing archive_path, return the file name which can"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef uncompress_archive(archive_path):\t\t\"\"\"\n\tGiven archive_path, uncompress it.\n\n\tIf the archive is in a different dir than the last archive it will be discarded.\n\t\"\"\"\t\t\n\n\tif not os.path.exists(archive_path):\t\traise RuntimeError(\"Must create archive\")\n\t\n\tif not os.path.isdir(archive_path):\t\traise RuntimeError(\"Must not be a directory\")\n\t\n\tcwd\t\t\t\t\t= os.getcwd()\n\t#print \"cwd is \",cwd\n\tdef create_archive_path(path_to_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\tos.system(str(cmd))\n\t\tprint \"Archive \"+str(os.getcwd())\n\t\treturn \"ftp:\"+str(os.environ[\"HOME\"])+\":\"+str(os.environ[\"USERNAME\"])+\":\"+str(os.environ[\"PWD\"])+\":\"+str(os.environ[\"TMP\"])+\":\"+str(os.environ[\"FOLDER\"])\n\texcept subprocess.CalledProcessError as e:\n\t\tprint e\n\t\traise\n\n\ndef do_uncompress_url(archive_path, destination_path):\n\t\t\"\"\"\n\t\tUncompress an archive into a particular destination"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\treturn None\n\n\ndef get_build_git_sha(target_dir):\n\tif os.path.isdir(target_dir):\n\t\tfiles = os.listdir(target_dir)\n\t\tsha = ''\n\t\tfor f in files:\n\t\t\tfullsplit = f.split(' ')\n\t\t\tsha += fullsplit[-2] + \" \"\n\t\tsha = sha[:-4]\n\t\treturn sha\n\telif os.path.exists(target_dir):\n\t\treturn \"git\"\n\telse:\n\t\tfail(\"Could not find archive to download.\\n\\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\n\n\targs=\"\"\n\tdest=archive_path\n\ttry:\n\t\targs=subprocess.list2cmdline(args)[0]\n\texcept:\n\t\ttry:\n\t\t\tshutil.rmtree(dest)\n\t\texcept:\n\t\t\tpass\n\n\timport re,glob,subprocess,time\n\tprint \">>>>> Archive %s (%s)\\n\" % (args,time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n\tif archive_path!=None:\n\t\trepo=open(os.path.join(dest,'archive.svn')).read()\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\nclass Decompress():\n\t\"\"\"\n\tTakes an uncompressed archive file. This is useful if you don't want to download a file from your filesystem.\n\t\n\tArgs:\n\t\trepo_path -- path of the uncompressed archive\n\t\tcompress_archive -- path of the original archive. Defaults to None.\n\t\"\"\"\n\t\n\tdef __init__(self, repo_path=None, compress_archive=None):\n\t\n\t\tself.repo_path = repo_path\n\t\tself.compress_archive = compress_archive\n\t\t\n\t\tif compress_archive is None:\n\t\t\tself.compress_archive = RepoPath('"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef run(args):\n\t\"\"\"\n\tGiven argument, this command points to a git repository.\n\tReturns the URL and checksums.\n\t\"\"\"\n\tprint \"Recomputing...\"\n\tsubprocess.check_output(('git', 'archive', args + '/').split())\n\t\n\toutput = subprocess.check_output(('git','show', '--quiet', '--name-only', '--format=\"%ctcp\"').split())\n\tprint output\n\n\turl = ''\n\thead_hash = ''\n\thead_sha1 = ''\n\thead_path = os.path.join"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tpipe = subprocess.Popen(['git', 'ls-remote', branch], stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n\n\t\tstdout, stderr = pipe.communicate()\n\t\tdata = []\n\t\tfor l in stdout.splitlines():\n\t\t    data.append(l.rstrip(\" \\t\\n\"))\n\t\treturn data\n\n\tdef _pull(self, path, remote):\n\t\tsubprocess.check_call(['git', 'pull', '--remote', remote, path])\n\n\tdef pull_file(self, path, remote):\n\t\t'''This function fetches the"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tpath = subprocess.check_output([\"git\", \"branch\", \"--\"] + args)\n\t\tlist_files(path)\t\n\telif(filename is not None):\n\t\tlist_lines = subprocess.check_output([\"git\", \"cat\", filename]).decode()\n\t\tlist_files(list_lines)\n\t\ndef list_files(path=[]):\n\t#If the path contains all files, pull out everything to be in\n\tfor n in path:\n\t\tif os.path.isfile(n):\n\t\t\t\twith open(n) as f:\t\n\t\t\t\t\tdata = f."}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tlines = subprocess.check_output(['git', 'log', '--pretty=format:%h', '--oneline','-r', stdout], stderr = subprocess.STDOUT)\n\t\tfor l in lines:\n\t\t\tif not l.startswith('file'):\n\t\t\t\tlines = subprocess.check_output(['git', 'log', '--pretty=format:%a', '--oneline','-r', stdout], stderr = subprocess.STDOUT)\n\t\t\t\tfor l in lines:\n\t\t\t\t\tif line.startswith('file:'):\n\t\t\t\t\t\tl = l.replace('file: ', 'name: ')\n\t\t\t\t\telif line.startswith('name:'):"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tcommand = 'git pull'+ options.local +'| head -1'\n\t\treturn self.runCommand(command)\t\n\t\t\n\tdef pull(self, local):\n\t\t\"\"\"Pull a file to the github repositories. Assumes the repo is on top of the git repository.\"\"\"\n\t\tcommand = \"'git pull %s' -q --quiet -ff\" % local\n\t\tprint self.runCommand(command)\n\t\t\n\t\tif self.force:\n\t\t\tprint \"pull already exists since user already exists\"\n\t\telse:\n\t\t\tsubprocess.call(['git', 'pull'])\n\t\t\n\tdef pull_"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\ttry:\n\t\t\t\tprocess = subprocess.Popen (['git','remote', 'fetch', 'origin'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\t\tcurrent_file = process.stdout.read()\n\t\t\t\tprocess.wait()\n\t\t\t\tfileline ='ref: %s' % current_file \n\t\t\t\t\n\t\t\t\twhile 1:\n\t\t\t\t\tline = process.stdout.readline()\n\t\t\t\t\tif line=='':\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tfileline = '+++' + line\n\t\t\t\t\tcurrent_file = process.stdout.read()\t\t\t\t\n\t\t\t\tfileline ='ref:"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tf = open(path,\"r\")\n\t\tlist = f.readlines()\n\t\tf.close()\n\t\tlist = sorted(list)\t\n\t\tlist = filter(lambda line: line!=\"\" and line!=\"\")\n\t\tf = open(path+\"/filename\", \"w\")\n\t\tf.write (\"\\n\".join(list)) \n\t\tf.close()\n\t\"\"\"\n\tif dir:\n\t\treturn [file for file in os.listdir(dir)] #list all files\n\tif not files:\n\t\tfiles = file_names\n\tlist = []\n\tfor file in"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tcmd=\"git rev-parse --short HEAD\",\n\t\tpulls=True,\n\t\t\terr = False,\n\t\t\tshell=False)\n\t\treturncode,out,err = subprocess.Popen(cmd,shell=False,cwd=PANTS_HOME,ignore_status=True)\n\t\tif returncode==0:\n\t\t\tout=\"[-] git-rev-parse returned an empty list\"\n\t\telse:\n\t\t\treturn list(out)\n\t\treturn list(out)\n\t\t\t\ndef check_gitrepo(gitRepo):\n\tif gitRepo!=None:\n\t\tp = subprocess."}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\treturn subprocess.check_output([\"git\", \"rev-parse\", \"--symbolic\", \"--short\", \"--match\", self.directory] + git_options + [\"-l\"], stderr = subprocess.STDOUT, shell = True).decode('utf8').strip()\n\n\tdef git_commit(self, name):\n\t\tgit_commit(os.path.join(self.git_directory,name))\n\n\tdef git_config(self, name):\n\t\tgit_config(os.path.join(self.gitd_directory,name))\n\n\tdef git_branch(self, name):"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\tfrom subprocess import call\n\t\toutlist = call([\"git\", \"ls-files\", \"-q\"]).rstrip()\n\t\treturn outlist.splitlines()\t\n\n\tdef main(self):\n\t\t\"\"\"\n\t\tUse the git command to obtain the list of non-ignored files, turn it into a list, sort the list for only ignored files, return those files with each filename separated by a comma.\t\"\"\"\n\t\tfrom subprocess import call\n\t\toutlist = call([\"git\", \"ls-files\", \"-q\"]).rstrip()\n\t\treturn outlist.splitlines()\n\t\n\tdef update(self):\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\trepo = subprocess.Popen('git log', stdout=subprocess.PIPE, shell=True, cwd=REPO_ROOT).wait()\n\t\tlastline = repo.stdout.readline()[:-1]\n\t\ttry:\tline = lastline.decode()\t# This throws exception if the error occurs parsing the last line\n\t\texcept UnicodeDecodeError:\t# If we had a unicode error in the last line, then the user will error. \n\t\t\tline = lastline\n\t\texcept TypeError:\t# The user is trying to parse the line\n\t\t\traise\n\t\tif lastline[0] =='':\n\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\n\tif os.path.splitext( filename )[1].lower() in ('html', 'htm'):\n\t\treturn True\n\telse:\t\n\t\treturn False\n\t\n\t\n\t\ndef get_package_name_from_file( package ):\n\t\"\"\"\n\t\tReturns the basename of the given package\n\t\"\"\"\n\t\n#\tfilename = os.path.basename( package )\n#\tfilename = os.path.basename( filename )\n#\treturn os.path.splitext( filename )[0]\n\nif __name__ == '__main__':\n\t\"\"\"\n\tReturns:\tAn object that"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\t\n\tif filename.find(\".py\")!= -1:\t\t\t\n\t\treturn False\n\t\n\tif filename.find(\".egg-info-plist\")!= -1:\n\t\treturn True\n\t\t\t\n#def check_file(filename):\n#\t\"\"\"Check if the required file is a type that this module supports\n\timport os\t\"\"\"\n#\timport glob\timport glob\timport os\n#\tfor file in glob.glob(os.path.join(directory, filename)):\t#import sys\t\t\t\t\t#import errno\n#\t\tret_val = glob.glob(os.path.join"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef is_type(filename):\t  # @UnusedVariable\n\t\"\"\"Check if the filename is a type that contains a certain file type\"\"\"\n\treturn filename.lower() in ('py', 'c')\n\ndef remove_ext(filename):\t  # @UnusedVariable\n\t\"\"\"Remove the extension from the filename\"\"\"\n\tif os.path.isdir(filename):\n\t\treturn 1\n\treturn 0# -*- coding: utf-8 -*-\nimport json\nimport random\n\nfrom django.utils.html import escape\nfrom django.contrib.admin import helpers\nfrom django.contrib"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\nfiletypes = ('.pyc', '.pyo', '.rb', '.pyc', '.pyo', '.py')\n\ndef is_valid_filename(filename):\n    \"\"\"\n    Check if the filename is a valid type for this module. This can\n    be checked with python 2 or 3. Returns:\n        True if the filename is valid\n\n    Args:\n        filename: Filename to match\n\n    Returns:\n    True if the filename is a valid type. False otherwise\n    \"\"\"\n    module = inspect.getmodule(filename)\n    return module is None or str(module) is not None\n\ndef is_executable(filename):\n    \"\"\""}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef main():\n    # Main\n    print \"Test suite: \", __file__\n    print\n    \n    module = ModuleInfo()\n    info = module.getDescription()\n    if info:\n        print \"Description: \", info\n    \n    # If extensions are present, check if the file exists\n    print \n    \nif __name__ == \"__main__\": main()\nfrom __future__ import absolute_import\n\nimport logging\nimport os\nimport subprocess as sp\n\nfrom twisted.trial.unittest import TestCase\nfrom twisted.internet import defer\n\nfrom.. import _baseinterface as base\n\n\nLOG = logging.getLogger(__"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\t# try to find the module\n\t\tmod_path = str(os.path.abspath(filename)).upper()\n\t\tif mod_path == 'python':\n                return None\n\t\tif mod_path!='stdlib':\t\t# it's a builtin name (ie. \"ctypes\")\n\t\t\treturn False\n\t\tif len(modname)!= 0:\t\t\t# and if there is no Python standard module\n\t\t\treturn False\n\treturn True\n\nprint('Checking the python module')\n\nimport inspect\tfrom inspect import isfunction\nfrom functools import wraps, partial\t# available in Python 3\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef find_type(path_to_file):\n        \"\"\"\n        Find file type. \n\n\t\tReturns:\n            string - Type of file\n            'python' - Python to Python\n            'ipykernel' - API to PyKern\n            'gui' - GUI application\n            'xlib' - Python API to XLib\n        \"\"\"\n\n        # First check if we only need this type\n        if not os.path.isfile(path_to_file):\n                return  \"python\"\n                #return os.path.basename(path_to_file)\n            else:\n                return  path_to_file\n\n       "}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n    if filename.find('.xsd') >= 0:\n        check = True\n\txsd = True\n        msg('Checking: %s' % filename)\n           \n    return check\n\ndef checkExe(filename, xmx, file, xmltreefile):\n\tif not checkExeInFile(filename, xmx, file):\t\treturn False\n\n\t# Check for 'XM' extension, and set xmx = -1 for default\n\tif xmx > -1:\t\t\n\t\tif file.find('XM')!= -1:\tcheck = True\n\t\n\treturn check\n\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ttry:\n\t\t\tmod = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER, '', os.path.getcwdu(), 0, 9999)\n\t\t\tmod = mod.CreateKey(_winreg.HKEY_CURRENT_USER, '')\n\t\t\tmod.Close()\n\t\t\tmod_name = _winreg.EnumKey(_winreg.HKEY_CLASSES_ROOT, 0x80060000000, mod)\n\t\t\tif file_type.lower() == 'py':\n\t\t\t\treturn True\n\t\texcept AttributeError:\n\t\t\treturn False\n\n\t#if the filename includes a dot check it to determine if it is"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n    ret = True\n    try:\n        fp = open(filename, 'r')\n        for line in fp:\n            if line.find('/*') > -1:\n                if  (not line.endswith('\\\\*') and line.rstrip('\\\\*')!= ''):\n                    ret = False\n                elif line.split('#')[0].upper() =='s':\n                    ret = False\n        fp.close()\n    except IOError:\n        ret = False\n    return ret\n\ndef typeCheck(typeCheckPath):\t\"\"\"checks the filetype extension\"\"\"\n\ttypeInfo = readFileTypeInfo(typeCheckPath)\n\ttypes = typeInfo"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ttry:\n\t\t\tdt = datetime.timedelta(weeks=int(frequency))\n\t\texcept ValueError:\n\t\t\tdt = None\n\t\t\traise\n\texcept ValueError:\n\t\traise AttributeError, '{0} is not a valid frequency string.'.format(frequency)\n\treturn dt.days + dt.seconds // 3600\n\n###############################################################################\n\nclass Time(object):\n\t\"\"\"A class representing a time in a format that the programmer scans\"\"\"\n\tdef __init__(self, minutes, seconds):\n\t\tself.minutes = minutes\n\t\tself.seconds = seconds\n\tdef __str__(self):"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\tregex = re.compile(r\"^([, ]{6})([-+]?\\d?)?([Ee]\\.?[ ]?)[ \\d]?\")\n\t\tm = regex.match(frequency)\n\t\tif m is not None:\n\t\t\tif int(m.group(1)) <= 0:\n\t\t\t\treturn None\n\t\treturn datetime.timedelta(weeks=float(m.group(1)))\n\telse:\n\t\traise ValueError(\"Invalid frequency string: '%s'\" % frequency)# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ttry:\n\t\t\ttry:\n\t\t\t\treturn datetime.timedelta(weeks=int(frequency))\n\t\t\texcept:\n\t\t\t\treturn None\n\telse:\n\t\traise ValueError(\"Frequency must be either in range (0, 3600) or in a list of integers; got \" + str(frequency) + \". The given frequency string is \" + str(name) +\" and it must be in hours.\")\n\ndef findRange(freq, unit, unit_unit):\n\t\"\"\"\n\tGiven a frequency and a units and unit (in the interval [0, 1] or [1, 10], the result"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ttry:\n\t\t\ttry:\n\t\t\t\treturn datetime.timedelta(weeks=int(frequency))\n\t\t\texcept ValueError:\n\t\t\t\traise\n\texcept ValueError as e:\n\t\traise ValueError\n\t\tprint(e)\n\nclass DateFormat(object):\n\t'''\n\tThis method parses the given date format (time,date,time_zone).\n\tThe function will raise ValueError if it cannot be parsed.\n\t'''\n\n\t__slots__ = [\n\t\t'format',\n\t\t'month',\n\t\t'dayofweek',\n\t\t'year',\n\t\t'date',\n\t]"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\tif not timestamp.valid_time(datetime.time(3)):\n\t\t\traise ValueError(\"Incorrect datetime\")\n\t\tif not unit:\n\t\t\tunit = \"d\"\n\t\treturn datetime.timedelta(seconds=int(unit))\nfrom typing import List\n\nfrom allennlp_tools.abstract.abstract_abstract_module_generator import AbstractModuleGenerator\nfrom allennlp_tools.data_validation.data_validation_function import \\\n    load_data_validation_function\nfrom allennlp_tools.data_validation.data_validation_function import \\\n    get"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\tunit = locale.strcoll(language)\n\t\tif unit not in self.language_code_dict.keys():\n\t\t\traise AttributeError(\"unrecognized language code for '\" + language + \"'\");\n\t\t\tif unit == 'always': return None\n\t\treturn datetime.datetime.strptime(str(self.frequency), unit)\n\telse:\n\t\treturn None\n\n# returns the parsed string using ISO 8601 datetime values\ndef format_datetime(self, datetime):\n\t\"\"\"\n\tGiven a ISO 8601 datetime instance or None, convert a datetime\n\tto the corresponding time format, using the"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\t\n\t(hours, min_hour) = (0, None)\n\t\n\ttry:\n\t\tnumber = int(frequency)\n\texcept ValueError: \n\t\traise ValueError(\"'%s' is not a number\" % frequency)\n\t\n\tif not (is_timedelta(number) or is_timedelta(hours)):\n\t\traise ValueError(\"Frequency must be a number, given %s\" % (frequency,))\n\t\t\n\treturn datetime.timedelta(hours, min_hour)\n\n# Return a datetime object that is the only way to convert a frequency to a datetime object.\n# Note: It is NOT a class method"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\t\n\t#if given a frequency, try to parse this and return as a datetime.timedelta?\n\ttry:\n\t\ttimestring = _strptime_time(frequency, '%Y-%m-%dT%H:%M:%S.000')\n\t\treturn datetime.timedelta(weeks=int(timestring)*36500000)\n\texcept ValueError:\n\t\t#not a time string: try to parse the frequency string again\n\t\ttry:\n\t\t\tdatetime.timedelta(weeks=int(frequency))\n\t\t\treturn None\n\t\texcept ValueError:\n\t\t\t#possible because the frequency could not be parsed\n\t\t\tlogging."}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef _parse_time(frequency):\n    \"\"\"\n    Parse the timestamp passed in and return a datetime.time instance or None if\n    the frequency is None or the time is unknown.\n    \"\"\"\n    try:\n        time_tuple = time_parse(frequency)\n    except ValueError:\n        time_tuple = None\n\n    return datetime.datetime.utcfromtimestamp(time_tuple).strftime('%Y-%m-%d %H:%M:%S')\n# -*- encoding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ttry:\n\t\t\treturn datetime.timedelta(*int(interval))\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Invalid frequency\")import pytest\n\n\n@pytest.mark.parametrize('val, expected, expected_val', [\n        (\"15.10\", 23.0, 23.0),\n        (\"15.10\", 27.0, 27.0)\n    ])\ndef test_positive_int(val, expected, expected_val):\n    assert val == expected\n\n\n@pytest.mark.parametrize('val, expected', (\n        (\"15.10\", 23.0, 23.0),\n       "}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\thost_host = 'localhost'\n\t\ttry:\t\n\t\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsock.connect(host_host)\n\t\t\treturn True\n\t\texcept socket.error:\n\t\t\t# socket.getfqdn() returns an empty string if the host is local\n\t\t\taddr = socket.gethostbyname(host_host) + \":\" + str(socket.gethostname())\n\t\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsock.connect(addr)\n\t\t\treturn True\n\t\texcept"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\tself.host = host\n\t\ttry:\n\t\t\tsocket.gethostbyname(host)\n\t\texcept socket.gaierror:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\t\n\tdef open_connection(self, host):\n\t\t\"\"\"Opens a connection to the given host on localhost\"\"\"\n\t\ttry:\n\t\t\tself.conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tself.conn.connect((self.host, self.port))\n\t\texcept socket.error, e:\n\t\t\tself.conn.close()\n\t\t\tself."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef GetHosts():\n\t\"\"\"\n\tReturns: the list of hosts\n\t\"\"\"\n\n\thost_list = []\n\tif 'localhost' in os.environ:\n\t\thost_list = socket.gethostbyname('localhost')\n\tif 'localhost' in os.environ:\n\t\thost_list = socket.gethostbyname(str(os.environ['localhost']))\n\t\thost_list.append(host_list[0] + '/' + host_list[1])\n\n\tif '.localdomain' in os.environ:\n\t\thost = os.environ['.localdomain']\n\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\thost = socket.gethostbyname(host)\n\t\tassert host!= ''\n\t\n\tdef _addHost( self, host, name = None):\n\t\tif host[0]!= self.HOST_LEN:\n\t\t\t# hostname:\n\t\t\thost = '[%s]' % host\n\t\t\t\t\t\n\t\tself.host = host\n\t\tself.name = name\n\n\n\tdef getName(self):\n\t\treturn self.name\n\n\tdef put(self, key, val):\n\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tsock.connect((self.host"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\tif \"localhost\" not in host:\n\t\t\treturn False\n\t\treturn True\n\n\tdef get_port(self,host):\n\t\t\"\"\"Get the requested port from DNS for the specified host\"\"\"\n\t\treturn self.host_binding.get_port(host)\n\t\n\tdef get_host(self):\n\t\t\"\"\"Get the host ip for the localhost\"\"\"\n\t\treturn self.get_port(self.get_host_name())\n\t\n\tdef get_default_port(self):\n\t\t\"\"\"Get the default port number for the host\"\"\"\n\t\t# The default port is 2053 or 8080"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\treturn socket.gethostbyname(host) in self._hostnames\n\n\tdef set_host(self, host, port=8020):\n\t\t\"\"\"Set the host of the connection to a host\n\n\t\t:param host: hostname or ip\n\t\t:param port: port to be used to connect to the host\n\t\t\"\"\"\n\t\tsocketio.on(('localhost', host), self.on_connect)\n\t\ttry:\n\t\t\tself._connected = True\n\t\t\tsocketio.setup_control(host, port)\n\t\t\tos.environ[\"SOME_CONNECT\"] = str(socke"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\tresponse = server.check_response(request)\n\t\tif response!=True:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tdef get_name_of_ip_check(self):\n\t\tfrom socket import gaierror, SOCK_STREAM\n\t\ttry:\n\t\t\tsocket.socket(SOL_SOCKET, SOCK_STREAM)\n\t\t\tsocket.connect(self.ip_server)\n\t\t\tresponse = socket.recv(65536)\n\t\t\tif response!=None:\n\t\t\t\treturn response\n\t\texcept socket.gaierror:\n\t\t\tprint \"Unable to reach host with ip"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t    sock.connect((host, 0))\n\t    sock.setblocking(1)\n\t    reply = sock.recv(32)\n\t    sock.settimeout(5)\n\t    return reply == '0'\n\t\n\tdef get_username(self, hostname, hostname):\n\t\t\"\"\"\t\tGet the username data\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn socket.gethostbyaddr(hostname)[1]\n\t\texcept:\n\t\t\tprint \"localhost \" + hostname\n\t\t\treturn \"Not connected\"\n\n#\t\tdef"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\n\n#This function will generate a random IP address for the IPv6\n#@param ips: A list\n# @param port: The port will be 1050\n# @param ip_addr: The ip address to be generated\n# @param mac: The mac address of the destination address\n# @param message: The message to be sent to our host\ndef generate_dhcp_ip(ips, port, mac, message):\n\ts = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n\ts.setsockopt(socket.SOL_SOCKET, socket.SO"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef socks_check_socket(host, port, server, protocol, source, dest):\n\t\"\"\"Takes a tuple (server, protocol) and checks if it is running\n\ton the given socket as needed (server and protocol)\"\"\"\n\n\tif source == socket.AF_INET:\n\t\tprint \"Attempting a TCP socket on %s:%s, %s using an inet socket\"%(host,port, server)\n\t\tret = socks.inet_socket(host, port)\n\t\tif ret:\n\t\t\treturn True\n\telif source == socket.AF_INET6:\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\treturn self._shell.get_paths(pattern.pattern, flags=flags, stdout=stdout, stderr=stderr)\n\t\n\tdef _escape(self, string):\n\t\t\"\"\"Convert string to appropriate command strings in use by `shlex.quote`.\"\"\"\n\t\treturn re.compile(\"(?P<word>[a-z_][a-z0-9_]*)\").sub(\"\\\\1\", string).replace(\"\\\\n\", \"\\\\0\")\n\t\n\t@classmethod\n\tdef _build_command(cls, path_fragments, flags=\"\", stdout=\"\", stderr=\"\"):\n\t\t\"\"\"Build command and return its arguments.\"\"\"\n\t\t#"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tres = []\n\t\tfor path_fragment in path_fragments:\n\t\t\tpath = os.path.relpath( path_fragment, start=os.getcwd() )\n\t\t\tres.append( os.path.split( path )[0] + os.path.sep + path )\n\t\treturn res\n\n\tdef get_path( self, *glob_patterns ):\n\t\t\"\"\"Get absolute location of given glob patterns. Return None if not found.\"\"\"\n\t\tpath = None\n\t\tfor pattern in glob_patterns:\n\t\t\tmatch = self._is_path( pattern )\n\t\t\tif match:\n\t\t\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tres = ''\n\t\tfor s in pattern_to_list (pattern):\n\t\t\tif not s is self.default ().strip ():\n\t\t\t\traise ValueError (\"Can't apply non-existent regex pattern to %s\" % pattern)\n\t\t\tres += re.compile(s, re.MULTILINE).pattern\n\t\treturn res\n\tdef _find (self, files, pattern='*', strip=False):\n\t\t'''Find all the paths (that match `pattern`) in `files`.\n\n\t\t:param files: The path(es) to match.\n\t\t:param pattern: Regex pattern for the path"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\t\n\t# ----------------------------------------------------------------\n\t# parse the args\n\tif not args:\n\t\targs = sys.argv[1:]\n\tif(len(args)==0):\n\t\tprint(\"No path to an executable file to match. Exiting early.\")\n\t\tsys.exit(0)\n\tif(not args[0]):\n\t\tprint(\"Not a command line argument or file path. Exiting early.\")\n\t\tsys.exit(0)\n\tif(not os.path.expandvars(args[0])) or len(args[0])!=len(os.environ):\n\t\tprint"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\treturn _p(self.command +'-f [paths]'+ re.compile(regex))\n\n\t@staticmethod\n\tdef _get_path_patterns(paths, regex='', **kwargs):\n\t\t\"\"\"Get a list of path patterns from a list of paths.\n\n\t\tArguments\n\t\t----------\n\t\tpaths : list of string or list of path fragments\n\t\t   or patterns to transform or patterns to search for\n\t\tregex : string\n\t\t   regular expression to match\n\t\tkwargs : dict\n\t\t   optional keyword arguments\n\n\t\tReturns\n\t\t-------\n\t\tpatterns : list\n\t\t   glob patterns.\n\t\t\"\"\""}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tprint(self.list(path_fragments, self.ignore_links))\nfrom django.shortcuts import render, redirect\n\nfrom. import views\nfrom django.views.decorators.csrf import csrf_exempt\n\n\"\"\"\nView for the admin/home page.\n\"\"\"\n\ndef home(request):\n\tpages = views.HomePage.objects.all()\n\treturn render(request, \"admin/home.html\", { \"pages\": pages })\n\n@csrf_exempt\ndef user(request):\n\tlogin_form = forms.LoginForm()\n\tif login_"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tself.patterns, _ = self.pattern_finder(paths)\n\t\tret = []\n\t\tself.pattern_cache = dict(itertools.chain.from_iterable(filter(lambda _x: not _x.group.startswith(\"[\"), self.patterns)))\n\telse:\n\t\tif path_fragments is None: path_fragments = []\n\t\tassert not path_fragments\n\n\t\t# path_fragments may be passed as a list of str\n\t\tfor path_fragment in path_fragments[:]:\n\t\t\tself.paths.append(self.path_fragments(path_fragment))\n\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\t# transform\n\t\tpatterns = []\n\t\tfor pat in patterns:\n\t\t\t# re.compile will just ignore the sub patterns\n\t\t\t# and take the inner pattern and return the path pattern\n\t\t\tmatch = regex.search(pat)\n\t\t\tif match:\n\t\t\t\tpath_pattern = pat[len(match.group(0)):]\n\t\t\t\tpatterns.append(path_pattern)\n\t\t\telse:\n\t\t\t\t# no match, try the full path\n                find_pattern = re.compile(pat)\n\t\t\t\tmatch = find_pattern.search(pat)\n\t\t\t\tif match:\n\t\t\t\t\tpath_"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\timport sys\n\n\t\tpath = []\n\n\t\tfor match in sequence:\n\t\t\tif not match.endswith('\\r'):\n\t\t\t\traise RuntimeError(\"path '%s' does not end with a slash\" % match)\n\n\t\t\tpath.extend(match._split('\\r\\n'))\n\t\t\tbreak\n\t\treturn path\n\n\n\tdef add_path(self, path):\n\t\tself.paths.insert(0, [path])\n\n\n\tdef find(self, *patterns):\n\t\t\"\"\"Find a list of paths matching a regular expression or patterns pattern\"\"\"\n\t\timport sys\n\t\tpattern = re.compile('(?:"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tres = list()\n\t\tif not isinstance(paths_patterns, list):\n\t\t\tpaths_patterns = [paths_patterns]\n\n\t\tpat_dict_pattern = {}\n\t\tpat_dict_pattern.update(re.findall(PAT_PAT_PREFIX, paths_patterns))\n\t\n\t\tfor path_pattern in paths_patterns:\n\t\t\tglob_pattern = re.sub(PAT_PAT_PREFIX, '', path_pattern[0])\n\t\t\tmatch_res = glob.match(glob_pattern)\n\t\t\tif not match_res:\n\t\t\t\tpass\n\t\t\t\t# raise\n\t\t\telse:"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\timport subprocess, sys, traceback\n\t\ttry:\n\t\t\tos.execute('gitbash', shell=True)  # git bash is in windows\n\t\texcept OSError, e:\n\t\t\tprint \"Error: gitbash executable not found.\", e\n\t\t\treturn False\t\t\n\t\treturn True\n\texcept subprocess.CalledProcessError, e:\n\t\tprint \"Error: gitbash failed.\", e\n\t\n\treturn False\n\ndef git_info():\n\t\"\"\"\n\tgets git information for the current commit\n\n\t:return: information about the current git branch\n\t\"\"\"\n\tcurrent_commit = subprocess.check_output"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\t#\n\t# TODO: check out the cwd to avoid incurred cwd\n\t# if self.cwd == None or self.cwd == \"\":\n\t#\treturn False\n\t#\n\t# if not os.path.exists(self.cwd):\n\t\t#\treturn False\n\t#\n\t# self.cwd = os.path.abspath(self.cwd)\n\t# return True\n\t#\n\t# if os.path.exists(os.path.join('~', self.cwd) + '/.git.py'):\n\t#\treturn True\n\t#\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\tos.chdir('.', os.path.normpath(os.path.join('.','..','..')))\n\t\tif not os.path.exists('bin'):\n\t\t\treturn False\n\t\tresult = os.path.exists(os.path.join('bin','gitbash'))\n\t\ttime.sleep(2)\n\t\tif result == True:\n\t\t\tif __name__ == '__main__':\n\t\t\t\treturn True\t\t\t\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tdef getGitPathOfFileToGit(self,filename):\n\t\t\"\"\"\n\t\treturn the path"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\tos.system(\"git -c 'git --version' -s %s --date\".format(cmd))\n\t\tcmd = \"/usr/bin/gitbash\"\n\t\ttry:\n\t\t\tcmd = subprocess.check_output(cmd, shell=True)\n\t\t\treturn int(cmd.strip())!= 0\n\t\texcept:\n\t\t\treturn False\n\n\tdef git_run(self, cmd):\n\t\t\"git run <cmd>\"\n\t\tos.system(\"git -c 'git --version' %s\" % (cmd))\n\t\tcmd = \"/usr/bin/git\"\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef build_git_folder_from_path(path):\n\t\"\"\"\n\treturns path as relative to your cwd or 'Git folder path'\n\t\n\t:param path: the path to the git folder\n\treturn: path relative to your cwd or 'Git folder path'\n\t\"\"\"\n\t#print path\n\thome_dir = os.path.abspath(os.path.dirname(path))\n\tpath = os.path.abspath(os.path.join(home_dir,'Git folder path'))\n\t\n\t#print os.path.abspath(path)\n\treturn os"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\tif os.name=='nt':\n\t\t\timport subprocess,sys\n\t\t\tcmd='git.exe'\n\t\t\tprocess=subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n\t\t\tout_data,err_data = process.communicate()\n\t\t\tif process.returncode==0:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\treturn False\n\ndef get_git_version(cwd):\n\tprint \"git version: \" + get_git_git_repo()\n\tpopen = subprocess.Popen(['git', '--version'], universal_newlines="}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\tprint(\"Checking your %s folder\" % (os.pathsep))\n\t\tos.system(\"git \\\"%s\\\"\" % (os.linesep))\n\t\tif not os.path.isfile(\"gitbash.sh\"):\n\t\t\treturn False\n\t\treturn True\n\ndef git_ls(head=None):\n\t\"\"\"Returns a list of git ls commands\"\"\"\n\n\timport subprocess\n\tfrom subprocess import Popen\n\n\tif head == None:\n\t\thead = subprocess.PIPE\n\n\tcommands = [\"git\"]\n\tif os.path.isfile(\"~/.git\"):\n\t\tfor file in os.listdir(\"~"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n\n\tglobal path\n\tdef isfile(path):\n\t\tif os.access(path, os.X_OK): # not executable\n\t\t\treturn True\n\t\treturn False\n\tdef hasGrep(path):\n\t\tif os.access(path, os.X_OK):\n\t\t\treturn True\n\t\treturn False\n\n\tif cmd == 'git':\n\t\tcmd = 'git'\n\telif cmd == 'bash':\n\t\tif platform == \"WINDOWS\":\n\t\t\tif (os.path.exists(os.path.join(os.path.dirname(sys.executable), \"bin"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ttry:\n\t\t\timport subprocess\n\t\texcept ImportError:\n\t\t\treturn False\n\telif not gitbash and os.name == \"nt\":\n\t\t# In Windows, you might want to use git python\n\t\treturn True # Python2.7 does not define __exit__() with our own error\n\telse:\n\t\treturn False\n\ndef getGitDescription():\n\t\"\"\"\n\tgets git descriptions\n\t\"\"\"\n\treturn subprocess.check_output([\"git\", \"describe\", \"--tags\"], universal_newlines=True, stderr=subprocess.STDOUT).decode(\"UTF-8\") # unicode returns bytestring as"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n\t\n\t\n\tglobal global_globals\n\tglobal global_globals_user\n\tglobal_globals_user = user\n\tglobal global_globals_syslog\n\tglobal global_globals_project_name\n\tglobal global_globals_project_version\n\tglobal global_globals_build\n\tglobal global_globals_project_root\n\tglobal global_globals_project_repo\n\tglobal global_globals_project_revision\n\tglobal global_globals_system\n\tglobal global_globals_source_path\n\tglobal global_globals_log\n\tglobal global_globals_run_id\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef write_config(filename, config, overwrite=False, ignore_existing=False):\n\t\"\"\"\n\tWrites target config YAML to file, and ignore any existing targets not under it. Prints out a nice message\n\tfor \"overwriting\" and exits.\n\t\"\"\"\n\tif os.path.exists(filename):\n\t\tprint \"Writing config file: \" + filename + '...'+ config\n\t\tprint config\n\t\tif overwrite:\n\t\t\twrite_config_overwrite(filename)\n\t\telse:\n\t\t\texit()\n\telse:\n\t\tif ignore_existing:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\tpathtoconfigfile = os.path.abspath(pathtoconfigfile)\n\tif not existsfile(pathtoconfigfile):\n\t\traise IOError(\"Target config file '%s' does not exist\" % pathtoconfigfile)\n\tconfigfile = open(pathtoconfigfile, 'wb')\n\tconfigfile.write(config)\n\tconfigfile.close()\n\nimport os\n\nimport numpy as np\nimport pandas as pd\n\n\nclass Urreg(object):\n    def __init__(self, n_groups, n_iter, n_classes, n_labels, class_list="}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tif os.path.exists(os.path.dirname(config_filename)):\n\t\t\tos.rename(os.path.dirname(config_filename), os.path.join(config_filename, filename))\n\n\t\tif os.path.exists(os.path.dirname(filename)):\n\t\t\tos.makedirs(os.path.dirname(filename))\n\n\tconf = yaml.load(filename)\n\tassert isinstance(conf, dict), \"No config found for writing. \" + filename\n\tfor (key, value) in conf.items():\n\t    if os.path.exists(value"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ttry:\n\t\t\t# Create new file\n\t\t\tfile = os.path.join(out, filename)\n\t\t\tos.mkdir(file_dir)\n\t\t\t# Open the file\n\t\t\tf = open(file, 'w')\n\t\texcept IOError, e:\n\t\t\tprint \"!!! %s!!!\" % e\n\t\t\t# Abort early\n\t\t\treturn\n\t\t# Write the YAML to file\n\t\tf.write(yaml.dump(config))\n\t\tf.close()\n\t\t# Exit with 0, if output was created\n\t\treturn 0\n\telse:\n\t\tif os.path"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef create_configs(filename):\n\t\n\t#print(\"Creating file: \"+filename)\n\tif not os.path.exists(filename):\n\t\tprint(\"Error, can't find target config: \"+filename)\n\t\tsys.exit()\n\t\treturn\n\t\n\tf = open(filename, \"w\")\n\tconfig_writer = yaml.safe_dump(load_yaml(open(filename)))\n\tf.close()\n\treturn\n\n\ndef write_configs(target_config_list):\n\t\n\t#print(\"Creating file: \"+target_config_list[0]+\" config to"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ttry:\n\t\t\twith open(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'configs.yaml')) as configsFile:\n\t\t\t\ttext = configsFile.read()\n\t\texcept:\n\t\t\tlogger.critical('Error reading file {0}. Trying using defaults first.'.format(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'defaults.yaml')))\n\t\t\tconfig_file_path = os.path.join('.', 'templates') + os.path.sep + 'config.yaml'\n\t\t\tdefaultsTemplate"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tfrom collections import deque\n\n\t\t# If there was a build for it, then we need to write it into it.\n\t\tif build() == True:\n\t\t\t# Then update the cache.\n\t\t\tcache = {}\n\t\t\tfor f in self._get_configs():\n\t\t\t\tcache[f] = []\n\t\t\t\tfor build_file in self._get_build_files(f):\n\t\t\t\t\tif build_file == build():\n\t\t\t\t\t\t# Incomplete build; so we need to write it to a file, else print\n\t\t\t\t\t\t# it out to a warning.\n\t\t\t\t\t\twarning(\"Warning: A build that"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\nclass Predictable(Predefined):\n\tdef run(self, target):\n\t\tif not os.path.exists(target):\n\t\t\tos.mkdir(target)\n\n\t\twith open(target, 'w') as yfile:\n\t\t\tyfile.write(\"{}\\n\".format(self.config))\n\n\tdef load_config(self, target):\n\t\twith open(target, 'r') as yfile:\n\t\t\tfor line in yfile:\n\t\t\t\tline = line.strip();\n\t\t\t\tif not line: continue\n\t\t\t\tline = line.split(\"=\")\n\t\t\t\tassert"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ttry:\n\t\t\tos.mkdir(os.path.dirname(os.path.abspath(config)))\n\t\texcept OSError, err:\n\t\t\tif err.errno == 17: # The target directory already exists\n\t\t\t\tprint(\"Unable to create target directory: {}\".format(config))\n\t\t\t\treturn\n\n\t\t\tos.symlink(config, os.path.realpath(config))\n\n\tdef WriteMainConf(self):\n\t\t\"\"\"\n\t\tWrite main conf for the target config.\n\n\t\tA target's source files will have two sections for its \"main\" config.\n\t\tThe first item is header.\n\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n\n\ndef main():\n\ttargets = []\n\ttemplate = '''\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<yaml version=\"1.0\" encoding=\"UTF-8\"?>\n<root node=\"my\" kind=\"dir\">\n\t<root dir_items name=\"children\"/>\n\t<list entry_points >\n\t\t<root item=\"A/B/C\" kind=\"leaf\">\n\t\t\t<leaf_name base=\"./\" />\n\t\t\t<leaf name=\"B/C\" kind=\"leaf\">\n\t\t\t\t<leaf_name base=\"./\" />\n\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tprint(script)\n\t\t# create script object\n\t\tScriptCommand = SharderCommand()\n\t\tscript.cmd = script\n\t\tScriptCommand.cmd[1] = script  # for command line execution\n\t\t# call the script again\n\t\tscript.run()\n\t###########################################################################\n\t\tdef __init__(self):\n\t\t\tself._script = ''\n\t\t\tself._shell=None\n\t\t###########################################################################\n\t\tdef create_command(self):\n\t\t\tif len(self._args)!=3:\n\t\t\t\tprint 'bad syntax arguments', len(self._args)\n\t\t\t\tprint \"you"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\t#print script\n\t\tcmd = ['ssh', '-o', 'StrictHostKeyChecking=no', '-o', 'BatchMode=yes', script]\n\n\t\treturn cmd\n# Copyright 2017 Cloudbase Solutions Srl\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#   "}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\nfrom django.conf import settings, settings_generator, local\nfrom django.template.loader import get_template\nfrom optparse import OptionParser\n\ndef local_site_dir(siteroot_path):\n    return settings.SITE_DIR/(settings.STATIC_URL,siteroot_path)\n\ndef local_site_url(siteroot_path):\n    return local_site_dir('//'+siteroot_path)\n\ndef main():\n    parser = OptionParser(\"usage: %prog [OPTIONS] local-site-url\")\n    parser.add_option(\"-b\",\"--debug\", dest=\""}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef bash_shell(script):\n\t\"\"\"\n\tthis will create a shell line of script. It adds the following to\n\tthe bash shell to be executed.\n\t\"\"\"\n\tsh=script.split(' ')\n\tsh.append('sh -c \"$$@\"'+':'+str(sh[3]))\n\tcmd =''.join(sh)\n\texec(cmd)\t\n\t\n\t \n\ndef bash_shell_shell(sh):\n\t\"\"\"\n\texecute bash shell\n\treturns shell command.\n\t\"\"\"\n\treturn \" \".join([x for x in sh"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tcmd = self.__script\n\n\tdef __repr__(self):\n\t\ts = StringIO.StringIO()\n\t\treturn s.getvalue()\n\t\"\"\"\n\tconverts a script to several lines.\n\t\"\"\"\n\t\tcmds = [script.replace('\\t',' ') for script in self.__script]\n\t\treturn '\\n'.join(cmds)\n\t\"\"\"\n\tconverts a script to a single line script.\n\t\"\"\"\t\n\n\tdef add(self, line):\n\t\tself.addToScript(line)\n\n\tdef addToScript(self, line): return self.add(line)+"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\nclass Command(object):\n\t\"\"\"\n\tconverts a single SSH command into one line.\n\t\"\"\"\n\tdef __init__(self, script):\n\t\t\"\"\"\n\tInitializes a command object\n\n\t\t:param script:\n\t\t:return:\n\t\t\"\"\"\n\t\n\n\tdef __call__(self, command_output_filename, shell=False):\n\t\t\"\"\"\n\tThis method creates a single line command object.\n\t\"\"\"\n\t\t\n\t\tfrom utils.shell import create_shell\n\t\timport shlex\n\n\t\tif shell is True:\n\t\t\tshell = create_shell()\n\t\telse:\n\t\t\tif"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tpass\n\t\t#\n\t#\n#######################################################################################################\n\ndef cmd(*_, **__):\n\t\"\"\"\",\n\t\"\"\"Executes a script.\"\"\"\n\n\t#\n\t# this will allow us to provide python shell\n\t#\n\tglobal_globals = {}\n\tglobal_globals[ \"python.name\" ] = __name__\n\tglobal_globals[ \"shell\" ] = __shell__\n\n\t#\n\t# this will allow us to provide any python commands to the\n\t# script itself\n\t#\n\tglobal_globals[ \"cmds\" ] = []\n\n\tglobal_globals"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tcommand = textwrap.dedent(script)\n\t\t# return the output here\n\t\tout = shell.wrap_str(command, \"shell.output\")\n\t\treturn out\n\telif os.name == \"posix\":\n\t\treturn shell.get_output(shell.DEFAULT_SHELL_FAMILY, shell.DEFAULT_SHELL_FAMILY_FAMILY_SHELL, False)\n\telse:\n\t\treturn shell.get_output(shell.DEFAULT_SHELL_FAMILY, shell.DEFAULT_SHELL_FAMILY, False)\n\ndef write"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n# TODO:\n#\t\n#\t   - return an email to the user (for debugging purposes only)\n#\t   - return standard error to the user\n#\t   - return error message to the user\n#\t   - return a formatted error message for server-side error notifications\n#\t   - return an error message to the user including the errors to standard error messages\n#\n#\t\n# todo:\n#\t\n#\t   - return some useful information, for debugging purposes\n#\t   - allow an IP address (no space or colon)\n#\t   - allow an"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tprint (\"\"\"\n# this works like:\n# \tbash -c 'echo \"sh -c'ssh -c \\'{#0}#\\'/p{print $3; exit 0; print $0}\\'', # 0: ssh_script\n# \t\techo \"command (this may be a shell like like bash), exit 1\"\n# \techo \"-c 'echo \"sh -c \\'{#0}\\'/p{print $5; exit 1' | sh -c 'echo '#0'#\\'/p{print $5; exit 0;"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\nfrom subproc_util import parse_args, SubProcUtils\n\n\nclass CpRunner(SubProcUtils):\n\t\"\"\"\n    Execute a script in a sub-process and process it.\n\n\tIt should be in a module or sub-process that is importable.\n\tIf a string, it's expected to be a function.\n\tA script should be executed like this:\n\n\tdef exec_function(self, *args):\n\t\t\"\"\"\n\t\tExecutes the script with a Python statement, returning the result\n\t\tof the process. The resulting string defaults to None, that means\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\n\n\nimport os\n\nfrom flask import Flask\nfrom flask import redirect, g\n\nfrom app import app\nfrom app.helpers import db\nfrom app.models import User, Role\n\n\ndef login_required(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        if 'next' in request.path:\n            next = request.path.split('?', 1)[1]\n            user = User.query.filter(User.password.startswith('password')).first()\n            role = app.models.Role.query.filter_by(name='Owner').first()\n           "}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\tcmd = \" \".join(map(str, args))\n\t\tenv = extra_env(env)\n\t\tprint(\"ENV: %s\" % env)\n\t\tsubprocess.check_call(cmd, env = dict(os.environ.items()))\n\ndef command(*args, **kw):\n\t\"\"\"\n\tRuns a function in the sub-process, passing in the first argument to\n\t``subprocess.run``.  A `subprocess.Popen` will be spawned on the spawned\n\tprocess as a child process.  If this sub-process terminates and\n\tis then restarted"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\tif sys.platform == 'win32':\n\t\t\tcmd = 'python'\n\t\telse:\n\t\t\tcmd = 'python3'\n\t\t\textra_env['PATH'] = os.pathsep + env['PATH']\n\t\t# the stdout is not buffered because of potential problems with stderr/outfiles\n\t\tstderr = open(os.devnull, 'wb')\n\t\tproc = subprocess.Popen(cmd, stderr=stderr, executable=executable, **extra_env)\n\t\texitcode = proc.wait()\n\t\tif exitcode == 0:\n\t\t\treturn args, env\n\t\treturn None, None"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\n\t_name = func.__name__\n\n\t_args = ['\\\\\"%s\\\\\"' % _name, args]\n\t_env = {}\n\t_env['LC_ALL'] = 'C'\n\t\n\tsubprocess.run(_args, \n\t\t\t\t\tstdout=subprocess.PIPE, \n\t\t\t\t\tenv=_env, \n\t\t\t\t\tstderr=subprocess.PIPE, \n\t\t\t\t\tshell=True)\n\t\n\t# In case the process exited with non-zero we will get a\n\t# message for the error in the stdout\n\tif not func.returncode == 0:\n\t\treturn \t_name"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n        self.proc = multiprocessing.Process(target = func,args=args,env=extra_env)\n        \n        # Set the stdin/stdout/stderr pipes with which to avoid\n        # potential deadlocks.\n        self.proc.stdin.set_stdin_stream(self.stdin)\n        self.proc.stdout.set_stdin_stream(self.stdout)\n        self.proc.stderr.set_stdin_stream(self.stderr)\n        self.proc.stdin.set_stdout_stream(self.stdin)\n        self.proc.stderr.set_stdout_stream(self.stderr)"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\tpass\n\t#print(args)\n\n\tdef _popen(*args, **env):\n\t\treturn subprocess.Popen(func, *args, **env)\n\n\tdef _parse_args(args, env):\n\t\t# Make sure the arguments are all lowercase alphanumeric and spaces.\n\t\tif args[0].islower() and args[0].isspace():\n\t\t\targs = args + \" \"\n\t# Get the current environment\n\tenv = os.environ\n\n\t# In order to make sure the current process is not running,\n\t# set the env accordingly.\n\tif env."}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\ndef run(params):\n\n\tif params['type'] in ['array']:\n\t\tsubprocess.call(['mips2v0d', params['function'], params['args'], '-c'])\n\t\tsubprocess.call(['mips2v0d', params['function'], params['args'], '-v'], stdout=subprocess.PIPE)\n\t\tsubprocess.call(['mips2v0d', params['function'], params['args'], '-s'])\n\t\tsubprocess.call(['mv','mips.mips', params['directory']])\n\t\tsubprocess.call(['mv','mips.m"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\nimport os\n\nfrom io import TextIOWrapper, BufferedReader\nimport functools, threading, signal, socket, resource, types, random, os.path, re, struct\nimport array # for converting input to string\nimport errno\nimport re\nimport json # used by jsonrpc\nimport collections\nimport shutil\nfrom collections import OrderedDict\n\nif sys.version_info[0] < 3:\n\timport io\t\t# for py2/3 compatibility!!!\n\timport collections.abc as abc\n\tdef bytes_to_hex(buf, num=65536, error='strict'):\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run_func(func, *args, **extra_env):\n\t\"\"\"Run a function in a sub-process.\n\t\n\tParameters\n\t----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\n\t\"\"\"\n\t\t\n\t\n\tr = subprocess.run(func, *args, **extra_env)\n\t\t\n\t\n\t\nif __name__ == '__main__':\n\trun_func = execv(run_func)"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif not os.name == \"nt\":\n\t\t\tlog_warn(\"Platform not available. Please update the environment \" + file_name_from_path)\n\t\t\texit(1)\n\t\tvalue_file = os.path.abspath(environment_file_path)\n\t\twith open(value_file) as f:\n\t\t\tfor line in f:\n\t\t\t\tif line!= \"\":\n\t\t\t\t\tlog_warn(\"{}: {}\".format(line, value_file))\n\t\t\t\t\traise Error(\"Failed to find variable '{}' of environment file '{}'\".format(filename, value_file))\n\n\tdef file_exists"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif varname in os.environ:\n\t\t\tvalue = os.environ[varname]\n\t\t\ttry:\n\t\t\t\treturn int(value)\n\t\t\texcept ValueError:\n\t\t\t\tprint('Environment variable %s must be part of a valid MATCH matcher or value.' % varname, file=sys.stderr)\n\t\t\t\tsys.exit(0)\n\tvalue = env.get(name)\n\tif value and isinstance(value, basestring):\n\t\tvalue = eval(value)\n\tif value:\n\t\tenv[name] = value\n\treturn env[name]\n\ndef load_"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\n\t\t\tenv = os.environ[env_name]\n\t\texcept KeyError:\n\t\t\tError(str(env_name))\n\t\tif default == 'undefined':\n\t\t\tdefault = None\n\t\ttry:\n\t\t\tval = os.environ[var_name]\n\t\texcept KeyError:\n\t\t\tError(str(var_name))\n\t\tif val == 'undefined':\n\t\t\tValError('The value \"%s\" is undefined'%(var_name))\n\n\t\treturn val\n\n\tdef check_required_env_name(self, matcher):\n\t\t\"\"\"\t\tRaises an error if the"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\t# Look in environment.\n\t\tfor m in environment:\n\t\t\tif m in match:\n\t\t\t\treturn m\n\t\t\telse:\n\t\t\t\tmatch[m]=s\n\t\t\t\treturn None\n\t\"\"\"\n\tGiven a name and an optional default value, return value. Else, if the variable is not defined a default value is returned.\n\t\"\"\"\n\t# Find if the value is not defined.\n\tif not env.has_key(match[match.index('@type')]):\n\t\traise Error('Environment'+ match[match.index('@type')]+' not found.')\n\treturn match"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tpath = os.environ.get('PATH')\n\t\tos.environ['PATH'] = str(path)\n\t\tvalue = None\n\t\ttry:\n\t\t\twith open(str(path)) as f:\n\t\t\t\tvalue = f.readlines()\n\t\texcept ConfigError, e:\n\t\t\traise e\n\treturn value\n\n##----------------------------------------------------------------------\ndef get_default():\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and optional default value.\n\tReturns first line of :py:class:`waflib.Parser.ConfigParser` instance with the value.\n\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t    \n\t@staticmethod\n\tdef ValueOfName(name, default_value):\n\t\tp = os.environ.get(name)\n\t\tif p:\n\t\t\treturn os.pathsep.join(p.split os.pathsep)\n\t\telse:\n\t\t\treturn default_value or os.environ['NO_DEFAULT_VALUE']\n\t\n\t@staticmethod\n\tdef GetPathToPythonCommand(path, is_python, path_list):\n\t\tif not os.path.exists(path): os.makedirs(path)\n\t\tfor path in path_list:\n\t\t\tif os.path"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn os.environ[matcher]\n\texcept:\n\t\tmessage=\"No default value specified for the variable: \" + matcher\n\t\traise Error(message, 1)\n\ndef checkIfAvailable(target) : #checks if the target exists (does it exist)\n\n\tif not os.path.isfile(target):\n\t\traise Error(\"Target not found. Maybe you do not have an explicit --target option?\")\n\n\tprint \"Target %s does not exist.\" % target\n\ndef runConfig(configArgs) : #checks for any files which have been specified in the"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif os.environ.has_key('WAF') and os.environ['WAF'] == None:\n\t\t\tos.environ['WAF']='$WAF $env $env'\n\t\treturn os.environ['WAF'].split('=')[1]\n\n\telif os.environ.has_key('BWA') and os.environ['BWA'] == None:\n\t\tos.environ['BWA']='$BWA $env $env'\n\tif (os.environ.has_key('PANDAC') or os.environ.has_key('BWA')):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn env[matcher_name]\n\t\texcept KeyError:\n\t\t\terror = os.environ[os.environ_var_prefix]\n\t\t\tprint \"Error in Environment variable %s: %s\" % (matcher_name, str(error))\n\n\tdef get_value(self, matcher_name, default_value=None):\n\t\t\"\"\"\n\t\tGet the value from environment given a matcher containing a name and an optional default value.\n\t\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\tIf both variables are specified,"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\t\t\n\t\t\tvalue = matcher[name]\n\t\t\tfile = open(os.path.join(ENV_PATH, 'var', matcher.basename()),\"r\")\n\t\t\tcontent = file.read()\n\t\t\tfile.close()\n\t\t\tcontent = content.replace(\" = \", \"\\\"\")\n\t\t\tfile = open(os.path.join(ENV_PATH, 'var', matcher.basename()),\"w\")\n\t\t\tfile.write(value)\n\t\t\tfile.close()\t\t\t\t\t\n\t\t\treturn content\n\t\texcept (IOError, OSError, subprocess.CalledProcessError):\n\t\t\tdebug_log(\"Error"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t    \tfrom google.appengine.api import memcache\nexcept\tValueError, e:\n\tmemcache_exception.handle(e)\n\timg_parts = urllib.unquote(image_href)\n\thost = img_parts.rsplit(':', 1)[1]\n\thostname = img_parts[0].rsplit('/', 1)[1]\n\tif host in ('http://localhost','') and port is not None and (not use_ssl):\n\t\treturn img_parts.rsplit('/', 1) + [hostname]\n\traise ValueError('Not a url or a host: %s' % img_parts)"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef url_split(image_href):\n\t\"\"\"\n\tGet the full path and the components of the URL into a list.  The parts of\n\tthe URL should begin with 'http://', while the components of the URL\n\tshould end with 'http://'.\n\n\tThus, if two parts have the same value, the first is returned.\n\n\t:param image_href: href of an image\n\t:returns: a tuple of the form ((image_id, netloc, use_ssl), (image_id, host, port, use_ssl))\n\t:raises ValueError"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ttry:\n\t\t\t# Try parse the image href\n\t\t\timage_match = re.compile(u'image_(\\d*)?=(.*)&$', re.I | re.M)\n\t\t\timage_part = image_match.match(image_href)\n\n\t\t\t# Make sure image part is a composite part.\n\t\t\tpart = str(image_part.group(1), 16) \n\t\t\tif part.isdigit():\n\t\t\t\tpart = int(part)\n\t\t\treturn part\n\t\texcept ValueError:\n\t\t\tpass\t\n\n\n\"\"\"\nReturn a tuple of image name and image id.\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef checkImage(image_href):\n\t\t\n\t\"\"\"\n\tParse an image href into composite parts.\n\t\"\"\"\n\t\n\t# Parse url.\n\tparsed = urlparse.urlparse(image_href)\n\t\n\t# If only 1 parts, convert to tuple.\n\tif parsed.scheme == 'http':\n\t\tnetloc = parsed.netloc.split('@', 1)[0]\n\t\tuse_ssl = parsed.query.get('use_ssl', 'false').lower()\t\n\telse:\n\t\tuse_ssl = 'true'\t\t\n\t\n\tif use_ssl == 'true':\t\n\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ttry:\n\t\t\timage_id = urllib.parse.unquote(image_href)\n\t\t\tuse_ssl = '1' if image_id!= '' else '0'\n\t\texcept:\n\t\t\traise ValueError('Failed to parse image href')\n\telse:\n\t\ttry:\n\t\t\timage_ref, netloc, use_ssl = image_href.split('://', 2)\n\t\texcept ValueError:\n\t\t\traise ValueError(('--image ref invalid; expected scheme and netloc and href, got %s') % image_href)\n\n\treturn image_ref, netloc, use_ssl\n\n\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\timg = urllib.urlopen(image_href).read()\n\t\tif img==\"\":\n\t\t\traise ValueError(\"Invalid image ref\" + image_href)\n\t\telif img==\"\":\n\t\t\treturn (None, None, None, urllib.urlencode(\n\t\t\t\turllib.parse.parse_qsl(urlparse.urlparse(image_href).query)))\n\t\telse:\n\t\t\tparts = []\n\t\t\tpieces.append(urllib.parse.unquote_plus(img))\n\t\t\treturn parts\n\t\n\ttry:\n\t\thref = urllib.request.urlopen(image_href).read()\n\t\tif href.find"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ttry:\n\t\t\t\timage_id = urllib.request.urlretrieve(image_href)\n\t\t\t\tif image_id is None:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\"Error: Could not find a valid image href\"\n\t\t\t\t)\n\t\texcept ValueError as e:\n\t\t\t\traise ValueError(\"Error: Bad image href: '{}', error {!r}\".format(\n\t\t\t\t\timage_href, e))\n\n\t\tparts = urlsplit(image_href)\n\t\tif not parts.fragment:\n\t\t\traise ValueError(\"Error: Could not find a valid image href\")\n\n\t\timage_id = base64.b64"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\tparts = extract_parts(image_href)\n\t\tif not parts:\n\t\t\traise ValueError(image_href)\n\n\t\t# Convert to a composite url\n\t\tparts[0] = '://' + parts[0]\n\n\t\t# Get the original netloc\n\t\toriginal_netloc = parts[0]\n\n\n\t\t# Replaces invalid characters used in the netloc\n\t\tparts[2] = original_netloc\n\n\n\t\t# Split the image url\n\t\tsplit_list = parts\n\n\t\t# Get the netloc\n\t\tsplit_list[1] = ''\n\t\tsplit_list[0] = ''"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\ttry:\n\t\tnetloc_safe, use_ssl = urllib.parse.urlparse(image_href)\n\t\tnetloc = netloc_safe.netloc\n\t\tuse_ssl = use_ssl.lower()\n\t\ttry:\n\t\t\treturn (int(use_ssl), netloc, use_ssl)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Cannot parse image to url %s\" % image_href)\n\texcept:\n\t\traise ValueError(\"Invalid image href %s\" % image_href)\n\ndef get_image(image_href):\n\t\"\"\"\n\tParse an image_"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\tparts = []\n\t\tif image_href.startswith('@'):\n\t\t\ttry:\n\t\t\t\turl = urllib.urlretrieve(image_href, image_href[1:])\n\t\t\t\tparts.append(\"http://www.bing.com{}\".format(url))\n\t\t\t\tparts.append(\"https://www.bing.com{}\".format(url))\n\t\t\texcept httplib.BadStatusLine as e:\n\t\t\t\traise ValueError(\"{} is an error: {}\".format(image_href, e))\n\t\telif image_href.startswith('file://'):\n\t\t\tparts.append(\"http://www.google.com{}\"."}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef list_path_components(input_path):\n\t\"\"\"\n\tParse the given path.\n\t\"\"\"\n\t# Parse the input path\n\tlist_path=[]\n\tfor x in input_path:\n\t\tlist_=parse_path_element(x)\n\t\tlist_path=list_[0]\n\t\n\t# Return the list\n\treturn list_path\n\n\ndef remove_path_components(input_path):\n\t\"\"\"\n\tRemove all components up to the last '/' in the input path.\n\t\"\"\"\n\tlist_path=[]\n\tfor x in input_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    def _walk_list(seq):\n        if not _sep_sep:\n            _gen_string(seq, _sep_sep, input_list)\n            return\n        if not seq:\n            return\n        path = _gen_string(seq, _sep_sep, input_list)\n        for elem in path:\n            yield elem\n \ndef _gen_string(sequence, sep, input_list):\n   \"\"\"Generate str string for given sequence\"\"\"\n   path = []\n   for elem in sequence:\n\t path.append(os.path.join(_str1path_sep, elem))  \n   return"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef file_list_to_file(input_list):\n    \"\"\"\n    Return a list of all file names.\n\n    @param input_list: list with file names (must be strings).\n    @return: list of all file names\n\n\"\"\"\n\tif str_case_sensitive(input_list):\n\t    return [str_case_insensitive(name)\tfor name in input_list]\n\telse:\n\t    return input_list\n\ndef list_to_file(path_list, file_extension, extension = \".txt\"):\n\t\"\"\"\n\tConvert a list of paths to a"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef remove_string_list(input_list):\n    \"\"\"Removes every string in list input.\"\"\"\n\n\ndef remove_string(string_list):\n    \"\"\"Removes element from list.\"\"\"\n\n\t# If string_list is empty, return\n\tif len(string_list):\n\t\tfor i in range(len(string_list)):\n\t\t\tprint string(string_list[i])\n\telse:\n\t\tprint(string_list)\n\ndef print_result(text, filename = ''):\n    \"\"\"Prints string.\n\nArgs:\n    text: string, text to print \n\nReturns:"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\nclass DirList(FileList):\n\t\"\"\"\n\tUsed to convert a path list into a FileList object.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    The converted list\n\nRaises:\n    TypeError\t\"\"\"\n\t\nclass DirException(Exception):\n\t\"\"\"Error when a directory or file does not exist.\n\t\"\"\"\n\tpass\n\nclass FileList(object):\n\t\"\"\"\n\tIterator over a string list.\n\nArgs:\n    path: list of strings\n\nRaises:\n    FileException\t\"\"\"\n\tdef __init__(self):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    def remove_os(string_list, file_handle):\n \tprint string_list\n\t\n    list_of_names = list_of_names.split()\n    # Find the longest possible pathname and read all\n    # sub files if needed\n    longest_name = None\n    longest_file = 0    \n    if len(list_of_names) > len(input_list):\n        longest_file = 1\n        for i in range(len(list_of_names)-1):\n            pathname = input_list[i]\n            if i == len(list_of_names)-2 or pathname[-"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef remove_trailing_os(input_list):\n    \"\"\"Remove trailing os seperators\"\"\"\n    for index in range(input_list.count(os.sep)):\n        os.remove(inputs[index])\n\ndef main():\n\t'''Main program function'''\n\t# Initialize a list of strings\n\tinput_list = [os.pardir] * 256\n\t# Remove trailing os seperators\n\tremove_trailing_os(input_list)\n\t# Find a maximum length of each string\n\tmaxlen = len(input_list)\n\t# Make a copy of"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef isURL(text):\n    \"\"\"Test if the given string is a URL\"\"\"\n    if not text:\n        return False\n    return len(text) > 0 and not text[-1] in \"mailto:\"\n\n\ndef removeFile(filename):\n\t\"\"\"Remove an existing file\"\"\"\n\tos.remove(filename)\n\t\t#print \"removed\"\n\n\ndef get_user_ip(user_ip_list, username = \"IP\"):\n\tif \" \" in username:\n\t\tusername = username[username.find(\" \"):]\n\telse:\n\t\tusername = username[0]\n\t\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef filename_string(input_list, pathname):\n\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef file_string(input_list, pathname):\n\t\"\"\"\n\tIterate over a string string and remove trailing os seperator characters.\n\nEach string is tested"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\nclass StringList:\n    \"\"\"\n    A list of lists of strings.\n    \"\"\"\n    def __init__(self):\n        self.s = []\n    def append(self, string):\n       self.s.append(string)\n    def size(self):\n       return len(self.s)\n    def appendleft(self, left):\n       self.s.insert(right, left)\n    def rfind(self, sep):\n       sep = sep.upper()\n       sep = str(sep).find(sep)\n       return sep\n\nclass Dir:\n\n    def __init__(self):\n        self.sep ="}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tl=re.findall(regex1, str)     \n\t\treturn regex2string(l[0])\t\n\t\t\ndef findInString(String, pattern1):   \n\t\"\"\"\n\tfunction used to check if the given string is a recognizable pattern\n\t\n\tExample: check if the string is 'dog'\n\t\"\"\"\n\t#return True\n\t#return False\n\t\ndef removeGroup(Group, regex1):   \n\t\"\"\"\n\tfunction used to remove the matching groups\n\t\n\tExample: remove the 'dog' group from the regex1\n\t\"\"\"\n\tif not all(["}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tfrom fnmatch import translate\n\t\ttranslate.translate(text.maketrans(\"\", \"\", \"\"), re.UNICODE) \n\nclass MyParserWithLexer(MyParser):\n\t\"\"\"This class implements syntax rules for the Mystery parser.\n\t\"\"\"\n\t# TODO: This should be moved somewhere more\n\t# TODO: Add a check function to handle the case where the regex in a\n\t#              regex pattern is too cumbersome to make parsing simpler.\n\tdef __init__(self, text):\n\t\tsuper(MyParserWithLexer, self).__init__(text)\n\n\tdef parse("}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ts=\"(?P<s>[ \\t]+(?P<pat>[ \\t\\n]+)*)(?P<p>\\S+)\"\n\t\tm=re.search(s, string)\n\t\tif m:\n\t\t\tg=m.group('p')\n\t\t\tm=re.search(r'(\\d+)\\s*(?:\\Z)', g)\n\t\t\tif m and m.group('pat') and not m.group('s'):\n\t\t\t\tstring=string+\"\\Z\"\n\t\telse:\n\t\t\tpass\n\treturn string\n\"\"\"\n\n\ndef get_all_matches(text):\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tpattern = ''.join(re.findall(pattern, s))\n\t\treturn pattern\nregexPattern = re.compile('(?!\\(.){1,4}|\\(.){1,5}\\)') # regex pattern\ndef strip_chars(string):\n\t\"\"\"\n    Remove characters that are in a string. The string includes all the characters in the string\n    if necessary. Remove all characters not in the string but all others, and then the entire string.\n    \"\"\"\n\t\ndef normalize(word):\n\t\"\"\"\n\tTake a word and translate it all to lowercase, removing non-white characters.\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tre_pattern = re.compile(unicode(unicode(re.compile(str(string)))) + \"$\")\n\t\treturn re_pattern\n\n\"\"\"\nThis method extracts each list of possible string values\n\nIt takes into account the regex pattern\n\"\"\"\ndef extract_all_strings():\n\t\"\"\"\n\tThis method extracts all possible strings\n\t\"\"\"\n\t\n#\t\ttry:\n\t\tinput_string = raw_input()\n\t\t\n\t\tprint \"Extracting string values...\"\n\t\n\texcept EOFError: \n\t\t#print \"I can't read the string.\"\n\t\tprint \"Try again with"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef _find_next_line(self, line): \t\t# return the following regex that matches line\n\t\twhile line.next()!= \"|\\n\":\n\t\t\tif line.next() == \"\\n\":\n\t\t\t\tline = line[1:]\t\t# remove last \\n char\n\t\t\t\tl_index = line.index(\"\\n\")\n\t\t\t\tif l_index < 0:\n\t\t\t\t\treturn \"\"\t\t# never reach end of line\n\t\t\t\treturn line\n\t\n\t\t\n\t\"\"\"\n\tConverts a string to regex pattern\n\t\"\"\"\n\n\tdef _find_first(self, line): \t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ttry:\n\t\t\tregex = re.compile( regex_str )\n\t\t\tres = re.sub( pattern, regex\t, regex_str )\n\t\texcept Exception as e:\n\t\t\traise ValueError( \"Can't convert string to regular expression\" )\n\t\treturn res\n\ndef re_matchAll( string ):\n\t\"\"\"\n\tre_matchAll()\n\t\"\"\"\n\tpattern = ''\n\tif string:\n\t\tfor char in string:\n\t\t\tif ord(char) <128 : r = chr( ord( char ) )\t\n\t\t\telse:\n\t\t\t\tr = chr("}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tm = re.search(pattern, unicode(text))\n\t\treturn m;\n\t\"\"\"\n\tthis method returns a list of any regex pattern that doesn't match the regex pattern\t\"\"\"\n\tdef any_any_regex_regex_pattern(text, pattern):\n\t\tfor match in regex_patterns:\n\t\t\tfor regex in regex_re:\n\t\t\t\tif re.search(regex, unicode(text)):\n\t\t\t\t\treturn list(match) + [regex]\n\t\"\"\"\n\tthat is, given a regex pattern string, and all the regexes in the input string\n\tis a list"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\treturn re.compile(r\"^\".join([r\"'([^']+)\"', r\"'([^']+)'\"]), flags=re.IGNORECASE)\n\nclass XSDTypeError(Exception): pass\n\ndef xsd_type_from_char(string, allow_zero=True):\n\tif string is None:\n\t\treturn None\n\tif not string:\n\t\treturn None\n\n\t# xsd_type_from_file()\n\tregex = re.compile(r\"\\d+\")\n\n\t# If no regex matches any characters, then a number is the default.\n\n\tif type(string)"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tregexpat\t=\"<.*?%s(.*):\" % re.escape(searchstr)\n\t\tregexpat = re.sub(_searchstr, r\"\\1\", regexpat)\t\n\t\treturn regexpat\t\n\t\n\t@staticmethod\n\tdef _parse_subn(subn):\t\"\"\"\n\tThis method is used to parse subn in string \"\"\"\n\t\tsubn = subn.replace(\"\\1\", \"\")\n\t\tsubn = subn.replace(\"\\t\", \"\")\t\n\t\tsubn = subn.replace(\"\\n\", \"\")\n\t\t# add separator\n\t\tsubn = subn"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tsubprocess.call [ sys.executable ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n\tdef get_result(self):\n\t\treturn self.result\n\n\tdef get_error(self):\n\t\treturn self.error\n\n\tdef print_result(self):\n\t\tself.result = []\n\n\tdef wait(self):\n\t\tself.result = None\n\t\tif self.result_ready(timeout=60):\n\t\t\tself.result_ready(timeout=60)\n\t\telse:\n\t\t\traise Exception(\"Timeout has left!\")\n\n\tdef wait_and_raise"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tself.logger.debug(' '.join(args if args else command))\n\t\tself.logger.debug('')\n\t\ttry:\n\t\t\tp = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n\t\t\tself.logger.debug(' '.join(command if command else p.communicate()))\n\t\t\tself.logger.debug('')\n\t\t\tself.logger.debug(' '.join(p.communicate()[0:100]))\n\t\t\tself.logger.debug('')\n\t\t\tp.wait()\n\t\t\tif p.returncode is None or p.returncode"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tsubproc\t\t= subprocess.Popen(cmd, shell=(not args.quiet) and os.environ.get(\"SANDBOX_DISABLE\"))\n\t\t(status_code, output) = subproc.communicate()\n\t\tstatus = subprocess.Popen(cmd, shell=True).wait()\n\t\treturn status\n\texcept subproc.NoSuchProcess:\n\t\tif os.path.exists(args.file):\n\t\t\tos.remove(args.file)\t\t# This should be ignored by any exception handling script\n\n\tprint(\"[+] Executing command\")\n\tcommand = args."}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tsubprocess.call(command)\n\n###########################\n## The main program program starts here ##\n###########################\n\nif __name__ == \"__main__\":\n\t\n\t# Set all the directories if one run in bash\n\tcwd = os.path.join(os.path.expanduser('~'), '..', '..', '..')\n\tenv = os.environ.copy()\n \tenv['PATH'] = cwd\n \ttry:\t\t   os.chdir(cwd) \n\n\texcept OSError:\tpass\t\t\n\n\t# Add python modules to path\n\tsys.path.append(os.path.join(cwd"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tsubprocess.call(list(map(os.system, commands.split(commands))))\n\texcept OSError:\n\t\tprint >> sys.stderr, 'Not installed'\n\ndef main(argv=None):\n\tif not argv:\n\t\targv = sys.argv\n\n\tif len(argv) > 1:\n\t\tprint \"Usage: \" + __name__ + \" <command>\"\n\t\traise SystemExit\n\tif argv[1:] == ['-','']:\n\t\targs = []\n\telif argv[1:] == ['-m','']:\n\t\tpass\n\telse:\n\t\tprint \"Not"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\treturn subprocess.check_output(cmd)\n\ndef make_dir(path):\n\t\"\"\"\n\tMake any folders under the given path and return them.  The returned paths will stay\n\tabsolute.\n\t\"\"\"\n\t\ndef mkdir(path):\n\t\"\"\"\n\tMake any folders under the given path and return them.  The returned paths will stay\n\tabsolute.\n\t\"\"\"\n\t\ndef link_to_file(src, dst, overwrite = False, mode = 'link'):\n\t\"\"\"\n\tWrite a symlink to a file or directory.\n\tsrc should be an absolute path"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif not command in [None, \"\"]:\n\t\t\treturn 'Unable to find {0} command \"{1}\" in the PATH'.format(command, string_rep)\n\t\n\tdef get_output():\n\t\t\"\"\"Return a list of output files and commands. All files should be in\n\t\tthe 'OUTPUT_DIRECTORY' subdirectory of the PATH under the default PATH.\"\"\"\n\t\toutputs = subprocess.check_output(['find', '/vagrant-tools/vagrant-tools', 'vagrant-tools/vagrant-tools/bin'])\n\t\toutputs_directories = [x for x in"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\treturn subprocess.check_output(args,shell=not cmd)\n\t\t\n\tdef __str__(self):\n\t\treturn \"The SkyBeeBot is running on \" + self.name +\".\"\n\t\n\nclass SkyBeeBot(object):\n\tdef __init__(self,user,passWD,t_file):\n\t\t\"\"\"Initializes the object. The user is assumed to point to the proper user before doing this.\"\"\"\n\t\tself.user = user\n\t\tself.passWD = passWD\n\t\t# Create instance of a connection to the API. This allows us to connect to Sky"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(command,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\t\texcept SystemExit:\n\t\t\tprint('\\n[!] Error call subprocess.Popen().')\n\t\t\treturn False\n\n\t\tout,err = process.communicate()\n\tprint('* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * -***\\n\\n* * * * * * * * * * *"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t(pid, cmd) = os.popen2(cmd)\n\t\t# Run the program if provided by calling from command line argument.\n\t\tif not os.path.isfile(sys.argv[0]):\n\t\t\tprint \"Error running \" + cmd + \" (using stdin first)\"\n\t\t\tsys.exit(1)\n\t\t# Execute the program within the terminal.\n\t\tcmd_output = sys.stdout.write(cmd + \" \" + str(pid) + \"\\n\")\n\t\t# Return the output of the program.\n\t\tsys.stdout.flush()\n\t\tos.system("}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n    def setUp(self):\n\t\"\"\"\n        Create test server for tests\n        \"\"\"\n        # Create Server Class\n        srv_host = '10.1.1.1'\t # IP address for Server Service\n        srv_port = 5683\t\t\t\t\t# Server port\t\n        self.server_class = server.ServerClass.from_ipaddr(srv_host, srv_port)\n        \n        # Check Request\n        try:\n\t\t\tclient_addr = ipaddress.ip_address(srv_host)\n            if client_addr[0] == 4:\t# IPv4 address of client is IPv4\n\t\t\t\tself."}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\tdef makeNode(self,addr,type=\"INVITE\",proto='IPV4'):\n\t\t\"\"\"\n\t\tCreates a socket node and return a TCPConnection.\n\t\t\n\t\tArgs:\n\t\t\t-- addr -- IPv4 address or IPv6 address\n\t\t\t-- type -- IPv4|NS\tif 'INVITE', use the socket from localhost:80\n\t\t\t-- proto -- protocol of the connection (default: 'INVITE')\n\t\tReturns:\n\t\t\tThe TCPConnection.\n\t\t\"\"\"\n\t\tself.socket.setsockopt(IPPROTO_IPV6,INET6,1)\n\t\t\n\t\treturn self"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\tv6_addr = validate_ipv6(ip_str_addr)\n\t\t\n\t\t\"\"\"\n\t\tTest if IPv6 is an IP/v6 address address\n\t\t\"\"\"\t\t\n\t\tif ip_addr in valid and valid.count(ip_addr) == 0:\n\t\t\t\"\"\"\n\t\t\tTest IPv6 address\n\t\t\t\"\"\"\n\t\t\tip_addr_4 = ip_str_addr.replace(\"0.0.0\", \"\")\n\t\t\tip_addr_1 = ip_str_addr.replace(\"0.0.0\", \"\")\n\t\t\tip_addr_0 = ipaddress.ip_"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\tdef test_ip_and_address(self):\n\t\t\"\"\"Test IP address of IPv4/IPv6\n\n\t\t\"\"\"\n\t\tif self.IPV4_ADDRESS is None:\n\t\t\traise test_util.test_case._TC_V7_ERR_ADDR_NOT_SPECIFIED\n\t\t\n\t\tif self.IPV6_ADDRESS is None:\n\t\t\traise test_util.test_case._TC_V7_ERR_ADDR_SPECIFIED\n\t\t\n\t\tif self.IPV4_ADDRESS == ipaddress.IPv4:\n\t\t\tpass\n\t\telse:\n\t\t\tself._tester = ipaddress."}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\tdef test_ipv4_port(cls, IPv4_port):\n\t\t\"\"\"\n\t\tTest if IPv4_port is valid and get the port \n\t\t\"\"\"\n\t\tcls.IPv4_port = IPv4_port\n\t\ttest_class = cls\n\t\tIP4_config = ipaddress.ip_interface('192.168.0.100/24')\n\t\tif test_class:\n\t\t\treturn IP4_config.port\n\t\telse:\n\t\t\treturn 1\n\n\tdef test_ipv4_address(cls, IPv4_address):\n\t\t\"\"\"\n\t\tTest if"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ndef assertAddressEqual(source_address, destination_address):\n\t\"\"\"\n\tAssert if two IP addresses are equivalent\n\n    \t>>> ipaddress.ip_network('10.0.2.2/24')\n    \tTrue\n    \t>>> ipaddress.ip_network('10.0.2.2/240')\n    \tTrue\n    \t>>> ipaddress.ip_address('10.0.2.2')\n    \tFalse\n\n\t\"\"\"\n\tif ipaddress.ip_network(source_address).version!= \\\n\t\tipaddress.ip_network(destination_address).version:\n    \t\traise Exception(\""}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\tdef test_get_ip_address_list(self):\n\t\t\"\"\"\"\"\"\n\t\tip_address = ipaddress.IPv4Network('1.2.3.4') \n\t\tself.assertEqual(ip_address, [])\n\t\t\n\tdef test_get_ip_address_list_multiple(self):\n\t\t\"\"\"\"\"\"\n\t\tip_address = ipaddress.IPv4Network('::1') \n\t\tself.assertEqual(ip_address, ['1.2.3.4'])\n\nif __name__ == '__main__':\n    unittest.main()\"\"\"\nTests for the email sending API"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ttry :\n\t\t\tip = ipaddress.ip_network(str(ip))\n\t\texcept :\n\t\t\tprint(\"IP address is not valid\", ip)\n\t\t\treturn False\n\t\tip = ip.split(' ')[0]\n\t\tif ip == '127.0.0.1' :\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\nclass ip_network_acl(unittest.TestCase) :\n\n\t\"\"\"\n\tTest if IPv6 address or not\n    \t\"\"\"\n\tdef test_Ipv6Address(self) :\n\t\t\"\"\" IP address\n        \t\"\"\"\n\t\ttry :"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\tself.assertEqual(ipaddr.__str__(),\t\"2001:db8::1\")\t# non-FQDN - no IPv4 address\n\t\tself.assertEqual(ipaddr.IPv4Address('2001:db8::1'),\tip.inet_pton(0x0ffff, 0x16ffff))\t# with invalid IP address\n\t\tself.assertEqual(ipaddr.IPv4Address('2001:db8::1',4),\tip.inet_ntop(0x0ffff, 0xffffff))\t# with invalid ip address\n\t\tself.assertEqual(ipaddr.IPv4Address('"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass PGPublicKeyTest(unittest.TestCase):\n\n    def randomKey(self, x, n=16):\n        return chr(n) ^ ord(x.encode('hex')+x),\n\ndef test_pgp(pub_key, priv_key): \n    for n in range(1, 20):\n        x, y = pub_key()\n        if x == priv_key:\n            break\n        n = (n*(2**x.level()))+1\n        k = PGPKey()\n        k.n = n\n        k.x = x\n        k.y = y\n       "}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\nfrom pysolr.bnf.parser.rdflib import Parser\t\nfrom pysolr.bnf.parser.triples import (RDFSTree, RDFSGraph, RDFSGraphQuery)\n\nclass DfsGraph():\n\t__slots__ = ('name','subClassOf',)\n\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself.subClassOf = set()\n\t\tself.nameHash = dict()\n\t\t\n\tdef __str__(self):\n\t\treturn '{}: {} (subClassOf:{})'.format(self.name"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\troots = set()\n\t\tfor gpr in gpr_graph.get_sorted_graphs()[0]:\n\t\t\tif gpr not in roots:\n\t\t\t\troots.add(gpr)\n\t\t\twhile gpr.child(0).parent!= gpr.parent(0):\n\t\t\t\troots.add(gpr.child(0).parent(0))\n\t\treturn roots\n\n\tdef find_roots_leaf(self, gpr_graph, cats, predicate_depths):\n\t\t\"\"\"\n\t\tGiven a GPrTree rooted at 'gpr_graph', and a list"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t# We will also consider an intermediate descendant to the parent:\n\t\tif (parent == rdflib.Graph):\n\t\t\tchildren = set ()\n\t\telif (parent == rdflib.Graph): \n\t\t\tchildren = set ( (self._subClassOfClass, self._subClassOfClass, self._subClassOfClass) )\n\t\telse:\n\t\t\traise ValueError, \"Can not find a non-nested descendant of the parent of the root.\"\n\t\treturn list(filter ( lambda x: x, children ) )\n\tdef get_roots( self ):\n\t\t\"\"\"Get all roots of the graph"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\t# get some information, like 'node' or \"node2\"\n\t# FIXME: if the key is \"node\" then the value will be 'node2'\n\tfor p in graph.query(skos.Graph):\t\t# loop through graph-parents\n\t\tp_parent = p.graph.node.parent\n\t\n\tnode_relations = []\t\n\tparent_relations = []\n\t\t\n\t\t\t\n\tfor p in graph.query(skos.Graph):\t\t# loop through graph-parents\t\t\n\t\tp_parent = p.graph.node.parent\n\t\t\n\tfor"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\tresults = self.search(graph)\n\t\n\tif not results:\n\t\treturn {}\n\t\n\tpaths = set(res.graph.keys())\n\t\n\troots = {}\n\n\tfor prop in results[0]:\n\t\tfor parent in results[1]:\n\t\t\ttop = self.subClassToGraph(prop, parent)\n\t\t\tif top not in roots:\n\t\t\t\troots[top] = {}\n\t\t\troots[top][prop] = [top, prop]\n\t\n\treturn roots\n \n\n\ndef find_root_names(graph, subClassOf, predicate = None):\n\t\"\"\"Find"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t# We must have a set so we can identify it right.\n\t\tgraph_root = None\n\t\tdescendants = set()\n\t\tfor (child, prop, parent) in hierarchy_list:\n\t\t\tif graph_root is None:\n\t\t\t\tgraph_root = parent\t\t\n\t\t\tif parent in descendants:\n\t\t\t\tcontinue\n\t\t\tif not graph_root.id in descendants:\n\t\t\t\tdescendants.add(graph_root.id)\n\t\t\t\tgraph_root = rdflib.ConjunctiveGraph()\n\t\t\tgraph_root.subClassOf = rdflib.RDF.type\n\t\t\tif len"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tpath = graph.get_path(path)\n\n\t\tresults = set()\n\t\tfor r in path:\n\t\t\tfor c in r:\n\t\t\t\tresults.add(c)\n\t\t\t\n\t\troot = rdflib.subgraph.MultiDiGraph()\n\t\troot.graph.walk_nodes(resurrect=\">\")\n\t\tfor n in results:\n\t\t\tif n == root.graph.subgraph.subclassOf:\n\t\t\t\tr = rdflib.RDF.subgraph(n)\n\t\t\t\treturn r\n\t\"\"\"\n\t\t\tif n in results:\n\t\t\t\tr = rdflib.RDF."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t# Get the nodes of the hierarchy.\n\t\troot_nodes = []\n\t\tnode_order = []\n\t\tfor root, _, _ in rdflib.Tree.parse(in_stream):\n\t\t\t# Get if it is the first and the children of the sub-class\n\t\t\t# hierarchy.\n\t\t\tif root.predicate in (\"child\", \"prop\", \"parent\"):\n\t\t\t\t\t\troot_nodes.append(root.id)\n\t\t\telse:\n\t\t\t\t\t# Get if we're on the last sub-class parent\n\t\t\t\t\tif root in node_order:\n\t\t\t\t\t\t# Make sure the tree is valid"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\tdef recursive_traverse(a, b):\n\t\t\tif not b:\n\t\t\t\treturn\n\t\t\tif not hasattr(a,'subClassOf'):\n\t\t\t\t# if no subClassOf is defined, create an EmptyGraph\n\t\t\t\t# if there is no subClassOf, we'll explore its root\n\t\t\t\t# so that we won't explore its children\n\t\t\t\t# for it to contain subClassOf\n\t\t\t\tif not hasattr(b,'subClassOf'): a.subClassOf = EmptyGraph(b)\n\t\t\t\ta.subClassOf.prop = b.prop\n\t\t\t\ta.sub"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\n\t# This will return triples (child,prop,parent), i.e. the direction\n    # of  A/B/B  for A/B; where A and B are both the graph and a\n    # graph.\n\treturn []\n\ndef find_roots(graph, rdflib.RDFS.subClassOf, skos_from_subclass=None):\n\t\"\"\"\n\tFind all the roots of a subgraph of a subgraph of another graph.\n\nFind all the roots of a graph with two names separated by a space and\nthen get the roots of: A/B"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry:\n\t\t\tstr\n\t\texcept NameError:\n\t\t\t# Assume Python 2.x\n\t\t\tstr = str.decode('utf-8')\n\n\t\tf = open(YAML_FILE, 'w')\n\t\tf.write(yaml.dump(yaml_dict, default_flow_style=False))\n\t\tf.write(str)\n\n\t\tf.close()\n\n\t\treturn str\n\n\tdef write(self, source, destination):\n\t\t# Import files are hard-coded here!\n\t\timport os\n\t\tfilename = os.path.join(self.settings['paths']['dest'],"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\timport re\n\t\tyaml.dump(self.py2unicode(), self.open(\"data/source/source.yml\", \"w\"))\n\tdef _generate_source_data(self):\n\t\t\"\"\"Generate the source data\"\"\"\n\t\tfrom tempfile import gettempdir, NamedTemporaryFile\n\t\ttempdir = gettempdir()\n\t\twith NamedTemporaryFile(suffix=\".py\") as py3_file:\n\t\t\tpy3_file.write(self.py2unicode())\n\t\tpy4_file = tmp_file = tempfile.NamedTemporaryFile()\n\t\tsource_data_list = yaml.load(py3_file."}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tself.stream = stream\n\t\tself.save(stream)\n\tdef dump(self, stream):\n\t\t\"\"\"\n\t\tDump to a py2-unicode or py3-string \n\t\t\"\"\"\n\t\tf = stream.write\n\t\tif self.stream is None:\n\t\t\tf.write('')\n\t\telse:\n\t\t\t# Write bytearrays\n\t\t\tf.write(yaml.dump({'array':[]}, default_flow_style=False))\n\t\t\t# Remove empty arrays\n\t\t\tf.write(yaml.dump({'array':[]}, default_flow_style=True))\n\t\t\t#"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\t#print yaml.dump({'data': 'hello_world', 'indent': 2}, default_flow_style=False)\n\t#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n\"\"\"\n\nCreated on 13 de Jun 15 11:49:51 2009\n\n@author: Josef Perktoldun\n\"\"\"\n\n# This script contains the two functions corresponding to Morphology-data-structures, which are the methods for Morphology calculations. All functions are defined in these dictionaries.\n#\n#\n# @author: Josef"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tself.config.data = {}\n\n\tdef __call__(self, obj=None, data=None):\n\t\t\"\"\"\n\t\tA generator function which calls the given callable obj or data in self.config.data or returns\n\t\tthe default value\n\t\t\"\"\"\n\t\tif obj is None and data is not None:\n\t\t\treturn self.config.get(self.default, lambda: data)\n\t\telse:\n\t\t\treturn self.config.get(self.default, lambda: obj)\n\n\tdef yaml(self, data=None):\n\t\t\"\"\"\n\t\tLoad yaml data, return default"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tobj = yaml.safe_dump(ast.literal_eval(unicode_obj), default_flow_style=False, default_flow_style_args=())\n\texcept:\n\t\tobj = ast.literal_eval(unicode_obj)\n\n\t# TODO: Make this a string and then return the unwrapped string. \n\treturn objfrom.base import *\n\nclass SiffIO:\n\n\tglobal config\n\n\tdef __init__(self):\n\n\t\tself.globalConfig = config\n        return\n\n\tdef parseSifs(self, line):\n\t\tlines = line."}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tdata= yaml.safe_load( f )\n\t\tfor key, val in data.items() :\n\t\t\tout.write( '\\t' + key + '=' + str( val ) + '\\n' )\t\t\t\n\t\tout.write( '\\n' )\n\n\tif __name__ == '__main__':\t\t\n\t\tout = open('exportResults','w+')\n\t\tprint( \"Export results to:\" )\n\t\t# for key, val in data.items():out.write( '\\t' + key + '=' + str(val) + '\\n' )\t\n\t\tout.close"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tf = StringIO(\"{0}\\n\".format(yaml.dump({\"1\": {\"a\": 1, \"b\": 2}, \"2\": {\"a\": 2, \"b\": 3}})).encode('utf-8'))\n\t\td = {}\n\t\tyaml.dump(d, f)\n\n#\tfrom py3unicode import unicode_\n\t\n#\tfrom py2unicode import basestring\n\t\n#\timport pprint\n\t\n#\tfrom py3unicode import unichr\n\t\n#\t#from py2unicode import binary_\n\t\n\n\n        \n\n#\timport pprint\n\n#\timport getopt\n#"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tyaml_lines = [ item for item in map(dump, py3_lines) if isinstance(item, yaml.Scalar) ]\n\t\treturn yaml.dump(indent='\\t\\t', stream=sys.stdout, Dumper=SafeDumper)\n\n\tyaml_lines = dump(py2_lines)\n\n\treturn yaml.dump(indent='  '), py3_lines\n\t\"\"\"\n\tif __debug__:\n\t\t__indent = True\n\t\t_debug('        {0}   dump {1}'.format(py3_lines, indent))\n\treturn yaml.dump(indent"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\treturn \" \".join(lines)\n\t\t\"\"\"\n\t\"\"\"\n\tWrite out the file\n\t\"\"\"\n\tout = open(file_name, 'w')\n\tyaml.dump(get_items_dict(), out, default_flow_style=False)\n\n\t'''\n\ndef main():\n\t'''\n\tPrint program entry\n\t'''\n\n\targs = get_sys_argv()\n\n\tif args[1]!= \"--config\":\n\t\tprint \"Please specify the config file from the program\"\n\t\tprint \"Usage:\\npython get_items\"\n\t\tsys.exit"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\n\t# logging options \n\t_default_logger = {\n\t\t'level': logging.DEBUG,\n\t\t'type': 'warning',\n\t\t'description': False,\n\t\t'type_name': 'Python'\n\t}\n\t\n\tdef __init__(self):\n\t\t\"\"\"Initialise the application logger\"\"\"\t\n\t\tlogging._levelLogger.append(self)\n\t\tlogging.getLogger('gazoo.application').addHandler(sys.stderr)\n\t\n\tdef printError(self,message):\n\t\t\"\"\"Raise a RuntimeError in case the application is dead and will print a message to the"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef __init__(self, name, id=None):        \n\n\t\tself.logger = logging.getLogger(name)\n\t\tself.log_file = logging.FileHandler(os.path.join(os.getcwd(),\"%s%s%s.log\"% (name,_APP_NAME,id)))\n\t\tself.log_file.setLevel(logging.DEBUG)\n\t\t#logging.basicConfig(filename=True)\t\n\t\t\n\t\t#logging.debug(\"DEBUG: %s\\tFORMAT: %s\\t#ID: %s\" % (True, self.format, id))"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tself.logger = logging.getLogger('')\n\t\tlogging.basicConfig(filename=\"/tmp/my_logger.txt\", level=logging.DEBUG,format='%(asctime)s : %(message)s')\n\n\tdef dump_debug(self):\n\t\tlogging.info('\\n')\n\t\tlogging.info(\"%s: My application logger for %s has been dumped from  %s\"% (datetime.datetime.now(), sys.argv[0],self.programName))\n\t\n\tdef isDebug(self):\n\t\treturn self.logger.level==logging.DEBUG\n\ndef _log(text):\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\ndef log(string):\n\tprint(string, end=' ')\n\tlogging.basicConfig(filename=\"wsc.log\",level=logging.DEBUG) \n\t\n#==============================================================================\ndef updater(log1,log2):\n\t\"\"\"Function to get new log name from a list of 2-tuples\"\"\"\n\tprint(\"  updater: update log list\")\n\tlog1 = list(log1)\n\tlog2 = list(log2)\n\treturn log1,log2\n\n#==============================================================================\ndef printlog(log):\n\t\"\"\"Print function\"\"\" \n\tprint(\"%s\" %"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n###################################################################\n# Start-up\n###################################################################\n\nstartLogger = logging.getLogger('Server')\nserverName = 'tweets_v4' \n\n# Get the IP address of your tweets server\nprint( 'Connecting to Tweets server...' )\nserver = socket.socket( socket.AF_INET, socket.SOCK_STREAM )\nserver.bind(('localhost', 5432))\nprint(\"Server starting on: \"+serverName)\n\n# Server port\nserverPort = 19697 #Server port is required\n\n# Send the request with the username and password to the server\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nclass Logger(object):\n\t\"\"\"Log helper class\"\"\"\n\t\n\tdef __init__(self):\n\t\t\"\"\"Initialise logger\"\"\"\n\t\tself.logger = logging.getLogger('app')\n\t\t\nclass MyLoggerPlugin(Logger):\n\t\"\"\"Custom logger plugin\"\"\"\n\t\n\tdef __init__(self):\n\t\t\"\"\"Create custom logger\"\"\"\n\t\tLogger.__init__(self, 'app')\n\t\t\n\tdef process(self, msg):\n\t\t\"\"\"Update and set date handler, and set hostname as appropriate\"\"\"\n\t\tprint 'Got data (date handler):', msg.date\n\t\tlogging.info(\"Send timestamp"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tlogger = logging.getLogger('general_gui')\n\t\tlogger.setLevel(logging.DEBUG)\n\n\t\t# set up path to logging directory for output files.\t\n\t\tpath = os.path.abspath(str(__file__).replace(' ','_')) + os.sep\n\n\t\t# get current project directory\n\t\thome = os.getcwd()\n\t\t#get current working directory\n\t\tbasedir = os.path.dirname(os.path.abspath(os.path.join(__file__,os.pardir)))\n\n\t\t# write log output files\n\t\tlogfile = open(os.path.join("}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tlog = logging.getLogger(__name__)\n\n\t\t#logging.basicConfig(level=logging.DEBUG)\n\t\t#logger.setLevel(logging.DEBUG)\n\n\t\t#setup logging\n\t\tlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\t#logger.info('Logging to server at %(address)s' %  {'address': config['SERVER_HOST'],})\n\t\tlog.debug(\"Logging level %d\" % (logging.DEBUG))\n\t\t#logger.critical('Logging level %d'"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tif (not root_logger.handlers):\t\t\t\n\t\t\troot_logger.setLevel(logging.DEBUG)\n\telse:\n\t\t# No root logger, log everything\n\t\troot_logger.setLevel(logging.NOTSET)\n\ndef start_logging(target, *args):\n\tlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG, filename=str(target))\n\ndef stop_logging(target):\n\tlogging.shutdown()\n\ndef run_single_program(program):\n\t\"\"\"\n\tRun a program using multiple programs\n\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tclass MessageHandler(logging.Handler):\n\t\t\t'''classdocs'''\n\t\t\tdef __init__(self,level,formatter,**kwargs):\n\t\t\t\t'''init'''\n\t\t\t\tlogging.Handler.__init__(self,**kwargs)\n\t\t\t\tself.level=level\n\t\tformatter = logging.Formatter(fmt='%(levelname)-8s %(message)s')\n\t\thandler = MessageHandler(level=self.level,formatter=formatter)\n\t\tself.addHandler(handler)\n\t\ndef main():\n\t'''main app'''\n\tlogger=logging.getLogger('muse.main')\n\tlogger.info('init"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\na = np.zeros(shape)\t\n\ndef make_data_block(data):\n\t\"\"\" create block by appending one block if it's defined\n\n\t\t*data*\n\t\t\tarray of rows and columns\n\t\t*index*\n\t\t\trow where data belongs to\n\t\"\"\"\n\t\ndef arrayify_columns(data, index, dtype=None):\n\t\"\"\" return an array of columns with a dtype for each index \"\"\"\n\t\n\t\n\treturn np.dstack([block.data[i] for block in data[index[:,col]] for i in range(data.shape[1])])\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t    \n# create a numpy.ndarray and dtype\n\nshape = tuple(map(np.dtype, shape))\n  \n# create a numpy.ndarray with a shape and dtype\nwith (np.dtype(np.uint8) if dtype[0] else np.dtype(np.uint16)) \\\nin shape:\n\tdata = tuple(map(np.dtype, data))\n# create a numpy.ndarray with a shape and dtype\nwith tuple(map(np.dtype, shape)) in shape:\n\tdata = tuple(map(np.dtype, data))\n\n# create"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\t'''\ndef create_array(shape, dtype):\n\t\"\"\"\n\tCreates new array for the array given its shape and dtype.\n\tA new Array will be created if not already.\n\nParameters\n----------\nshape : tuple\n    shape of the array\ndtype : `numpy.dtype`\n\tdata-type of the array to create\t\"\"\"\n\t\t'''\n\tReturns an new array representing the matrix\n\tgiven an array and its shape and dtype.\n'''\n\t\tif shape == ():\n\t\t\tshape, dtype = dtype.as_tuple()\n\t\t\treturn M = empty_matrix(shape"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\n\t# initialize a random variable to ensure the correct dtype is passed.\n\t# Note that the size of the array can not change with zero, such that\n\t# dtype is a dtype type.\n\tself._init_uniform_(np.random.uniform(size=shape))\n\tself._name = \"Uniform\"\n\n\t# compute an array\n\tif self._doEpscale():\n\t\tif self.dtype.is_integer and self.shape[0] == 0: self._init_linear_(self.shape[0], 0)\n\t\t\tm = self.shape[0"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\t# Get the type\n\treturn dtype \n\n\t\nclass H3D(object):\n\t\n\t#def __init__(self, width, height,**kwargs):\n\t#\tself.width=width\n\t#\tself.height=height\n\tdef __str__(self):\n\t\tn = 'h3d'\n\t\treturn '%s %s   \\n' % (n, (self.width,self.height))\n\t\n\t\n\t\n\tdef __repr__(self):\n\t\treturn self.str()\n\t\n\t\n\t\n\tdef __iter__(self):\n\t\treturn np.arange(self."}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n#\tThis part sets up the variable space for data-shaped functions \n\ndef space(shape=None, dtype=np.float64, name=None):\n\t\"\"\"\n\tThis function will return a copy of the array,\n\ta new array with size equal to np.max(shape.values(0)) of \n\tdata-shaped functions.\"\"\"\t\n\n\tdef space(self, size=None):\n\t\tif size is None:\n\t\t\tn = self.np.max(self.shape)\n\t\telse:\n\t\t\tn = self.shape[1]\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef toArray(arr : np.array) :\n\t\t\n\t\"\"\" \t\t\tConverts an array to a numpy array\n\t\t\n\t\tParameters\n\t\t----------\n\t\t\tarr : numpy.array\n\t\t\ttype : float\n\t\n\t\n\tParameters\n\t----------\n\tarr : ndarray\n\t\tarray to be converted\n\tdtype : type\n        optional : float\n\t\t\ttype = float\n\t\n\t\n\tReturns\n\t----------\n\tarr : dtype\n\t\tThe array to be converted\n\t\"\"\"\n\tif type(arr) == np.dtype('float'):\n\t\treturn arr\n\telse:\n\t\treturn np.fromfunction(lambda"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\nshape = (10,10)\n\n\n# The original array used for the first column is missing\nd2 = np.array([10,1,2,3,4], dtype=np.uint8).view(dtype)\n\n\n# Create the second column\ncols = np.array(d2, dtype=np.int8)\n\n\n# Create the third column\ncols = np.array(d2, dtype=np.uint16)\n\n\n# Create the fourth row\nrows = np.array(d2, dtype=np.uint32)\n\n\nprint(\"test"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tshape = tuple(shape)\t# flatten\n\tdtype = np.dtype(dtype) # numpy.dtype(dtype)\n\tdtype = np.dtype(dtype)  # numpy.dtype(dtype)\n\t\n\tif len(shape) == 2:\n\t\tarr = np.empty(shape, dtype)\n\t\tarr[:] = array(shape, dtype)\n\t\treturn arr\n\t\t#return array([x%100 for x in np.array(shape)])\t\n\telif len(shape) == 3:\n\t\tarr = np.empty(shape, dtype)\n\n\n\treturn arr"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tfor size in shape:\n\t\t\n\t\tif np.size(shape)!=shape[0]:\n\t\t\traise ValueError(\"shape must be of dimension %s but was %i\" % \n\t\t\t\t(np.size(shape),np.size(shape[0])))\n\n\tif dtype in [np.dtype(np.float32),np.dtype(np.float64)]:\n\t\tdata=np.zeros((np.product(shape))).reshape(shape)\n\t\t\n\telif dtype in [np.dtype(np.int32),np.dtype(np.int64)]:\n\t\tdata=np."}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\ttry:\n\t\t\tscipy.special.digamma\n\texcept AttributeError:\n\t\timport math\n\t\tsigma = 0.1\n\ttry:\t\n\t\tscipy.special.dizeta\n\texcept AttributeError:\n\t\timport math\n\t\tsigma = 0.1-0.2\n\t\t\n\tif method == \"gaussian\":\n\t\tm = sigma* (math.cosh(0.4*(float(sigma)/0.2)+math.cosh(0.4*(float(sigma)/0.4)+math.cosh(0.4*(float(sigma)/0.6)+math.cosh("}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\tif np.isnan(self.x):\n\t\t\treturn np.nan\n\t\tdq_x = np.sqrt((self.x*self.x-self.y*self.y))\n\t\treturn 0.1*dq_x\n\t\t\n\tdef get_f(self):\n\t\t\"\"\"\n\t\tGenerate Gaussian distribution function from current position (in dx and dy). \n\t\t\"\"\"\n\t\t\n\t\tf = scipy.special.fmin_sgn(self.x)\n\t\tf = f+1 if self.f < 0.015 else -0.05\n\t\treturn f\n\t\t\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\tplt.figure(figsize=(12,8))\n\t\tplt.plot(x,y, '.g--', linewidth=2, alpha=.9)\n\t\t#plt.plot(x,y, '--k', linewidth=2)\n\t\tplt.fill_between(x,y, y+sigma, alpha=2*sigma)\n\t\t#plt.plot(x,y, '--y', linewidth=2)\n\t\t#plt.fill_between(x,y, y+sigma, alpha=2*sigma-0.2)\n\t\t#plt.fill_between(x"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\treturn np.sqrt(( (2 * self._mu) ** 2 + ( 2 * self._sigma) ** 2 ) / np.sqrt( 4 ) )\n\n\t\"\"\"\n\tGauss centered around 1.0 with a sigma of 0.1 and return its mean.\n\n\t>>> kmeans = gaussian( sigma = 0.1, sigma_max = 20 )\n\t>>> print kmeans\n\t0.05\t\"\"\"\n\tdef __init__( self, sigma = 0.1 ) :\n\t\tself._sigma = sigma\t#sigma for sigma_max\n\t\tself._mu = 0."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n#\t\t\"\"\"  \t\n#\t\tGaussian centered around 0.2 with a sigma of 0.0.\n#\t\t\"\"\"\n#\t\tdef __init__(self, N, C, sigma=0.001):\n#\t\tself.sigma = sigma\n#\t\tself.N = np.atleast_1d(N)\n#\t\tself.M = np.atleast_1d(C)\n#\t\tX = np.array([N, C, 1])\n#\t\tself.X = np.array(X)\n\t\n\tdef sample_"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\tp_c = np.sqrt(2.*(sigma - sigma[0]))**0.5\n\t\tp_c = np.abs(p_c)\n\t\treturn p_c\n\n\n\n\ndef _createS(Nside, Nside_corr):\n\t\"\"\"\n\tHelper function for calculating singularity coefficient matrix\n\tand then using the Gaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\n\n\tsigma = Nside_corr*(Nside**2)-sigma\n\tsigma = (sigma-sigma[0])*(Nside_corr*(Nside"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\treturn [np.random.random((2,2))\tfor i in range(10)]\t\n\ndef sigmoid_func(x):\n\t\"\"\"\n\tComputes sigmoid function of x element\n\t\"\"\"\n\treturn 1.0/(1+exp(-x))\t\n\n\ndef l2_regularizer(theta):\t\n\t\"\"\"\n\tL2 regularization function\n\t\"\"\"\t\n\treturn 0.000001\n\t\t\n#def L1_regularizer(theta, p):\n\ndef leaky_relu_func(theta):\t\n\t\"\"\"\n\tGradient:\n\t\t- sigmoid of theta"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef _eval(self, t0):\n\t\t\"\"\"\n\t\tEvaluate the gaussian \n\t\t\"\"\"\n\t\treturn 0.5*np.exp(-(t0 - self.mu)*self.sigma*t0) + 0.5*np.exp(-(self.mu - t0)**2)\n\t\t\n\nclass QA (object):\n\n\tdef __init__(self, C):\n\n\t\tself.C = C\n\t\tself.B = None\n\t\tself.mu = None\n        \n\tdef _get(self):\n\t\t\"\"\"\n\t\tGenerate a Gaussian centered around the position of the"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\ndef kernel_gauss(x, sigmas, **kwargs):\n\t\"\"\"\n\tKernel of Gaussian centered around 0.2 with a given sigma sigma and return \n\tan estimate of the kernel and estimate its inverse kernel.\n\t\"\"\"\n\treturn np.exp(-((x-0.2)**2/sigsmas)**2/np.sqrt(2))\n\ndef kernel_gauss_from_noise(x, sigmas, noise_std=0.05):\n\t\"\"\"\n\tGaussian based on kernel, using Skew-Hastings Gaussian with sigma\n\tof 0.2"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\tprint \"Gaussian centered around 0.2 with sigma of 0.1\"\n\t\tsigmaFactor = ((len(s) - 5) * 4 - 3) / 4\n\t\tmean, std, cov = mtp.mtp(N1, s, sigmaFactor)\n\t\tprint \"Mtp_test\"\n\t\tprint cov\n\t\t#\n\t\t# 1. Find all points at sigma\n\t\tn1, n2, n3, n4, n5, n6, n7 =  mtp.myFit(M1, sigmaFactor)\n\t\tprint \"Mtp_test"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlogger = logging.getLogger(\"parser\")\n\t\tlogging.basicConfig(filename=CONFIG_FILENAME, level=logging.INFO)\n\t\tlogger.debug(\"Initialisation\")\n\n\t\t# Load configuration file\n\t\tconfig_file = os.path.join(CONFIG_FILES_DIR, CONFIG_FILENAME)\n\t\ttry:\n\t\t\tlogging.config.dictConfig(config_file)\n\t\texcept IOError:\n\t\t\tlogger.warning(\"Config file not found %s.\" % config_file)\n\t\t\treturn (\"\", 0, [\"ERROR: config file not found.\"])\n\t\tlogger.debug(\"Parsing into sequence"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tconfig_files = []\n\t\tall_logs = []\n\t\tlog = list()\n\t\tfor config_filename in configs:\n\t\t\tlog.append([os.path.normpath(fname) for fname in config_filename.split(',')]\n\t\t\tall_logs.append(logging.LogRecord(config_filename=config_filename))\n\n\t\tlogs = LogRecordParser(config_files)\n\t\tfor log in logs:\n\t\t\tlog.process()\n\n\t\tfor event in config_filenames:\n\t\t\tlog.process(event)\n\n\t\tif log.warnings:\n\t\t\tlog.warnings = dict"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tall_errors = {}\n\t\tfor filename in filenames:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tall_errors[filename] = list(map(lambda line: line.strip(), f.readlines()))\n\t\tfor filename in all_errors:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tall_errors[filename].extend(list(map(str, f.readlines())))\n\t\treturn all_errors, []\n\n\t@staticmethod\n\tdef _split_config(source_lines):\n\t\t\"\"\"Split the string input into (config, sequence"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\nclass Config(object):\n\t\"\"\"This class represents YAML configuration used. Also stores data that can be\n\tstored in a.yml file with the file name as well as the following attributes:\n\tkey: the name of the configuration file, i.e. a valid file name\n\tvalue: the value of the configuration file, i.e. a valid object file name\n\t\"\"\"\n\tdef __init__(self, file_dict, key):\n\t\tassert isinstance(file_dict, dict)\n\t\tself.key = key\n\t\tself.value = yaml.load(file_dict"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tfor cfgfilename in filenames:\n\t\t\twith open(cfgfilename, 'r') as fh:\n\t\t\t\t# Remove the header, the header number, and anything after it.\n\t\t\t\tfh.readline()\n\n\t\t\tlogname = os.path.split(cfgfilename)[1]\n\t\t\tcfg_parsed = parse.parse_config(\n\t\t\t\tfh.readline(), \n\t\t\t\tlogging.DEFAULT_CONFIG['config'], \n\t\t\t\tlogname, \n\t\t\t\tlogfilename)\n\n\t\t\tresults = [ cfg_parsed ]\n\t\t\t\n\t\t\tfor result in log_result(logname, cfg_parsed):\n\t\t\t\tresults.append("}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlogfiles = [os.path.join(self._config_root, p) for p in glob.glob(os.path.join(self._config_root, '*.log'))]\n\t\tif not logfiles:\n\t\t\treturn None, []  # No logfiles were found\n\n\t\tfor f in logfiles:\n\t\t\ttry:\n\t\t\t\toutput = eval('./'+f.rpartition('.')[0]+'.out')\n\t\t\t\tlogging.LogRecord(filename=f, logging.DEBUG, output=output)\n\t\t\texcept:\n\t\t\t\tlogging.LogWarning('Problem parsing input file \"'+f"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t# TODO: add support for checking that the sequence contains a valid log file path for the target\n\t# given target\n\tdef __init__(self, target, target_file):\n\n\t\tself._target = target\n\t\tself._target_file = target_file\n\t\tself._target_files = {}\n\t\tself.log_recs = []\n\t\tself.log_file_counter = [0,0]\n\t\tself.log_file_errors = []\n\t\tself.parse_errors = False\n\t\tself.errors = []\n\t\n\tdef __call__(self, target_"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n#\tparser = argparse.ArgumentParser(description='parse logs from a given configuration file')\n#\tparser.add_argument('-i','--in-file',type=str, choices=['out.json','in.json'],help='a file for the parsed configurations')\n#\tparser.add_argument('-o','--out-file',type=str, choices=['out.json'],help='a file for the parsed configurations')\t\n#\tparser.add_argument('-v','--verbose-mode',action='store_true',help='display verbose output from the logging module')\n#\tparser."}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlog.debug(\"Parsing configuration files...\")\n\t\tconfig_files = [filename for filename in config_filenames if filename.startswith(\"configuration\")]\n\t\tlog.debug(\"Found %i configuration files in %d paths... \", len(config_files), len(config_files[0]))\n\t\tfor filename in config_files:\n\t\t\tlog.debug(\"Processing configuration file: '%s'\", filename)\t\n\t\t\tself.configuration_file = filename\n\t\t\tself.parsed_config = parse_configuration(open(filename))\n\t\tlog.debug(\"Found %i parsed configuration files in %d paths..."}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\t\n\tlogging.basicConfig()\n\tlogger=logging.getLogger()        \n\tlogger.setLevel(logging.INFO)\n\tlogger.addHandler(logging.NullHandler())\n\t#logging.debug(logger.handlers)\n\thandler=logging.FileHandler(\"log.log\")\n\t#handler.setLevel(logging.NOTSET)\n\t#handler.setFormatter(logging.Formatter(\"%(asctime)s: %(levelname)s %(message)s\",\n\t#                        \"%Y-%m-%d %H:%M:%S\"))\n\thandler.setLevel(logging.DEBUG)\n\tfileHandler="}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tif isinstance(obj, basestring):\t# If given string\n\t\t\tobj = unicode(obj, 'utf-8')\n\t\treturn obj\n\t\t\n\tdef _get_char_length(self, ch):\n\t\t\"\"\"\n\t\tGet a maximum length (2^len(ch)) using the given code sequence.\n\t\tThis is the maximum length supported by Python's builtin\n\t\tlist.\n\t\t\"\"\"\n\t\tmaxv = 2**len(ch)\n\t\tif len(ch)>maxv:\t# If length exceed max, truncate to max\n\t\t\treturn maxv\n\t\treturn 2**(len("}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tdata = obj.encode('utf-8')\n\t\treturn np.frombuffer(data, dtype=np.object, count=obj.count,\n\t\t\t\t\t\t\tcopy=False)\n\n#######################\n# Read files#\n#######################\n\ndef read_input_files(objfilename):\n\t\"\"\"\n\tThis function reads input file ``filename``\n\n\tParameters\n\t----------\n\tobjfile : string or StringIO\n\t\tIf obj is a string, it is assumed to be a file. If the\n\t\tobj is a File object, the returned data is contained\n\t\tin the object. If"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\treturn obj #unicode\n\n\t#from enigma import set_fixed_point_hook\n\t#from skin import parseString\n\t#char2id = {}\n\t#id2char = {}\n\t#id2long = {}\n\tdef set(self, id):\n\t\tif id in id2char:\n\t\t\traise Exception(\"unicode id already defined!\")\n\t\tif len(id2char) > 1:\n\t\t\traise Exception(\"String id needs to be at most 1: %d\"%(len(id2char)))\n\t\tif id in id2long:\n\t\t\traise Exception(\""}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\treturn obj\n\tdef encode(self, obj):\n\t\t\"\"\"\n\t\tGenerates a base-64 encoded string of the specified type,\n\t\tand is passed to the constructor. \t\"\"\"\n\t\treturn base64.b64encode(str(obj))\n\tdef __str__(self):\n\t\treturn str(self.encode(self))\n\nclass Binary:\n\t\"\"\"\n\tA binary object holds the actual bytes in binary form,\n\tassuming the object is a binary string.\n\n\tThis class can be used in a combination with the usual binary\n\tinterface and both the ``encode"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tif isinstance(obj, string_types):\n\t\t\treturn obj.encode(\"utf-8\")\n\t\telse:\n\t\t\treturn obj\n\n\ndef string_to_array(text):\n\tif isinstance(text, str):\n\t\treturn numpy.frombuffer(text, numpy.uint8).tostring()\n\telse:\n\t\treturn numpy.frombuffer(text, numpy.int8).tostring()\n\ndef int_to_array(integer):\n\tif isinstance(integer, str):\n\t\treturn numpy.asarray(integer[:8], numpy.int16).tostring()\n\telse"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tif isinstance(obj, (str, bytes)):\n\t\t\tobj = obj.decode('ascii')\n\t\treturn obj\n\n\ndef get_ascii(buf):\n\t\"\"\" Returns the ASCII-encoded character represented by ``buf``\"\"\"\n\t\treturn unicode(buf, 'utf-8', errors='ignore')\n\n\n#\n#  The Python versions used are based on\n#  PyCon, by Andrew Bowman and the D.C.-Schulze-Cannabische Patricksen\n#  (https://github.com/andrewbowman/PyCon/tree"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\nclass StrString(String):\n    def __init__(self, obj,*args,**kwds):\n        if type(obj)!=Text:\n            raise ValueError(\"You can only pass str objects as strings.\")\n        if len(obj)<>8:\n            raise ValueError(\"You can only pass unicode objects as strings.\")\n        String.__init__(self,*args,**kwds)\n        self.obj = py3compat.safe_decodebytes(obj)\n        self.string_type ='string'\n\n    def _to_string_obj(self):\n        self.bytes = []\n        self.string_len = len(self"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ttry:\n\t\t\tout = np.string_at_buffer(self.infile, 1, self.nbytes)\n\t\t\t# If the object is utf-8 and can be safely stored\n\t\t\tif not isinstance(out, np.bytes_or_unicode):\n\t\t\t\tout = self.nbytes + out\n\n\t\texcept Exception as e:\n\t\t\tprint \"ERROR: Could not write byte array. :\"+ str(e)\n\n\t\treturn out\n\n\tdef write(self, obj, outfile):\n\t\t\"\"\"Write ``obj`` into outfile.\"\"\"\n\n\t\ttry:\n\t\t\tout = np.string"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tif isinstance(obj, str):\n\t\t\tx = obj\n\t\t\tobj = unicode(obj, \"utf-8-sig\", \"ignore\")\n\t\t\tobj = obj.encode(\"UTF-8\")\n\t\treturn obj\n\n\n#class NumpyFilelike :\n#\t\"\"\"\n\tThis function returns the numpy.ndarray.\n\tNote that this is the main purpose of numpy.lib\n\tmethod, not on Windows if you aren't running this module when you're\n\trunning on Windows.\n\t\"\"\"\n#\tdef __init__(self, fname, dtype=numpy.str, encoding=numpy"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ttry: return codecs.decode(obj, 'utf-8')\n\t\texcept UnicodeDecodeError: \n\t\t\treturn obj\n\nif sys.version_info[0] >= 3:  try:\n\tunicode\n\t\t = str\n\t\tbasestring\n\t\t = str\nexcept NameError:\n\tunicode = str\n\ndef to_unicode(obj): return obj\n\ndef to_str(obj): return obj\n\ndef to_bool(obj): return obj\n#\n# Copyright 2013-2020 Universit\u00e9 catholique de Loremont Bioinformatics, SAC, R"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\nTimeBase = datetime.datetime.fromtimestamp\n\n\nclass Bpm(object):\n\tdef __init__(self, value, value_type):\n\t\tself.value = value\n\t\tself.value_type = value_type\n\n\tdef __lt__(self, other):\n\t\treturn self.value < other.value_type\n\n\tdef __gt__(self, other):\n\t\treturn self.value > other.value_type\n\n\tdef __ge__(self, other):\n\t\treturn self.value >= other.value_type\n#!/usr/bin/python\n# -*- coding:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tif cls._is_valid_tz(tz):\n\t\t\treturn cls.get_valid_tz(tz).localize(ticks)\n\t\telse:\n\t\t\traise ValueError(\n\t\t\t\t\"Invalid timezone: {0} for day of week 0\".format(\n\t\t\t\t\ttsutil.fmttime(ticks)))\t\n\t\n\tdef _set_tz(self, tz):\n\t\tself.tz = tz\n\t\tglobal tz_offset\n\t\ttz_offset = tz_offset + self.hour_offset\n\n\tdef normalize(self):\n\t\treturn Time(self.seconds * (10 ** 6), self"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\t# get the tick values\n\ttry:\n\t\t\ttick_values = round(ticks / 1000.0, 6)\n            if tz is None:\n\t\t\t\ttz = current_timezone\n\t\t\telse:\n\t\t\t\ttz = tz.localize(current_timezone)\n\n\t\t# convert ticks to datetime objects\n        except (ValueError, OverflowError):\n\t\t\t# tz should be non-UTC\n\t\t\tpass\n\t\telse:\n\t\t\t# return the time\n\t\t\tdt = datetime.datetime.fromtimestamp(ticks)\n\t\t\t# add the result\n\t\t\treturn cls(dt, tz)\n\n\tdef from_unix(self,"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tif ts2int(ticks) < 0:\t# positive:\n\t\t\t# invalid tick\n\t\t\traise ValueError(\n\t\t\t\"Can't convert ticks to intervals\")\n\t\telse:\t\t\t\t# out of bounds:\n\t\t\treturn cls(ticks=ts2int(ticks))\n\n\tdef __init__(self, ticks=0):\t\tif not self.intervals:\n\t\t\tif tz is None:\n\t\t\t\ttz = self._get_tz()\n\t\t\tself.intervals = [None] * ts2int(ticks)\n\t\telse:\n\t\t\tself._intervals = [None] * ts2"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    @util.memoized_property\n    def scale(self):\n        if self._is_truncated():\n            return self._resolution + (self._resolution % 864000000000003)\n        else:\n            return (self.get_epoch() % 864000000000003) * 1000000\n\n    def make_ticks(self):\n        \"\"\"\n        Generate time from ticks.\n        \"\"\"\n        return time.mktime(util.utc_to_timestamp(self.get_ticks()))\n\n    def get_ticks(self):\n        return time.mktime(util.utc_to_timestamp(self.get_ticks()))\n\n    @classmethod\n    def"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    def __init__(self, ticks, tz=None, unit=Unit.MINUTES):\n        \n        assert cls.__mro__ is not None, \"cannot instantiate unrecognised Base class\"\n\n        # set time to start time based on nearest ticks, else fall back to midnight\n        if isinstance(ticks, int):\n            start = int(ticks), int\n\n        elif isinstance(ticks, bool) or isinstance(ticks, float):\n            start.minute, start.second = int((ticks & 0xff) >> 8), int((ticks & 0xff) >> 16)\n        elif isinstance(ticks, str):\n            start"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tif not isinstance(ticks, int):\n\t\t\traise ValueError(\"ticks should be an integer\")\n\t\tif (units.is_integer(ticks) and abs(ticks) >= 86400000000000):\n\t\t\traise ValueError(\"ticks should be an integer between 0 and 86400000000000\")\n\t\tdt = datetime.utcfromtimestamp(ticks)\n\t\treturn cls.from_dt(dt, tz)\n\t\t\n\tdef from_dt(cls, dt, tz=None):\n\t\t\"\"\"\n\t\tCreate a time from an dt object.\n\t\tThis uses the values from the calendar (not offset), or from"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tassert isinstance(ticks, int)\n\t\tif ticks == 86400000000000:\n\t\t\treturn None\n\t\treturn cls(ticks=ticks, tz=tz)\n\ndef from_tz(tz):\n\t\"\"\"\n\tCreate a Time from timezone (e.g. UTC), or return None if no valid\n\ttime zone was given\n\t:type tz: str\n\t\"\"\"\n\t\treturn cls(tz=tz)\n# -*- coding: utf-8 -*-\r\n\r\nfrom module.plugins.internal.Weather import Weather, get_file_size, get_system"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\ttsec = ticks * 86400 + round(ticks,1)\n\t\ttmin = -tsec * 1440 + round(ticks,1)\n\t\ttsec = cls(tmin, tsec)\n\t\treturn tsec\n\ndef is_valid_time(tsec):\n\t\"\"\"\n\tcheck if ``tsec`` is valid given a time span as a float\n\n\t:type tsec: Time\n\t:param tsec: Time span as float\n\t:rtype: bool\n\t:return: True if a time span is now valid\n\t:rtype: bool\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\treturn cls(*util.epoch_to_utc(ticks, tz=tz, is_whole=True))\n\ndef from_milliseconds(tms, tz=None):\t\"\"\"\n\tCreate time from milliseconds (millisecs since midnight).\n\n:param tms: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if tms is out of bounds\n    (0 <= tms < 100000000000000)\t\"\"\"\n\t\treturn cls(*util.milliseconds_to_utc(tms))\n\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version is None:\t\t\t# no other case\n\t\t\treturn dict(protocols=[(None, 0)]*len(cls.type_handlers))\t# no default handlers\n\n\t\thandler_dict = {}\n\t\tfor t in protocol_version:\n\t\t\thandler_dict[t] = None\n\t\treturn handler_dict\t\t\t # return a dict of supported handlers\n\nclass BoltProtocolHandler(object):\n\t\"\"\"\n\tAn abstract class implementing the Bolt protocol handler\n\ton the given `protocol_version`\n\t(e.g. \"3.2\"), which is a"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\treturn dict(\n\t\t\tprotocol_version = protocol_version\n\t\t)\n\ndef protocol_version_tuple():\n\t\"\"\"Returns a tuple containing protocol version.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn tuple(protocols[protocol_version])\n\t\texcept KeyError:\n\t\t\tpass\n\ndef protocol_version_string(protocol_version):\n\t\"\"\"\n\tReturns the protocol version as string that describes the protocol.\n\t:param protocol_version: string representing the protocol version\n\t\"\"\"\n\t\treturn protocol_version_tuple() + '.' + protocol_version\n\nclass ListenerProtocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\tif protocol_version is not None:\n\t\tif protocol_version[0] > 2:\n\t\t\traise TypeError(\"Only python 2.4 supported in protocol version\")\n\t\t\n\t\tif protocol_version[1] < 6:\n\t\t\traise TypeError(\"Only python 6 supported in protocol version\")\n\t\t\n\t\tif protocol_version[2] > 9:\n\t\t\traise TypeError(\"Only python 9 supported in protocol version\")\n\t\t\n\t\t\n\t\t\n\tfor handler_class in cls.__subclasses__():\n\t\n\t\tif type(handler_class.__name__[:-2]) == tuple:\n\t\t\tif version_info < handler_class"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.PROTOCOL_VERSIONS[0]\n\telse:\n\t\tif not hasattr(protocol_version, '__iter__'):\t\t\t# pragma: no cover\n\t\t\traise TypeError(\"protocol_version must be a tuple of strings and lists\")\n\t\tif not protocol_version in cls.PROTOCOL_VERSIONS:\n\t\t\traise TypeError(\"protocol version must be one of: %r\" %\n\t\t\t\tcls.PROTOCOL_VERSIONS)\t\t\n\t\tprotocol_version = cls.PROTOCOL_VERSIONS[protocol_version]\t# pragma: no cover\n\n\treturn {"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn cls._PROTOCOL_HANDLER_SET[protocol_version]\n\texcept KeyError:\n\t\t\n\t\traise TypeError, \"cannot find %s protocol handler in %s\" % (\n\t\t\tcls.__name__, cls.__module__)\n\t\t\n\t\t# Check for supported protocols.\n\t\tif protocol_version is None:\n\t\t\t# Get default protocol version.\n\t\t\ttry:\n\t\t\t\tsupported_protocols = get_supported_protocols()\n\t\t\texcept Exception as e:\n\t\t\t\traise TypeError(\"invalid protocol version '%s'\" % e)\n\t\t\t\n\t\t\t# Check for supported supported protocol versions.\n\t\t\tfor"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\tsupported = list(cls.supported)\n\t\n\tif protocol_version is None and len(supported) > 0:\n\t\tprotocol_version = supported[0].version\n\n\tif protocol_version is not None:\n\t\tversion_tuple = (protocol_version,)\n\telse:\n\t\tversion_tuple = ()\n\t\n\treturn dict(zip(supported, version_tuple))\n\n\ndef _validate_channel_types(cls, types):\n\t# Make sure no exceptions occur\n\tif cls.__name__ in types or len(cls.__bases__) == 0:\n\t\traise TypeError(\""}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    def all_protocols():\n        \"\"\"\n        Return a dictionary of available all specific protocols,\n        including the protocols' keys and their version\n\t\tin descending order.\n        \"\"\"\n        supported_map = {\n            None: protocols.DEFAULT_PROTOCOL,\n            3: protocols.PROTOCOL_VERSION,\n            5: protocols.DEFAULT_PROTOCOL,\n        }\n            \n        return supported_map[protocol_version]\n\n    def get_protocol_version(protocol_version):\n        \"\"\"\n        Retrieve a specific protocol version string from a protocol version tuple\n        :param protocol_version: tuple identifying an explicit protocol version\n        :return: version tuple for available protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t     \n\n\t# default port for port forwarding\n\tport\t= cls.PORTS[\"default\"]\n\t\n\t# default protocol implementation for protocol version (e.g. 9)\n\timplementation = cls.PROTOCOLS[\"default\"]\n\t\n\t# set the default protocol implementation for protocol version\n\timplementation.default_implementation = cls.PROTOCOLS[\"default_java\"]\n\t\n\t# override default protocol implementation for protocol version \n\timplementation.implementation = cls.PROTOCOLS[\"default_java_0\"]\n\t\n\t# override default protocol implementation for protocol version\n\timplementation.implementation.implementation = cls.PROTOCOLS[\"default\"]"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \tif not protocol_version:\n     \t\traise TypeError(\"protocol_version list not passed, either tuple not \"\n\t\t\t\t\t                   \"passed, or None\")\n             \n\t\treturn\tprotocol_version\t\n\telse:\n\t\t\t\t\treturn\tget_protocols(*protocol_version)\n\t\texcept Exception as e:\n\t\t\traise Exception(e)\n\t\t\t\ndef list_protocol_handlers():\n\t\t\"\"\"Return a list of registered Bolt protocol handlers.\"\"\"\t\n\t\treturn list()\t\n\telse:\n\t\treturn get_protocols(*default_protocols)\n\t\nimport os\nimport fcntl\nimport time\t\t\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    allowed_protocols = [None, 'protocol']\n    def initialize_protocol_handlers():\n        \"\"\"\n        Initialize a new Bolt protocol handlers, which is the dictionary that\n        is returned with supported protocol versions.\n        \n        A default implementation uses the dict defined in this\n        module. Only supports versions >= 3.5 and >= 3.8; in \n        the latter case, the protocol implementations will default to None.\n\t\"\"\"\n\t\t# Determine if this Bolt protocol handler is currently present\n\t\tif 'protocol' in cls:\n\t\t\tcls = cls['protocol']\n\t\telse:\n\t\t\traise TypeError(\n                    'The"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\treturn cls([val.encode('utf-8') for val in values])\n\n\n# A mapping from strings to Bitmap for encoding to bitmap\n_codec = {}\n\ndef get_codec(name):\n\t\"\"\"\n\tRetrieve the bookmarks codec.\n\t\"\"\"\n\treturn _codec.get(name, None)\n\n# Baremarks supported for this version, using the following:\n# - http://www.bittorrent.com/\n# - http://btc-creek.com/index.php\n# - http://www.doughell.com/"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\nclass Bookmarks:\n\t\"\"\"\n\tRepresents a collection of open bookmarks.\n\n:param library_path: The path to the library file\n:type library_path: str\n\"\"\"\n\tdef __init__(self, library_path):\n\t\tself.library_name = library_path.replace(\"?\", \"_\")\n\t\tself.loaded_library = False # True if the bookmark has been loaded.\n\t\tself.current_bookmarks = set()\n\t\tself.next_bookmark_value = None\n\t\tself.load_bookmarks(library_path)\n\tdef __"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor value in values:\n\t\t\tif value!= ':':\n\t\t\t\tvalue = ':' + value # remove trailing space\n\t\t\t\tvalue = unicode(value.replace('\\n', ''), 'utf8', errors='ignore')\n\t\t\t\tvalue = value.encode('utf8')\n\t\t\telse:\n\t\t\t\tvalue = unicode(value, 'utf8', errors='ignore')\n\n\tdef to_raw_values(self):\n\t\t\"\"\"\n\t\tReturn a List of raw Bookmarks.\n\t\t\"\"\"\n\t\treturn list(map(self.from_raw_value, self._values))\n\n\tdef _validate_"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif len(values) == 0:\n\t\t\treturn None # Nothing to do!\n\t\telse:\n\t\t\tvalues = [int(val) for val in values]\n\t\treturn cls._from_values(values)\n\t\n\t# Return new Bookmarks object.\n\t@classmethod\n\tdef get_by_key_raw(cls, key):\n\t\treturn cls.query \\\n\t\t\t\t.filter(cls.key == key) \\\n\t\t\t\t.with_entities(cls.bookmarked) \\\n\t\t\t\t.first()\n\n\t# Modify bookmark if the bookmark was not set previously.\n\t@"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\traw_values = []\n\t\tcur_index = 0\n\t\twhile cur_index < len(values) - 1:\n\t\t\tif values[cur_index].isdigit():\t\n\t\t\t\tcurrent_index = cur_index + 1\n\t\t\t\tbreak\n\t\t\telse:\t\n\t\t\t\tcur_index = cur_index - 1\n\n\t\t\tvalue = values[current_index]\n\t\t\tcur_values.append(value)\n\t\t\tcur_index += 1\n\n\t\treturn Bookmarks(values=raw_values)\n\n\tdef get_raw_data(self):\n\t\treturn '\\xa0'.join("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tassert len(values)<2,\"len(values) should be less than 2 elements\"\n\t\tassert all((value in (0,1) for value in values),\"values must be strings\")\n\t\tdata = []\n\t\tfor value in values:\n\t\t\tif not isinstance(value,str): continue\n\t\t\tval = []\n\t\t\tval.append(value)\n\t\t\tval.append(int(value))\n\t\t\tdata.append(val)\n\t\treturn cls(data)\n\tfrom_raw_values.__module__ = cls.__module__\n\tfrom_raw_values.__name__"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tbmarks = []\n\t\tfor val in values.split(\",\"):\n\t\t\tval = val.split(\";\")\n\t\t\tif len(val)!= 2:\n\t\t\t\tcontinue\n\t\t\tbmarks.append(Value(val[0], val[1], val[2]))\n\t\treturn Bookmarks(bmarks)\n\n\tdef to_raw_values(self):\n#\t\treturn str.join(values,\"\")\n\t\traise Error(\"Cannot serialize to raw bookmark strings.\")\n\n\tdef __unicode__(self):\n\t\treturn u\"Bookmarks from %s: %s\" % \\\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\n\t\n\tvalues = [values] if isinstance(values, six.string_types) else values\n\n\tbookmarks = Bookmark.objects.from_raw_values(values)\n\n\treturn bookmarks\n\n\ndef from_file(file):\n\t\"\"\"\n\tDeserialize a bookmarked file. The bookmarks will be pickled. You can also\n\tuse this method if you want to be able to pickle a bookmarked file\n\twithout a JSON object.\n\t\n\t:param file: File object\n\t:type file: io.BytesIO\n\t\"\"\"\t\n\t\n\tif file.name.endswith('.json'):"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tb = Bookmarks()\n\n\t\tfor char in values:\n\t\t\tb.append(Char(char))\n\t\treturn b\n\n\ndef raw_format(raw_string, font):\n\t\"\"\"\n\tconvert raw_string from a Bookmarks object to its Unicode string\n\t\"\"\"\n\n\traw_string = u(raw_string)\n\n\ttext_type = 'Bookmark'\n\n\tif 'unicode' in font.font_properties:\n\t\ttext_type = 'unicode'\n\n\tif font.ascii_letters_only and not raw_string.isalpha():\n\t\ttext_"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\timport json\n\t\tfrom lxml import etree\n\n\tif not values:\n\t\tvalues = json.loads(values)\n\n\ttags = {}\n\tfor val in values:\n\t\ttag = Tag(val)\n\t\ttags.setdefault(tag.name, []).append(tag)\n\n\ttagmap = {}\n\tfor tag in tags.keys():\n\t\tif tag in tagmap:\n\t\t\tt = tagmap[tag]\n\t\telse:\n\t\t\tt = {}\n\t\t\ttagmap[tag] = t\n\n\treturn Bookmarks(tags)\n\n\ndef write_bookmark(bookmarks):\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tassert isinstance(seq, compat_str)\n\t\tseq = seq[::-1] if seq[-1] is None else seq\n\t\treturn (seq, type(seq))\n\n\tdef _get_seq_type(seq):\t\t\treturn seq if isinstance(seq, compat_str) else _type_from_name(seq)\n\n\tdef _get_attr(attribute):\t\t\"\"\"Return a (attribute, type)\"\"\"\n\t\tif isinstance(attribute, compat_str):\n\t\t\tif attribute in seq_attrs:\n\t\t\t\treturn (seq_attrs[attribute], _type_from_name"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\t# pragma: no cover\n\t\t\treturn None, None\n\t\tfrom. import _types, __sequences\n\t\tif sequence is None:\n\t\t\tseq = _type_registry.get(type(seq)).next()\n\t\ttry:\n\t\t\tif bufsize is int:\n\t\t\t\tstart = bufsize\n\t\t\telse:\n\t\t\t\tstart = max(bufsize - 1, 0)\n\t\t\tend = bufsize\n\t\texcept TypeError:\n\t\t\ttry:\n\t\t\t\trange = __sequences.__getslice__(start, end)\n\t\t\texcept ValueError:\n\t\t\t\trange = __sequences.__getslice__(start, end"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = _sys.maxint\n\t\tend_marker = _buffer.find(seq, len(seq))\n\t\tif end_marker == -1: \n\t\t\treturn _empty_sequence(*seq[:len(seq)])\n\t\tif bufsize <= 0:\n\t\t\tif seq[-1:] == '\\n':\n\t\t\t\treturn (_empty_sequence(*seq[:-1]))\n\t\t\telse: # pragma: no cover\n\t\t\t\treturn (_empty_sequence(*seq))\n\t\t# this is a bit aggressive, so no special magic, we don't do it\n\t\treturn (_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = getattr(seq, 'bufsize', None)\n\t\tif bufsize is not None:\n\t\t\traise TypeError(\"Argument 'bufsize' is a reserved word.\")\n\t\telse:\n\t\t\treturn (seq,)\n\tdef getvalue(self, bufsize=None):\n\t\t\"\"\"Return the value in self's encoding (a string).\"\"\"\n\t\ttry:\n\t\t\treturn self._checksize()\n\t\texcept AttributeError:\n\t\t\tpass\n\t\telse:\n\t\t\tif bufsize is None:\n\t\t\t\treturn ''\n\t\t\telse:\n\t\t\t\tif bufsize == 0:\n\t\t\t\t\t# The current"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbuf = []\n\t\t\tbuf.extend(_get_seq_with_type(seq, bufsize))\n\t\telse:\n\t\t\tbuf = [ord(c)\n\t\t\t\tfor c in _get_seq_with_type(seq, bufsize)]\n\t\treturn buf\n\telse:\n\t\treturn tuple(seq)\n\n\ndef _get_seq_with_value(seq, bufsize=None):\t\"\"\"\n\tRead data from a (long) string.\n\t\"\"\"\t# If bufsize is specified then use it\n\tif bufsize is None:\n\t\tbufsize"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\treturn seq[0], seq[1], seq[2], 0\n\ndef unpack_record_of_type(fname, dtype=None):\n\t\"\"\"\n\tReturn tuple (record, subtype)\n\tfrom file *fname*. Returns *record* in a tuple\n\tof *subtype*.  Any subtype in the subtype\n\twill be submited based on the type of *fname*. (i.e., in\n\textension with '.p5' extension); otherwise None.\n\t\"\"\"\n\t\tname, fext = os.path.splitext(fname)\n\t\tsuffix = fext["}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tseq, type = seq[0], seq[1]\n\t\tassert type >= 0, 'Sequence type {} is too large'.format(type)\n\t\treturn seq, type\n\t\n\tdef _get_seq_with_type_and_length(self, seq):\t\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq* (or is the sequence type, if \n\t*type* is the first item in the list),\n\totherwise |type| is 0.\n\t\"\"\"\t\t\t\n\t\tif type in self._type_and_length:\n\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize:\n\t\t\tbufsize = MAX_BUFFER_SIZE\n\n\t\tif bufsize == 0:\n\t\t\treturn seq, seqtype\n\n\t\telif bufsize > 0:\n\t\t\tstart = 0\n\t\t\tend = bufsize\n\t\t\twhile start <= end:\n\t\t\t\tyield seq[start:startpos+end]\n\t\t\t\tstart = pos = end\n\t\t\t\tend = bufsize\n\n\t\telse:\n\t\t\traise ValueError('buffer size must be > 0!')\n\n\t\treturn seq, (bufsize, start)\n\n\ndef _get_seq_with_subsequence(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence,"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif isinstance(seq, str):\n\t\t\tseq = str(seq)\n\t\tif bufsize:\n\t\t\t# If we have a sequence type, we can use the buffer\n\t\t\t# object from which the sequence is written. Otherwise,\n\t\t\t# we may have made a large number of readings from the\n\t\t\t# buffer for example.\n\t\t\tseqs = (s.encode(getattr(stream, bufsize)) for s in seq.split())\n\t\telse:\n\t\t\n\t\t\tfrom cStringIO import StringIO\n\t\t\tseqs = list(seq) ### Python 3.x and up\n\t\t\ttry:\n\t\t\t\tseq = cStringIO"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\ttype = None\n\t\ttypeseq = None\n\t\tfor i, d in enumerate(seq):\n\t\t\tif d.type!= type:\n\t\t\t\tcontinue\n\t\t\tseq = d.sequence\n\t\t\ttypeseq = typeseq or seq[i]\n\t\tif typeseq in (seq[i], seq[i+1]):\n\t\t\treturn (type, seq[i+1])\n\t\telse:\n\t\t\treturn (seq[i], seq[i+1])\n\n\t_fields = [\"sequence\", \"type\"]\n\n\tdef read(cls, seq, bufsize=None):\n\t\t\"\"\"Read *"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\tother = 0.0\n\t\tassert self.scale!= other\n\t\tif recompute and self <= other:\n\t\t\tfactor = self.scale\n\t\telse:\n\t\t\tfactor = other\n\t\tif self.is_float():\n\t\t\tx = 1/factor\n\t\telse:\n\t\t\tx = 0.0\n\t\tself.set(x, factor*other)\n\n\tdef recalculate(self, other=None):\n\t\t\"\"\"Recompute scale for :meth:`~lena.stats.LenaStat` to be of given\n\t\tvalue. This is only"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tassert self.scale is None or other is not None\n\t\tassert other is None or recompute is not False\n\t\tif recompute:\n\t\t\tself.scale = _scalefunc()\n\t\tself.recalc()\n\n\tdef __add__(self, other, fill_value=None):\n\t\tresult = self.copy(fill_value)\n\t\tresult.scale = other.scale # type: NonNegativeIntegers if is a Number\n\t\t\t  # else call __radd__ which is the default\n\t\tresult.recalc()\n\t\treturn result\n\n\tdef __sub__(self, other,"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t     \n\t# this is a bit bit complicated:\n\t# the following is for efficiency\n\t# if not self.__size__ >= self.size:\n\t# \traise lena.LenaValueError\n\n\tdef __add__(self, other):\n\t\traise lena.LenaValueError, \"can't add an empty histogram\"\n\n\tdef __sub__(self, other):\n\t\traise lena.LenaValueError, \"can't substract an empty bin\"\n\n\tdef __mul__(self, other):\n\t\traise lena.LenaValueError, \"can't multiply an empty"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tself._validate_value(other, str)\n\t\treturn self._scale\n\n\tdef _validate_value(self, other, str):\n\t\ttry:\n\t\t\tvalue = float(other)\n\t\texcept ValueError:\n\t\t\traise lena.LenaValueError(\"%s is not a float\" % self)\n\t\tif not self >= 0:\n\t\t\traise lena.LenaValueError(\"%s is not smaller than zero\" % self)\n\t\treturn value\n\n\tdef __getslice__(self, i, j):\n\t\tself._validate_value(\"min\", i, \"min\")\n\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\ttry:\n\t\t\tassert other is None or other < 0\n\t\texcept nailgun.exceptions.LenaValueError:\n\t\t\treturn NotImplemented\n\n\t\tif other is not None and self is other:\n\t\t\t# self and other have changed\n\t\t\ttry:\n\t\t\t\t# self and other have changed, rescale self to other\n\t\t\t\treturn scale(self.value(), other.value())[0]\n\t\t\texcept IndexError:\n\t\t\t\treturn NotImplemented\n\n\t\ttry:\n\t\t\t# Infer to the scale given a recompute argument\n\t\t\tif recompute:\n\t\t\t\treturn self.value() * other\n\t\t\telse:"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif type(other) == float:\n\t\t\tother = Float(other)\n\t\t\n\t\tif self.length < other.length:\n\t\t\traise lena.lenaError(other, 0, False)\n\n\t\tif recompute and self._rescale:\n\t\t\tself.computeScale()\n\t\t\t\n\t\nclass Divide(Numeric):\n\tdef __init__(self, value, width):\n\t\tNumeric.__init__( self, value=value )\n\t\tself._width = width\n\t\t\n\tdef __repr__(self):\n\t\treturn '<Divide (%sx) %s>' % (self._width,"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif isinstance(other, str):\n\t\t\tother = float(other)\n\t\tassert isinstance(other, float)\n\t\tassert other >= 0\n\t\tassert other <= self\n\t\tif recompute:\n\t\t\tassert -other <= self\n\t\treturn  self.__rescale(other)\n\t\n\tdef multiply(self, other, recompute=False):\n\t\t\"\"\"\n\tMultiply *other* by the value in *self*.\n\nIf *other* is ``None``, return a scalar value.\nIf its exponent was computed before,\nit is computed and stored for subsequent use\n(unless"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\ttry:\n\t\t\tother = other.normalize(recompute)\n\t\texcept NormalizerError:\n\t\t\tother = None\n\t\tif other is None:\n\t\t\treturn self\n\t\tif not isinstance(other, (float, int_types)):\n\t\t\tother = {self: other}\n\t\tscale = other.get(self, 0)\n\t\tif scale is None or scale is None:\n\t\t\tscale = 0.0\n\t\treturn scale * other\n\n\nclass BinnedHistogram(object):\n\t\"\"\"\n\tRepresents a binned histogram.\n\tIf you wish to use a different instance of"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif recompute and other is None and other!= 'none':\n\t\t\treturn self * other\n\t\treturn self._copy_image(other)\n\n\t#----------------------------------------------------------------------\n\t## Divide - return range and index (the result in a\n\t# function call).\n\t#\n\t# If only one argument is given, that index needs\n\t# to be applied to the elements in the *other* list;\n\t# the latter is an error condition.\n\t#\n\t# For example, ``x[0]`` = array([0, 1, 2, 3], dtype=int32)"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\treturn _utils.scale(self, other, recompute)\n\n\tdef shift(self, dx=-1, dy=-1):\n\t\t\"\"\"\n\tShift the histogram, respecting the unit vectors. If *dx* is specified, shift\n\tinto the right and up sides of the histogram. The number of\n\tbins and the range must be positive, or the dimension of *dx*,\n\ti.e., the range on *dx* is set to 1.\n\nIf *dy* is specified, shift into the left and right sides of the histogram.\nThe number of bins and range"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\treturn self._getscale(other)\n\tscale = property(scale)\n\n\tdef shift(self): return self._shift(self._n, other=self._n)\n\tshift = property(shift)\n\n\tdef norm(self): return self._norm(self._n, other=self._n)\n\tnorm = property(norm)\n\n\tdef scale_distance(self, other=None):\n\t\ttry: return 1\n\t\t# NOTE: This might fail if the coordinates are not orthogonal\n\t\t#   or equal, causing the error to be 1, even if it happens"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tassert isinstance(other, (numbers.Integral, float))\n\t\tassert other >= 0\n\t\tassert other.startswith(\"x\")\n\t\tassert other.startswith(\"y\")\n\t\tassert other >= -1\n\t\tassert isinstance(other, float)\n\t\treturn self.__setscale(other)\n\n\t# returns the x coordinate of the scale in the graph units, accounting that it's the same\n\t# as the scale of the graph\n\tdef getx(self) :\treturn self.__x\n\n\t# returns the y coordinate of the scale in the graph units, accounting that it's the same"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif type(other) is NotSpecifiedObject:\n\t\t\tother = self\t\n\n\t\tif other is None:\n\t\t\treturn self.scaled()\n\n\t\treturn GraphScale(self, self.scaled().resize(other))\n\n\tdef size(self):\n\t\t\"\"\"\tGet or set the size of this graph.\n\t\t\"\"\"\n\t\treturn GraphSize(self.scaled().size())\n\n\tdef size_x(self):\n\t\t\"\"\"\tGet or set the size of the x coordinate.\n\t\t\"\"\"\n\t\treturn GraphSize(self.scaled().size_x())\n\n\tdef size_"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\treturn GraphScale(self._scale, self._scale[0], other)\n\t\n\tdef __getitem__(self, i):\n\t\treturn self._scale[i]\n\ndef get_scale(graph):\n\t\"\"\"\n\tGet the scale of a graph.\n\n\tReturns\n\t-----------\n\ttuple (int, int)\n\t\tThe result of :func:`scale_graph`\n\t\"\"\"\n\t\n\tif not is_empty_string(graph):\n\t\treturn map(lambda x: scale(x), graph)\n\telse: return graph.scale\n\ndef __str__(l):"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\ndef toGraph(self):\n\t\n\tretGraph = []\n\tpos = []\n\n\ttry:\n\t\tpos = self._pos\n\t\tpos.extend(self._point)\n\texcept AttributeError:\n\t\tpos = [self._pos[0]] * len(self._point)\n\n\tfor x in self._point:\n\t\tretGraph.append(Graph(x[0], x[1], x[2], x[3], x[4], pos[0], pos[1]))\n\tself._pos = pos\n\treturn retGraph\n\n\nclass Graph(object):\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\tother = float(\"inf\")\n\t\treturn GraphScale(self, other)\n\t\t\n\tdef scale_relative(self, other):\n\t\t\"\"\"\n\trescale the graph by a relative coordinate.\n\n\tArgs:\n\t\tother (int, str): the relative-coordinate at which to rescale\n\t\tself (obj): The graph being rescaled\n\n\tReturns:\n\t\tGraphScale: the new graph scaled to the current one\n\t\"\"\"\n\t\tif not isinstance(other, GraphScale):\t# If other is not a GraphScale instance, return its type\n\t\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\nclass Node(object):\n\t \"A collection of nodes.\"\n\t def __init__(self, loc, loc2): #local variables\n\t\t self.loc = loc\n\t\t self.loc2 = loc2\n\t\t self._coords = None\n\t\t self._errors = []\n\t\t self._bounds2 = None\n\t\t self._index = 0\t#index to set\n\t\t\t\n\t\t #create any missing properties...\n\t\t self.requiredProps = {}\n\t\tdef addProportion(self, name, value):\n\t\t self._coords.insert(0, self.loc2[len(self.loc2)//len(self.loc)//len"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other == None:\tother = self._graph\n\t\t#assert(x is None or y is None or z is None)\n\t\treturn self.__get_scale(self._graph, other)\n\t\n\tdef copy(self):\n\t\t\"\"\"Return a deep copy of the graph.\n\t\t\n\t\tAny coordinate may be duplicated. Any value given in *self* will be\n\t assigned the value of the coordinate if different from\n        *other*.\n\t\t\n\t\tReturn a deep copy of the graph shallow as this\n\t\tcould not change the graph.\n\t\t\"\"\"\n\t\tif self.coord == None:\n\t\t\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif (other is None) or (other!= None and isinstance(other, (list,tuple)) and len(other)==3):\n\t\t\t# convert other array\n\t\t\ttry:\n\t\t\t\tout_arr = numpy.ndarray(other, 'i')\t# numpy 1.8, as well as numpy 2.0\n\t\t\t\to_arr = numpy.ndarray(other,'i')\t# numpy 1.7.1\n\t\t\t\tfor x in range(len(out_arr)):\n\t\t\t\t\tout_arr[x] = numpy.ravel(o_arr[x])\t# 1-D array"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\tret = self._get_offset()\n\t\telse:  # already a numeric value\n\t\t\tret = _new_graph_object(self)\n\t\t\tret._x, self._y, self._z = other\n\t\t\treturn ret\n\n\tdef scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has a unknown or zero scale,\n"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\t\n\thist = Histogram() if histogram.data.ndim==1, hist\n\thist.add_hist(histogram.data, hist_to_graph=hist)\n\t\n\treturn hist._get_data()[0] * make_value(\n\t\tbins[histogram.data].min(),\n\t\thist.data)\n\t\n\ndef histo_to_edge(histo, field_names=None):\n\tif histo.data is None or histo.data.ndim==1:\n\t\t# Return bin content based on bin content\n\t\treturn convert_histogram(histo)[field"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\thist_to_graph_params = deepcopy(hist)\n\t\thistogram_to_graph_params['Histogram'].set_limits_min(5)\n\t\thist_to_graph_params['Histogram'].set_limits_max(50)\n\t\thist_to_graph_params['Histogram'].set_title('%s[%s].' % ('histogram', hist_to_graph_params['histogram'][0]))\n\t\t\n\t\t# calculate the histogram scale\n\t\t\n\t\tscale_scale = \"log10\"\n\t\n\t\thist_to_graph_params['Histogram'].hist_axis = [1.0,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\nclass NodeError(Exception):\n    pass\n\t\n\t\nclass Bin(object):\n    \"\"\"A histogram node.\"\"\"\n    \n    def __init__(self, x, y):\n        \"\"\"Construct the bin.\n        \n        Args:\n            x (integer): Bin X coordinates.\n            y (integer): Bin Y coordinates.\n        \"\"\"\n        self.x = x\n        self.y = y\t\n        self.x_range, self.y_range = zip(*[float(x) / x_step for x_step in\n                                            zip(self.x, self.y)])\n        self.scale = 1.0/max(self.x"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\t\n\tbins = np.ndarray(num=len(hist), dtype=float)\n\t\n\tfor i_column, column in enumerate(hist_columns):\n\t\tbin_value = hist[i_column]\n\t\tif make_value is not None and column[-1:] == '':\n\t\t\tcolumn = column[:-1]\n\t\t\n\t\tbin_value = make_value(column)\n\t\t\n\t\tbins[i_column] = np.maximum(bin_value/np.max(bin_value), 1)\n\t\n\t# We use the histogram type for the histogram type\n\tif histogram_type == \""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tassert(type(make_value) == _hist_type)\n\t\tassert(is_numeric(make_value) == True)\n\n\t\t# TODO: Make sure it does all work correctly.\n\t\t#return _hist_to_graph(hist, make_value, get_coordinate)\n\telif make_value is None:\n\t\tmake_value = lambda *args: None\n\n\tassert(type(field_names) == list)\n\n\tfield_names = [field_name.upper() for field_name in field_names]\n\tfields = []\n\tfor field"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tif not hist.dtype.char == 'i' and not hist.shape == hist.shape[0]:\n\t\t\traise Exception('Hist bin must have type \"index\" or \"shape\"')\n\t\tif hist.dtype.kind!= 'c':\n\t\t\traise Exception('Hist bin must have numeric type.')\n\t\ttry:    # Convert all the values into a list of floats\n\t\t\tif hist.dtype.kind!= 'i':\n\t\t\t\traise Exception('Cannot convert hist values into a numpy array')\n\t\t\tval = np.array(hist).astype(float)\n\t\texcept TypeError:\n\t\t\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\ndef hist_to_series(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.hist` to a series.\n\n*make_value* is a function to set the value of a graph's points.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create series's error bars.\nFor example, to create a series with errors\nfrom a histogram where bins contain a named tuple with fields *mean*, *mean_error* and a"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tfor node in graph.nodes:\n\t\t\tvals, err = histogram(node, make_value, get_coordinate)\n\t\t\tif make_value is None:\n\t\t\t\tvals = vals[1:]\n\t\t\t\terr = err[1:]\n\t\t\telse:\n\t\t\t\tvals = make_value(vals)\n\t\t\t\terr = make_value(err)\n\n\t\t\tretvals.append((node, vals, err))\n\n\t\tgraph.graph._agnostic += 1\n\n\t# the graph is not the root of the histogram \n\t#\n\t# If there are not sufficient points on any nodes,\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t# Note that the histogram is stored internally in\n\t\t# this dictionary--not sure how it is here--\n\t\tif (hist.keys() == list) and (len(hist) == 1) and (hist[-1] == \"x\"):\n\t\t\treturn hist.values()[0]\n\t\telif (hist.keys() == None) or (hist == []:\n\t\t\treturn \"\"\n\n\t\t# For a single histogram, only the number and all of the\n\t\t# point's context are returned.\n\t\tif (\"bin\" in get_coordinate) and ((abs(make_value(c)) =="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t# print'make_value', make_value\n\t\terror = hist[make_value]\n\t\tif  isinstance(error, TypeError):\n\t\t\t# print 'Error occured', error\n\t\t\traise TypeError('make_value is not integer')\n\n\t\t# Check if the graph's point, axis, and scale are the same\n\t\tif hist.shape[1]!= hist.shape[0]:\n\t\t\traise RuntimeError('Expected number of binpoints and columns')\n\n\t\t# Make the value in the form of a tuple (x, y).\n\t\tx, y = error\n\t\tif len(x"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\t# FIXME: we should make this class a class that will fail\n\t\t# when implementing the method from a non-class method.\n\t\t\n\t\tdef isInterface(c):\n\t\t\t\"\"\"\n\t\t\tReturns ``True`` if the given class is present in the\n\t\t\tprovided C{iface.providedBy} or the interface is not\n\t\t\tcompliant with the interface in this context.\n\t\t\t\"\"\"\n\t\t\ttry:\n\t\t\t\treturn interface.providedBy(iface.providedBy(c))\n\t\t\texcept:\n\t\t\t\treturn False\n\t\n\tdef assertNotIn(self, iface, methodName):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    method = __getattr__\n    method_attr = None\n    method_value = None\n\n    def __getattr__(self, method):\n        \"\"\"\n        Return the candidate method.\n\n        :param method: The candidate method (a string,\n        :class:`~class:`~pyzope.interface.interface.Interface` and the\n        interface name, which should refer to a descriptor of the\n        class to which to use if the method has no `providedBy`\n        method).\n        :returns: the candidate method\n        :rtype: any\n        :raises: :exc:`~pyzope.interface.InvalidArgumentError`\n        \"\"\"\n        if not"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    def __new__(cls, *args, **kw):\n        \"\"\"\n        Class instantiation, but only a subclass of it (instead\n        of subclassing) can be invoked directly from the *original*\n        __init__, where the base `__init__` implementation of the *original*\n        __init__ method is determined by the `self.abstract` method.\n\n        Any arguments this base class may create might raise the TypeError(s)\n        containing the invalid argument value.\n\n        Example::\n\n            class Foo:\n                pass\n\n            foo = Foo()\n\n            class Bar(Foo):\n                pass\n\n            # a class with a metaclass may register *foo itself"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn _verify(iface, candidate, tentative==False)[0]\n\t\n\n# -----------------------------------------------------------------------------\n# Utility functions\n# -----------------------------------------------------------------------------\n\ndef _get(name, default=NO_DEFAULT, required=NO_VALID, error=NO_DEFAULT):\n    \"\"\"Helper function to fetch a `Name` object for the given input.\"\"\"\n    if name is NO_DEFAULT:\n        return default\n    try:\n        return getattr(zope.interface, name)\n    except AttributeError:\n        raise NotSupportedError(\"Unsupported argument ({})\".format(name))\n\t\t\t\ndef _validate(plugin_factory):\n    \"\"\"Helper function for"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\nclass Component(object): #XXX (XXX) - needs doc\n\t__implements__ = classmethod(implements)\n\t__requires__ = ()\n\t#__version__ = 1\n\t__version__ = '1.0a11'\n\t__all__ = ['name']\n\t__native__ = True\n\n\tdef __init__(self, name, interface):\n\t\tself.iface = interface\n\t\tself.name = name\n\t\t\n\tdef provide(self, obj):\n\t\t\n\t\tif hasattr(obj, 'providedBy'):\n\t\t\tobj = obj.providedBy()\n\t\t\n\t\treturn self"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # check that the candidate object provides us the interface\n    # (via the interface specification). The first check is:\n    #  * *provides*\n    #     is `twisted.internet.interfaces.IInterface` (as if the interface\n    #     contains the interface and *any* interface)\n    if iface.providedBy == candidate:\n        return True\n\n    # this is the first error\n    if not _verify(iface):\n        return False\n    \n    # check only the following methods are provided - all they\n    # are (via the @providedMethod or @providedAttribute).\n    # The second check is:\n    # "}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t if (vistrails.component.valid(iface, candidate, tentative)) or \\\n\t\t\t\t(vistrails.component.contains(iface, candidate, tentative)):\n\t\t\traise zope.component.Invalid\n\n\tif vistrails.component.valid(iface, candidate, tentative):\n\t\tif not candidate.providesBy(iface, tentative):\n\t\t\traise zope.interface.Invalid\n\n\treturn True\n\n@implementer(IView)\n@implementer(IViewBuilder)\nclass ViewBuilder:\n\t\"\"\"A Builder which does the rendering of a view"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    def __new__(cls):\n        \"\"\"\n        Create a new candidate for the passed class.\n\n        :return: a new class that passes ``iface`` if it passes\n         ``iface.providedBy`` and wraps ``iface`` to be callable and\n         returns the value of the function when invoked.\n        :rtype: `~zerofon.plugin.zope.interface.BaseConcreteProvider`.\n\n       .. note::  If the plugin has been tested, the methods that use\n           ``_verify`` and ``_check_provider`` must not be used directly.\n\n        \"\"\"\n\n        if not callable(iface):\n            raise Invalid(plugin"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t return _implements(iface, candidate, tentative, vtype)\n\n## ---- Implementation of interface types ---- #\n\nclass Interface:\n    \"\"\"A base interface to all interfaces, including types and methods.\n    \"\"\"\n    interfaceName = None\n\n    implements(interfaces.IUnknownInterface)\n    \n    properties = None\n    \n    def provides(self):\n        \"\"\"Return the list of all this interface.\n\n        This is an instance of L{Interface} subclass and does not have any\n        other methods.\n\n        Returns\n        -------\n        list: A list of L{Interface} objects.\n        \"\"\"\n        return []\n\n    def unimplemented(self, name"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\tvalid_methods = set([\n\t\t\t\tzope.interface.IInterfaceMethod,\n\t\t\t\tzope.interface.IUnknownInterfaceError,\n\t\t\t\tzope.interface.IInterfaceMissingAttributeError,\n\n\t\t\t\tzope.interface.IProtocolClass, # a protocol class\n\t\t\t\tzope.interface.IInterfaceMethod, # a method\n\t\t\t\tzope.interface.IProtocolAttribute,  # ctor method\n\n\t\t\t\tzope.interface.IInterfaceDecorator,\n\t\t\t\tzope.interface.interface.IInterfaceRequestMethod, # ctor method\n\n\t\t\t\tzope.interface.IInterfaceContrib"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    def _checkNotCalledError():\n        \"\"\"\n        Tests for this method call if this fails. This method is called\n        before calling this method if the last step is returned.\n        \"\"\"\n        return True\n    \n\n    def verifyUnimplementedMethod(method):\n        \"\"\"\n        Verify that implementing a method was not called by an\n        expected implementation. This is the reason why this method\n        has not been implemented yet.\n        \"\"\" \n        return not hasattr(method, '__implemented')\n\n\n    # this function should be overriden\n    @verifyObject\n    def checkImplements(iface):\t\n        \"\"\"\n        Verify that implementing a method was called by an\n        expected"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    return (interfaceClass and interfaceClass.__virtual__(interface) and\n            interfaceClass.__virtual__(candidate) and\n            provider.implementedBy(candidate) and\n            interfaceClass.__virtual__(tentative) and\n            candidate.providedBy(candidate))\n\n\ndef hasAdapterSet(interface, interfaces, interfaceClass, tentative=False):\n    \"\"\"\n    Checks if I interfaces set an interface for which this component's interface\n    is declared.\n\n    If the 'iface' is a string it will check the interfaces provided by\n    'iface.providedBy'. The 'interfaceClass' should be an\n    interface if it it must implement"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\nclass IStructative(object):\n\t\"\"\"\n\tMixin class used to provide some validation for the\n\t'implements<xsi.xsd:string>'field in the schema.\n\nThe idea also consists of the code for the fields of all interfaces.\n\nA valid method or name must implement an instance that has an\n  attribute, which will be the ``name`` attribute (e.g. 'hello'). For example, a\n  method ``id_number``, methods are used to pass id number from the\n  form ``/xsi:IdNumber``, and methods in the form ``/x"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\traise Invalid('No such error')\n\n\t# Find the first error that will be present. First error is a\n    # tuple or list. Then two error types are collected:\n\terrors = [(e.condition, e) for e in candidate.errors]\n\t# Errors that could possibly be present\n\tbad_failures = [(e.condition, e) for e in errors]\n\tif best_match is None:\n\t\tbest_error = None\n\telse:\n\t\tbest_error = best_match[0]\n\tself.log(u'Considering candidate \"%s"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\t# This is the first step (i.e.: `iface.providedBy` and\n        # `iface.providedByBy`).\n\t\tfor name, err in getFunctions(candidate.methods)[:-1]:\n\t\t\t# We need two special cases: `iface.providedBy` has a different\n\t\t\t# attribute `providedBy` (which is always True).\n\t\t\tif err.providedBy is None:\n\t\t\t\tcontinue\n\t\t\tif err.providedBy.providedBy is None:\n\t\t\t\terr.providedBy = True\n\t\t\t\tcontinue\n\t\t\t# If there's only one attribute, and the second one"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\tif tentative:\t\t\t# only consider the error present in the targeted\n\t\t\tresult = defer.fail(failure.Exception(\n\t\t\t\t'test: unexpected failure when'+\n\t\t\t\tstr(targeted).split('.')[0] + \" does not have the required interfaces in\\n\" +\n\t\t\t\t\"testing the adapter's implementation!\\n\" +\n\t\t\t\ttraceback.format_exception(*traceback.extract_tb(targeted))).callMethodName)\n\t\telse:\n\t\t\tif not hasattr(identificationInterfaceType, 'providedBy'):\n\t\t\t\tvalidation.verifyInterface(iface, candidate, "}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\ndef verifyBoolean(key):\n    \"\"\"\n    Verify if *key* is a Boolean.\n\n    :param key: The key to verify.\n    :return: ``True`` if the supplied key is a Blitz Boolean or ``False``\n           otherwise.\n    \"\"\"\n    if isinstance(key, boolean) or isinstance(key, (integer, float, int, long)):\n        return key\n    return (not isinstance(key, (tuple, list, dict, set, frozenset))\n            and not (isinstance(key, str)\n                     or isinstance(key, unicode)))\n\n\nfrom.models import *\nfrom django.db"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn False\n\nclass Test(object):\n\t'''Tests for :class:`zope.interface.iface.Object`'''\n\t'''\n\t'''\n\tIdeally, this will make it impossible to access an object by a different\n    object, rather than a :class:`zope.interface.interface`, but since the\n    *class* will change its own properties.\n\t\n\tIn order to support multiple methods, this class can be used in\n\torder to register other classes that require a particular interface.\n\t'''\n\tdef __init__(self, owner, interface):\n\t\t'''\n\n\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\ttry:\n\t\t\tmethod = getattr(iface.providedBy, '__call__')\n\t\texcept AttributeError:\n\t\t\traise interfaces.InvalidInterface(self.provider) # Illegal __call__()\n\n\nclass ObjectRegistry:\n\t\"\"\"Object classes for managing ``Object`` interface.\"\"\"\n\tpass\n\n\tdef __init__(self):\n\t\tself._objects = {}\n\t\tself._names = {}\n\n\tdef __call__(self, f):\n\t\tfor name in dir(f):\n\t\t\tv = getattr(f, name)\n\t\t\tif (v is not None):\n\t\t\t\tself._add_object"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n@implementer(interface.IObjectInfo)\nclass _Info(object):\n    \"\"\"\n    An object that provides the information about an object.\n\n    The information is in the following structure::\n      {\n          \"path\" : str,\n          \"uid\" : uid,\n          \"gid\" : gid,\n          \"path\" : [str],\n          \"uid\" : uid,\n          \"gid\" : gid,\n          \"path\" : [str],\n          \"mode\" : \"r\",\n          \"readable\" : bool,\n         ...\n          \"writable\" : True,...\n      }\n\n    and its keys are set to their"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\traise NotImplementedError(iface)\n\ndef verifyGlobalClass(iface, candidate=None, tentative=True):\t\"\"\"\n\tVerify that *candidate* should provide *iface*.\n\t\"\"\"\n\t\tif candidate:\n\t\t\tiface = candidate.valu(iface)\n\treturn iface\n\ndef verifyClassIsSame(self, interface, candidate):\n\t\t\"\"\"\n\t\tCheck if *candidate* has common name.\n\t\t *If not, return None, return False\n\t\t\"\"\"\n\t\treturn hasattr(self, candidate.name()) and\\\n\t\t\tgetattr(self, candidate.name()) == getattr"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\ttry:\n\t\t\tiface = iface.decode(\"utf-8\")\n\t\texcept:\n\t\t\t# I'm not sure why... this is the right exception\n\t\t\treason = \"Unable to convert a %s to unicode: %s\" % (iface.__class__.__name__, iface)\n\t\t\tfor line in sys.stderr:\n\t\t\t\tif \"Invalid encoding in UTF-8\" in line: line = line.decode(\"UTF-8\")\n\t\t\t\treason = reason + line\n\t\ttry:\n\t\t\tcandidate = unicode(iface, \"utf-8\", \"ignore\")\n\t\texcept UnicodeDecodeError:\n\t\t\terr"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\traise NotImplementedError(\"Can not handle %s\" % candidate.__class__.__name__)\n\n\tdef verifyInterfaces(ifaceList=None):\n\t\t\"\"\"\n\t\tVerify the *ifaceList* must be a *list*.\t\"\"\"\n\t\traise NotImplementedError(\"Can not handle %s\" % ifaceList.__class__.__name__)\n\n\tdef createTestbed(self):\n\t\t\"\"\"\n\t\tMake a testbed from the database and return the results.\n\t\tReturns the database connection object.\"\"\"\n\t\treturn self._openTestbed()\n\"\"\"\n\nPython Interchangeable Virtual Instrument Library\n\nCopyright (c) 2012"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif iface is None:\n\t\t\treturn False\n\n\t\tfor eachClass in candidate.__class__.__mro__[1:]:\n\t\t\tif className(eachClass) in iface.__bases__:\n\t\t\t\treturn True\n\n\t\treturn False\n\n\tdef verifyMethod(iface, method, required=None):\n\t\t\"\"\"\n\t\tVerify that the method *iface* may be implemented to be the actual method.\n\t\tIf *required* is specified, it's used to determine whether the required\n\t\tclass we can call to*this* method is acceptable (i.e., *method* is an\n\t\tinstance of a subclass with"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tfrom Components.SystemInfo import SystemInfo\n\t\tif not iface:\n\t\t\treturn False\n\n        return (iface == candidate) or (\n\t\t\t(not modulePath or modulePath == '' or\n\t\t\t  isinstance(modulePath, str)) and\n\t\t\t(not ttentative) or\n\t\t\t(candidate in (iface, candidate) or\n\t\t\t\t(not modulePath and\n\t\t\t\t\tnot isinstance(modulePath, str)) and\n\t\t\t\t(not ttentative))\n\n\tdef isSupported(self):\n\t\treturn self.getCapabilities(self.getCapabilities.getValue()).getObject()\n\n\tdef getCapabilities("}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t  \n\t    def verify_class(iface):\n\t        # if we do an unsaturate attempt... \n\t\t\n\t        if iface == candidate:\n\t                raise Exception(\"Couldn't connect to %s. Check you are using %s\"%\\\n\t\t\t\t\t\t\t\t(candidate, iface)) \n\t\t\n\t        else:\n\t                raise Exception(\"Could not verify %s\"%iface) \n\t\t\n\t\t# if the user passed it without the 'tentative' flag then it must\n\t\t# have been provided through the method rather than the \n\t\t# method, then it's a potential failure condition in \n\t\t#   verifyClass()"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\t# Check that the device is a proper one that can provide the *entative* keyword\n\t\t# The following is for validation\n\t\tfor a in candidate.getFeatures().asList():\n\t\t\tif a.feature[0].getValue() == iface:\n\t\t\t\treturn a\n\t\tif tentative:\t\t\t\t# If this method is not found, you can't\n\t\t\tprint('Error: Not found suitable instance at %s for verification' % addr)\n\t\treturn None\n\t\n\tdef assertDevice(self, iface, candidate = None):\n\t\t# Check that an IP has the given *iface*"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif candidate.name!= iface:\n\t\t\traise ValueError(\"Unexpected interface type %s (expected %s, got %s)\" \\\n\t\t\t\t% (iface.__class__.__name__,\n\t\t\t\tcandidate,\n\t\t\t\ttype(iface).__name__))\n\telse:\n\t\tassert not candidate.name in iface.__dict__.keys()\n\t\tassert not candidate.parenttype and candidate.parent is not None\n\t\tassert not candidate.config and candidate.default is None\n\t\tassert not candidate.retired and candidate.config is None\n\t\tassert not candidate.retired and not candidate.retired_"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tverifyClassName(iface, candidate, tentative=tentative)\n\ndef verifyIfaceModule(iface, candidate):\n\t\"\"\"\n\tVerifies that *candidate* may correctly provide *iface* as a module.\n\tTo be used with caution, this checks that exactly one candidate needs to be provided.\n\t\"\"\"\n\tif candidate is not None:\t\tif (not all(not candidate in iface.list)):\t\t\treturn\t# Only one module (or a module-only one); can be verified\n\tiface.verify(any(iface.list))\t\t\t\t# Allow all"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\t# Invalidate that we are not dealing with a valid module; we don't check\n\t\t# directly for invalid module names. We always check for a module name\n\t\t# with a suffix.\n\t\tif str(candidate)!= str(iface):\n\t\t\tlogging.debug('Module name %s does not specify an interface, returning False', candidate)\n\t\t\treturn False\n\n\t\tclassName = str(candidate)[2:-1]\n\t\tlogger.debug('Module name %s may specify an interface, returning True', className)\n\t\tverify = False\n\t\tvalid = False\n\n\t\tif found_class"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tfor c in bases:\n\t\t\tk=c.__mro__[0]\n\t\t\tif k not in explicit_mc:\n\t\t\t\tkw=k.__dict__.copy()\n\t\t\t\tif 'baseclass' in kw:\tkw['baseclass']='type'\n\t\t\t\telse:\tkw['baseclass']=c.__class__\n\t\t\t\tif c.__name__.endswith('__bases__'):\tbaseclass=c.__name__[:-2]\n\t\t\t\tself._merge_classes(\n\t\t\t\t\t[k]\n\t\t\t\t\tfor k,l in zip(kw.values(),l)\n\t\t\t\t\tif hasattr(c,'__bases"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tfor base in _iterbases(bases):\n\t\t\tif not hasattr(base,'metaclass'):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif not hasattr(base, '__bases__'):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif not hasattr(base, '__dict__'):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\t# Find metaclass for each class\n\t\t\tclasses = [a.__name__ for a in base.__bases__ if a.__name__ == 'AbstractMetaClass']\n\t\t\tfor cls in classes:\n\t\t\t\tif ((cls.__name__.startswith('Meta') or cls.__name__.startswith('__')) or\n\t\t\t\t\t\t(cls.__name__.startswith('Abstract')):\n\t\t\t\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = {}\n\t\tfor base in bases:\n\t\t\tif not hasattr(base, \"__metaclass__\"):\n\t\t\t\tcontinue\n\t\t\tname = base.__name__\n\t\t\tif name.startswith(\"AbstractBase\") or name == \"__metaclass__\":\n\t\t\t\ttry:\n\t\t\t\t\tmeta = AbstractBaseMeta(base)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\t\t\tbase = base.__bases__[1]\n\t\t\telse:\n\t\t\t\t# if type(name) == str: # use base.__dict__ or base.__member__ name directly\n\t\t\t\t\t# name.split(\".\")"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = __all__\n\t\tif sys.version_info < (2, 7):\n\t\t\tmod_classname = bases[0].__name__\n\t\telse:\n\t\t\tmod_classname = bases[0].__module__\n\n\t\t# try the 'cls' metaclass\n\t\tfor i in range(1, len(bases), 2):\n\t\t\tif bases[i] == '__new__' and bases[i+1] == '__new__' and cls not in explicit_mc:\n\t\t\t\treturn cls, mod_classname, bases[i+1]"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc or bases == (object,):\n\t\t\treturn (None, bases)\n\t\ttry:\n\t\t\tmeta = __metaclass__ = bases[0].__metaclass__\n\t\texcept (TypeError, AttributeError):\n\t\t\tpass\n\t\telse:\n\t\t\treturn meta.__new__(meta, bases, bases[1:])\n\t\treturn meta\n\ndef determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc or bases == (object,):\n\t\tname = bases[0].__name__\n\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t#if base not in bases:\n\t\t#\t#first base\n\t\t#\tbases.append(base.__module__)\n\t\tif explicit_mc:\n\t\t\tif type(explicit_mc) == list:\n\t\t\t\tfor base in explicit_mc:\n\t\t\t\t\tif isinstance(base, type) and issubclass(base, types.ClassType):\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tmetaclass = type(base)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\tifMetaclass = class_registry.get(metaclass)\n\t\t\t\t\t\tif not metaclass:\n\t\t\t\t\t\t\tclass_registry[metaclass] = metaclass\n\n\t\t\t\t\ttry:"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = bases & cls.__metaclass__\n\t\tcls = mro_metaclass.get(explicit_mc, None)\n\t\tif not cls:\n\t\t\traise RuntimeError(\"abstract base class found\")\n\t\targs = inspect.getargspec(cls)[0]\n\t\tfor a, b in args.items():\n\t\t\ta = (a,) if firstlevel else (b,)\n\t\t\tif bases & a in cls.__bases__:\n\t\t\t\tif explicit_mc is not None and \\\n\t\t\t\t    a not in explicit_mc:\n\t\t\t\t\traise RuntimeError(\"'%s' is"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif bases and bases.__class__ not in [type, type(bases)]:\n\t\t\traise Exception(\"Unable to detect a metaclass from a subclass\")\n\n\t\ttry:\n\t\t\t# get metaclass as a builtin metaclass\n\t\t\ttry:\n\t\t\t\treturn bases.__dict__[explicit_mc]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\t\t\t# if there's a __metaclass__, assume subclass is an extension\n\t\t\t# of the metaclass\n\t\t\tif implicit_mc:\n\t\t\t\ttry:\n\t\t\t\t\tbases.pop(implicit_mc)\n\t\t\t\texcept KeyError:\n\t\t\t\t\traise Exception(\"Unsupported metaclass {0!r}\".format("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc == None:\n\t\t\texplicit_mc = dict((base, []) for base in bases)\n\t\tall_bases = set(chain.from_iterable(_m for _m in explicit_mc.keys())) #TODO: use __all__ once, we know which already\n\t\tbase = None\n\t\t\t\n\t\t# Try to find a metaclass from both bases\n\t\tfor basename, bases in iter_classes(baseclasses):\n\t\t\ttry:\n\t\t\t\t# this only checks the case where all bases are singleton\n\t\t\t\tif base not in all_bases:\n\t\t\t\t\t# this needs to be"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tassert(len(bases) == 1) or (len(bases) == 2) or (len(bases)!= 3)\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = 1\n\t\tbase_cls = bases[0].__metaclass__\n\t\tif base_cls in ('Type', 'ClassType'):\t\t\t\t# Assume 1\n\t\t\tif hasattr(bases[0], '__name__'):\n\t\t\t\tdefault_cls = bases[0].__name__\n\t\t\t\tknown_bases = [x for x in bases[1:] if isinstance(x, explicit_cls)]\n\t\t\t\tif default_cls"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tkey = self._marker_class(key, default)\n\t\tout = self[key]\n\t\tdel self[key]\n\t\treturn out\t\n\tdef setdefault(self, key, default=__marker):\n\t\t\"\"\"\n\t\tD.setdefault(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself[key] = value = self.pop(key)\n\t\texcept KeyError:\n\t\t\tout = self.pop(key, default"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tvalue = super(OrderedDict, self).pop(key)\n\t\texcept KeyError:\n\t\t\tif default!= self.original.get(key, default):\n\t\t\t\tvalue = default\n\t\t\treturn value\n\n\tdef append(self, key, item):\n\t\ttry:\n\t\t\tsuper(OrderedDict, self).append(key, item)\n\t\texcept ValueError:\n\t\t\tself.dict.setdefault(key, []).append(item)\n\t\telse:\n\t\t\tself.original[key] = item\n\t\t\tself.dict[key] = self.todict()\n\n\tdef popitem"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\t\t\t\t\t\tif isinstance(self[key], dict): \n\t\t\t\t\t\t\t\t\tdel self[key]\n\t\t\t\t\t\t\telif isinstance(self[key], list): \n\t\t\t\t\t\t\t\t\tdel self[key]\n\t\t\t\t\t\t\t\telse: \n\t\t\t\t\t\t\t\t\traise KeyError(key)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\traise KeyError(key)\n\t\t\t\t\traise KeyError(key)\n\t\t\telse: return self[key]\n\t\telse: \n\t\t\treturn self.__getitem__(default) \n\t@staticmethod\n\tdef __setitem__(i,j,val):\n\t\ti = i.lower()\n\t\tj = j.lower()"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\tdel self[key]\n\t\telif default is self.__marker:\n\t\t\treturn default\n\t\treturn default\n\t\t\n\tdef append(self, key, value):\n\t\tself.__insert(key,value)\n\n\tdef popitem(self):\n\t\ttry:\n\t\t\treturn self.__seq[-1]\n\t\texcept IndexError:\n\t\t\traise KeyError(key +'not in list')\n\n\tdef setdefault(self,key,default=None):\n\t\tself.__insert(key,default)\n\t\n\tdef __len__(self): return len(self.__seq)\n\t\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\t\n\t\t\treturn self[key].pop(d)\n\t\texcept KeyError:\n\t\t\treturn default\t\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), return and remove specified key and return the corresponding value.\n\t\tIf key is not found, return and remove all keys; all missing keys are removed.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._dic[self._pos+self._size]\n\t\texcept KeyError:\t\t\n\t\t\treturn {}\n\n\tdef __contains__(self, key):\n\t\treturn key in self._"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tif self.has_key(k):\n\t\t\t\tdel self[key]\n\t\t\t\tif default is __marker:\n\t\t\t\t\traise\n\t\t\t\treturn default\n\t\t\telse:\n\t\t\t\traise\n\t\treturn v, len(self)\n\n\tdef clear(self):\t\"\"\"\n\tD.clear() -> None\n\tRemove all references to the underlying value.\n\t\"\"\"\n\t\tself.clear_mutex()\t\t\t# to be implemented by subclasses\n\nfrom. import util\n\n\nclass Data:\n\n    def __init__(self"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t    \tkey = key.lower()\n\t    \tif key in self.__dict__:\n\t      \t\t\tv = self.__dict__[key]\n\t      \t\t\tif len(v)>0:\n\t      \t\t\t\treturn v[-1]\n\t      \t\t\t\telse:\n\t      \t\t\t        return default\n\t      \t\t\tr = _marker(key, v)\n\t      \t\t\tif r not in self.data:      \t\t\tself.data[r] = default\n\t      \t\t\treturn r\n\t\t\t\telse:\t\treturn v\n\t      \t\t\telse:   return None\n\t      \t\t\telse:      "}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\td = self.stack[self.head-1]\n\t\t\tself.stack.remove(d)\n\t\t\treturn d[key]\n\t\texcept IndexError:\n\t\t\treturn default\n\ndef popitem(n, d=None):\t\"\"\"\n\tD.popitem(n) -> v, remove and return the specified dict item.\n\tIf no value is found, returns d. If given and/or d is given, it returns\n\td.\n\tIf given and/or d is not found, d is returned.\n\tIf given and/or d is"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self or not d:\n\t\t\tval = default\n\t\t\tself.pop(key, None)\n\t\treturn self.pop(key), val\n\nclass SortedDict(Mapping):\n\tdef find(self, elem):\n\t\tif elem not in self:\n\t\t\tself[elem] = self.setdefault(elem, [])\n\t\t\tself[elem].sort()\n\t\t\tif len(self[elem]) > 1:\n\t\t\t\traise KeyError(elem)\n\t\t\telse:\n\t\t\t\treturn self[elem]\n\t\treturn self[elem]\n\nclass DictAdapter(MutableMapping):\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key not in self:\n\t\t\tkeylist = list(self)\n\t\t\tkeylist.remove(key)\n\t\t\tvalue = default\n\t\t\tif key in keylist:\n\t\t\t\tvalue = keylist.pop(key)\n\t\t\treturn value\n\t\td = self[key]\n\t\tdel self[key]\n\t\tvalue = d.pop(key, d.get(default))\n\t\treturn value\n\n\tdef __iter__(self):\t\t\n\t\tfor key in self.keys():\n\t\t\tif key in self.data:\n\t\t\t\tyield key\n\t\traise StopIteration\nfrom base import"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tresult = (None, None)\n\t\tresult_tuple = self.heap.pop()\n\t\tif result_tuple:\n\t\t\treturn result\n\t\tresult = (key, None)\n\t\tresult_tuple = self._sort_key(result_tuple, tuple(self.heap))\n\t\tif result_tuple:\n\t\t\tself.heap.append((result_tuple, key))\n\t\telse:\n\t\t\tself.heap.append((key, value))\n\t\t\tself._heapify()\n\t\treturn result\n\n\tdef popleft(self):\n\t\t\"\"\"\n\t\tRemove and return the first"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tkey = self.position + self.width\n\t\treturn key, self.value\n\n\tdef get(self):\n\t\t\"\"\"Return a (value, last) tuple (key, last).\n\t\t\"\"\"\n\t\tkey, last = self.getkey()\n\t\treturn self.position, self.value\n\n# Natural Language Toolkit: Tanh Quad Machine\n#\n# Copyright (C) 2001-2017 NLTK Project\n# Copyright (C) 2001-2005 NLTK Project\n# Author: Steven Bird <stevenbird1@gmail.com>\n#        "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tkeys = sorted(self.keys())\n\t\tdel self[keys[0]]\n\t\treturn key, [v[1] for v in values(keys) if v[0] not in self.keys()]\n\n\tdef keys(self):\t\t\n\t\t\"\"\"\n\t\tGenerate a list of keys in sorted order. This can be used to sort\n\t\titems in order from most frequent to least frequent.\n\t\t\"\"\"\n\t\tkeys = sorted(self.keys(), key=lambda item: item[0])\n\t\t# Remove items to maintain order.\n\t\tfor item in reversed(keys):\n\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\ttry:\n\t\t\titemkey = self.list.pop(0)\n\t\t\tif self.type == TYPE_VALUE_LIST:\n\t\t\t\tself.value = self.value[0]\n\t\t\t\treturn (self.key, self.value)\n\t\t\telif self.type == TYPE_VALUE_DICT:\n\t\t\t\tfor key, value in self.value.items():\n\t\t\t\t\tif key == itemkey:\n\t\t\t\t\t\tself.value = value\n\t\t\t\t\t\tbreak\n\t\texcept IndexError:\n\t\t\treturn (None, None)\n\n\tdef push(self, key, value):\n\t\tself.list.insert"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif self.__item is not None:\n\t\t\tself.__item.pop(0)\n\t\t\tself.__item = None\n\t\treturn self.__item\n\n\tdef popleft(self):\t\"\"\"\n\tRemove the topmost character in the list and return False.\n\t\"\"\"\n\t\tl, e = self.pop()\n\t\tself.__item = (l + e)[0]\n\t\ttry:\n\t\t\tif not self.__map[self.__item].startswith(self.key):\n\t\t\t\traise KeyError\n\t\texcept TypeError:\n\t\t\tpass\n\tdef unpop(self):\n\t\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tmax_value = 0\n\t\tcount = 0\n\t\tfor i, (key, value) in enumerate(self._unique_keys):\n\t\t\tif key == self._unique_keys[i][1]:\n\t\t\t\tcount += 1\n\t\t\telse:\n\t\t\t\tbreak\n\t\tif count == 0:\n\t\t\tself._unique_keys[0] = ''\n\t\t\tself._unique_keys[-1] = ''\n\t\t\tself._unique_keys.append([key, value])\n\t\t\n\t\telse:\n\t\t\tself._unique_keys.pop(0)\n\t\t\tself._unique_keys.append(["}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\ttry:\n\t\t\treturn self._cache[self._index]\n\t\texcept KeyError:\n\t\t\tdel self._cache[self._index]\n\t\t\tself._index += 1\n\t\t\treturn (self._index, self._value or \"\")\n\n\tdef pop(self):\t\"\"\"\n\tRemove and return the empty tuple in sorted order.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._cache[self._index]\n\t\texcept KeyError:\n\t\t\tself._index -= 1\n\t\t\treturn self._index, \"\"\n\nclass DictWrapper(object):\n\t\"\"\"\n\tA helper class to encapsulate"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__q.get_nowait()\n\t\texcept Queue.Empty:\n\t\t\tpass\n\t\texcept AttributeError:\n\t\t\traise\n\t\n\tdef get(self):\n\t\t\"\"\"\n\tFetch the `key` of the top item.\t\n\t:return: `None` - No result, or one of it\n\t\"\"\"\n\t\td = self.__q.getl()\n\t\tif (len(self.__q) > 1):\n\t\t\treturn d[1]\n\t\telse:\n\t\t\treturn None\n\t\n\tdef popitem(self):\n\t\t\"\"\"\n\tRemove and"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn savetxt(self[\"epg\"].getCurrentServiceRef(), (self.x.getCurrentTime(), self.y.getCurrentTime(), self.title))\n\n\tdef keyLeft(self):\n\t\t\"\"\"\n\t\tLeft click on the left side of the key map?\n\t\t\"\"\"\n\t\tself[\"epg\"].left()\n\n\tdef keyRight(self):\n\t\t\"\"\"\n\t\tRight click on the right side of the key map?\n\t\t\"\"\"\n\t\tself[\"epg\"].right()\n\n\tdef keyNoRepeat(self):\n\t\t\"\"\"\n\t\tSkip to all if channelselection is disabled, in"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif self.pop_key:\n\t\t\twhile len(self.items):\n\t\t\t\tkey, value = self.pop_key\n\t\t\t\tself.pop_key = None\n\t\t\t\tself.add_item(key, value)\n\t\t\t\tif self.overwrite:\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tself.items.pop(0)\n\t\t\treturn self.items.popitem()\n\t\telse:\n\t\t\treturn (self.get(None, self.default_item), None)\n\tdef get(self, item, default=None):\t# for compatibility\n\t\tkey = self.key"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tself._currentItem = self._popKey(NO_ROWS, False)\n\t\tif len(self._items) == 0:\n\t\t\tself.deleteitem(self._currentItem)\n\t\treturn self._items.pop(self._currentItem.key, None)\n\n\tdef push(self, key, value):\t\"\"\"\n\tPut the `(key, value)` pair on the stack.\t\n\n\t:Parameters:\n\t\tkey: The key to store\n\t\tvalue: The value to append to the stack to\n\t\t\"\"\"\n\t\tif key not in self._items:\t#"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\ttry:\n\t\t\tkey = self.pop(0)\n\t\texcept IndexError:\n\t\t\traise ValueError(\"popitem: pop is empty\")\n\t\ttry:\n\t\t\tindex = self.pop(1)\n\t\t\tif index == -1:\n\t\t\t\tself[key] = (key, None)\n\t\t\telse:\n\t\t\t\tself._dict[key] = (index, None)\n\t\texcept IndexError:\n\t\t\traise ValueError(\"popitem: failed to pop item off the stack\")\n\n\tdef popitem_recursive(self):\t\t\"\"\"\n\tRemove the cached previously set key in the stack."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif self.cache.has_key(self.key(0)):\n\t\t\tdel self.cache[self.key(0)]\n\t\t\treturn self.cache[self.key(0)]\n\t\treturn None\t\n\t\n\tdef pop(self):\t\"\"\"\tPull out the highest value for the key.\t\"\"\"\n\t\tif not self.cache.has_key(self.key(0)):\n\t\t\tself.cache[self.key(0)] = {}\n\t\tif self.cur_item is None:\t\n\t\t\tcur_item = self.cur_key[0]\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self[\"list\"].getCurrent()[0]\n\n\tdef popitemindex(self):\t\"\"\"\n\tRemove and return the `(key, index)` pair most recently used.\t\"\"\"\n\t\treturn self[\"list\"].getCurrentIndex()\n\n\tdef popitemsize(self):\t\"\"\"\n\tRemove and return the `(key, size)` pair most recently used.\t\"\"\"\n\t\treturn self[\"list\"].getCurrentSize()\n\n\tdef popitemalignment(self):\t\"\"\"\n\tRemove and return the `alignment` key and most recently used.\n\t\"\"\"\n\t\treturn self[\"alignment\"]\n\n\tdef pop"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\titem = self.cur_item\n\t\tif item == None:\n\t\t\treturn None, None\n\t\tlast_data_key = self.cur_data_key\n\t\tlast_data_val = self.cur_data_val\n\t\tfor item, last_data_item in self.items:\n\t\t\tkey = (last_data_key, item)\n\t\t\titem = last_data_item[1]\n\t\t\tif key in self.dirty_keys:\n\t\t\t\treturn last_data_item\n\t\treturn None\n\n\tdef popvalue(self):\n\t\t\"\"\"\n\t\tRemove"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif len(self._stack) > 1:\n\t\t\t(key, value) = self._stack.pop()\n\t\t\tself._stack.append((key, value))\n\t\telse:\n\t\t\tself._stack.append((0, None))\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tParse a `key` without the `default` argument. This is a recursive\n\t\tsearch if the key hasn't been set yet. In that case an empty\n\t\tlist is returned.\n\t\t@param key:\n\t\t@return:\n\t\t@rtype:\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tself.calls.pop(getkey(MINION_KEY_MAX), None)\n\t\treturn getvalue(self.calls)\n\n\tdef getvalue(self, key, default=None):\n\t\t\"\"\"\n\t\tReturns a tuple in the form (key, value) as described above.\n\t\t\"\"\"\n\t\tif key in self.calls:\n\t\t\traise UnknownKeyError(key, self.calls[key])\n\t\treturn default\n\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tLike a dictionary.setdefault(), but make a copy if ``default``\n\t\tis"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif len(self._items)!= 1:\n\t\t\treturn None, None\n\t\tkey, value = self._items[0]\n\t\tif key not in self.cache:\n\t\t\tself.cache[key] = (None, None)\n\n\t\tif key in self.cache and value == self.cache[key][0]:\n\t\t\tself.cache[key][1] = value\n\t\t\treturn (key, value)\n\t\telse:\n\t\t\tif self.cache[key][2] is value:\n\t\t\t\tself.cache[key][1] = None\n\t\t\t\tself.cache["}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tlastval = self.lastkey\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tk, v = self.pop(lastval)\n\t\t\texcept IndexError:\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tif v is None:\n\t\t\t\t\tk, v = k[-1], v[-1]\n\t\t\tlastval = v\n\t\t\tlastkey = k\n\n\tdef popitemsort(self, *args):\n\t\t\"\"\"\n\tRemove all duplicates from the frontmost list in sorted order.\n\tThis is a more efficient but less accurate.\n\t\"\"\"\n\t\ttry:\n\t\t\tkey, lastval"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tkey = self.cur_key\n\t\tvalue = self.cur_value\n\t\tif key not in self.used:\n\t\t\tself.used.append(key)\n\t\telif value in self.used:\n\t\t\tself.used.remove(value)\n\t\treturn (key, value)\n\t\n\tdef top(self):\n\t\t\"\"\"Fetch topmost `(key, value)` pair.\"\"\"\n\t\treturn self.popitem()\n\n\tdef top_bottom(self):\n\t\tself.clear()\n\t\treturn self.popitem()\n\n\tdef bottom(self):\n\t\t\"\"\"Fetch biggest"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tcur_entry = self.cur_data()\n\t\tif not self.is_valid():\n\t\t\tself.error()\n\t\t\treturn None\n\t\tif cur_entry[0] is not None and \\\n\t\t\tcur_entry[1] is not None:\n\t\t\tself.cur_entry = self.cur_data()\n\t\treturn (cur_entry[0], cur_entry[1])\n\n\tdef insert_data(self, item, key):\n\t\t\"\"\"\n\t\tInsert the `self\" entry at position `item`. `key` is the key to be\n\t\tinserted."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn self._items.pop()\n\tdef popitem_sort(self):\n\t\t\"\"\"\n\tAdd and return a function that will be called when the list is sorted.\n\tThis function should be called when a key is removed, and whenever a\n\titem is placed, the function should return the item the sorted item had.\n\t\"\"\"\n\t\treturn self._get_last_keys()\n\tdef popitem_get(self):\t\"\"\"\n\tRemove the previously-sorted item if it was a valid one or, if not.\n\tReturns `(key, value)` pair after popping"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tlastkey = self.lastkey\n\t\tcurselitem = self.cursel\n\t\tcurrsel = None\n\t\tif not self.items:\t# If we don't have an item list or the root key is empty.\n\t\t\t# Get the root key.\n\t\t\tif lastkey is None:\n\t\t\t\tcurselitem = self.cursel\n\t\t\t\tself.cursel = None\n\t\tif curselitem is not None:\n\t\t\tif cursel!= lastkey:\t# If there are more than one item, pop it.\n\t\t\t\tself.items.pop(0)\n\t\t\telse"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tself.tk.call('tkPopItem', self._w)\n\tdef move(self, index_or_widget, index=None, item=None):\n\t\t\"\"\"\n\t\tMove `index_or_widget` to the head `index`. If `index` is specified,\n\t\tthis keyword specifies the index of the most recently used item.\n\t\tIf `index` is not specified, the first item in the list is moved from\n\t\tthat position to it, so if that list is not already used, the position\n\t\tof the last item will shift to it.\n\t\t\n\t     See Also"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\titem = self.queue.pop(-1)\n\t\tif self.queue and len(self.queue) == 0:\n\t\t\tdel self.queue[0]\n\t\treturn item\n\nclass _ListProxy:\n\t\"\"\"\n\tThis class makes a dict-like interface (mostly used by `imaplib` instead of\n\t`imap4`).  This is useful in most situations, where one item is needed\n\tto fetch an item several times.  The `key` argument lets you\n\tretrieve `key` items from the cache.  By default, the return value is\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tkey = self.stack[-1]\n\t\tif key == self.separator:\n\t\t\tself.popitem_separator()\n\t\telse:\n\t\t\tself.items[-1][None].pop(key)\n\t\t\t\n\tdef popitem_separator(self):\n\t\t\"\"\"\n\t\tPop and pop the first item from the `itemlist`.  Return (key, value) pair, or None if `None`.\n\t\t\"\"\"\n\t\tif len(self.items) > 0:\n\t\t\tif len(self.items) == 0:\n\t\t\t\tdel self.items[-1]\n\t\t\telse:"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif not self.data:\n\t\t\treturn None, None\n\n\t\tres, key = self.data.popitem()\n\t\tif res is None:\n\t\t\titem = (key, self.data.pop(key) or None)\n\t\t\tres = (key, self.data.pop(key) or None)\n\t\treturn item\n\n\tdef poplast(self, n=None):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair, optionally starting with or\n\t\tnone, of `n` elements. If `n` is specified, the result is not stored.\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif len(self._cache) == 0:\n\t\t\treturn None\n\t\t\n\t\tkey = self._cache.popitem()[1]\n\n\t\treturn (key, self._cache.get(key, ()))\n\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet and save a pickled object to the file. \n\t\t\"\"\"\n\t\tif isinstance(value, dict) and 'value' in value.keys() and not value.has_key('value'):\n\t\t\tkey_and_value = (key, value)\n\n\t\t\tif key not in self._cache:\n\t\t\t\tself._cache"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\t# TODO: support other objects\n\t\treturn self._items.pop()\n\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\tRemove a key (key may be in the cache), or raise KeyError if it doesn't exist.\n\t\tNote that unlike most other lookups, the removal will not delete the key if the key is found\n\t\tin the cache.\n\t\t\"\"\"\t\n\t\tif self[key]:\n\t\t\tself._items = self._cache.pop(key)\n\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tRemove the key (or all elements with *"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn self.value\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturn all the `key` pairs in it.\n\t\t\"\"\"\n\t\treturn list(self.iterkeys())\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tReturn the `value` pairs in it.\n\t\t\"\"\"\n\t\treturn self.itervalues()\n\n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tSet the (key, value) pair to `default` if it doesn't exist.\n\t\t\"\"\"\n\t\tif key in self:\n\t\t\tvalue = self[key]\n\t\t\tif value"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif not self.item:\n\t\t\traise IndexError(\"popitem() after the non-existing \" + str(id(self)))\n\t\tkey, value = self.item.popitem()\n\t\tif key == id(self):\n\t\t\traise ValueError, \"key pair is not removed\"\n\t\treturn key, value \n\tdef setdefault(self, key, default=None):\n\t\t\"\"\"\n\t\tReplaces the key with the value of the queue and the\n\t\t*default* value. The key is also removed if you set otherwise.\n\t\tIf the queue is full, then its parent is changed to"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\ttry:\n\t\t\titem = self._dict[self._heap[self._root].pop(self._key)]\t# delete item\n\t\t\tif self._key in self._heap:\t\t# remove it\n\t\t\t\tself._heap.pop(self._key)\t\t# add it\n\t\t\t\tself._list.remove(self._key)\t\t# remove the item\n\t\t\t\treturn 1\t\t\t\t\t\t# remove the item\n\t\t\t\t#\n\t\t\telse:\n\t\t\t\tassert False\t\t\t\t\t\t# unreachable\n\t\t\t#\n\t\texcept KeyError:\n\t\t\treturn key, value\t\t# end def\n\t\n\tdef"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\ttry:\n\t\t\tif self[self.index(self.key)] == value:\n\t\t\t\tdel self.liststore[self.index(self.key)]\n\t\texcept:\n\t\t\tself.liststore[self.index(0)] = (key, value)\n\t\t\tself.index(self.key)\n\t\tif self.index(self.key) == 0:\n\t\t\tself.key = None\n\t\telse:\n\t\t\tself.delete(self.key)\n\n\tdef poplist(self):\t\"\"\"\n\tRemove and return a random list.\t\"\"\"\n\t\tliststore"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tkey = self.random_element\n\t\treturn self.random_element[key]\n\t\n\tdef pop(self, key):\t\n\t\t\"\"\"Remove `key` at `random.randint()` position.\n\t\t\n\t\tIf key is not in a range a new random position is appended, and\n\t\tthe same size will be added at each random iteration.\n\t\t\n\t\tReturn None.\n\t\t\"\"\"\n\t\trng = self.random_element\n\t\tidx = rng.random()\n\t\trandom.seed(self.random_element[key])\n\t\tdata = rng[idx]\n\t\tself.random_element"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif self.item!= None: self.item.popitem(last=True) # if key is given, pop it off\n\t\treturn self.key, self.pop(0)\n\t\n\tdef popitem(self):\t\"\"\"\n\tGet and return the random item from the dict.\n\t\"\"\"\n\t\tif self.item!= None:\n\t\t\tself.item.popitem()\n\t\treturn self.key\n\t\t\n\tdef setdefault(self, key, default=None):\n\t\tif self.has_key(key): self.item[key] = default\n\t\treturn default\n\n\tdef new"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tself._data(self.pos)\n\t\ttry:\n\t\t\tnode = self.node_map[self.key(self.item(self.pos)))\n\t\texcept KeyError:\n\t\t\tnode = None\n\n\t\tself.item(node)\n\t\treturn node\n\n\tdef setpos(self, pos):\n\t\t\"\"\"\n\t\tSet a random ``(key, value)`` pair.\t\n\t\tReturns (node, old_key) tuple.\n\t\t\"\"\"\n\t\ttkey = self.key(pos)\n\t\tif tkey is None:\n\t\t\traise ValueError('Key %s could not be"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif self._len == 0:\n\t\t\treturn None\n\n\t\tif len(self) == 0:\n\t\t\treturn\n\n\t\treturn self._keys[self._len]\n\n\tdef getdata(self):\n\t\t\"\"\"\n\t\tReturn the underlying data structure for a Key.\n\t\t\"\"\"\n\t\treturn self._data\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tYields the items of the Key data structure.\n\t\t\"\"\"\n\t\tyield self._data\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tIterate over the key data structure.\n\t\t\"\"\"\n\t\tfor k in self._"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\titems = []\n\t\twhile i in self:\n\t\t\titem = self[i]\n\t\t\tif item is None:\n\t\t\t\titems.append(key)\n\t\t\telse:\n\t\t\t\titems.append(item)\n\t\treturn items.pop()\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return an item from the stack.\t\"\"\"\n\t\tcur = self[0]\n\t\t(key, value) = cur[2:] # make sure we return None\n\t\tself.insert(len(cur), [key, value])\n\t\treturn self.pop(0)\n\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tkey, val = self.stack.pop()\n\t\tself.stack.append((key, val))\n\t\treturn key, val\n\n\tdef chooseitem(self):\t\"\"\"\n\tChange your choice, by index or by score.\n\t\"\"\"\n\t\tif self.choice == self.NO:\n\t\t\ti = self.pop(0)\n\t\telif self.choice == self.BOTH:\n\t\t\ti = self.pop(self.stack[-1][0])\n\t\telse:\n\t\t\ti = self.take(0)\n\t\tnext = self.stack[-1][2]"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tvalue = self._popitem()\n\t\tif value == self._lastKey:\n\t\t\tself._popitem()\n\t\t\tself._setKey(self.key())\n\t\telse:\n\t\t\tself._setKey(value)\n\n\tdef poprightkey(self):\t\"\"\"\n\tRemove and return a random key pair.  Do not pop anything, nor do it if present,\n\tand return `None`.\n\t\"\"\"\n\t\tif self.key() == None:\n\t\t\treturn None\n\t\telse:\n\t\t\tself._popitem()\n\t\t\treturn self[self.key()][-1"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(\"\\W\")\n\t\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the out-style parameter regular expression.\n\n\t\tReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn re.compile(\"\\W\")\n\n\tdef _create_at_regex(self, m):\n\t\t\"\"\"\n\t\tCreate the call, call or match pattern.\n\n\t\tThis method is only concise when the user has a command or\n\t command with an incorrect pattern.\n\n\t\tReturns the call. This method should be used to"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self._create_simple_re.compile(\"[^\\w$]*{0,20}$\", re.IGNORECASE)\n\t# End static function\n\n\t@staticmethod\n\tdef _create_simple_re_obj(pattern) :\n\t\t\"\"\"\n\t\tCreates a simple object, with basic parameters to match the pattern.\n\n\t\tExample pattern:\n\n\t\t>>> _create_in_regex(pattern)\n\t\tPattern({\n\t\t\t\"A\" : {\n\t\t\t\t\"pattern\" : \"^[A-Z]{2,2}\"\n\t\t\t},\n\t\t\t\"B\" : {\n\t\t\t\t\"pattern\""}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern(re.compile(settings.IN_RE), \"in-style\")\n\n\tdef _create_optional(self) -> Optional[str]:\n\t\t\"\"\"\n\t\tCreate an optional parameter.\n\t\t\"\"\"\n\t\treturn Pattern(re.compile(settings.IN_OPTIONAL), \"optional\")\n\n\tdef _create_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the regular expression.\n\t\t\"\"\"\n\t\tregex = Pattern(\"(?!^#[0-9]{1,3})+(?!^#[0-9]{1,3})(?!^#"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn re.compile(self._parameter_regexp);\n\t#\n\tdef _search_regex(self, in_str: str):\n\t\t\"\"\"\n\t\tReturn regular expression search.\n\n:param str in_str: String defining the type of in-style\n\t\tparameter regular expression to extract from (string).\n\nExample:\n\tdef _search_regex(in_str: str):\n\t\treturn re.compile(str(in_str));\n\tdef _create_in_regex(self):\t\"\"\"\n        Returns string suitable as a regex string, but can be used"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tm = self._match_regular_expression(\".*\")\t\t\t# type: ignore[attr-defined]\n\t\treturn re.compile(m, re.VERBOSE | self._re_flags)\n\t\t\ndef _create_regex_function_match(self, path: str) -> Pattern:\t\"\"\"\n\tCreate a function matching the regular expression of a parameter.\n\n:param path: Path to match\n:param re.Regex|str pattern: Regular expression (regular expression).\n:returns: Pattern that matches the regular expression\n\"\"\"\n\t\treturn _create_regex(self._regex_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tself._patterns = []\n\t\tparam = self._str_to_str(param)\n\t\tstr_to_regex_re = self._to_regex({\n\t\t\tself._is_parameter_reg(param): re.compile(param)\n\t\t})\n\t\treturn str_to_regex_re\n\n\tdef _str_to_str(self, param) -> str: # pylint:disable=W0614\n\t\tif param in self._regex_params:\n\t\t\tparam = self._regex_params[param]\n\t\treturn param\n\t\n\tdef _get_param(self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.__class__(\n\t\t\tre.compile(match_pattern),\n\t\t\tself.parameter_regex(MATCHED_REGEX),\n\t\t\tself._create_regex(),\n\t\t\tself.parameter_regex(IN_PREFIX + regex),\n\t\t\tself._create_regex(in_prefix='$', in_suffix='$'),\n\t\t\tself.parameter_regex(REGEX),\n\t\t\tself.parameter_regex(IN_PREFIX + regex, in_suffix='$'),\n\t\t\tself.parameter_regex(REGEX + REGEX, in_suffix='$$'),\n\t\t)\n\n\tdef _create_regex(self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tm = self.__class__._create_regex(self._in_regex)\n\t\treturn Pattern(m)\n\tdef _create_format_string_type(self, format_type) -> str:\t\"\"\"\n\tCreate the string format parameter string type.\n\nReturns the string format parameter string (``{string format string format syntax}``).\n\nReturns the string format parameter string type (``{string format syntax string type format},\n  ``{string format string format syntax string type syntax format}``) (see also\n  :func:`~pyparsing.Regex.format` for function descriptions of string types)."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn _re_compile(\"\\[\" + _param_in_regex + \"\\]\")\n\t\n\n##~~ Command line parser\n\n@_cmd_opts()\n@functools.total_ordering\nclass CommandLineParser:\n\t\"\"\"\n\tAn application-specific command-line interface.\n\n\tBehaves like a regular expression:\n\tadd --name=value [list of <params>...]\n\tupdate --name=value [list of <params>...] [string]\n\ttest --name=value [string] [test]\n\t\"\"\"\n\t\n\tdef parse(self, name=None, help"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern(\n\t\t\tself.pattern.upper(),\n\t\t\tre.compile(self.re.string),\n\t\t\tflags=re.IGNORECASE\n\t\t)\n\n\nclass LLProcessor(Lexer):\n\t\"\"\"\n\tProcess a chunk of text into multiple lines. The chunking\n\tprocess does *no* filtering, as the process does *not*\n\tfilter the text. The LLProcessor will also return the resulting\n\tresult, so you must add some filtering flags.\n\t\"\"\"\n\n\tclass TokenizeContext(tokenize.TokenizeContext):\n\t\tdef __init__(self, data"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn self._converter\n\n\t##\n\t# Returns the converter of the given argument converter.\n\t#\n\t# @param self The instance of the parameter style converter\n\t# @param converter The function that has been called to convert a parameter\n\t#\tstyle converter, as returned by :meth:`ParameterStyleConverter.convert_arg_converter`.\n\t# @param value The converted value.\n\tdef value_converter(self, converter, value):\n\t\t\"\"\"\n\t\tReturns the parameter style converter.\n\n.. note::\n\tThis function does not actually take a value directly as the parameter"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tfrom pygmin.converter import _converter\n\n\t\treturn _converter()\n#!/usr/bin/env python3\n\nimport requests\nimport json\nfrom pygmin.utils import get_response\nfrom json import dumps\n\n\nclass PicasaHip(object):\n    def __init__(self):\n        self._id = None\n        self._response = None\n        self._headers = None\n        self._headers_dict = None\n        self._token = None\n        self._refresh_token = None\n        self._state = None\n\n    def get_ids(self):\n        self._refresh_token ="}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter()\n\n\nclass _Converter(object):\n\t\"\"\"\n\tThis class implements the conversion process of the DS2DA converter.\n\t\"\"\"\n\n\tdef _call(self, name, *args):\n\t\t\"\"\"\n\t\tCalls appropriate converter.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\nclass DS2DAConverter(_Converter):\n\t\"\"\"\n\tA converter for DS2D parameter definitions.\n\t\n\t:param name: Name of the parameter declaration.\n\t:type name: str\n\t:param flags: Initial parsing flags.\n\t:type flags:"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._converter\n\n\t# ---------\n\t# getters / setter APIs -------------------------------\n\t# ---------\n\nfrom abc import ABCMeta, abstractmethod\nimport json\nimport itertools\nimport requests\nimport sys\nimport traceback\nimport requests.exceptions\nimport logging\nimport os\nimport logging.handlers\nimport traceback\n\nimport yaml\n__metaclass__ = ABCMeta\n\n\nclass Configuration(dict):\n\n    \"\"\"\n    A general configuration type, mainly to hold useful information\n    and methods we'll use to store configuration data, to hold common\n    parameters\n    \"\"\"\n\n    # dictionary keys to class instances that we need"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\ttry:\n\t\t\treturn _converting._Converter(_converting.to_python(self._paramstyle_converter), converter)\n\t\texcept TypeError:\n\t\t\treturn _converting._Converter('dummy', converter)\n\t\t\n\n\nclass _Converter(object):\n\t\"\"\"\n\tThe converter class, used to convert a complex parameterization of a given type into an internal\n\tParameterStyle representation.\n\n\tArguments:\n\t\t\n\t\t* converter : parameter style converter (instance) or converter (string)\n\t\t* paramstyle_converter : parameter style converter (instance)\n\t\t* paramstyle_converter_value"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tconverter = self._converter()\n\t\treturn lambda p: converter.convert(\"parameter\")(p)\n\n\tdef get_default_value(self, _get_value):\t\t\"\"\"\n\tReturns the default value for the format.\n\t\"\"\"\n\t\t_get_value = self._converter().get_default_value\n\t\treturn _get_value(_get_value)\n\n\tdef to_python(self) -> _converting._Converting:\t\t\n\t\tif isinstance(self._converter(), list):\n\t\t\tresult = []\n\t\t\tfor item in self._converter():\n\t\t\t\tif not"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tself._style_converter = _Converting().create(\n\t\t\tself._converter_name, self._converter_name, None)\n\t\treturn self._style_converter\n\t\n\t##~~ get_converter_by_name\n\t@property\n\tdef get_converter_by_name(self) -> _Converting.ParamStyleConverter:\n\t\t\"\"\"\n\tReturns the parameter style converter associated with the converter.\n\t\"\"\"\n\t\treturn self._get_converter(self._converter_name)\n\t\n\tdef _get_converter_by_name(self, converter_name: _Converter."}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tassert not hasattr(self, \"_converter\"), self._converter\n\n\t\treturn self._converter\n\n\tdef _get_variable(self, variable: str) -> _variable:\t\t\n\t\t\"\"\"\n\t\tGet a variable from the model, if it is a variable name.\n\t\t\"\"\"\n\t\tif isinstance(variable, (str, unicode)):\n\t\t\t\n\t\t\tfor variable_entry in self._variable_entries:\n\t\t\t\tvalue = variable_entry.getValue()\n\t\t\t\tif value is not None:\\\n\t\t\t\t\tif isinstance(variable_entry, (list, tuple)):\n\t\t\t\t\t\tvariable = list(variable_entry)\n\t\t\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting.GetConverter(self)\n\n\tconverter = property(_get_converter, _set_converter)\n\n\nclass _Converter:\t\"\"\"\n\tUtility class for converter objects.\n\n\tThe :class:`Converter` object accepts a parameter style converter\n\t and converts it to a :class:`_Converting` object.\n\t\"\"\"\n\tpass\n\n\n############################################################################\n########## MAPPING ##########\n############################################################################\n# The below functions are the public API, which are mostly used in a\n#  python script, but it is also used by other modules:\n############################################################################\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tconverter = self._converter()\n\t\ttry:\n\t\t\t# Set default values from parameters, as `converter` would return an empty\n\t\t\t# dictionary instead of an explicit value in the dict.\n\t\t\tconverter.fill_defaults_from_values(converter.DEFAULTS)\n\n\t\texcept TypeError:\n\t\t\traise TypeError('Unknown parameter type: \"%s\"' % converting._parameter)\n\n\t\treturn converter\n\nclass BooleanConverters(_converting._ConvertingBase):\n\t\"\"\"\n\tA class to create a parameter style parameter style converter.\n\n\t:param _conv: The parameter style converter.\n\t"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\ndef isocam(dt_str):\n    \"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nIn the absence of a `/ or `//`, the standard is `/`. In this case they\nreturn an instance of a :class:`datetime.datetime`. If you want to convert\nthis stream to a ``datetime.datetime`` instance with a `/`\nargument on its second argument, it may raise a :class:`iso8601.ParseError`.\n\nSupported time formats (see ISO-8601 and RFC 1123, and RFC 1973):\n"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\tnow_iso = dt_str.encode('iso-8859-1')\n\t\tiso_parse(now_iso)[0]\t= now_iso[-3:]\n\t\t\n\t\t# If any remaining component of the string can't\n\t\t# be unparseable into an ISO format, we raise a ParseError\n        except ParseError:\n\t\t\t raise ParseError(\"invalid ISO-8859-10 date string\")\n\t\t# It's not possible to always parse the strings that are\n\t\t# not the current ISO format, and thus must include an unparseable\n\t\t# component.\n\nclass TimeOffset(tzinfo):\n    \"\"\""}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\t# TODO: Allow multiple datetime formats with different formatting\n\t#      schemes. Currently, formating and parsing of datetime objects\n\t#       should return a date object with the same tzinfo as the\n\t#       ISO-8601 standard (e.g. `20130411T13:02:38+00:00`) and if\n\t#       possible to read the timezone is ignored\n\n\treturn dt_str.parse(dt_str.utcoffset())\n\ndef isoparsedate(dt_str):\t\"\"\"\n\tParse an ISO-8601 string into :class:`datetime.date`.\n"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\tif isinstance(dt_str, (str, binary_type)):\n\t\t\treturn _parsedate_tzstr(dt_str).timetuple().tm_isdst\n\treturn None\n\ndef _parsedate_tzstr(dts):\n\t\"\"\"Utility to parse a datetime-like date string (it may raise :exc:`ValueError`\n\tif the given string is invalid).\n\n\tThis allows parsing an ISO-8601 date into a timezone offset\n\tlike\n\t:func:`datetime.timedelta` objects.\n\n\t:param dts: The date/time string to be parsed, in the"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\ttry:\n\t\t\treturn datetime.datetimeparse(dt_str)\n\t\texcept ValueError:\n\t\t\t# This means that the user has specified invalid \"datetime\" as\n\t\t\t# parameter, so throw a validation error.\n\t\t\traise ParseError(\"Not a valid ISO-8601 date.\")\n\n\nif __name__ == '__main__':\n\t\"\"\"Main function to parse a given ISO-8601 date string into a python\ndatetime.datetime object::\n\n\t\"\"\"\n\timport unittest\n\tfrom datetime import datetime\n\timport locale\n\n\tdef format_date(datestr):\n\t\t\"\"\"Convert a single ISO-8601"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t        return tzoffset.parse(dt_str, tz=tz)\n\nclass dateutil_timezone(tzutc):\n\t\"\"\"\n\tA subclass of ``tzutc`` that implements the Python tzinfo API.\n\n\n\tThis class is to be used with UTC-aware time zones.\n\tThe ``timezone`` attribute of the ``tzutc`` object is set,\n\tand the object is used for the ``tz`` attribute, if the\n\t``tz`` attribute is not present in ``tzutc``.\n\n\t.. warning::\n\n\t\tUse with pytz internally.\n\n>>> from pytz import timezone\n>>>"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\traise UnparserException('invalid ISO-8601 format')\n\n#----------------------------\n\ndef parse_iso8601(dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string to a :class:`datetime.datetime`.\n\n\tThis function formats the string to parse correctly, and returns\n\ta :class:`datetime.datetime` object as well.\n\t\"\"\"\n\t\t# this is an experimental implementation of the `strptime` package\n\t\t# which treats as one-digit years long time.\n\t\t# we parse a RFC-compliant string from RFC 1123 to GMT.\n\t\t# this"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n#   Universal time format\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n\n\n#...............................................................................#\n#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n#\n#   This implementation is mainly to provide a convenient parser for \n#    daylight data and time formats and to the parser.\n#    Most of the time formats are accepted.\n#    See http://www.w3.org/TR/REC-html22-time-parsing-and-timezone-strings.html\n#    for info on other formats.\n#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\tfrom datetime import datetime, timezone\n\t\treturn cls._parse_datestr(dt_str, dateutil.tz.tzutc, 'iso8601', ignore_tz_errors)\n\nif sys.version_info[0] >= 2:\n\tdef _is_null(value):\n\t\tif isinstance(value, str) or isinstance(value, unicode):\n\t\t\treturn value is None\n\t\treturn value == '(null)' or value\n\n\tdef _is_datetimeobject(value):\n\t\tif isinstance(value, dateutil.tz.tzlocal):\n\t\t\treturn 'DATE' in"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t\treturn _parse_iso8601(dt_str)\n\n\n\ndef _parse_iso8601(dt_string):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\tThe default value is the ISO-8601 standard, which the library uses.\n\tThe :meth:`parse` method is not supported directly by all the\n\tcomparisions for the ISO time format and is provided mostly in\n\tcases where the time parts are missing or not handled.\n\n\tSome commonly supported formats are:\n\n\t- ``YYYY-MM-DD`` or ``YYYY"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n# XXX: Support for this and use that instead of using a \"datetime.datetime\"\n# class, not a class to support the exact format string format.\n\n# XXX: Support for now that this method can be used *after* the\n# parsing of time.strptime() or time.strftime() methods.\n\n# XXX: Support for this and use the parse function directly. A faster\n# algorithm could be implemented with the parse function, although they're\n# not supported. Do not use it directly.\n\n# XXX: Support for this and use the parse function directly. A faster\n#"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n__version__ = \"1.0.0\"\n__author__ = \"Bernard Warnner (@warnner15)\"\n\nimport time\nimport datetime\n#if 'from.parsers import *' in sys.modules.keys():\n#    __version__= \"0.1.0\"\n#else:\n#    __version__ = \"1.0.2\"\n\ndef date2num(date_string, format=\"%Y-%m\"):\n    \"\"\"\n    Utility function to parse a date/time string into a numeric integer\n    representation\n\n    :param string date_string"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\nclass TimezonesParser(datetime.tzinfo):\n    \"\"\"\n    Timezones are one or more time zones for dates parsed from a\n    :class:`datetime.date` object, including hours, minutes and\n    hours/days.\n\n    This class takes either one or two strings or a single\n    :class:`datetime.datetime` object. In order to create a subclass of\n    :class:`time.tzinfo`, it will be necessary to specify an encoding\n    (e.g., for ``latin1`` and ``utf8`` for `tzfile`), or provide\n    an :attr:`tzinfo` attribute for ``tzinfo`` if it"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n\t\n\t# Validate and optionally convert the given string.\n\t\t\n\ttry:\n    \t\tdate_string = timestr.split(':',1)[0]\n    \texcept IndexError:\n    \t\traise ParserError('no format specified')\n \t\t\n    tzinfo = kwargs.get('tzinfo')\n \t_checktype(tzinfo, datetime.datetime)\n    _checkint(timestr,'time')\n    _checkint(timestr,'fuzzy_with_tokens')\n    _checkfuzzy(date_string, 'date')\n\tstring = text_type(date_string)\n    return date_type(string)\n \n\t#"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n#\n# (C) Copyright 2009 Roman Yakovenko\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\nimport socket\n\ntry:\n\tfrom locale import *\nexcept ImportError:\n\tpass\nelse:\n\tif not isinstance(tzinfos, basestring):\n\t\traise TypeError(\"The C locale type should be string or None\")\n\t\n\ttry:\n\t\tfrom dateutil.parser import parse\n\t\tif not isinstance(tzinfos, basestring):\n\t\t\traise TypeError(\"The C locale type should be string or None\")\n\t\t\n\t\tif ((not isinstance(tzinfos, (tuple, list)) and \n\t\t  len(tzinfos) == 1)):\n\t\t\tfrom dateutil.parser import parse\n\t\t\ttzinfo = tuple([int"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\traise TypeError(repr(timestr) + \" requires an \" + repr(default).decode('utf-8'))\n\t\t#raise OverflowError(\"not enough C numbers\")\n\ndef translate(value):\n\t\"\"\"\n\tTranslate the given value into a string using Python's\n    `unicode` function.  Any string value passed to an ``translate()``\n    function will be left intact instead of being translated to the\n    local locale before a ``LocaleError`` is raised.  The return value\n    will be a tuple of localized locale names, or the original string if a\n    ``LocaleError`` is raised, containing"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n\n\nclass tzinfo(datetime.tzinfo):\n    \"\"\"The UTC clock.\"\"\"\n\n    def __init__(self, offset=None):\n        \"\"\"\n        Constructor. The offset is ignored if timezone is ``utc`` not given.\n\n        \"\"\"\n        super(tzinfo, self).__init__(offset)\n        self.utc = False\n\n    def utcoffset(self, dt):\n        \"\"\"\n        Return the datetime object from UTC.\n\n        :param dt:\n            Date and time (as string, format string, to set the offset,\n            which is ignored if timezone is ``utc``).\n\n        :return:\n            Local time.  None if not timezone is given"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t        \nclass TimeZone(tzinfo):\n\t__slots__ = ['zone']\n\t\n\t__doc__ = \"\"\n\t\n\tdef __init__(self, name=None, zone=None):\n\t\t\"\"\"Create a new time zone.\n\n\t\tNote that the default value is ``None``.  Because UTC\n\t\ttimezones and timezones do not have timezones,\n\t\tthe default value is ``None``.\n\n\t\t:param name:\n\t\tThe name of the time zone (an optional C string, no\n\t\textension)\n\t\t:type name: ``unicode``\n\t\t:param zone:\n\t\tAnother"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t    \n    def __new__(cls, string, **kwargs):\n        if isinstance(string, unicode):\n            pass\n        elif isinstance(string, str):\n            return _make_parser(string, **kwargs)\n        elif isinstance(string, unicode):\n            return string[:string.index(string) - 1]\n        else:\n            raise TypeError(\n                \"Unable to convert '%s' to a recognized string\" %\n                type(string).__name__)\n        \ndef _make_parser(string, **kwargs):\n    \"\"\" Creates a :class:`type` parser.\n\n    Any arguments listed in the\n    string can be either a :"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass _UnambiguousTimezone(tzinfo):\n\t\"\"\"\n\tAn ambiguous :class:`tzinfo`_ suitable for the timezone-aware\n\tdatetime-object.\t\"\"\"\n\t# We don't care about the ambiguous representation (if it's a UTC\n\t# naive datetime) since that's the first occurrence of a tzinfo.\n\n\t# XXX: Is this necessary? (or a re-implementing\n\t# tzinfo-style class-decoration)? Anyway, the naive object itself is only\n\t#  a subclass, that doesn't need to be a class.  We don't ever make\n\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\tdef __init__(self, t):\n\t\t\"\"\"\n\t\tInitialize a new (non-ambiguous) datetime object representing the\n\t\tfollowing time, i.e the time this object is in, expressed as\n\t\t`t`.\t\n\t\"\"\"\t\n\t\tself.t = t\n\t\tself.time = t.replace(tzinfo=None)\n\t\t\n\t\t# If this is a fold, we add information about time between\n\t\t# two instances.\n\t\tif t.fold:\n\t\t\tself.time += t.time\n\t\t\n\t\t# The number of microseconds since the year, for\n\t\t# example"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n# A subclass, derived from datetime\n\nclass Date(tzinfo):\n    \"\"\"\n    A tzinfo that supports the following properties which you can provide:\n        - 'tzname': Returns the current timezone in a given time zone, or\n        - 'tzoffset': Returns the offset in time in tz-aware time zone, or zero if the\n        current user's timezone doesn't support it.\n    \"\"\"\n    \n    def __init__(self, tzstring='UTC'):\n        \"\"\"\n        Create an instance.\n        \n        :param str tzstring: String used for the time zone in the\n            string representation of UTC.\n        :return: None\n        \"\"\"\n       "}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"Not a timezone aware datetime.\")\n\n\t\tst=dt.utcoffset()\n\t\tif dt.tzinfo is None:\n\t\t\t# Can't do an \"unambiguous\" adjustment on a datetime, in this case that's the first occurance\n\t\t\tif st:\n\t\t\t\tdt=dt.replace(tzinfo=st)\n\t\t\telse:\n\t\t\t\tdt=dt.replace(tzinfo=None )\n\t\telse:\n\t\t\tst-_=dt.utcoffset()\n\t\t\tif dt.tzinfo is not None:\n\t\t\t\tdt=dt.astimezone(st)\n\n\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ttry:\n\t\t\t# If it's a naive datetime, see if we have a valid tzinfo.\n\t\t\t# If the zone is valid, the tzinfo we'll use to look up will be\n\t\t\t# in, and then just use that tzinfo, then return.\n\t\t\tfrom calendar import tzinfo as tzdt\n\t\t\tdt = dt.tzinfo\n\t\texcept (AttributeError, TypeError, ValueError, OverflowError):\n\t\t\t# The naive datetime had invalid tzinfo details.\n\t\t\treturn None\n\t\tif not hasattr(dt, 'tzinfo'):\n\t\t\twarn(\"Could not determine timezone of the '%s' \" % dt)\n\t\t\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ndef fromlocal(tz):\n\t\"\"\"\n\tGiven a timezone-aware datetime in the timezone, returns a timezone-aware\n\tdatetime in the specified timezone.\n\n:param tz: A :class:`timezone.timezone` object.\t\"\"\"\n\tif not tz.tzinfo:\n\t\ttz = timezone(tz)\n\treturn tz.convert(dt)\n\nclass time(tz):\n\t\"\"\"\n\tA Timezone.\n\tWraps timestamps in a very flexible-box, single character timestamp.\n\t\"\"\"\n\t\n\tdef __str__(self):\n\t\treturn''.join(["}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\tif not isinstance(dt, datetime):\n\t\traise TypeError, \"Expected a naive datetime in %r (tzinfo) for %r.\" % (dt, self.__class__.__name__)\n\t\t\n\tif not self.is_mixed():\n\t\treturn self.__class__(\n\t\tdt.replace(tzinfo=self.tz).replace(tzinfo=None),\n\t\tpytz_aware=dt.astimezone(self.tz),\n\t\tutc=dt.replace(tzinfo=None),\n\t\tfold=dt in self.fold,\n\t\tfold_by_ref=dt in self.fold_by_ref"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tif dt is None:\n\t\treturn None\n\n\tend = dt + relativedelta(minutes = 1)\n\tstart = start + relativedelta(minutes = 1)\n\tassert self._foldable_time(end)\n\t\n\t# Add the folding rule if applicable\n\tif end!= self.next_week_start:\n\t\tend += relativedelta(months=1)\n\tnewnext = self.next_week_end + end\n\tstart += newnext\n\tassert self._foldable_time(start) is None\n\treturn self.next_week_start + relativedelta(minutes"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nfrom itertools import izip\n\n\nclass T(tzinfo):\n\t\"\"\"\n\tA tzinfo class which can be used by the :class:`datetime.tzinfo`\n\tclasses.\n\n\t:param offset: The timezone-local offset in seconds.\n\t:type offset: :class:`timedelta`\n\t:param tzinfo: A :class:`datetime.tzinfo` subclass, which will\n\t\tbe used. If ``None``, this is the default offset.\n\t:type tzinfo: :class:`tzinfo`\n\n\t\"\"\"\n\t# NOTE: We keep the `utcoffset` class because it's passed a *reference"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ndef toutc(dt): #XXX: Use to_timezone\n\t\"\"\"\n\tGiven a tz-aware datetime in a given timezone, calculates a\n\tUTC time (UTC) in the timezone.\n\t\"\"\"\n\ttzname = to_timezone(dt.tzinfo)\n\ttz = pytz.timezone(tzname)\n\treturn (dt.hour * 24 + dt.minute * 60 + dt.second  +  # XXX: what to do about this?\n\ttz)\n\n\n#\n#  Misc\n#\n\n\nclass Suffix(str):\n\tpass\n\n"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    def parse(self, dt=None):\n        if dt is None:\n            dt = datetime.datetime.now()\n        if self._tzinfo is None:\n            if dt.tzinfo is None:\n                return None\t\t\t\n        if (dt.tzinfo is None or self._tzinfo == dt.tzinfo):\n            # use the default timezone\n            # unless the user explicitly set with the --with-tz option\n            # then return now\n            return dt.replace(tzinfo=self._tzinfo)\n\t\n    def parse_string(self, string):\n        string = string.replace(\"+\", \"%2A\")\t\t\n        string = string."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    return dt(*tzinfo())\n\n\ntimezone = default_tzinfo\nfrom distutils.core import setup\nfrom os import path\nimport logging\nimport sys\n\ndef setup_path(top_path):   \n    logging.debug(\"running setup path: {}\".format(top_path))\n    def create_dirs_if_not_exists(top_path):                \n        dir_contents = list()\n        try:\n            os.mkdir(top_path)\n        except OSError as exc:\n            # Can't create directories if it already exists.\n            if exc.errno!= errno.EEXIST:\n                raise\n        dir_"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    def replace_tzinfo(dt, tzinfo):\t\"\"\"\n\tReplaces naive ``.tzinfo`` for ``dt`` if necessary. Returns\n\tan aware :py:class:`datetime.datetime` instance for ``dt`` if there\n\tis one, and the tzinfo otherwise.\n\n\tThe naive result will be stored using the ``tzinfo``\n\targument. The naive result must be naive\n\t(see the `tzinfo` parameter below) as it follows the exact same\n\tas the naive result in that example:\n\n        >>> print(dt.replace(tzinfo=timezone.utc))\n        2014-01-01"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\ttry:\n\t\t\treturn dt.replace(tzinfo=tzinfo).astimezone(tzinfo).replace(tzinfo=None)\n\t\texcept pytz.UnknownTimeZoneError:\n\t\t\tpass\n\t\t\t\nfrom __future__ import division, print_function\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\nfrom sklearn.decomposition import PCA\nimport cPickle as pickle\nimport scipy.sparse.linalg as splu\nimport scipy.sparse.linalg.dsolve as dsolve\n\n\ndef build_dataset():\n    y_i, x_j = np.loadtxt('y_t_i'),"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\treturn default_tzinfo(dt, tzinfo)\n\n\nutc = UTC\n\n_tzinfo_registry = {}\n_default_tzinfo = object()\n\n# We are using an absolute date since Python's strftime.  So, the _default_tzinfo\n# is used for easing times.\n_default_tzinfo_tzinfo = ut.tzinfo(LOCALTZ,\n                                standard_name=None,\n                                days=-1)\n\n# Python's strftime gives strange behavior when called from within the Python's\n# strftime function or when called from within python.\n_default_date_format = \"%a,"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\tif isinstance(dt, datetime):\n\t\t\tdt = dt.replace(tzinfo=default_tzinfo(dt, tzoffset(0,0), dflt_tz) + tzinfo)\n\t\treturn dt\n\n# ===================================================\n# Conversion functions\n# ===================================================\ndef parse(dt, tzinfo=tzoffset(None, None)):\n    \"\"\"\n    Parse a datetime object into a Python object (``datetime.datetime``, ``time.localtime,\n    time.time``, ``datetime.time..``, etc.). Return a\n    :py:class:`datetime.datetime`. The parsed datetime will have a timezone\n    set which"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    dt_ = dt - dt.utcoffset(dt) if dt else dt\n    return dt_\n\n# ------------- END OF API --------------\nfrom setuptools import setup, find_packages\nfrom redistill import __version__\n\n\nrequires = [\n    'pyramid>=0.7',\n    'pyramid_chatter',\n    'pyramid_chatter.api',\n    'pyramid_chatter.interfaces',\n    'pyramid_chatter.rest_api',\n    'pyramid_chatter.wsgi_api'\n]\n\ntests_require = [\n   'mock',"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\t# UTC doesn't have the standard implementation\n\t\t# of the standard utc implementation, so we can just do that\n\t\t# ourselves as our default :py:class:`datetime.tzinfo` value\n\t\treturn dt.replace(tzinfo=None)\n\ndef _utc_timestamp():\t\"\"\"\n\tConverts a naive timestamp time tuple to seconds since the epoch.\n\t\"\"\"\n\tsecs = getattr(datetime, '%s_secs' % (time.time()))\n\tif secs is None:\n\t\tif dt is None:\t\n\t\t\treturn 0\n\t\treturn secs/1000000.00\n\treturn"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\ndef default_datetime_now(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetime only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_datetime_now\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_datetime_now(parse('2014-01-01"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    dt = dt.replace(tzinfo=tzinfo)\n    if tzinfo:\t\n        dt = dt.astimezone(tzinfo)\n    return dt\n\n#\n# Timezone functionality\n#\n\ndef time_zone(dt):\n    \"\"\"\n\tReturns the tzinfo instance for the given time zone.\n\t\"\"\"\n\n    return default_tzinfo(dt, pytz.utc)\n\ndef timezone(dt, time_zone):\t\n    \"\"\"\n\tReturns the tzinfo instance for the given timezone.\n\t\"\"\"\n\n    return timezone(dt, time_zone)\n\ndef timezone_info(dt, time_zone):\n   "}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\ndef cut_strings(before, after):\n\t\"\"\"\n\tCutter the strings after the given string.\nReturns:\n    The string after the after cut and optionally the old str.\n\t\"\"\"\n\tbefore = before[0].replace(r'\\\\',' ',1)\n\tafter = after[1].replace(b'\\\\',' ',1)\n\tbefore = before.replace(b'\\\\',' ',1)\n\tafter = after.replace(b'\\\\',' ',1)\n\twith open(b'in.csv','a') as csv_in:\n\t\tfor text,pos in"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\nif __name__ == \"__main__\":\n    main()import re\n\nfrom coalib.bearlib import deprecate\nfrom coalib.misc.Formating import format_repr, set_best_metavar\n\n__doc__ = \"\"\"\n``deconstructible`` takes a list of tuples as its first two elements: the first element of\nthe tuple is the first value for the second. The second element is the second in-place\ntuple, and the first value in the tuple has the same length as the one given in the latter.\n\nThe first tuple has the same length as the second"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\ndef set_line(infile: bytes, afterfilename: bytes) -> None:\t\"\"\"\n\tUse readline() to reset the current line and file pointer.\n\nArgs:\n    infile: the input file to read the lines from.\n    afterfilename: the filename of the file with the new content.\n\nRaises:\n    ValueError: if there's nothing before the given file pointer.\n\"\"\"\ndef remove(infile: bytes, delimchar: bytes, offset: bytes) -> int:\t\"\"\"\n\tRemove the specified byte from a certain offset.\n\t'''\n\tout = infile[offset:offset"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# split file before before after after delimiters\n\t\tif before == after:\n\t\t\treturn\n\t\t# split file after after\n\t\telif not self._data:\n\t\t\treturn\n\t\t# check file types\n\t\telif after.startswith('#'):\n\t\t\tfileType = 'unknown_opens'\n\t\t\tif before == \"\":\n\t\t\t\tself._data = self._cut_delim(after.lstrip('#'))\t# discard first line\n\t\t\telse:\t\t\t\t\t# replace the first 'opens'\n\t\t\t\tself._data = self._cut_delim(before.rstrip('#'))\t# discard first line\n\t\t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n##\tdef split_and_split_path(self, path: str, strip_root_from=False,\n##\t\t\t\t ignore_errors=False)\t\t\t\t\t# pragma: no cover\n##\t\t\"\"\"\n\tSplit paths starting with these delimiters and performing\n\t\tthe following operations:\n\t\t1. Use this function to perform an import.\n\t\t2. Do a 'find' operation. Will throw exceptions if anything\n\t\t\tfound. If ignore_errors=True, the entire path will be dropped.\n\t\t3. If the end of the path is invalid, the remaining characters will be"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\nclass CutOff(object):\n\t\"\"\"\n\tRemoves the cut off.\n\t\"\"\"\n\tdef __init__(self, filename: str) -> None:\n\t\tself.filename = filename\n\ndef remove_cut_off(\n    file_path: str, filename: str\n) -> None:\t\t\n\t\"\"\"\n\tRemoves the cut off from a file, by replacing\n\tthe path of the file with the full path given in the\n\tcurrent directory.\n\t\nArgs:\n    file_path: Filename to read a cut off from.\n\tfile_path: Path that contains"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\t\n\tbefore = before.replace(b';/', b'\\\\')\t# type: ignore\n\tafter = after.replace(b';/', b'\\\\')\t# type: ignore\n\tbefore = before.replace(b'\\n', b'\\n')\t\t\t# type: ignore\n\tafter = after.replace(b'\\r', b'\\r')\t\t# type: ignore\n\t\n\tif (str(len(before)) - len(after)) & 0xFF == 0:\n\t\tself.b_set_cut_chars()\n\telif (str(len(before)) & 0xFF"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\ndef make_cut_chars(before: bytes, after: bytes) -> None:\n\n\tdef process_cut_char(m: list, p: int) -> None:\n\t\tif p == 0:\n\t\t\treturn after\n\t\tif p == 1:\n\t\t\treturn before\n\t\tif p < len(before):\n\t\t\treturn process_cut_char(t, p)\n\n\twith open(os.path.join(self.path, 'cut.seq'), 'r', encoding='utf-8') as f:\n\t\tseq = [s.rstrip('\\r\\n')"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n\t\t\n\tif before!= after:\n\t\tlogger.verbose(\"Split before and after arguments:\")\n\t\tlogger.debug(\"Before: {}\".format(before))\n\t\tlogger.debug(\"After: {}\".format(after)) \n\t\traise RevertError(_(\"Cannot delimit/split in file after {0}\").format(after))\n\t\n\t\n\n\tif self.cut_chars!= None:\n\t\tdel self.cut_chars\n\t\t\n\tfor char in self.cuts:\n\t\tif char.startswith(\"-\"):\n\t\t\tcut_char = char[1:]\n\t\t\tcut_char = cut_char.strip()\n\t\t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\ndef get_filename_at_end(filename, file_length):\n\t\"\"\"\n\tReturn the absolute path of the file (path + file_length + 1)\n\t\"\"\"\n\treturn os.path.join('.', os.path.join(getcwd(), filename+file_length))\n\ndef get_end_of_filename(filename, file_length):\t\"\"\"\n\tReturn the number of file_length characters after filename or None if the length\n\tof the returned string is less than file_length.\n\t\"\"\"\n\treturn os.path.splitext(get_filename"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif request.method == 'POST':\n\t\t\tresponse_payload = request.data\n\t\telif request.method == 'HEAD':\n\t\t\tresponse_payload = request.data\n\n\t\tvalid_request_methods = ['GET', 'POST', 'DELETE', 'PUT', 'PATCH']\n\n\t\tif response_payload not in valid_request_methods:\n\t\t\traise NotEnoughMorrientRequest(request)\n\n\t\treturn response_payload\n\n\ndef parse_data_from_response(data: Text = None) -> Optional[DataFrame]:\n\t\"\"\"\n\tParse the data. Returns a DataFrame"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tcheck_response(request)\n\t\tif (request.is_legacy() and not request.is_user_private(user=user)):\n\t\t\treturn None, None, -1\n\n\t\tif is_user_active_call(user, request) and request.call_mode == UserCallModes.USER_REGISTER and len(request.call_params) > 0:\n\t\t\tif user in request.call_params:\n\t\t\t\treturn is_user_active, None, request.call_params[user]\n\t\t\treturn False, None, None\n\t\tif request.call_mode =="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tprivate_message = None\n\t\tif request.typeStr == \"request\":\n\t\t\tprivate_message = request.payload\n\t\t\tif private_message is None:\n\t\t\t\treturn None\n\n\t\tif private_message[0]!= \"_\":\n\t\t\treturn False\n\n\t\t# Handle requests with no public private message.\n\t\t# The request type cannot be any of these.\n\t\tif request.typeStr == \"request_empty\":\n\t\t\treturn False\n\n\t\tif request.typeStr == \"request_signed\":\n\t\t\tpublic_payload = self.signer.sign_jwt(private_message)\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif not request.content:\n\t\t\treturn False\n\n\t\trequest = make_request_and_check_permissions(request, DiasporaPermissionStatus.PRIVATE)\n\t\tif request.content:\n\t\t\tmsg = {'code': 2,'message': \"private request\", \"reason\": \"private message\"}\n\t\t\treturn msg\n\t\treturn False\n\n\tdef check_is_spam(self, content: str):\n\t\t\"\"\"\n\t\tReturn true if the message is actually spam, false otherwise.\n\t\t\"\"\"\n\t\tif len(content) < 2:\n\t\t\treturn False\n\t\tif len("}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\treturn request.requestType!= requestType.request\n\n\tdef testRequest(self, session: Session):\n\t\tprint('Test', 'identify_request:'+ session.getGlobalMessage(),'session', session)\n\t\tpayload: Dict[str, Union[bool, bytes, types.TracebackType]]\n\t\trequest = session.get(\n\t\t\trequestType.id,\n\t\t\tdata=payload,\n\t\t\tsession=session)\n\t\tpayload = json.loads(request.body, object_hook=json_decoder)\n\n\t\tself.assertDictEqual(\n\t\t\tpayload,\n\t\t\trequest.headers"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif not request.type:\n\t\t\t# Request is a Diaspora request, check if it satisfactory.\n\t\t\treturn False, \"Invalid signature\"\n\t\t\t\n\t\treturn True, None\n\n\tdef resolve_msg(self, context: Dict[str, Any]) -> list:\n\t\tglobal_headers = self.parse_headers(context[\"general\"], context[\"headers\"])\n\t\tglobal_body = self.parse_body(context[\"body\"], context[\"request\"])\n\t\tglobal_headers[\"X-Diaspora-Request-Header\"] = global_headers[\"X-Diaspor"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\ttry:\n\t\t\tpayload = request['_messages'][request['_id']]['response']['publicBody']['html']['message']\n\t\t\tif payload['reply']['kind'] == \"text\":\n\t\t\t\tif request['_id'] == \"payload.asl\":\n\t\t\t\t\treturn request['id']\n\t\t\t\tfor a in request['_messages']['publicMessage']['links']['self'].keys():\n\t\t\t\t\tif request['_messages']['publicMessage']['links']['self'][a] == payload['reply']['id']:\n\t\t\t\t\t\trequest['id'] = a\n\t\t\telse:\n\t\t\t\trequest['id'] = payload['reply']['id']\n\t\texcept"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\treq = DiasporaRequest()\n\t\tif not req.public:\n\t\t\treturn req.is_anonymous()\n\t\treturn False\n\ndef get_diaspora_public_message(request: RequestType):\n\tdiaspora_request = get_diaspora_request(request)\n\tdiaspora_public_message = diaspora_request.create_message()\n\t# add header to diaspora_message.json\n\tpayload = diaspora_request.serialize()\n\tdiaspora_public_message"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\treturn request_data.type == ClientMessage.private and \\\n\t\t\t\t(request.signature == request_data.signature)\n\n\ndef decode_request(data):\n\n\t\"\"\"\n\tDecodes message from message. If a Diaspora request has multiple\n\tmessages, the first one with `None` will be sent. If no payload in the\n\tmessage could be processed, return the actual request.\n\n\tReturn only single request but response is not processed.\n\t\"\"\"\n\tmessages = data.get(\"messages\", [])\n\tfor message in messages:\n\t\t# ignore the payload if"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tassert (\n\t\t\t# Only allow for legacy request data if an existing request has an existing\n\t\t\t# payload.\n\t\t\t(request.get_headers().get_all(\"content-location\") == request.get_path()).all(False)\n\t\t\tand response.is_status_code() == 303\n\t\t\tand request.is_ajax()\n\t\t)\n\n\t\tif response.status_code() == 200:\n\t\t\treturn parse_response(response)\n\n\t\treturn response.status\n\n\tdef check_token(self, token_type: TokenType):\n\t\t\"\"\"\n\t\tTry to check"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request == RequestType.MatrixReq\n\n\tdef get_identity(self):\n\t\treturn self._matrix_identity\n\n\tdef get_status(self, state: State):\n\t\t\"\"\"\n\t\tReturn a Status object for the specified client state and its associated\n\t\tnumber of transmission\n\t\t\"\"\"\n\t\tstatus = status_map.get(state.state_number(), None)\n\t\tif status is None:\n\t\t\tif state.is_transmitted_in_terminal():\n\t\t\t\traise ValueError(\"State {} is not transmitted in terminal.\".format(state.state_number()))\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\ttry: \n\t\t\tidentifier = request.identifier.lower()\n\t\t\tif identifier in self.request_list:\n\t\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\n\tdef generate_random_name() -> str:\n\t\tif not seed:\n\t\t\treturn None\n\t\n\t\tfrom random import shuffle\n\n\t\tif seed == 0:\n\t\t\treturn'matrix'\n\t\telif seed == 1:\n\t\t\tshuffle(list(settings.matrix_seeds))\n\t\t\treturn'matrix'\n\t\telse:\n\t\t\tseed = int(seed)\n\t\t\n\t\treturn seed\n\n\tdef generate_random_color()"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.match_dictionary()[\"value\"] > 0\n\n# Returns the name of the user who made the request by id\ndef user_id_with_name(user_id_to_name: Dict[Text, Text]) -> Text:\n\t\"\"\"\n\tConverts an identifier string to a user's name\n\tArgs:\n\t\tuser_id_to_name -- mapping key string to user name\n\tReturns:\n\t\tuser_id_to_name -- mapping key string to user name\n\t\"\"\"\n\ttry:\n\t\treturn user_id_to_name[user_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif(request.form[\"status\"] == \"success\"):\n\t\t\treturn True\n\t\telif(request.form[\"status\"] == \"error\"):\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\nclass Massus:\n\tdef __init__(self, data: Tuple[str, str]):\n\t\tself.is_locked = False\n\t\tself.uuid = uuid.uuid4()\n\t\tself.json_data = json.dumps(data)\n\t\tself.type_and_status = json.loads(self.json_data)\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif not (request.request_type.find('/'+request.request_type[0:-4].upper())==len(request.request)): return False;\n\t\t\n\t\ttry:\n\t\t\tif request.request_type[-4:]=='?query?':\n\t\t\t\tquery = request.args.split('?',1)[0].split('?',1)[0]\n\t\t\telse:\n\t\t\t\tquery = request.request_type[-4:]\n\t\t\t\n\t\t\ttry:\n\t\t\t\tresponse = json.loads(query)\n\t\t\t\tif response.get('type', 'no data') == 'no query': return False\n\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn True\n\n\tdef check_request_for_matrix(self, matrix_id: str, request: RequestType) -> bool:\n\t\t\"\"\"\n\tCheck if the currently displayed matrix should be presented as a message.\n\t(Currently only for a Matrix request (since MatrixRequest objects have all of the\n\trequired type parameters included.\n\t\"\"\"\n\t\treturn False\n\n\tdef check_matrix_name_and_input_params(self, matrix: MatrixRequest) -> None:\n\t\t\"\"\"\n\tIdentify a particular matrix\n\t\tThis will just return the matrix if successfully identified\n\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.type == MatrixTaskRequestRequestType.NOT_IMPLEMENTED:\n\t\t\treturn False\n\n\t\telif request.type is MatrixTaskRequestType.PROCESSING:\n\t\t\t# print(\"Identifying \" + request.type)\n\t\t\treturn False\n\n\t\treturn True\n\n\tdef get_request_description(self, request: RequestType) -> str:\n\t\t\"\"\"\n\t\tRetrieves the user's description.\n\t\t\"\"\"\n\t\tif request.type == MatrixTaskRequestType.NOT_IMPLEMENTED:\n\t\t\treturn ''\n\n\t\treturn 'Matrix Task request type {} could not be identified.'."}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif bool(request.has_type()):\t\n\t\t\treturn request.type() == MatrixRequest.MATRIXREQUEST\t\t\n\t\treturn False\n\n\tdef get_matrix_set(self, request: MatrixSet) -> bool:\n\t\t\"\"\"\n\t\tReturns all matrix sets in this matrix set for this input. Returns false\n\t\twhen a matrix request's request is missing (i.e. does not exist or is disabled).\n\t\t\"\"\"\n\t\tfor matrixset in request.matrix_sets() :\n\t\t\tif matrixset in self._matrix_sets[request]:\n\t\t\t\treturn True\n\t\treturn False\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif isinstance(request, Request) or isinstance(request, Response):\n\t\t\treturn True\n\n\t\treturn False\n\n\tdef verify_request(self, request: RequestType) -> bool:\n\t\tif request.is_matrix_request():\n\t\t\tresponse = request.get_matrix_request()\n\n\t\t\tif response:\n\t\t\t\tself.logger.debug(\"Request from the backend is {0}\".format(response))\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef get_request_by_id(self, request_id: str) -> Request:\n\t\t\"\"\"\n\t\tTry to parse the request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif not isinstance(request, MatrixRequest):\n\t\t\traise TypeError('MatrixRequest expected type is MatrixRequest')\n\t\tif request.type!= MatrixRequestType.matrix:\n\t\t\traise TypeError('Request type mismatch: %s' % request.type)\n\t\tif request.name.lower() == 'Matrix' and request.type!= MatrixRequestType.matrix:\n\t\t\traise TypeError('Request name mismatch: %s' % request.name)\n\t\treturn True\n\treturn False\n\ndef decode_request(request: str) -> JsonResponse:\n\t\"\"\"\n\tTry to decode the matrix\n"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt.days==0:\n\t\t\treturn str(dt.strftime(\"%A\"))\n\t\tif dt.hours==0:\n\t\t\treturn str(dt.strftime(\"%w\"))\n\t\tif dt.days==1 and dt.hours==0:\n\t\t\treturn str(dt.strftime(\"%1w\"))\n\t\tif dt.seconds==0 and dt.days==0:\n\t\t\treturn str(dt.strftime(\"%H\"))\n\t\tif dt.seconds>=60*60*24:\n\t\t\treturn str(dt.strftime(\"%S\"))\n\t\tif dt.seconds>=60*60*24"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_graph(graph_name, path = None):\n\t\"\"\"\n\tGets an existing graph from the supplied graph_name.\n\tIf path is provided, it searches for the name of the graph, and returns it.\n\tIf no graph is found, returns None.\t\"\"\"\n\t# If no graph can be found, just make one for now\n\tif graph_name is None:\n\t\tgraph_name = get_session().graph_name\n\tgraph = None\n\t# Get"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt >= D:\n\t\t\t# it's within the same day\n\t\t\treturn \"0:00:00\"\n\t\treturn str(dt) # to account for timezone.toordinal() on systems where dt is a number\n\t\n\t@staticmethod\n\tdef format_timedelta(day, fmt='%Y %m'):\n\t\t\"\"\"\n\t\tFormats a datetime in a way that D* nodes expect and returns a timedelta in order\n\t\tto represent the datetime.\n\t\t\"\"\"\n\t\treturn datetime.timedelta(0, _HOUR_IN_DTHRDS[day])\n\t\t\n\tdef get_datetime("}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime(\"%d %B %Y\").replace(\"T\", \"T\")\n\n\n# Convert a datetime from the given string representation into a\n# dt_string representation.\ndef dt_string(dt_val):\n\tdt_string = dt_val.replace(\"|\", \"%\")\n\n\t# Turn the output into a python datetime.\n\tdt_str = dt_string.replace(\"-\", \"%\")\n\n\treturn dt_str\n\n\n# Calculate the dt string based on the current time.\n# @param current_time is the time of the current node in seconds\n#    "}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn format_dt(dt)\n\tdef format_dt_in_time(dt):\t\"\"\"\n\tFormats a time in the way that D* nodes expect, using a formatted string. This is useful\n\tif you want to format a time in a different format, for example:\n\n\t>>> now = datetime.now()\n\n\t>>> now = arrow.utcnow()\n\t>>> now = now.replace(year=2020, month=6, day=10, hour=0, minute=0, second=0)\n\n\t>>> now = arrow.utcnow().replace(year=2020,"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif not dt:\n\t\t\treturn None\n\n\t\treturn dt.strftime(\"%y-%m-%d\")\n\n\n# For debugging only!\ndef get_node_attributes(node):\n\tattr_value = None\n\tif 'value' in node.attrib:\n\t\tval = node.attrib['value']\n\n\t\tif val and val == \"\":\n\t\t\tattr_value = None\n\t\telif val == \"0\":\n\t\t\tattr_value = None\n\tif 'attrs' in node.attrib:\n\t\tatt = node.attrib['attrs']\n\n\t\tif att and att in [\"id\","}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tformat_string = \"\"\n\t\tif isinstance(dt, basestring):\n\t\t\tvalue_format_string = d_to_latex_string(dt)\n\t\telif isinstance(dt, datetime.datetime):\n\t\t\tvalue_format_string = d_to_latex_string(dt.strftime(\"%Y-%m-%d %H:%M\"))\n\t\telse:\n\t\t\tvalue_format_string = d_to_latex_string(dt.strftime(\"%d-%m-%Y %H:%M\"))\n\t\tformat_string += \"<dt>\" + value_format_string + \"</dt>\"\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tdt = dt.format(**d_timezone).strftime(\"%H:%M:%S\")\n\t\tmsg = ''\n\t\tif dt[3]:\n\t\t\tmsg = 'A '\n\t\telif dt[2]:\n\t\t\tmsg = 'May '\n\t\telif dt[1]:\n\t\t\tmsg = 'Tues '\n\t\tmsg +=''.join(d_tz.get(x[0] for x in d_tz))[1:25] + '...'\n\t\treturn msg\n\treturn frappe.db.get_default('label', '')\n\ndef get_fields(d):\n\tdef"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime(\"%d %b %Y %H:%M:%S\")\n\n# Return the list of nodes, where each node is a tuple\ndef get_sorted_nodes(nodelist):\n\tsorted_nodelist = []\n\tfor node in nodelist:\n\t\tnode_name = \"{0} {1}\".format(node[1], node[2])\n\t\tsorted_nodelist.append(node_name)\n\treturn sorted_nodelist \n\n# Start a process of running D nodes and running post-processing D in parallel\n# This is necessary to avoid some error conditions.  This"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\n#--------------------------\n# DIMENSIONS\n#--------------------------\ndim_map=\"\"\"\n-----BEGIN HIDE /H\n$Header$\nGPRINT \"DIVISORS\"\nMDI_TYPE \"GEXPID\"\nGPRINT \"DATA\"\nMDI_DATE_OF_DATE \"20120614T11:16:07.8365339Z\"\nDATE_OF_DATE \"20120614T15:14:08.4365338Z\"\nDATE_OF_DATE=\"20120614T22:29:10.87991799Z\"\nMDI_DATE_"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\t# Search for tag word matches text\n\t\tmatched = None\n\t\tfor m in REPLACER_PATTERN:\n\t\t\tif m.search(text):\n\t\t\t\tmatched = m\n\t\t\t\tbreak\n\t\tif matched:\n\t\t\tmatched = set([match.string for match in REPLACER_PATTERN])\n\t\t\tret_set = set(matched)\n\t\t\tfor token in matched:\n\t\t\t\tret_set = ret_set & token\n\t\t\treturn matched, ret_set\n\t\telse:\n\t\t\treturn set(), None\n\t# Find tag word.\n\ttry:\n\t\treturn re."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\twords = set()\n\t\tfor tag in re.findall( r'(.*?)(?:\\|[\\[\\]^|(){}=])', text ):\n\t\t\ttag = tag.strip()\n\t\t\tif re.findall( r'((.*?)\\|([\\[\\]^|(){}=])', tag ):\n\t\t\t\tname = tag.split('/')[-1].split(' ', 1)[0].replace('&quot', '')\n\t\t\t\tif name not in words: words.add(name)\n\t\t\t\telse: words.remove(name)\n\t\t\telse: words.add(tag)\n\t\twords = words.intersection( set"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\treturn set(tag for tag, tags in get_tags(text, replacer))\n\ndef convert_to_unicode_string(text: str):\t# nocoverage\n\t\"\"\"\n\tConverts a string to Unicode, but with an encoding that has charset removed.\n\t\"\"\"\treturn utf8_repr(text).encode('UTF-8', \"replace\")\n\t\ndef remove_encoding(text: str):\t\n\n\t\"\n\tRemoves a specific encoding of a string.\n\t\"\"\"\t\t\n\n\treturn re.sub(r\"[\\'\\\"]\", \"\", text, flags=re."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n\t\n\tif replacer is None:\n\t\trepl_tags = _repl_tags_regexp().findall(text)\n\t\trepl_tags = list(repl_tags)\n\t\trepl_tags = [x.strip() for x in repl_tags]\n\t\t\t\n\t\t# Use split_tag() to split the text\n\ttags = split_tag(text)\n\ttags = [re.sub(\"\\s+\", \" \", x) for x in tags]\n\t#print(tags)\n\ttags = [re.sub(\"\\s+\", \" \", x).strip() for x in tags]"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n\n\tif None is replacer:\n\t\treplacer = lambda s: s.replace(u\" \", replacer)\n\n\ttry:\n\t\tif text == \"1\":\n\t\t\ttext = \"# %s \\n\" % replacer(0)\n\t\telif text == \"2\":\n\t\t\ttext = \"# %s \\n\" % replacer(1)\n\t\telif text == \"4\":\n\t\t\ttext = \"# %s \\n\" % replacer(2)\n\t\telif text == \"4/5\":\n\t\t\ttext = \"# %s \\n\""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif replacer is None:\n\t\t\tdef repl(matchobj: MatchObject) -> Optional[MatchObject]:\n\t\t\t\treturn matchobj.text_before + matchobj.head_after + matchobj.replacement_text\n\t\t\treturn MatchList(\n                re.compile(r\"(?i){{([^}}]*)(.*?)}}\"),\n                repl,\n                [\n                    (r\"\"\"(?ix)<(?:[\\w\\-]+)[^>]*\\b(?ix))\"\"\",\n                  match_text[len(replacer) + match.start():]),\n                ])\n\t\ttag_re = re.compile(r"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif replacer is not None:\n\t\t\tf = self.replacer\n\t\telse:\n\t\t\tf = _strip_tags_by_tag_regex.sub\n\n        try:\n\t\t\ttext = text.decode('utf-8')\n\t\texcept UnicodeDecodeError:\n\t\t\tpass\n\t\ttry:\n\t\t\traw_text = f(text)\n\t\texcept UnicodeDecodeError:\n\t\t\treplacer_fn = getattr(f, replacer)\n\t\t\tret = text\n\t\telse:\n\t\t\treplacer_fn(raw_text)\n\t\t\tret = replacer_fn(raw"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tres = set()\n\t\tfor char in text:\n\t\t\tif replacer(char):\n\t\t\t\tres.add(char)\n\t\treturn res, res.union([\"x.\" + char for char in re.split('[0-9,]', output)[1]])\n\t\t\t\n\tdef tag_block(str: str) -> tuple:\n\t\t\"\"\"\n\t\tFind tags inside a python block and replace them with the tag of the\n\t\tstring.\n\t\tThe string will be parsed into a list of tags.\n\t\t\"\"\"\n\n\t\ttags_list = TagBlock.filter_block(str)\n\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n\t\n\tpattern = re.compile(r\"^\\w*tags(\\d+)\")\n\t\n\tre_list = re.findall(pattern, text)\n\t\n\tif replacer:\n\t\treturn re_list\n\t\n\tre_list = parse_html_tags(text)\n\t\n\tresult_set = set()\n\tfor re_tag in re_list:\n\t\tif replacer(re_tag):\n\t\t\tresult_set.add(text)\n\t\n\treturn result_set\n\n'''\n# Helper function to remove extra whitespace from tag_start\ndef replace_extra_whitespace_"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\n## TRY TO RUN THIS POP OF BLOCK TAGS TO PREPEND AND ADDING STEPY\n## FUNCTION DEFINITIONS FOR EXTERNAL REPLACEMENTS.  This function is a utility\n## function which will convert the text before the replacement, and return\n## a set of tags and any replaced text.  Internally, a set of tags will be found\n## instead of an exact match.\n\ndef substitute_text(text: str, replacer: Callable[[_PUNCTUATION_TOKEN], bool],\n                 replacement_data: Union[str, Any],\n                "}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tpass\n\n\tdef process_linkifying_links(text):\n\t\t\"\"\"\n\t\tProcess linkifying links in text, adding some attributes and linkifying URLs.\t\"\"\"\n\t\tpass\n\n\tdef process_links(text):\n\t\t\"\"\"\n\t\tProcess links in text, adding some attributes and linkifying links.\t\"\"\"\n\t\tpass\n\n\tdef parse_links(text):\n\t\t\"\"\"\n\t\tProcess links in text, adding some attributes and linkifying links in a list.\n\t\t\"\"\"\n\t\tpass\n\n\tdef search_links(text):\n\t\t\"\"\"\n\t\tSearch the"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor text_link, text_links in text_links:\n\t\t\tlinks = text_link.split(\"\\n\")\n\t\t\tfor text_link_new in links:\n\t\t\t\tnew_text = remove_empty_text_element(text_link_new)\n\t\t\t\tif new_text and new_text not in text:\n\t\t\t\t\ttext.append(new_text)\n\n\t\t\tfor link in text_links:\n\t\t\t\ttext_link_new = text_link if link == \"\\n\\n\\n\" else text_link\n\t\t\t\ttry:\n\t\t\t\t\tlinkify(text_link_"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor link in text.split('\\n'):\n\t\t\tif link!= '':\n\t\t\t\tfor linkline in link.split('\\n'):\n\t\t\t\t\tattributes = linkline.split('#')\n\t\t\t\t\tif len(attributes)!= 2:\n\t\t\t\t\t\tprint \"Error: Links in text for: \" + link + \" not a valid linkline length.\"\n\t\t\t\t\t\tsys.exit(1)\n\t\t\t\t\tif len(attributes) == 3:\n\t\t\t\t\t\tlink1 = attributes[1].split('/')\n\t\t\t\t\t\tlink2 = attributes[2].split('/')\n\t\t\t\t\t\tlink1 = [link1[0], link1[1],"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ts = re.search(r'(<a.+?)[\\n]+\\n', text)\n\t\tif not s:\n\t\t\treturn text\n\n\t\ts = re.search(r'(</a>.*?</tr>\\n(\\s*(<a.+?>)?)</a>.*?<a.+?>){1}', text)\n\t\tif not s:\n\t\t\treturn text\n\n\t\tfor m in re.finditer(r\"<a.+?>([\\w]+)</a>\", s.group(1)):\n\t\t\ts = re.sub(r'<p[^>]*>', '\\g<p[^>]*>', s.group(1))"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tif text.split('\\n')[0] == '<span' :\n\t\t\tfor i in range(len(text)):\n\t\t\t\ttext[i] = text[i].rstrip()\n\n\t\ttext = ''.join(self.process_text(n) for n in text) + '\\n'\n\t\t\n\t\tdoc_content = re.compile(r\"\"\"\n\t\t\t<span id='c-content'>(\\[[^>]*?)[^>]*(</a>.+?) # link\n\t\t\t<span id='w-content'>(\\[[^>]*?)[^>]*(</a>.+?) # text\n\t\t</"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor link in text:\n\t\t\tif link['type'] == 'text' or link['type'] == 'link':\n\t\t\t\tif link['text'].startswith('http:') or link['text'] == None or link['text'].startswith('https:'):\n\t\t\t\t\tadd_textlink(text, link)\n\t\n\tdef add_html_links(html_text):\n\t\t\"\"\"\n\t\tAdd links to html, adding some attributes and linkifying html links.\t\"\"\"\n\t\tfor link in html_text:\n\t\t\tif link['type'] == 'text' or link['type'] == 'file':"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor textLink in text.split('\\n'):\n\t\t\tfor key in KEYS.keys():\n\t\t\t\tif textLink[KEY] == KEYS[key]:\n\t\t\t\t\tlink = '<a href=\"' + KEYS[key] + '\" title=\"' + KEYS[key] + '\" class=\"' + KEYS[key] + '\"></a>'\n\t\t\t\telse:\n\t\t\t\t\tlink = '<a href=\"' + KEYS[key] + '\" title=\"' + KEYS[key] + '\">'+ KEYS[key] + '</a>'\n\t\t\t\tif isinstance(TEXT, basestring) or isinstance("}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tif text.startswith(\"<\"): text = text[1:]\n\t\telif text.startswith(\">\"): text = text[1:]\n\t\ttry:\n\t\t\t# Find all tags\n\t\t\twords = [\"%s=%s\" % (k, text.replace(k,'%s')) for k in GIMELOTI_TAGS+text.split() if k.strip()]\n\t\t\tfor w in words:\n\t\t\t\tif re.search(\" <\", unicode(w).lower()):\n\t\t\t\t\ttext = w\n\n\t\texcept Exception, e:\n\t\t\treturn\n\t\telse: text = \"<br>%s</"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tif type(text) is str:\n\t\t\ttext = text.replace(\"\\n\", \"<br />\")\n\t\t\ttext = text.replace(\"/\", \"<br />\")\n\t\t\ttext = text.replace(\"\\t\", \"<span class=\\\"falert\\\">\")\n\t\t\ttext = text.replace(\"'\", \"&apos;\", 1)\n\t\t\ttext = text.replace(\"\\r\", \"&#252;\")[2:-2] # \\\\\"\n\t\t\ttext = text.replace(\"'\", \"&quot;\")\n\t\treturn text\n\texcept AttributeError as e:\n\t\tprint \"process_text_links threw an exception.\"\n\ndef get"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext.links\t= []\n\t\ttext.tags\t\t= []\n\t\ttext.href\t\t= []\n\t\ttext.attrs\t= []\n\t\ttext.styles\t\t= []\n\t\tfor link in text.links:\n\t\t\ttext.links.append(self.linkify(link.href))\n\t\t\ttext.tags.append(self.linkify(link.tags))\n\t\t\ttext.href.append(self.linkify(link.href))\n\t\t\ttext.attrs.append(self.linkify(link.attrs))\n\t\t\ttext.styles.append(self"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tcontent_type_list = []\n\t\ttry:\n\t\t\tcontent_type_json = _get_json(url, headers)\n\t\t\tcontent_type_dict = get_content_type_list_for_content_type(content_type_json)\n\t\t\tif not content_type_dict: return None\n\t\texcept SQLObjectNotFound:\n\t\t\treturn None\t\n\texcept ConnectionError as ex:\n\t\traise ConnectionError(ex)\n\telse:\n\t\treturn content_type_dict[0]\n\t\ndef get_content_type_list_for_content_type("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif not url.startswith(\"http:\") or url.startswith(\"//\"):\n\t\t\treturn None\n\t\tres = self.session.get(self.url + url, headers=self.headers)\n\t\treturn res.headers.get(\"Content-Type\")\n\n\tdef get_file(self, name: str) -> List[FileEntry]:\n\t\t\"\"\"\n\t\tFetch the file information from the remote server.\n\t\t\"\"\"\n\t\turl = self.url + name\n\t\tself.session.get(url, cookies=self.cookies)\n\t\tres = self.session.get(url, headers"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\theaders = {'Content-type': 'text/html; charset=utf-8'}\t# type: Dict[str, str]\t\t\t\t\t\t\t\t\t\t\t# https://developer.github.com/v3/repos/dcos-developers/blob/master/docs/api/v1.7/fetching-content-type-v1.7.md\n\t\n\tresponse: List[requests.ResponseElement]\n\t\n\tif url == 'https://github.com/kid/dcos-sdk':\n\t\tresponse = requests.Response()\t\t\t\t# type: list[requests.ResponseElement]"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif url.startswith(\"https://\"):\n\t\t\treturn None\n\t\thost = urllib.parse.urlparse(url).netloc\n\t\t# TODO: this should also return 'application/x-www-form-urlencoded'\n\t\turl = f\"{host}:{id}\"\n\t\tcontent_type = None\n\t\trequest = requests.head(url=url)\n\t\tif request.status_code == 200:\n\t\t\tcontent = request.text\n\t\t\tdel request\n\t\telse:\n\t\t\tlogging.warn(\"Failed to fetch content type from {url!r}.  Unable to extract from it!\".format("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tparams = dict.fromkeys([\"url\"], None)\n\t\theaders = None\t\t# type: Dict[str, str]\n\t\tif url.startswith(\"https://\"):\n\t\t\tparams[\"verify\"] = False\n\n\t\telif len(url) > 0 and url[0] == \"{\":\t\t\t# starts with closing /\n\t\t\t\t\t\t\t\turl = url[1 :]\n\t\t\turl = re.sub(\"[/]\\\"\", \"/\", url, index=kwargs.pop(\"index2\"))\n\t\telif all(url == \"\"):\t\t\t\t# no ending slash\n\t\t\treturn\n\t\telse:\n\t\t\turl_parts ="}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif not (url.startswith('local://') or url.endswith('.git')):\n\t\t\traise GitCommandError.MissingRemoteUrl(url)\n\t\n\t# Get the content type from the HEAD of the remote url.\n\tremote_head = fetch_remote_heads(url)\n\t\n\tif not remote_head:\n\t\traise GitCommandError.MissingRemoteUrl(url)\n\t\n\t# Fetch the content type for the remote url.\n\ttry:\n\t\tcontent_type = remote_head.split('/')[1]\n\texcept IndexError:\n\t\traise GitCommandError.MissingRemoteUrl(url)\n\t\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\treq = RequestBase()\n\t\treq.url = url\n\t\trequest = AsyncResult()\n\t\trequest.headers = {}\n\t\tresponse = await urlopen(req, timeout=5)\n\t\tresponse.raise_for_status()\n\t\tif response.headers.get('Content-Type') is None:\n\t\t\treturn None\n\t\traw_response = response.headers['Content-Type']\n\t\tmatch = re.search(\".*content.*text/plain\", raw_response)\n\t\tif match is None:\n\t\t\treturn None\n\t\treturn match.group(1)\n\n\t@override\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\treq = requests.get(url)\n\t\tif req.status_code > 204:\n\t\t\twarnings.warn(\n\t\t\t\tf\"Content-Type of URL ({url}) ({req.status_code}) is \"\n\t\t\t\t\"unstable while fetching the content type because it \"\n\t\t\t\tf'docuses to be compressed. This is a temporary workaround, '\n\t\t\t\t'e.g. a version of the file got too long (50s for the '\n\t\t\t\t'first 30 chars in the content type).',\n\t\t\t\tfilename=f\"download-content-type.txt\",\n\t\t\t\tcategory="}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\theaders = {\n\t\t\t\"Content-type\": HEAD_HEADER_CONTENT_TYPE,\n\t\t}\n\n\t\tr = requests.get(url, headers=headers)\n\t\treturn r.content.decode(\"utf8\")\n\nclass Client:\n\t\"\"\"\n\tClient class. See http://code.google.com/p/psutil/\n\t\"\"\"\n\t_socket = None\n\t_address = None\n\t_pid = None\n\t_returncode = None\n\t_output = None\n\t_returncode_code = None\n\n\tdef __init__(self, address: str, pid"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif url.startswith(\"https://\"):\n\t\t\turl = \"https://\" + url\n\t\treturn True if url.endswith(\"json\") else False\n\nclass _ContentFetcher:\n\tdef __init__(self, local_path: str):\n\t\tself._local_path = local_path\n\t\tself._content_type = fetch_content_type(local_path)\n\t\tself._content_cache = {}\n\n\tdef fetch_content(self, content: urllib.HTTPResponse) -> None:\n\t\t\"\"\"\n\t\tFetch content in the local folder\n\t\t\"\"\"\n\t\tif self._"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag == 't'\n\ndef test_tag_to_tag(tag: str, word: str) -> bool:\n\n\t\"\"\"\n\tTest TagToTag function (returns True if it was accepted as a tag, False otherwise).\n\t\"\"\"\n\t# Return True if tag found, otherwise False\n\n\treturn word in ['t', 't']\n\ndef test_tag_to_untag(tag: str, word: str) -> bool:\n\t\"\"\"\n\tTest TagToUntag function (returns True if it was accepted as a untag, False otherwise).\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag in word_map:\n\t\t\treturn True\n\treturn False\n\n# TODO: Create random word (should make this just test if it's not good)\n\n# TODO: Remove tag from wordmap (if it has no such word!\ndef get_word_map():\n\t\"\"\"\n\tGet words map of word for the given word.\n\tReturns None if it's not good, or a list of word mappings suitable for\n\tgenerating the word.\n\t\"\"\"\n\tword_map = get_word_map_from_dictionary()\n\tif word_map is"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\ttoken_tag = Tag(tag)\n\t\ttoken_list = token_tag.tag.word.split(\" \")\n\t\t\n\t\tassert tag in token_list\n\t\t\n\t@staticmethod\n\tdef assertValidTag(tag: str) -> dict:\t\"\"\"\n\tTest a valid tagged format.\n\t\"\"\"\n\t\tassert _validate(tag) == _validate_tag_from_tag(tag)\n\t\t\n\t@staticmethod\n\tdef assertValidTagFormat(tag: str) -> dict:\t\"\"\"\n\tTest a valid tagged tag format.\n\t\"\"\"\n\t\treturn { _k:v for _"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\ttag_name = tag.lower()\n\t\tfor pattern in [u\"abc\", u\"bcd\", u\"123\"]:\n\t\t\ttag_pat = re.compile(pattern)\n\t\t\tif tag_pat(tag_pat):\n\t\t\t\treturn True\n\t\treturn False\n\tdef test_special(tag_name: str) -> bool:\n\t\traise ValueError(u\"bad special tag: {!r}\".format(tag_name))\n\n\tdef test_invalid_tag(tag_name: str) -> bool:\n\t\traise ValueError(u\"invalid tag: {!r}\".format(tag"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\t# check presence\n\t\treq = tag_lib.Tag()\n\t\treq.tag = tag\n\t\tok_res = tag_lib.tag(req)\n\t\tself.assertTrue(ok_res)\n\n\t@raises(ValueError)\n\tdef test_tag_invalid_key(self) -> None:\n\t\t\"\"\"\n\t\tVerify that passing a tag that is not valid will raise a ValueError\n\t\t\"\"\"\n\t\treq = tag_lib.Tag.validate(None) # None is valid\n\t\treq.tag = 'bad tag'\n\t\tok_res = tag_lib.tag"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in RE.tags\n\n\n# A tag which should definitely be allowed to be used for\n# a tag:\n#   The value should be False for 'foo' as a tag and can never be\n#   used as a tag\n#:   A tag where it can be used as a tag\n#:   The value can't be 'bar' to an tag\n#:   An empty tag\n#:\n# The first case will be considered 'foo'\n#: so the other words will be ignored.\n#   If the first is 'bar', then all other words will be accepted.\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag == \"x\" or tag == \"y\":\n\t\t\tassert tag in ['a', 'b']\n\t\telif tag == \"z\" or tag == \"p\":\n\t\t\tassert tag in ['a', 'b', 'c']\n\n\nmain()# -*- coding: UTF-8 -*-\n\"\"\"\nCreated on Wed, 01 Jan 2013\n\n@author: jasper\n\"\"\"\n\n\"\"\"\n    Instructions:\n\n\t1) get the name of a person\n\t2) get the person's name, e.g. Javier \n\n\n\"\"\"\n\nimport"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tp = Tag(tag)\n\t\tassert p.tag == tag\n\n\tdef test_tag_invalid_tag_string(self) -> bool:\n\t\t\"\"\"\n\t\tTag values can be given as tag names or as empty strings.\n\t\t\"\"\"\n\t\ttag_dict = {'tag': 'value'}\n\t\ttag = Tag(tag_dict['tag'])\n\t\twith pytest.raises(ValueError) as e:\n\t\t\tassert tag == tag_dict\n\t\tassert \"Empty tags are not permitted. Allowed tags: 'value'\" in str(e)\n\n\t\ttag = Tag(tag_dict"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag == '<':\n\t\t\ttoken = self.token()\n\t\t\tif token == '>':\n\t\t\t\treturn True\n\t\telif tag == '>=':\n\t\t\treturn True\n\t\telif tag == '\"':\n\t\t\treturn False\n\t\telif tag == '&':\n\t\t\treturn False\n\t\telif tag == \"<\":\n\t\t\treturn False\n\ndef get_tag(pos: int) -> int:\n\t\"\"\"\n\tGet the index of the first occurrence of the 'token' by index 'pos'.\n\t\"\"\"\n\t# Get the index of the first occurrence of the 'token' in 'pos'\n\t"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tpos = tag.rindex('_')\n\t\ttag_val: str = tag[pos+1:]\n\t\tpos = tag_val.find('.w_')\n\t\tif pos >= 0: return False\n\t\treturn True\n\texcept:\n\t\treturn False\n\nif __name__==\"__main__\":\n\ttry:\n\t\ttest()\n\texcept TagNotFoundException:\n\t\tprint \"Test word must be properly recognized.\"\n\t\tsys.exit(1)# (C) British Crown Copyright 2012 - 2015, Met Office\n#\n# This file is part of Iris."}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn dict(xml.iteritems(node))\n\tdef parse_xml_doc(node)\t\t\t\t\t\t\t:\tpass\n\tdef run(self, xml):\t\t\t\t\"\"\"\n\t\tParse an xml doc.\n\t\tArgs:\n\t\t  xml (etree.Document): the xml doc\n\t\t\"\"\"\n\t\tself.log('Parsing %s' % xml)\n\t\tfor name, child in xml.getroot().items():\n\t\t\tif isinstance(child, etree.Element):\n\t\t\t\tif 'tag' not in child:\n\t\t\t\t\traise InvalidXML(\"invalid child %s\" % child)\n\t\t\t\tself."}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\t\t\n\td=dict(attrs)\n\tfor child in node:\n\t\tif child.tag == 'child':\n\t\t    d[child.attrib['name']]=child.attrib.get('value','')\n\t\telse:\n\t\t    del d[child.tag]\n\t\n\t\n\n\n# -*- coding: utf8 -*-\n# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tchilds = {}\n\t\tdef is_parent():\n\t\t\treturn childs.has_key(node.tag)\n\t\n\t\tfor child in node.children:\n\t\t\tif is_parent():\n\t\t\t\tcontinue\n\t\t\tchilds[child.tag] = child\n\n\t\treturn childs\n\t\t\n\tdef get_xml(self):\t\"\"\"\n\tReturns the result of the xml conversion. The result is a dict.\n\t\n\tUsage:\n\t\tself.get_xml() -> {'value': 'value'}\n\t\t\n\t\"\"\"\n\t\tvalue = self.value\n\t\tif value is None:\n\t\t\treturn"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tnewnode = copy.deepcopy(node)\n\t\tif hasattr(node, 'tag'):\n\t\t\tfor k, v in node.tag.attribute.items():\n\t\t\t\tnewnode[k] = xml_children_as_dict(v)\n\t\treturn newnode\n\n\tdef xml_add_tag(self, tag):\n\t\traise TypeError('Cannot add child tag to tag tree.')\n\n\tdef xml_attr(self, attr):\n\t\tself.xml_add_tag(attr)\n\n\t\tif attr.name in ('class', 'data', 'default'):\n\t\t\tself.xml"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn dict(list(node.children))\t\n\n#--- GTK specific stuff\nclass XML_DECLARITIES:\n\tdef __init__(self, app):\n\t\tself._app = app\n\t\tself._tree = Gio.File.new_for_path(Gio.File.TreeBuilder)\n\t\t\n\t\tself._treeview = Gtk.TreeView()\n\t\tself._treeview.append_column(self._treeview_node_for_tag_name('type'))\n\t\tself._treeview.append_column(self._treeview_node_for_tag_name"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tif node.tag!= 'xml':\n\t\t\treturn { 'tag' : 'children', 'elements' : node.tag }\n\t\tchild = node\n\t\tchildren = []\n\t\tfor child in node.iter('child'):\n\t\t\tif child.tag not in ['tag', 'name']:\n\t\t\t\tcontinue\n\t\t\tif child.tag == 'name':\n\t\t\t\tchildren.append(child.text)\n\t\t\telse:\n\t\t\t\tchildren.append({ 'tag' : child.tag, 'children' : child.text })\n\t\treturn children\n\nclass ConvenienceClass:\n\t\"\"\"\n\t"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn util_xml.get_xml_children(node)\n\n\ndef xml_element_as_dict(tag):\n\treturn util_xml.get_xml_element(tag)\nimport json\n\n\nclass Node:\n    \"\"\"A Base class for all Nodes.\"\"\"\n\n    def __init__(self, id, children=None):\n        self.id = id\n        if children is not None:\n            self.children = children\n\n    @property\n    def value(self):\n        \"\"\"The value of this element.\n\n        :return: The value of this element.\n        \"\"\"\n        return self.children[self"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\n\t\n\tdef recursive_dict(node):\n\t\tchildren = {}\n\t\tif node.tag!= \"xml\":\n\t\t\tchildren = xml_children_as_dict(node)\n\t\treturn children\n\treturn recursive_dict(node, doc_class)# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\td = {}\n\t\tfor child in node.childNodes:\n\t\t\tif child.tagName in ['xml', 'html', 'text']:\n\t\t\t\td[child.tagName] = child.nodeValue\n\t\treturn d\n\n\n@register_introspection_system()\nclass DjangoIntrospection(object):\n\t\"\"\"\n\tImplements the DjangoIntrospection system.  This will either automatically load\n\tdjango.contrib.auth into your PYTHONPATH with an AUTHENTICATION_BACKENDS and\n\tautomatically set the authentication backends.\n\t\"\"\"\n\t# The default database password is's3cret' by default so that it can\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tattr=node.attrib\n\t\tif(attr=='nodes'):\n\t\t\tfor childChild in node.getchildren():\n\t\t\t\tfor key, value in childChild.iteritems():\n\t\t\t\t\tif(attr=='children'):continue\n\t\t\t\t\tif(key==attr):\n\t\t\t\t\t\tdictDict[key]=value\n\treturn dictDict\n\ndef write_json_file(node):\n\n    with open(JSON_FILE_PATH,'w') as json_file:\n\t\tjson_file.write(node)\n\t\tjson_file.close()# -*- coding: utf-8 -*-\nfrom __future__ import"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender = session.query(EmailDomain.name).filter(EmailDomain.name == sender_handle, EmailDomain.email == sender_handle).first()\n\t\tentity = session.query(EmailDomain.name).filter(EmailDomain.name == entity_handle, EmailDomain.email == entity_handle).first()\n\t\tsession.commit()\n\n\texcept NoResultFound:\n\t\tsession.rollback()\n\t\tlogger.warning('EmailDomain not found: %s\\nContacting an unknown person will not reply.' % sender_handle)\n\t\tmessage = 'Invalid email address.'\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\ttry:\n\t\t\tsender_sender = None\n\t\t\tsender_entity_id = entity_handle.attributes['entity_id']\n\t\t\tsender_object = entity_handle.parentNode\n\t\t\tsender_sender_sender = self.get_sender_and_entity(\n\t\t\t\tsender_entity_id, sender_object).email\n\t\t\tsender_entities = sender_sender.getElementsByTagName(\"from\")\n\t\t\tsender_entities.sort(key=lambda e : e.getAttribute(\"is_to\"))\n\t\t\tsender, = sender_entities\n\t\t\tsender_handles = sender.getElementsByTagName(\"to\")\n\t\t\tsender"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tif sender_handle.startswith('//') and sender_handle.endswith('/') and sender_handle.startswith('http'):\n\t\t\t# We have a URL on the url, so we need to re-open the event\n\t\t\turl = os.path.join(self.get_url('events/sender/' + sender_handle), '/message/handleReceipts.cgi')\n\t\t\tresponse, content = self.fetch(url)\n\t\t\tself.check_response(response, content)\t\n\t\telif not sender_handle.startswith('//') and not sender_handle.endswith('\\\\') and sender"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tif sender_handle!= entity_handle:\n\t\t\treturn False\n\n\t# If this is the sender we got, make sure it's a valid entity, else it's not an author and not an entity.\n\tif not sender_handle:\n\t\tsender_handle = check_sender_and_entity_handle_match(sender_handle, sender_handle)\n\t\n\tperson = get_person_from_handle(sender_handle)\n\tif not check_person_in_person_collection(person, recipient):\n\t\treturn False\n\n\treturn True\n\ndef get_email_hash"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tpass#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tassert sender_handle == sender_handle_expected\n\t\tassert entity_handle == entity_handle_expected\n# encoding: utf-8\n\nimport copy\nimport os\nimport sys\nimport threading\nfrom datetime import datetime, timedelta\n\nfrom django.utils.translation import ugettext as _\nfrom django.conf import settings\nfrom django.http import Http404, HttpResponseBadRequest, HttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom sentry.api.serializers import serialize\nfrom sentry.api.serializers.serializers import serialize_sentry_api\nfrom sentry.models import Release"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tpass\n\n\t# Verifying sender match and entity match.\n\tentity_handle = frappe.get_doc(\"Member\", frappe.defaults.get_user_from_doctype(sender_handle))\n\tsender_email = frappe.db.get_value(\"Account\", sender_handle, \"email\", as_dict=True)\n\tsender_name = frappe.db.get_value(\"Account\", sender_handle, \"name\", as_dict=True)\n\treceiver_email = frappe.db.get_value(\"Account\", sender_handle, \"email_address\")\n\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_handle = str(sender_handle)\n\t\tentity_handle = str(entity_handle)\n\t\tif (sender_handle == entity_handle) and (entity_handle == sender_handle):\n\t\t\treturn True\n\ndef check_sender_and_entity_match_without_sender(sender_handle, entity_handle):\t\"\"\"\n\tCheck if the sender does not have a sender.\n\tUnless the sender matches the entity, this will not throw an error.\n\t\"\"\"\n\t\tsender_handle = str(sender_handle)\n\t\tentity_handle = str(entity"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tassert sender_handle == entity_handle\n\n\t\tassert sender_handle.startswith('diaspora: ') and sender_handle.startswith('mailto:')\n\t\tassert sender_handle.endswith(' ')\n\n\t\tassert_almost_equal(sender_handle, entity_handle, error_msg=(sender_handle, \"Incorrect entity object\"))\n\ndef check_sender_and_service_handle_match(sender_handle, service_handle):\n\t\"\"\"\n\tEnsure that sender and service handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tawait ctx.message.channel.send(message.author.format(sender_handle, entity_handle))\n\n\tdef check_receiver_author(receiver_handle):\t\"\"\"\n\tCheck that receiver author is same. For de-duodies, we need to verify that receiver has not been authorized yet\n\t\"\"\"\n\t\tawait ctx.message.channel.send(message.author.format(receiver_handle))\n\ndef check_account_presence(message):\n\t\t\"\"\"\n\t\tCheck message status if it doesn't exist or if it is an already known presence.\n\n\t\tIf a"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\t\n\tif document_path:\t\t\n\t\tnode_info_path = document_path.value\n\n\tnode_info_document_url = \"http://edge-control.diaspora.software.sf\" + url\n\n\treq = urllib.request.Request(node_info_document_url)\n\n\tif doc_path and os.path.isfile(doc_path):\t\t\t\t\n\t\t#if the url doesn't contain a directory or if that doesn't contain a full extension.well-known document, it takes the full base url, with the document name as parameter, and the version"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif not document_path:\n\t\t\treturn {}\n\t\tif document_path == None:\n\t\t\tdocument_path = config.get_defaults('nodeinfo', 'document')\n\n\t\turl = urlparse.urljoin(configuration.get_web_root(), document_path)\n\t\tresp, content = self._requester.request(url, method='GET', data=urlencode_postdata_headers(data), headers={'Accept': 'text/html'})\n\t\treturn_code = findtext(content, 'Response JSON', resp.content)\n\t\tif return_code:\n\t\t\traise ApiError('"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\treturn [nodeinfo.get_nodeinfo_from_url(i)\n\t\t\t for i in self.get_nodes(url, document_path=document_path)]\n\n\t# From http://nodeinfo.diaspora.software\n\tdef get_first_node(self, nodeid):\n\t\t\"\"\"Return first node matching id.\n\t\t\"\"\"\n\t\tfor r in self.get_nodes(nodeid):\t\t\t\n\t\t\treturn r\n\n\tdef get_nodeinfo_from_url(self, url):\n\t\t\"\"\"Returns NodeInfo from given URL.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = get_document_path()\n\t\turl = url.rstrip('/')\n\t\tpath_query = \"http://example.com/path/?%s\" % document_path\n\n\t\tresp = self._get(url, \"POST\")\n\n\t\tif resp.status_code in [200]:\n\t\t\t# Get the DocumentInfo object and then update\n\t\t\tdocinfo = {}\n\t\t\twith open(self.get_well_known_document_path(), 'w') as w:\t\n\t\t\t\tw.write(resp.read())\n\n\t\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\treq = urllib.request.Request(url)\n\t\treq.add_header('accept', 'text/html')\n\t\tif document_path:\n\t\t\treq.add_header('X-Document-Path', document_path)\n\t\tfor key in ('content-length', 'content-type', 'content-encoding'):\n\t\t\treq.add_header(key, urllib.header.header_value(key))\n\t\treq.add_header('Accept-Language', 'en-us')\t\n\t\treturn self._do_request(req)\n\n\t#TODO: test if we"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\treturn get_node_url(url, is_well_known=True, is_well_known_document_path=document_path)\n\n\ndef setup_test_environment_with_doc(docroot, nodeinfos_path=None):\n\t\"\"\"\n\tSetup the environment with docroot, adds the nodeinfo directory, the node.\n\n\t:arg docroot: A dict, which contains root and docroot path\n\t:arg nodeinfos_path:  Custom NodeInfo and NodeInfo.paths\n\t:returns: None.\n\t\"\"\"\n\t\tenv_var = get"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = tempfile.mktemp()\n\t\tif not os.path.exists(document_path):\n\t\t\treturn {'status' : 'failed', 'id'  : document_path}\n\t\tdocument_path = os.path.realpath(document_path)  # avoid problems with url-aware conversion\n\t\tcmd = 'wget %s | cut -f1-d' % document_path\n\t\tif '--format' in sys.argv:\n\t\t\tcmd +='%s' % sys.argv[sys.argv.index('--format"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = url.split('://')[-1]\n\t\tnodeinfo_path, doc = self.get_node_document(nodeinfo_path)\n\t\tif doc is None or url == 'file://' + nodeinfo_path:\n\t\treturn nodeinfo_path, doc\n\t\tif document_path is None:\n\t\t\t# If we are on a file, use the url directly\n\t\t\treturn self.get_node_url_from_path(nodeinfo_path)\n\t\telse:\n\t\t\twith open(self.get_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = os.path.join(os.getcwd(), config.get_node_info_document_path())\n\t\tdocument_path = os.path.normpath(document_path)\n\n\t\tif not self.path_exists(document_path):\n\t\t\traise ValueError(\"Path '%s' does not exist.\" % document_path)\n\n\t\tfh = file(document_path, \"r\", encoding=\"utf-8\")\n\t\tresult = json.load(fh)\n\t\tfh.close() # This must be done first\n\n\t\tif"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\tnodeinfo = Client.get(url)\n\tif not nodeinfo or not nodeinfo.is_woko:\n\t\treturn\n\ttext = nodeinfo.get_nodeinfo_document('well-known')\n\tif text:\n\t\treturn TextWrapper(text)\n\telif document_path is not None:\n\t\tpath = nodeinfo.get_nodeinfo_document_path()\n\t\tif os.path.exists(path):\n\t\t\tif not document_path and not not path.endswith('/'):\n\t\t\t\tpath_no_document = os.path.join('documents"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tassert (len(doc) > 0)\n\t\tassert (len(signature) > 0)\n\t\tassert (signature[0] >= 0)\n\t\tassert (signature[1] >= 0)\n\n\t\tassert (len(public_key) == 12)\n\t\tassert (len(doc) == len(signature))\n\t\tfor i in range(len(signature)):\n\t\t\tassert len(signature[i]) == len(doc[i]) == signature[i]\n\n\t\tassert (len(public_key[4]) == 24)\n\t\tassert (len(doc[4]) == len"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tmsg = \"\"\n\t\tmsg += \"<message xmlns:email='http://example.com'><toaddr='public-doc@example.com'><cc=freenode@example.com> <fromaddr='doc-owner@example.com'> docowner@example.com</cc><body xmlns:email='http://example.com'> doc owner@example.com</body></message>\"\n\t\tmsg += \"<signature xmlns:email='http://example.com'><toaddr='Signature-doc@example.com'><cc=freenode@example.com> <fromaddr"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\ttry:\n\t\t    signer = hashlib.new(\"sha1\")\n\t\t    ser = doc.getElementsByTagName(\"sender\")[0].firstChild.data\n\t\t    signer.update(ser)\n\t\t    signer.update(doc.getElementsByTagName(\"timestamp\")[0].timeStamp)\n\t\t    signer.update(doc.getElementsByTagName(\"signature\")[0].signaturestring)\n\t\t    doc.getElementsByTagName(\"signed-signature\")[0].clear()\n\t\t\n\t\t    signature.update(signer.digest())\n\t\t    signature.update(signer.digest().encode(\"hex\"))\n\n\t\t    signature = signature.upper()\n\t\t    signature = signature[:-7]\n\t\t    # get first n-1 bytes of the XML"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\treturn signature in doc.getElementsByTagName('signature/signed')[0].attributes.value.split(';')\n\t\n\tdef verify_signature_rfc3986(self, signature):\n\t\t\"\"\"\n\t\tVerify to see if the signed XML elements returned by :py:meth:`verify_signature`\n\t\treturn ``True`` if the signature matches the input signature.\n        \"\"\"\n\t\ttry:\n\t\t\txmldoc = xml.dom.minidom.parseString(signature)\n\t\t\tsignature_match = [match for match in xmldoc.getElementsByTagName('signature').childNodes \\\n\t\t\t\t\t\tif match.tagName =='signature', 0]\n\n\t\t\t#"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tsignature.append(signature.sign(public_key))\n\t\tsignature.sign(doc)\n\n\treturn signature\n\t\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\ndef verify_signed_signature(content, signed_by_name, signed_response, signing_authority, signature):\n\t\"\"\"\n\tVerify the signed XML elements with confidence that the claimed author did actually generate this\n\tmessage.\n\t\"\"\"\n\t\n\tsignature_dict = {}\n\t\n\t# If the signed signature is invalid then the server must be invalid that it was not signed\n\tif signature == '' and signed_by_name == '':\n\t\n\t\"\"\"\n\tVerify signed XML elements to have confidence that the claimed author did actually generate this\n\tmessage.\n\t\"\"\"\n\tdoc = etree"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Verify message signatures are from the signature. Don't ask it again once\n\t\t# all the signatures are trusted.\n\t\tsignatures = signature.split(\"<br/>\")\n\t\tif len(signatures)!= 3:\n\t\t\tprint >> sys.stderr, \"Invalid signature for relayable signature %s. Must be a 3-element list: %s\\nFor the following cases, type your signed message:\\nhttps://docs.\"% (signature, signature)\n\t\t\treturn False\n\t\tif len(signatures[0])!= 10 or (signatures[0][0]!= '>'):\n\t\t\tprint >> sys"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tassertion= Signature.parse(signature) \n\t\tsubject=assertion.subject\n\t\treturn assertion.get_subject().as_string()!= subject\n\ndef verify_signature_subject(issuer, subject, signature):\n\t\"\"\"\n\tVerify the signed XML elements sent via subject to ensure that the signature is\n\tcorrect. This test checks the signature against the subject. An alternative\n\tverification will be added if the subject is not signed.\n\t\"\"\"\n\tassertion= SigningAssertion.parse(signing_signature_subject(issuer, subject))\n\tif not assertion.signature.is_correct("}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\nclass RelayableValidator:\n\t\"\"\"\n\tA simple tool that allows verifying the relayable signature of a signature.\n\tThis is useful if you want to use the RelayfulSignature tool to verify that the message\n\tsignatures are correct. The main use case is to verify that the signed signature\n\tdid actually generate this message. The other use cases are to verify that the message\n\tsignatures didn't have any of those signature. \n\t\"\"\"\n\tdef __init__(self, signature):\n\t\tself.signature = signature\n\tdef verify_signature(self, doc, signature):\n\t\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tdef getSignature(xmlObject):\n\t\t\tassert doc == signature\n\t\t\tassert len(xmlObject.childNodes) == 1\n\t\t\treturn signature, xmlObject.childNodes[0].data.encode(\"utf8\")\n\t\tsignatures = getSignature(doc.encode(\"utf8\"))\n\t\tsigned = signers.get(signature, None)\n\t\tif signed:\n\t\t\tassert len(signed) >= 4\n\t\t\tassert signed[4].encode(\"utf8\")!= signature\n\t\t\tsigners[signature] = signed\n\t\tassert signed is None or signed.signature!= signature\n\t\t\n\nclass TX(Serial"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\t# check if document is empty\n\t\tif not document:\n\t\t\traise Exception(\"You have to provide a document.\")\n\n\t\t# parse the raw document \n\t\tlines = document.strip().splitlines()\n\t\t\n\t\tfor line in lines:\t\n\t\t\tline = line.strip()\t\n\t\t\tif line and not line.startswith(\"##\"):\t\t\t\t\t       \n\t\t\t\tmatch = extract_diaspora_webfinger(line)\t\n\t\t\t\tif len(match) == 2:\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn match[0],match[1]\n\t\t\t\t\texcept:\n\t\t\t\t\t\treturn (None,None)"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdata = \"\"\n\t\ttry: \n\t\t\tdata = webfinger_to_dict(document)\n\t\t\tfor value in data:\n\t\t\t\tdata += \"\\t\" + value + \",\"\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Error parsing webfinger document: {}\"\n\t\t\t\t  .format(e))\n\t\treturn data \n\n\treturn json.loads(parse_diaspora_webfinger(document))\n\n\ndef url_to_domain_from_request(url: str) -> Tuple[\"str\"], str:\n\t\"\"\"\n\tBuild the domain name and domain"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn parse_diasi_webfinger.from_json(document) if document else {}\n\n\tdef parse_http_response_uri(document: str) -> Dict:\t\"\"\"\n\tParse http response URI\n\t\"\"\"\n\t\treturn parse_http_response.from_json(document) if document else {}\n# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tres = {\n\t\t\t\"data\": {\n\t\t\t\t\"data\": {\n\t\t\t\t\t\"type\": \"website\",\n\t\t\t\t\t\"url\": document.strip()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn res\n\n\tdef is_webfinger_valid_with_validation(self, document: str) -> bool:\n\t\t\"\"\"Check if we have information about the webhook and if valid, update.\"\"\"\n\t\ttry:\n\t\t\tdoc = frappe.get_doc(\"Webfinger\", document)\n\n\t\texcept (WebfingerError, WebfingerRequestError) as err:\n\t\t\tfrappe."}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tresult_doc = frappe.import_file(os.path.join(os.path.dirname(__file__), 'webfinger-results.json'))\n\t\tresult = result_doc.get(document['_id'])\n\t\tif \"id\" in result:\n\t\t\td = _parse_html_diaspora_webfinger(result['id'])\n\t\tif \"title\" in result and \"description\" in result:\n\t\t\td = _parse_html_diaspora_webfinger(result['description'])\n\t\tfor key, value in iteritems(d):\n\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn {}\n# DNAQs to do filtering of MSA/CDCs for comparison\n\nimport numpy as np\nimport scipy.sparse as sp\n\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport msa_cdc as msa\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\nfrom diaspora_nmea import MSAResults\nfrom diaspora_nlc import NLCCD\n\nimport sklearn.naive_bayes\nfrom sklearn.metrics import"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\t# For JSON output, just parse it as JSON format\n\t\tdocument = diaspora_webfinger.transform_document_to_raw(document, default_encoder)\n\n\t\treturn json.loads(document)\n\nif __name__ == '__main__':\n\tapplication = create_app(\n\t\t__name__,\n\t\ttemplates_folder='/var/www/templates',\n\t\tstatic_folder='static'\n\t)\n\tapp.debug = True\n\t# Register the custom Flask application\n\tapplication = create_app(__name__ + 'WebAPI', __name__ +"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\t# Parse data\n\t\tif re.match(\"(?:[a-zA-Z0-9_\\.\\(\\)]+)\", document):\n\t\t\tdocuments = document.split(\".\")\n\t\telif re.match(\"(?:[a-zA-Z0-9_\\.\\(\\)]+)\", document):\n\t\t\tdocuments = document.split(\";\")\n\t\t\tdocuments = [x.lower() for x in documents]\n\telse:\n\t\traise InvalidDocument(\"invalid json document format\")\n\n\t# Format output\n\twith open(os.path.join(settings['Diaspora_root'], \"diaspora"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tjson.load(document)\n\t\twebfinger = get_diaspora_webfinger_to_json(json.load(document))\n\t\treturn webfinger\n\n\tdef get_user_id(self, diaspora_webfinger_uid: str) -> str:\n\t\t\"\"\"Extract Id of the user from Diaspora webfinger.\n\n\t\t:param diaspora_webfinger_uid: User ID\n\t\t:return: Id of user\n\t\t\"\"\"\t\n\t\tif '.' in diaspora_webfinger_uid"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tresult: Dict\t\t= {}\n\t\tresult['name'] = None\n\t\tresult['links'] = None\n\t\tif 'link' not in document:\n\t\t\tresult['link'] = document\n\t\t\tresult['name'] = get_page_name(document)\n\t\t\tjsondata = json.dumps(parse_data_html(document), default=encoder)\n\t\t\tparser = etree.HTMLParser()\n\t\t\thtml = lxml.html.fromstring(jsondata, parser=parser)\n\t\t\troot = html.getroot()\n\t\t\tfor child in root:\n\t\t\t\tif child.tag == 'article"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\twith urllib.urlopen(handle) as f:\n\t\t\t\t# pylint: disable=E1101 # Allow to make it available as a file.\n\t\t\t\tdocument = f.read().decode()\n\t\t\t\tif document == b'<html>':\n\t\t\t\t\t# pylint: disable=F0401\n\t\t\t\t\treturn None\n\n\t\t\t\tdoc = xml.dom.minidom.parseString(document)\n\t\t\t\t# pylint: enable=E1101 # Allow to make it available as a file.\n\t\t\t\treturn document\n\t\t\t# Unable to read/use documents.\n\n\t\texcept Exception: # pylint: disable=broad-except\n\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\thandle = handle.strip()\n\t\texcept (ValueError, AttributeError):\n\t\t\tprint \"Invalid WebFinger document detected:\"\n\t\t\tlogger.warning(\"Failed to parse WEBFinger document: {}\".format(str(handle)))\n\t\t\thandler = None\n\t\t\treturn None\n\t\n\t\tlogger.info(\"Using webfinger web page: %s\", handle)\n\t\t\n\t\tr = requests.get(server, headers=headers, data=f)\n\t\theaders['User-Agent'] = get_ua()\n\t\ttry:\n\t\t\tr = r.text\n\t\texcept AttributeError:\n\t\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\t_logger.debug(\"looking for webfinger document {handle} in the given resource\".format(handle=handle))\n\n\t\twebfinger_document = None\n\t\ttry:\n\t\t\twebfinger_document = webfinger.document(handle).get()\n\t\texcept:\n\t\t\tpass\n\n\t\treturn webfinger_document or None\n\n\tdef send_email(self, send_webfinger_emails: Callable[..., None]]) -> None:\t\"\"\"\n\tSends the given webfinger email to each recipient.\n\t\"\"\"\n\t\tpass\n\n\tdef get_webfinger"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\turl = 'http://www.fucking-cable.org/' + handle.strip() + '/'\n\t\tf = urllib2.urlopen(url)\n\n\t\thtml_response = f.read()\n\t\tparsed = re.search(br'var\\svalue = (.*);', html_response)\n\t\tif parsed:\n\t\t\treturn parsed.group(1)\n\telse:\n\t\traise Exception('Could not parse WebFinger webfinger document from')\n\ndef get_document_data(handle: str) -> Dict[str, str]:\n\treturn dict([(el"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tresp = yield from requests.get(\n\t\t\t\turllib2.Request(webfinger_document_url+path.split('/')[-1], 'r').content,\n\t\t\t\theaders={\n\t\t\t\t\t'User-Agent': http_vendor,\n\t\t\t\t\t\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n\t\t\t\t}\n\t\t\t)\n\t\t\tif resp.status_code == 200:\n\t\t\t\treturn resp.text.rstrip()\n\t\texcept ValueError:\n\t\t\traise webfinger_exception('"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\t# Attempt to retrieve an existing webfinger document\n\t\t\treturn finger_client.load_webfinger_document(handle)\n\t\texcept WebException as e:\n\t\t\tif e.code() == 404:\n\t\t\t\t# New webfinger document does not exist. Maybe I couldn't find it!\n\t\t\t\tpass # None or other error. Try with a try again.\n\t\t\t\telse:\n\t\t\t\t\t# New web finger document already exists.\n\t\t\t\t\ttry_retrieve = webfinger_client.create_webfinger_document(handle)\n\t\t\t\t\treturn try_retrieve"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn get_webfinger_document(handle, True)\n\t\texcept Exception as e:\n\t\t\tlog.error(e)\n\t\t\tlogger.exception(e)\n\t\t\treturn None\n\n\t@log_call\n\tdef get_webfinger_document(handle: str) -> Optional[Document]:\n\t\ttry:\n\t\t\treturn get_webfinger_document(handle)\n\t\texcept Exception as e:\n\t\t\tlog.error(e)\n\t\t\tlogger.exception(e)\n\t\t\treturn None\n\n\t@log_call\n\tdef get_doc("}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif not handle:\n\t\t\thandle = webfinger_database.get_resource('/webfinger')\n\n\t\ttry:\n\t\t\thandle_docs = get_remote_files_from_path(handle, None, False)\n\t\t\tif not handle_docs:\n\t\t\t\traise NotFoundError('No Webfinger document found for {0}'.format(handle))\n\t\texcept NotFoundError:\n\t\t\traise NotFoundError('No Webfinger document found for {0}'.format(handle))\n\n\t\t# We only need the first occurence of the document, and can the document be retrieved?\n\t\tif"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tdocument = Document.objects.get(handle=handle)\n\t\t\treturn document\n\t\texcept:\n\t\t\n\t\t\t\n\tdef get_content_html(handle: str, encoding: str=None) -> str:\n\t\t\"\"\"\n\t\tGet content of the content_html.\n\t\t\"\"\"\n\t\t\n\t\tcontent = Content()\n\t\tcontent.content = request.session.get(\"content_html\", None)\n\t\tcontent.content_type = request.session.get(\"content_type\", None)\n\t\t\n\t\tif content.content and \"html\" not in content.content_type:\n\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn handle.split('.', 1)\n\t\texcept:\n\t\t\treturn None\n\ndef fetch_document(handle: str, force: bool = False, timeout: Timeout = 10) -> Document:\t\"\"\"\n\tThe main public interface function for fetching a webfinger document.\n\n\t:param str handle: The webfinger document to fetch\n\t:param bool force: Force fetch\n\t:param int timeout: Maximum amount of time to wait for the fetch to complete\n\t:return: Document instance (WebfingerDocument)\n\t:rtype: :class:`Webfinger"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tprint \"Reading webfinger document %s at %s\" % (handle, handle.name)\n\t\tdoc_id = \"\"\n\t\t# Use a local file name\n\t\twith open(handle.name) as handler:\n\t\t\tfor line in handler:\n\t\t\t\tline = line.strip()\n\t\t\t\tif line.startswith(\"ID:\"):\n\t\t\t\t\tdoc_id = line[6:]\n\telse:\n\t\tprint \"Fetching document from %s\" % handle.name\n\t\tid = raw_input(\"Enter the Diaspora webfinger id:\")\n\t\tif id.lower"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thtml = fetch_from_url(\"https://%s.diaspora.com/api/%s%s/\" % (api_host, diaspora_url,\n                                                                           handle))\n\t\t# If there's a page, assume the first page\n\t\tif html is not None:\n\t\t\t# It's not complete.\n\t\t\treturn parse_diaspora_webfinger(html)\n\t\t# If the handle contains a comment, assume that it's the page itself\n\t\tif not handle.startswith(\"Comment\"):\n\t\t\t# If it doesn't contain a page, assume that"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tdiaspora_webfinger = handle.info().as_dict()\n\t\treturn diaspora_webfinger\n\n\t# ------------------------------------------------------------------------\n\tdef store_data_in_remote_doc(self, doc, idc):\n\t\t\"\"\"\n\t\tStore a remote Diaspora webfinger document, so that subsequent calls to\n\t\tretrieve can continue without any further reading.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdoc = document.fromstring(doc)\n\t\t\td = {self.doc_class.get_name(doc.get(i)) for i in self"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tif handle.name.lower().split('s')[1].capitalize() in ['WebFinger']:\n\t\t\tres = handle.get_document(None)\n\t\t\tif res is not None:\n\t\t\t\treturn res\n\t\telse:\n\t\t\tres = extract_xml(handle)\n\t\t\treturn res\n\telse:\n\t\treturn None\n\ndef retrieve_and_parse(handle):\t\"\"\"\n\tRetrieve a Remote handle and parse it.\n\n\t:arg handle: Remote handle to retrieve.\n\t:returns: dict\t\"\"\"\n\t\tif handle.name.lower().split('s')[1"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tresponse = requests.get(\"https://diaspora.gov/web/w3/desiphon/search_type.json\", params = {\n\t\t\t\t\t\t \"q\" : str(handle) } )\n\t\tsoup = BeautifulSoup(response, 'html5lib')\n\t\tdiv_id = soup.select(\"#response_doc\").select(\"select id\").val[0]\t\n\t\tdiv_url = soup.find_all(\"div\")[1]+'/'+div_id\n\t\tdoc_id = div_id\n\t\treturn get_diaspora_webfinger(doc_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tresp = requests.get(url=handle.url).json()['document']\n\t\tfor item in resp[\"data\"][\"webfinger\"]:\n\t\t\tname = item['name'].split('=')[0]\n\t\t\tlinks = item['webfinger']['links']\n\t\t\tname = name.replace('&amp;', '&')\n\t\t\tname = name.replace('&gt;', '&gt;')\n\t\t\tname = name.replace('&lt;', '<')\n\t\t\tname = name.replace('&quot;', \"'\")\n\t\t\tlink = \"https://diaporaapi.com/\" + links[0]['href'] + \"?\" +"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\trespuesta = {}\n\t\trespuesta['id']['remote'] = handle.get()\n\t\trespuesta['id']['docid'] = handle.get()\n\t\trespuesta['id']['webfinger'] = handle.get()\n\t\trespuesta['lastmodification'] = _get_current_datetime(handle.get())\n\t\trespuesta['access']['remote'] = self._get_access_remote(handle)\n\t\trespuesta['access']['doctype'] = handle.get().strip()\n\t\trespuesta['access']['docid'] = handle."}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn_doc = None\n\t\tdoc_url = re.search(r\"(https?|ftp|hg|http|ssh):\\/\\/diaspora\\.(org|server\\.(net))/([0-9]+)\", handle.url).group(1)\n\t\tdoc = Document(url=doc_url)\n\t\tdoc['url'] = doc_url\n\t\tdoc['author_name'] = \"Diaspora\"\n\t\tdoc['version'] = \"1.0.0\"\n\t\tdoc['authors'] = [x.encode('utf-8').strip() for x"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\nclass Narrative:\n\t\"\"\"Class to retrieve information about digital digital websites\n\t\"\"\" \n\t_dict = None\n\t\"\"\"Class to retrieve information about digital digital websites\"\"\"\n\t_diaspora_client = None\n\t\"\"\"Class to retrieve information about digital digital websites\"\"\"\n\t_webfinger_url = None\n\t\"\"\"Class to retrieve information about digital websites\"\"\"\n\tdef _get_diaspora_client(self):\n\t\t\"\"\"\n\t\tReturn a DigitalWebFinger object\n\t\t\"\"\"\n\t\treturn self._diaspora_client\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t     \n\tdiaspora_webfinger = get_diaspora_webfinger(handle)\n\tdocument = diaspora_webfinger\n\t\n#    document = document.replace(u'\\\\', u'\\\\amp').replace(u'/>', u'</').replace(u'&amp;', '&').replace(u'&imgsrc=', 'image:' + image_id + '-webfinger/' + image_id + '_imgsrc', 'imgsrc=' + diaspora_webfinger['webfinger']['uri'] + '/imgsrc')"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turl = None\n\t\ttry:\n\t\t\td = get_url(url=host)\n\t\t\tsoup = BeautifulSoup(d)\n\t\t\thosts = soup.findAll('meta').find_all('meta', {'id': host}) \n\t\t\tfor host in hosts:\n\t\t\t\tif 'x-xrd-meta-host' in host and host['x-xrd-meta-host'].find('x-xrd-meta-host') > -1:\t\n\t\t\t\t\td = get_url(url=host)\n\t\t\t\t\tsoup = BeautifulSoup(d)\n\t\t\t\t\thost_meta = soup.find"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tmeta = {}\n\t\thosts = host.getElementsByTagName(\"host\")\n\t\tfor host in hosts:\n\t\t\tmeta[host.getAttribute(\"id\")] = True\n\t\treturn meta\n\tdef retrieve_node_metadata(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Node metadata document.\"\"\"\n\t\tmeta = {}\n\t\tnode = host.getElementsByTagName(\"node\")\n\t\tfor node in node:\n\t\t\tmeta[node.getAttribute(\"id\")] = True\n\t\treturn meta\t\n\tdef retrieve_service_metadata(host):\n\t\t\"\"\"\n\t\tRetrieve a remote Service metadata document.\n\t\n:arg host:"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turl = 'http://%s/diaspora/metadata/xml/remote/diaspora-' % host\n\t\tdoc = etree.fromstring(urllib.urlopen(url))\n\t\treturn doc\n\n\t##\n\t# Get Diaspora metadata from remote database.\n\t##\n\t\n\tdef fetch_diaspora_host_meta_metadata(self):\n\t\thost_id_url = '%s/diaspora/metadata/xml/remote/metadata' % self.host\n\t\txml_doc = self.retrieve_xml_from_url(host_id"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tres = XRA.DRIVERS.get('api/diaspora/host-meta', endpoint=host)\n\t\t\n\t\treturn XRA.DRIVERS.get('diaspora/host-meta', endpoint=host, response_key=res.key.id, auth=get_auth_token())\nfrom __future__ import unicode_literals\nimport functools\nimport logging\nimport os\n\nimport re\n\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.core.urlresolvers import reverse, reverse_lazy\nfrom django.http"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tresultClass = dia.DRIA.get_instance()\n\n\t\t# get all remote meta data\n\t\tremote_meta = resultClass.retrieve(hostname=host, ignore_cache=True, ignore_errors=False)\n\t\tremote_meta = remote_meta.drop(\"metadata\", ignore_cache=True, ignore_errors=False)\n\t\tremote_meta = remote_meta.execute()\n\n\t\t# get all metadata document\n\t\tmetadata = remote_meta.get(\"metadata\", drop=True)\n\n\t\treturn(XRD(dra=metadata, host=host))\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\ttry:\n\t\t\t\treturn XRD()\n\t\texcept exceptions.ConnectionFailure:\n\t\t\t\tpass\n\t\tfinally:\n\t\t\tif host:\n\t\t\t\ttry:\n\t\t\t\t\tdom = etree.fromstring(host)\n\t\t\t\t\tfor child in dom:\n\t\t\t\t\t\tif child.tag == 'host':\n\t\t\t\t\t\t\treturn child.find('./metadata/host').tag\n\t\t\t\t\t\t\t\n\t\ttry:\n\t\t\tdom = etree.fromstring(host)\n\t\t\tfor child in dom.iter():\n\t\t\t\tif child.tag =='server':\n\t\t\t\t\t\treturn child\n\t\t\treturn None\n\n\t\texcept xml.parsers.expat.ExpatError as ex"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\txml_path = \"xml/diaspora/%s\" % host\n\t\ttry:\n\t\t\ttree = etree.parse(xml_path)\n\t\t\tdiaspora_host_meta = XMLRPC_DIASPERIAL_HOST_META[tree.getroot()]\n\t\texcept TypeError:\t\t\t\t\t\t\t# pragma: no cover\n\t\t\traise KeyError, \"malformed host meta in '%s'\" % xml_path\n\t\telse:\t\t\t\t\t\t# pragma: no cover\n\t\t\treturn Radidsupervisor._parse_host_meta(diaspora_host_meta)\n\n\tdef"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tmeta_obj = get_connection(\"/rest/diaspora/host-meta/\"+get_host_meta_id_param(host))\n\t\t\n\t\treturn DiasporaMeta(meta_obj, \"d\")\n\ndef retrieve_service_meta(service):\t\"\"\"\n\tRetrieve a remote Service meta document.\n\n:arg service: Service to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tmeta_obj = get_connection(\"/rest/service-meta/\"+get_service_meta_id_param(service))\n\t\t\n\t\treturn RDS(meta_obj"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\n\ttry:\t\t\n\t\t# parse host infomation from Diaspora XML data file\n\t\txml_text = xml_load_from_xml_text(host).xml()\n\t\thost_host = parse_host_info_from_xml_text(host)\n \t\tif \"hostname\" not in xml_text:\n\t\t\treturn None\t\t\t\t\n\t\t\n\treturn host_host\t\t\n\ndef get_host_info(host):\n\t\"\"\"\n\tRetrieve a remote host-meta document from Diaspora XML data file.\n\t\n\t:arg host: Host to retrieve from\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tpass\n\n\treturn session.service('diaspora', HTTP_GET, host=host, output='xml')\n\n\ndef get_host_diaspora_network(host, session):\n\t\"\"\"\n\tGet a HTTP Diaspora network document.\n\n\t\n\t:arg host: Host to retrieve from\n\t:arg session: Session to which the Diaspora network\n\t:returns: ``XRD`` instance\t\"\"\"\n\t\tpass\n\n\treturn session.service('diaspora', HTTP_GET, host=host, output='xml')\n\n\ndef get"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tkwargs['headers'] = kwargs.get('headers', {})\n\t\tkwargs['timeout'] = kwargs.get('timeout', 10)\n\t\ttry:\n\t\t\tif len(kwargs):\n\t\t\t\turl = \"{}?{}\".format(url, urllib.urlencode(kwargs))\n\t\t\tlogging.info(\"Sending document {0}\".format(data))\n\t\t\tresponse = requests.post(url, data, timeout=timeout)\n\t\texcept requests.exceptions.ConnectionError as e:\n\t\t\tif isinstance(e, requests.exceptions.Timeout):\n\t\t\t\traise RuntimeError(_(\"Connect timed out while sending document via POST\"))\n\t\t\traise"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\treturn (requests.put(url, data, headers=data, timeout=timeout),\n\t\t\t\t)\n\n@pytest.fixture(params=params, ids=id_generator(10))\ndef data_url_data(request, tmpdir):\n\treturn data_url_generator(request, tmpdir)\n\n@pytest.fixture(params=params, ids=id_generator(10))\ndef timeout_url_config(request, tmpdir):\n\treturn timeout_url_generator(request, tmpdir)\n\n@pytest.fixture(params=params, ids=id_generator(10"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\treq = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\t\tif not req.ok:\n\t\t\traise APIException(req.content)\n\t\tresponse = req.text\n\t\terrors = req.json()\n\t\tif response and 'error' in errors:\n\t\t\treturn response[0]\n\t\treturn (response, errors)\n\n\tdef submit_document(url, **kwargs):\n\t\t\"\"\"\n\t\tSubmits a document (URL). Takes a :class:`requests.Request` object which is sent to requests.\n\t\t:arg url: Full url"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\ttry:\n\t\t\timport requests\n\t\t\treq = requests.post(url, data, timeout=timeout, headers=request_headers, **kwargs)\n            if method == \"post\":\n\t\t\t\t# POST requests are redirected because you can modify the document.  Don't want to break out of the function.  Also, add headers if specified.\n\t\t\t\treturn req.headers\n\t\t\telse:\n\t\t\t\t# Check response status code\n\t\t\t\tif type(req.status_code) == int:\n\t\t\t\t\t\tresp_headers = { 'content-type' : 'application/x-www-form-urlencoded',\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tdef req(method, data, headers):\n\t\t\treturn self.request(method, url, data, headers, *args, **kwargs)\n\t\tresp = req(\"http://localhost:%d/send\" % self._port, \"POST\", headers)\n\t\tbody = resp.read()\n\n\t\tresp_code = int(resp['code'])\n\t\terror_obj = etree.fromstring(body)\n\n\t\treturn resp_code, error_obj\n\texcept HTTPException as ex:\n\t\tif not isinstance(ex.resp.status, int):\n\t\t\tprint(ex.resp.status,"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tdata = dict(data)\n\t\tif \"text\" in data:\n\t\t\tdata[\"content-type\"] = \"application/x-www-form-urlencoded\"\n\t\tif \"encoding\" in data:\n\t\t\tdata[\"content-type\"] = \"application/x-www-form-urlencoded\"\n\t\tif \"params\" in data:\n\t\t\tparams = urllib.urlencode(data[\"params\"])\n\t\t\tdata[\"params\"] = params\n\t\tif \"redirections\" in data:\n\t\t\tdata[\"redirections\"] = str(int(data[\"redirections\"]) + 1)\n\t\tif \""}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tr = requests.post(url, data=data, timeout=timeout, headers=headers, **kwargs)\n\t\tif r.status_code!= 200:\n\t\t\traise Exception(str(r.status_code) + \" and \" + str(r.text) + \":\" + str(r.response.text) + \" has not been sent.\")\n\t\telse:\n\t\t\treturn r.status_code, r.text\n\n\tdef _check_content_type(self, content_type, data):\n\t\t\"\"\"\n\t\tDetermines if the content is correct or not, with the"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\treturn _send_request(method, url, args, kwargs, timeout)\n\t\t\n\tdef send_multipart(url, data, multipart=1, timeout=10, method=\"post\", *args, **kwargs):\n\t\t\"\"\"\n\tHelper method to send multipart data via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg multipart: Set multipart"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif method not in [\"post\", \"put\"]: return None, None\n\t\tif data is not None: body = data\n\t\telse: body = None\n\t\turl = \"%s%s\" % (base_url, url)\n\t\t#print(\"post with data: %s\" % body)\n\t\theaders = kwargs.get(\"headers\", {})\n\t\tdata = encode_multipart_formdata(data, headers)\n\t\trequest = urllib2.Request(url,\"POST\", body, headers)\n\t\tret = urllib2.urlopen(request).info()\n\t\tif ret[\"http_status"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\treq = requests.post(\n\t\t\turl,\n\t\t\tdata=data.encode(\"utf-8\"),\n\t\t\theaders={\n\t\t\t\t# Default content type\n\t\t\t\t \"Content-Type\": \"application/json\",\n\t\t\t\t\n\t\t\t},\n\t\t\ttimeout=timeout\n\t\t)\n\t\tif req.status_code == requests.codes.ok:\n\t\t\treturn req.json()\n\t\t\t#return req.status_code, str(req.text)\n\t\telse:\n\t\t\traise ValueError(req.text)\n\n\tdef get_document(self, url):\n\t\t\"\"\"\n\t\tReturn a document object"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif keys not in dic:\n\t\t\traise Exception('The nested key %s must exist' % key)\n\t\tfor val in dic[keys]:\n\t\t\tdic[keys].remove(key)\n\t\t\t\n\treturn dic\n\n'''\nthis is a function because we always want to wrap your function into a\ndictionary and remove the outer key from the dic and add their new child\nkeys to this dictionary.\n'''\ndef dict_pop(dic, key, *keys):\n\t\"\"\"\n\tdelete a value from a nested key into a dictionary\n\nthis is the inverse of dict_insert, but"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n\t\n\t# Check if the value is a root key\n\tif isinstance(val, list):\n\t\t# append a new list\n\t\tdict.__insert__(dic, '', key, *keys)\n\t\t# recursively insert the list\n\t\tfor x in val:\n\t\t\tdict.__insert__(dic, x, key, *keys)\n\n\t# Insert nested values in the dictionary recursively\n\tfor key in keys:\n\t\tret = insert(dic, key, 'key1.key2.key3', *keys)\n\t\treturn ret\n\n# -*- coding: utf-8 -*-\n#    This"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n# \tkeys = keys.replace(key, val)\n# \tkeys = list(keys)\n\n\t# \tkeys = sorted(keys)\n# \tkeys.reverse()\n# \tfor k in keys:\n# \t\tprint key\n\n\t# \tprint 'insert keys:' \n\t# \tprint 'keys: '\n\t# \tfor k in keys:\n# \t\tprint str(k)\n\n\t# \tprint 'put new_key...'\n\t# \tdic.insert(new_key, 'val') "}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n\t\n\treturn dict.setdefault(key, val)\n\t\n\n#####################################\nimport os, inspect, shutil\n\nfrom..core import types, utils, ast\n\nclass ASTParser(object):\n\t\n\tdef parse(self, path, **kwargs):\t\t\n\t\t\"\"\"\n\t\textract a parsed AST from a file and return a tree of\n\t\tthe actual parsed ASTs.\n\t\t\n\t\t:param path: string path where to get the parsed AST from\n\t\t:param kwargs: dictionary of keyword arguments\n\t\t\"\"\"\n\t\tif 'kwargs' in kwargs:\n\t\t\tkwargs.update(kwargs['kwargs'])\n\t\t\n\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic[key] = val\n\n\t# insert the value\n\tlist_insert(dic, 'val', 'key1.key2'.split('.'))\n\t\n\t# save the string into dict\n\tdict_save(dic, 'dict_save.txt','saved value in dictionary')\n\n# -*- coding: utf-8 -*-\n#\n# Copyright \u00a9 2012 - 2015 Michal \u010ciha\u0159 <michal@cihar.com>\n#\n# This file is part of Weblate <https://weblate.org/>\n#\n# This program is"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic['val'] = val\n\t\tdic['key'] = key\n\t\tdic['keys'] = keys\n\t\ttry:\n\t\t\tdic['parents'] = [key]\n\t\texcept NameError:\n\t\t\tif isinstance(val, dict):\n\t\t\t\tif key not in dic['keys']:\n\t\t\t\t\tdic['keys'].append(key)\n\t\t\t\t\tdic['parents'] = [key]\n\t\twhile len(dic) > 0 :\n\t\t\tval, key = keys.pop()\n\t\t\tdic[val.strip()] = dict_insert(dict.get(val, {}).get(key, ''), val,"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n#import yaml\n#yaml.safe_dump(dic, dict_dumper, default_flow_style=False, sort_keys=True)\n#from pprint import pprint\n#pprint(dic.iteritems(), indent=2)\n#pprint(dic)\ndef dict_dumper(d, *args, **kwargs):\n\t#print \"DEBUG:: dict_dumper %r len(keys) : \\n %r\" % (type(d), len(keys))\n\tdict_flatten_args = [arg.strip() for arg in args] if args else []\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic1 = dict\n\t\tfor k in keys:\n\t\t\tif k in dic:\n\t\t\t\tdic1[k] = dic[k]\n\t\tif dic1!= val:\n\t\t\treturn\n\t\ti = iter(vals)-1\n\t\tfor k in dic:\n\t\t\tdic1[k] = dic1[i]\t#insert at top level, first key in the dictionary\n\t\titerm = iter(dic.iterkeys())\t#iterate over this key in the dictionary\n\t\tif key in dic and dic[key]==val:\n\t\t\tlst.append(dic1)\n\n\treturn"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tval = key.split('.')\n\t\tif keys:\n\t\t\tval = [] + [key] + val\n\t\tif isinstance(val, basestring):\n\t\t\tlst = [] + val\n\t\telse:\n\t\t\tlst = [val] + val\n\t\t\tlst.insert(0, val)\n\t\tif len(lst):\n\t\t\tdic.setdefault(key, lst)\n\t\t\tdic[key] = lst\n\telif isinstance(dic, dict) and key in dic :\n\t#\t\traise Exception()\n\telse:\n\t\t#\traise Exception('Bad key: '+str(key))"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic = dict(dic)\n\t\tdic = copy.deepcopy(dic)\n\t\tfor key in keys:\n\t\t\tlist_insert(dic, key, key, *keys)\n\n\treturn dic # -*- coding: utf-8 -*-\nimport django.core.validators, django.core.validators.RegexRangeValidator\nfrom django.db import models\nfrom django.db.models import CASCADE\nfrom django.db.models.functions import Lower\nfrom django.core.validators import RegexValidator\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django import"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn CType(\"list\", ListOfFileNamesType, settings_dirs, spec_option)\n\t\t\n@conf\ndef file_type_dict():\n\treturn {\n\t\t'basic' : ['int','str'],\n\t\t'string' : ['str', 'unicode'],\n\t\t'binary' : ['bool','str', 'bool'],\n\t\t'real' : ['float', 'float', 'float'],\n\t\t'string' : ['int', 'int', 'int'],\n\t}\n\n@conf\ndef file_path(path):\n\t'''Return the path to"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\td = {}\n\t\tfor s in settings_dirs:\n\t\t\td[s] = build_complex_type(settings_dirs, spec_option)\n\t\treturn d\n\tlist_of_file_names.signature = _list_of_file_names\n\ndef list_of_directory_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\t\n\td = {}\n\tfor s in settings_dirs:\n\t\tif s is None: continue\n\t\td[s] = build_complex_type(settings"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\t_logger.debug(\"creating IniType complex type %s\" %(spec_option))\n\n\t\ttype_definitions = {\n\t\t\t\t\t\t'real'\t: 'infiniblised',\t# Real\n\t\t\t\t\t\t\t'vector'      :'vector',\t# Real\n\t\t\t\t\t\t\t'vectorx'     :'vector',\t# Real\n\t\t\t\t\t\t\t'vectory'     :'vector',\t# Real\n\t\t\t\t\t\t\t'triangle'     : 'triangle',\t# Real\n\t\t\t\t\t\t\t'hexagonal'   : 'hexagonal',\t# Real\n\t\t\t\t\t\t\t'hexagonal_polygon' : 'hexag"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tall_file_names = {}\n\t\tfor each_file_name in os.listdir(settings_dirs):\n\t\t\tif each_file_name in all_file_names:\n\t\t\t\tcontinue\n\t\t\tall_file_names[each_file_name] = {\n\t\t\t\t\"short_name\": each_file_name.split(os.path.sep) \\\n\t\t\t\t[str(x) for x in os.path.basename(settings_dirs[each_file_name].split(os.path.sep)) \\\n\t\t\t\t]\n\t\t\t}\n\t\treturn all_file"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tout_file_names = {}\n\t\tout_file_names['file_name'] = []\n\t\tout_file_names['in_file'] = []\n\n\t\tfor directory in settings_dirs:\n\t\t\tif spec_option =='spec_file':\n\t\t\t\tspec_option = directory\n\t\t\t\tcontinue\n\n\t\t\tif spec_option.find('.in') >=0:\n\t\t\t\tout_file_names['file_name'].append(directory)\n\t\t\telse:\n\t\t\t\tout_file_names['in_file'].append(directory)\n\t\t\n\t\t# For the INI types, create a"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tif spec_option not in settings_dirs:\n\t\t\tsettings_dirs[spec_option] = {}\n\t\tsettings_dirs[spec_option][section] = {}\n\n\tresult = []\n\tfor sect, keys in settings_dirs.items():\n\t\tif sect not in sections:\n\t\t\tsections[sect] = {}\n\t\tsection_options, section_values = sections[sect], {}\n\t\tfor key, var in keys.items():\n\t\t\tif var == '__all__':\n\t\t\t\tcontinue\n\t\t\tif key == \"__\":\n\t\t\t\tcontinue\n\t\t\tkey = section_option + key\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn create_typespec_file_names(settings_dirs, spec_option)\n\n# --------------------------------------------------------------------------------------------------\n# Classes for IniTypes\n# --------------------------------------------------------------------------------------------------\n\nclass IniTypeBase(object):\n\t\"\"\"\n\tBase class defining the `BaseIniType` class\n\n\tThe `BaseIniType` class will have attributes:\n\n\t\t.. attribute : `str`\n\t\t\tThe type that this IniType is in\n\n\t\t.. attribute : `str`\n\t\t\tThe name (or label) of this IniType (generally taken from `settings_dirs`)\n\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tret_val = []\n\t\tfor f in settings_dirs:\n\t\t\tif os.path.isdir(f):\n\t\t\t\tfor root, dirnames, filenames in os.walk(f):\n\t\t\t\t\tfor f2 in filenames:\n\t\t\t\t\t\tret_val.append(os.path.join(root, f2))\n\t\treturn ret_val \n\n\t##\n\t##\n\t##\tCreates all the inits\n\t##\n\t##\n\tdef create_all_inits(self):\n\t\t\"\"\"\n\t\tCreate a list of all the inits\t\n\t\t\"\"\"\n\t\tall"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tlist_of_files = []\n\t\tfor subdir, dirs, files in os.walk(settings_dirs):\n\t\t\tfor file_name in files:  # list them in reverse order\n\t\t\t\tif (file_name.endswith(spec_option) or file_name.endswith(\".png\") or file_name.endswith(\".jpg\") or file_name.endswith(\".jpeg\")) and any([re.match(r\"\\.{4}\",file_name)]):\n\t\t\t\t\tlist_of_files.append(os.path.join(subdir, file_name))\n\t\treturn list_of_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tassert(isinstance(spec_option, basestring) )\n\t\tassert(os.path.isdir(settings_dirs) )\n\t\tif spec_option!= 'all':\n\t\t\tnames = list()\n\t\t\tfor (src_dir, dst_dir, filenames) in os.walk(settings_dirs):\n\t\t\t\tfor filename in filenames:\n\t\t\t\t\tif filename.startswith('.'):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tfiles = os.path.join(src_dir, filename)\n\t\t\t\t\tnames.append(files)\n\t\t\treturn names\n\t\telse:\n\t\t\treturn []\n\tdef read_file_from_"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tcls.manager = salt.config.manager.__name__\n\t\n\t@classmethod\n\tdef get_config_mgr(cls, args):\n\t\t\"\"\"\n\t\tFinds and returns the config manager instance\n\t\t`@returns: A `config.ConfigParser` object\n\n\t\t\t:returns: A `config.ConfigParser` object\n\t\t\"\"\"\n\t\treturn cls.manager\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t# TODO: Add proper exceptions\n\n\t\tif not HAVE_CONFIG_MANAGER:\n\t\t\traise exceptions.ConfigManagerNotFound()\n\t\tself.manager = self.get_config"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\timport ConfigParser\n\t\timport os\n\t\tcls.__config = ConfigParser.ConfigParser()\n\t\tcls.__config.read(os.path.dirname(__file__) + '/.config/ansible.conf')\n\t\tcls.__config.setdefault('ANSIBLE_GLOBAL', cls.__config.get('DEFAULT', 'ANSIBLE_GLOBAL'))\n\n\tdef _parse_args_and_config_options(cls):\t\"\"\"\n\tGets the CLI options as a list.\n\t\"\"\"\n\t\tparser = ConfigParser(defaults=[('--debug',  default = False), ('cli_args=', None), ('config_parser', None"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls(\"ansible\", ConfigManager())\n\nclass ConfigManager:\n\t\"\"\"\n\tThis class implements the class \"configmanager\".\n\t\"\"\"\n\tdef __init__(self, parent):\n\t\t\"\"\"\n\t\tInitializes a ConfigManager instance.\n\t\t\"\"\"\n\t\tself.parent = parent\n#!/usr/bin/python\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tmanager = {}\n\t\t\n\t\t# load the manager\n\t\toptions={}\n\t\tconfigObj=cls.configObj\n\t\tconfigObj.load()\n\t\tfor name,value in cls.configObj.items():\n\t\t\tif isinstance(value, BaseConfigParser.ConfigParser):\n\t\t\t\t# load the config file as a dictionary or from a ConfigParser\n\t\t\t\toptions[name] = value.read(value._configfile())\n\t\t\telif isinstance(value, (list,tuple)):\n\t\t\t\t# if we have an existing configuration file, use it as a dictionary\n\t\t\t\t# or from a list\n\t\t\t\tmanager[name] = dictionary"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._module.params['config_manager']\n\t\n\tdef is_default_config_manager(self):\n\t\t\"\"\"\n\t\tChecks to see if there is a default config manager installed.\n\t\tReturns:\n\t\t    Boolean if the configuration manager has been installed\n\t\tRaises:\n\t\t    ConfigManagerNotFound if the manager isn't configured, but\n\t\t\tits parent object was accessed from.\n\t\t\"\"\"\n\t\tclist = self.list_default_config_manager()\n\t\tfor c in clist:\n\t\t\tif c is None:\n\t\t\t\tcontinue\n\t\t\tif c.name == self._parent.name"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn get_config_manager().get_config_manager()\n\n@conf.commands.register\ndef command_config(conffiles=(), aliases=(), configfiles =()):\n\t\"\"\"\n\tCommand-line config CLI commands.\n\n\t:param filenames: command-line filename(s) to get option from (i.e.\n\t\t\t\t\t [opt1, opt2, opt3])\n\t\"\"\"\n\tcommands.append(cls.cli_command(conffiles, aliases=aliases, configfiles=configfiles))\n\n@conf.commands.register\n@click.argument('configfiles', metavar"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\ttry:\n\t\t\treturn cls.config_manager\n\t\texcept AttributeError:\n\t\t\traise AttributeError(\"No config manager defined for %s\" % cls.name)\n\n\tdef _get_config_manager(cls, config_dir):\n\t\tif config_dir is None:\n\t\t\tconfig_dir = os.path.join('.', config.get('default.dir', 'etc'))\n\t\t\treturn cls.config_manager = config_manager.config_manager(\n\t\t\t\tcls.config_dir,\n\t\t\t\tconfig.get('default', 'config.yaml'),\n\t\t\t\tos.path.join"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef get_path(self, path):\n\t\t\"\"\"\n\t\tReturn a specific path or None.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef get_module_path(self, module_path):\n\t\t\"\"\"\n\t\tReturn a specified path, or None.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef get_options(self):\n\t\t\"\"\"\n\t\tReturn a list of options to be used for this module.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef get_description(self, module_basename):\n\t\t\"\"\"\n\t\tGenerate a human-readable version of the module"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.manager.configobj()\n\t\n\tdef has_section(self, section):\t\"\"\"\n\tChecks if the section exists in the config.ini. Failing if it does.\n\t\"\"\"\n\t\tsection = getattr(self.parent, section)\n\t\treturn section in self.config.sections()\n\t\n\tdef _create_config_path(self, path=None):\n\t\tconfig_path = '/etc/ansible/ansible.cfg'\n\t\tif not path:\n\t\t\tpath = self.config.get(section='general.vars.path', default=path)\n\t\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tconfig_dir = os.path.join(cls.cfg.dir, 'etc', 'ansible.cfg')\n\t\tname = 'ansible_cfg_manager_' + cls.name\n\t\treturn cls(config_dir, name)\t\n\n\nclass Owner(CBaseConfigManager): pass\t# TODO\n\n\"\"\"\nCopyright (c) 2015 Red Hat, Inc. All Rights Reserved.\n\nThis software may be modified and distributed under the terms\nof the BSD license. See the LICENSE file for details.\n\"\"\"\n\nimport logging\nimport os\n\n\nLOG = logging.getLogger(\"openshift"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls(_workspaceManager)\n\t\t\n\tdef _workspace_manager(self):\t\"\"\"\n\tReturns the current user's current user's workspace manager.\t\"\"\"\n\t\treturn self._workspaceManager.__class__()\n\t\t\n\tdef _workspace_manager_get_active_workspace(self):\n\t\t\"\"\"\n\t\tCreates the current user's active workspace manager.\t\"\"\"\n\t\t\n\t\treturn self._workspaceManager.active()\n\t\n\tdef _workspace_manager_set_active_workspace(self, user_id):\n\t\t\"\"\"\n\t\tRegisters the current user's active workspace manager.\n\t\t\n\t\t:param user_id:"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tfrom..settings import WORKSPACE_MANAGER\n\t\treturn WORKSPACE_MANAGER\n\n\t@classmethod\n\tdef get_application(cls):\n\t\t\"\"\"\n\t\tReturns a reference to the :class:`Application`.\n\t\t\"\"\"\n\t\tfrom..settings import APPLICATION\n\t\treturn APPLICATION\n\n\t@classmethod\n\tdef get_all(cls):\n\t\t\"\"\"\n\t\tReturns all objects in a :class:`Application`.\n\t\t\"\"\"\n\t\treturn cls.all_dbs[cls._appname()]\n\n\t@classmethod\n\tdef set_default_workspace(cls, name):\n\t\t\"\"\"\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._windows._workspace_manager\n\tdef CreateNew() :\n\t\t\"\"\"\n\t\tCreates a new one.\n\t\t@return : W\n\t\"\"\"\n\t\treturn cls(_windows.CreateNew())\n\tdef Open(cls, uri, flags=wglCreateWindowFlags.FULL_SCREEN_OR_CHECK):\n\t\t\"\"\"\n\t\tOpens the application in wgl format.\n\t\t@param uri.\n\t\t@param flags: Initial window flags. Must be set with WGL_CONTEXT_DEFAULT.\n\t\t@return: None\n\t\"\"\"\n\t\twith resource_alloc.Resource"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn wolf.ws(config).workspace_manager()\n\n\n# -----------------------------------------------------------------\nimport datetime, random\nimport time\nfrom datetime import datetime, timedelta\nimport json\nimport os, socket, pickle\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse_lazy\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.db.models import Count\nfrom django.http import HttpResponse\nfrom django.conf.urls import patterns\nfrom django.utils.http import urlencode\nimport simplejson as json\nfrom django.shortcuts import render_to_response\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t return cls.Workspace\n\t\t# return None\n\t\n\tdef add_workspace(self):\n\t\t\"\"\"\n\t\tAdds the workspace.\n\t\t\"\"\"\n\t\t# return\n\t\t\n\tdef list_workspace(self):\n\t\t\"\"\"\n\t\t\tReturns the list of supported objects in the workspace (list of\n\t\t\tsupported objects)\n\t\t\"\"\"\n\t\t# return None\n\t\t\n\tdef save_object_to_db(self, workspace):\t\n\t\t\"\"\"\n\t\tSave obj into the database.\n\t\t\n\t\tArgs:\n\t\t\tworkspace (Workspace): object\n\t\t\"\"\"\n\t\t# Check if obj is supported\n\t\tif isinstance(workspace"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._ws_manager\n\n\tdef set_workspace_manager(cls, manager):\n\t\t\"\"\"\n\t\tSets the workspace manager.\n\t\t\"\"\"\n\t\tcls._ws_manager = manager\n\n\t@property\n\tdef client(self):\n\t\t\"\"\"\n\t\tGets the client.\n\t\t\"\"\"\n\t\treturn self._client\n\n\t@client.setter\n\tdef client(self, client):\n\t\t\"\"\"\n\t\tSets the client.\n\t\t\"\"\"\n\t\tif client is not None:\n\t\t\tself._client = client\n\n\t@property\n\tdef workspace(self):\n\t\t\"\"\"\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls.workspace_managers[cls.workspace][1]\n\t\t\n\tdef clear_workspace_managers(self):\n\t\t\"\"\"\n\t\tClears all the workspace managers\n\t\t\"\"\"\n\t\tfor _i in range(0,3):\n\t\t\t_i=getattr(_instrument, self._instruments.name+\"_\"+str(_i))\n\t\t\tif hasattr(_i,\"workspace_manager\"):\n\t\t\t\t_i.workspace_manager = None\n\t\n\tdef get_workspace_managers(self):\t\"\"\"\n\t\tRetrieves the active workspace manager\n\t\t\"\"\"\n\t\treturn _instrument.workspace_managers\n\n\tdef"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._workspace_manager\n\n\t@property\n\tdef active_workspace(self):\n\t\t\"\"\"\n\t\tA reference to the currently active Workspace object.\n\t\t\"\"\"\n\t\treturn self._active_workspace\n\n\t@property\n\tdef name(self):\n\t\t\"\"\"A reference to the currently active workspace.\n\t\t\"\"\"\n\t\treturn self._name\n\n\t@property\n\tdef is_workspace_manager(self):\n\t\t\"\"\"\n\t\tWhether this workspace manager is active.\n\t\t\"\"\"\n\t\treturn self._is_workspace_manager\n\n\tdef get_workspace_by_name("}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\traise Exception('This must be implemented by child classes')\n\t\timport copy\n\nfrom django.template import Library, Node, Origin, TemplateSyntaxError\nfrom django.utils import html\n\n\nregister = Library()\n\n\n@register.tag\ndef page_cache(parser, token):\n    \"\"\"\n    Sets the page cache backend to a cache instance.\n\n    Usage::\n\n      {% page_cache 'page.html' %}\n      {% page_cache 'page.html' 'page.html.changed_count' with 2 as changes %}\n    \"\"\"\n    bits = token.split_contents()\n\n    if len(bits)!="}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._workspace_manager\n\n\t@workspace_manager.setter\n\tdef workspace_manager(self, workspace_manager):\t\"\"\"\n\t\tSets the workspace manager.\t\n\n\t\tArgs:\n\t\t\t* workspace_manager: a WorkspaceManager instance.\n\n\t\tRaises:\n\t\t\t* AttributeError if not a WorkspaceManager instance.\n\t\t\"\"\"\n\t\tif isinstance(workspace_manager, WorkspaceManager):\n\t\t\tself._workspace_manager = workspace_manager\n\t\telse:\n\t\t\ttry:\n\t\t\t\tself._workspace_manager = WorkspaceManager(self)\n\t\t\texcept AttributeError:\n\t\t\t\traise"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.plugins.get(True)\n\n\t@classmethod\n\tdef get_all_plugins(cls):\n\t\treturn cls.plugins.values()\n\n\t@classmethod\n\tdef has_plugin(cls, plugin, settings=None):\n\t\t\"\"\"\n\t\tReturns a boolean indicating if there is a plugin.\n\t\t\"\"\"\n\n\t\treturn cls.plugins.get(plugin)\n\n\tdef get_plugin(self, plugin):\n\t\t\"\"\"\n\t\tReturns the plugin for the given plugin.\n\t\t\"\"\"\n\t\treturn self.plugins[plugin]\n\n\tdef get_plugin_by_id"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugin_manager\n\t\n\t@property\n\tdef plugins_names(cls):\n\t\t\"\"\"\n\t\tGets the names of all plugins.\n\t\t\"\"\"\n\t\treturn cls._plugins_names\n\t\n\t@property\n\tdef plugins_dir(cls):\n\t\t\"\"\"\n\t\tGets the directory path of all plugins.\n\t\t\"\"\"\n\t\treturn cls._plugins_dir\n\t\n\t@property\n\tdef plugin_manager(self):\n\t\t\"\"\"\n\t\tGets the plugin manager.\n\t\t\"\"\"\n\t\treturn self._plugin_manager\n\t\n\t@property\n\tdef plugins_manager(self):\n\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn {\n\t\t\t\t'status': cls.status,\n\t\t\t\t'dist': {\n\t\t\t\t\t\t'plugin': cls.name,\n\t\t\t\t\t\t'status': cls.status,\n\t\t\t\t\t\t'id': cls.id\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t@classmethod\n\tdef get_list(cls):\t\t\n\t\t\"\"\"\n\t\tReturns the plugin list to be added.\n\t\"\"\"\n\t\tplugin_manager = cls.get_plugin_manager()\n\t\tlist = cls.plugins.keys()\n\t\tfor plugin in list:\n\t\t\tif plugin not in cls.plugins:\n\t\t\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\ttry:\n\t\t\tfrom Plugins.Extensions.Infopanel.Inhibit.plugin import InhibitPlugins\n\t\t\treturn InhibitPlugins, InhibitPlugins.getInstance()\n\t\texcept:\n\t\t\tcls._errorMessage(\"PluginManager\", \"Error getting plugins\")\n\t\t\treturn None\n\tdef reload_settings(self):\n\t\t\"\"\"\n\t\tReload all settings. The code uses the plugin manager in order to\n\t\tevaluate the contents of other plugins. If they are in fact\n\t\tused, their settings will be recovered in-place.\n\t\t\"\"\"\n\t\tself._reloadPlugins()\n\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugins\n\n\t@classmethod\n\tdef get_plugin_manager(cls):\t\"\"\"\n\t\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugin_manager\n\t\n\t@classmethod\n\tdef get_plugin_base_class(cls):\t\"\"\"\n\t\tGets the plugin base class.\n\t\t\"\"\"\n\t\treturn cls.__plugin_base_class\n\t\n\t@classmethod\n\tdef get_plugins(cls):\n\t\t\"\"\"\n\t\tGets a list of plugin objects.\n\t\t\"\"\"\n\t\treturn cls._plugins\n\n\t@classmethod\n\tdef get_configuration(cls):\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugins_manager\n\n\tdef register(self, instance):\n\t\t\"\"\"\n\t\tRegisters an instance for the plugin manager. This is used internally\n\t\tby the `register` keyword to get a plugin instance by passing it\n\t\tto the base `register` method and not the instance itself.\n\n\t\t:param instance: The plugin instance for which this plugin manager is\n\t\t\tregistered.\n\t\t\"\"\"\n\t\tself._plugins_manager[instance] = instance\n\n\tdef unregister(self, instance):\n\t\t\"\"\"\n\t\tUnregisters an instance for the plugin manager. This should be called\n\t\tfrom"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tdef plugin_manager():\t# Plugin manager\n\t\t\treturn cls._plugins.default()\n\n\t\treturn plugin_manager\n\n\t@classmethod\n\tdef allPlugins(cls):\n\t\t\"\"\"\n\t\tGets the plugin manager.\n\t\t\"\"\"\n\t\tif cls._plugins is None:\n\t\t\tcls._init()\n\n\t\treturn cls._plugins.all()\n\n\t@classmethod\n\tdef initPlugins(cls):\n\t\t\"\"\"\n\t\tInitializes the plugin manager.\n\t\t\"\"\"\n\t\tcls._plugins         = PluginManager().getAllPlugins()\t# Plugin manager\n\n\t\t# Plugins loading\n\n\t\t# All Plugins\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn settings().getPluginManager().pluginManager\n\n\t@classmethod\n\tdef is_plugin_installed(cls, name):\n\t\t\"\"\"\n\t\tCheck whether a plugin name can be installed.\n\n\t\t:param name: plugin name\n\t\t:type name: string\n\n\t\t:returns: True if plugin name can be installed\n\t\t:rtype: boolean\n\t\t\"\"\"\n\t\tfor plugin in cls.plugins:\n\t\t\tif plugin.name == name:\n\t\t\t\treturn True\n\t\treturn False\nimport logging\nfrom typing import Mapping, Optional\n\nimport numpy as np\nimport pycbc.types as t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tname = getattr(cls, \"PluginManager\", \"\")\n\t\treturn getattr(cls, \"PluginManager\", PluginManager)()\n\n\nclass ServiceBase(object):\n\tdef __init__(self, service_properties):\n\t\tself.base_path = os.path.join(\".\")\n\t\tself.__config = yaml.load(os.path.join(self.base_path, \"config.yaml\"))\n\t\tself.__config.update(self.__config.get(\"configuration_manager_properties\", {}))\n\t\t\n\t\tself.update_base_path()\n\t\t\n\t@classmethod\n\tdef update_"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn get_plugin_manager(cls)\n\nclass PostPluginManager(PluginManager):\n\t\"\"\"\n\tGet the plugin manager for the user's posts. Add one with the url, get the plugin, and get the\n\tcomment for that post.\n\t\"\"\"\n\t\n\n\n\tdef init_plugin(self):\n\t\tif not hasattr(self.local_site, 'posts') or not hasattr(self.local_site, 'getposts'):\n\t\t\traise Exception(\"The PostPluginManager has not been initialized\")\n\t\tif not hasattr(self.post_site, 'items') or not"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content:\n\t\t\t# TODO: handle more complicated data\n\t\t\tvalidate_yaml_contents(spec_content)\n\n\t\terrors = []\n\t\tfor fname, obj in cls.objects.all().order_by('modified'):\n\t\t\terrors.append('Missing %s for field [%s]' % (fname, obj.name))\n\n\t\tif errors:\n\t\t\traise IRValidationErrorException('\\n'.join(errors))\n\n\t\treturn {}\n\n\tdef load_from_file(self, fname):\n\t\tif self._is_valid_spec(fname):\n\t\t\t# validate file\n\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\t\n\tvalidator = None\n\t\n\tif spec_content:\n\t\twith io.open(spec_content[0], encoding='utf-8') as spec_content_file:\n\t\t\treturn validator.validate(spec_content_file)\n\t\n\traise IRValidationErrorException(\"missing required fields\")\n\t\t\ndef validate(args=None, validate_type=None):\n\t\"\"\"\n\tValidates the given arguments and returns the result. The arguments can be a flat\n\tlist or None. \n\t:param args: list or None\n\t:param validate_type: None or list/tuple,"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n#################################################################################################\n#################################################################################################\nclass Spec(object):\n\t\"\"\"\n\tA minimal YAML spec that provides a user interface.\n\t\"\"\"\n\tdef __init__(self, content, file_path=None):\n\t\tself._parser = self.__init__()\n\t\tself.content = content\n\t\tfile_path = file_path or ''\n\t\tself.file_path = file_path\n\n\t\tself._options = {}\n\n\tdef __copy__(self):\n\t\treturn Spec(\n\t\t\tcontent=self.content,\n\t\t\tfile_path=self.file_path,\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content and spec_content and spec_content.startswith(b'\\n'):\n\t\t\t# only if the spec file needs an unstripped line\n\t\t\t# (we can still use newlines in YAML)\n\t\t\treturn yaml.safe_load(spec_content)\n\t\telse:\n\t\t\t# otherwise we fail with exception\n\t\t\traise IRValidationErrorException\"\"\"Test suite for the Honeywell cloud API methods.\"\"\"\n\n\nimport json\nfrom unittest import TestCase\n\nimport requests_mock\n\nfrom requests.exceptions import HTTPError as RequestsHTTPError\n\nimport responses\nfrom httmock import stubbed\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t \n\t# validate if the provided spec is a spec file\n\tdef validate_json(spec_content):\n\t\ttry:\n\t\t\twith open(spec_content, 'r') as spec_file:\n\t\t\t\tspec = yaml.safe_load(spec_file)\n\t\t\treturn spec\n\t\texcept yaml.YAMLError as e:\n\t\t\tlog.critical('Error loading spec from the specified text file: %s'%e)\n\t\t\treturn {}\t\n\t\n\t# validate a specification against the content of the spec file\n\tdef validate_spec(obj):\n\t\t\"\"\"\n\t\tvalidates that every"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\n\nclass RDFA(RDFABase):\n\t\"\"\"\n\tThis is a base class for RDFA objects.\t\n\tIt's inspired by the 'python' library, and can be used as a simple base type in\n\ta 'class of RDFABase'.\n\t\"\"\"\n\tdef __init__(self, **data):\n\t\tassert 'id' in data, 'invalid RDF object ID'\n\t\tself.id = data['id']\n\t\tself.version = data['version']\n\t\tself.file_content = []\n\t\t\n\t\tfor field in self"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None:\n\t\t\t\traise IRValidatorException(\"spec_content must be set\")\n\t\tif spec_content.startswith('http://localhost:8080/'):\n\t\t\ttry:\n\t\t\t\tspec_content = urlopen(\"http://localhost:8080/\").read().decode('ascii')\n\t\t\texcept IOError:\n\t\t\t\traise IRValidatorException(\"spec_content doesn't seem to exists!\")\n\t\ttry:\n\t\t\tspec_content = spec_content.strip('\\n').replace(' ', '').replace(\"'\", '').replace('\"', '').encode('unicode-escape')\n\t\texcept:\n\t\t\traise IR"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n# get all spec files from the spec source file\ndef get_spec(spec_source):\n\timport os, os.path\n\tfor filename in os.listdir(spec_source):\n\t\tfull_filename = os.path.join(spec_source, filename)\n\t\ttry:\n\t\t\twith open(full_filename, \"r\") as fd:\n\t\t\t\tdoc = yaml.load(fd)\n\t\t\tif doc[\"version\"] == spec_version:\n\t\t\t\treturn doc\n\n\t\texcept yaml.YAMLError as e:\n\t\t\t# if non-YAML error, just continue on loading"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\n\t\n\tif spec_content is None:\n\t\traise IRValidationErrorException(\"spec file must contain the required data\")\n\t\n\treturn cls(spec_content)\n\ndef validate_file(cls, file_uri=None):\n\t\"\"\"\n\tValidates that file uri has all required fields\n\n\t:param file_uri: file uri\n\t:raise IRValidationErrorException: when missing fields\n\n\t:return: File object parsed from uri (file uri is a list only)\n\t\"\"\"\n\tif not file_uri:\n\t\traise IRValidationErrorException(\"Missing value for uri file\")"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None:\n\t\t\tspec_content = frappe.get_doc('File', spec_file)\n\t\tif not spec_content.is_valid():\n\t\t\traise IRValidationError('This file is invalid or has the wrong format')\n\t\t\n\t\tcontent_dict = dict({\n\t\t\t'ref': spec_content.get_field('content').value,\n\t\t\t'is_file': spec_content.format == 'csv',\n\t\t\t'is_folder': spec_content.is_folder,\n\t\t\t'is_file_web': is_file_web,\n\t\t\t"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n\t\n\tif cls.__doc__:\n\t\tif cls.__doc__.endswith(\"SCHEMA\"):\n\t\t\treturn cls.__doc__.rstrip(\".schema\")\n\t\telse:\n\t\t\traise IRValidatorException(cls.__doc__)\n\t\n\n\tif yaml_file:\n\t\twith open(yaml_file, \"r\") as f_fd:\n\t\t\treturn cls._validate_yaml(yaml_fd)\n\t\n\t\t\n\tif cls.__doc__:\n\t\ttry:\n\t\t\treturn cls.__doc__.decode(\"utf-8\")\n\t\texcept UnicodeDecodeError:\n\t\t\n\t\t\traise IRValidatorException(\"no decoding for file '{0"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n# ----------------------------\t\n\n# ------------------------------------------------------------\n# read metadata and validate\n# -------------\n\n# check the given file exists\ntry:\n\t_validate_file(yaml_file)\nexcept IRError:\n\t_logger.warning(\"File is absent in YAML file, will use defaults and validate data\")\n\treturn {}\t\n\n\n# check the schema\ntry:\n\tschema = _parse_yaml_schema(yaml_file)\nexcept IRError as e:\n\t\t_logger.error(e.message)\n\t\treturn\n\n\n# check all required fields are"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif not yaml_file:\n\t\t\tyaml_file = cls.get_file()\n\t\tif not os.path.isfile(yaml_file):\n\t\t\traise IRValidatorException(\"Could not find '%s' in '%s'\" % (yaml_file, Config.cfg_file))\n\t\tvalidated_data = yaml.load(yaml_file)\t\n\t\tif validated_data.get(\"required\",False):\n\t\t\traise IRValidatorException(\"This validation should only be set in a \" + \\\n\t\t\t\t\t\"one instance at a time\")\n\t\telse:\n\t\t\tdict_missing"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file:\n\t\t\ttry:\n\t\t\t\tyaml_file.read()\n\t\t\texcept:\n\t\t\t\traise\n\n\t\t\tfor key in cls.all_fields().keys():\n\t\t\t\tif key in cls.required_fields():\n\t\t\t\t\tif not cls.remove_required:\n\t\t\t\t\t\traise IRValidationError('Missing required fields: \"{}'.format(key))\n\t\t\t\tif not cls.field_exists(key):\t\t\t\t\n\t\t\traise IRValidationError('Missing required field: \"{}\"'.format(key))\n\n\t\treturn cls\n\n\tdef create_new_object_from_yaml(cls, yaml_file"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\ttry:\n\t\t\tobj = cls.from_YAML(yaml_file)\t\t\t\t# validate the file\n\t\t\treturn obj\t\t\t\t# validate that the file has been loaded\n\t\texcept ValueError as e:\n\t\t\tcls._logger.error(\"Invalid data '%s': %s\", yaml_file, e)\n\t\t\traise IRValidationErrorException()\n\t\t\n\t@classmethod\n\tdef validate_from_yaml(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a single YAML file has at least at least one required and a list of required fields\n\n\tIf no provided `yaml"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tdata = {}\n\t\tif yaml_file:\n\t\t\twith open(yaml_file) as yl:\n\t\t\t\tdata = yl.read()\n\t\t\n\t\tfor field, value in cls._data:\n\t\t\tif not field in values:\n\t\t\t\traise IRValidatorException(\"Missing required field '{0}'. {1}\".format(field, values[field]))\n\t\t\tif field in cls._required:\n\t\t\t\tbreak\n\t\t\n\t\treturn data\n\n\tdef validator(self, value):\n\t\t\"\"\"\n\t\tValidates the value against the validator's input\n\t\t:param value: Required value to validate against\n\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\n#\tdef __init__(self, yaml_file, strict=True):\n#\t\tIRValidator.__init__(self, strict)\n\n#\tdef validate_from_yaml(self, yaml_data):\n#\t\t# check if a validation rule is there\n#\t\tif yaml_data.get('hasRequiredAttributes'):\n#\t\t\tfor required in yaml_data.get('requiredAttributes'):\n#\t\t\t\tif required.get('kind') not in self.__VALID_KIND__:\n#\t\t\t\t\traise IRValidationErrorException('This validator schema requires a valid Kind"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file == '':\t\n\t\t\traise IRValidationErrorException(\"Missing YAML file!\")\n\t\t\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as f:\n\t\t\t\tdata = yaml.load(f)\n\t\t\t\tif'schema' not in data:\n\t\t\t\t\traise IRValidationErrorException(\"Missing YAML schema.\")\n\t\t\t\ttry:\n\t\t\t\t\tschema_obj = data['schema']\n\t\t\t\texcept KeyError:\n\t\t\t\t\tlogger.error('Invalid schema definition for \"%s\".' % data['name'])\n\t\t\t\t\traise IRValidationErrorException(\"Missing YAML schema.\")\n\t\t\t\tif isinstance(schema_obj"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file is None:\n\t\t\tyaml_file = os.path.join(cls.DEFAULT_TEMPLATE_PATH, 'rni_config_loader.yaml')\n\n\t\tif yaml_file.endswith('.yml'):\n\t\t\ttry:\n\t\t\t\tparser = SafeConfigParser()\n\t\t\t\t\tparser.read(yaml_file)\n\t\t\t\t\tvalidate_from_yaml(parser)\n\t\t\t\t\tyaml_file = os.path.join(cls.DEFAULT_TEMPLATE_PATH, 'rni_config_loader.yml')\n\t\t\texcept yaml.ParserError as x:\n\t\t\t\traise IRValidatorException(\n\t\t\t\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif cfg['type'] =='schema':\n\t\t\tfor schema_obj in cfg['models']:\n\t\t\t\tif 'name' in schema_obj:\n\t\t\t\t\tif 'type' not in schema_obj:\n\t\t\t\t\t\tmsg = 'Missing required field \\'type\\' key! '\n\t\t\t\t\t\traise IRValidationError(msg)\n\t\t\t\t\tif schema_obj['type'] not in cls.schema_type_names:\n\t\t\t\t\t\tmsg = 'Invalid schema name, {0} is not supported.'.format(schema_obj['type'])\t\n\t\t\t\t\t\traise IRValidationError(msg)\n\t\t\t\t\telif schema_obj['type"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude_dict_keys = []\t\n\t\texclude_keys = []\n\t\tfor key in parser_dict:\n\t\t\tif not self._include_groups_key.has_key(key):\n\t\t\t\tinclude_dict_keys.append(key)\n\t\t\n\t\t\tif key in self._include_groups_re.keys():\n\t\t\t\texclude_keys.append(key)\n\n\t\tinclude_dict_keys.append( self._exclude_keys )\n\t\texclude_dict_keys = []\t\n\t\tfor key in include_dict_keys:\n\t\t\tinclude_dict_match = self._"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tinclude_group= {}\n\t\tfor path in self.files:\n\t\t\tif os.path.isfile(path):\n\t\t\t\tfor line in open(path, \"r\").readlines():\n\t\t\t\t\tm = re.search(\"\\[(\\s*include)(\\s*(.*?)\\]\", line)\n\t\t\t\t\tif not m: continue\n\t\t\t\t\tinclude_group[m.group(1)]= m.group(2)\n\t\treturn include_group\t# This is a generator\n\n\tdef process_include_group_section(self):\n\t\tself.check_includes()\n\t\tself.scan_includes()"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tself._includes = parser_dict.get(\"include\",\".\")\n\t\tself._group_regex = re.compile(self._group_regex,\"(..*?)\\s*\\|(include|\\\")(.*)$\")\n\n\tdef _update_excludes(self, parser_dict):\n\t\t\"\"\"\n\t\tChecks that the user specified blacklist includes from the user's\n\t\tspecification. If it is, removes any entries from the group_regex\n\t\tlist.\n\t\t\"\"\"\n\t\tblacklist_config = parser_dict.get(\"blacklist\",\"[all]\")\n\n\t\tif blacklist_config == \"[all]\":\n\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfor (i, group_items) in enumerate(parser_dict['groups']):\n\t\t\ti_to_add = i in ('includegroup', 'excludegroup', 'header',\n\t\t\t\t'include_dir', 'line_number', 'include_file', 'exclude_file',\n\t\t\t\t'group', 'line')\n\t\t\tif 'include%i' % i in group_items:\n\t\t\t\tparser_dict['include%i' % i].update(_include_group(group_items))\n\t\t\t\tparser_dict['exclude%i' % i] = _exclude_group(group_"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif 'EXTRA_INCLUDE_GROUPS' not in parser_dict:\n\t\t\tparser_dict['EXTRA_INCLUDE_GROUPS'] = {}\n\t\tparser_dict['EXTRA_INCLUDE_GROUPS'].update(\n\t\t\tself.extra_include_groups)\n\t\tparser_dict['INCLUDES'] = parser_dict['EXTRA_INCLUDE_GROUPS'].pop('INCLUDES')\n\t\tparser_dict['DIRS'] = parser_dict['EXTRA_INCLUDE_GROUPS'].pop('DIRS')\n\n\tdef _parse(self):\n\t\t\"\"\"\n\t\tParse every possible include directive.\n\t\t\"\"\"\n\t\tif self."}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# Parse the spec file.\n\t\tfor key, group_def in parser_dict['include_groups'].items():\n\t\t\tparser_dict[key] = eval(group_def)\n\n\t'''\n\tFinds the relative imports (incl. py_modules) of a file.\n\tReturns tuples (relative_from, relative_to), where:\n \t1. Relative import names that are relative to <module>.xml\n \t2. Relative import names that have the same name as <module>.xml (includes the same absolute path).\n\t3. Absolute import paths from <module> to"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tgroups = yaml.safe_load(parser_dict)\n\t\tinclude_names = {}\n\t\tif groups.get('include'):\n\t\t\tfor group in groups['include']:\n\t\t\t\tif not group in self._parsed_include_groups:\n\t\t\t\t\tself._parsed_include_groups.append(group)\n\t\t\t\t\tinclude_names[group] = self._parsed_include_groups\n\t\treturn include_names\n\n#\n# Parse the spec files of the `PIP-6.4.2 spec file\n# <http://www.pjlotag.org/specs/rfc/rfc"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tassert parser_dict is not None, \"missing groups not accepted\"\n\t\tfor group_name, group_dict in parser_dict.iteritems():\n\t\t\t# remove whitespace from the group name\n\t\t\tgroup_name = group_name.strip()\n\t\t\tgroup_name = group_name.replace(' ', '_')\n\n\t\t\tif group_name not in self.include_groups:\n\t\t\t\tparser_dict[group_name] = {}\n\n\t\t\tparser_dict[group_name]['group'] = self.include_groups[group_name]\n\n\t\treturn parser_dict\n\n\tdef process_build"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tdef _include(template_data, template_lines):\n\t\t\ttemplate_data = template_data % { 'file' : template_lines[0] }\n\t\t\tif not template_data.endswith(template_lines[-1]):\n\t\t\t\treturn template_data\n\t\t\twith open(self.__include_file, 'r') as f:\n\t\t\t\ttemplate_data = f.read().strip()\n\t\t\tif template_data.endswith(template_lines[-1]):\n\t\t\t\ttemplate_lines.pop()\n\t\t\t\t\n\t\tself.__include_sections = [k.split(',')[0] for k"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfrom nikola.nodes.inline_file import InlineFile\n\t\tparser_dict = ast.literal_eval(parser_dict)\n\t\tif (\n\t\t\t(parser_dict.pop('regex', '').strip() and (\n\t\t\t\t'regex' in parser_dict.keys()\n\t\t\t\tand parser_dict['regex']!= ''\n\t\t\t))):\n\t\t\tadd_paths = lambda x: os.path.join(\n\t\t\t\tparser_dict['directory'], x, 'include_paths.txt'\n\t\t\t)\n\t\t\tparser_dict['regex'] ='re.compile( \\s*%"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tfor key, value in self.extra_spec_defaults.items():\n\t\t\tif not key in self.defaults:\n\t\t\t\ttry:\n\t\t\t\t\tself.defaults[key] = resolve_argument(value, self.root)\n\t\t\t\texcept frappe.MandatoryFieldError:\n\t\t\t\t\tpass\n\t\tself.defaults.update(self.get_defaults_from(include_missing=False))\n\n\tdef get_defaults_from(self, include_missing=False):\n\t\tif not self.defaults:\n\t\t\treturn {}\n\n\t\t# do not allow \"include\" argument, instead set from root"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn SpectrumSpec(spec_defaults=dict(\n\t\t\tspec_defaults=extract_defaults(self.spec_defaults),\n\t\t\tsource_defaults=extract_defaults(self.source_defaults),\n\t\t\tsource=self.source,\n\t\t\tsub_spec_defaults=extract_defaults(self.sub_spec_defaults),\n\t\t\tsub_source_defaults=extract_defaults(self.sub_source_defaults),\n\t\t\tsub_sources=self.sub_sources,\n\t\t\t\tspec_sources=get_spec_sources(self.spec_sources),\n\t\t\tsub_spec_"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\ttry:\n\t\t\tself.__build()\n\t\t\tassert os.access(self.name, os.X_OK)\n\t\texcept OSError as e:\n\t\t\traise InvalidPackageError(e)\n\n\tdef __build(self):\n\t\t\"\"\"\n\t\tBuilds the package.\t\n\t\t\"\"\"\n\t\tself.spec = get_spec_from_specs(self.name, self.default_spec_path, self)\n\t\tself.name = self.spec.name\n\t\tself.__init_hooks_dict()\n\t\tself.__call_hooks_list()\n\t\t# If we should"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tsrc_args = []\n\t\tsrc_opt_args = []\n\t\tfor v in self.specs:\n\t\t\tif(isinstance(v, basestring) and v.strip()!=''):\n\t\t\t\tsrc_args.append(v)\n\t\t\telif( isinstance(v, list) and isinstance(v[0], basestring) and isinstance(v[1], basestring) ): # single list\n\t\t\t\tsrc_opt_args.extend(v)\n\t\t\telse:\n\t\t\t\tsrc_args.append(v)\n\n\t\tif src_args:\n\t\t\t(src_defaults, src_args_new"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tfor e in spec_items:\n\t\t\tif isinstance(e, dict):\n\t\t\t\tfor k,v in e.items():\n\t\t\t\t\tself.set_defaults(e.get(k,{}).get(v))\n\t\t\telif isinstance(e, list):\n\t\t\t\tfor e in e:\n\t\t\t\t\tself.set_defaults(e)\n\n\tdef get_config_files(self, path):\n\t\t\"\"\"\n\t\tFind the.spec file, which references the module, and return True if it exists.\n\t\tThe config files have precedence over local source.\n\t\t\"\"\"\n\t\tpath ="}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.type = spec.find('=')\n\t\tif not self.type:\n\t\t\tself.type = self.DEFAULT_TYPE\n\t\tself.default = spec[self.type:].rstrip()\n\n\tdef get_args(self, conf):\n\t\targs = self._read_config(\n\t\t\tconf=conf,\n\t\t\tdefault=self.default)\n\n\t\tif self.type == 'dir':\n\t\t\targs = [self.default, '-p', self.path] + args\n\t\telif self.type == 'file':\n\t\t\targs = [self.default, '-c', '-"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tfor spec, spec_default in self._spec_defaults.items():\n\t\t\tfor source in spec_default:\n\t\t\t\tparams = spec.split(' ', 2)\n\t\t\t\tname = source.upper().replace('-', '_')\n\t\t\t\tsource = source.replace('-', '_')\n\t\t\t\tif 'default' not in spec_default:\n\t\t\t\t\traise ValueError('Unknown parameter \"%s\". Using the default value for parameter \"%s\"' % (name, name))\n\t\t\t\ttry:\n\t\t\t\t\tindex = params[0].index('(')\n\t\t\t\t\tdefault, _ = range(index + 1, len(params), index + 2)"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.sources = []\n\t\ttry:\n\t\t\treturn self.read_default_sources(self.default_sources)\n\t\texcept (IOError, ValueError):\n\t\t\tpass\n\t\tself.parse_config_file(self.default_sources)\n\t\tself.default_sources = []\n\n\tdef read_default_sources(self, sources=None):\n\t\t\"\"\"\n\t\tParse defaults based on user configuration.\n\n\t\t:param sources: Default sources. Defaults to None, as if\n\t\t\tuser-supplied defaults are specified but no sources are specified.\n\t\t\"\"\"\n\t\tif sources is None"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tparser = self.get_parser(self.get_path())\n\t\tfor option, value in self.options.items():\n\t\t\tif isinstance(value, str):\n\t\t\t\tparser.values.add_value(option, value)\n\n\tdef get_option_default(self, option, action=None, default=None):\n\t\t\"\"\"\n\t\tResolve option's default and return a single value if one was\n\t\treceived.\n\t\tReturns None if no default is specified\n\t\t\"\"\"\n\t\tif action is None:\n\t\t\toption = self._get_option(option)\n\t\treturn self"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tdefaults = {}\n\n\t\tif c_source[0] == 'python':\n\t\t\t# python-config was used before\n\t\t\t# it was used by a plugin like the --python-config option itself\n\t\t\t# where the python plugin will be loaded from.\n\t\t\t# For now, it is a list of strings, where each item represents a\n\t\t\t# shell variable, the first item is the name of its first argument,\n\t\t\t# and the second item is an environment variable, etc.\n\t\t\t#\n\t\t\t# See \"variables\" in source.py for more detailed details:\n\t\t\t# https://"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdef deprecated_options(self):\n\t\t\t\"\"\"\n\t\t\tThe deprecated options used by this package.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\treturn {\n\t\t\t\t\"all_of_others\": self.all_of_others,\n\t\t\t\t\"force_upgrade\": self.force_upgrade\n\t\t\t}\n\t\treturn deprecated_options\n\n#!/usr/bin/env python\n# File created on 2015-12-17 by GNS3\n#\n#  Copyright 2016 Netcinnami\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\targs = { k: getattr(self, k) for k, v in self.__dict__.iteritems() if v is not None }\n\t\treturn self.__class__.__name__ + \" -- deprecated arguments: {}\".format(\n\t\t\tjson.dumps(args))\n\n\n##########\n#   CLASS-NAME:   get_default_value\n##########\n\ndef get_default_value(default):\n\t\"\"\"\n\tGet the default value of the current instance of this callable.\n\n\tIf we change default_value to be None, it is computed based on the\n\tdefault_value_from"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn { 'description': self.__doc__ }\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom datetime import date\n\nfrom django.core.exceptions import ValidationError\nfrom django.utils.translation import ugettext, ugettext_lazy as _\n\nfrom cms.exceptions import MultipleObjectReturned, PluginBase404\nfrom cms.models import Page\nfrom cms.plugin_base import CMSPluginBase\nfrom cms.plugin_pool import plugin_pool\nfrom cms.utils.treehelpers import (get_placeholder_ids, get_page_id,\n                                       get"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {\n\t\t\t\"args\"     : {\n\t\t\t\t\"deprecated\" : {\"default\" : {\n\t\t\t\t \t\"dest\"   : \"module\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tdef register_arguments(self, parser):\n\t\t# register 'args'-style arguments\n\t\tparser.add_argument(\n\t\t\t\"args\",\n\t\t\tnargs=\"*\",\n\t\t\thelp = \"Argument(s) to append to deprecated.\",\n\t\t\tmetavar = \"ARG\"\n\t\t)\n\n\t\treturn parser.get_subparsers(\n\t\t\tdest = \"add\",\n\t\t\tmetavar = \"ARG\","}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {\"new_value\": self.new_value, \"new_default\": self.new_default,\n\t\t        \"new_help\": self.new_help, \"old_value\": self.old_value,\n\t\t        \"old_default\": self.old_default, \"old_help\": self.old_help}\n\nclass OptionContainer(object):\n\n\tdef __init__(self, name, description):\n\n\t\tself.name = name\n\t\tself.description = description\n\t\tself.add_option(self.name, \"\")\n\t\tself.add_option(self"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\targs = {}\n\n\t\t# TODO: Use dict.clear(args) here.\n\t\tfor k,v in vars(self):\n\t\t\tif k in ('options',):\n\t\t\t\targs[k] = v\n\n\t\treturn args\n\n\tdef __repr__(self):\n\t\treturn self.get_deprecated_args()\n\n\tdef to_dict(self):\n\t\td = vars(self)\n\n\t\tdeprecated_args = get_deprecated_args()\n\n\t\td[\"options\"] = self[\"options\"]\n\t\td[\"args\"] = self[\"args\"]\n\n\t\treturn d\n\ndef create"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated = {}\n\t\tfor arg in dir(self):\n\t\t\tif not self.do_not_deprecate(arg):\n\t\t\t\tdeprecated[arg] = getattr(self, arg)\n\t\treturn deprecated\nfrom testframework.base import OsfTestCase\nfrom tests import factories as factories_factories\nfrom tests.factories import AuthUserFactory, UserFactory\nfrom osf.models import Identifier, Tag\n\n\nclass TestNodeTaggingFactory(OsfTestCase):\n\n    def test_tag(self):\n        user = AuthUserFactory()\n        project = ProjectFactory(creator=user)\n        tag = TagFactory(\n            node"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tif self.options.use_deprecated:\n\t\t\tfor section in self.options.deprecated:\n\t\t\t\tdeprecated_options = getattr(self, section, {})\n\t\t\t\tdeprecated_options.update(compat_parse_requirements(self.options.deprecated_options))\n\t\t\t\treturn deprecated_options\n\t\telse:\n\t\t\treturn {}\n\n\tdef init(self):\n\t\tif not getattr(self, 'options', None):\n\t\t\t# set defaults (since we already initialized them)\n\t\t\tself.options = configdict.ConfigDict()\n\t\tif'verbose' in self.options:\n\t\t\tself"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict(frappe.local.conf).get(\"deprecated_args\", {\"doctype\": self.doctype},\n\t\t\t\t\tupdate(self.get(\"no_deprecation_fields\") or {}),\n\t\t\t\t\tignore_permissions=True)\n\n\n\tdef set_user_role(self):\n\t\t\"\"\"Set user role\"\"\"\n\t\tif self.user_permission:\n\t\t\tself.user_permission = frappe.db.get_value(\"User\", self.user_name, \"has_permission\")\n\t\t\tif not self.user_permission or (self.enabled and self.user_permission!= \"disabled\"):\n\t\t\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict(type=\"argsort\",\n\t\t\t\t\tchoices=config_argsort.get_argsort_choices(self) +\n\t)\n\nclass PrintOptions:\n\tdef __init__(self, *args, **_kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\n\ndef create_new():\n\t\"\"\"\n\tMake the first time all instances are used as argument to this one.\n\n\tWe add a method \"add_argument_group\" to the parent class that add the group\n\targuments to the module's docstring.  We then call this as a child of\n"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor c, v in ans_file_args.items():\n\t\t\tif v in cli_args:\n\t\t\t\tif cli_args[v].lower() == answer_file_args[c].lower():\n\t\t\t\t\tprint \"Argument '{}' for script '{}': {:.0f} > {{:.0f}}\".format(\n\t\t\t\t\t\tcli_args[c],\n\t\t\t\t\t\tcli_args[c].lower(),\n\t\t\t\t\t\tself.script_location.rstrip('s').capitalize(),\n\t\t\t\t\t\tself.program_name,\n\t\t\t\t\t\tcli_args[c].upper()\n\t\t\t\t\t\t)\n\n\tdef main(self"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tself.validate_args(cli_args)\t\t\t\n\t\n\tdef validate_cli_args_for_version(self):\t\t\n\t\tfor cli_args, answer_file_args in answer_file_args.items():\n\t\t\tif isinstance(argparse.Values(cli_args.config), dict):\t\t\t\t\t\t\n\t\t\t\tfor key, value in answer_file_args.items():\n\t\t\t\t\tif not keyword.iskeyword(key):\t\t\t\n\t\t\t\t\t\tcli_args[key] = value\n\t\t\t\n\t\t\n\tdef get_args_for_version(self, version):\n\t\tfor cli_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\terrors = []\n\t\twarnings = []\n\n\t\tif self.type == 'argument':\n\t\t\targuments_dict = cli_args.get(\"argument\")\n#\t\t\tif \"type=file_name is not required\" in arguments_dict\n\t\telif 'type' in arguments_dict and arguments_dict['type'] == 'file_path':\t\n\t\t\terrors = cli_args.get(\"allowed_args\", None)\n\t\t\twarnings = cli_args.get(\"allowed_args\", None)\n\t\t\n\t\tfor error_arg in errors:\n\t\t\terror_message = str(error_arg"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor arg, v in answer_file_args.items():\n\t\t\tif arg in self.help_list:\n\t\t\t\tself.parser.print_help()\n\t\t\t\tprint(\"Invalid arg: '{0}'\".format(arg), file=sys.stderr)\n\t\t\telif arg in self.args_list:\n\t\t\t\tself.parser.print_args()\n\t\t\t\tprint(\"Invalid arg: '{0}'\".format(arg), file=sys.stderr)\n\t\t\telif arg in self.help_dict:\n\t\t\t\tself.parser.print_help()\n\t\t\t\tprint(\"Invalid arg: '{0}'\".format"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\told_warning_message = self._deprecation._deprecated_options.__str__()\n\t\t\t\n\t\tnew_warning_message = cli_args['deprecated']['title']\n\t\t\n\t\tcli_args['options'].help = 'Use --help or --all to list available commands.\\n\\n'\n\t\told_warning_message = new_warning_message\n\t\t\n\t\tif not cli_args['options'].help:\n\t\t\tlogger.error(self._deprecation._deprecation_warnings)\n\t\t\texit(1)\n\t\n\t\t\n\t\tif old_warning_message == new_warning_message:\n\t\t\treturn 0\n\t\n\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif answer_file_args is None:\n\t\t\traise ValueError(\"No answer file supplied to validate_arg_deprecation\")\n\t\tdeprecated, args = answer_file_args.items()\n\t\tfor var in deprecated:\n\t\t\tif not cli_args.has_key(var):\n\t\t\t\t# ignore argument unless it is specified by the caller\n\t\t\t\tif var not in cli_args:\n\t\t\t\t\tcontinue\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"The %s argument that is not given to the function %s. It provided no entry in %s.\\n\"\n\t\t\t\t\t\"The name of the program in question"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tdeprecation_flag = False\n\t\tfor arg in self.deprecated_arg_names:\n\t\t\tif arg not in cli_args:\n\t\t\t\t# No conflict\n\t\t\t\tnew_arg = arg\n\t\t\t\tdeprecation_flag = True\n\t\t\t\tcli_args[arg] = new_arg\n\n\t\tif len(deprecation_flag)!= 0:\n\t\t\tsys.exit(\"ERROR:\", \"deprecated arguments missing.\")\n\n\tdef __update_parser(self, arg_parser, answer_file_args):\n\t\t\"\"\"\n\t\tUpdates the parser with parsed input data.\n\t\tThis method updates all the argument parser options"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif 'warning' in cli_args and cli_args['warning'] and 'always' in cli_args['warning']:\n\t\t\treturn\n\t\ttry:\n\t\t\tif not answer_file_args or not answer_file_args['default']:\n\t\t\t\treturn\n\t\texcept:\n\t\t\tprint self.do_help()\n\t\t\treturn\n\n\t\tif not self.cli_args and '--help' not in cli_args:\n\t\t\tprint self.do_help()\n\t\t\treturn\n\n\t\tself.deprecated_arg = None\n\t\tdeprecated_arg_string = \"\"\n\t\tfor key, value in"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor arg in answer_file_args.keys():\n\t\t\tif (arg not in cli_args.keys()) or cli_args[arg]==\"None\":\n\t\t\t\tprint(\"Warning: The argument %s is not valid\" % arg)\n\n\tdef valid_answer(self, cmd, answer_file_args=None):\n\t\t\"\"\"\n\t\tReturns true if this answer is valid.\n\t\t\"\"\"\n\n\t\tif answer_file_args is None:\n\t\t\tanswer_file_args = {\"command\":cmd}\n\n\t\tif answer_file_args[\"command\"] in [\"list\",\"list_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\n# TODO: test this\nclass V1ParameterValidationTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.v0 = V1Parameter(data.v1(5), 20)\n        self.v1 = V1Parameter(data.v1(30), 10)\n\n    def test_validate_missing(self):\n        with self.assertRaises(cli_exc.Missing):\n            v1 = V1Parameter(data.cli_missing(15, 0, 'bad'))\n\n\nif __name__ == '__main__':\n    unittest.main()\n# -*- coding: utf-"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn [Option(option=Option.SYSTEM_OPTION, argument=\"guest_name\", default=None, help='guest name'),\n\t\t\t\tOption(option=Option.SYSTEM_OPTION, argument=\"guest_type\", default='e1000', help='guest type'),\n\t\t\t\tOption(option=Option.SYSTEM_OPTION, argument=\"autostart\", default=None, help='enabled'),\n\t\t\t\tOption(option=Option.SYSTEM_OPTION, argument=\"start\", default=None, help='start'),\n\t\t\t\tOption(option=Option.SYSTEM_OPTION, argument=\"stop\", default=None, help='stop"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn { \"option\":\"\", \"description\":\"Help of virsh\", \"metavar\":\"[opt1,opt2]\", \"option_metavar\":\"[opt1,opt2]\", \"optional\":False, \"metavar\":None }\n\n\tdef get_dom_for_option(self, option_name):\t\t\t\"\"\"\n\tGets the dom object for a certain option\n\t\"\"\"\n\t\treturn self.get_option(option_name).get_element()\n\n\tdef get_element_for_option(self, option_name):\t\t\t\"\"\"\n\tGets the element of a certain option (option)"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\targs = command_name.split()\n\n\t\tparser = OptikConfigParser(\n\t\t\tcommand_name, configfile=self.config_file\n\t\t)\n\n\t\t# parse command line options\n\t\tfor optn in parser.option_list:\n\t\t\tval = getattr(parser.get_option(optn), \"value\", None)\n\t\t\tif val is not None:\n\t\t\t\targs.append(optn)\n\t\treturn args\n\n# -- Utilities ------------------------------------------------------------------\n\n###############################################################################\n# -- Test ------------------------------------------------------------------\n#\n# @since: 12.07.2017\n#   A lot"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tparser = self.add_subparsers(\n\t\t\ttitle='Commands',\n\t\t\tdest='subparser_commands',\n\t\t\thelp='The command name(subparsers) to get options for',\n\t\t)\n\t\t\n\t\tparser.add_argument('-c','--configuration',choices=cmdconfdict.commands.keys(),\n\t\t\thelp='configuration type for commands (default: none)')\n\t\tparser.add_argument('-u','--user',nargs='+',type='str',choices=cmdconfdict.commands.keys(),\n\t\t\thelp='user to connect to (default: none)')\n\t\tparser.add_argument"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn [x.split() for x in get_all_options(command_name=command_name)]\n\n\tdef add_options(self, parser):\n\t\t\"\"\"\n\t\tAdd options for the provided name\n\t\t\"\"\"\n\t\tself._add_options_to_parser = parser\n\n\tdef add_options_as_argument(self):\n\t\t\"\"\"\n\t\tAdd options as an argument for the command\n\t\t\"\"\"\n\t\tset_command_name_option(self._add_options_to_parser.get_parser_option_specs(), self.get_command_name())\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tresult = {command_name: []}\n\t\tfor argstring in self.command:\n\t\t\toption_spec = get_parser_argument(self.command, argstring=argstring)\n\t\t\toption_spec.print_help()\n\t\t\tresult[command_name].append(option_spec)\n\t\treturn result\n\n\tdef build_command(self, options):\n\t\tcommands = [option.command for option in options]\n\t\toptions_str = \" \".join(commands)\n\t\tcommand_str = \" \".join(sorted(options_str))\n\t\t# TODO: this is"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn [ OptionSpec(**{\"option\": command_name})]\n\n\nclass Command(object):\n\t\"\"\"\n\tA class that represents the command-line interface\n\t\"\"\"\n\n\tdef __init__(self, cli_name, cli_params, logger, options):\n\t\tself.cli = None\n\t\tself.cli_name = cli_name\n\t\tself.cli_params = cli_params\n\t\tself.logger = logger\n\t\tself.__cli_logger = logger.name +'' + self.cli_name +'' + str(self.cli_params['timeout'])"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self._options.getParser().getOptions()\n\n\tdef get_cli_option_value(self, cli_name, option, default=None):\t\t\n\t\t\"\"\"\n\t\tGets option value set for cli_name\n\t\tReturns empty string if not found\n\t\t\"\"\"\n\t\tcli = self.get_cli()\n\t\topt_value = cli.cliOptions.getOptionAttr(cli_name, option, default)\n\t\tif opt_value == \"\":\n\t\t\treturn \"\"\n\t\treturn cli.cliOptions.getOptionValue(cli_name, option)\n\n\tdef add_"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\toption_list=self._get_parser_option_specs_generic()\n\n\t\tcommand_opts=command_name+' '+command_option_list\n\t\treturn option_list\n\n\tdef validate_option(self, option):\n\t\t\"\"\"Validates the option and returns the parsed option object\"\"\"\n\t\ttry:\n\t\t\treturn option.option\n\t\texcept AttributeError as e:\n\t\t\t\t\traise e\n\n\tdef get_argument_spec(self, option_name):\n\t\t\"\"\"\n\t\tGet a specific argument specification from the command option\n\t\t\"\"\"\n\t\ttry:\n\t\t\tv=self.get"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tout = []\n\t\tcli = self.cli_factory.create()\n\t\tif cli.has_option('--help'):\n\t\t\tcli.print_help()\n\t\targuments = cli.get_arguments()\n\n\t\tfor arg_to_get in arguments:\n\t\t\tif arg_to_get == '-help':\n\t\t\t\tsys.exit(0)\n\t\t\tif \"--\" in arg_to_get:\n\t\t\t\tcli_name = arg_to_get.split(\"-\")[0]\n\t\t\t\tout += cli.get_option(cli_name)\n\t\n\t\toptions = {}\n\t\t\n\t\tfor command"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tspec = self._config.get(option_name)\n\n\t\treturn spec\n\n\t#~~ get_option_spec API\n\tdef list_options(self):\n\t\treturn self._config.options.values()\n\n\tdef list_options_for_section(self, section_name):\n\t\tresult = {}\n\n\t\tfor option in self._config.options.values():\n\t\t\tresult[section_name] = option\n\n\t\treturn result\n\n\t#~~ get_option_spec\n\tdef get_option_spec(self, option_name):\t\t\t\"\"\"\n\tGets the specification"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self._options.get(command_name, (None, 0))[argument_name]\n\n\t\n\t#########################################################\n\t#\t\tMiscellaneous functions\t\n\t#########################################################\t\t\n\tdef _handle_default(self, cmd, args, kwargs):\n\t\t\"\"\"\n\t\tHandles the default value of the command.\n\t\t\n\t\tArgs:\n\t\t\tcmd (str): The action that is being run\n\t\t\targs (Sequence[str]): Optional arguments passed to the command\n\t\t\tkwargs (Dict[optional, str]): Optional custom key/value pairs to append as command arguments into\n\n\t\tReturns:\n\t\t\tNone\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self._option_spec_map[command_name][argument_name]\n\tdef get_arguments(self, command_name, args):\t\"\"\"\n\tGets the list of arguments for given command name and arguments.\n\t\"\"\"\n\t\treturn self._arguments[command_name]\n\tdef get_option_spec_map(self, command_name, arg_name):\t\"\"\"\n\tGets the specification for the specified option name and parameter name.\n\tIf both are present this will return all the options.\n\t\"\"\"\n\t\treturn self._option_spec_map[command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\toption_spec  = \"\"\n\n\t\tfor option in self.options:\n\t\t\toption_spec += \" [option name=%s]\\n\" % (option.name)\n\t\t\toption_spec += \" [option argument_name=%s]\\n\" % (option.key)\n\t\t\toption_spec += \" [option help_string=%s]\\n\" % (option.help_string)\n\t\treturn option_spec\n\n\n\tdef parse_args(self):\n\t\t\"\"\"\n\t\tParses the command line arguments. Also populates this object\n\t\twith any additional configuration parameters that were provided to this"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\toption_spec = {}\n\n\t\tif input_value_list:\n\t\t\tspec = input_value_list[command_name][argument_name]\n\t\telse:\n\t\t\tspec = {}\n\t\treturn option_spec\n\n\tdef generate_option_spec(self) :\n\n\t\t\"\"\"\n\t\tGenerates the specification for a selected option name, which expects to have the option parameter.\n\t\t\"\"\"\n\n\t\tif not hasattr(self, 'input_value_list') or not self.input_value_list:\n\t\t\traise InvalidParameterValue(\"No valid options selected! Please specify at least one option to"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn {\"OPTION_SPEC\": [command_name, argument_name]}\n\n\tdef get_option_value(self, command_name, argument_name, argument_value):\n\t\t\"\"\"\n\t\tGets the value of the specified option.\n\t\t\"\"\"\n\t\treturn self._get_option_spec_value(command_name, argument_name)\n\n\tdef set_option_value(self, command_name, argument_name, argument_value):\n\t\t\"\"\"\n\t\tSets the value for the specified option.\n\t\t\"\"\"\n\t\tself._set_option_spec_value(command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tspec = dict()\n\t\tspec['name'] = command_name\n\t\tspec['action'] = 'get'\n\t\tspec['command'] = self.get_option(command_name, argument_name)\n\t\treturn spec\n\t\n\tdef get_option(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the specified option. The name of the default command\n\t\tmay be specified. The method is usually called from the configuration\n\t\tfile.\n\n\t\t@return: The specified option.\n\t\t\"\"\"\n\t\treturn self.execute('get-option', {'name': argument"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn get_option_spec(self.client, command_name, argument_name)\n\n\tdef add_argument(self, command_name, argument_name):\t\"\"\"\n\tAdds a new argument to the command's argument list. This\n\tmethod should be overridden by subclasses that override this method.\n\t\"\"\"\n\t\treturn self._add_argument(command_name, argument_name)\n\n\tdef remove_argument(self, command_name, argument_name):\t\"\"\"\n\tRemoves a given argument from the command's argument list. This\n\tmethod should be overridden by base classes"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tspec = self._get_config(command_name).get(argument_name, None)\n\t\tif spec is None:\n\t\t\treturn DEFAULT\n\t\treturn spec\n\n\tdef write_option(self, section, option, value, command_name):\n\t\treturn super(Options, self).write_option(section, option, value, command_name)\n\n\tdef get_commands(self):\n\t\t\"\"\"\n\t\tReturns the list of all command names defined in this module. If\n\t\tthere are multiple names defined, this method will return the complete\n\t\tlist.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tspecification = dict()\n\t\tif \"-\" in argument_name:\n\t\t\tspecification[\"name\"] = argument_name.split(\"-\")[0]\n\t\tif \"-\" in argument_name:\n\t\t\tspec = argument_name.split(\"-\")[1]\n\t\telse:\n\t\t\tspecification[\"name\"] = argument_name\n\t\t\tspec = \"default\"\n\t\treturn specification\n\t\t\n\tdef check_options(self):\t\t\t\n\t\t\"\"\"\n\t\tChecks that all required options are available.\n\t\t\"\"\"\t\t\t\n\t\tfor option_name, option in self.options.iteritems():\n\t\t\ttry:\n\t\t\t\tself.check"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\targs = re.split('\\s*,\\s*\\n{1,\\s*\", args)\n\t\targs = filter(None, args)\t# filter out empty values\n\t\t# filter out lists and strings by default\n\t\targs = args[:]\n\t\targs.sort()\n\t\tfor a in args:\n\t\t\ta, sep, kwargs = a.partition(' ', 1)\n\t\t\tkwargs = [x + '\\n' for x in kwargs]\n\t\t\tdel kwargs[-1]\n\t\t\tkwargs = map(lambda x: x.strip(), kwargs)\n\t\t\tkwargs = [kwargs]\n\t\t\ta ="}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\t# list of possible arguments, separated by a double space\n\t\t# (if multiple arguments given)\n\t\tparams = args.split()\n\t\tif params:\n\t\t\treturn params[:2]\n\t\t# if arg has an optional one: return nothing\n\t\telse:\n\t\t\treturn []\n\n\tdef check_args(self, args):\n\t\t\"\"\"\n\t\tReturns True if the argument list passed is a valid list of arguments\n\t\t\"\"\"\n\t\tparams = args.split()\n\t\t# if it is a list with 2 elements, they\n\t\tif len(params) == 2:\n\t\t\treturn True"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\targs = args.strip()\n\t\targs = args.lower()\n\t\targs = sanitise_arguments.filter(args)\n\t\treturn args\n# Copyright (c) 2013 Mirantis Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the License);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n#"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tcmd = [\"Silence\"]\n\t\tif len(args) > 0:\n\t\t\tif type(args[0]) == list:\n\t\t\t\tfor index,arg in enumerate(args[0]):\n\t\t\t\t\tcmd.extend(shlex.split(arg))\n\t\t\telse:\n\t\t\t\tfor arg in args:\n\t\t\t\t\tcmd.extend(shlex.split(arg))\n\t\treturn cmd\n\n\t###########################################################\n\t# get the value of the argument\n\t########################################################### \n\tdef get_value(self, arg_name):\n\t\t\n\t\targs = list(self.get_silent_args(self."}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\t# list of arguments to parse\n\t\ttry:\n\t\t\treturn args[:]\n\t\texcept:\n\t\t\treturn []\n\t\n\tdef parse_args(self, arg):\n\t\t\"\"\"\n\t\tParse a single argument name into its corresponding\n\t\targument name.\n\n\t\t:param arg: Argument name.\n\t\t:return: An argument name as a list of arguments\n\t\t\"\"\"\n\t\tif arg[0:9] == \"arg\":\n\t\t\treturnarg = arg[9:]\n\t\telif arg[-1:] == \"arg\":\n\t\t\treturnarg = arg[:-1]\n\t\telse:\n\t\t\traise ValueError"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\targs = self.filter_args(args)\n\n\t\treturn filter( self.get_silent_arg, args )\n\n\tdef get_silent_arg(self, argv):\n\t\t\"\"\"\n\t\tSilencing by default. Only use with care for the first\n\t\targument.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn int( argv )\n\t\texcept ValueError: \n\t\t\treturn None\n\n\tdef filter_args(self, argv):\n\t\t\"\"\"\n\t\tFilter the list of arguments returned by Silencing.\n\t\t\"\"\"\n\t\targs = []\n\n\n\t\t# Silence"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\targ_dict = get_dict_from_ini_file(args)\n\t\treturn list(arg_dict.keys())\n\n\tdef is_valid_slience(self, s):\n\t\treturn s.startswith(\"SLIENCE_:\")\n\n\tdef get_max_slience(self, s):\n\t\treturn len(s) - 1\n\n\tdef validate_boolean(self, s):\t\"\"\"\n\tboolean validation.\n\t\"\"\"\n\t\ts = s[2:]\n\t\tif not s or re.match(\"^[A-Z]{2}[a-z].$"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\ndef clear_silent_args(self, name):\n\t\"\"\"\n\tclears the argument of a silence call.\n\nUsage:\t\n\t./set_silent_args myfile.name\n\t./set_silent_args myfile.name myfile\n\t./set_silent_args myfile.name\n\t./clear_silent_args myfile.name\n\"\"\"\n\t\ndef get_verbose_args(self, name):\n\t\"\"\"\n\tGet a list of args you would like to execute.\n\t\"\"\"\n\t\ndef execute(self, args):\n\t\"\"\""}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tif self.has_silence('c'):\n\t\t\treturn [ s if not x.endswith('\\t') else x[:-3] for x in args ]\n\t\telse:\n\t\t\treturn [], False # not silenced, can be used in uncommenting below\n\nclass SilencedArgs(tuple):\n\tdef split(self):\n\t\t\"\"\"split the input argument into a multiple of 4 items\n\n\t:rtype: the split argument list\"\"\"\n\t\treturn self[0], self[-2]\n\n\n# --- Silenced Arg Parser----------------------------------------------------------------#\n\nclass Silenced"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\n    def check_for_complete_command(self, argv, valid_commands):\n        \"\"\"\n        Check if'self.shell_command' is valid.\n\t:param argv: The command line or list of input arguments.\n\t    Note that'self.command' can be empty as well.\n\t:param valid_commands: list of valid command names\n\t\n    \"\"\"\n        for cmd in self.shell_command:\n                if cmd not in valid_commands:\n                        self.print_to_file('Silencing command %s'%(cmd))\n        return True\n\n\n    def check_for_running_command"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif not hasattr(self,'required_args'):\n\t\t\tself.required_args = (args.name, args.description, args.license, args.package,\n\t\t\t\targs.version_id, args.description, args.license_id,\n\t\t\t\targs.packager)\n\n\tdef validate_packager(self, args):\t\"\"\"\n\t\tCheck if the user provided a required packager. If it does not define one,\n\t\traise an exception if the required packager is not a valid one (except for\n\t\tunknown packages).\n\t\t\"\"\"\n\t\tif not callable(args"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif not args:\n\t\t\treturn\n\t\tif isinstance(args, str):\n\t\t\targs = [args]\n\n\t\tdef check_fn(arg):\n\t\t\tif arg not in self.args and arg in self.kwargs:\n\t\t\t\tfrappe.throw(_('{0} Required field, {1} provided').format(\n\t\t\t\t\t_('All mandatory arguments'), _(arg or '')\n\t\t\t\t))\n\t\t\telif arg not in self.args and not arg.find('='):\n\t\t\t\tfrappe.throw(_('{0} Required field, {1} missing or empty').format(\n\t\t\t\t\t_('All mandatory arguments'), _("}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\t# Get the value and validate required arguments against the current object\n\t\t\t# and raise an error in case something goes wrong.\n\t\t\targ_value, arg_description = arg.split(' ')\n\t\t\ttry:\n\t\t\t\targ_value = int(arg_value)\n\t\t\texcept ValueError:\n\t\t\t\traise SageError('Argument must be an integer value (--name) when parsing a new file', parent=self, error_class=Sage.Error)\n\t\t\t# Store the value and error type information in the list so that it can be used as a dict."}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tself._validate = []\n\t\tself._validate.extend(args)\n\t\n\tdef validate_arguments(self):\n\t\tfor arg in self._validate:\n\t\t\tif not arg.isset() and arg.default is not None and self.is_required:\n\t\t\t\traise ArgValidationError(\"Argument has a default value {0}, which is not set. You can use {} \".format(arg.default, arg.default))\n\t\n\tdef add_validators(self, validator):\t\t\n\t\tself._validators = []\t\t\n\t\tself._validators.append(validator)\n\t\n\tdef clean(self,"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif not isinstance(args, (tuple, list)):\n\t\t\targs = (args,)\n\t\tfor arg in args:\n\t\t\tif not isinstance(arg, (basestring, unicode)):\n\t\t\t\tfrappe.throw(_(\"Row #{0}: field {1} should be a string or unicode\").format(arg, self.meta.get_label(arg)))\n\n\tdef set_default(self, fieldname, val):\n\t\t\"\"\"Set to default value for a meta field. Used in conjunction with validate_required_args in docfield\"\"\"\n\t\tself.validate_required_args([fieldname])\n\n\t\tmeta"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor (key,value) in frappe.local.user_list_kwargs.iteritems():\n\t\t\tif value:\n\t\t\t\targs.pop(key)\n\t\t\t\tvalidator = get_validator(self.doctype, self.name, args, context=args)\n\t\t\t\tif not validator.validate():\n\t\t\t\t\tfrappe.throw(_(\"Missing required {0} ({1})\").format(_(self.doctype), self.name), InvalidUserError)\n\n\tdef validate_update_time(self):\n\t\t\"\"\"Validates users using date greater than the specified date.\"\"\"\n\t\tfrappe.message_log.clear()\n\n\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in self.required_args:\n\t\t\tif not arg in args:\n\t\t\t\tfrappe.throw(_(\"{0} must be specified as a parameter in {1}\").format(self.doctype, self.meta.get_label(\"required\")))\n\n\tdef set_missing_values(self, args):\n\t\t# only if missing from user\n\t\tdoc = frappe.get_doc(self.doctype, self.fields[self.fieldname])\n\t\tif not doc.fieldtype or not doc.fieldtype in [\"Dynamic Link\", \"Dynamic Link\"] or doc.fieldtype == \"Link\""}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\ttry:\n\t\t\tfor arg in args:\n\t\t\t\tis_required = arg.startswith('-D')\n\t\t\t\tif is_required:\n\t\t\t\t\tcontinue\n\t\t\t\tmsg = \"\"\n\t\t\t\tif '--' in arg:\n\t\t\t\t\tmsg = \"A -- are required\"\n\t\t\t\telif '--' in arg:\n\t\t\t\t\traise argparse.ArgumentError(\"The 'options' argument is not valid because the command needs to be specified.\")\n\t\t\t\tif not (is_required and msg and msg[0] == '--'):\n\t\t\t\t\traise argparse.ArgumentError(\"The 'options' argument is provided but does not match the expected options (default:"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tret = True\n\t\tfor p in args:\n\t\t\tif not isinstance(p, dict): ret = False\n\t\treturn ret\n\tdef __call__(self, env):\n\t\t\"\"\"\n\t\tCall the tool.\n\n\t\tArgs:\n\t\t    env - the complete environment definition\n\t\tReturns:\n\t\t    a tuple (returncode, stdout, stderr)\n\t\t\"\"\"\n\t\tret = self.check_requires_args(env)\n\t\tif not ret:\n\t\t\treturn (1, '', [])\n\n\t\toutput = \"\"\n\n\t\tenv._log(\"Calling the tool\")\n\t\tcall(self, output, env)"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor key, value in iteritems(args):\n\t\t\tif key in SKIP_IN:\n\t\t\t\tcontinue\n\t\t\tif value is not None and not isinstance(value, Value):\n\t\t\t\treturn\n\n\t\t\tif not args[key]:\n\t\t\t\treturn\n\n\tdef validate_with_options(self):\n\t\tif not frappe.db:\n\t\t\traise frappe.ValidationError, _('{0} has no database configured. Please configure {1}')\n\t\tif not frappe.local.flags.disable_interspersed_queries:\n\t\t\t# disable itersinicity of queries at first level\n\t\t\tif not frappe"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tcommand_spec = _process_optiondict(options_spec)\n\t\tcommand_names_match = re.search('\\w+', command_name)\n\n\t\tif command_names_match:\n\t\t\targs_match = _process_optiondict(options_spec)\n\t\t\treturn [command_name]\n\t\t# If no required_when conditions were matched in command name\n\t\tif not options_spec.get('required_when'):\n\t\t\traise Exception('No required_when specified')\n\t\tif command_names_match:\n\t\t\traise Exception('Multiple required_when specified')\n\t\t\n\t\traise"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\treturn super().get_conditionally_required_args(command_name, options_spec, args)\n\n\nclass _CommandCondition( object ):\n\t# this class implements the `has` and `not` methods that must be overridden to provide\n\t# proper values for certain actions.\n\n\tdef __call__(self, command_name, options_spec, args):\n\t\tif options_spec is None:\n\t\t\toptions_spec = {}\n\t\tif options_spec.has_key('required_when' ):\n\t\t\trequired_when = options_spec['required_when']['value']"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tcommand = self.get_option_by_name(command_name).lower()\n\t\t\n\t\tif command =='required_when':\n\t\t\treturn options_spec.required_when\n\t\telif command == 'argument_type':\n\t\t\tcommand_args = options_spec.argument_type\n\t\t\tif command =='required':\n\t\t\t\traise ArgumentTypeError(\"Required argument should be at least one string argument, either'required_when' or 'argument_type'\")\n\t\t\tif command == 'argument_type':\n\t\t\t\twith_args = [\"required_when\"]\n\t\t\t\tif options_spec.argument_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tinputs = {}\n\t\toptional_inputs = []\n\t\tfor condition in options_spec:\n\t\t\tif(isinstance(condition, str)):\n\t\t\t\tcondition = expression(condition)\n\n\t\t\tvalue = condition(*args).as_default()\n\t\t\tinputs[condition] = value\n\t\toptional_outputs = {}\n\t\tfor condition in options_spec:\n\t\t\tif(isinstance(condition, str)):\n\t\t\t\tcondition = expression(condition)\n\n\t\t\tvalue = condition(*args).as_default()\n\t\t\toptional_outputs[condition] = value\n\t\tresult_outputs_values = []\n\n\t\tfor option in"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t# Note that if this method fails we pass 'default args' to the\n\t\t# command_name in the form of the form:\n\t\t# 'foo_bar_baz' or 'foo_bar_baz: baz'\n\targuments = None\n\tfor parameter in options_spec:\n\t\tif parameter[1].get('required_when', required_when):\n\t\t\tcontinue\n\t\targument = False\n\t\tfor option in parameter[1].get('options', []):\n\t\t\tcommand = command_name\n\t\t\tfor value in option[1].get('values', ()): # skip empty\n\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\targs = self.to_list(args)\n\t\tmatched_conditions = list(set(options_spec).intersection(command_name))\n\t\tif matched_conditions:\n\t\t\tall_required_when = [condition for condition in args if condition in matched_conditions]\n\t\t\tif len(all_required_when):\n\t\t\t\treturn all_required_when\n\t\treturn list()\n\n\tdef _format_required_when(self, arg_spec):\n\t\trequired_when = list(set(arg_spec.keys())).intersection(self._get_conditionally_required_args("}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\nclass CommandList(list):\n\t_always_arg = 'command'\n\t_never_arg = 'options'\n\n\tdef __init__(self):\n\t\tself.command = None\n\t\t#self._options_spec = None\n\n\t@property\n\tdef command(self):\n\t\t\"\"\"\n\t\tReturn the command for testing.\n\t\t\"\"\"\n\t\treturn self._command\n\tdef __str__(self):\n\t\treturn self._command\n\n\tdef __call__(self, command, *args, **kwargs):\n\t\t\"\"\"\n\t\tReturn the list of `command` for testing."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\trequired_when = None\n\t\tif options_spec:\n\t\t\tfor option_name, option_spec in options_spec.iteritems():\n\t\t\t\trequired = False\n\t\t\t\tif option_name =='required_when':\n\t\t\t\t\tif required:\n\t\t\t\t\t\trequired = True\n\t\t\t\tif option_name in options_spec:\n\t\t\t\t\trequired = True\n\t\t\t\t\tcontinue\n\t\t\t\tif option_name =='required':\n\t\t\t\t\tif required:\n\t\t\t\t\t\trequired = True\n\t\t\t\tif option_name in options_spec:\n\t\t\t\t\trequired = True\n\t\t\t\tif option_name in options_spec:\n\t\t\t\t\trequired = True"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t(required_when, condition) = self.process_option(options_spec)\n\t\treturn self._get_option_value_for_command_argument(command_name, required_when, condition)\n\n\nclass OptionsAction(object):\n\t\"\"\"\n\tA command that executes a command through the CLI.\n\t\"\"\"\n\n\tdef __init__(self, command_name, options, args):\n\t\tself.command_name = command_name\n\t\tself.options = options\n\t\tself.args = args\n\n\tdef process_option(self, options):\n\t\t\"\"\"\n\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t# Check if this command requires that the given condition was given\n\t\t# in the command's options specification. If so, return an empty list.\n\t\tparsed_spec = self._spec_name_regex.match(command_name)\n\t\tif not parsed_spec:\n\t\t\treturn []\n\t\ttry:\n\t\t\trequired_when = str(parsed_spec.group('required_when'))\n\t\t\tif required_when not in self.required_when_list:\n\t\t\t\tself.required_when_list.append(required_when)\n\t\texcept:\n\t\t\tself.required_when"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.maxLength:\n\t\t\tfrappe.throw(_(\"Length must be less than allowed length. {0}\").format(self.maxLength))\n\n\tdef validate_limit_after_transaction(self):\n\t\tlimit = self.get_limit_after_transaction()\n\t\tif limit and (not self.data or (limit['allocated_qty'] <= 0 or self.data.get('billing_qty'))) or (limit['allocated_qty'] < self.current_batch_size):\n\t\t\tfrappe.throw(_(\"Amount cannot be greater than {0}\")\n\t\t\t\t\t."}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\traise Exception(\"Length length is unusual for this type. It must be a positive integer.\")\n\t\n\t\n\tdef __call__ (self, value):\n\t\tvalue = super(BoolBool, self).__call__(value) \t# make it an bool object\n\t\tif (value < 0):\n\t\t\traise ValueError(\"Value must be positive.\")\n\t\tif (value > 0):\n\t\t\traise ValueError(\"Value must be greater than 0.\")\n\t\tif (not (int(value) < 0 or int(value) > 255)):\n\t\t\treturn -3\n\t\tif (value == 0):\n\t\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > length_arg:\n\t\t\traise Exception, \"Length of a command must be longer than %d characters\" % length_arg\n\t\telse:\n\t\t\tassert len(args) == 1, \"Expected a single value\"\n\tdef __init__(self, name, description, type, default=None, short_description=None):\n\t\tassert type not in ('int32','short', 'long')\n\t\tself.name = name\n\t\tself.description = description\n\t\tself.type = type\n\t\tself.default = default\n\t\tself.short_description"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > len(list(args)):\n\t\t\traise BadArguments(self.__class__.__name__ +'must take at least %d arguments' % len(args))\n\t\telif len(args) < len(list(args)):\n\t\t\traise BadArguments(self.__class__.__name__ +'must accept exactly %d arguments' % len(args))\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif not args:\n\t\t\treturn super(Integer, self).__call__(*args, **kwargs)\n\t\treturn super(Integer, self).__call__(*args"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif (len(args['value']) >= 6):\n\t\t\t#print \"VALID\", args['value']\n\t\t\tself.check_range_length(args['value']), 'Value must be between 6 and 6'\n\n\tdef is_valid(self):\n\t\tif (self.type == 'ip'):\n\t\t\treturn self.value.startswith(ip_address_prefix)\n\n\t\tif (self.type =='mask'):\n\t\t\treturn self.value.count('1') == 2\n\n\t\tif (self.type =='mac'):\n\t\t\t#print \"TEST\", self.value\n\t\t\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.default_length:\n\t\t\tfrappe.throw(_(\"Length must be at most {0} characters.\").format(self.default_length))\n\n\tdef get_item_map(self):\n\t\treturn ItemTypeMap(item_id=cint(self.item_id), description=cstr(self.description))\n\n\n\n\n\ndef get_item_list(args):\n\t\"\"\"\n\tGet the list of the item types.\n\tArgs[0]: (type, name)\n\t\"\"\"\n\ttry:\n\t\tlist_of_type ="}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\n\t\n\tif 0 <= len(args) <= MAX_LENGTH_FOR_INPUT_VALUE:\n\t\treturn True\n\n\treturn False\n\n\nclass Acknowledge:\n\t\n\tdef __init__(self, name: str, data: dict, args: dict):\n\t\tself.name = name\n\t\tself.data = data\n\t\tself.args = args\n\t\t\n\tdef _serialize(self) -> None:\n\t\t\"\"\"\n\t\tSerialize the provided Acknowledge object.\n\t\n\n\t\t:return: The serialized Acknowledge object.\n\t\t:rtype: dict\n\t\t\"\"\"\n\n\t\treturn {\n\t\t\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.max_length:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tdef parse_args(self, name=None, prefix=None):\n\t\t\"\"\"\n\t\tParse the given args with the user-specified prefix.\n\n\t\t:param name: The name to look for the arguments.\n\t\t:type name: string\n\t\t:param prefix: The first prefix to look for the arguments.\n\t\t:type prefix: string\n\t\t:returns: A list of args, or None if there wasn't a match.\n\t\t\"\"\"\n\t\targs = tuple"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn (not args) and (len(args) > get_num_bytes())\n#\telif args.startswith(\"data_length\"):\n#\t\t\targs = args.split()\n# \t\tif args[0]!= \"__\":\n#\t\t\tmsg = \"Wrong length of command (%s): %s\\n\" % (args[0], args[1])\n#\t\t\traise ValueError(msg)\n#\t\t\n#\tdef __str__(self):\t\t\treturn self.to_txt()\n\n\tdef add_args_check(self, args):\n\t\t\""}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif self.length is not None:\n\t\t\treturn [str(i) for i in range(8)]\n\tdef __str__(self):\treturn \"Asymmetric: \" + str(self.symmetric)\n\nclass Plain(object):\n\t\"\"\"A simple wrapper around the python-magic built-in repr.format() call to show\n\tall arguments and then convert them into a nice string.\n\t\"\"\"\n\tdef __init__(self, string): self.string = string\n\nimport _format_dict\n\n_format_repr = _format_dict.Format(string"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn''.join(args).lower() in self.choices\n\nclass ConfigSubsectionMenu:\n\tdef __init__(self, key, label=None, command=None):\n\t\tself.key = key\n\t\tself.label = label\n\t\tself.command = command\n\n\tdef apply_commands(self):\n\t\tif self.command and len(self.key) > 0:\n\t\t\tself.commands.insert(0, self.key)\n\n\tdef check_command(self):\n\t\ttry:\n\t\t\treturn str(self.command).lower() in self.commands\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\nclass Mint(Model):\n\t\"\"\"\n\tA convenience method to use Mint\n\t\"\"\"\n\tname = CharField(max_length=50,db_index=True)\n\tvalue = CharField(unique=True)\n\tdefault_value = CharField(max_length=_MAX_VALUE)\n\n\tdef validate_choices_args(self, args):\n\t\tif not isinstance(args,list):\n\t\t\targs = [args] \n\t\tfor x in args:\n\t\t\tif len(x)!=1:\n\t\t\t\traise TypeError(\"Expected value for argument %s to be in %"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not args:\n\t\t\twebnotes.msgprint(_(\"Please enter at least one parameter for which value could not be converted.\"))\n\t\t\traise Exception(_(\"Please enter a valid choice for the following parameters: {0}.\").format(\", \".join(frappe.form_dict.keys())))\n\t\tif not get_choices_from_dictionary(args) == get_choices_from_dictionary(dict(zip(get_choices_from_dictionary(args), args))))\n\n\tdef validate_choices_for_children_with_default(args):\n\t\t\"\"\"Validate which values of child choices we have set of"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif args not in [None, []]:\n\t\t\tself.error(\"Wrong number of values passed for: {0}.\".format(self.dest))\n\nclass ConfigSetError(Exception):\n\tdef __init__(self, value):\n\t\tself.error_msg = _(\"Could not set up configuration\")\n\n\nclass Config:\n\tdef __init__(self, options):\n\t\tself.allow_empty = options.allow_empty\n\t\tself.plugins = []\n\t\tfor p in options.get(options.dest, []):\n\t\t\tif p in ['all', 'true']:\n\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\targs = utils.coerce_to_list(args)\n\t\tchoice = None\n\t\tif not args:\n\t\t\treturn False\n\n\t\tif not isinstance(choice, (list, tuple)):\n\t\t\ttry:\n\t\t\t\tchoice = (choice,)\n\t\t\texcept:\n\t\t\t\traise ValueError('Expected a list or tuple of choices, but received '\n\t\t\t\t\t'+type(choice).__name__)\n\t\tif len(choice)!= 2:\n\t\t\traise ValueError('Choice can only have one value')\n\n\t\tfor item in choice:\n\t\t\tif not isinstance(item, (int, float, bool, str)):\n\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tself.message(_('This argument should be one of the available choices'))\n\t\tret = False\n\t\tfor o in args:\n\t\t\tif ((o in self.choices.keys()) or (o == 'all')):\n\t\t\t\tret = True\n\t\t\t\tbreak\n\t\treturn ret\n\n\tdef validate_choices(self, choices):\t\"\"\"\n\tCheck if available choices matches any of the choices.\n\n:param choices: The list of choices to check.\t\"\"\"\n\t\tfor c in choices:\n\t\t\tif c[1] == 'all':\n\t\t\t\treturn True\n\t\treturn False\n\n\tdef _"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not (isinstance(args, list) and\n\t\t\tisinstance(args[0], (list, tuple))):\n\t\t\treturn\n\n\t\tfor d in self._choices:\n\t\t\tif not isinstance(d, (tuple, list)):\n\t\t\t\treturn\n\n\t\tif len(args) >= self._numargs:\n\t\t\traise InvalidChoiceException(arg=args[0])\n\n\tdef make_valid(self, value):\n\t\t\"\"\"\n\t\tValidate the chosen value.\n\n\t\t:param value: The value to validate.\t\n\t\t:type value: basestring\n\t\t\"\"\"\n\n\t\tif value is None:"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn getattr(args, \"choices\", []).lower() in self.VALID_CHOICES\n\n\t@property\n\tdef max_repeats_allowed(self):\n\t\t\"\"\"\n\t\tThis value is overridden by any combination of choices.\n\t\t\"\"\"\n\t\treturn getattr(self, \"max_repeats_allowed\", DEFAULT_MAX_REPUTATION_LENGTH)\n\n\tasync def __str__(self):\n\t\treturn str(self.name)\n\n\tdef __repr__(self):\n\t\treturn str(self.name) + \"(%d/%d)\" % (self.max_repeats_allowed, self."}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tvalid_choices = [\n\t\t\t('no', 'Nothing specified.'),\n\t\t\t('choice', 'Choices selected are: '),\n\t\t]\n\t\tfor arg_val, choice in enumerate(args):\n\t\t\tif arg_val not in valid_choices:\n\t\t\t\tfrappe.throw(_(\"Choices are {0}\").format(\", \".join(valid_choices)))\n\n\tdef validate_choices_value(self, args):\n\t\t\"\"\"\n\t\tCheck if choices value is valid, taking into account the default value.\n\n\t\t:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, basestring):\n\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, str):\n\t\t\targs = [args]\n\t\tfor i in range(len(args)):\n\t\t\tif not inChoice(*args[i]):\n\t\t\t\tprint \"Error validating choice argument %d for user '%s'.\" % (i, args[i])\n\t\t\t\tsys.exit(1)\n\n\tdef make_new_choice_obj(self, choice_args):\n\t\t\"\"\"\n\t\tCreate a new choice-object with given choice arguments.\n\t\tThe new choice-obj will be a subclass of that class defined in\n\t\t:class:`~pychinneph"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args[0], list) and len(args[0]) > 0:\n\t\t\tself.min_value = args[0][0]\n\t\t\tself.max_value = args[0][1]\n\t\telif isinstance(args[0], tuple) and len(args[0]) > 0:\n\t\t\tself.min_value = args[0][0]\n\t\t\tself.max_value = args[0][1]\n\ndef validate_string(value):\n\tif isinstance(value, (str,unicode)):\n\t\treturn value\n\telif is_empty"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, string_types):\n\t\t\tif (not re.search(r'^([-0-9]+)\\.' + args, args, re.I)):\n\t\t\t\tself.message = _(\"Please use only the first letter of the minimum and maximum letter.\") + \" Use --minimum-number-of-digit.\"\n\n\tdef validate_min_max_width(self, args):\t\"\"\"\n\tCheck that values in the values in the min and max values have the given width and that at least\n\tone of the given values exceeds minimum width.\n\n\t:param args: The received arguments."}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn len(sorted(args, key=attrgetter('min','max')))\n\n\tdef is_float(self, str):\t\"\"\"\n\tReturns True if argument is a float.  (False) if it is a non-numeric type.\n\t\"\"\"\n\t\treturn str\n\n\tdef validate(self, value, min, max):\t\"\"\"\n\tValidator that takes value, min, and maximum values and returns\n\tTrue if validator returns False. Suitable for integer and float arguments.\n\t\"\"\"\n\t\tif value is not None and not isinstance(value, (int, float)):\n\t\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\n\t\n\t# check if we have any min/max args.\n\tif self.min_max:\n\t\tif self.min_max < 0 or self.min_max > 65535: # min/max is a string, not in the range - e.g. '1'\n\t\t\traise argparse.ArgumentTypeError, \"%s min/max must be in range - (inclusive)\" % self.name\n\t\telse:\t\n\t\t\t# check if all min/max values are between the minimum and maximum (inclusive).\n\t\t\tif self.min_max >= self.max_min or self.min"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 2:\n\t\t\tmsg = \"Minimum value must be between minimum and maximum value.\"\n\t\t\traise ValueError(msg)\n\n\t\tif args[1] < args[0]:\n\t\t\tmsg = \"Minimum value must be between minimum and maximum value.\"\n\t\t\traise ValueError(msg)\n\n\tdef _validate_maximum_args(self, min_arg, max_arg):\n\t\t\"\"\"\n\t\tCheck if value of arguments is between maximum and minimum values.\n       \n\n\t\tRaises:\n\t\t\tTypeError: if value can't be converted to minimum.\n\t\t\tValueError: if value too high"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, basestring):\n\t\t\traise TypeError(\"Value of '%s' must be a string.\" % (args.__class__.__name__,))\n\n\t\tmin_val = args\n\t\tmax_val = args + 2\n\n\t\tif min_val < min(self._min, self._max):\n\t\t\traise ValueError(\"You must pass at least one value (from -5.0 to 1.0).\")\n\n\t\treturn max_val \n\nclass MaxVals(Parameter):\n\t\"\"\"\n\tMaximum and minimum value of an algorithm.\n\n\t:param max_val: The value"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor item in args:\n\t\t\tif item >= min(value=1):\t# if we entered any value, then it's too low\n\t\t\t\tcontinue\n\t\t\telif value > max(value=1):\t# if we entered any value, then it's too high\n\t\t\t\tcontinue\n\n\t\t\tif item < min(value=1):\n\t\t\t\traise ValueError(\"Argument '{0}' lower than the value '{1}'\"\n\t\t\t\t\t.format(item,min(value=1),max(value=1)))\n\n\tdef process_validate(self, arg):\n\t\tfor arg_name in arg["}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\ttry:\n\t\t\tself.min_max = float(args[5])\n\t\texcept:\n\t\t\tpass\n\n\tdef validate_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between maximum and minimum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\ttry:\n\t\t\tself.max_max = float(args[4])\n\t\texcept:\n\t\t\tpass\n\n\tdef validate_type(self, type):\t\"\"\"\n\tCheck if type is a valid string value or a number.\n\t\n\t:param type: The type of type."}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\ttry:\n\t\t\tmin_max = float(args)\n\t\texcept ValueError:\n\t\t\traise TypeError('The value of required arguments should be in [min_min,max_min,max_max]')\n\t\t\treturn\n\n\t\tif min_max < 0 or max_min < 0:\n\t\t\traise ValueError('The values of required arguments do not satisfy minimum and maximum values')\n\n\tdef calc_limits(self, limits):\n\t\t\"\"\"\n\t\tCalculate min and max coordinates in a set of specified limits.\n\t\t\"\"\"\n\t\tif len(limits) == 0:\n\t\t\treturn tuple()\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > 1 and len(args[1]) > 50:\n\t\t\tprint('Usage: check_min_max.py {0} {1}'.format(args[0], args[1]))\n\t\t\tsys.exit()\n\ndef get_minimum_min_args(args):\n\t\"\"\"\n\tCheck if value of arguments is between minimum and minimum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > 1 and len(args[1]) > 50:\n\t\t\tprint('Usage: check_min_min.py {"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\t# Convert 'foo: bar' to 'bar' - this way it can be used to generate the string 'x: bar'\n\t\targ_type, arg_type_name = subcommand.find_command_options(type_name, option_name)\n\t\t# if none exists - return None\n\t\tif arg_type is None:\n\t\t\targ_type = complex\n\t\t\treturn arg_type, None, None\n\t\telse:\n\t\t\t# build the complex type\n\t\t\tcomplex_arg_type, arg_type_name, complex_type_name, type_value = spec"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tif not hasattr(type_name, '__call__'):\n\t\t\traise TypeError('argument type name must be a string')\n\t\treturn getattr(type_name, subcommand)\n\t\t\n\tdef create_complex_argumete(self, commandname, type_name, arg_list,\t\"\"\"\n\tCreates an argumet from an argument list\n\n:param commandname: the command name\n:param type_name: the complex type name\n:param arg_list: arguments to the command\n:param spec_options: options for argument-type\n\n:return: a complex type instance\t\"\"\""}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\t# we're given a list of options: each is a suboption with a type name and a\n\t\t# list of option names.\n\t\targuments = []\n\n\t\tif type_name in self.options:\n\t\t\targuments.append(self.options[type_name][0])\n\t\telse:\n\t\t\tsub_choice = self.get_option(type_name, option_name)\n\t\t\targuments.append(sub_choice.type_argument(sub_choice))\n\n\t\treturn type(sub_choice.type_argument(argument_name), arguments)\n\n\tdef get_value"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\n\n\t\n\n\nclass Command_test(Command):\n\n\t\"\"\"\n\tTest the basic command interface\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself._subcommands = []\n\n\n\tdef add_subcommand(self, name,subcommand):\n\t\t\"\"\"\n\t\tRegisters a specific command in the class\n\t\t\"\"\"\n\n\t\tassert subcommand in self._subcommands\n\n\t\tassert not hasattr(subCOMMAND, \"cmdspec\"),\"Unrecognised command class {}\".format(subcommand.cmdspec.class_name)\n\n\t\tself._subcommands.append(subcommand)\n\n\n\tdef add_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treal_arg_type = TypeSpec(subcommand,type_name,option_name)\n\t\tif real_arg_type:\n\t\t\treturn real_arg_type\n\t\treturn ComplexArgu(subcommand,complex_cmd=complex_cmd)\n\n\tdef generate_argumet(self):\n\t\t\"\"\" Build the \"argumet\" of the argument type if it is a complex\n\t\ttype, otherwise it assumes that the argument is to be of the first\n\t\ttype of type and that the type's type's command\n\t\tis a complex type\n\t\t\"\"\"\n\t\tif not"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\t# get the real type type\n\t\tif type_name == complex_argumet_type:\n\t\t\treturn type_name\n\t\telif type_name == complex_argumet:\n\t\t\treturn command_options_map[command_options_map.get(arg, _parser.unknown_arg)]()\n\t\telse:\n\t\t\traise Exception(\"not handling type for complex argumet: {}\".format(type_name))\n\nclass ArgumentTypeError(Exception): pass\n\nclass _Arg:\n\t\"\"\"\n\tAn Arguector for an ArgumentParser\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn ComplexArgumenterFactory.create(argument_type=type_name, argument_name=option_name,\n\t                                        nargs=\\\n\t\t                           len(subcommand.arg_list), description=u'%(default)s %(type)-s %(option)-s \"%(type-argumu)-s\"', spec_option=option_name)\n\nclass CppCtypeToXmlPrinter(CppCtypeToXmlElementPrinter):\n\tdef build_element(self, spec_option, argument_type):\n\t\treturn argument_type.argument_type.__class__\n\nclass Cpp"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tassert isinstance(type_name, str)\n\t\tassert isinstance(option_name, str)\n\t\tsubparser = subcommands.commands[subcommand]\n\t\tinstance = self._arg_type_manager._create_complex_arg_type( subcommand, type_name, option_name )\n\t\tif subparser is not None:\n\t\t\tinstance._parser.add_argument_group(subparser)\n\t\tinstance._parser.set_defaults( nargs=1 )\n\t\tinstance._parser.set_defaults( validator=self._validator )\n\t\tinstance._build_subcommand( subcommand"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\ttype_instance_name = \"\"\n\t\ttype_spec_name = \"\"\n\t\tif type_name!= \"all\":\n\t\t\ttype_instance_name = \"-{0!s}\".format(type_spec_name)\n\t\t\ttype_spec_name = \"-all\"\n\t\t\ttype_name = \"-{0!s}\".format(type_name.lower())\n\t\t\n\t\treturn {\n\t\t\t\"type\": \"complex\",\n\t\t\t\"command\": subcommand,\n\t\t\t\"action\": \"execute\"\n\t\t}\n\t)\n\n\ndef create_argument_set(self, action_name,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tassert type_name in spec_option_name_map.keys(), \"Invalid complex type specification name %r\"%type_name\n\t\tif spec_option_name[0] in spec_option_name_map[type_name]:\n\t\t\treturn spec_option_name_map[type_name][spec_option_name]\n\t\telse:\n\t\t\treturn complex_argumet_type(command=self, type_name=type_name,\n\t\t\t\toption_name=option_name, value=option_value, spec_option=spec_option)\n\tdef type"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t# This function is used to pass custom args to ansible module\n\t\t# to pass a dictionary like {\"custom_args\" : [{\"ansible_playbook.yml\":\"default/playbook.yml\", \"ansible.yml\":\"default/test_playbook.yml\", \"playbook.yml\":\"default/playbook_main.yml\", \"cli_args\": [\"--foo=bar\"],\"ansible_cfg\":{}}]\n\tcustom_args=[]\n\t# Inject the options in the CLI args list into custom dictionary\n\t# eg: {\"foo\": {\"bar\": \"baz\"} }\n\tfor entry in args:\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = split_args(args)\n\t\tcontrols = args.split(\";\")\n\t\tnested_args = {}\n\t\tfor arg in args:\n\t\t\tif len(arg) > 0:\n\t\t\t\tval = val.translate(self.unichr)\n\t\t\telse:\n\t\t\t\tval = arg.translate(self.unichr)\n\t\t\tnested_args.update(self.resolve_complex_args(arg))\n\n\t\tcontrol_args = {}\n\t\tfor control in controls:\n\t\t\tif control!= \"\":\n\t\t\t\tcontrol_args.update({control:val})\n\t\tnested_args."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t\n\t# Parse command line arguments\n\tparsed_args = []\n\tfor arg in shlex.split(args):\n\t\tparsed_arg = arg.strip().lower()\n\t\tif parsed_arg == '--list':\n\t\t\tparsed_args.append(self.args)\n\t\telif parsed_arg == '--host':\n\t\t\tparsed_args.append(args[args.index('--host')])\n\t\telif parsed_arg == '--port':\n\t\t\tparsed_args.append(args[args.index('--port')])\n\t\telif parsed_arg == '--template':\n\t\t\ttry:\n\t\t\t\tparsed"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\ndef read_from_yaml(filename, file_loader):\n\t\"\"\"\n    The main function. Read in the file, then write to the yaml file.\n\tThe resulting dict will include the keys in the yml file:\n\t\tdict: The input dict to be passed to the yaml parser\n\t\"\"\"\n\tparser = yml.YMLParser()\n\tyml.yaml_loader(yaml_data=read_yaml(filename), file_loader=file_loader)\n\treturn parser.parse()\n\ndef parse_file(filename):\n\t\"\"\"\n    This function returns a"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tspec = yml_parser.parse_spec(args)\n\n\t\tcontrol_args = dict()\n\n\t\t# Control_args from the yaml is the first of the args, and not\n\t\t# the rest by the yaml, so we can just iterate the args in the order\n\t\t# in the nested dict and merge those into the spec if it was not present in\n\t\t# the main spec (which can then be merged in the spec)\n\n\t\tcontrol_args = dict(map(lambda z: control_args.setdefault(z['key'], {}), spec['args']))\n\t\tfor arg in spec"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t# TODO: implement in sub module\n\t\tcontrol_args = {}\n\t\tnested_args = []\n\n\t\t# split_args is supposed to work for legacy reasons\n\t\tparsed = utils.config_from_file(self._options.raw_spec)\n\t\tif parsed and args.get('flat_args', ''):\n\t\t\tcontrol_args, nested_args = parsed.split_args()\n\t\t# check for custom\n\t\tif self._options.custom:\n\t\t\tcontrol_args = utils.add_custom_options(control_args, self._options.custom)\n\n\t\treturn control"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t\n\tassert len(args) is 1\n\t# get the inputs dict\n\tinputs = dict(args[0].values())\n\n\tif \"climate\" in inputs:\n\t\tif \"custom\" in inputs[\"custom\"]:\n\t\t\tcustom_var_spec = inputs[\"custom\"][[\"climate\"]]\n\t\t\tif len(custom_var_spec) == 3:\n\t\t\t\tparam_str = custom_var_spec[0][0]\n\t\t\t\t# this is the custom data so we need to convert for\n\t\t\t\t# the climate:\n\t\t\t\tparam_name = custom_var_spec["}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\ndef get_custom_dict_and_control_args(args):\t\"\"\"\n\tSplit input arguments to control the custom behavior.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\t\n\n\n### Playbooks ######################################################################\n\nclass Playbook(YamlObject):\n\tdef __init"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tif not isinstance(args, dict):\n\t\t\traise ModuleError(\"Argument 'args' must be a dict\")\n\t\tcontrol_args = args.get('control_args')\n\t\tnested_args = args.get('nested_args')\n\t\treturn (control_args, nested_args)\n\n\tdef get_nested_custom_and_control_list(self, control_list):\n\t\tif not isinstance(control_list, (list, tuple)):\n\t\t\traise ModuleError(\"Argument 'control_list' must be a list\")\n\t\tcontrol_list = sorted(control_list"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\nclass YamlParser:\n\t\"\"\"\n\tYAML parsing class used in ansible_playbooks class.\n\tAllows access to a collection of YAML files\n\t\"\"\"\n\tdef __init__(self):\n\t\t# YAML\n\t\tself.yml_output = None\t\t\t# the file that will get the YAML output\n\t\t\n\t\t# Modules\n\t\tself.mods_path      = None\t\t\t# path relative to Modules directory\n\t\t\n\t\t# Python\n\t\tself.python_code      = None\t\t\t# the code that should be run inside the specified file\n\t\t\n\t\tself.python_module  = None\t\t\t"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tvars_dict = deepcopy(vars_dict)\n\t\tfor k, v in iteritems(vars_dict):\n\t\t\tif k not in extra_vars:\n\t\t\t\textra_vars.append(k) \n\t\t\tif k not in vars_dict:\n\t\t\t\tvars_dict[k] = v\n\treturn vars_dict\n\n\ndef get_vars_to_vars(vars_list, vars_by_name, remove_vars=True):\tdef get_dict_for_var_name(var, to_remove=False):\n\t\tresult = {}\n\t\tfor name in vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars and not callable(extra_vars):\n\t\t\tprint('not implemented! \\n\\n'+\n\t\t\t\t\t'Example:\\n\\n' \\\n\t\t   \t'  { \"arg1\": \"b.variable1\", \"arg2\": \"b.variable2\"}\\n\\n')\n\t\t\treturn vars_dict\n\t\tfor var in variables:\n\t\t\tif isinstance(var, list):\n\t\t\t\tmerged_vars = merge_extra_vars((var,) + extra_vars)\n\t\t\telse:\n\t\t\t\tmerged_vars = merge_extra_vars(vars_dict[var"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars == None: extra_vars = vars_dict\n\t\tdiff = {}\n\t\tfor k in extra_vars:\n\t\t\tif (k not in vars_dict) or (not vars_dict[k]):\n\t\t\t\tdiff[k] = { 'comment' : 'Var %s not found in vars_dict' % k }\n\t\treturn vars_dict, None, diff\n\nclass Plugin:\n\t\"\"\"\n\tPlugin Class.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tInit plugin using ``name``.\n\t\t\"\"\"\n\t\tself"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\tvars_dict.update(extra_vars)\n\t\n\treturn vars_dict\n\ndef get_variables(args):\n\tvariables = variables_manager().get_variables(args)\n\tresult = {}\n\tfor name, value in variables.iteritems():\n\t\tif name not in result:\n\t\t\tresult[name] = None\n\t\tresult[name] = sanitize_variables(value)\n\t\n\treturn result\n\n\ndef get_options(args):\n\toptions = options_manager().get_options(args)\n\t\n\tresult = {}\n\tfor item"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor k,v in extra_vars.iteritems():\n\t\t\tkeys = extra_vars.get(k,[])\n\t\t\tvv=vars_dict.get(k,{})\n\t\t\t\n\t\t\tif isinstance(vv,str) and not isinstance(vv,bytes): vv=bytes(vv).encode('ascii')\n\t\t\t\tvv=codecs.encode(vv, \"hex\")\n\t\t\telse:\n\t\t\t\tvv.update(vars_dict.get(k,[]))\n\n\t\treturn vars_dict, extra_vars\n\n\n#----\n# The following is a list of the items to be merged for the"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor k,v in iteritems(vars_dict):\n\t\t\tif k in extra_vars:\n\t\t\t\tif v not in extra_vars:\n\t\t\t\t\textra_vars.append(v)\n\n\t\t\t\t# Merge ``extra-vars`` with ``v`` in a new dict, if the key doesn't exist yet.\n\t\t\t\tif k not in v:\n\t\t\t\t\tv[k] = v\n\n\n\n\t\t\t# Add all items to a new dict\n\t\t\tif k in vars_dict:\n\t\t\t\tif isinstance(vars_dict[k], dict):\n\t\t\t\t\tif not isinstance(v, MutableMapping):\n\t\t\t\t\t\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\t\n\t# remove extra-vars from vars_dict, if present\n\tfor item in extra_vars:\n\t\tif item in vars_dict:\n\t\t\textra_vars.remove(item)\n\t\n\t# replace items in vars_dict with new dictionary\n\tfor item in vars_dict:\n\t\tif item in vars_dict:\n\t\t\tvars_dict[item] = vars_dict[item] - extra_vars\n\n\t# write out the rest\n\tif extra_vars:\n\t\tvars_dict = vars_dict.copy()\n\t\tvars_dict[\"$extras\"] = \"\\"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars is not None:\n\t\t\tvars_dict = dict((k, v) for k, v in vars_dict.items())\n\t\tvars_dict.update(extra_vars) if extra_vars else None\n\n\t\ttmp = {}\n\t\tfor k in sorted(vars_dict.keys()):\n\t\t\ttmp[k] = vars_dict[k]\n\n\t\treturn tmp\n\n\t@property\n\tdef extra_options(self):\n\t\treturn self._extra_options\n\n\tdef _parse_template(self, template_str):\n\t\t\"\"\"\n\t\tParse ``template"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor var in extra_vars:\n\t\t\tif not isinstance(var, types.ModuleType):\n\t\t\t\traise ValueError(\"extra_vars must be a module\")\n\t\t\tvars_dict[var] = extra_vars[var]\n#\n# @register_model_class(\"Model\")\n# @register_input_spec({\n#\t\t\"__inputspec\": InputSpec(\n#\t\t\tinput_shape=(None,), input_dtype=np.object, dtype=np.object),\n#\t})\n# @register_output_spec(BaseFSO, \"Model\")\n#"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor key, var in vars_dict.items():\n\t\t\tif vars_dict[key] is not None:\n\t\t\t\tif isinstance(var, (tuple, list)):\n\t\t\t\t\tvars_dict[key] = merge_extra_vars(vars_dict[key], var)\n\t\t\t\telse:\n\t\t\t\t\tvars_dict[key] = var\n\n\n\t# Replace environment variables with provided values\n\toldvars_dict = os.environ.get('XCODE_ENV_VARS', {}).copy()\n\tos.environ['XCODE_ENV_VARS'] = dict(\n\t\t\"CODE_"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t# TODO: replace this with the CLI\n\t\textra_vars['ansible_playbook'] = playbook_path\n\t\treturn ir_workspace.send(ir_plugin, extra_vars, verbose=verbose)\n\texcept RuntimeError as e:\n\t\treturn e\n\n\treturn await ansible_playbook(ir_workspace, ir_plugin, False, True, extra_vars)\n\ndef ansible_playbook_playbook_pull(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tPushes a playbook file to the infrared workspace.\n\n:param ir_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\t\n\tif verbose: debug(\"Calling ansible_playbook() module...\")\n\trun(ir_workspace, ir_plugins, playbook_path, None, debug=debug)\n\n\tif verbose: debug(\"Calling ansible_playbook() module...\")\n\trun(ir_workspace, ir_plugins, playbook_path, verbose, debug=debug)\n\nimport numpy as np # from scipy.ndimage import imread, imresize\nfrom tastypie.exceptions import TastypieException\nfrom django.conf import settings\n#from django.contrib.auth.models import User\nfrom django.contrib"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\t\n\tif verbose is None:\n\t\tverbose = 0\n\t\n\tcommand_prefix = \"ansible-playbook %s/ansible-playbook\" % ir_plugin.uri\n\t# FIXME: add tests for plugin-level variables\n\targs = {\n\t\t\"cli\": \"--quiet\",\n\t\t\"dry\": \"--dry-run\",\n\t\t\"script\": \"--name\",\n\t\t\"playbook_path\": playbook_path,\n\t\t\"verbose\": verbose\n\t}\n\t\n\tif ansible_args is None:\n\t\tansible_args = \"\"\n\t\n\t\n\textra_vars = {\n\t\t\"ansible_path"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ndef is_ansible_playbook(playbook_path):\n    return os.path.splitext(playbook_path)[0] in ['.yml', '.ts']\nif not is_ansible_playbook(\"ansible-playbook\"):\n    sys.stderr.write(\"ansible-playbook must end with '.yml'\")\n    sys.exit()\n\nif __name__==\"__main__\":\n\n    main()\n\nfrom twisted.web import resource, static\r\n\r\ndef get_session_state(session):\r\n\tsession['session_state'] = {}\r\n\treturn session['session_id']\r\n\r\ndef get_last"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\tpass\t# TODO: Add this\n\ndef ansible_playbook_cli(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\t\textra_vars=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook-cli' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\nclass InfraredPlugin:\n\t\"\"\"\n\tHandles running tasks and gathers the tasks.\n\t\"\"\"\n\n\tdef __init__(self, r, ir_workspace):\n\t\t\"\"\"\n\t\t:param r: an Infrared workspace object\n\t\t:param ir_workspace: the 'ir_workspace' object\n\t\t:return: None\n\t\t\"\"\"\n\n\t\tself.ir_workspace = ir_workspace\n\n\t\tself.ir_task = r\n\n\t\tself.task_name = None\n\t\tself.task = None\n\n\n\tdef _initialize(self):\n\t\tif self.ir_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t_logger_output(ir_plugin, \"Invoking a playbook \" + playbook_path)\n\n\t\tret = None\n\t\tblock = None\n\t\twhile len(_out) >= _MAX_BLOCK_WIDTH:\n\t\t\tblock = _out[0]\n\t\t\t_logger_output(ir_plugin, \"Invoking a block of \" + str(_MAX_BLOCK_WIDTH) + \" bytes\", block)\n\t\t\tblock = block.strip()\n\t\t\t_logger_output(ir_plugin, \"Invoking a playbook \" + playbook_path, block)\n\t\t\ttry:\n\t\t\t\tret ="}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t# pylint: disable = global-statement\n\tlogging.basicConfig(\n\t\tfilename=playbook_path,\n\t\tformat=\"%(asctime)s.%(msecs)03d:%(levelname)s::%(message)s\",\n\t\tdatefmt='%Y-%m-%d %H:%M:%S',\n\t\tlevel=logging.INFO\n\t)\n\ttry:\n\t\tansible_playbook.__package__\n\texcept AttributeError:\n\t\targparse.ArgumentParser.add_argument(\n\t\t\t'--playbook-path',\n\t\t\tdest='playbook_path',\n\t\t\tmetavar='<ANSIBLE_PATH>',\n\t\t\thelp='The"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\nclass Informatizer(object):\n\t\"\"\" Handles the inventory of tasks in /ansible.ini /etc/ansible.\"\"\"\n\t\n\t_INFA_FILE=\"/ansible/ansible\"\n\t# Default arguments of the playbook\n\t_DEFAULT_ARGS = {}\n\t__DEFAULT_PLAYBOOK = \"/ansible/ansible/playbooks\"\n\t# Default arguments of the plugin when playbook_path is missing\n\t__GET_HOST_PLUGINS = \"hosts\"\n\t\n\tdef __init__(self, inventory=None, debug=0, debug_hosts=None, run=None, debug_patterns="}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\tassert sys.version_info < (3, 0), \"Python 3 only supports Python 3 as well.\"\n\t\tassert \"ansible_args\" in extra_vars, \"Unknown keyword arguments are set.\"\n\n\t\tif verbose: verbose = verbose.upper()\n\n\n\t\tif 'ansible_playbook' in extra_vars:\n\t\t\targs = extra_vars['ansible_playbook']\n\t\t\tif isinstance(args, str):\n\t\t\t\targs = shlex.split(args)\n\n\t\tif 'ansible_playbook' in extra_vars:\n\t\t\tif isinstance(args, dict):\n\t\t\t\tansible_playbook(ir"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif len(ir_workspace.groups)!= 2 or not (ir_plugin or isinstance(ir_plugin, InfraredPlugin)):\n\t\t\t\t\traise InfraredPluginError('invalid group name passed to Ansible CLI')\n\t\tresult = dict() # result[group_name] = the group, is the inventory, and the groups list\n\t\t\tfor group_names in ir_workspace.groups.values():\n\t\t\t\tfor group in ir_workspace.groups[group_names]:\n\t\t\t\t\tansible_vars = dict(ansible_vars.copy())\n\t\t\t\t\tfor variable in group:\n\t\t\t\t\t\tansible_vars"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# Make sure we have the workspaces that were set up for the plugin\n\t\tif vars_dict and vars_dict['name'] not in vars_dict:\n\t\t\tcli_args = [cli_args]\n\t\t\n\t\tif vars_dict and vars_dict['name'] in gr_env:\n\t\t\t# Create a working workspace that provides the variables\n\t\t\tworkspace = gr_env[vars_dict['name']]\n\t\tif vars_dict and not vars_dict['name'] in ir_env:\n\t\t\t# This allows us to set defaults in all vars\n\t\t\toptions = []\n\t\t\tfor"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tcli_args.extend(vars_dict.get(\"playbook\", []))\n\t\tvars_dict_to_list(vars_dict.get(\"ansible_args\", []))\n\n\t\tprint(\"*** Playbook started from inventory\")\n\n\t\tplaybook = IPlaybook(ir_workspace)\n\t\tplaybook.init_playbook()\n\n\t\tfor playbook_name in playbook.get_playbook().keys():\n\t\t\t# get variables for playbook\n\t\t\tplaybook_vars = playbook.get_playbook()\n\t\t\tvariables = playbook_vars.get(\"ansible_vars\", [])\n\n\t\t\t# play directly, if needed\n\t\t\tif"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n#\tStart the playbook\n\tlog = logging.getLogger('ansible.cli')\n\tbase_vars = utils.get_config_vars()\n\topts = {'host': base_vars.get('host', 'localhost'),\n\t\t\t'playbook_opts': base_vars.get('playbook_opts', 'playbooks'),\n\t\t\t'playbook_opts_files': base_vars.get('playbook_opts', 'playbooks.yml'),\n\t\t\t'task_cmd': base_vars.get('task_cmd', 'ansible-playbook'),\n\t\t\t'task_cmd_args': base"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\n\timport time\n\tnow = datetime.datetime.now()\n\ttime_elapsed = now - last_modified\n\t\n\tif (time_elapsed.seconds > 900):\n\t\tlast_modified = datetime.datetime.now()\n\t\t\n\t\ttime_to_run = time.time()\n\t\tif (last_modified > time_to_run.seconds):\n\t\t\tlast_modified = time_to_run\n\t\n\t# run cli\n\tresult = _run_ansible(cli_args, vars_dict, ir_workspace, ir_plugin)\t\n\t\n\t# calculate time difference\n\tdiff"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\targs = vars_dict.copy()\n\t\targs.update(cli_args)\n\t\t# In case of a role we don't need to override the config module for the role\n        if isinstance(args.get(\"role\"), types.StringType):\n\t\t\tif args.get(\"config\"):\n\t\t\t\targs[\"config\"] = Config.load_from_file(args[\"config\"])\n\t\t\targs[\"plugins\"] = None\n\t\tfor key, value in args.iteritems():\n\t\t\targs[key] = cli_arg(value)\n\t\t# Make sure args are the same type as args[...]\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# load additional vars dict to pass to ansible cli\n        import re\n        # import argparse\n        # parser = argparse.ArgumentParser(\n            #    description='AnsibleCLI for Infrared Ware, with no args in extra vars but Ansible API',\n            #    epilog='usage: ansible-inventory create_ir_playbook.py '\n            #    'vars_dict: the variables dict')\n        # parser.add_argument('inventory',\n        #    nargs='?',\n        #    help=\"inventory name, e.g.,'my_inventory'\")\n        # vars = parser.parse_args()\n        # vars_dict ="}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tresults = {}\n\t\t# initialize variables to be set in the CLI\n\t\t# FIXME: for now, they will be overwritten by CLI later\n\t\tvars_dict['__ansible__'].update(vars_dict)\n\n\t\tif cli_args:\n\t\t\tcli = AnsibleCLI(cli_args, **vars_dict)\n\t\t\tresults = cli.run()\n\t\telse:\n\t\t\tresults = cli.run()\n\n\t\treturn results\n\n\tdef wait(self, hosts, playbook_path, timeout, run_type=\"playbook\", **kwargs):\n\t\t\"\"\"\n\t\tWaits until the playbook gets done"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t           \n\t# FIXME: Do this after refactoring config module?\n\t# FIXME: Add the required args to our module\n\tplaybook = AnsiblePlaybook(ir_workspace, vars_dict, args)\n\tres=playbook.run(ir_plugin)\n\treturn res\n\t\t \n\n#\n# Main execution function\n#\n\n\nif __name__ == \"__main__\":\n\tif __name__==\"__main__\":\n\t\t_run_playbook(__name__,__doc__,__file__)\n#     Copyright 2016 Netflix, Inc.\n#\n#     Licensed under the Apache License"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tansible_results = cli_runner_runner.run_ansible(vars_dict, ir_workspace, ir_plugin)\n\n\t\tansible_results_dict = {}\n\t\tfor key, val in ansible_results.items():\n\t\t\tansible_results_dict[item] = val\n\n\t\treturn ansible_results_dict\n\n\t@staticmethod\n\tdef check(options, vars_dict):\n\t\t\"\"\"\n\tPerform a playbook\n\t\"\"\"\n\t\tif not check_playbook_enabled(options):\n\t\t\tcli_args = [ \"--check\", options.playbook, cli_args ]\n\t\t\t"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor key, value in values_dict.items():\n\t\t\tvalues_dict[key] = process._convert_str(value)\n\t\treturn values_dict\n\n\tdef map_none_to_args(self, parser_name, values_dict):\t\"\"\"\n\tWhen the value cannot be converted, we assume the argument type to be text. This can make\n\tit impossible to convert certain values if it doesn't have an explicit type to be set\n\t\"\"\"\n\t\tvalues_dict = _convert_none_to_str(values_dict)\n\t\tif '\\0' in values"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = values_dict.split('&')\n\t\tfor item in values_dict:\n\t\t\tval_type = item.split('=')\n\t\t\tif len(val_type)!= 2:\n\t\t\t\tcontinue\n\t\t\tvalue_type = val_type[0]\n\t\t\tvalue_val = values_dict[val_type[1]]\n\t\t\tif type(value_val) == str:\n\t\t\t\tvalue_val = value_val.encode('ascii')\n\t\t\tvalue_val = value_val.decode('unicode_escape')\n\t\t\t# The value must not have two quotes around\n\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = values_dict[:]\n\t\tfor key, value in values_dict.items():\n\t\t\tif type(value) is unicode:\n\t\t\t\tval = value.encode('utf8') # FIXME: we would be a Unicode string in unicode handling here\n\t\t# do some hacks and fix strings to unicode before returning\n\t\t# FIXME: replace with \"replace_\" but don't know why this could be used by the other processes\n\t\tvalues_dict = filter_keys(values_dict, ('name', 'type', 'default'))\n\t\tvalue_dict = dict([item for item in"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif not values_dict:\n\t\t\tvalues_dict = {}\n\t\tfor value_name, value_type in values_dict.items():\n\t\t\t\tvalue_value = values_dict.get(value_name, None)\n\t\t\t\tif value_value:\n\t\t\t\t\tif value_type == \"string\":\n\t\t\t\t\t\tvalue_value = utils_putils.convert_to_str(value_value)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvalue_value = { \"valid\" : utils_putils.validate_str(value_value) }\n\n\t\t\t\tvalues_dict.setdefault(value_name,"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = dict(utils.set_non_ascii_command_values(values_dict))\n\t\t_add_non_cli_argument(parser_name, values_dict)\n\n\t\tif parser_name in self.subcommands:\n\t\t\t# add all subcommands which must come in here\n\t\t\tfor subcommand in self.subcommands[parser_name].subcommands:\n\t\t\t\tif \"=\" in subcommand.split(\"=\")[0]:\n\t\t\t\t\tvalues_dict[\"__subcommand\"] = Subcommand(command=subcommand, name=subcommand)\n\n\t\tif not parser_name"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = dict(values_dict)\n\t\tfor key in values_dict.keys():\n\t\t\tvalue = values_dict.get(key)\n\t\t\tif value!= None:\n\t\t\t\tvalue = value.lower()\n\t\t\tvalues_dict[key] = value\n\t\treturn values_dict\n\n\tdef get_network_connection(self, uri, net_name=\"testnet\"):\n\t\t\"\"\"\n\tFetches the connection to a given connection object.\n\tThe uri, net_name -- the connection name (default to libvirt default) \n\twill be used.\n\t\n\t\tThis function"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\n\n\tfor key in values_dict:\n\t\tif key in values_dict[parser_name]:\n\t\t\tif type(values_dict[parser_name][key])!= type(\"string\"):\n\t\t\t\tparser_name += '_convert_%s' % key\n\t\t\t\tparser_name = keys_dict[parser_name].replace(key, \"\")\n\t\t\t\tvalues_dict[parser_name] = value_conversion._convert_string\n\t\t\telse:\n\t\t\t\tparser_name += '_convert_%s' % key\n\t\t\t\tparser_name = keys_dict[parser_name].replace(key, \"\")"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tassert isinstance(values_dict, dict)\n\t\tassert \"name\" in values_dict\n\t\tfor key in value_dict:\n\t\t\tassert key in values_dict, \"Unknown argument: %s\" % key\n\t\t\tif key in [\"-A\", \"-l\", \"--list\"]:\n\t\t\t\tself._convert_bool_to_list(values_dict[key], values_dict[key])\n\t\t\telif key in (\"--list\", \"--debug\"):\n\t\t\t\tdebug = False\n\t\t\telif key in (\"-V\", \"--verbose\"):\n\t\t\t\tself._convert_bool_to_bool(values_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = dict( values_dict )\n\t\tfor (key, arg) in values_dict.items():\n\t\t\t# Only do conversions if values_dict.get(key)\n\t\t\tif isinstance( values_dict[key], str ):\n\t\t\t\tvalues_dict[key] = values_dict[key].decode()\t\n\t\t\t# Only do conversions if values_dict.get(key) contains string \n\t\t\tif isinstance( values_dict[key], list ):\n\t\t\t\tfor item in values_dict[key]:\n\t\t\t\t\t# If argument is of type list:\n\t\t\t\t\tif not"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tv = {}\n\t\tfor arg_name, arg_val in values_dict.items():\n\t\t\tv[arg_name] = value_to_str(arg_val)\n\n\t\tv[\"help_command\"] = \" \".join(v[\"help_command\"].split())\n\t\tv[\"help_command_args\"] = values_dict\n\t\tv[\"help_no_wait\"] = v['help_no_wait'].to_string()\n\t\tv[\"help_command_no_wait_string\"] = value_to_str(v[\"help_command_no_wait"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tmeta_dict = {}\n\t\twith open(os.path.join(plugin_dir), 'r') as f:\n\t\t\tfor line in f:\n\t\t\t\tx = line.strip()\n\t\t\t\tif x in plugin_spec_flatten_dict:\n\t\t\t\t\tcontinue\n\t\t\t\tx = {k:v for k,v in _strip_dict(x).items()}\n\t\t\t\tmeta_dict[x] = x\n\t\treturn meta_dict\n\n# vim: ts=4 :expandtab\n#!/usr/bin/env python\nfrom __future__ import with_statement\nimport sys,"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\toutput = {}\n\t\tplugin_spec = get_plugin_spec_for_dir(plugin_dir)\n\t\tfor plugin_attr, plugin_value in plugin_spec.items():\n\t\t\t\tsplit_val = plugin_value.split('=')\n\t\t\t\tout = {}\n\t\t\t\tfor split_val in split_val:\n\t\t\t\t\tsplit_val = split_val.replace('\"', '').strip()\n\t\t\t\t\tout[split_val] = plugin_attr +'' + split_val\n\t\t\toutput[plugin_attr] = out\n\t\treturn output\n\n\tdef get_plugin_spec_for_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\t# get the spec-paths, then get all of them\n\t\tspec_paths = get_config('plugin_specs', {'plugin_dir': plugin_dir}) # this will raise exception if the plugin has no properties\n\t\tall_dicts = flatten(get_config_files_list(spec_paths))\n\t\t# now for each spec, add it to the list of all plugins specified in the spec_paths dict\n\t\tfor spec in all_dicts:\n\t\t\tplugin_spec = get_plugin_spec(spec)\n\t\t\tif plugin_spec_has_keys(plugin_spec)"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n##\t\t\t\tfile_data = os.path.join(plugin_desc_dir,'plugin_spec','plugin_spec_file_data')\n##\t\t\t\t\n##\t\n##\t@classmethod\n##\t\tdef flatten_files_data(cls, plugin_spec_dir):\n##\t\t\treturn cls.load_json(os.path.join(plugin_spec_dir,'plugin_spec','plugin_spec_file_data'))['plugin_specs']\n\t\n\t\n##\t\t\t\t\n##\n## def save_json(self, filename):\n##\n##\tjson_file ="}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n# ################################################################################\n#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n# \tGet directory from package\n# ################################################################################\ndef get_plugin_spec_dir_path(plugin_dir):\t\n# \t\treturn os.path.join(*get_plugin_spec_file_paths(plugin_dir))\n\n# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n# \tLoad plugin spec from the path\n# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ndef load_plugin_spec_from_path(plugin_dir):\n# \t\treturn os.path.join(*read_plugin_spec_file"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\t\n\t# Parse the plugins directory to a dictionary\n\tflat_dir = os.path.normpath(plugin_dir)\n\tplugin_path_to_dict = {}\n\n\tif not os.path.isfile(plugin_path_to_dict):\n\t\traise InvalidPath('{0} is not a valid path to the plugin'.format(plugin_path_to_dict))\n\n\t# Load the plugins\n\tplugin_info = load_json_file(os.path.join(flat_dir, 'plugins.json'))\n\tplugin_settings = plugin_info['settings']\n\t\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\ndef get_plugin_spec_name(plugin_dir):\n\t\n\tdef _parse_name(d):\n\t\tif type(d) is list:\n\t\t\treturn''.join(map(lambda x: x.strip('/\\\\'), d))\n\t\telse:\n\t\t\treturn d\n\t\n\tname = re.sub(re.compile(r'/[^\\\\']+', re.UNICODE), \"\", get_plugin_spec_flatten_str(plugin_dir))\n\treturn name.replace(' ', '_').lower()\n\n# @TODO: Implement all the necessary code\n## This is just an"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\t\n\tplugin_file = os.path.join(plugin_dir, \"{0}.{1}.json\".format(CONFIG_SHOWS, VERSION))\n\tplugin_string = \"\\n\".join([\"plugin {0}\".format(p.capitalize() if len(p) % 2 else p) for p in plugin_file.split(\"_\")])\n\t\n\treturn {\"yaml\": plugin_string}\n\n# Returns a dictionary which is a simple interface for making the process to run\n# It needs not be as flexible as I have been writing a simple interface so the JSON file\n# will be pretty readable ("}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tconf_dict = {}\n\t\tif os.path.exists(os.path.join(plugin_dir, \"plugins\", \"config.json\")):\n\t\t\tconf_dict = Plugin.read_yaml(os.path.join(plugin_dir, \"plugins\", \"config.json\"))\n\t\telse:\n\t\t\treturn conf_dict\n\n\ndef extract_plugin_config(conf_dict, is_local=False):\n\tdef get_plugin_specs(plugin):\n\t\tplugin_conf_dict = get_plugin_spec_flatten_dict(plugin)\n\n\t\t# plugin."}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn dict(\n\t\t\tplugin_source=plugin_dir,\n\t\t\tplugin_target=PLUGIN_TARGETS[plugin_target],\n\t\t\tplugin_name=plugin_spec.get(plugin_name),\n\t\t\tplugin_file={plugin_file})\n\n# Create a flat namespace\nget_namespace_spec = lambda plugin_dir, plugin_spec: get_plugin_spec_flatten_dict(plugin_dir)\t# @UnusedVariable\ndef get_namespace_spec_with_template(plugin_dir, template_file):\t# @UnusedVariable\n\t\"\"\"\n\tCreates a"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif self.config:\n\t\t\tret = self.config_get_config()\n\t\t\tif ret == 'y':\n\t\t\t\tos.environ['CONFIG'] = self.configfile\n\t\t\telse:\n\t\t\t\tos.environ['CONFIG'] = self.configfile\n\t\telse:\n\t\t\tret = self.default_config\n\t\t\tos.environ['CONFIG'] = self.configfile\n\t\treturn ret\n\t\n\n\tdef file_load(self, *files):\n\t\t\"\"\"Reads list of files (in utf-8) from the environment.\"\"\"\n\t\tret_files = []\n\t\tfiles = [f for f"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif 'LD_LIBRARY_PATH' in os.environ:\n\t\t\tenv = os.environ['LD_LIBRARY_PATH']\n\t\t\tif os.environ['PATH'] is not None:\n\t\t\t\tself.config_path = os.getenv('PATH')\n\t\t\telse:\n\t\t\t\tself.config_path = os.environ['SystemRoot'] + os.environ['ProgramFiles'] / \"config\"\n\t\telse:\n\t\t\traise Exception(\"Environment variable LD_LIBRARY_PATH not defined in environment variables!\")\n\n\tdef load_config(self, data):\n\t\tif data:\n\t\t\tfile ="}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self.env_dict['env'].get()\n\t\texcept ValueError:\n\t\t\traise IOError('Invalid config path specified: %s' % self.env_dict['env'])\n\n\t\tif not value:\n\t\t\traise EnvironmentError('No value was given to config path')\n\n\t\tself.env_dict['env']['PATH'] = value\n\n\t\tself.__config_template.close() # close the environment file\n\n\tdef __repr__(self):\n\t\treturn 'ConfigEnv'\n\n\tdef __str__(self):\n\t\treturn str(self.env_dict)\n\n\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not self.environment: self.environment = self.environ['DEFAULT']\n\n\t\t\tconfig_path = self.environment['CONFIG_PATH']\n\t\t\tif config_path and not os.path.exists(config_path):\n\t\t\t\twith open(config_path) as fp:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfor line in fp.readlines():\n\t\t\t\t\t\t\tif line:\n\t\t\t\t\t\t\t\tself.env[config_path] = line.strip()\n\t\t\t\tlogging.debug(\"Set defaults\")\n\n\t\t\tif not self.dry_run:\n\t\t\t\tsettings = dict((k, v) for k, v in"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\treturn self._config.get('CONFIG_FILE')\n\n\tdef load(self, path):\n\t\t\"\"\"\n\t\tLoad the environment variable, it is loaded as a dict. An exception is raised if\n\t\tpath is an invalid file.\n\t\t\"\"\"\n\t\treturn self._config.get('ENVIRONMENT')\n\t\t\n\tdef set_environment(self, value):\n\t\t\"\"\"\n\t\tSet the environment variable.\n\t\tIf an environment variable is specified, it is loaded as the default one.\n\t\tIf an environment variable is invalid, it is returned from os.environ\n\t\twhich is used to unset"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.config.save()\n\n\t@staticmethod\n\tdef get_config(config = {}, override = False):\n\t\t\"\"\"\n\t\tFind the config in config.get() and use it as default value for any\n\t\toverride settings. If `override` is true, the defaults will only be loaded if\n\t\tthere are existing overrides.\n\t\t\"\"\"\n\t\tif override:\n\t\t\tconfig['override'] = os.environ.get('PYSTANDALONE_OVERRIDE_DIR')\n\t\treturn ConfigReader.get_config_dict(config)\n\n\tdef has_config(self, path"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tfrappe.flags.in_install_os_module = False\n\n\t\tif self.conf_files:\n\t\t\tself.create_db()\n\n\tdef create_db(self):\n\t\t\"\"\"\n\t\tCreates the database. This must be called after any functions are\n\t\tcalled.\n\n\t\tOn first retrieval the database is done.\n\t\t\"\"\"\n\t\tself.create_database_structure()\n\n\t\timport frappe.db\n\n\t\tfrappe.init(frappe.defaults.get_app_name())\n\n\tdef create_database_structure(self):\n\t\tfrappe.db.create_table(\"db"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.config = {}\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself._config = os.environ.get('CONFIG')\n\n\tdef set_config(self, config):\n\t\t\"\"\"\n\t\tSet the configuration for path.\n\n\t\tIt is defined that the first time environment is initialized, then\n\t\tdefaults are used.\n\n\t\tArgs:\n\t\t\t\tconfig (list): A list of strings or dictionary representing\n\t\t\t\t  environment variables which can set the environment to.\n\t\t\t\t  If there are no environment variables, the environment\n\t\t\t\t overrides the 'default', which is empty.\n\n\t\t\"\"\"\n\t\tself._config = list(config)\n\t\tself._config.append('default')\n\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.configpath = self.configpath.replace('.py', '')\n\t\tself.envdir = os.path.join(self.configpath, 'env')\n\n\t\tif not os.path.isfile(os.path.expanduser(self.configpath)):\n\t\t\tprint('Error: Could not find config file \"' + self.configpath + '\" for path:'+ self.configpath)\n\t\t\treturn\n\n\t\twith open(self.configpath, 'r') as configfilehandle:\n\t\t\tself.envdata = json.load(configfilehandle)\n\t\t\tself.env_"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\t# The main entry point - we add all the command line subparsers, to\n\t\t# ensure that all the sub commands in subparser can understand the parser\n\t\t# command line arguments.\n\t\tfor subparser in root_subparsers:\n\t\t\tself.cli.add_parser(subparser)\n\n\tdef _parse_commands(self):\n\t\t\"\"\"\n\t\tParses the cli options specified in the options object itself. We will\n\t\tcall this in the same fashion as in get_cli().\n\t\t\"\"\"\n\t\tall_cli_opts = [] # list of Command instance flags we'll need a"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tfor spec in root_subparsers:\n\t\t\tname = spec.get_name_value('name')\n\t\t\tgroup = spec.get_name_value('group')\n\t\t\topts = spec.get_opt_value('cli', False)\n\t\t\troot_opts = spec.get_opt_value('cli', False)\n\t\t\tif root_opts:\n\t\t\t\targs = {}\n\t\t\t\targs['name'] = name\n\t\t\t\targs['group'] = group\n\t\t\t\troot_opts = '--%s' % args\n\t\t\t\troot_subparsers.insert(0, ROOT_CLS, args"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\troot_subparsers.insert(0, subparsers.parser)\n\n\tdef parse_options(self, options):\n\t\twith open(options.input, 'rb') as handle, open(options.output, 'wb') as handle:\n\t\t\thandle.seek(0, 2)\n\t\t\tfor line in handle:\n\t\t\t\tparts = line.split(':')\n\t\t\t\tfor index in range(0, len(parts), 2):\n\t\t\t\t\tif len(parts[index]) == 0:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tindex += 1\n\t\t\t\t\tcommand = parts[index]\n\t\t\t\t\tif not command in self."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\troot_subparsers.insert(0, self)\n\"\"\" Python Client for Spyne. \"\"\"\nimport sys\n\nclass Client(object):\n\tdef __init__(self, client_filename):\n\t\tself.clientfile = client_filename\n\t\tself.clientfile_prefix = \"client.py\"\n\n\tdef __bool__(self):\n\t\tprint('Spyne is starting server...')\n\t\tprint('Using client file: ', self.clientfile)\n\t\treturn True\n\n\tdef __del__(self):\n\t\tif self.__bool__() :\n\t\t\tsys.exit()"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tsuper(Command, self).__init__()\n\t\tself.subparsers = subparser.add_subparsers(title=\"subparser help\")\n\n\tdef run(self, argv):\n\t\t\"\"\"\n\t\tExecute the command by parsing all the arguments on the command\n\t\tline passed in.\n\t\t\"\"\"\n\t\t(args, option) = demisto.executeCommand(self.subparsers.parse_args(argv))\n\t\t\t\n\t\tif not args.debug:\n\t\t\tprint(\"arguments in error:\\n\" + str(args.exceptions.keys()))\n\n\t\tif option == '-h':\n\t\t\tif"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tpass\n\n\tdef create_subparser(self):\n\t\t\"\"\"\n\tCreates an entry point that performs any initialization for a parser.\n\n:raise InvalidOptions: when there is no parser specified.\n\"\"\"\n\t\tparser_name = self._construct_parser_name()\n\n\t\t# Add the current argument parser to the main entry point,\n\t\t# this allows us to modify options, etc.\n\t\tparser_subparsers = parser_name.add_subparsers(dest='parser_name')\n\n\t\tself._create_subparser(parser_name, parser_subparsers)\n\n\tdef"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself._add_opt_parser(\n\t\t\t'extend',\n\t\t\t[\n\t\t\t\t'-f',\n\t\t\t\t'-o',\n\t\t\t\t'--spec=',\n\t\t\t\t'-c',\n\t\t\t\t'-n',\n\t\t\t\t'--no-update',\n\t\t\t\t] + self.options.keys(),\n\t\t\troot_subparsers\n\t\t)\n\t\tparser = self._subparsers['extend']\n\t\tparser.set_defaults(func=self.call_extend_cli)\n\t\tparser.add_argument(\n\t\t\t'filename',\n\t\t\taction='store',\n\t\t\thelp = 'Name of spec file to extend'"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\t# The main subparser's parser holds its config options.\n\t\t# Add their options to the parser's subparser's\n\t\t# arguments.\n\n\t\t# Add everything inside a list which can be passed to the\n\t\t# extention so that subcommands can extend the whole set.\n\t\tself._add_arguments(root_subparsers, ArgumentGroup(action=ArgumentGroup.ADD))\n\n\t\t# Add all the options parsed as subcommands so that default options\n\t\t# can be passed in through with ArgumentParser.add_argument.\n\t\tself._add_arguments(root_subparsers, argparse)"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself.parser.set_subparsers(_extra_cli_options(root_subparsers), namespace=self.parser_namespace)\n\nclass SpecError(Exception): pass\nclass SpecFatalError(Exception): pass\nclass SpecRuntimeError(Exception): pass\n\ndef extend_command_line_args(parser, args):\tpass\n\tparser.add_argument('--no-source', action ='store_true')\n\tparser.add_argument('--no-target-file', action ='store_true')\n\tparser.add_argument('build-dir', default = 'build"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\troot_subparsers.append(self.cli_spec)\n\n\t@staticmethod\n\tdef add_cli_options(parser):\n\t\tparser.add_argument(\"--file\", help=\"file to use, if empty, the program will start from this directory.\")\n\t\tparser.add_argument(\"--section\", choices=[\"global\", \"local\", \"global_global\"], help=\"section to add to the namespace.\")\n\t\tparser.add_argument(\"--extension\", required=True, help=\"file extension\")\n\t\tparser.add_argument(\"--env-version\", type=str, help=\"version to add to"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not root.endswith(\"/\"):\n\t\t\troot += \"/\"\n\n\t\tif not path:\n\t\t\treturn False\n\t\telif root.endswith(\"/\"):\n\t\t\tpath = root[:-1]\n\t\t\troot = root[:-1]\n\n\t\tif root not in (\"\", \"/\"):\n\t\t\treturn True\n\n\t\treturn False\n\n\tdef get_pkg_config_root(self, cfgmap):\n\t\t\"\"\"\n\t\tReturns pkg_path, to which the config map has already been created\n\t\t\"\"\"\n\t\tfor o in cfgmap.get('packages', {}):\n\t\t\troot = \"\"\n\t\t\tfor p in o"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tpath = path.lstrip()\n\n\t\tif root == root or not path:\n\t\t\traise InvalidPathError('Invalid root path')\n\n\t\tpath = path[len(root):]\n\n\t\tif not path:\n\t\t\traise InvalidPathError('Empty root path')\n\n\n\t\treturn os.path.normpath(os.path.join(root, path))\n\n#!/usr/bin/env python\r\n\r\nfrom collections import defaultdict\r\n\r\nimport copy\r\nimport json\r\nimport logging\r\nimport lxml.html\r\n\r\n\r\nfrom e2e.util import get_dict, strip_xml, unconvert_"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not path.startswith(\"/\"):\n\t\t\treturn path\n\t\treturn remove_path(path, root)\n\n\tdef is_mount_point(self, file_name=None):\n\t\t\"\"\"\n\t\tReturn 1 for linux, -1 for Windows,\n\t\totherwise return 0 for other file names.\n\t\t\"\"\"\n\t\tis_file = \"\"\n\n\t\tif file_name is None:\n\t\t\tfile_name = self.__file__\n\t\tif self.__mount_info__ == u\"/bin/bash\":\n\t\t\tis_file = \"\\\\\"\n\t\tif self.__mount_info__ in"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\t# Remove empty files.\n\t\tif os.path.exists(path):\n\t\t\tos.remove(path)\t\t# Ensure path is empty\n\t\telse:\n\t\t\t# Not valid file, return None.\n\t\t\treturn None\n\n\t\t# Re-raise exception on success.\n\t\traise e\n\telif type(path)!=type(()):\n\t\traise InvalidPath(\"Path must be a string\")\n\telse:\n\t\t# Get filename from path\n\t\tfilename = os.path.split(path)[1]\n\t\t# Remove directory if exists.\n\t\tif os.path.isdir"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tpath = path.strip('/')\n\t\tif path == root:\n\t\t\traise Error(\"Could not find root at path '%s'\" % path)\n\treturn path\n\n\nclass PackageReader(object):\n\tdef __init__(self, config):\n\t\tself.path = None\n\t\tself.config = config\n\t\tself.pkg = None\n\t\tself.error = 0\n\n\tdef parse_command(self):\n\t\tself.cmd = self.config.env.DEST_BINARIES or [self.config.env.EXECUTABLE]\n\t\treturn [p for p in self.cmd."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif path == \"\" or os.path.lexists(root):\n\t\t\treturn\n\t\ttry:\n\t\t\tos.remove(path)\n\t\t\traise Exception(\"Unable to remove file from root!\")\n\t\texcept OSError:\n\t\t\tpass\n\n\t@staticmethod\n\tdef check_cache_in_directory(file_name):\t\t\t\"\"\"\n\tCheck that the cache is still up-to-date (i.e. it was untracked).\n\t\"\"\"\n\t\tcache_mtime = os.stat(file_name).st_mtime\n\t\tif cache_mtime > time.mktime(os."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not root or not path or not os.path.exists(root):\n\t\t\treturn False\n\t\tif not os.path.isdir(path):\n\t\t\traise ValueError(\"Can not open {} to remove root of {}\".format(path, root))\n\t\ttry:\n\t\t\tos.remove(path)\n\t\t\treturn True\n\t\texcept OSError:\n\t\t\tpass\n\t\texcept:\n\t\t\tpass\n\n\t\treturn False\n\n\tdef safe_copy(d, path):\n\t\t\"\"\"\n\t\tDeep copy method, raise exception on failure.\n\t\t\"\"\"\n\t\tif isinstance(d, bytes):\n\t\t\treturn"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif not path.endswith('/'):\n\t\t\tpath += '/'\n\n\t\tif root == '.':\n\t\t\tif os.path.isdir(path) and path!= '.':\n\t\t\t\tpath = path + '/'\n\n\t\t\t\ttry:\n\t\t\t\t\tutils.rmtree(path)\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\telif not os.path.isdir(path) and not path.endswith('/'):\n\t\t\traise Exception(\"Error removing root directory '%s' - it does not exists\" % path)\n\treturn path#!/usr/bin/env python\n\nfrom optparse import OptionParser\nimport os\nimport time\nimport json\nimport redis"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif path is None:\n\t\t\treturn root\n\t\tif not isinstance(root, basestring):\n\t\t\troot = os.path.join(root, 'root')\n\t\tif os.path.isdir(root):\n\t\t\ttry:\n\t\t\t\tshutil.rmtree(root)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tif not os.path.islink(root):\n\t\t\t\tpass\n\t\treturn root\n\n\tdef exists(self, env):\t\"\"\"\n\tFind and return whether a given environment (the root directory,\n\tspecified in the environment variable C{PATH_INFO}) is an actual file. "}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif path == '/':\n\t\t\treturn ''\n\t\tbasePath = path[root:].split(os.sep)\n\t\tdirList = os.listdir(basePath)\n\t\tfor dir in dirList:\n\t\t\tdir = os.path.normpath(os.path.join(basePath, dir))\n\t\t\tif dir!= 'common' and dir!= 'ui':\n\t\t\t\tif not os.path.exists(os.path.join(dir, '.git')):\n\t\t\t\t\traise GitError(\"can't find %s, %s - %s\" % (dir, root, repo))\n\t\t\t"}]}
