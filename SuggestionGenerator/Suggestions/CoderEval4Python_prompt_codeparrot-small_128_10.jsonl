{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\thours_per_day_t = nanoseconds / 86400.0 # 24hrs\n\thours_per_second = (hours_per_day_t * 3600 + 86400.0)/24.0\n\n\t# Calculate an hour by checking our calendar object (in this case our\n\t# local time starts at UTC)\n\tlocal_time = local_time_with_tz_offset(tz, tz=tz)\n\tlocal_time = local_time_with_tz_offset(tz, tz=tz)\n\tdt = local_time - local_time_"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\"\"\"\n\ttz = tz if tz is not None else time._NoValue\n\ttz = tz if tz is not None else localtime._NoValue\n\tif tz is None:\n\t\ttz = datetime.utcnow()\n\treturn tslib.to_datetime(nanoseconds, tz)\n\t\n\"\"\"\n\tHydrator for `Epoch` and `FloatTime` values.\n\n\t:param epoch:\n\t:param float time:\n\t:return: Epoch\n\t\"\"\"\ndef parse_epoch(epoch):\n\t\"\"\"\n\t\n\tParse data from a list of epoch values"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\ndef time_delta(seconds):\n\t\"\"\"\n\tConvert the time between Unix epoch and time. The default\n\ttime is 30 days, so the value `seconds` must stay the same that\n\tthe default value, like `hours` or `minutes`.\n\t:param seconds: Time in seconds. Time must be seconds since\n\tUnix epoch and so on.\n\n\t:return: The delta between Unix epoch and time.\n\t:rtype: float\n\t\"\"\"\n\t\nclass LocalTime(datetime.tzinfo):\n    \"\"\"\n\tTimezone info for Local time.\n\t\n\tSee also :func:`"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\t\n\t# The Hydra format uses `strftime` to handle floating point values, so we'll pass it in as floats\n\t# values that are not recognized by `strftime` internally so that the Hydra format uses values that the Hydra format ignores\n\t# it. This should not be necessary.\n\tif not tz:\n\t\ttz = get_timezone(context.HADRA_DATETIME_FORMAT)\n\t\n\treturn _format_time(nanoseconds, tz=tz)\n\n\ndef add(val, units=None):\n\t''' Add a `val` to the end"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\ndef utc_from_unix_timestamp(timestamp, tz=TIME_ZONE):\n\t\"\"\"\n\tConvert a unix timestamp `unix timestamp`_ to a UTC time.\n\t\n\t:param timestamp:\n\t:param tz:\n\t:return: UTC time\n\t\"\"\"\n\tutc = None\n\tfor component in timestamp.split(\" \"):\n\t\tif not component or component[0] == '-':\n\t\t\tcontinue\n\t\tutc = int(component)\n\t\tif utc!= 0:\n\t\t\tutc += int(utc) * 86400 * 1000\n\tutc = int(timestamp)\n\treturn utc\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\ndef time_hours_ago(dt):\n\t\"\"\"\n\tCompute the time of a `LocalTime` and return its hours.\n\t\n\t:param dt:\n\t:return: Time\n\t\"\"\"\n\t\ndef time_seconds_ago(dt):\n\t\"\"\"\n\tCompute the time of a `LocalTime` and return its seconds.\n\t\n\t:param dt:\n\t:return: Time\n\t\"\"\"\n\t\ndef timezone_from_date(dates):\n\t\"\"\"\n\tCalculate a timezone corresponding to one or more times of a number of timestamps in the past.\n\t\n\t:param dates"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\treturn nanoseconds.hour * 3600 + nanoseconds.minute\n\t\n\tdef is_local_time_value(self, t_value):\n\t\t\"\"\"\n\t\tCheck `Time` against the local times `t_value`, then return the first time `t_value`.\n\t\t\n\t\t:param t_value: Time to compare.\n\t\t:return: `LocalTime` value to compare to `t_value`.\n\t\t\"\"\"\n\t\treturn t_value\n\t\n\tdef is_utc_local_time_value(self, local_t_value):\n\t\t\"\"\"\n\t\tCheck `"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\ndef get_time():\n\t\"\"\"\n\tReturn the epoch time in UTC.\n\n\tThis value is the epoch time in Julian days. On the Julian day, on the other hand,\n\thas the default timezone.\n\n\t:return: Julian epoch time in UTC\n\t:rtype: (string)\n\t\"\"\"\n\n#####\n#  SEGMENTS\n#####\n\ndef seconds_between(seconds: Union[int, str], milliseconds: float) -> float:\n\t\"\"\"\n\t`Seconds in the future.\n\n\t:param seconds:\n\t:param milliseconds:\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\ndef _to_time(nanoseconds, tz=None, convert_to_local=True):\n\t\"\"\"\n\tReturns a Time object with timezone specified by `nanoseconds`.\n\t\n\t:param nanoseconds:\n\t:param tz: A time zone to convert\n\t:param convert_to_local: Whether to convert from local time to local time\n\t:return: Time object\n\t\"\"\"\n\tif tz is None:\n\t\ttz = timezone()\n\tif convert_to_local:\n\t\ttz = timezone.localtime(zero_zone())\n\t\t\n\treturn Time("}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\tif nanoseconds:\n\t\t\treturn nanoseconds\n\t\tseconds = nanoseconds / _Time.HOUR\n\n\t\tif tz is not None:\n\t\t\ttz = tz(*tz)\n\n\t\treturn datetime.datetime(datetime.MINYEAR, \tseconds, \tseconds, \tseconds, seconds, \tseconds, \tseconds, \tseconds)\n\n\ndef utc_to_str(dt=datetime_.utcnow()):\n    \"\"\"\n\tConvert a UTC datetime object to local time. Returns time object.\n\t:param dt:\n\t:return:\n\t\"\"\"\n\t#"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\tdef format_timedelta(x):\n\t\treturn '{0:d}:{1:02}:{2:02}m'.format(int_from_seconds(x),int_to_seconds(value))\n\t\t\n\treturn dehydrate_timedelta\n# This file is part of Indico.\n# Copyright (C) 2002 - 2020 CERN\n#\n# Indico is free software; you can redistribute it and/or\n# modify it under the terms of the MIT License; see the\n# LICENSE file for more details.\n\nfrom __future__ import unicode_literals\n\nfrom flask"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\t\n\tm = re.match(r'^(\\d+)([^\\d]+)\\s+(\\d+)\\s+(\\d+)([s|P]?\\d+\\.\\d+)$', value)\n\tif m is None:\n\t\traise ValueError(\"Value for timedelta cannot be formed\")\n\n\tnum_days = int(m.group(1))\n\tdays = (int(m.group(2)) if m.group(2) else 10) + (num_days if num_days else 0)\n\treturn datetime.timedelta(days)\n\n\ndef in_date(dt):\n\t\"\"\"\n\tCheck whether"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\ndef add_timedelta_type(s):\n\ttry:\n\t\tadd = __add_timedelta_type(s)\n\texcept:\n\t\tadd = type(s)()\n\treturn add\n\nclass timedelta_time(timedelta):\n\t\n\tdef __init__(self, hours=0):\n\t\ttimedelta.__init__(self, float(hours))\n\t\tself._delta = 0\n\t\n\tdef __timediff__(self, td):\n\t\tdelta = self._delta\n\t\thours = self._delta * td.days\n\t\treturn seconds_from_delta(delta)\n\t\n\tdef __add"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\treturn datetime.timedelta.to_value(value)\n\t\n\tdef __str__(self):\n\t\treturn time.strftime(\"%H:%M:%S %Y\", time.gmtime())\n\t\n\tdef __call__(self, value):\n\t\t\"\"\"\n\t\t`timedelta` values do not support subtraction: only\n\t\tdehydated values will be rounded to the nearest multiple of 10\n\t\tat the corresponding time.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttimedelta_value = datetime.timedelta(0)\n\t\t\tif issubclass(type(value), int):\n\t\t\t\ttimedelta_value = dt.timedelta("}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\ndef parsedatetime(value):\n\t\"\"\"\n\tDehydrate `datetime`, `datetime` or list\n\t:param value:\n\t:type value: list\n\t:return:\n\t\"\"\"\n\tsplit_value = s3_datetime.split(value)\n\tif len(split_value) == 1:\n\t\tvalue = split_value[0]\n\telse:\n\t\tvalue = datetime.strptime(split_value[0], \"%Y-%m-%d\")\n\treturn value\n\ndef build_duration(unit, hours, duration, format = \"%Y"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\nfrom core.models import Human, Card, Monster, Location, Media, Video, Fall\n\ndef get_location_for_date(year, month, day):\n\tif day!= 'null' and day!= 'None':\n\t\treturn Location.objects.filter(year=year, month=month, day=day).first()\n\telse:\n\t\treturn None\n\ndef get_today_time():\n\treturn datetime.now().date() + timedelta(days=7)\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\nclass date_format_string(object):\n\t\n\tdef __init__(self, value, format=None):\n\t\tself.value = value\n\t\tself.format = format\n\t\t\n\tdef __repr__(self):\n\t\treturn u\"<DateFormatString '%s' from '%s'>\" % (self.value, self.format)\n\n\nclass date_struct_value(object):\n\t\n\tdef __init__(self, value):\n\t\tself.value = value\n\n\tdef __repr__(self):\n\t\treturn u\"<DateStructValue '%s' from '%s'>\" % (self."}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t \n\treturn value/timedelta(0)\n\n\ndef dehydrate_timedelta_2(value):\n\t\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t \n\treturn value/timedelta(60)\n\n\ndef dehydrate_string(value):\n\t\"\tReturns a string representation of a string.\"\n\treturn \"'%s'\" % value if isinstance(value, str) else value.encode('utf-8')\n\n\ndef dehydrate_array(value"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\nclass DateTime(object):\n\t\"\"\"\n\tDate Time in the database.\n\t\n\t:type start: DateTime\n\t:param start:\n\t\n\t:type stop: DateTime\n\t:param stop:\n\t\n\t:type duration: DateTime\n\t:param duration:\n\t\n\t:type end: DateTime\n\t:param end:\n\t\n\t:type name: String\n\t:param name:\n\t\n\t:type date: DateTime\n\t:param date:\n\t\n\t:type datetime: DateTime\n\t:param datetime:\n\t\n\t:type datetime: DateTime\n\t:param datetime"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_list(value):\n\t\"\"\"\n\tDehydrator for list values.\n\t\n\t:param value:\n\t:type value: list\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate(value):\n\t\"\"\"\n\tDehydrator for numeric values.\n\t\n\t:param value:\n\t:type value: number\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_tuple(value):\n\t\"\"\"\n\tDehydrator for tuple values.\n\t\n\t:param value:\n\t:type value: tuple\n"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\nclass Probe(Protocol):\n\t\"\"\"\n\tBase class for :class:`probe` values, which is used to control the\n\tprobability of each feature.\n\t\n\t:param start: Value start.\n\t:type start: float\n\t:param value: Value.\n\t:type value: bool\n\t:param probability: Probability.\n\t:type probability: int\n\t:return:\n\t\"\"\"\n\t\t\n\t\n\t\n\nclass Gibbs(Probe):\n\t\"\"\"\n\tThe base class for Gibbs values.\n\t\n\t:param start: Value start.\n\t:"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\treturn value.strftime('%H:%M:%S')\n\nclass Time(object):\n\t__slots__ = (\"minute\", \"second\")\n\n\tdef __init__(self, value, minutes, second):\n\t\t\"\"\"\n\t\t:param value: Time of the minute\n\t\t:type value: Number\n\t\t:param minutes: Time of the minute\n\t\t:type minutes: Number\n\t\t:param second: Time of the minute\n\t\t:type second: Number\n\t\t:return: None\n\t\t\"\"\"\n\t\tself.minute = minutes\n\t\tself.second = seconds_from_seconds(value"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\ttry:\n\t\ttimestring = value.__class__.__name__\n\texcept:\n\t\treturn value\n\treturn Time(timestring)#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\ndef get_indepuration_for_time(value = None, max_time = 300, time = None):\n\t\"\"\"\n\tGiven a time object (e.g. a time string), computes the relative indepuration\n\tvalue to the time.  If either time or None is supplied, the current time\n\tis assumed to be a time string.  If the time is an instance of the\n\ttime object, it is treated as a time-equivalent between the time time\n\tinstances.  If either time or `time` is an instance of a time object,\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\nclass Time(object):\n\t\n\tdef __init__(self, value):\n\t\tself._value = value\n\t\n\t@classmethod\n\tdef from_dict(cls, d):\n\t\ttz = d[str(d.keys()[0])]\n\t\tdatetime_ = struct.unpack(\"b\", tz.value)[0]\n\t\tvalues = Time(datetime_.to_dict()[\"value\"])\n\t\treturn values\n\t\n\t@classmethod\n\tdef from_string(cls, tz_str):\n\t\ttz = tz_with_defaults(\n            tz_str,\n            time_zone_code=str(\""}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\nclass TimeStatistic(object):\n\t\"\"\"\n\tRepresents data retrieval of the relevant parameters.\n\t\n\t:param timestamp:\n\t:type timestamp: datetime\n\t:param user_id:\n\t:type user_id: str\n\t:param user_name:\n\t:type user_name: str\n\t:param timestamp:\n\t:type timestamp: datetime\n\t:return:\n\t\"\"\"\n\tdef __init__(self, timestamp, user_id=None, user_name=None, user_type='anyone'):\n\t\tself.timestamp = timestamp\n\t\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\n\treturn value.second + value.microsecond / 1e6\n\n\nclass Broadcaster(object):\n\t\"\"\"\n\tRepresents a single blackrock.\n\t\n\t:param source_id:\n\t:type source_id: int\n\t:param data:\n\t:type data: dict\n\t:return:\n\t\"\"\"\n\t\n\n\n\tdef __init__(self, source_id, data):\n\t\tassert isinstance(source_id, int),\\\n\t\t\t'Need single valid source_id in constructor arguments.'\n\t\tassert isinstance(data, dict),\\\n\t\t\t'Need data in constructor"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_date(value, format=None):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: Date\n\t:param format:\n\t:type format: string\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_time_format(value):\n\t\"\"\"\n\tDehydrator for `time_format` values.\n\t\n\t:param value:\n\t:type value: Time_Format\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_date_"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\ndef normalize(value):\n\t\"\"\"\n\tNormalize values before it's used.\n\t\n\t:param value:\n\t:type value: float\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_time_in_ns(value):\n\t\"\"\"\n\tDehydrator for `time` values in a string.\n\t\n\t:param value:\n\t:type value: str\n\t:return:\n\t\"\"\"\n\t\n\t\n# -*- coding: utf-8 -*-\r\n\r\nimport xml.dom.minidom\r\n\r\nclass Card: pass\r\n\r\nclass Node(Card):"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\t\n\tfor value_type in [float, int, long, complex]:\n\n\t\tif value_type(value):\n\t\t\treturn value \n\n\tprint\n\tprint 'Error!', u'Value:',\n\tprint'   with type:', value_type(),\n\tprint'   with value:', value.dehydrate(float)\n\tprint'   with value:', value.dehydrate(float)\n\tprint\n\traise\n\ndef to_float(*x):\n\t\"\"\"\n\tConvert the argument to a floating point number,\n\twhich can be used for formatting.\n\t\n"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\tif isinstance(value, tuple):\n\t\tx, y = value\n\telse:\n\t\tx, y = value[0], value[1]\n\t\n\tp = Point(x, y)\n\t\n\tif p.is_simple():\n\t\treturn \"simple\"\n\n\tp *= p.degree\n\t\n\treturn p\n\n\"\"\"\n\tReturns a set of points in a cube.\n\tIt is the inverse of unnormalized.\n\t\"\"\"\ndef get_coordinates():\n\treturn [\n\t\t(-10, 10),\n\t\t(-0.5, 5),\n\t\t(5."}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\ndef normalize_line(line):\n\t\"\"\"\n\tNormalize the line according to the DEM\n\t:param line:\n\t:type line: LineString\n\t:return:\n\t\"\"\"\n\t\ndef get_normalized(point, index):\n\t\"\"\"\n\tGet normalized point\n\t:param point:\n\t:type point: Point\n\t:param index: \n\t:type index: int\n\t:return:\n\t\"\"\"\n\t\ndef get_point(point, normal):\n\t\"\"\"\n\tGet point on vector\n\t:param point:\n\t:type point:"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\t\n\tpoint = Point(value.x, value.y, value.z)\n\n\treturn point\n\nfrom django.conf.urls import url\nfrom django.contrib import admin\n\nfrom models import *\n\n\nurlpatterns = [\n    url(r'^$', 'admin_views.home', name='home'),\n    url(r'^edit/(?P<pk>\\d+)/$', 'admin_views.edit', name='edit'),\n    url(r'^password_change/', 'django.contrib.auth.views.password_change', name='password_change'),\n    url(r'^password_reset"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\nclass AstroData:\n\n\tdef __init__(self, points, x_axis, radius):\n\t\t\"\"\"\n\t\t:param points:\n\t\t:type points: list\n\t\t:param x_axis:\n\t\t:type x_axis: Axis\n\t\t:param radius:\n\t\t:type radius: float\n\t\t:return:\n\t\t\"\"\"\n\t\n\n\tdef extractAxis(self, axis, data):\n\t\t\"\"\"\n\t\tReturns the specified axis from the given data.\n\t\t\n\t\t:param axis:\n\t\t:type axis: Axis\n\t\t:param data:\n\t\t:type data:"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\nclass Point(object):\n\tdef __init__(self, x, x_in):\n\t\tself.x = x\n\t\tself.x_in = x_in\n\nclass PointArray(object):\n\tdef __init__(self, elements):\n\t\tself.array_length = len(elements)\n\tdef __repr__(self):\n\t\treturn \"PointArray({})\".format(str(self.array_length))\n\tdef __len__(self):\n\t\treturn self.array_length\n\tdef insert(self, x):\n\t\t\"\"\"\n\t\tInsert the input point"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\t\n\tfrom pymel.core import *\n\tfrom pymel.utils import Matrix\n\tmatrix = Matrix(value)\n\t\n\tpointData = point( matrix )\n\t\n\tdef dehydrate(x, y, z):\n\t\t\"\"\"\n\t\tAdds/subtracts the point with the point at (x, y, z)\n\t\t\"\"\"\n\t\t# If the curve is a Ray curve at the origin, don't use it with the\n\t\t# point.\n\t\tif not isinstance(x, Triangle):\n\t\t\tx = Triangle(x)\n\t\tif not isinstance(y,"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_rect(value):\n\t\"\"\"\n\tDeeply dehydrate the shape from a rectangular array.\n\t\n\t:param value:\n\t:type value: Rectangular\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_hull(value):\n\t\"\"\"\n\tDehydrator for Hull data.\n\t\n\t:param value:\n\t:type value: Hull\n\t:return:\n\t\"\"\"\n\t\ndef dehydrate_curve(curve):\n\t\"\"\"\n\tDehydrates the curve from a point or"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\treturn Point(value.lat) + value.lng\n\ndef add_points(points, direction = \"s\"):\n\t\"Add points, one per direction.\"\n\tfor value in points:\n\t\tdehydrate_point(value)\n\t\n\treturn direction\n\n#!/usr/bin/python -tt\n#\n# Copyright 2013 Intel Corporation\n#\n# Author: Chris Hupa\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\ndef get_timeofday(value):\n\t\"\"\"\n\tGet the time of day, in hours.\n\t\n\t:type value: int\n\t:return: The value in the timezone of Hour.\n\t\"\"\"\n\t\ndef get_starttime_of_day(value):\n\t\"\"\"\n\tReturn the starttime of day, in minutes.\n\n\t:type value: int\n\t:return: The value in the timezone of Hour.\n\t\"\"\"\n\t\ndef get_endtime_of_day(value):\n\t\"\"\"\n\tReturn the endtime of day, in hours"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\nclass ClipBox(object):\n\t\"\"\"\n\tHolds a bounding box on which this class is to be used.\n\t\"\"\"\n\tdef __init__(self, x, y, width = None, height = None, **kwargs):\n\t\tself.shape = (int(math.floor(x)), int(math.ceil(x)), int((y+height)/2)) # this is the new \"X\" and \"Y\" coords\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.radius = size.radius # this is the radius (in pixels"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\nclass Record:\n\t\"\"\"\n\tA record object represents a database schema for a user.\n\t\n\t:param record_name: the name of the record\n\t:param table: the name of table that holds all of the records in that\n\t:param where: the query containing subfields containing all the records\n\t:param values: optional query containing all subfields in that field\n\t:param index_key: optional index to identify the record in the database.\n\t:param index_key_type: optional index of the key type in the records_table\n\t:param type: optional type"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t    \n    def list_values(self):\n\t\"\"\"\n\tReturn the list of key entries.\n\t\"\"\"\n\t    \n    def key_data(self, key):\n        \"\"\"\n        Constructs a dict representing the value of the given key.\n\t\t:param string key: key to be added.\n\t\t:return: entry\n\t\t\"\"\"\n        import datetime\n        if self.__dict__.has_key(key):\n            val = self.__dict__[key]\n            datetime.datetime.strptime(val, '%Y-%m-%d').tz = datetime.timezone.utc\n            return val\n        else:\n            return None\n    \n    def"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\t\n\treturn self._keys\n\t\n\tdef append(self, key):\n\t\t\"\"\"\n\t\tAdd a key to the record. If an exception occurs, an error is raised.\n\t\t:param key: key to add\n\t\t:type key: str\n\t\t\"\"\"\n\t\t\n\t\tif not self.keys_empty:\n\t\t\tself._keys.append(key)\n\t\n\t\n\tdef __iter__(self):\n\t\treturn iter(self._keys) + self._keys\n\t\n\tdef iterkeys(self):\n\t\treturn self._iterkeys\n\t\n\tdef keys_empty(self):\n\t\treturn self.keys()\n\t\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\treturn self.__dict__.keys()\n\t\n\tdef keysByKeys(self, keys, defaultUniqKey=False):\n\t\t\"\"\"\n\t\tReturn keys as a list of keys named with keys in keys,\n\t\tin-order. \n\t\t\n\t\t:param Keys names: list of names\n\t\t:param Default UniqKey: (optional) default if keys is not\n\t\t\t\t\t\t checked (default=False)\n\t\t:return: keys in keys, or False if there are no keys\n\t\t\"\"\"\n\t\tkeys = [key for key in keys if key in self.__dict__]\n\t\tkeysSorted = list("}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\tdef update(self, update_func, update_rec_name, delete_rec_name=False):\n\t\t\"\"\"\n\t\tCreate or update a record, or delete it.\n\t\t\n\t\t:param update_func: Function to be called with each (name, value) pair.\n\t\t:type update_func: callable or None\n\t\t:param update_rec_name: Record's name\n\t\t:type update_rec_name: str\n\t\t:param delete_rec_name: Record's name on be deleted\n\t\t:type delete_rec_name: bool\n\t\t:return:"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\treturn list(self.keys())\n\t\t\n\tdef get(self, key):\n\t\t\"\"\"\n\t\tGet a value.\n\t\t\n\t\t:param key:\n\t\t:return: value\n\t\t\"\"\"\n\t\t\n\t\treturn self.get_value(key).value()\n\t\t\n\tdef set(self, key, value):\n\t\t\"\"\"\n\t\tSet a value to a given key.\n\t\t\n\t\t:param key:\n\t\t:param value:\n\t\t:return: True/False; Set changed false to false.\n\t\t\"\"\"\n\t\tif self.contains(key):\n\t\t\tself.get_value(key).value"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\t\n\treturn set([self.entry.keyName for self.key in self.records])\n\n\t# TODO: Fix in the next section.\n\t# def values(self):\n# \tif not self.values:\n# \t\traise KeyError\n\n\n\nclass ProposalHistory(list):\n\tdef __init__(self, proposalID, entry, isModified = True):\n\t\t\n\t\tself._entry = Proposal(proposalID, entry)\n\t\tself._isModified = isModified\n\t\t\n\tdef keys(self):\n\t\treturn list(self)\n\n\t# TODO: Fix in the next"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n#def get_records(keyName):\n#\t\"\"\"\n\tReturn all the records associated with a key.\n\t\n\t:param keyName: name of key\n\t:return: record lists\n\t\"\"\"\n\t\n#def checkkey(recordName):\n#\t\"\"\"\n\tGiven a named record key, check to see if this record contained\n\tin the database.\n\t\"\"\"\n\n#\tGiven a key, return True, if it is at the time of creation.\n#\t\"\"\"\n\t\n#\tif keyName in self:\n\t\treturn True\n\t\t\n\t\n\tpass\n\n\nif"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\t\n\tpath = Path()\n\t\n\t# Parse the key file\n\twith open(path.args[1]) as file:\n\t\trecords = file.readlines()\n\t\n\t# Retrieve the all the keys\n\tkeys = []\n\t\n\t# Read each record\n\tfor record in records:\n\t\tall_key = {}\n\t\tall_key['key'] = {}\n\t\tall_key['key'][str(record[0])] = record[1:-1]\n\t\tall_key['key'][str(record[-1])] = str(record)\n\t\t\n\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\treturn self._keys\n\t\n\tdef __len__(self):\n\t\treturn len(self._keys)\n\t\n\tdef _keys_as_tuple(self):\n\treturn self._keys\n\ndef _is_key(key):\n\treturn isinstance(key, int) and key in self._keys\n\ndef is_key(key):\n\treturn _is_key(key)\n\ndef is_key_as_tuple(key):\n\treturn get_key_as_tuple(key) in self._keys#!/usr/bin/python\nfrom __future__ import absolute"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is not None:\n\t\tif isinstance(protocol_version, (tuple, list)):\n\t\t\tprotocol_version = tuple([protocol_version])\n\n\t\tversion = None\n\t\tif isinstance(protocol_version, (list, tuple)):\n\t\t\tversion = tuple([protocol_version])\n\n\t\tif protocol_version is None:\n\t\t\tcls.logger.error('Unexpected protocol version given. '\n\t\t\t\t'Possible values:\\n%s', str(PROTOCOL_VERSIONS.keys()))\n\t\t\tif protocol_version is None:\n\t\t\t\tcls.logger.error(\"No protocol version was specified"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tdef get_handler(self, version):\n\t\t\"\"\"\n\t\tRetrieves the instance of the specified version, raising an\n\t\tUnknownVersion exception if none is available.\n\t\t\n\t\t:param version: integer indicating the version to fetch. May be \n\t\t\ta numeric and can be anything other than a number, in which case, \n\t\t\tthis function will return None.\n\t\t\"\"\"\n\t\tif version in self.versions:\n\t\t\t# Already satisfied this version is required, so this handler is \n\t\t\t# available.\n\t\t\treturn self.versions.pop(version, None)\n\t\t\n\t\t# Unknown version?\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\n\tif protocol_version is None:\n\t\tprotocol_version = (0, )\n\t\n\tif not isinstance(sequence, dict):\n\t\traise TypeError('sequence must be a dictionary')\n\t\n\tports = protocol_version[1]\n\tprotocols = []\n\t\n\tfor protocol_version in range(ranges[0], ranges[-1] + 1):\n\t\tif protocols[protocol_version] <= 0:\n\t\t\tprotocols.append(protocol_version)\n\t\n\tprotocol_handler_pairs = [(port_name, dict(port_name=port_name)) \\\n\t\t\tfor port_name in"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\ttry:\n\t\t\tprotocol_version, default_version = version\n\t\texcept ValueError:\n\t\t\traise TypeError(\"No version was provided for automatic port \"\n\t\t\t\t        \"handling\", version_string)\n\t\n\tversion, handler = cls.get_version_and_handler_for_protocol(protocol_version)\n\t\n\tif not handler:\n\t\traise TypeError(\"Failed to obtain a handler for protocol \"\n\t\t\t\t\t\"version %s\" % self.protocol_version, version_string)\n\t\n\thandler_list = handler.handler_list\n\tif not handler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\treturn {\n\t\tk: cls(k, protocol_version)\n\t\tfor k, cls in cls.__dict__.items()\n\t}\n\n#TODO: This needs to be fixed after we implement any type checking.\ndef protocol_checker(test_id):\n\tassert type(test_id) is type\n\tassert isinstance(test_id, int)\n\tprotocol_checker = cls()\n\tprotocol_checker.test_id = test_id\n\treturn protocol_checker"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\t# TODO support more than 2.0 in future\n\thandler_handler_cls = dict(map(str, cls._thrift_handler_versions))\n\t\tif protocol_version is not None:\n\t\t\thandler_handler_cls.update({version[0]: protocol_version})\n\t\t\t\n\t\t# TODO support more than 2.1.2 support\n\t\tif protocol_handler_cls is None:\n\t\t\thandler_handler_cls = dict(map(str, cls._thrift_handler_names))\n\t\treturn handler_handler_cls\n\t\t\n\t# backwards compatibility\n\tdef _thrift_handler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = 3\n\telif protocol_version < 5:\n\t\traise TypeError, \"protocol_version must be passed in a tuple\"\n    \n\treturn {\n\t\t3 : BoltClient,\n\t\t4 : BoltProtocol,\n\t\t5 : BoltServer,\n\t\t6 : Protocol,\n\t\t7 : BoltProtocol,\n\t\t8 : None,\n\t\t9 : None,\t\t# 0xfff0f, 0xffffff\n\t\t10 : None,\t# 0xffffff, 0x8080\n\t\t11 : None,"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tversion = (0, 0)\n\telif isinstance(protocol_version, int): port_no = protocol_version\n\telse: raise TypeError(\"'protocol_version' must be an integer\")\n\t\n\ttry:\n\t\treturn dict((port_no,) for port_no in protocol_version)\n\texcept:\n\t\ttry:\n\t\t\treturn BoltProtocol.objects.get(version=protocol_version)\n\t\texcept BoltProtocol.DoesNotExist:\n\t\t\traise BoltProtocol.DoesNotExist\n\t\n# encoding: utf-8\n# module Py"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = DEFAULT_PROTOCOL_VERSION\n\t\n\tif protocol_version not in (\"3\", \"5\"):\n\t\traise TypeError(\"Unsupported protocol version\")\n\t\n\tresult = {}\n\t\n\tfor version in protocol_version:\n\t\tresult[version] = cls(version = version)[version]\n\t\n\tif protocol_version!= \"4\":\n\t\tresult.pop(\"supported_protocol_version\", None)\n\t\n\treturn result\n\n\ndef _protocol_version(protocol_version):\n\t\"\"\"\n\tProvided the protocol version of the protocol.\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tdefault_handler = dict([(tuple(sys.version_info[:3]),)] * 4)\n\t\tavailable_version_tuples = default_handler.items()\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = (3,0)\n\t\tif protocol_version in available_version_tuples:\n\t\t\thandler_class = cls[available_version_tuples.index(protocol_version)]\n\t\telif protocol_version in protocol_handler_map:\n\t\t\thandler_class = protocol_handler_map[protocol_version]\n\t\telse:\n\t\t\traise TypeError(\n\t\t\t\t"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    if not isinstance(metadata, dict):\n\t\traise ValueError(\"metadata parameter should be an attribute of the class `metadata` and the key is the name (either'metadata')\")\n\n\tdef wraps(function):\n\t    def inner(wrapped):\n\t        def wrapped_function(self, *args, **kwargs):\n\t        self.metadata = self._get_metadata(**kwargs)\n\t        wrapped_function(wrapped)\n\t        return wrapped\n\t\twrapped_function.func_name = \"wrap_{0}\".format(function.__name__)\n\n\t    return inner\n\treturn wraps\n#!/usr/bin/"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \tself._log(\"Transaction: Beginning transaction: %s\", metadata)\n\t    \ttry:\n\t\t## We allow a user to retrieve additional information, so I can get it.\n\t    \t        if metadata is None:   \n\t\t##            metadata = { 'user': None,'status': None }\n\t    \t\telse: # user supplied\n\t\t##            metadata = metadata.copy()\n\t\tif timeout:\n\t        \tself._log(\"Operation: %s seconds. Duration: %s seconds. Expired: %s\", (timeout, timeout * 1000, metadata))\n\t    "}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\tdef decorator(fun):\n\t\t\tif isinstance(metadata, dict):\n\t\t\t\t\tfor metadata_value in metadata.values():\n\t\t\t\t\t\tfun.__name__, fun.__doc__ = metadata_value\n\t\t\t\t\t\tfun = _create_decorator(fun)\n\t\t\tif fun.__name__ == \"Transaction\":\n\t\t\t\tfun.__name__, fun.__doc__ = fun.__doc__, fun.__doc__\n\t\t\treturn fun\n\t\treturn decorator(f)\n\n\t@return_function\n\tdef txn():\n\t\t\"\"\"\n\t\tThis function returns the current transaction.\n\t\t\n\t\t@rtype: DBms transaction object\n\t\t\n\t\t**Example"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t    \n\tif timeout is None :\n\t\ttimeout = float('inf')\n\t\n\tif not callable(metadata):\n\t\traise TypeError(\"metadata should be callable\")\n\t\n\tif isinstance(metadata, types.ListType):\n\t\tmeta = metadata\n\telif metadata is None:\n\t\tmeta = dict()\n\t\tmetadata = dict()\n\ttype(meta).append(\"metadata\")\n\t\n\tlog_query = sql.Query(metadata, sql.StrTypes([dbms.METADATA_SCHEMA]))\n\tlog_transactions = sql.Query(metadata, sql.StrTypes([dbms.METADATA_TRANSACTION"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t     \n\tif (metadata):\n\t    return _do_transaction(metadata)\n\t\n\tif timeout:\n\t    return _do_transaction(timeout)\n\n\tif progress.has_connection_state():\n\t    # return the DBMS state\n\t\t\n\t    if metadata and '_meta' in metadata:\n\t        transaction_result = dbms.update(metadata['_meta'],\n\t\t\t              transaction_state=dbms.TransactionStates.WAITING, \n\t\t\t              duration=0,\n\t\t\t              timeout=timeout)\n\n\t    else:\n\t    transaction_result = None\n\t\t\t\n        return transaction_result\n\t\n\treturn _do_transaction(None)\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\tif metadata is None:\n\t\t\tmetadata = self._metadata\n\t\telse:\n\t\t\tmetadata = metadata.copy()\n\t\tmetadata = { k:v for k, v in metadata.items() if not k.startswith(\"_\") }\n\n\t\tdef txn(txn_id, transaction_time, owner=None, name=None, description=None, priority=None, _owner=None, _ownerInfo=None):\n\t\t\t\"\"\"\n\t\t\t\tCreate a transaction\n\n                :param txn_id: the transaction identifier\n\t\t\t\t:param transaction_time: the transaction time\n\t\t\t\t:param owner: a `ne"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\tif metadata is None: metadata = {}\n\t\tlogger.debug(\"query.log() - Checking for metadata...\")\n\t\t\n\t\ttry:\n\t\t\tif timeout is not None: raise NotImplementedError\n\t\texcept NotImplementedError:\n\t\t\tlogger.debug(\"query.log() - No timeout specified yet\")\n\t\t\treturn\n\t\texcept:\n\t\t\treturn None\n\n\t\tquery = \"\"\"\n                SELECT COUNT(*) AS count,\n\t\t\t\tSELECT DISTINCT person_from \n\t\t    \t\tFROM \n\t\t\t\t(SELECT DISTINCT name FROM Person) \n               \tWHERE person_from like :name\n\t\t\t\t     \tAND person_from IS NOT NULL)\n\t\t    OR \n\t\t\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\tself._logger.info(\"Query %s.execute() %s. Unit of work requested to execute in %s seconds\", metadata.get('query'), timeout, self.get_execution_time(metadata))\n\t\treturn self.store_input({'query': metadata.get('query')}, timeout)\n\t\n\tdef get_session_transaction_log(self):\n\t\t\"\"\"\n\t\tThis function returns log file's name (including query), log file's file name and the log's timestamp.\n\t\tThis function must be overloaded.\n\t\t\"\"\"\n\t\treturn self._session.log.name + self"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\tif metadata is None: metadata = {}\n\t\telif isinstance(metadata, dict): metadata = metadata.items()\n\t\tmetadata.sort()\n\t\tquery = \"\"\"\n\t  SELECT %(columns)s\n\t\t  FROM %(tables)s\n\t     WHERE \n\t\t      %(metadata)s\n\t\t      \"\"\" % {'columns': \" \".join(metadata), 'tables': metadata,'metadata': metadata}\n\t\treturn execute_and_wait(query, timeout)\n\n\tdef transaction_progress(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapped(self, *args, **kwargs):\n\t\t\ttry:"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\n\tdef wrapper(func):\n\t\tquery = func.__table__.find()\n\t\tresult = query.one_or_none(\"SELECT \" + func.__tablename__ + \" * \" + query.fields + \" FROM \" + func.__tablename__ + \" where \" + func.__tablename__ + \" > 0\", \n\t\t\t{\n\t\t\t\t\"table\": func.__tablename__ + \"\",\n\t\t\t\t\"fields\" : func.__tablename__ + \" \",\n\t\t\t\t\"limit\": min(1, func.__table__.c.id_field),\n\t\t\t\t\"orderby\" : \"date DESC\",\n\t\t\t\t\""}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\timport json\n\t\ti = 0\n\t\tfor j in range(len(self._items)):\n\t\t\tif self._items[j].key == key:\n\t\t\t\ti = i + 1\n\t\treturn i\n\tdef __setitem__(self, item, value):\n\t\t\"\"\"\n\t\tReplace the key with the specified value.\n\t\t\n\t\t:param item: item key\n\t\t:param value: the value to place \n\t\t\"\"\"\n\t\tself._item = item\n\t\tself._values = value\n\t\tself.save()\n\t\treturn self\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tReturn"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\treturn int(key, 16)\n\t\n\t\t\n\tdef set(self, key, value):\n\t\"\"\"\n\tSet the key-value pair of the given variable.\n\t\n\t:param key: a key\n\t:param value: a variable value\n\t:return: None\n\t:rtype: None\n\t\"\"\"\n\treturn 0\n\t\t\n# Copyright 2014 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport logging\nimport os\n\nfrom telemetry import decorators\nfrom telemetry import"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\nclass Index:\n\t\n\tdef __init__(self):\n\t\t\n\t\tfrom pyperclip.perclip import Perclip\n\t\tself.__root_object = None\n\t\tself.__current_frame = None\n\t\tself.__parent_object = None\n\t\tself.__parent = None\n\t\tself.__parent_frame = None\n\t\tself.__children = None\n\t\tself.__items = list()\n\t\t\n\tdef append(self, index):\n\t\t\"\"\"\n\t\tAdd index to an item's children list.\n\t\t\n\t\t:param index: index\n\t\t:type index: int\n\t\t\"\"\"\n\t\n\t\n\tdef remove("}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\n#\n# Functions\n#\n\n\tdef getAllItems(self, key, default=[], order_item=0):\n\t\t\"\"\"\n\t\tReturn all items for the item specified in the given key.\n\t\t\n\t\t:param key: the key\n\t\t:param default: default values to return\n\t\t:param order_item: a number, defaults to 0\n\t\t:return: all\n\t\t:rtype: dict\n\t\t\"\"\"\n\t\t\n\t\tresult = {}\n\t\tfor obj in self.getDatasetsByKey(key).getAll():\n\t\t\t\t\t\n\t\t\tif order_item >= obj[0][0]:\n\t\t\t\t"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tkey = re.sub('[^a-zA-Z]', '', key)\n\t\treturn self.index_db_list.index(key)\n\t\t\n\tdef index_db_item(self, item_list, key):\n\t\t\"\"\"\n\t\tGet the index of a single item.\n\t\t\n\t\t:param item_list: list\n\t\t:param key: a key\n\t\t:return: index\n\t\t:rtype: int\n\t\t\"\"\"\n\t\tif type(item_list) is list:\n\t\t\treturn self.index(item_list[0])\n\t\treturn self.list_db."}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\ndef get(self, key):\n\t\"\"\"\n\tReturn the value of the given item.\n\t\n\t:param key: a key\n\t:return: value\n\t:rtype: object\n\t\"\"\"\n\t\ndef set(self, key, value):\n\t\"\"\"\n\tSet the value of the given item.\n\t\n\t:param key: a key\n\t:param value: value\n\t:return: new value\n\t:rtype: dict\n\t\"\"\"\n\treturn {'type':'set', 'key':key, 'value':value}\n\ndef append(self, item):\n\t"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tvalue = self.__dict.get(key)\n\t\ttry:\n\t\t\tvalue = int(value)\n\t\texcept Exception as e:\n\t\t\traise FDASError(f\"Attempt to get an index of item '{key}' : {e}\")\n\t\treturn value\n\t\n\t'''\n\tReturns the closest item in the index, or -1 if given as a value\n\t\n\t:return: index or -1 if it does not exist\n\t:raises FDASError: if unable to get an index\n\t:rtype: int\n\t'''\n\t\tif None in (self.__dict, self"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\ndef move(key, value):\n\t\"\"\"\n\tPlace `value` in the stack.\n\t\n\t:param key: key to move\n\t:param value: value to move\n\t:return: position at the new location\n\t:rtype: int\n\t\"\"\"\n\t\ndef remove(key, value):\n\t\"\"\"\n\tRemove value from a stack.\n\t\n\t:param key: key to remove\n\t:param value: value to remove\n\t:return: position at the position\n\t\t:rtype: int\n\t\"\"\"\n\t\ndef pop(key, default = \"\"):\n\t\"\"\"\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\tif not self._data.has_key(key):\n\t\t\treturn -1\n\t\telif not self._data[key]:\n\t\t\treturn 1\n\t\treturn self._data[key][0]\n\t\t\nclass FileCache(object):\n\t\n\t\n\tdef __init__(self, filedir = '~/.cache', cachefilename = None):\n\t\t# Initialize the list of files in the filedir\n\t\tself._files = []\n\t\tself._cachefilename = cachefilename\n\t\n\tdef __getitem__(self, item):\n\t\t# Return the file/directory and cachefilename\n\t\tif not isinstance(item"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\treturn self[key] if self.is_empty() else len(self)\n\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tReturn the item at the given index.\n\t\n\t:param key: a key\n\t:return: key\n\t:rtype: int\n\t\"\"\"\n\t\treturn self[key] if self.is_empty() else key\n\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSet item at the given index.\n\t\n\t:param key: a key\n\t:param value: value to set\n\t:type value: tuple"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\treturn self._get(keys)\n\n\ndef to_list(dictionary, keys=(), format=1, sep=','):\n\t\"\"\"\n\tConvert records to a comma-separated format list\n\t\n\t:param dictionary: the record dictionary.\n\t:param keys: list of keys\n\t:param format: index value to include\n\t:param sep: index value to use\n\t\"\"\"\n\t\tif not keys or keys[0] == '{':\n\t\t\treturn keys\n\t\tif isinstance(keys[0], list):\n\t\t\treturn comma.join([to_list(dictionary.get(k, str"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tresult = []\n\t\tif isinstance(keys, (list, tuple)):\n\t\t\tsorted(keys)\n\t\tfor item in keys:\n\t\t\tif item is None: continue\n\t\t\tresult.append(item)\n\t\treturn result\n\nclass _Key(object):\n\tdef __init__(self,keyName,fields):\n\t\tself.keyName = keyName\n\t\tself.fields = fields\n\nclass _RawRecordField(object):\n\tdef __init__(self,parent,parentName='rec',fieldTypes=[],\n\t\t\tfields={}):\n\t\tself.parent = parent\n\t\tself"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\treturn self.__items(self.keys(*keys))\n\t\n\tdef keys(self, *args):\n\t\tret = []\n\t\tif len(args) > 1:\n\t\t\tfor index, value in enumerate(args[1:], start=1):\n\t\t\t\tif value and isinstance(value, dict):\n\t\t\t\t\tfor key, value in iteritems(value):\n\t\t\t\t\t\tret.append(key)\n\t\t\t\t\t\tbreak\n\t\t\t\telif value:\n\t\t\t\t\tret.append(key)\n\t\tif len(args) == 1:\n\t\t\treturn ret\n\t\treturn ret\n\t\t\n\tdef keysNone(self, index):"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif keys:\n\t\t\treturn list(self._values_for_keys(keys))\n\t\telse:\n\t\t\treturn self._values()\n\n\tdef _values(self):\n\t\treturn list(self._fields)\n\n\tdef _fields(self):\n\t\treturn [dict(value=[None] * len(key)) for key in self._fields]\n\n\tdef _from_db_object(self, db_object):\n\t\tfor key, value in db_object._db_data.iteritems():\n\t\t\tkey.value = value\n\t\t\n\tdef get_value(self, index,"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif not hasattr(self, \"_values\"):\n\t\t\tself._values = list()\n\t\tvalues = list(self.to_list(keys))\n\t\tfor i, item in enumerate(values):\n\t\t\tif item is None: continue\n\t\t\tif isinstance(item, dict) and i in self._data.keys():\n\t\t\t\titem = [item[k] for k in self._data.keys() if item.has_key(k)]\n\t\t\tif i > 0:\n\t\t\t\tif item not in values:\n\t\t\t\t\tvalue = self._data[item]\n\t\t\t\t\tkeys = self.keys"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif isinstance(keys, basestring):\n\t\t\tkeys = [keys]\n\t\treturn self._values(keys, self._find_matching_keys(keys))\n\t\n\t@classmethod\n\tdef _values(self, keys, values):\n\t\tif not hasattr(values, 'keys'):\n\t\t\tvalues.keys = self._keys_for(keys)\n\t\tif not list(values.values()):\n\t\t\treturn list(values)\n\t\tvalues.keys.extend(keys)\n\t\treturn list(values.values())\n\t\t\n\n# =======================================================================================\n# --- Record\n# =======================================================================================\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\t\n\tif isinstance(keys, BaseSet):\n\t\tkeys = list(keys)\n\t\tkeys = keys.keys()\n\t\tkeys.sort()\n\t\tkeys = keys[:20] + [\"values\", \"idx\", \"key\"] + keys[20:]\n\telif isinstance(keys, tuple):\n\t\tkeys = list(keys)\n\t\tkeys = keys[10:]:  # list\n\t\tkeys.sort()\n\t\tkeys = keys[:20] + [\"values\", \"idx\", \"key\"] + keys[20:]\n\n\treturn self._values(keys=keys)\n\nclass BaseModel"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif keys:\n\t\t\treturn self.engine.sql(\"\"\"\n\t\t\tSELECT { \n\t\t\t\tkey, \n\t\t\t\tvalue \n\t\t\t}\n\t\t\tFROM \n\t\t\t\t{table}.{where}{columns}\n\t\t\tWHERE \n\t\t\t\t{fields}\n\t\t\tORDER BY \n\t\t\t\t{order_by_index}\n\t\t\tLIMIT 1\n\t\t\t\"\"\".format(\n\t\t\t\ttable=self.table_name,\n\t\t\t\twhere=self.format_query_string(keys).join('(%s)' %(c) for c in self.where),\n\t\t\t\tcolumns=self.column_names,\n\t\t\t\torder_by_index=self."}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\tif self.index:\n\t\t\treturn self.index.get(keys, self)\n\t\telse:\n\t\t\treturn self.keys(self)\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tIndex entry in the record, optionally filtering results\n\t\twhich are returned in sequence.\n\t\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self.index.get(key, self)\n\t\telse:\n\t\t\treturn self.keys(self)\n\t\n\tdef values_get(self, *keys):\n\t\t\"\"\"\n\t\tReturn the result of the search, optionally filtering results\n\t"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\tret = []\n\t\n\t\n\tdef _filter(items, include, values=[]):\n\t\t\"\"\"\n\t\tReturns whether or not include certain values according to the\n\t\tkeys given in `keys`. If `include` is None, ignore them.\n\n\t\t:param items: list of field names as strings\n\t\t:param include: Boolean, True/False to filter only items in given keys\n\t\t:param values: List of values to be included\n\t\t:return: List of values\n\t\t:rtype: list\n\t\t\"\"\"\n\t\tfor item in items:\n\t\t\tif isinstance(item, basestring):\n\t\t\t\t\t"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\t\t\n\tif self.item_key:\n\t\t# add key\n\t\tout = dict(self.item_key)\n\t\tif isinstance(out, dict):\n\t\t\tout = out.keys()\n\t\telse:\n\t\t\tout = [out]\n\t\tout.sort()\n\t\tkey = self.item_key.get(key) or self.default_item_key\n\t\tout.sort()\n\t\tif key not in out:\n\t\t\tkey_str = str(self.value())\n\t\telse:\n\t\t\tkey_str = '[%s]' % key\n\t\ttry:\n\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tif not keys and len(self) == 0:\n\t\t\treturn\n\t\t# Check the indexes and keys and add it to the dictionary\n\t\tfor entry in self:\n\t\t\ttry:\n\t\t\t\tk, v = entry\n\t\t\t\tk = self._convert_key(k, v)\n\t\t\texcept UnicodeError:\n\t\t\t\tk = None\n\t\t\t\tv = None\n\t\t\tif k is not None and k not in key:\n\t\t\t\tif isinstance(v, (list, tuple)):\n\t\t\t\t\tv = [v]\n\t\t\t\traise InvalidData(_(\"Invalid data value for {key} field: {v}\"\n\t\t\t\t\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\ttry:\n\t\t\tkey_map = dict(enumerate(keys))\n\t\texcept TypeError:\n\t\t\tpass\n\t\telse:\n\t\t\tif key_map:\n\t\t\t\tfor key in keys:\n\t\t\t\t\tif key in key_map:\n\t\t\t\t\t\tvalue = key_map[key]\n\t\t\t\t\t\tyield value\n\t\t\t\treturn\n\ndef items(self, *args, **kwargs):\n\t\"\"\"\n\tProvide the methods :meth:`~flask.current_app.make_response` to\n\tmanage lists of available :class:`~flask.current_app.<appname>.\n\t`args` and :class:`~"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tif self.items().__len__() < len(keys):\n\t\t\traise IndexError, \"Record keys cannot be empty or more\"\n\t\tkey = self.__class__.__getitem__(keys[0])\n\t\treturn dict(self.__table['data'][key])\n\t\n\tdef getfield(self, index_or_key):\n\t\t\"\"\"\n\t\t\n\t\t:param str index_or_key: Indices of the field, specifying the\n\t\t\tcurrent index or keys\n\t\t:return: the field or None\n\t\t:param int id: Index or key to be looked up\n\t\t:returns: field name or None"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tself._cursor.execute('''SELECT * FROM records\n\t\t\t        WHERE key in (%s)''', keys)\n\t\tout = {}\n\t\tfor row in self._cursor.fetchall():\n\t\t\tout[row] = row[key] if row[key] is not None else None\n\t\treturn out\n\t\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturn the keys of this record.\n\t\t\n\t\t:return: the keys of this record\n\t\t:rtype: list of strings (name, value)\n\t\t\"\"\"\n\t\treturn list(self[:])\n\t\n\t@property\n\tdef first_value(self"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tif self._keys is not None:\n\t\t\tidx = set(keys)\n\t\t\tfor key in idx - self._keys:\n\t\t\t\tself._keys[key] = idx[key]\n\t\n\tdef get(self, fieldname):\n\t\tif fieldname in self:  \n\t\t\treturn self[fieldname]\n\t\telse:\n\t\t\traise KeyError, fieldname\n\t\n\tdef delete(self, fieldname):\n\t\tif fieldname in self:  \n\t\t\tself.pop(fieldname)\n            if fieldname not in self:\n\t\t\t\tself.insert(fieldname)\n\t\t\n\t\n\tdef dict(self, *args): return self"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tkeys = keys or set()\n\t\tfor subvalue in self:\n\t\t\tfor key in keys:\n\t\t\t\tvalue = self._get_value(key)\n\t\t\t\tif value is not None:\n\t\t\t\t\tif value is None:\n\t\t\t\t\t\tvalue = \"\"\n\t\t\t\t\telif vtype(value)!= vtype(None):\n\t\t\t\t\t\traise IndexError, \"invalid value for index %i for\" %(key)\n\t\t\t\tsubvalue.data(child=key)\n\t\treturn None\n\tdef _get_value(self, key):\n\t\ttry:\n\t\t\tresult = self._db[key]\n\texcept KeyError:"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\n\t# check key or only one index is given\n\tif not keys:\n\t\treturn self._get_default_value()\n\t\n\t# check fields already provided\n\tif hasattr(self, '__fields__'):\n\t\tall = []\n\t\tfor key in keys:\n\t\t\tif len(self.__fields__[key]) > 0:\n\t\t\t\tall.append(self.__fields__[key])\n\t\t\t\n\t\tif len(all) == 1:\n\t\t\treturn {}\n\t\telse:\n\t\t\traise IndexError(f\"Unknown key {keys}\")\n\t\n\t# read values\n\tkeys = tuple(key for key"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\titems = list(keys)\n\t\tfor k in items:\n\t\t\tif (k[:1] == key and \n\t\t\t\tkeys[k-1][:1] == key and \n\t\t\t\tk[0] == k[1] and \n\t\t\t\tk[1]!= key):\n\t\t\t\ttry:\n\t\t\t\t\ti = self[k]\n\t\t\t\t except IndexError:\n\t\t\t\t\ti = \"\"\n\t\t\t\t# If key was not found:\n\t\t\t\tif i:\n\t\t\t\t\traise IndexError\n\t\t\t\titems[k] = i\n\t\treturn dict(items)\n\t\n\t@classmethod\n\tdef from_dict(cls, d"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\tkey = keys[0]\n\t\tif not isinstance(key, int):\n\t\t\tkey = [key]\n\t\treturn dict(self.__dict__, **{key[index]: value for index, value in enumerate(self[key])})\n\n# --------------------------------------------------------------------\n#\n# This function is used to generate a unique ID for a record.\n#\n# @return: the unique ID\n# @rtype: String\n# @remarks: This function can raise \"KeyError\" exception\nclass RecordRecord:\n\tdef __init__(self):\n\t\tself.version = 0\n\t\tself.id ="}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\ndef pop_u8(self):\n\t\"\"\"\n\tRemove the first 16 byte of data, returning them as a big-endian\n\t8-bit unsigned int.\n\t\"\"\"\n\t\ndef _write_array_file(file, addr, u24_offset, b_offset):\n\t\"\"\"\n\tSimulate a write file that writes the supplied number of bytes to the array\n\tfile.\n\t\n\t@param:\n\t\tfile: file to use for writing\n\t\taddr: array address of write\n\t@param:\n\t\tb_offset: offset into the array, in bytes\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn b'\\x00\\x00\\x00\\x00'\n\n\n@bacpypes_debugging\nclass Get(Base):\n\n\tDEBUG = False\n\n\t@classmethod\n\tdef create(cls):\n\t  return get_u16()\n\n\t\nclass Post(Base):\n\n\t__slots__ = ['size']\n\n\t__bytes__ = [\n\t\t'size', ]\n\t\n\tdef __init__(self, size):\n\t  if size < 2**(8-1):\n\t\traise NameError('Post size must be >2^8 bits long.')\n\n\t\tself.size = size\n\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn ord( self.read(1)[-2] )\t\t# Return big-endian byte array\n\n# ----------------------------------------------------------------------\n# Main function that we try and use below.\n\ndef main():\n\t\"\"\"\n\tThe main routine that is going to try to decode the entire\n\toutput file, this is not a very lightweight method.\n\n\tThis method is very generic. In particular, this method tries to\n\tre-use the \"U16-endian\" decoding step to decode certain things up into\n\tunicode strings.\n\t\"\"\"\n\t\ttry:\t\t\t# Try to decode the raw"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tdata = self.data()\n\t\tlength = len(data) - 1\n\t\tresult = b''\n\t\twhile data:\n\t\t\tresult += (ord(data[-1]) << 8)\n\t\t\tdata = data[1:]\n\n\t\t# the result format is:\n\t\t# 0x020000 = <c7f9234e70e7e40d8213a0d8d9c92f8>,\n\t\t# 0x020001 = <16dbb32f8a4da55a6f6d6f34b8c1356d5>,\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\nclass Deserializer:\n\t\"\"\"\n\tDecrypts a sequence into a buffer, deduping the fields and value\n\twith each pair individually.\n\t\"\"\"\n\tdef __init__(self, buffer):\n\t\tself.buffer = buffer\n\t\tself.current = 0\n\n\tdef write(self, value):\n\t\tself.current += 1\n\t\treturn self.current\n\n\tdef read(self, number):\n\t\tself.current -= number\n\t\treturn self.current\n\n\tdef getValue(self, *args):\n\t\ttmp = \"\"\n\t\twhile self.current <"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tdata = struct.pack(\"&i\", 0)[2::-1]\t\n\t\t\n\nclass RTPMessage(object):\n\t\"\"\"\n\tA RTP Message instance represents one encoded message.\n\t\"\"\"\n\n\tdef __init__(self, message=None):\n\t\t\"\"\"\n\t\t@parameters: A RTP message\n\t\t\"\"\"\n\t\tself._message = message\n\n\tdef __str__(self):\n\t\t\"\"\"Return the string representation of the RTP message.\"\"\"\n\t\treturn self._message\n\n\nclass RTPMessageEncoder(object):\n\t\"\"\"\n\tA RTP MessageEncoder instance encodes itself"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\treturn struct.unpack(\"<I\", self.s.read(2))[0]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\t\treturn struct.unpack(\"<I\", self.s.read(2))[0]\n\t\n\tdef pop_ushort(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned short unsigned short integer.\n\t\"\"\"\n\t\treturn (self.s"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tdata, = self._buffer.pop(0)\n\t\treturn data,\n\n\tdef pop_u8(self):\n\t\"\"\"\n\tRemove the last 4 bytes of data, returning them as a big-endian\n\t8-bit unsigned integer.\n\t\"\"\"\n\t\tdata, = self._buffer.pop(0)\n\t\treturn data,\n\n\nif __name__ == '__main__':\n\tif len(sys.argv) == 0:\n\t\tprint 'Error: I have no input. We have to provide either a file name, or a list of file names.'\n\telif"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\nclass GaugeType(DataType):\n\t\"\"\"\n\tThis is the actual Gauge type, which can be used to determine how\n\tthe gauge is represented using struct.unpack.\n\t\"\"\"\n\t__slots__ = ('_num_of_data', '_guc_data')\n\t\n\tdef __init__(self):\n\t\t\"\"\"Initializes the metric type\"\"\"\n\t\tself._num_of_data = 0\n\t\tself._guc_data = None\n\t\t\n\tdef __str__(self):\n\t\t\"\"\"Return a formatted string representation of this metric\"\"\"\n\t\treturn \"<GaugeType size=%"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t\tu16_list = ''.join(\n\t\t\tchr((\n\t\t\tr&0xffff\t# 0x30000000\n\t\t\t)+((\n\t\t\tr>>16 \t\t# 0x20000000\n\t\t\t)-1         # 0x40000000\n\t\t\t+0x3000000 \t\t# 0x20000000\n\t\t\t+0x40000000\n\t\t\t+0x400        # 0x40000000\n\t\t\t+0x20000000\n\t\t\t+0x40000000\n\t\t\t+0x40000000\n\t\t\t+0x1000        # 0x100000000\n\t\t\t+0x1000        # 0x10000"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif dehydration_hooks:\n\t\t\tqid = self._decode_id(qid, del_handler=dehydration_hooks)\n\t\telif id is not None:\n\t\t\tqid = self._decode_id(qid, id_handler=self.handler)\n\t\telse:\n\t\t\traise ValueError(\"Specify dehydration hooks\")\n\t\t\n\t\tif qid < 0:\n\t\t\tdel qid\n\t\t\tbreak\n\t\t\n\t\tlog.msg('Checking for record %d', id)\n\t\t\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\trecord = self.handler(q"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif dehydration_hooks is None:\n\t\t\tdehydration_hooks = {}\n\t\tfor event in self.events.values():\n\t\t\tevent.discrimination_hooks = dehydration_hooks.get(event.type, ())\n\t\t\tevent.handlers.extend(dehydration_hooks.get(event.type, []))\n\t\tfor method in dehydration_hooks:\n\t\t\tdel dehydration_hooks[method]\n\t\tfor method_in_decoders:\n\t\t\tif (method_in_decoders!= None and\n\t\t\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif qid==-1:#keep record for record\n\t\t\treturn\n\t\tif dehydration_hooks==None:dehydration_hooks=[{}]\n\t\tfor hooks in dehydration_hooks:\n\t\t\thooks['type']={}\n\t\t\thooks['value']={}\n\t\t\tfor hooktype in hooks['type']:\n\t\t\t\tclasshook = handlers['type'][hooktype]\n\t\t\t\thooks['type'][hooktype]['value'][hooktype]=classhook\n\t\t\tfor hooktype in dehydration_hooks[qid]:\n\t\t\t\thooks['type'][qid][hooktype]['"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t        if dehydration_hooks is None:\n\t\t        dehydration_hooks = {}\n\t     \n\tself.msg(\"discard\")\n\tself.msg(\"discard %d records starting with %0.3f milliseconds... \" % (n, time() - t))\n\t\n\tfor fn, (typ, dehydration_func, handlers) in handlers.items():   \n\t    self.msg(\"%16s.%-15s: received %s\" % (fn, typ.__name__, dehydration_func.__name__))\n\t\t\n\t    try:\n\t        # add the handler\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tassert len(nd)>=1\n\t\tassert len(qid)>=1\n\t\tself._record.append(cls_packstream_message(nd))\n\t\t\n\t\tif isinstance(dehydration_hooks, dict):\n\t\t\tdehydration_hooks = filter(lambda k, v: k in dehydration_hooks.keys(), kded)\n\t\tself._dehydrate_hook_map = {}\n\t\tif dehydration_hooks:\n\t\t\tself._dehydrate_hook_map = filter(lambda k, v: k in dehyd"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tif handlers is not None:\n\t\t\traise ValueError('handlers must be a sequence')\n\t\tif n is not None:\n\t\t\tif not 0 <= n <= 255:\n\t\t\t\traise ValueError(\"n must be between 1 and 255\")\n\t\tif n > 0:\n\t\t\tif len(handlers) == 0:\n\t\t\t\traise ValueError(\"No handlers were specified\")\n\t\t\tdel(handlers[0])\n\t\n\tdef __getitem__(self, key):\n\t\tif key in self.queues:\n\t\t\tqueue = self.queues[key]\n\t\t\tdel(queue[QUEUE_NAME])\n\t\telse:\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\ndef set_logger_handler(handler):\n\t\"\"\"\n\tSafely sets log handler to a function of type\n\t:param handler: a callable object, to which the current\n\t:type handler: callable\n\t:rtype: callable\n\t:return: the function returned when the response is sent to the handler\n\t\"\"\"\n\t\n\t\n\tdef handler_log(request):\n\t\tglobal logger\n\t\ttry:\n\t\t\tlogging.getLogger(__name__).setLevel(logging.INFO)\n\t\texcept:\n\t\t\tpass\n\t\t\n\t\tclass LogHandler(object):\n\t\t\t\"\"\"\n\t\t\tA logging handler that"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t    \n\n\t\tif dehydration_hooks is None:\n\t\tdef dehydrate(msg):\n\t\t\treturn  'undehydrate %d' % msg\n\n\t\tdef unhydrate(msg):\n\t\t\treturn 'undehydrate %s' % msg\n\n\t\tdef handle_record(result, record):\n\t\t\tself._send(\n\t\t\t\tmsg = result.msg, \n\t\t\t\tpriority = result.priority, \n\t\t\t\ttags = result.tags, \n\t\t\t\ttag_count = result.tag_count, \n\t\t\t\ttimestamp = record.timestamp, \n\t\t\t\tvalue = result"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t     \n\t        self._queue.put(\"DISCARD\", \"%d,%s\" %(n, qid))\n\t        if n < 0:\n\t\t  self._queue.put(\"DISCARD\", \"There are no documents to discard\")\n\t            self._output('No documents to discard'); return None\n\n\t    elif len(dehydration_hooks) > 0:\n\t        self._queue.put(\"DISCARD\", dehydration_hooks)\n\t        self._queue.put(\"DISCARD\", \"This is a sequence of dehydration functions\")\n\t        self._output('Press"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t\tfrom collections import OrderedDict\n\t\tdecoder = None\n\t\tfor type, func in dehydration_hooks.items():\n\t\t\tif isinstance(func, self.handlers):\n\t\t\t\th = func(n = n, qid = qid, handlers = handlers)\n\t\t\t\tdecoder = func\n\t\t\telse:\n\t\t\t\t\t decoder = getattr(self,func)\n\t\treturn decoder\n\n\tdef peek(self, n = -1, qid=-1, dehydration_hooks=None, handlers=None):\n\t\t\"\"\"\n\t\tRetrieves or consumes n bytes from the queue or \n\t\treturns them"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tassert type(db) is str\n\t\tassert type(imp_user) is str\n\t\tassert type(metadata) is dict or {}\n\t\tassert type(timeout) is int\n\t\tassert timeout > 0\n\t\tself.db = database\n\t\tself.imp_user = imp_user\n\t\tself.dehydration_hooks = dehydration_hooks\n\t\tself.dehydration_hooks[\"begin\"] = self._begin\n\n\t\tself.hooks = []\n\t\tcallbacks = {}\n\t\thooks = {}\n\t\thooks[\"begin\"] = self._begin\n\t\thooks[\""}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\t# Default mode.\n\t\tif mode in (\"READ\", \"WRITE\", \"READ_BLOCK\") or mode == \"READ_BLOCK\":\n\t\t\tmode = \"READ\"\n\t\telif mode in (\"READ_BUFFER\", \"READ_MAP\" , \"READ_DICT\",\n\t\t       \"READ_LIST\",  \"READ_SET\",  \"READY\") or mode == \"READY\":\n\t\t\tmode = \"READ_BLOCK\"\n\t\t# Default timeout\n\t\tif timeout:\n\t\t\ttime.sleep(timeout)\n\t\t# Set the DB.\n\t\tif not bookmarks:\n\t\t\tbookmark_values = []"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\treturn self.write_message(\n\t\tmode=mode,\n\t\tbookmarks=bookmarks,\n\t\tmetadata=metadata,\n\t\ttimeout=timeout,\n\t\tdb=db,\n\t\timp_user=imp_user,\n\t\tdecode_responses=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\thandlers=handlers,\n\t\thydration_hooks={\n\t\t\tk:v for (k,v) in\n\t\t\tself.get_handler_map().items()\n\t\t},\n\t\ttimeout=timeout\n\t\t)\n\n\n\tdef close"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tif mode is None:\n\t\t\tmode = self.default_mode()\n\t\t\n\t\tif bookmarks is None:\n\t\t\tbookmarks = self.get_bookmarks()\n\t\t\n\t\tif metadata is None:\n\t\t\tmetadata = self.metadata()\n\t\t\n\t\tif imp_user!= 'beaker':\n\t\t\t# Use the standard UserInterface by default\n\t\t\tif type == 'dict':\n\t\t\t\treturn self.insert_txn(self.bind(imp_user,bookmarks),meta,timeout=timeout)\n\t\t\telif type == 'dict':\n\t\t\t\treturn self.insert_txn(self.bind(imp_user,bookmarks"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tif self.hook in self.hydration_hooks:\n\t\t\tlogging.debug(\"Already handling %s hook\"%(self.hook))\n\t\t\treturn self.dispatch(self.hydration_hooks[self.hook])\n\t\telse:\n\t\t\traise bolt._exceptions.UnsupportedOperation(\"Only %s \"\n\t\t\t\t\"hooks implemented for %s\" % (self.hook, self.hook_name))\n\n\tdef dispose_transaction(self, transaction_name, metadata=None, hooks=None,\n\ttimeout=None, db=None, imp_user=None, de"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\tassert mode in (\"READ\", \"WRITE\")\n\t\tassert bookmarks and len(bookmarks) == len(self.blocks.values())\n\t\tassert metadata and'metadata' in metadata\n\t\tassert 'transaction_namespace' in metadata\n\t\tassert ('transaction_key' in metadata)\n\t\tassert 'transaction_name' in metadata\n\t\tassert 'transaction_id' in metadata\n\t\t# Check for dehydration hooks\n\t\tfor hook in dehydration_hooks:\n\t\t\tassert callable(hook)\n\t\t# Iterate over the bookmark values\n\t\tfor event, events in self.blocks.items"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\ttry:\n\t\t\tconn = self.get_or_create_conn(mode, bookmarks, metadata, timeout)\n\t\t\tyield self.handle(\"begin\", conn, imp_user, dehydration_hooks,\n\t\t\t\t\thydration_hooks, handlers)\n\t\texcept InvalidParams:\n\t\t\tconn.close()\n\t\t\tyield self.handle(\"begin\", conn)\n\t\n\tdef update(self, message):\n\t\tconn = self.get_or_create_conn(mode=\"UPDATE\", bookmarks=[], metadata={},\n\t\t\t\t\t\ttimeout=self.timeout())\n\t\t\n\t\tif message.find(\".\")!= -"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\t_LOG_DEBUG(\"_begin(): (mode=%r) tags=%r bookmarks=%r \"\n\t\t    \"timeout=%r db=%r imp_user=%s dehydration_hooks=%r hydration_hooks=%r handlers=%r\" %\n\t\t    (mode, options, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks))\n\t\t\n\t\tif mode not in (\"READ\", \"WRITE\", \"REMOVED\"):\n\t\t\traise TypeError(\"Invalid mode arguments'mode', '%s', \" % mode)\n\n\t\tcommand = (type, bson"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\n\t\n\t_logger.debug(\"Begin: %s\", text or \"\", exc_info=True, extra={\n\t\t    \"reason\": \"Begin not implemented (yet I don't know why it didn't do it right)\",\"reason_type\": type(value).__name__}, exc_info=True)\n\t\n\tself._logger.debug(\n\t\t\"\"\"\n\t# The connection information is obtained from the connection-info table\n\t# (the.transaction_id key defined here), in this example: (1, \"user@host.com\", [10, \"user@host\", \"user@host."}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t    \n\t\n\tif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"Mode must be READ or WRITE\")\n\t\t\n\ttry:\n\t\tbolt = self._connection.begin(mode, bookmarks, metadata, timeout,\n\t\tself._imp_user, self._dehydration_hooks, hydration_hooks, handlers, timeout, tags=tags)\n\texcept Exception:\n\t\tlogger.exception(\"Error when trying to begin the transaction\")\n\t\tsys.exc_clear()\n\t\traise\n        \n\t\n\treturn bolt\n\t\n#-------------------------------------------------------------------------------\r\n# pyglet\r\n# Copyright ("}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\teven = int(n)\n\t\tif even > 0:\n\t\t\treturn even\n\t\telse:\n\t\t\treturn (n%6) + 1\n\n\tdef round_even_to_odd(*args):\n\t\"\"\"\n\t>>> round_even_odd(*3)\n\t    2\n\t    >>> round_even_odd(*3.2)\n\t    2\n\t    >>> round_even_odd(*3.5)\n\t    2\n\t    >>> round_even_odd(*6)\n\t    2\n\t    >>> round_even_odd(7)\n\t    1\n\t    >>> round_even_odd(7.2)\n\t    1\n\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn int(math.ceil(2*n/(4*math.pi)))\n\n\tdef _round_half_to_odd(n):\n\t\treturn int(math.ceil(-2*n/(4*math.pi)))\n\n\treturn _round_half_to_even\n\n\tdef __init__(self, n, **kwargs):\n\t\tif(n<0): n__isn = n\n\t\telse: n__isn = n % 3 and n % 2\n\n\t\tsuper(Round_Half_to_Even, self).__init__(self, n, **kwargs)"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn int(int(n * (n + 1) / 10.0) / (n + 1))\n\n#################################################################################################################################################################################################################################\n#################################################################################################################################################################################################################################\n\n\n@type(item_type=c_int)\nclass Image:\n\tdef __init__(self, file):\n\t\tpass\n>>>>>>> b875702c9c07ab5012e52ff4337439b03918f453\n#!/usr/bin/env python\n\nimport os, sys\nfrom distutils.core import setup\nfrom setuptools import find_packages\n\n\nwith open(os."}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\tn = round(n)\n\t\tif n!= 3:\n\t\t\treturn int(n) % 2\n\t\telse:\n\t\t\tn = int((n - 3) / 2)\n\t\t\tif n!= 3:\n\t\t\t\treturn 0\n\t\t\telse:\n\t\t\t\treturn n\n\t\n\t@classmethod\n\tdef get_even_digits_from_n(cls, n, digit=True):\n\t\t\"\"\"\n\t\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even("}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\tf = (float(n) - int(n)-2)\n\t\treturn round(f)\n\texcept ZeroDivisionError:\n\t\treturn 0.0\n\ndef round(val):\n\t\"\"\"\n\t>>> round(float('inf'))\n\t    inf\n\t    >>> round(float('inf'),2)\n\t    inf\n\t\t>>> round(float('inf'),3)\n\t    inf\n\t\t>>> round(float('infinity'),4)\n\t    infinity\n\t\t>>> round(float('inf'),4)\n\t    infinity\n\t\t>>> round(float('infinity'),3)\n\t    infinity\n\t\t>>> round("}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn int(round(n * 5))\n\treturn round(n * 3)\ndef is_odd_even(n):\n\t\"\"\"\n\t>>> is_odd_even(3)\n\tTrue\n\t>>> is_odd_even(7)\n\tFalse\n\t>>> is_odd_even(7, n=4)\n\tTrue\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn n!= (3 if n < 4 else 7)\n\t# return abs(a + b + c)!= a * b + c\n\ndef is"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn int(math.floor(n))\n\n\tdef get_even_half(n):\n\t    \"\"\"\n\t>>> get_even_half(2)\n\t    2\n\t    >>> get_even_half(3)\n\t    3\n\t    >>> get_even_half(1.2)\n\t    1\n\t    >>> get_even_half(4)\n\t    4\n\t    >>> get_even_half(4.2)\n\t    4\n\t    >>> get_even_half(4.5)\n\t    4\n\t    >>> get_even_half(4.7)\n\t    4\n\t    >>> get_even_half(4.2)"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\treturn round(n * (1 - (1.0 / math.pi)) * n, abs=not n)\n\n# http://www.fenja.io/blog\n\n# http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/651706\ndef round_even(n):\n\t\"\"\"\n\t>>> round_even(2)\n\t    2\n\t    >>> round_even(2.5)\n\t    2\n\t\t\n\t    >>> round_even(1.0)\n\t    1\n\t        >>> round_even(2.0)\n\t        2\n\n\t\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\tif n < 200:\n\t\t\treturn 5\n\t\telif n < 400:\n\t\t\treturn 4\n\t\telse:\n\t\t\treturn n-4\n\n\tdef fix_even(i,a):\n\t'''\n\tfix_even(i,a)\n\t\n\t:param i:\n\t:param a:\n\t'''\n\t\treturn a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a+a\n\t'''\n\treturn"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\tif (n < 0):\n\t\t\treturn n\n\t\tn += int((-n) / 2.0)\n\t\t\n\t## this is a good idea to know how to handle 2 or 5\n\tclass RoundHalfToEven:\n\t\t\"\"\"\n\t\t>>> round_half_to_even(.1)\n\t   .2\n\t   .5\n\t   .7\n\t\t\n\t    >>> round_half_to_even(.7)\n\t   .7\n\t\t\"\"\"\n\t\tdef __init__(self, n = 0):\n\t\t\tself.n = n\n\t\t\tself.r = {}\n\t\tdef round_half_to_even"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tdef make_p1():\n\t\t\"\"\"\n\t\tCreate a Point subclass. Used for simple geometry data for generating a basic \"field\".\n\t\t\"\"\"\n\t\t\treturn Point([\n\t\t\t\tPoint.create_point(name=str(name+'__p1%s__%s'%(field_srid,srid_map[name]))\n\t\t\t\t\tfor field_srid in field_srid_map.keys()\n\t\t\t\t])\n\n\t\tdef make_p2():\n\t\t\t\"\"\"\n\t\t\tCreate a Point subclass. Used for simple geometry data for generating a basic \"point\".\n\t\t\"\"\"\n\t\t\treturn Point([\n\t\t\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n\n\n\tclass DeepPoint(Point):\n\t\tdef __iter__(self):\n\t\t\treturn iter([(f.type, f.wkt) for f in self.geometry])\n\n\t\tdef __getitem__(self, index):\n\t\t\t# NOTE : index is a number of fields - 1 - 4\n\n\t\t\td = super(Point, self).__getitem__(index)\n\t\t\tif d is None:\n\t\t\t\traise IndexError(\"Point %s has no such field(s)\" % index)\n\n\t\t\treturn Point(d)\n\n\t\tdef __add__(self, other):\n\t\t\tresult = Point.__class__(self."}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tif 'type_name' in fields:\n\t\t\tps = types.Point\n\t\telse:\n\t\t\tps = types.Point\n\t\tif srid_map:\n\t\t\t# Re-create it to save performance.\n\t\t\tps.srid_set = srid_map.copy()\n\t\t# Create points.\n\t\tps.x, ps.y, ps.z = Point(ps.x, ps.y, ps.z, ps.x + ps.y, 0, srid=4326)\n\t\tif 'type' in fields:\n\t\t\tps.type = fields['type'].upper"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tif name is None:\n\t\t\treturn None\n\n\t\tif '_sql_type' in fields:\n\t\t\treturn fields['_sql_type'](self.get_model_from_fields(name, fields, False, srid_map))\n\t\telse:\n\t\t\treturn None\n\n\tdef create_geom(self, geom):\n\t\t\"\"\"Create a Geometry subclass from the provided geom.\"\"\"\n\t\tif self.geom_type == ogr.DSOG:\n\t\t\treturn self.geom_class(GEOSGeometry(self.point_type(geom.geom.srid), geom.geom.srid, geom.geom"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tpoint_dict = dict(zip(fields, fields))\n\t\tpoint_new = Point(point_dict)\n\t\tfield_list = []\n\t\tfor attr_name, attr_value in iteritems(point_dict):\n\t\t\tif attr_name in (\"pointlength\",\"pointwidth\",\"pointheight\"):\n\t\t\t\tattribute = {}\n\t\t\t\tif getattr(point_new,attr_name):\n\t\t\t\t\tif attr_value:\n\t\t\t\t\t\tfield_list.append((attr_name,attr_value))\n\t\t\t\telse:\n\t\t\t\t\tsetattr(point_parent,attr_name,attr_value)\n\t\t\t\n\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tif not isinstance(fields, basestring):\n\t\t\tfields = [fields]\n\tif not fields:\n\t\traise ValueError(\"Points should have at least one field as field.\")\n\tpoints = []\n\tfor item in fields:\n\t\tpoints.append(Point(item, srid_map[item]))\n\treturn points\n\nclass PointQuerySet(object):\n\t\"\"\"\n\tThis class will help obtain points from the database into a set of points.\n\t\"\"\"\n\tdef _fetch_data(self, field):\n\t\t\"\"\"\n\t\tThis function can take either:\n\t\t\t- A \""}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tfields = [(f['attname'], f[f['attname']])\n\t\t\tfor f in fields\n\t\t\tif isinstance(f, (DictField, ListField))\n\t\t\tand issubclass(f.field.type, (AutoField, ForeignKey)))]\n\t\tfield = None\n\t\tif len(fields) > 1:\n\t\t\tfor f in fields:\n\t\t\t\t# Add any implicit references to the base classes of the fields class.\n\t\t\t\tif f[1] is not None and f[1][0] is None:\n\t\t\t\t\tfor sub_class in (f[1][1] for f in fields):"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tpoint_class = None\n\t\tfor fld in fields: \n\t\t\ttry:\n\t\t\t\tpoint_class = Point.objects.get(base_ptr=fld, srid=4326)\n\t\t\texcept Point.DoesNotExist:\n\t\t\t\tpoint_class = None\n\t\t\tif point_class is None:\n\t\t\t\tbreak\n\t\treturn point_class\n\n#===============================================================================\n# A Geographic Intersection class.\n#===============================================================================\nclass Area(models.GeoModel):\n\t\"\"\"\n\tRepresents the area of a set of attributes and fields.\n\t\"\"\"\n\t# The x, y range of the two points to"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\treturn getattr(Point, name), Point(**fields)\n\ndef create_point(geom_type=Point):\n\t\"Generates a Point.\"\n\tif geom_type is not Point:\n\t\traise TypeError(\"Got invalid geom_type %s\" % type(geom_type))\n\treturn Point(geolocate(geom_type, srid_map.get(settings.GEOMETRY_TYPE, 'POINT')))\n\ndef line_generator(lines):\n\t\"A function that generates a list of points.\"\n\tline_list = []\n\tfor line in lines:\n\t\tline_"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\tname = geom_to_point_type(name).replace('POINT', 'POINT4')\n\n\treturn type_mapping[field_type(name, fields, srid_map)](field_type(name, fields, srid_map)).__new__(cls)\n\n# Returns the geometry for a named field\ndef _get_field_geometry(name):\n\t_name = to_unicode(name).replace('-', '_')\n\treturn Point.get_geometry(_name)\n\n@classmethod\ndef get_geometry(cls, geometry_field):\n\treturn Geometry.get(Geometry"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\t@deprecate(message)\n\t\tdef foo(x):\n\t\tif not inspect.ismethod(x):\n            raise TypeError, \"A type-dependent method should be decorated with @deprecated(x) for functions.  (i.e. the decorated method may be added in place of ``foo(...)``, which is a function object, which is evaluated before the *args* are removed)\")\n\treturn foo\n\ndef deprecated(message):\n\t\"\"\"\n\tDecorator for deprecating functions and methods and method names.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\treturn lambda *a: doc(foo.__dict__, *a)\n\t\n\tdef _foo_deprecated(func):\n\t\treturn lambda x: doc('foo', x, func)\n\n\tdef _bar_deprecated(func):\n\t\treturn lambda x: doc(foo.__doc__, func.__doc__)\n\n\t# test that deprecated arguments are forwarded correctly\n\t@deprecated(\"Deprecated arguments are only supported with arguments\")\n\tdef foo():\n\t        return Foo()\n\t\n\t@deprecated(\"Deprecated arguments are only supported with attributes\")\n\tdef bar():\n\t        return Foo()\n\t\n\t@deprecated(\"Deprecated arguments"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    return lambda f: f()\n\n\ndef _deprecated_function(f):\n\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated('args')\n\t    def arg_args(args=None, varargs=None, varkw=None, defaults=None, kws=None, defaults_copy=None, kwonlyargs=None, kwonlydefaults=None, signature=None, kwarg=None, kwarg_defaults=None, kwarg_kws=None, kwarg_defaults_copy=None):\n\t    @deprecated('bar')\n\t    def bar(a, (x,"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\tif not message:\n\t    raise ValueError(\"foo takes at least one argument \"\n\t\t\t\"but not more than one\")\n\t    \n        @wraps(message)\n        def decorator(*args, **kwargs):\n            return _dec_decorator(message, *args, **kwargs)\n            \n        @wraps(message, methods=[\"foo\"])\n        def decorated_method(*args, **kwargs):\n            msg = 'foo must be deprecated in favour of %(function)s and'\\\n                'function %(method)s'\n            raise TypeError(msg)\n        \n        @wraps(message, methods=[\"foo\"])\n        def decorated_method():\n           "}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\tdef deprecating(func):\n\t\tdef deprecating_wrapper(*args, **kwargs):\n\t\t    return func(*args, **kwargs)\n\treturn deprecating_wrapper\n\nclass MagicMeta(type):\n    \"\"\"\n\tMetaclass that allows for class methods to be marked as private,\n\tbut not a public and private.\n\t\"\"\"\n    def __new__(mcs, name, bases, attrs):\n\n        # Note that we make this *public* and private\n        # private at the top level - so this prevents us from being imported\n        # when the class is accessed.\n        \n        return super"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\t\nclass Decorator(logging.Logger):\n\t\"\"\"\n\tDecorator and method for converting a logger to a :class:`logging.Logger`.\n\t\n\t:keyword __name__: the instance identifier\n\t:keyword default: the default representation of the logger\n\t:keyword handler: the function to call after logging has been processed.\n\t\"\"\"\n\t\n\tdef __init__(self, handler=True, name=None):\n\t\t\"\"\"\n\t\tInitializes a new logger.\n\t\t\n\t\t:keyword handler: whether to use this function, or to use a\n\t\t        class level logger\n\t\t:keyword name: the class name"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\t    assert method_decorator not in decorators, message\n\t@deprecated(\"foo() is deprecated in favour of 'bar'!\")\n\tdef foo(x):\n\t        pass\n\t    \n    @deprecated(\"'foo()' has been deprecated in favour of 'bar'\")\n\tdef foo(x):\n\t\treturn \"foo function ignored\"\n    foo.__name__ = '_foo'\n\tfoo.__doc__ = '''\n\tDecorator for deprecating decorators that are not decorated yet.\n\n\tThe decorator wraps methods that are added to a class. See module\n\tdocumentation for further details.\n\t'''"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    \n\t    @staticmethod\n\t    def foo(foo, bar):\n\t        foo()\n\t\t\n\t    @staticmethod\n\t    def bar(foo, baz):\n\t        pass\n\n\t:param str foo:\n\t    string the string that will be returned. \n\t:param str bar:\n\t    string the string that will be returned. \n\t\t\n\"\"\"\n\t    def decorator(func):\n\t\tdef _wrap(arg, foo, bar):\n\t\t    if _DEBUG: print \"Calling '%s' with %r, %r instead\" % (func.__name__, foo, bar)\n\t\t    try: \n\t\t    return func(arg, foo, bar)\n\t\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\tdef wrapper(method):\n                def wrapper_method(*args, **kwds):\n                        return method(*args, **kwds)\n                return wrapper(method)\n\t\twrapper.__name__ = function.__name__\n\t\twrapper._deprecated = deprecated\n\t\treturn wrapper\n\n\t:param method: the type of function\n\t\t:param arg: the argument\n\t\t:param kwds: the keyword arguments\n\t\"\"\"\n        functools.update_wrapper(wrapper, method, arg, kwds)\n\n\treturn wrapper\n\n#-----------------------------------------------------------------------------\n#  Copyright (c) 2009-2011, IPython Development Team.\n#\n# Distributed under the"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t     \n\tdef _deprecated(self, old_msg, newname):\n\t\tmsg = '\\n'\n\t\tmsg += (old_msg or new_msg)\n\t\treturn warnings.warn(msg, category=PendingDeprecationWarning)\n\n\tdef new_function(self, old_msg, new_msg, name):\n\t\t\"\"\"\n\t\tDecorate a function or method.\n\t\t\"\"\"\n\t\twarnings.warn(self._deprecated(old_msg, new_msg), category=PendingDeprecationWarning,\n\t\t\t\t\t_mod=self.new_function)\n\tdef __call__(self, old_"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn code\n_inline_r_setup.inline_r_setup = 0\n\ndef inline_r_set(\n    code: str,\n    options: Dict[\"Options\", \"ConfigWriter\"],\n) -> str:\n\t\"\"\"\n\tSet inline configuration in R, and return the rendered config.\n\tIf R is not configured, an exception is raised.\n\t\"\"\"\n\t\tcode = _inline_r_setup.inline_r_set(code, options)\n\treturn _InlineR_parse(code)\n\n#!/usr/bin/env python\n# Copyright (c)"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tglobal AINOSET\n\t\tglobalsvar = AINOSET[-1]\n\t\tif globalsvar in ('R', '_in_aioss_env'):\n\t\t\tcode = os.environ.get(globalsvar)\n\t\telse:\n\t\t\traise NoOptionError(\"R environment var '%s' not set!\" % globalsvar)\n\t# Now set the variable name for this R in aioss_app.\n\treturn r_setup(code)\n\ndef _inline_r_cleanup(code: str) -> str:\n\tglobal AINOSET\n\n\t# Now remove"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tis_configure = False\n\t\tis_r_configured = False\n\t\tif environment('R'):\n\t\t\tis_configure = environment('R')\n\t\tif environment('S'):\n\t\t\tis_configure = environment('S')\n\t\tif environment('env'):\n\t\t\tis_configure = env.get('NAV_CONFIG', False)  # type: bool\n\t\treturn is_configure\n\ndef _disable_r_setup() -> bool:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tr = env.get_env(code.replace(\"\\r\", \" \").title())\n\n\t# Add the first two lines of the code using %r to avoid confusing\n\t# the test coverage in pytest.raises(Exception), and also add these lines\n\t# with the error.\n\tr = re.sub(\"`#.*$`(.*?)`\", r\"\") \\\n\t\t.format(*map(str, code)) \\\n\t\t.format(*map(str, r.__traceback__)) \\\n\t\t.format(env.line_offset, str(code) < 0)\n\n\t#"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tenvdir = os.getcwd()\n\n\t\tif path.isdir(envdir):\n\t\t\tprint (\"Warning: Path does not exist at destination dir %s.\" % envdir)\n\n\t\telif path.exists(envdir):\n\t\t\tprint (\"Directory %s already exists at destination dir %s.\" % (envdir, rcdir))\n\n\t\telse:\n\t\t\tprint (\"Error, path does not exist at destination dir %s\" % rcdir)\n\t\t\tos.chdir(rcdir)\n\t\treturn rcdir\n\n\tdef _update_build(self, project, version, compiler=None):\n\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\treturn code\n\n\t# This method will take two strings, a string, and a tuple of parameters.\n\tdef _handle_r_options(self, code: str, code2: str) -> str:\n\t\tif not self.env.has_resource('config'):\n\t\t\treturn code\n\n\t\t# This logic tries to set the required parameters, in order\n\t\t# to set these before sending it to R.\n\t\tfor param in self.env['config']['R'].params.items():\n\t\t\tif param.startswith('r_'):\n\t\t\t\tparam_value = self._get_real_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\ndef post_setup(options) -> str:\n\t\"\"\"\n\tThis function gets added to the ``R`` object after every R initialization.\n\t\"\"\"\n\t\ndef get_config_file(config_path: str) -> str:\n\treturn os.path.join(config_path, R_CONFIG_FILE)\n\t\ndef read_config_file(config_path: str) -> None:\n\t\"\"\"\n\tThis function gets added to the ``R`` object after every R initialization.\n\t\"\"\"\n\t\ndef run(args: str) -> None:\n\t\"\"\"\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\t# we'll use our own global value to prevent issues if the\n\t\t# environment variable is being changed. We'll use a special\n\t\t# 'env' value, which will override an environment variable\n\t\t# of the form 'foo=bar' (or use a default value for the 'bar'\n\t\t# argument to initialize R.\n\t\t# We will use a 'env' value, which will override a variable\n\t\t# of the form 'env(FOO=BAR)', which will generate a config\n\t\t# for 'foo=bar'.\n\t\t_GLOBAL_CONFIG = Config(os.environ"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\tcode = code.replace(\"#define R__R_DEBUG\", \"DEBUG_RAISE\")\n\t\tcode = code.replace(\"#define R__R_VERBOSE\", \"VERBOSE_RAISE\")\n\n\t\tcode = code + \"(R)D_DONT_CONFIG_THROUGH_CONFIG(0)\"\n\t\tcode = code + \"(R)R_DEBUG(CONFIG)\"\n\t\tcode = code + \"(R)R(DEBUG)\"\n\t\tcode = code + \"(C)D_DEBUGGER_DEBUG(1)\"\n\t\tcode = code + \"(C)D_DOGGER_VERBOSE("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\twith os.chdir(os.path.dirname(code)) as code_curdir:\n\t\t\twith open(code_curdir + '/setup.cfg') as code_file:\n\t\t\t\treturn code_file.read()\n\ndef _inline_c_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of C cannot be configured via env variables, but can\n\tonly be configured via C options once C has started. These are set here.\n\t\"\"\"\n\t\twith os.chdir(os.path.dirname(code)) as code_curdir:\n\t\t\twith open("}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef main(args):\n\t\"\"\"\n\tMain routine of xargs. Turns on or off the GUI.\n\t\"\"\"\n\timport os\n\tos.environ['XPARVISOR_GUI_ID'] = str(os.getppid())\t\t\n\tos.environ['XPARVISOR_APP_ROOT'] = os.getenv('XPARVISOR_APP_ROOT', '')\t\n\n\t# Setup XPARVISOR\n\tXPARVISOR_ROOT = os.getenv('XPARVISOR_ROOT', '')\n\tXPARVISOR_PORT = os."}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n@contextmanager\ndef _prepare(args, command, target_concurrency):\n\tif target_concurrency == 1 and hasattr(_os,'mkfifo'): # not available in Python 2.7\n\telse:\n\t\tos.mkfifo(target_concurrency)\n\tyield command\n\n@contextmanager\ndef _stderr(args, command):\n\tif not hasattr(_os, 'dup2'):\n\t\tos.dup2(args.fileno(), 0)\n\ttry:\n\t\targs.flush()\n\tfinally:\n\t\tdel _os\n\twith _prepare(args, command, _"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\"\"\"\n\tdef __init__(self, color='', target_concurrency=1, *args):\n\t\tself.color = color\n\t\targs = tuple([str(i) for i in args])\n\t\tfor i in args:\n\t\t\tif i.startswith('@'):\n\t\t\t\ti = i[1:]\n\t\t\tself.set_value(i, color)\n\n\tdef set_value(self, i, color):\n\t\t\"\"\"\n\t\tSets a PTY color and executes the argument\n\t\ton ``self.color`` and ``self.target_concurrency``\n\t\t\n\t\t:args i: A"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\tresult = subprocess.call(['xterm','-c','-T','-t',str(target_concurrency),'@'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\treturn result.replace(\"\\n\",\"\\n\")\n\n\tdef xargs_default(self,target_concurrency=1,cpu=1,mem=1000000,**kwargs):\n\t\treturn \"--cpu %s\\n--mem %s\\n--cpu %s\\n--mem %s\\n\" % (cpu,mem,target_concurrency,int(extra_args(self.xterm_default),))\n\n\tdef x"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\tyield (Color('white'), 'green')\n\t\tif curses.tiggtg(\"xterm\"):\n\t\t\tyield (Color('white'),'red')\n\tyield (\n\t\tColor('white', color), \n\t\t('A', target_concurrency * 3),\n\t\t\n\t\t'b',\n\t\t'w',\n\t\t'b',\n\t\t)\n\t\t\ndef yargs(\n\t\"\"\"\tA simplified implementation of yargs.\n\t\n\tcolor: Make a pty if not on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\tyield (Color('"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef make_pty(\n\targs=None\n):\n\n\tpty = os.environ.get( 'TERM', 'xterm' )\n\tif pty == 'dumb':\n\t\treturn ptys.dumb\n\telif pty == \"xterm\":\n\t\treturn ptys.xterm   # for GTK+\n\n\tif args is None:\n\t\targs = shlex.split(args)\n\n\tproc = subprocess.Popen( args )\n\n\tif pty == 'dumb':\n\t\tstdout = subprocess.PIPE\n\telif pty == \"xterm\":\n\t\tstdout = subprocess.PIPE\n\telse:"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef xargsint(\n\t\"\"\"\n\tGiven a pty xargs string, produce a pty in order to execute\n\txargs for each command.\n\t\n\tThis utility will execute all xargs in a separate process using the\n\tterminal program.\n\t\n\tReturns:\n\t\tA string of the xargs if it can be executed\n\t\"\"\"\n\tstdout = xargs(\n\t\t\"\"\"\n\t\t{\n\t\t\t# The xargs is a list of arguments that may already be executed\n\t\t\t# but not yet executed (except for outputting command output)\n\t\t\t# If the user just wants this, then"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\ndef exec_command(\n\tproc_name,\n\tcmdstring,\n\ttimeout=100000,\n\tstdin_string=None,\n\tstdout_string=None,\n\tstderr_string=None,\n\tstdin=None,\n\tstdout=None,\n\tstderr=None,\n\tenv=None,\n\tshell=None,\n\tstderr_pipe=None,\n\ttty=None\n):\n\t\"\"\" Execute a command in a subprocess for an executable\n\tparam argv: List of command arguments to pass to subprocess\n\tparam timeout: Timeout for command execution (in"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t \n\tdef __init__(\n\t):\n\t\tself.target_concurrency = None\n\t\tself.color = 'rgb(0, 0, 0)'\n\t\t\n\tdef run(self, cargs):\n\t\t\"\"\"\n\t\tRun a set of arguments.\n\n\t\t:param cargs: A list with the arguments to use.\n\t\t\"\"\"\n\t\tself.set_cmd_list([\n\t\t\t'xterm -e %s/xterm.exe' % (self.target_concurrency,)])\n\t\tif len(cargs) > 1:\n\t\t\tprint('WARNING: Xterm is using multiple args."}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\nclass FTCPutput(FancyFancy, unittest.TestCase):\n\n\n\t@classmethod\n\tdef setUpClass(cls):\n\t\t\"\"\"\n\tThe fastest way to use the pipes is to generate the same pipes to\n\topen pipe files from the operating device.\n\t\n\t\"\"\"\n\t\n\n\tdef __init__(self, pipe_file):\n\n\t\timport fcntl\n\t\tif os.name!= 'nt':\n\t\t\tself.fd = os.open(pipe_file, os.O_RDONLY)\n\t\t\tself.pipe_fd = os.fdopen(self.fd, 'w')"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\treturn list(seq)\n\ndef shuffle(arr: List[str]) -> bool:\n\t\"\"\"\n\tShuffle the strings\n\t\"\"\"\n\t\tsortedarr = [x if not x.isalpha() and x!= \"_\" else \"!\" for x in arr]\n\t\treturn random.shuffle(sortedarr)\n\nclass List(object):\n\t\"\"\" A list of elements \"\"\"\n\tdef __init__(self):\n\t\tself._elements = []\n\n\tdef __len__(self):\n\t\treturn len(self._elements) + 1\n\n\tdef __contains__(self, element: str):"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t    \n\tif len(seq) == 0:\n\t    \tseq = _shuffle()\n\treturn seq\n\ndef _shuffle(seq: Sequence[str], keep):\n\t\"\"\"\n\tShuffle a sequence (default implementation)\n\n\tArgs:\n\t\tseq: A sequence to shuffle the sequence of\n\t\n\tKeyword Args:\n\t\tkeep_first: If True, the order of the sequences is preserved.\n\t\n\tReturns:\n\t\tA sequence\n\t\"\"\"\n\n\ti = np.arange(len(seq))\n\n\tif len(seq) % 3 == 0:\n\t\ti = i * 3"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\ndef _merge_sequences(seq: Sequence[str], keep_right: int = 0, merge_length: int = 2) -> Sequence[str]:\n\t\"\"\"\n\tMerges two sequences and keeps the smallest length\n\t\"\"\"\n\tif keep_right < merge_length:\n\t\treturn seq\n\telif keep_right < len(seq):\n\t\tyield seq[:keep_right].rstrip(' ')\n\t\treturn seq[keep_right:]\n\telse:\n\t\treturn seq\n\ndef is_file_name(a_file_name: str) -> bool:\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tnew_list = []\n\t\tshuffled_length = len(seq)\n\t\twhile shuffled_length - shuffled_length!= 0:\n\t\t\tshuffled_length -= shuffled_length\n\t\t\tsequence = seq[shuffled_length:shuffled_length + shuffled_length - 1]\n\t\t\tnew_list.append(sequence)\n\t\t\tsequence = seq[:-shuffled_length]\n\t\treturn new_list\n\"\"\"\nhomeassistant.components.recorder.rfq_device_tracker\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nProvides functionality to record rfq information about"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\ndef _shuffle(seq, length):\n\t\"\"\"\n\tShuffle\n\t\"\"\"\n\t\n\n\tret = []\n\tfor i, item in enumerate(seq):\n\t\tlength *= random.randint(1, length-1)\n\t\tif i == length-1:\n\t\t\tret.append(item)\n\t\telse:\n\t\t\tret.append(item +'')\n\treturn ret\n\ndef _shuffle_with_random_string(seq, length):\n\t\"\"\"\n\tRandomly shuffle \n\t\"\"\"\n\t\n\t\n\tfor item in range(length):\n\t\tif item > length and"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\t# Shuffling happens twice!\n\t\tdef func(seq: Sequence[str]) -> str:\n\t\t\twhile (len(seq) >= len(seq)):\n\t\t\t\tyield seq[0:-len(seq) - 1], seq[0:len(seq) - 1]\n\n\t\treturn _shuffle(first_item=_shuffled(seq),\n                         first_item=_shuffled(seq[:len(seq))])\n\n\ndef filter_string(seq: str) -> List[str]:\n\t\"\"\"\n\tFilter out strings\n\t\"\"\"\n\t\treturn [\n\t\tsequence.replace('"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tfor item in seq:\n\t\t\tseq_copy = Seq(seq)\n\t\t\tif item in seq_copy:\n\t\t\t\tseq_copy.remove(item)\n\t\t\tyield seq_copy\n# Copyright 2015 Stijn Decotis\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\ndef _split_sample(sample: str) -> Iterable[str]:\n    \t\"\"\"\n    Function to split a sample into training and test sets\n\n    Args:\n        sample: Sample to split into train/test sets\n\n    Raises:\n        InputValueError if samples must contain only experts\n    \"\"\"\n        if set(sample).issubset(set(_shuffled_items(sample))):\n            raise InputValueError(\"sample contains duplicate items\")\n        if set(sample).issubset(_shuffled_items(sample + sample)):\n            return _shuffled_items(sample)[::len(sample)]\n        raise InputValueError"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\tlst = []\n\n\t\tfor i, e in enumerate(seq):\n\t\t\tlst.append(e[:i])\n\n\t\treturn lst\n\n\n#\n# This was copied from\n# http://stackoverflow.com/a/13548225/172635\n# modified slightly extensively on the same page\n#\ndef _check(a: Iterable[str]) -> int:\n\tb_seq: Sequence[str]\n\tassert b_seq!= a[0]\n\tassert b_seq[0] == a[1]\n\tb_seq = b_seq._sh"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t   # For each element in the sequence, put the corresponding element at random\n\t   # and then return it. \n\t   # This could break up repeating in a long time so that the shuffling\n\t   # code is good.\n\t   # random.shuffle(seq)\n\t   shuffled_seq = np.concatenate((seq, np.zeros(shape=(1))))\n\t   shuffled_seq = shuffled_seq[:-1]\n\t   return shuffled_seq\n\n\tdef _shuffle_by_permutation(self, items, shuffling, batchsize): \n\t\tshuffle_idx = np.random"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\tp = _VERSION_RE.match(s)\n\t\tif p is None:\n\t\t\traise RuntimeError(\"couldn't parse version string {}\".format(s))\n\t\treturn int(p.group(1)), int(p.group(2))\n\n\tdef hash_string(self, s) -> bytes:\n\t\t\"\"\"\n\t\thash the input string\n\t\t\"\"\"\n\t\treturn hash((self.hash_hex(c) for c in s.decode('ascii')).digest())\n\n\tdef hash_hex(self, num) -> bytes:\n\t\t\"\"\"\n\t\thash the input hex string"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\ts = s.split(\".\")\n\t\t# version (number of digits, format, optional)\n\t\tif len(s) == 3:\n\t\t\tm = s[0]\n\t\t\ttry:\n\t\t\t\tassert m.group('p') is not None\n\t\t\t\treturn m.group('p')\n\t\t\texcept (AssertionError, ValueError):\n\t\t\t\tpass\n\t\t\treturn 1, 0\n\ttry:\n\t\ts = s[0]\n\texcept (AssertionError, ValueError):\n\t\treturn 2, 0\n\ndef version_from_release(s: str) -> int:\n\tm = re_"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t return VersionTuple(int(s), \"master\")\n\nfrom. import __version__\n\n__all__ = [\"get_version\", \"version_tuple\", \"VersionTuple\", \"parse_version\", \"parse_versionTuple\", \"parse_versionTuple\"]\n\"\"\"\nDjango settings for ossd2-sentry project.\n\nGenerated by 'django-admin startproject' using Django 1.9.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.9/topics/settings/\n\nFor the full list of settings and their values"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\t# https://github.com/dustin/poor-man/blob/master/VERSION_FILE.md#version-hash-for-file\n\t\tversion_hash = re.match(r'^__(.+)\\.\\d+$', s)\n\t\treturn version_hash.group(1), version_hash.group(2)\n\treturn False, False\n\nclass PokerValidator(poor_validator):\n\t@classmethod\n\tdef is_registered(cls, poor_command):\n\t\tcls.parse_poor_command(poor_command)\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\t# ignore empty string so version will be compared in a 0.X.x case (ie not empty )\n\t\treturn 0, 0\n\n\t# the first match is always present, after looking for\n\t# a git commit date,\n\t# and a git tag\n\t# we assume the tag is unitid.\n\t# if we can find the version, we assume it is the last commit\n\ttry:\n\t\tcommit_re = re.findall('([^)]+)$', s)\n\texcept:\n\t\tcommit_re = s\n\n\tif commit_re:\n\t\treturn 0"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n\t\n\tfor char in re.split(b'\\W+', s):\n\t\ttry:\n\t\t\trver = int(char)\n\t\t\tbreak\n\t\texcept ValueError:\n\t\t\tbreak\n\t\n\tif version >= 20:\n\t\treturn (2, 2, rver)\n\t\n\treturn (2, 0, 0)\n\n\ndef version_diff_in_str(s: str, version1: str) -> Tuple[int,...]:\n\t\"\"\"\n\tcompare versions\n\t\"\"\"\n\t\n\t\n\tmatches = search.findall(s)\n\n\tif not matches:\n\t\treturn (0, 0"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\n\treturn parse_version(s)\n\ndef parse_version_str(s: str) -> str:\n\t\"\"\"\n\tpoor man's version comparison (a version string).\n\t\"\"\"\n\t\ndef is_version_equal(major_version: str, minor_version: str) -> tuple[int,...]:\n\t\"\"\"\n\tTests if'version' is equal to'major_version'\n\t\"\"\"\n\t\n\t\n\t\n\tif major_version!= minor_version:\n\t\tif major_version == minor_version or major_version == '1':\n\t\t\treturn (major"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\tver = int(s)\n\t\tif ver < 10 and 0 <= ver <= 100 and ver < 4000:\n\t\t\treturn ver, ver + 10, ver + 4000, ver + 8001\n\telse:\n\t\treturn ver, (ver+10)%100000, ver + 1000, ver + 12001\n\n__version__ = parse_version(open('version.py')).version\nfrom functools import wraps\nimport gc\nimport sys\nsys.modules['_cffi'] = 'cffi'\nfrom.cffi import _ffi\n\n\ndef _init_cffi():\n    global _ffi\n\n    return _ffi"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\ts = str(s, encoding='ascii', errors='ignore')\n\t\t# ignore 'P' (not in case of <major version number>), but ignore 'O' (in case of <minor version number>).\n\t\tif s[0]!= 'o' and s[0]!= '0':\n\t\t\traise RuntimeError(\"invalid POOR man's version!\")\t\n\t\ts = s[2:]\n\t\ta, b, revision, size = [s[:3], s[4:], s[6:], s[5:]]\n\t\tif a == 'alpha':\n\t\t\tsize = 'c"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t\timport re\n\t\tsearch = re.search(\n\t\t\tr\"version \\('(?P<major>\\d+)\\.(?P<minor>\\d+)\\.(?P<patch>\\d+)\\.(?P<build>\\d+)\\.\", s\n\t\t)\n\t\tif not search:\n\t\t\traise P4Error(\"unable to parse version\")\n\t\t\n\t\tmajor, minor, patch, build = search.group(\"major\", \"minor\", \"patch\", \"build\")\n\t\treturn (major >= Version.numeric) and (\n\t\t\tminor >= Version.numeric and int(patch) >= Version.numeric\n\t\t)"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\tfrom __future__ import absolute_import\n\t\timport shlex, re, subprocess\n\t\timport os\n\t\timport sys\n\t\tfrom builtins import str\n\t\timport logging\n\t\timport json\n\t\timport tempfile\n\t\tfrom subprocess import check_call\n\t\tfrom shebangs import find_filepaths\n\t\tfrom io import StringIO\n\t\tfrom tempfile import mkstemp\n\t\timport traceback\n\t\timport tempfile\n\t\tfrom.. import idiosync\n\t\timport shebangs_lib_base\n\n\t\tlogging.basicConfig(filename='Shebangs.log', level=logging.INFO)\n\t\tlogging.info"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\t# handle case where there's something in it\n\tif not len(cmd):\n\t\treturn (None, None)\n\t\n\tif '/' not in cmd:\n\t\traise Exception(\"Command does not have a '/' character\")\n\t\t\n\treturn (cmd, dict(\n\t\t.items()\n))\n\ndef get_env_var(name: str, default=None):\n\t\"\"\"\n\tReturn the given name parameter.\n\t\"\"\"\n\treturn os.environ.get(name, default)\n\n\ndef write_log_data_file(handler_name: str, filename: str,"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\tshebangs = list(args)\n\tshebangs = map(lambda x: x.startswith(\"sc\"), shebangs)\n\t\n\tparsed = normalize(sc_command(shebangs).rstrip() for sc_command in shebangs)\n\tparsed = normalize(sc_command(parsed).rstrip() for sc_command in parsed)\n\t\n\tparsed = normalize_path(sc_command(parsed).rstrip())\n\t\n\treturn tuple(parsed)\n\t\ndef normalize_text_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n\tprint(\""}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\treturn cmd[:2] + (cmd[2:] if cmd[2:] else '') + cmd\n\ndef get_command_line_from_args(cmd: tuple[str,...]) -> str:\n\t\"\"\"\n\tReturns the command and args passed to do_exec with the same\n\tcommand and arguments as cmd.\n\t\"\"\"\n\t\n\tcommand = cmd[:]\n\targuments = []\n\t\n\t# We ignore windows arguments in case some values are present...\n\tif sys.platform == \"win32\":\n\t\targuments.append('-i')\n\t\n\t# Note that windows argument is a"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t\n\t# fix_path = path + '.shebangs'\n\t# shebangs_path = path + '.shebangs'\n\t# result = call(fix_path, shell=True)\n\t# result = path + '.shebangs'\n\t# result = path + '.shebangs'\n\t# with open(shebangs_path, 'r') as fp:\n\t# \tshebangs = fp.readlines()\n\t # data = ''.join(['{0}\\n'.format(i) for i in data.splitlines()])\n\t# result"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n# the same for 'cmd' in case cmd is a file in cmd[0], the code below is not in cmd[0].hpp\ndef get_source_filename(line: str) -> str:\n\treturn os.path.basename(line)\n\n\n\ndef parse_source_lines(source_lines) -> Iterable[Generator[str], Generator[str]]:\n\t\"\"\"\n\tGiven a list of source lines to iterate over, split it into substrings.\n\n\tExample:\n\n\tParse \"foo.c++\" into\n  \t  [\"foo(cc)\", \"foo(cpp"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\tif not is_native(cmd):\n\t\t\traise TypeError(\"Expected shebangs to point to a byte string or a bytes object\")\n\t\tif isinstance(cmd, bytes) and sys.version_info[0:2] == (3, 1):\n\t\t\tcmd = cmd.decode()\n\t\tcmd = str.strip(cmd) # Convert to bytes before removing b''s\n\t\tif cmd[-1:] == b'\\n':\n\t\t\tcmd = cmd[:-1]\n\t\treturn cmd, cmd[:-1]\n\t#\t\treturn cmd, b'\\n'\n        \n# Copyright ("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\t# If cmd in a tuple of strings, it is a list of strings; just split and\n\t\t# process it through\n        if isinstance(cmd, str):\n\t\t\t# Strip whitespace\n\t\t\treturn cmd, cmd.split()\n\t\t# Else, just process it\n\t\t# But it might be even a normal python 3 thing that we don't understand\n\t\treturn cmd, cmd\n\nclass GlobConfig(object):\n\t\"\"\"\n\tConfiguration parser.\n\tI'm not clear how this is going to match\n\tin a class.  Instead, in a class we just parse it as a shell"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\t# If not set explicitly, remove shebangs\n\t\t# if possible to parse\n\t\tif not isinstance(cmd, str):\n\t\t\tlogging.warning(\n\t\t\t\t\"Invalid command from get_cmd for path: '{0}'\".format(\n\t\t\t\t\tcmd_path))\n\n\t\t# Remove shebangs\n\t\tcmd = remove_shebangs(cmd)\n\t\tcmd = remove_shebangs(cmd)\n\t\tif path in ['.', '..']:  # for path relative to shebangs\n\t\t\treturn cmd, cmd, path\n\t\tcmd = remove_shebangs(cmd)\n\t\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\tif isinstance(cmd, str):\n\t\t\ts = subprocess.check_output(shlex.split('\"' + cmd + '\"'), timeout=1)\n\t\telse:\n\t\t\t# TODO consider removing this.\n\t\t\ts = subprocess.check_output(cmd.split('\"'), timeout=1)\n\t\treturn s, cmd\n\n\nclass Shell(str):\n\t\n\tdef __new__(cls, raw):\n\t\treturn str.__new__(cls, raw)\n\n\nclass Version(str):\n\t\n\tdef __new__(cls, raw):\n\t\treturn str.new(raw.split()[0])\n\n"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\t# if key not specified, we get the default instead of using\n\t\t# that.\n\t\tif key is None:\n\t\t\tif lock is not None:\n\t\t\t\tcache[key] = lock\n\t\t\telse:\n\t\t\t\tcache[\"default\"] = key\n\t\telif key not in cache:\n\t\t\tcache[key] = callablecache(cache, key=key)\n\t\treturn cache[key]\n\treturn cached\n\n\ndef _get_lock(key, timeout=15):\n\t\"\"\"\n\tCreates an object that returns the corresponding lock object.\n\t\"\"\"\n\tif key in cache:"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tdef _memoized_func(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tcache.set(key, (func(*args, **kwargs), expire))\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn _memoized_func\n\treturn cached\n\n\ndef cached_property(func): # pragma: no cover\n\t\"\"\"Decorator to wrap a function with a simple memoizing\n\tproperty that saves its result in a.cache that can be used for\n\tthe memoizer.\n\t\"\"\"\n\t\tdef _memoized"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\t# this ensures that the cache only returns the same value every time\n\t\t# it's called.  If the cache key doesn't change, or has been\n\t\t# expired, no change should be done before the lock is released.\n\t\tresult = cache.get(key)\n\t\tif result:\n\t\t\t# when the lock hasn't expired, cache does have to be called,\n\t\t\t# if the value does not have yet been updated\n\t\t\tif lock is None or not result.has_expired():\n\t\t\t\tcache.set(key, result)\n\treturn cached\n\n\ndef cached_contextmanager("}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = _lock\n\t\telse:\n\t\t\tlock.acquire()\n\n\t\t@functools.wraps(cache)\n\t\tdef dec(*args, **kwargs):\n\t\t\tcache.put(get_cached(key, args, kwargs))\n\t\t\tcache.delete(get_cached(key, lock))\n\t\treturn dec\n\treturn cached\n\n\n# pylint: disable=no-self-use, missing-docstring\ndef clear_cached(**kwargs):\n\t\"\"\"\n\tDecorator to clear the cached results. This ensures that the cached\n\tresults are cleared when necessary."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\treturn functools.partial(cache=cache, key=key, lock=lock)\n#\n# Copyright (c) 2008-2015 Citrix Systems, Inc.\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\")\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#   distributed under the"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tcache_key = cache._cache_key\n\t\t_cache_val = cache_key, lock\n\t\tcls.cache_key = cache_key\n\t\tcls.cache_val = _cache_val\n\n\tdef __func__(cls, *args, **kwargs):\n\t\tdef cached(*args, **kwargs):\n\t\t\tret = cls.cache_val.get(*args, **kwargs)\n\t\t\tcache_key = cache._cache_key\n\t\t\tcache.set(cache_key, ret)\n\t\t\treturn ret\n\n\t\treturn cached\n\n\tcached = classmethod(cached)\n\t\"\"\""}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\tdef inner(fn):\n\t\t\tret = cache.get(key, _missing)\n\t\t\tif ret is not _missing:\n\t\t\t\tcache[key] = ret\n\t\t\treturn ret\n\t\tinner.__name__ = fn.__name__\n\t\tinner.__doc__ = getattr(cache, fn.__doc__)\n\t\tinner.__dict__['_missing'] = partial(cache.set, key)\n\t\treturn inner\n\treturn cached\n\"\"\"\n\ndef memoize(function):\n\t\"\"\"\n\tDecorator that caches a function's result in a cache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\t@wraps(cache)\n\t\tasync def wrapper(*args):\n\t\t\tlock = lock or threading.RLock()\n\t\t\tres = cache(*args)\n\t\t\tif lock.acquire(False) or thread.allocate_lock():\n\t\t\t\tlogging.warning('Attempted to retrieve: %s', res)\n\t\t\treturn res\n\t\treturn wrapper\n\treturn cached\n\n\nclass _CacheManager(object):\n\t\"\"\"\n\tA cache class to manage the function.\n\t\"\"\"\n\tdef __init__(self, func=None, key=None):\n\t\tself._cache = None\n\t\tself._"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tif lock is not None:\n\t\t\tlocked = cache.get(key)\n\t\t\tif locked and locked is not None:\n\t\t\t\treturn lambda fn: getattr(fn, lock)(*args, **kwargs)\n\n\t\tdef _wrap(fn):\n\t\t\t@wraps(fn)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif not args:\n\t\t\t\t\targ_str = \"()\"\n\t\t\t\telse:\n\t\t\t\t\targ_str = \",\"\n\t\t\t\tif fn is callable:\n\t\t\t\t\tlocked = fn(*args, **kwargs)\n\t\t\t\telif lock and lock.acquire(False):\n\t\t\t\t\tlocked = lock"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t\tcache_key = '_cached_' + key\n\t\tif lock is not None:\n\t\t\tcache._lock.acquire()\n\t\t\tif lock is not None:\n\t\t\t\treturn cache[cache_key]\n\t\telse:\n\t\t\tcache.setdefault(cache_key, callable(func))\n\t\t\treturn cache[cache_key]\n#\n# Burrowing Tools to run in the following python interpreter\n#\n# python3 -m unittest -m doctest\n# --python-module=xml_namespace -s pyxb.binding.saxs\n#\n# Copyright (C) 2007-"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tdef decorator(fn):\n\t\t\treturn cache(maxsize=maxsize, ttl=ttl, typed=typed, timer=timer)\n\t\treturn decorator\n\n\t@wraps(fn)\n\tdef wrapped(f):\n\t\t@functools.wraps(fn)\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\twith _ttl_cache.TTL(maxsize, ttl) as ttl_cache:\n\t\t\t\tif not typed:\n\t\t\t\t\tf(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tretval = fn(*args, **kwargs)\n\t\t\t\t\tttl_cache.setdefault(fn.__name__, retval"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\t_timeout_func = memoize\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves up to\n\t`maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\n\tdef __init__(self, func, name=None, timer=time.monotonic, ttl=600):\n\t\n\t\tself.func = func\n\t\tself.name = name\n\t\tself.timer = timer\n\t\tself.ttl = ttl\n\t\n\tdef __call__(self"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tif typed:\n\t\t\t@wraps(fn, set_name=True)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tkey = str(fn.__name__)\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = fn(**args, **kwargs)\n\t\t\t\thashValue = cache[key]\n\t\t\t\tif not typed:\n\t\t\t\t\treturn hashValue\n\t\t\t\treturn typed(hashValue)\n\t\telse:\n\t\t\t@wraps(fn)\t\t# use a decorator to cache, if needed\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\thashed = fn(*args, **"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tkey = (name, func.__name__, func.__doc__, func.__dict__)\n\t\tif typed:\n\t\t\treturn typed\n\t\telse:\n\t\t\treturn key\n\n\tdef __call__(self, func):\n\t\t@wraps(func)\n\t\tdef _wrapped(*args, **kw):\n\t\t\ttime.sleep(min(maxsize, 0))\n\t\t\treturn func(*args, **kw)\n\t\treturn _wrapped\n\n\t@ttl_cache\n\tdef __call__(self, func, *args, **kw):\n\t\tttl = self.__call__(func, *args, **kw)"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tself.ttl = ttl\n\t\tself.created = timestamp() # current time\n\t\tself.maxsize = maxsize\n\t\tself.timer = timer\n\t\tself.typed = typed\n\t\tself.cache_size = 0 # reduces number of objects\n\t\tself.count = 0\n\t\tself.hits = 0\n\n\n\n\tdef __call__( self, *args, **kwargs ):\n\t\t\"\"\"\n\t\t@rtype: L{CachedObject}\n\t\t@return: a caching object\n\t\t\"\"\"\n\t\tself.cached = Cache(min(self.maxsize, self.timer), maxsize=self.maxsize"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\tdef wrapper(fn):\n\t\t\t\t\"\"\"Decorator that caches and saves up to `maxsize`.\"\"\"\n\t\t\t\tdef wrapped(*args, **kwargs):\n\t\t\t\t\t\"\"\"Throw a cached version of fn with `maxsize`, then adds a\n\t\t\t\t\tnew cache key `ttl`.\"\"\"\n\t\t\t\t\tif hasattr(timeout, \"__call__\") and timeout() >= ttl:\n\t\t\t\t\t\ttimeout = timeout() - t0\n\t\t\t\t\telse:\n\t\t\t\t\t\ttimeout = 0\n\t\t\t\t\tr = fn(*args, **kwargs)\n\t\t\t\t\tif typed:\n\t\t\t\t\t\tcache_cls = CacheManager._cache_cls\n\t\t\t\t\t\tcache_cls.put(ttl, str"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\t# TODO test\n\tcache = {}\n\t\n\tdef decorate(f):\n\t\tcache[ttl] = f\n\t\treturn f\n\n\tdef decorate_decorator(f):\n\t\tcache[ttl] = f\n\t\treturn f\n\t\n\tfor function in reversed(func_list):\n\t\tif not hasattr(function, '__self__'):\n\t\t\tfunction = decorate_decorator(function)\n\t\t\t\n\t\tcache[function.__name__] = function\n\t\tdecorator = cache[function.__name__]\n\t\tif timer() >= ttl:\n\t\t\tretry = cache.get(function.__name__)\n\t\t\tif retry"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef decorator(fn):\n\t\tcache = fn\n\t\tif callable(cache):\n\t\t\tCACHE_MAX_SIZE = MAX_ITEMS * TTL\n\t\t\tcache.value = list(ttl(maxsize, ttl, callable, cache))\n\t\t\treturn cache\n\t\telif getattr(fn, '__cache__', None) is None:\n\t\t\tif typed:\n\t\t\t\tfn.__cache__ = Cache(maxsize=MAX_ITEMS, ttl=TTL, timer=timer)\n\t\t\telse:\n\t\t\t\tcache = lambda: cache_inner(fn, maxsize, timer)\n\t\telse:\n\t\t\tcache = lambda"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\n\tdef func_wrapper(self, wrapped, *args, **kwds):\n        # This has to be implemented in terms or by the subclass.  However,\n        # it seems that it is impossible to cache.\n        if not hasattr(wrapped, '__cache__') or not self.memoize(wrapped) or self.callable(wrapped):\n            setattr(wrapped, '__cache__', cache.get(wrapped.getcachekey()))\n            \n        # The function is going to already be called with timeout=None\n        # so check for timeout, and continue.\n        func = self._get_func(self.memoize(wrapped"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\tdef __init__(self, fn):\n\t\tself.fn = fn\n\t\tself.maxsize = maxsize\n\t\tself.timer = timer\n\t\tself._hash = None\n\t\tself._cached = {}\n\t\t\n\tdef __call__(self, *args, **kwargs):\n\t\tif self._cache is None:\n\t\t\tself._flush()\n\t\tif self._cache.get(self.fn):\n\t\t\tself._cache[self.fn](*args, **kwargs)\n\t\treturn self._cached[self.fn]\n\t\n\tdef _flush(self, expire_at=1):"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache = kwargs['cache']\n\t\t\tcache_key = \"mru_{0}\".format(str(id(func) + 1))\n\t\t\tres_cache = cache.get(cache_key, None)\n\t\t\tif res_cache is not None or caching.should_cache_cache(limit=maxsize, typed=typed):\n\n\t\t\t\tif hasattr(func, '__call__') and hasattr(func, 'im_func') and hasattr(func, 'im_meth'):\n\t\t\t\t\t# XXX We need to pass this to avoid re-implementing \""}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tif typed:\n\t\t\tdef _mru_cached(func):\n\t\t\t\tfrom whoosh.qparser import Query as QueryParser\n\t\t\t\tif isinstance(func, QueryParser):\n\t\t\t\t\treturn func\n\t\t\t\treturn func\n\t\telse:\n\t\t\tdef _mru_cached(func):\n\t\t\t\tquery = Query(func)\n\t\t\t\tquery.max = maxsize\n\t\t\t\tquery.max_hits = 10\n\t\t\t\treturn query\n\t\treturn _mru_cached\n\n\ndef parse(text):\n\t\"\"\"\n\tDecorator to parse text into a tuple of lists and numbers of\n\twords. \n\t\"\"\""}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef _mrucached(*args, **kwargs):\n\t\t\t'''MRU cache for a function.'''\n\t\t\targs = args[1:]\n\t\t\tif len(args) > maxsize:\n\t\t\t\t# The cache must be very small or too big, because of\n\t\t\t\t# large memory limitations.\n\t\t\t\traise OverflowError('maxsize %d must be <= maxsize of the cache' % maxsize)\n\t\t\tcache = args[0]\n\t\t\ttry:\n\t\t\t\t# MRU doesn't really need the caching, so put\n\t\t\t\t# a weak cache entry from cache to avoid\n\t\t\t\t# re-pickling\n\t\t\t\tcache["}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tif typed is False:\n\t\t\treturn mru_dict_factory(maxlen)\n\t\telse:\n\t\t\tdef memoize(cache_key):\n\t\t\t\tcache_key = unicode(cache_key, encoding='ascii')\n\t\t\t\tcache = mru_dict_factory(maxsize)\n\t\t\t\tif typed:\n\t\t\t\t\treturn cache\n\t\t\t\telse:\n\t\t\t\t\treturn cache_key\n\n\t\t\tncache = mru_list_factory(maxsize=maxsize)\n\t\t\tmemoize('m_%s = %s'%(\n\t\t\t\t'get-many-many', ncache.iterkeys()))\n\t\t\tmru_cache"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tfun = MemoizationFunction(maxsize, typed)\n\t\tfun._cache = memoize(fun._cache, maxsize, typed)\n\t\treturn fun\n\n#### Frequency Data ############################################################\ndef fast(f):\n\t\"\"\"\n\tA version of fast.\n\t\"\"\"\n\t\tf.__doc__ = \"\"\"\\\n\tThe function must implement the 'length' and 'length' attribute. The\n\tdefault value is 2MB.\n\t\"\"\"\n\t\tpass\n\ndef cache(fn, timeout=None):\n\t\"\"\"\n\tDecorator to cache results based on a most commonly used algorithm.\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef wrapper(f):\n\t\t\treturn type(f).__cache__(maxsize), f\n\t\treturn wrapper\n\treturn wrapped\n\nclass Times(object):\n\tdef __init__(self, name=\"Times\"):\n\t\tself.name = name\n\t\tself.timer = None\n\n\tdef elapsed(self):\n\t\tif (self.timer is not None):\n\t\t\tself.timer.elapsed()\n\t\t\treturn getattr(self.timer, \"elapsed\", 0)\n\t\telse:\n\t\t\treturn 0\n\nclass Times_Cache(object):\n\tdef __init__(self, name=\""}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tif typed:\n\t\t\treturn lambda fn: fn\n\t\telse:\n\t\t\treturn lambda fn: mru(fn)\n\treturn memoize(func)\n#\ndef get_key(key):\n\timport threading\n\treturn threading.current_thread().get_debugkey(key)\n\t\ndef memoize(func):\n\t# Note: this wrapper can be nested in a try-except block\n\t\"\"\"\n\tDecorator to cache a function that is in charge of creating\n\ta new object, then retrieving it from a cache.\n\t\"\"\"\n\t\n\tdef wrapper(*args,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\ndef mru_lru(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a LRU cache.\n\t\"\"\"\n\t\ndef mru_lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a LRU cache that keeps\n\tup to `maxsize` results based on a Free-Space.\n\t\"\"\"\n\t\ndef mru_lru_cache_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a LRU cache with mru's\n\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef wrapper(funct, fargs, doc):\n\t\t\tmaxsize = MAX_MRU_SIZE\n\t\t\twhile maxsize > 0:\n\t\t\t\tvalue = ffunct(*functargs, name=funct.__name__)\n\t\t\t\tif typed:\n\t\t\t\t\tdoc[name] = _typed_wrapped(value)\n\t\t\t\t\tdoc.update(value)\n\t\t\t\t\treturn\n\n\t\t\t# NOTE: The mru module provides a method signature.\n\t\t\t# We need to pass a callable function that takes a single\n\t\t\t# argument, the same as the first (to save and restore)\n\t\t\t# value, but it looks up the"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef ffunc(function):\n\t\t\t@wraps(function)\n\t\t\tdef g(*args, **kwargs):\n\t\t\t\tresult = function(args, kwargs)\n\t\t\t\tif typed and isinstance(result, tuple):\n\t\t\t\t\treturn tuple(result)\n\t\t\t\telse:\n\t\t\t\t\treturn result\n\t\t\treturn g\n\t\treturn mru_cache(maxsize=maxsize, typed=typed)\n\ndef mru_cached(*args, **kwargs):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (M"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\twrapper_func = _memoize_func(maxsize=maxsize, typed=typed)\n\t\twrapper = FunctionWrapper(wrapper_func, '__call__')\n\t\treturn wrapper\n\n\n###########\n# FUNCTIONS #\n###########\n\ndef _cached_call_count(function, *args, **kw):\n\t\"\"\"\n\tDecorator that takes advantage of the C API\n\t:meth:`~.function_cache` has the same signature as this::\n\n\n\n\t\tcache.call_count()\n\n\t:param function: Function to call\n\t:type function: a function\n\t:param args:"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tself.cache = {}\n\t\tself.maxsize = maxsize\n\t\tself._cache()\n\tif typed:\n\t\tcache_type = typed\n\telse:\n\t\tcache_type = lru\n\n\tdef cache_clear():\n\t\t__builtins__.clear()\n\t\tcache.clear()\n\t\t\n\t@lru_cache(maxsize=maxsize)\n\tdef lru_cache(*args):\n\t\t__builtins__.update(args)\n\t\t__builtins__.clear()\n\t\treturn __builtins__['_']\n\treturn _lru_cache\n\treturn lru_cache\n\ndef lru_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef decorating_function(*args, **kwargs):\n\t\t\tif typed:\n\t\t\t\treturn typed(cache(maxsize, args, kwargs))\n\t\t\treturn functools.wraps(fun(*args, **kwargs))\n\t\treturn decorating_function\n\telse:\n\t\tdef decorating_function(fun):\n\t\t\tdef no_memoization():\n\t\t\t\tif maxsize > 1:\n\t\t\t\t\tprint(\"No more memory to cache from: {}\\n\".format(fun.__name__))\n\t\t\t\treturn fun\n\t\t\tcache = {}\n\t\t\tfor i in range(maxsize):\n\t\t\t\tcache[i] = lambda *args"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\treturn LRU(maxsize)(wrapped)\n\ndef get_random_sequence(size=12, typed=False):\n\t\"\"\"\n\tDecorator to convert an iterable into a new sequence of random integers.\n\t\"\"\"\n\t\treturn random.sample(xrange(size), size)\n\ndef randomize_iterable(iterable, seed_func):\n\t\"\"\"\n\tDecorator to randomize an iterable into several random values.\n\t\"\"\"\n\t\treturn [random.choice(iterable) for i in range(seed_func())]\n\ndef randomize_iterable_random(iterable, seed_func):"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef wrapped_fun(*args):\n\t\t\tif typed:\n\t\t\t\treturn tuple(functools.partial(lru_cache, maxsize=maxsize)(*args))\n\t\t\treturn lru_cache(*args)\n\t\treturn wrapped_fun\n\treturn lru_cache(maxsize=maxsize, typed=typed)\n\n\ndef lru(f):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that returns the\n\tresult of it.\n\t\"\"\"\n\t\tdef wrapped(*args):\n\t\t\tif typed:\n\t\t\t\treturn tuple(functools.partial(lru, f, maxsize="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tcache = Simple_LRU(maxsize=maxsize, typed=typed)\n\telse:\n\t\t# cache is still too big for large inputs\n\t\tcache = cache_lru(maxsize=maxsize)\n\n\tcached = CacheCls(cache)\n\tif not cached:\n\t\tcache.store(func_name, args, callback)\n\treturn cache\n# This file is part of the GNOME Daemon project.\n#\n# Copyright (c) 2015, Anders Loggs.\n# Author: Colin Waltersson and Mattias Damashek, Benjam"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\treturn _memoize_func(wrapped, maxsize, typed)\n\treturn _memoize_func\n\n\ndef memory(maxsize=30000):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing function, returning the\n\tmemorized value after the limit is reached so that each cache entry has\n\ta value that matches the limit of the cache used.\n\t\"\"\"\n\t\treturn _memoize_func(memory, maxsize)\n\treturn _memoize_func\n\n\n@_jit.unbox('long long')\ndef long_mem_cache():\n\t\"\"\"\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef memoize(fn):\n\t\t\tlru = fn(maxsize=maxsize)\n\t\t\tif typed:\n\t\t\t\tlru_results = [lru()]\n\t\t\telse:\n\t\t\t\tlru_results = lru\n\t\t\treturn llru_results\n\t\treturn memoize\n\n\"\"\"Test pylearn-mllib.py: \n\"\"\"\nfrom.test_dataset import dataset\nfrom.pylearn_testing import *\n\n\nclass TestMLlib(unittest.TestCase, setUp):\n\n    data = dataset.data\n    npoints = dataset.npoints\n    y = dataset.y\n\n    def setUp(self"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef cached_lru():\n\t\t\t\"\"\"\n\t\t\t\tGet a function that saves up to `maxsize` items\n\t\t\t\tcached based on `typed`. If the cached value is not\n\t\t\t\tcached, it returns the function unchanged.\n\t\t\t\"\"\"\n\t\t\tif callable(cached_fnc):\n\t\t\t\tcached_fnc = cached_fnc()\n\t\t\t\n\t\t\tif typed: return cached_fnc(maxsize)\n\t\t\telse:\n\t\t\t\treturn cached_fnc()\n\n\t\tcached = cached_lru()\n\t\treturn cached\n\n\nclass _Funcs(object):\n\t\"\"\"\n\tWrap functions with some additional functions"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\tdef fwrapper(fn):\n\t\t\tdef wrapper(*args, **kw):\n\t\t\t\tkey = fn.__name__\n\t\t\t\tif key[:len(fn.__name__)-1] == fn.__name__[len(fn.__name__)-1]:\n\t\t\t\t\tcacheobj = fn.__dict__\n\t\t\t\t\tcacheobj.update({fn.__name__:fn(*args, **kw)})\n\t\t\t\t\tcacheobj._cached_cache = []\n\t\t\t\telse:\n\t\t\t\t\tcacheobj = fn.__dict__\n\t\t\t\t\tcacheobj.update({fn.__name__:fn(*args, **kw)})\n\t\t\t\ttry:\n\t\t\t\t\treturn"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\t# this algorithm has not been used: there is a small\n\t# performance penalty.\n\t\t# http://www.cs.ubc.ca/~cramble/python/\n\t# a little bit of work, but this was the only algorithm behind, I\n\t# have not gone to fix a very poor performance penalty for it.\n\t# The cost of this module is slightly less straightforward than it\n\t# by the `nltk.pipeline.FastVectorizer` class in that it only accepts\n\t# the word count of this function.\n\n\tdef __call__(self, x):"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\treturn _lfuCache.get(maxsize=maxsize, typed=typed)\n\tdef get_hasher(maxsize=128):\n\t\tmaxsize = maxsize * 2**32\n\t\tdef mcache(maxsize=maxsize):\n\t\t\treturn functools.partial(_lfuCache.get, maxsize=maxsize)\n\t\treturn mcache\n\treturn lfu\n\n# ===============================================================================\n# Copyright 2016 Jake Ross\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\tif typed:\n\t\t\tdef cache(f):\n\t\t\t\tfor _ in range(maxsize):\n\t\t\t\t\tif f(f._func):\n\t\t\t\t\t\treturn\t\t# already cached\n\t\treturn cache\n\treturn lfu_cache\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#-----------------------------------------------------------\n# pelisalacarta - XBMC Plugin\n# Canal para construir el canal para programa de canaladas\n# http://blog.tvalacarta.info/plugin-xbmc/pelisalacarta/\n#-----------------------------------------------------------\n#"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t\n\tdef wrapper(value):\n\t\t@functools.wraps(value)\n\t\tdef wrapper_func(*args, **kwds):\n\t\t\tif typed:\n\t\t\t\tvalue = fn(*args, **kwds)\n\t\t\t\n\t\t\telif maxsize < 0:\n\t\t\t\tif isinstance(value, types.GeneratorType):\n\t\t\t\t\treturn value()\n\t\t\t\telse:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult = value()\n\t\t\t\t\texcept TypeError as e:\n\t\t\t\t\t\traise TypeError('Unable to do caching of {}'\n\t\t\t\t\t)\n\t\t\t\t\treturn dict(max = maxsize, result = result)\n\t\t\telse:\n\t\t\t\tresult = fn(*args"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\t# NOTE: The list of values is guaranteed not to change the order in the\n\t\t# parameters of the function, so we use a single function to\n\t\t# get the result right.  This means that some cached implementations\n\t\t# call the result twice to be cached separately.\n\t\tdef cached(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tkey = result._last_key\n\t\t\t\tif maxsize is not None and (key is not None or typed):\n\t\t\t\t\tsize = maxsize\n\t\t\t\t\tif result"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\tcachekey = '__lfucache___' + os.path.basename(fn)\n\t\tif not typed:\n\t\t\tcachekey += '_'\n\t\tfunc = cached(maxsize, typed, cache=lfutimages.find,\n\t\t\tpriority=__caching__)\n\t\tcachekey += '_count_' + fn\n\t\treturn func\n\n\nclass FileCache(object):\n\n\tdef __init__(self, fn, maxsize=128, typed=False):\n\n\t\tself.__fn = fn\n\t\tself.maxsize = maxsize\n\t\tself.__typed = typed\n####################\n# File cache functions\n####################"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\tmaxlen = int(maxlen * 10**3)\n\t\tcached = getattr(fn.__cache__, 'cache', None)\n\t\tif cached is not None:\n\t\t\tif maxsize >= maxsize and not typed:\n\t\t\t\traise RuntimeError(\"cache value larger than maxsize\")\n\t\t\t\treturn cached\n\t\t\telse:\n\t\t\t\tif maxsize < maxsize:\n\t\t\t\t\tyield fn(maxlen)\n\t\t\t\telse:\n\t\t\t\t\tyield fn(maxlen - maxsize)\n\t\telse:\n\t\t\tcached = getattr(fn, '__cache__', None)\n\t\t\tif cached is not None:\n\t\t\t\tif maxsize < maxsize:\n\t\t\t\t\tyield fn(maxsize"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tPerforms a LFU in memory and returns the cached result.\n\t\t\tIf `typed` is set to `True`, the cached result will be\n\t\t\tautomatically decoded using the Python `typedump` Python type.\n\t\t\t\"\"\"\n\t\t\tmem = args[0]\n\t\t\ttry:\n\t\t\t\treturn memoize(mem, typed)\n\t\t\texcept ValueError:\n\t\t\t\tif maxsize > 0:\n\t\t\t\t\tprint(mem, \"cache_size: \" + str(maxsize))\n\t\t\t\t\treturn cached_data + cached"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\treturn lfu(maxsize=maxsize, typed=typed)\n\n\tdef lazy_lru_cache(maxsize=128):\n\t\t\"\"\"\n\t\tDecorator to load the cache if `maxsize` exists. Returns\n\t\t- None if cache file not present\n\t\t- `None` if cache is not available\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcachefile = open(CACHE_FILENAME, 'r')\n\t\texcept IOError:\n\t\t\treturn None\n\t\telse:\n\t\t\tif not has_lfu():\n\t\t\t\t# only cache files\n\t\t\t\treturn None if cachefile.closed else cachefile\n\t\t\treturn cache"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\t@wraps(fn)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif maxsize is None or maxsize < 1:\n\t\t\t\tmaxsize = maxsize\n\t\t\tif typed:\n\t\t\t\tresult = fun(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tresult = fun(*args, memodict=kwargs)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn lfu_cache\n\n\ndef lfu(maxsize=128, typed=False):\n\t\"\"\"Decorator to wrap a function with a cached version of a LFU\n\talgorithm.\n\t\"\"\"\n\t\t@wraps(fn)\n\t\tdef wrapper"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\nclass Paste(object):\n\t\"\"\"A Paste object backed by PasteManager.\n\t\n\tNOTE: Interaction is unusable if a new paste item is found.\n\t\"\"\"\n\tdef __init__(self, data, undo):\n\t\t\"\"\"Insert a new Paste item from a data dictionary\n\t\t\n\t\tparam data: The dictionary being replaced.\n\t\tprevents duplicate data\n\t\t\"\"\"\n\t\tself.data = data\n\t\tself.undo = undo\n\t\tself.items = []\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"Get the value of a given key\n\t\t\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\treturn self._value() and self._last()\n\n# --- Various methods for finding and manipulating strings / characters.\n\ndef to_unicode(text, encoding=DEFAULT_ENCODING):\n\tif isinstance(text, unicode):\n\t\treturn text\n\ttry:\n\t\treturn text.decode(encoding=encoding)\n\texcept TypeError:\n\t\treturn text\n\nclass _String(_StringObject, unicode): pass\n\n\n# --- Various utility functions for manipulating strings, unicodes, and unicode\n\ndef iter_split(text, sep=u\"-\", chars=None):\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tself._tk.call(\"tk_popitem\", self._w, str(self._txt))\n\tdef __delitem__(self, index):\n\tif index<0 : index=None\n\ti=self._index(index)\n\ttry: i=str(i)\n\texcept ValueError: raise TypeError(\"index must be an integer\")\n\tself._tk.call(\"tk_remove\", self._w, i)\n\treturn i\n\tdef __getitem__(self, index):\n\treturn self._tk.call(\"tk_getitem\", self._w, str("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\n#\t[key, value] = self._popitem()\n\t\n\t\n\tif key!= key:\n\t\traise KeyError, '%s: Key not found' % key\n\n\tkey = key[:-1]\n\treturn (key, value)\n\n\t#return self._popitem()\n\n# -*- coding: utf-8 -*-\n\nfrom..base import Base\n\n__all__ = ['User']\n\nclass User(Base):\n    def __repr__(self):\n        return '%s<%s>' % (self.first_name, self.last_name)\n\n    def __str__(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\nclass DnListIter(list):\n\tdef remove(self, item):\n\t\tassert 0 <= item<=len(self)-1\n\t\tself[item].remove(item)\n\t\n\tdef insert(self, item):\n\t\tassert 0 <= item <=len(self)-1\n\t\tn = self[item]\n\t\tself[n] = self[n].remove(item)\n\t\n\t#\n\t# Private functions, for all other parts of this class.\n\t#\n\t# Note that this can be used for list-like items, as long as you\n\t# want to"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\treturn self.data[self.key]\n\n\nclass HoldItem:\n\t\"\"\"\n\tHolds the items on the heap to be inserted.\n\n\tThis should only contain a (key, value) pair. The heap is not\n\tmoved.\n\t\"\"\"\n\tdef __init__(self, key, value):\n\t\tself.key, self.value = key, value\n\n\tdef __getitem__(self, key):\n\t\treturn self.value[self.key]\n\n\tdef append(self, old_item, new_item):\n\t\tif not old_item in self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\treturn (self.curPos, self.idx)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n### Internal Code ###\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nclass Pair:\n\t\"\"\"\n\tPure python part for representing a pair of values.\n\t\n\t:param vPair: (str) 2-tuple of (value, key) for key of the pair.\n\t:type vPair: (str) 2-tuple\n\t:param seq: sequence\n\n\t:returns: (bool, dict) True or False in turn.\n\t\n\t\"\"\"\n\n\tdef __init__(self, vPair, seq):"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\n#        if self.itemList[-1][1] == key:\n\t\treturn self.itemList[-1][1]\n\t\t\n    \n\tdef insert(self, key, value, **options):\n\t\t\"\"\"\n\t\tPotatoloes the key `key`.\n\t\t:param key: The key to be popped.\n\t\t:param value: The value to be appended.\n\t\t:param options: Additional options from `options` attribute of the item.\n\t\t\n\t\t:rtype: tuple of elements\n\t\t\"\"\"\n\t\t\n\t\tif not isinstance(value, basestring):\n\t\t\tvalue = value."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\n#\t\tkey = getKey(self.currKey)\n#\t\n#\t\tif key in self.currentItem:\n#\t\t\tdel self.currentItem[key]\n#\t\t\tkey = getKey(self.currKey)\n\t\n#\t\t\t\tif key in self.currentItem:\n#\t\t\t\t\treturn self.currentItem[key]\n#\t\t\traise KeyError, 'Key '+key+' not found.'\n#\t\n#\t\tkey = getKey(self.currKey)\n#\t\tif key in self.currentItem:\n#\t\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t\tself.stack.append((\"popitem\", None))\n\t\told, cur, val = self[-1]\n\t\tif cur[0] is val: # value isn't found in stack\n\t\t\tdel self[-1] # store stack\n\t\telse:\n\t\t\told[0] = val\n\t\t\told_stack.append(value)\n\nclass Template(tuple):\n\t\"\"\"\n\tClass for representing an individual template to provide\n\ttemplates to the caller.\n\t\"\"\"\n\t\n\t__slots__ = ()\n\t\n\tdef __init__(self):\n\t\tself.templates = []\n\t\tsuper"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif default is D.get(key, None):\n\t\t\tD[key] = default\n\t\treturn D[key]\n\ndef exists(d, key):\n\td.setdefault(key, None)\n\treturn 1\n\n##\n# Utils and utilities\n##\n\nclass _Assert:\n\t'''\n\tUtility class\n\t'''\n\t__metaclass__ = ABCMeta\n\n\tdef __init__(self, expected):\n\t\tself.expected = expected\n\n\t@abc.abstractmethod\n\tdef raise_if_not_implemented(self):\n\t\t'''\n\t\tRaises a NotImplementedError if"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in D:\n\t\t\tD[key] = default\n\t\treturn D[key]\n\n\tdef __setitem__(self, key, value):\n\t\tif not isinstance(key, basestring):\n\t\t\traise Exception('Key'+ key +'is not a string')\n\t\tif key not in self:\n\t\t\tself[key] = []\n\t\t\t\n\t\tself[key].append(value)\n\n\tdef pop(self, key, *args):\n\t\treturn self[key][-1] if key in self else None\n\t\n\tdef popitem(self):\n\t\titem = self["}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in D and default is None:\n\t\t\tD[key] = default\n\t\treturn D[key]\n\tdef __setitem__(self, key, value):\n\tD[key] = value\n\treturn D\n\n\ndef find_matching_keys(dict):\n\t\"\"\"\n\tFind matching keys from dictionary.\n\t\"\"\"\n\tif not len(dict):\n\t\treturn None\n\tcur_key = None\n\tfor key in dict:\n\t\tif key!= cur_key:\n\t\t\tcur_key = key\n\t\t\tfound = False\t\t# skip if the"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in self:\n\t\t\td = default\n\t\t\tself[key] = d\n\t\treturn self[key]\n\ndef insort(self):\n\tif self._iter is not itself.__iter__():\n\t\tself._iter.sort()\n\telse:\n\t\tfor elem in self:\n\t\t\tinsort(elem)\n\ndef find(self, x, y=None, x0=0):\n\t\"\"\"Find the lowest value in the list\n\tx0 -- first value in list 1\n\ty -- y = highest value in list\n\tx0 -- return"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key in self:\n\t\t\tself[key].update(default or {})\n\t\treturn self[key]\n\n\tdef __repr__(self, *args, **kwargs):\n\t\tkw = {}\n\t\tfor k,v in self.iteritems():\n\t\t\tif inspect.ismethod(v):\n\t\t\t\tkw[k] = v.__repr__(self, *args, **kwargs)\n\t\t\telse:\n\t\t\t\tkw[k] = repr(v)\n\t\treturn \"%s(%r)\" % (self.__class__.__name__, ( dict( kw ), self.iteritems(), args, kwargs ))"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif default is self.default:\n\t\t\treturn D[key]\n\t\telif D.has_key(self.name):\n\t\t\treturn D[self.name]\n\t\telse:\n\t\t\treturn default\n\tdef __getattr__(self, k):\n\t\ttry:\n\t\t\treturn D[self.name]\n\t\texcept KeyError:\n\t\t\tif k in D:\n\t\t\t\treturn D[k]\n\t\t\telse:\n\t\t\t\traise AttributeError, k\n\n\tdef __setattr__(self, k, value):\n\t\ttry:\n\t\t\tD[self.name]=(value,self.__getattribute__("}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\tdef __contains__(self, key):\n\t\treturn key in self\n\n\tdef __iter__(self):\n\t\tfor key in self:\n\t\t\tif not key in self:\n\t\t\t\tyield key\n\n\tdef find(self, key, default=None):\n\t\t\"\"\"\n\t\tD.find(key) -> list of D.get(key), which is a list of D.getkeys() that match a given condition\n\n\t\tParameters\n\t\t----------\n\t\tkey : string\n\t\t\tD."}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = default\n\t\treturn self[key]\n\n\tdef __set__(self, obj, v):\n\t\tif obj == None:\n\t\t\tself[key] = v\n\t\telse:\n\t\t\tif v is not None:\n\t\t\t\traise KeyError(\"Attempt to create undefined key: %s:%s\" % (obj, v))\n\n\nclass DefaultList(list):\n\t\"\"\"\n\tA default list which is the same as a list but it is more readable\n\tas a flat list.\n\t\"\"\"\n\tdef __setitem__(self"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\td = self.setdefault(key, default)\n\t\tif d is not None:\n\t\t\treturn d\n\t\tD.setdefault(self, k, d)\n\t\treturn d\n\n\tdef __contains__(self, key):\n\t\t# This is a shortcut for checking whether self.has_key(key)\n\t\treturn key in self or key in self.__dict__\n\n\tdef __repr__(self):\n\t\treturn \"DictType %s%s\" % (self.__class__.__name__, \", \"join([repr(x) for x in self]))\n\nclass DefaultDict(DictType"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\tif key not in D:\n\t\t\tD[key] = default\n\t\treturn D[key]\n\n\ndef deprecated(function):\n\t\"\"\"\n\tDecorate a function to be used with the pytest_addoption_callback function.\n\n\tUsage:\n\t\tdeprecated(\"hello\", \"warn\")\n\tA test case function has been deprecated. All tests will be\n\trun with the :py:func:`_pytest.runner.Deprecated` class.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tfor arg in args:\n\t\t\tif arg.id:\n\t\t\t\treturn func"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tD[key] = self.Default\n\t\treturn default\n\t\n\tdef pop(self, k, default=None):\n\t\t\"\"\"\n\tD.pop(k[,d]) -> D.pop(k,d) if k not in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tD[key] = self.Default\n\t\treturn default\n\t\n\tdef setdefault(self, k, d=None):\n\t\tif k not in D"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\td = self.get(key, default)\n\t\tif type(d) is dict and key in type(d):\n\t\t\treturn d[key]\n\t\treturn default\n\tdef copy(self):\n\t\t\"\"\"\n\t\td.copy() -> D(d)\n\t\td.update(dict, {k:v}) -> copy of D\n\t\t\"\"\"\n\t\tdict = self.__dict__.copy()\n\t\tdict.update(self)\n\t\treturn dict\n\tdef __setitem__(self, key, value):\n\t\tself.__dict__[key]=value\n\tdef __getitem__("}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._dict[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\nclass ConfigDict(UserDict):\n\t\"\"\"\n\tA dictionary that has been initialized as a dict-like object.\n\t\"\"\"\n\tdef __init__(self, defaults=None):\n\t\tsuper().__init__(defaults)\n\t\tself._lock = Lock()\n\t\tself.reset()\n\n\t@classmethod\n\tdef add(cls, name, value):\n\t\td = cls()\n\t\td.update(dict(config.items(name, default=['{}'])[1]))\n\t\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif type(key) == type(\"\"):\n\t\t\tif key in self.value:\n\t\t\t\treturn self.value[key]\n\n\t\t\telse:\n\t\t\t\treturn default\n\t\treturn self.value.get(key,default)\n\t\t#print self.value\n\n\tdef update(self,dic):\n\t\tdic.update(self.value)\n\n\n# \tkey1 = 0\n# \tkey2 = 2\n# \tkey3 = 3\n# \tkey4 = 4\n# \tkey5 = 5\n# \tkey6 = 6\n# \ufffd"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\treturn self[key] if k in self and isinstance(self[key], dict) else default\n\n\tdef getBoolean(self, key):\n\t\t\"\"\"\n\t\tReturn the value of a keyed string or None.\n\t\tKeys can contain either True or False for values.\n\t\t\"\"\"\n\t\treturn self[\"bool\"] if key in self and isinstance(self[key], bool) else self[key]\n\n\tdef getArray(self, key):\n\t\t\"\"\"\n\t\tReturn the values of a key's elements as a nested sequence of\n\t\t(key, val) tuples. When `key` is"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\t# d.get(k, d)... d has no default value, unless k is an integer, or must be a string or any callable python object, or a callable instance with a \"default\"\n\tif not hasattr(d, 'get') or hasattr(d, 'get_default'):\n\t\td.get_default = lambda : d\n\t\td.get = lambda : D()[default]\n\treturn d.get(key, default)\n\n\nclass Settings(UserDict.UserDict):\n\tdef __get__(self, prop, klass=None):\n\t\tif klass:\n\t\t\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key].get()\n\t\texcept KeyError:\n\t\t\treturn default\n\nclass DictProperty(object):\n\t\"\"\"\n\tA JSON property that points to a specific object.\n\tKeys in this property will be prefixed with 'o#'.\n\t\"\"\"\n\tdef __init__(self, key, dict_type):\n\t\tself.key = key\n\t\tself._dict_type = dict_type\n\t\tself.__doc__ = 'A dict for a property of type \"%s\".\\n' % dict_type.capitalize()\n\tdef __get__(self"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef set(self, key, value):\n\t\tself[key] = value\n\n##\n# Helper functions\n##\ndef print_tree(tree, indent=2, width=250, indentsize=40, nodesep=''):  # pragma: no cover\n\t\"\"\"\n\tPrint a tree of the form d1 <-> d0.  d is the corresponding D node.\n\t\"\"\"\n\tindent_table = [\"  \" * indent,\n\t\t  \" \"* (indentsize +"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\tval = self[key]\n\t\texcept KeyError:\n\t\t\treturn getattr(self, key)\n\t\treturn val\n\n# d.d.d.d.d.\nclass DictMixin(dict):\n\tdef __getattr__(self, key):\n#\tprint \"d.d.d.%s\" % key\n\t\tif key in self and len(self) == 1:\n\t\t\traise KeyError(key)\n\t\tself[key] = val = self.pop(key, default)\n\t\treturn val\n\nclass Config(BaseConfig):\n\t@property"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tif default is None:\n\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\treturn D[default]\n\n\tdef __contains__(self, k):\n\t\td = self.get(k, None)\n\t\tif d is None:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn d in self\n\n\tdef getall(self, k):\n\t\treturn self.get(k.lower(), d=None) or d\n\n\tdef pop(self, key, default=None):\n\t\td = self.get("}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tif lock:\n\t\t\tcache[key] = (cache[key], lock.release)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif timeout is not None:\n\t\t\t\ttimeout[0] = timeout[1]\n\t\t\ttry:\n\t\t\t\tvalue = cache[key](*args, **kwargs)\n\t\t\texcept AttributeError:\n\t\t\t\tvalue = cache[key] = func(*args, **kwargs)\n\t\t\t\treturn wrap(value)\n\t\t\treturn value\n\t\tmemoizedfunc = functools.partial(wrapper, cachedfunc)\n\t\tcache[key] = cachedfunc\n\t\tmemoizedfunc.__name"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tcache = cache or {}\n\t\t\n\t\tdef cachedmethod_cache(func):\n\t\t\t# Don't cache the key if the method is not a function,\n\t\t\t# because it's used internally by other implementations\n\t\t\tif hasattr(func, \"im_func\"):\n\t\t\t\tcache[func.im_func] = func.__func__.im_func\n\t\t\treturn func.im_func\n\t\tcache[type(cache[key])] = cachedmethod_cache\n\t\tcache.__wrapped__ = True\n\t\treturn cache[key]\n\t\tcache.key_func = key\n\t\tcache.__wrapped__.cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t    \n\tdef _cache_get(self, key, value, expires=None):\n\t    \n\t\tdef _memoized_get(self):\n\t\t\treturn self()\n\t\tif expires == None:\n\t\t\texpires = time.time()\n\t\treturn _memoized_get(self, key, self.wrapped(value), expires=expires)\n\t\n\t# do the cache lookup\n\t_cache = {}\n\t_cache_key = key\n\t\n\t_cache_set = set() # for tracking the cache value\n\t_cache_expire = {}\n\t_cache_delete = []\n\t_"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = threading.RLock()\n\t\tcache_key = hashkey\n\treturn cache_method\n\n\nclass Cache(dict):\n\t\"\"\"\n\tClass to cache functions.\n\tUsed to create a function object and cache a reference to an object.\n\tThe data is cached in a local memo, and saved with the function\n\tget, set methods, and delete functions for it.\n\t\"\"\"\n\n\tdef __init__(self, d, _method):\n\n\t\tself._d = d\n\t\tself._cache = dict()\n\t\tself._method = _"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tkey = key(cache)\n\t\t\tvalue = cache_get(key, func)\n\t\t\tif value is None:\n\t\t\t\tcache[key] = func\n\t\t\t\treturn func(cache)\n\t\t\treturn value\n\t\treturn wrapper\n\treturn cache_set(func, key, decorator)\n\ndef cachedproperty(getter, cache, lock=None):\n\t\"\"\"\n\tDecorator to wrap a class or instance method with a weak\n\tmemoized cache of the method's return value.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tcache[getter] = func"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tdef inner():\n\t\t\ttry:\n\t\t\t\treturn method(cache, key)\n\t\t\texcept (TypeError, TypeError, AttributeError) as ex:\n\t\t\t\tif cache.has_key(key):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcache[key].apply_async_cache_update(inner, arg)\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\t# FIXME I can't just ignore exceptions of the decorated function from cache\n\t\t\t\t\t\tpass\n\t\treturn inner\n\t\n\treturn cachedmethod\n\nclass CacheDecorator(object):\n\t\n\tdef __init__(self, cache, key):\n\t\tself.cache = cache\n\t\tself.key"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tif callable(key):\n\t\t\tcache = getattr(cache, key)\n\t\tif callable(lock):\n\t\t\treturn cache.cached_property(key, lock)\n\t\telif lock:\n\t\t\treturn cache.method(key, lambda *args: cache.get(key), lock)\n\t\telse:\n\t\t\treturn cache.method(key, lambda *args: cache.get(key), lock)\n\n\treturn cachedmethod\n# -*- coding: utf-8 -*-\n\"\"\"\n/***************************************************************************\n Name                 : DB Manager\n Description          : Database manager plugin for QGIS\n date                 : May 23"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tdef cached(self, *args, **kwargs):\n\t\t\tif args and not kwargs:\n\t\t\t\tdel kwargs['cache-sig']\n\t\t\t\targs = ()\n\t\t\tif args and not kwargs:\n\t\t\t\tdel kwargs['cache-args']\n\t\t\tif key is not self:\n\t\t\t\tif key in self._cache:\n\t\t\t\t\treturn self._cache[key]\n\t\t\t\telse:\n\t\t\t\t\tcache[key] = self()\n\t\t\t\t# XXX add exception handling when cache is rebuilt and cached\n\treturn cached\n\n#\n# Cache implementation.\n#\n\n\ndef cached_property(cls):\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tdef decorated(func):\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\treturn wrapper\n\t\treturn decorator(func)\n\treturn cachedmethod\n\t\ndef getresponse(responsekey):\n\t\"\"\"\n\tReturn a :class:`response <flask_restful.response.Response>` object for\n\ta given response key.\n\n\t:param responsekey: A string (or :py:class:`keylib.Werkzeug.response.responsekey.Werkzeug.responsekey.Werkzeug.responsekey`)\n\t:type responsekey"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\tif not callable(func):\n\t\t\tcache._cache = {}\n\t\t\tcache.debug = 0\n\t\t\treturn func\n\t\telse:\n\t\t\ttry:\n\t\t\t\td = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tn = d.__doc__ or \"<no code for {} from {}>\"\n\t\t\t\tn = n if n!= \"\" else \"<no docstring for {}>\"\n\t\t\t\td = n + \"<no code for {}>\" + d.__dict__\n\t\t\t\tcache[key] = d\n\t\treturn d\n\n\ndef memoize(func, args):\n\t\"\"\""}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\ndef format(e, level=0):\n\t\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param level: Maximum call stack level (default 0)\n\t:type level: int, default 0\n\t:return The exception string, exceeds the maximum level of the error and the message\n\t:rtype str\n\t\"\"\"\n\t\ndef format_traceback(tb):\n\t\"\"\"\n\tGenerate a formatted traceback, based on the traceback, above it's\n\ta traceback formatting function.\n\t:param tb:"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n\n\terror_type_map = {\n\t\t'exceptions': ('exc_str', 'exc_repr', 'exc_dict', 'exc_traceback'),\n\t\t'warnings': ('wc_exc_str_str', 'wc_exc_repr', 'wc_exc_dict', 'wc_exc_traceback'),\n\t\t}\n\n\tdef __init__(self, message='', extra_tb='', stack_level=0, verbose=True):\n\t\tself.message = message\n\t\tself.extra_tb = extra_tb\n\t\tself.stack_level = stack_level\n\t\tself"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\tif isinstance(e, Exception):\n\t\t\treturn \"ERROR\", str(e)\n\t\tif len(e.args) > len(e.kwargs):\n\t\t\treturn \"ERROR\", \"EXceeds to {} args: {}, kwargs: {}\".format(\n\t\t\t\tlen(e.args), len(e.kwargs),\n\t\t\t\ttype(e.args).__name__, \", \".join(str(e.args)[:max_level] + e.kwargs)\n\t\t\t)\n\t\tif e.args and isinstance(e.args[-1], KeyError):\n\t\t\treturn \"EXCEED\", \"Invalid key"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\n    def exc_name(e):\n\t    e_name_stack = []\n\t    for v in e.value.args:\n\t    obj = v.get_obj()\n\t    if obj is not None and obj.__class__.__name__ == e:\n\t\tobj_name = e.value.__class__.__name__\n\t    else:\n\t\tobj_name = e.value.__class__.__name__\n\t    e_name_stack.append(obj_name)\n\t    return e_name_stack\n\t\n\tif e.exc_type:\n\te_cls = str(e.exc_type) + \" \""}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\tif max_level < 30:\n\t\t\treturn '*** {} level was not enough ***'.format(max_level)\n\t\telif max_level < 5:\n\t\t\treturn '{} {} level was greater than 5 * for \"{}\";'.format(max_level, max_path_level, e.__class__.__name__, e)\n\t\telif max_level < 3:\n\t\t\treturn '{} {} level was greater than 3 * for \"{}\";'.format(max_level, max_path_level, e.__class__.__name__, e)\n\t\telse:\n\t\t\treturn '{} level was {}.'.format("}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\tname = type(type(e)).__name__\n\t\tif name.startswith(\"Exception\") and issubclass(e, type(None)) and issubclass(e, BaseException):\n\t\t\treturn e.exc_info()[2]\n\t\telse:\n\t\t\treturn \"%s(%s)\" % (e.__class__.__name__, e)\n\ndef excp(e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\treturn '%s (%r)' % (e.value.__class__.__name__, str(e))\n\ndef print_traceback(etype, exc_type, exc_value, tb):\n\t\"\"\"\n\tFormats an exception.\n\t:param etype: Exception object.\n\t:type etype: Exception\n\t:param exc_type: Exception instance.\n\t:type exc_type: Exception\n\t:param exc_value: Exception instance.\n\t:type exc_value: Exception\n\t:param tb: Formatted text.\n\t:type tb: str\n\t\"\"\"\n\t\tframes"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\ttry:\n\t\t\treturn 'Exception: %s' % e.args\n\t\texcept AttributeError:\n\t\t\tpass # exception not supported/defined in python 2.7 for older python versions\n\n\ndef exception_formatter(e):\n\t\"\"\"\n\tFormat an exception\n\t:param e: A Python exception instance.\n\t:type e: Exception\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\n\t\tif isinstance(e, Exception):\n\t\t\treturn 'Exception: %s' % e.args[0]\n\t\telse:\n\t\t\treturn 'Exception: %s'"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t # Check for correct format. Otherwise raise a warning.\n\t msg = 'The following code seems like there is no sufficient format in the output:\\n'\n\t n_level, n_level_str = max_level, max_level_str\n\t msg += 'There are {} levels with the maximum output level: {}.'.format(n_level)\n\t msg += 'The maximum output level of the traceback is higher than the allowed output level.\\n'\n\t msg += 'The message was:\\n'\n\t msg += n_level_str + '\\n'\n\t msg += 'Maximum level: {}.'.format(max"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\tif not isinstance(e, Exception):\n\t\t\traise TypeError(\"Argument not an Exception: \" + str(e))\n\n\t\tmsg = \"%d levels of %d passed, to file %s.\" % (\n\t\t\tmax_level, max_level <= 3, e.filename)\n\t\twritemsg(\"-\"*70+\">>> %s\" % msg, Color.YELLOW)\n\n\t\tret = \"\"\n\n\t\tif isinstance(e, Exception):\n\t\t\tif max_level > 1:\n\t\t\t\tret = colorize(\"red\", \"ERROR: \" + str(e))\n\n\t\t\t\tif e.filename."}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n# Get file name\nfile_name = os.path.basename(file_name)\n\n\n# Write the text in the given file\ntext = ''\n\n# For each line of the file\nfor line in raw_file:\n\t# Get text\n\tline_match = re.search(r'\\s+?', line)\n\tif line!= '':\n\t\ttext += line.strip()\n\t\t# Write the text\n\t\tif text_buffer and overwrite and not os.path.isfile(file_name):\n\t\t\ttext_buffer = text_buffer + ('\\n')\n\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\n\treturn check_text_buffer(text_buffer, encoding, overwrite_file_name = overwrite)\n\t\n\ndef write_text_to_file_or_folder_or_folder(text_buffer='', write_to_folders=False, overwrite_directories=True):\n\t\"\"\"\n\tWrite to text_buffer or folder name or folder in given\n\t:param text_buffer: The text output buffer.\n\t:type text_buffer: str\n\t:param write_to_folders: If True, directory write will be added to all folders if it exists\n\t:type write"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\tprint('Writing to {}'.format(file_name))\n\t\tf = open(file_name, 'w')\n\t\twriter = csv.writer(f, **FORMAT_FMT)\n\t\t# Header\n\t\twriter.writerow([])\n\t\t# Body\n\t\twriter.writerow([TEXT_BEGIN, TEXT_END, BLANKLINES_COUNT, str(len(text_buffer)), TEXT_END, BLANKLINES_START])\n\t\tfor i in range(len(text_buffer)):\n\t\t\twriter.writerow([TEXT_BEGIN, TITLE_END, \"\", TEXT_END, B"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\ttry:\n\t\tif file_name.startswith('R') and create_dir(file_name):\n\t\t\tfile_out = open(file_name, \"wb\")\n\t\tfile_out.write(text_buffer)\n\t\tfile_out.close()\n\t\tif overwrite:\n\t\t\ttry:\n\t\t\t\tos.remove(file_name)\n\t\t\texcept:\n\t\t\t\tpass\n\t\treturn 1\n\t\t\t\n\texcept:\n\t\tprint('create_dir() function fail!')\n\t\treturn -1\n\t\ndef get_text_to_file(file_name):\n\t#"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\ttry:\n\t\t\tos.chmod(file_name, mode=0644)\n\t\texcept OSError:\n\t\t\tif overwrite:\n\t\t\t\traise\n\t\tif overwrite:\n\t\t\tnew_text = ''\n\t\telse:\n\t\t\tnew_text = text_buffer\n\t\tfile_name = os.path.splitext(file_name)[0] + os.extsep + str(os.getpid()).strip()\n\t\twith open(file_name, 'w') as output_file:\n\t\t\toutput_file.write(new_text)\n\n# Create a queue to be send.\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\tif os.path.exists(file_name):\n\t\t\treturn os.system(\"cat %s | grep %s\" % (file_name, str(text_buffer)))\n\t\telse:\n\t\t\treturn os.system(\"grep %s | grep -v %s\" % (str(text_buffer), str(text_buffer)))\n\n#\n# Define the default directory that stores the text file\n#\ndef initialize_default_dir():\n\tglobal _default_dir\n\tglobal _default_file_name\n\tglobal _default_file_path\n\t_default_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\tfile_path = os.path.join(get_cache_dir(), file_name)\n\t\tfile_path_full = get_path_full_at_absolute_path(file_path)\n\t\tfile_path_with_suffix = file_path_full[len(os.path.sep):].replace('\\\\', '')\n\t\tif not overwrite or os.path.getsize(file_path_with_suffix) < getsize(file_path_full):\n\t\t\tif file_name.startswith(os.path.sep):\n\t\t\t\ttext_buffer = codecs.open("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(file_name):\n\t\treturn self.remove_text_from_file(file_name)\n\t\n\tf = open(file_name, \"w\")\n\tf.write(text_buffer)\n\tf.close()\n\t\n\tfile_info = os.stat(file_name)\n\t\n\tif file_info.st_size == 0:\n\t\treturn -1\n\t\n\tfile_size = os.stat(file_name)[stat.ST_SIZE]\n\t\n\tif file_info.st_mode not in (stat.S"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\t# Get the filename first name, then filename.\n\t\tbase_name = file_name[:basename()]\n\t\tfinal_name = file_name + _base_name(base_name) + '.txt' \n\t\t# Check if the final file name exists, and truncate with.txt.\n\t\tif os.path.exists(final_name):\n\t\t\ttext_buffer = text_buffer[:-len(' ',)]\n\t\ttext_buffer = codecs.open(final_name, 'w+', encoding=encoding)\n\t\ttext_buffer.write(text_buffer)\n\t\tresult = 0"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t\n#\twith open(file_name, 'wb') as file_out:\n#\t\t\tfile_out.write(text_buffer)\n#\t\t\tfile_out.flush()\n#\t\tfile_out.close()\n\n\ndef _strip_whitespace_lines(line):\n\t\"\"\"\n\tStrip whitespace from a python module return value from module.\n\t:param line: The line in between string.\n\t:type line: str\n\t:return: The original line.\n\t:rtype: str\n\t\"\"\"\n\n#\t_strip_whitespace_lines(_"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\tif file_name.startswith(\".\"):\n\t\t\treturn file_name\n\t\telse:\n\t\t\twith open(file_name, \"rb\") as f:\n\t\t\t\treturn f.read(encoding)\n\"\"\"\nA file containing the interface of the DSL-format.\n\n@author Alan M. Engemann\n@date 2012.09.2005\n\nA simple DSL-specific interface\n\n@author Tomasz Lopes\n@email wolf@gmail.com\n\n\"\"\"\nimport os\n\nimport wx\nimport wx.grid\nfrom wxgl.canvas import"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\twith file_lock:\n\t\tfile_buffer = None\n\t\t\n\t\ttry:\n\t\t\twith file_open(file_name, 'rb+') as f:\n\t\t\t\tstream = f.read()\n\t\t\n\t\texcept Exception:\n\t\t\tpass\n\n\t\tif not file_buffer:\n\t\t\tfile_buffer = stream\n\t\t\n\t\treturn file_buffer\n\n\t##############################################################################\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\n# Produced at the Lawrence Livermore National Laboratory.\n#\n# This file is part of Spack.\n# Created by Todd"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\ttry:\n\t\t\treturn file(file_name, 'r', encoding)\n\t\texcept IOError:\n\t\t\treturn None\n\ndef textbuffer_to_file(textbuffer, file_name=None, encoding=None):\n\t\"\"\"\n\tLoad a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param textbuffer: Text buffer\n\t:type textbuffer: str\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\tfile_buffer = \"\"\n\tfor line in open(file_name):\n\t\tfile_buffer += line.decode(encoding)\n\t\n\treturn file_buffer;\n\ndef is_string(string):\n\t\"\"\"\n\tCheck if string is a non-string string.\n\n\t:param string: the string to check\n\t:type string: str\n\t:return: True if string is a non-string string\n\t:rtype: bool\n\t\"\"\"\n\tnew_string = string.decode('utf-8')\n\tif not new_string:\n\t\treturn False\n\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\n\n\tbuffer = \"\"\n\t\n\n\ttry:\n\t\twith open(file_name, encoding=encoding) as f:\n\t\t\tbuffer = f.read().decode(encoding)\n\n\texcept:\n\t\tprint(\"Error opening file {}/lines...\".format(file_name))\n\t\treturn\n\t\n\tfinally:\n\t\ttry:\n\t\t\tf.close()\n\t\t\twith open(file_name, encoding=encoding) as f:\n\t\t\t\tbuffer = f.read().decode(encoding)\n\n\t\t\tif len(buffer) > 0:\n\t\t\t\treturn buffer\n\t\texcept:\n\t\t\tprint(\"Error"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n# Load the contents of a file if it's a file object\ndef file_as_textbuffer(file_name):\n\tif isinstance(file_name, basestring):\n\t\tfile_name = file_name.encode(encoding)\n\tif isinstance(file_name, unicode):\n\t\tfile_name = file_name.decode(encoding)\n\treturn file_name\n\n# Get the filename of a file toward it. If the path is an absolute path\n# an IOError exception is raised. If the file is read, this method will return None.\ndef file_to"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\ttry:\n\t\t\tf = open(file_name, encoding)\n\t\texcept (IOError, OSError):\n\t\t\tlogging.error(\"file_to_textbuffer: ERROR: Unable to open file '%s' due to errors: %s\", file_name, traceback.format_exc())\n\t\t\treturn None   # Should never happen\n\t\texcept:\n\t\t\tlogging.exception(\"file_to_textbuffer: ERROR: Unable to open text buffer.\")\n\t\t\treturn None   # Should never happen\n\n\t\t# Now read all remaining text and save it in a text buffer\n\t\tnext_text = \"\"\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\ttry:\n\t\treturn file.read(encoding)\n\texcept IOError, e:\n\t\t# If it doesn't exist, bail with a descriptive text\n\t\tif e.errno!= errno.ENOENT:\n\t\t\traise\n\n\ndef get_user_config(username):\n\t\"\"\"\n\tReturn USER configuration using the supplied preferences. \n\t\"\"\"\t\t\n\t\n\tif username is None:\n\t\treturn\n\telse:\n\t\treturn _get_current_user_config(username)\n\ndef set_user_config(username, value):\n\t\"\"\"\n\tSet USER configuration using a value"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\t\n\twith codecs.open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n\ndef write_to_mem(file_name, content, encoding=None):\n\t\"\"\"\n\tWrite a text buffer (UTF-8), using the specify encoding to write out the text.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param content: Content to write.\n\t:type content: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\tif not file_name:\n\t\t\treturn None\n\n\t\tif encoding not in (\"utf-8\", \"utf-16\"):\n\t\t\treturn None\n\t\twith open(file_name, encoding=encoding) as fd:\n\t\t\treturn fd.read()\n\ndef get_file_extension(file_name):\n\t\"\"\"\n\tGet file extension from file_name.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Optional extension identifier.\n\t:rtype: str\n\t\"\"\"\n\t\tif file_name and \\\n\t\t\t\t\t\t"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\treturn os.path.isfile(file_name)\n\n\ndef create_file_with_data(file_name, data):\n\t\"\"\"\n\tCreate a temporary file with given data to the given file name\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param data: Data to write to file.\n\t:type data: str\n\t:return: Return true (the operation succeeded) and error message on success:\n\t\t\tsuccess - If the file already exists, then data will be updated\n\t\t\terror - If the directory does not exist"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\ndef find_files(dir_path):\n    \"\"\"\n    Find root/root/sub/sub/... and all file subdirectories.\n\t:return: List of files that are files.\n\t:rtype list[str]\n\t\"\"\"\n\tfiles = []\n\tsub_dirs = os.listdir(dir_path)\n\tfor sub_dir in sub_dirs:\n\t    (root, files) = os.walk(dir_path + sub_dir)\n\tfor file_name in files:\n\t\tif (file_name.find('.' + file_name.split(os"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tfile_type = \"\"\n\t\tfilepath = file_name.lower()\n\t\tif not filepath.endswith(\".m3u\") or os.path.isfile(filepath):\n\t\t\tfile_type = \"m3u\"\n\t\telse:\n\t\t\tif os.path.isfile(filepath):\n\t\t\t\tfile_type = \"m4a\"\n\t\t\telse:\n\t\t\t\tfile_path_basename = os.path.basename(filepath)\n\t\t\t\tfile_path_ext = os.path.splitext(file_name)[1]\n\t\t\t\tif file_path_ext == \".m2u\":"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\n\t_file_name = _str(file_name)\n\n\tif not _file_name:\n\t\treturn False\n\n\treturn self._load_file_name_from_path(file_name)!= _file_name\n\n\nclass PluginManager(object):\n\n\tdef __init__(self, plugins, config_file=None, init_config=True, config_path=None):\n\t\tif (not init_config) and (not config_file):\n\t\t\tconfig_file = ConfigFile()\n\t\tself.init_config = init_config\n\t\tself.config"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\treturn util.file_exists(file_name)\n\t\nfrom django.contrib.auth.models import User\nfrom django.core.management.base import BaseCommand\n\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\n\nfrom witk_site.models import Site\nfrom witk_site.site_authn import SiteAuthn\nfrom witk_site import site_settings\n\nfrom common.models import Profile\n\nclass Command(BaseCommand):\n    help = \"Runs tests by default\"\n    help_purpose = \"Just runs tests with"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn os.path.isfile(file_name)\n\tdef is_valid_file_name(file_name):\n\t\t\"\"\"\n\t\tCheck if file name is a valid file name.\n\t\t:param file_name: File name.\n\t\t:type file_name: str\n\t\t:return: Return true (valid file name), false (invalid file name)\n\t\t:rtype bool\n\t\t\"\"\"\n\t\t# Note: For read access, we check for valid file name first.\n\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\n\tf = open(file_name)\n\t\treturn os.path.isfile(f.read())\n\ndef is_file_mode_valid(file_name):\n\t\"\"\"\n\tCheck if file name is valid and open file mode valid.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (okay), false (open file, non-existant) \n\t:rtype bool\n\t\"\"\"\n\t\n# -------------------------------------------------------------------------------------------------\n# Main --------------------------------------------------------\n\ndef main():\n\t\"\"\" main routine for script \"\"\""}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif path_exists(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\ndef file_exists(file_name, path):\n\t\"\"\"\n\tCheck if file exists.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist, or invalid), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\treturn path_exists(file_name) and file_exists(file_name, path)\n\n\ndef get_file(file_"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\tif file_name.endswith('.zip'):\n\t\t\treturn True \n\t\tfile_path = pathlib.Path(file_name)\n\t\treturn os.path.isfile(file_path)\n\n  \t\t\nclass SandboxHandler(RequestHandler):\n    \"\"\"\n    Handle sandbox functionality (as module level functions)\n    \"\"\"\n    def get(self):\n        self.response.headers['Content-Type'] = 'text/raw'\n        self.response.out.write(json.dumps({'action' :'sandbox', 'file_name' : self.request.data.get('file_name', \"\")}))"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t\twith open(file_name, \"rb\") as f:\n\t\t\treturn f.read().lower() == b\"false\"\n\nfrom __future__ import absolute_import, unicode_literals\n\nfrom contextlib import contextmanager\nfrom future.builtins import str\n\n\n@contextmanager\ndef contextualized_globals(global_scope, local_scope, name):\n    \"\"\"Context manager that preserves local variables when running tests with local variables.\n\n    A contextmanager that copies values of a variable will be overwritten in each\n    test run, overwriting it in the final test run.\n\n    Args:\n        global_scope: Scope"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tfrappe.db.sql(\"delete from `tabLog` where docstatus!=1 and status!=2\")\n\n\ndef get_data():\n\t\"\"\"\n\tGets data from file or database from Google Cloud Storage.\n\t\"\"\"\n\tif not os.path.isfile(CONFIG_FILE):\n\t\twith open(CONFIG_FILE) as infile:\n\t\t\tdata = json.load(infile)\n\t\treturn data\n\n\tdata = _get_file_metadata(CONFIG_FILE)\n\n\treturn data\n#\n# Copyright (c) 2004 The SCons Foundation\n#\n# Permission is hereby"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tcls.reset()\n\n\ndef get_output():\n\t\"\"\"\n\tReturns the absolute/relative path of the output.\n\t\"\"\"\n\treturn os.path.abspath(os.path.dirname(get_command_line().rstrip()))\n#!/usr/bin/env python3\n\"\"\"\nUse the Python Markdown module to generate markdown files, and save them into\nfile.\n\nUsage: Markdown.py file\n\"\"\"\n\nfrom markdown.extensions import Extension\nfrom markdown.util import etree\nimport sys\nfrom bs4 import BeautifulSoup\n\n\ndef generate_markdown(path, extension="}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\t__tracebackhide__ = True\n\nclass CachedConfigError(ValueError):\n\tpass\n\ndef _get_logger(cfg):\n\t\"\"\"\n\tGet the logging logger\n\t\"\"\"\n\ttry:\n\t\treturn cfg['loggers'][cfg['name']]['logger']\n\texcept KeyError:\n\t\treturn None\n\ndef _set_logging(logger):\n\t\"\"\"\n\tSet a new logging object\n\t\"\"\"\n\tcfg = logging.getLogger(logger['name'])\n\tcfg['handlers'][cfg['name']]['level'] = cfg['level']\n\tcfg['handlers']["}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\t# For the debug server, the log format will not be printed if DEBUG=True.\n\t\t# If DEBUG=False, no output is logged to std out.\n\t\t#\n\t\t# There are two ways to set log level at the same time,\n\t\t# one for DEBUG=True:\n\t\t#\t\t- Set a logging level at DEBUG to the 'DEBUG' logging level,\n\t\t#\t\totherwise it will be INFO, and if it would be set to DEBUG=False,\n\t\t#\tthen the log level will get the same messages to stderr.\n\t\t#\n\t\t# To"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\n\tdef __init__(self):\n\n\tdef _clear_handler(self, msg):\n\t\t\"\"\n\t\t\"\"\"\n\t\n\n\tdef _set_logging(self, event):\n\t\t\"\"\"\n\t\tSet logging\n\t\t\"\"\"\n\t\n\n\tdef _get_logging(self):\n\t\t\"\"\"\n\t\tReturn all logging\n\t\t\"\"\"\n\t\n\n\tdef __enter__(self):\n\t\tself._change_class('manager')\n\n\t\tself._reset_logging()\n\n\t\treturn self\n\n\tdef __exit__(self, *exc_info):\n\t\tself._change_class('manager')\n\n\nif"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\nif 'no-log' in sys.argv or not 'no_log' in env:\n\t_clear_logging()\n\n\nclass Bench(_Benchmark):\n\n\tdef setup(self):\n\t\tsuper(Benchmark, self).setup()\n\n\t\tself.benchmarkID = \"\"\n\t\tself.name = \"Bench\"\n\n\t\tself.max_time = -1\n\t\tself.max_iteration = -1\n\n\t\tself.benchmark_result = False\n\n\tdef benchmark(self, test, result):\n\t\tbench = super(Benchmark, self).benchmark(test, result)\n\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tglobal logger\n\t\tlogger.setLevel(\n\t\t\tlogging.WARNING if logger.level == logging.NOTSET else logging.INFO\n\t\t)\n\t\tlogger.handlers = []\n\t\tsys.stderr.seek(0)\n\t\tif logger.handlers:\n\t\t\tlogger.handlers += logger.handlers\n\t\tfor hdlr in logger.handlers:\n\t\t\thdlr(hdlr)\n\t# -------------------------------------------------------------------------\n\nimport json\nimport logging\nimport mimetypes\nimport os\n\n\n\nconfig = {}\nconfig_default = config.copy()\nconfig_default.update(config)\n\n\nif not hasattr("}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tcls.getLogInfo().reset()\n\t\tcls.readSettings()\n\t\tcls._last_settings = None\n\t\tcls._settings_changed_timer = None\n\t\tcls._settings_changed_eventHandler = None\n\t\tcls._settings_changed_event = None\n\t\n\tdef _onChanged(self):\n\t\t\"\"\"\n\t\tCalled when the settings are changed. This is used after the\n\t\tsettings are changed to occur.\n\t\t\"\"\"\n\t\tself._settings_changed_timer = QtCore.QTimer()\n\t\tself._settings_changed_timer.setSingleShot(False)"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tpass\n\nreset_logging = getattr(settings, 'PERMANENT_LOGGER_RESET_LOGGER_HANDLER', None)\n\n# A function to turn of logging output to the logs at the console's level\n# Eg. logging.info('foo bar bbb c') will print 'foo bar  bbb c'\nlog_output = logger.info\nlog_output = log_output or cls.__name__\nlog_output = log_output.replace('_', '-')\n@classmethod\ndef setup(cls):\n\t\"\"\" Initialise\n\t\"\"\"\n\t\t# Make sure a valid logger"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\tsys.stderr = \"\"\n\n\ndef _execute_setup(cls):\n\t\"\"\"\n\tExecute setup.sh script from setup.py\n\t\"\"\"\n\n\t\tsys.stderr = \"\"\n\t\targs = [sys.executable] + cls.setup_args + ['setup.py']\n\t\treturn subprocess.call(args) == 0\n\n\ndef _execute_shell(cls, cmd, exit_code=0):\n\t\"\"\"\n\tExecute shell command from setup.py\n\t\"\"\"\n\n\t\tsys.stderr = \"\"\n\n\tsys.stderr = subprocess.check_output(cmd, shell="}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\ndef _getTargetClass2(self, target):\n\t\"\"\"\n\tTry to get a class derived from an implementation.\n\t\"\"\"\n\t\n##\tfor i in xrange(classCount):\n\t\n##\t\tprint 'Checking', 'InterfaceType of interface class', i+1\n\t\nfrom interface import PCBaseClass, PrototypeClass, Interface\nimport interfaces\n__all__ = [\"PCBase\",\"Prototype\"]\n\nclass PC(PCBaseClass):\n\t\"\"\"\n\tBase class for all classes which are implemented as prototypes.\n\tOnly use to create a Prototype class per"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn self.__py_class__\n\tdef __getattr__(self, name):\n\t\tif name == '__py_class__':\n\t\t\treturn PyClassClass(self)\n\t\treturn __getattribute__(self, name)\n\tdef __call__(self, *args, **kwargs):\n\t\treturn self.init(**self.default())\n\n\tdef init(self, **kwargs):\n\t\treturn self\n\n#\n# The Python implementation of L{PyClassObject} subclasses.\n#\n\nclass PyClassObject(PyClassClassObjectBase):\n\t\"\"\"\n\tA python subclass that defines"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\n#\t\t# Get implementation\n#\t\tm = __import__(m, globals(), locals(), [m])[m]\n#\t\ti = m.__code__.co_argcount\n#\t\tf = getattr(m, 'Py' + r'.'+r'.'+r'.'+m.__name__)\n#\t\tt = m.__code__.co_name\n#\t\tv = m.__code__.co_varnames[i]\n#\t\tm = t[v]\n#\t\tv = m.__code__.co_consts[t[1]]\n#\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\t# No __import__ since we can't necessarily invoke __main__\n\t\tpass\n\n\nclass MockTarget(object):\n\t\"\"\"Stub target for a mock target\"\"\"\n\n\tdef __init__(self, name, bases, dct):\n\t\tobject.__init__(self, name, bases, dct)\n\t\tself._targets = {}\n\t\tself._mockTarget = None\n\n\tdef getTarget(self, name):\n\t\tif name not in self._targets:\n\t\t\tself._targets[name] = super().getTarget(name)\n\t\treturn self._targets[name]\n\nclass Mock"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn self._fallbackTargetClass\n\t#-----------------------------------------------------------------------------#\n\t# Interface prototype implementation.\n\t#-----------------------------------------------------------------------------#\n\t\n\tdef __init__(self, source, target, method, fallbackTargetClass):\n\t\tself.__targetClass = target\n\t\tself.__method = method \n\t\tself.__fallbackTargetClass = fallbackTargetClass\n\n\tdef _makeTargetClass(self):\n\t\treturn self.__targetClass\n\n\t#-----------------------------------------------------------------------------#\n\t#-----------------------------------------------------------------------------#\n\n\tdef __getattr__(self, attr):\n\t\tif attr == \"_method\":\n\t\t\traise AttributeError(\"No attribute called '%s'.\" % str(attr"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn type(self.target)\n\t#}\n\t#}\n\n#{{\n\nclass Filesystem(object):\n\t\"\"\"\n\tA file system object.\n\t\"\"\"\n\t#{{\n\tdef __init__(self, dir, files):\n\t\tself.dir = dir\n\t\tself.files = files\n\t\tself.opens_files = {}\n\n\t#}\n\t#}\n\t#}\n\n\t@property\n\tdef closed(self):\n\t\"\"\"\n\tWhether this file system has been closed.\n\t\"\"\"\n\t\treturn not bool("}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\treturn {\n\t\t\t'Py': 'Python',\n\t\t\t'Fallback': 'Python'\n\t\t}[self.implementation]\n\n\t@staticmethod\n\tdef _implementation(target):\n\t\tassert target.startswith('fallback_impl_')\n\t\treturn '.'.join([target[len('fallback_impl_'):], target[len('fallback_impl_'):]])\n\n\t@staticmethod\n\tdef _createTarget(target):\n\t\tassert isinstance(target, tuple)\n\t\ttarget_prefix, target_class = target\n\t\tname = target_prefix.replace('/', '_').lower()\n\t\tclassname = target_prefix."}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\tif sys.version_info[0] == 2 and sys.version_info[1] >= 6:\n\t\t\treturn 'Py%s.%s.%s' %(sys.version_info[0], sys.version_info[1],\n\t\t\t\tsys.version_info[2])\n\t\telse:\n\t\t\treturn 'Py%s.%s' %(sys.version_info[0], sys.version_info[1])\n\treturn None\n\n\nclass ErrorContext(object):\n\t\"\"\"\n\tCustomized behaviour for a specific error context - used by\n\ttracebacks when a"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\tif self.useFallback == 'Py':\n\t\t\treturn PyCapiObject\n\telse:\n\t\treturn PyCapiObject\n\n\n\nclass PyCppObject(object):\n\n\n\t\"\"\"\n\tDefines an externally defined class in a python module\n\t\"\"\"\n\n\tdef __init__(self, name, parent):\n\n\t\tself.name = name\n\t\tself.parent = parent\n\n\t\t# get the object\n\t\tself.object = PyCppObject()\n\n\tdef build(self):\n\t\t\"\"\"\n\t\tCreate and return the object\n\n\t\t:return: The generated object\n\t\t:rtype: Py"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t\ndef importCode(self,module,filename):\n\tif module.startswith('_'):\n\t\tmodule = ''\n\t\n\ttry:\n\t\tname,funcName,file=os.path.split(filename)\n\texcept: filename = filename+'.py'\n\t\n\tif name=='__builtin__':\n\t\tname=_UNSPEC_NAME\n\t\t\n\tdef getImplementationType():\n\t\t\"\"\"\n\t\tDefine this to return the implementation,\n\t\twith both the 'Py' call, with the implementation suffix,\n\t\tso 'PythonScript', if it's a binary distribution we're creating\n\t\tthe framework.\n\t\t\"\"\""}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\nclass _OrderedBase(object):\n\t\"\"\"\n\tAn ordered baseclass which can only be merged into an ordered collection.\n\t\n\tThere is no built-in, primarily intended use for ordering.\n\t\n\tThe underlying classes must override the following keyword arguments:\n\n\t- obj (optional): The object to merge. Any object with the same \n\t\t'get_value_from_data_structure' method should return an identical \n\t\tobject.\n\t\n\t- sort_key (optional): The object attribute to sort on, if\n\t\tnot specified, the object will be considered to be the first \n\t\t("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tif not orderings: return orderings\n\tout = []\n\tfor order in orderings:\n\t\tfor x, y, z in order:\n\t\t\tif list.__contains__ ([x,y,z]):\n\t\t\t\tout.append(x)\n\t\t\t\tout.append(y)\n\t\t\t\tout.append(z)\n\treturn out\n\n#######################################################################\n\n\ndef _mergeData(d1, d2):\n\t\"\"\"\n\tTakes two Data objects and makes a two-dimensional list of orderings\n\tas needed\n\t\n\tParameters\n\t----------\n\t\n\td1 : Data"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n_multi_key_list = []\ndef _get_multi_key(value):\n\tglobal _multi_key_list\n\t\n\tif value is None:\n\t\tvalue = key_set().intersection(\n\t\t\t_multi_key_list)\n\t\n\tif len(value) == 1:\n\t\treturn value[0]\n\telse:\n\t\treturn tuple(_get_multi_key(x) for x in value)\n\n# In python2 or later, the values can be turned into integer values\n# (as opposed to floating point, in which case the returned values\n#"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\ndef _mergeOrderingsWithReversedKeys(orderings):\n\t\"\"\"\n\tMerged sorts a set and removes all objects starting with the\n\tsame hash.\n\t\n\t.. code-block:: python\n\n\t\tsortedMins = sorted(\n\t\t\tset.difference(o) for o in orderings)\n\t\t\n\t\tmerged = [list(sortedMins)[0], list(sortedMins)[1]]\n\t\t\n\t\tsortedMins[0][-1] = merged[0]\n\t\t\n\t\tsortedMins[3][:] = merged[3][:] + merged[2]\n\n\n\t\n\tThis is a"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\ndef _check_is_orderable(obj, orderings):\n\t\"\"\"\n\tReturn true if the given object appears in the orderings\n\t\n\tThe object is considered a _orderable object.\n\t\n\tArgs:\n\t\n\tobj: the object whose _orderable() method returns true\n\t\n\tKeyword arguments:\n\t\torderings: the orderings, assumed to be either a list of _base_orderings or\n\t\ta single _base_orderings\n\t\n\tReturns: True - it is not a _orderable object, or has been overridden\n\t\n\tRaises:\n\t\tValueError: if the object is"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tif hasattr(orderings, \"__len__\"):\n\t\tif len(orderings) == 1:\n\t\t\torderings = [orderings]\n\t\treturn [items.pop(0) for items in orderings]\n\telse:\n\t\treturn [items.pop(0) for items in orderings]\n\ndef _mergeGrouper(pairs):\n\t\"\"\"\n\tCombine pairs of two dictionaries such that the pair has the same key or \n\tvalue that already occurs. It's also possible to combine multiple dictionaries of \n\tsame key values.\n\t\"\"\"\n\tout_"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tresult = []\n\tnumSides = 0\n\t\n\tif len (orderings) == 0: return result\n\t\n\tdef join_side (side):\n\t\tif side['sideOrder'] > len (result): return False\n\t\tresult.insert (0, side)\n\t\tside['sideOrder'] = 0\n\n\tfor item in orderings:\n\t\tif len (item) < len (result): result[-1].append(item)\n\t\tif len (result[-1]) < len (item): item[-1] = [item] + ([0]*(numSides + len"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t\n\tslen = 0\n\tllen = 0\n\tfor i in orderings:\n\t\tslen += _getOrderings(i)\n\t\tllen += i[1] - i[0]\n\n\treturn llen, orderings\n\t\n\ndef _getOrderings(i):\n\t# the list we have to go is:\n\t#   [ {'s' : 0, 'l': 1},\n\t#   {'s' : 1, 'l': 2}, \n\t#   {'s' : 2, 'l': 3} ]\n\t\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\t# XXX This should be a better way to deal with this - but for now\n\t\t# it'd better be to merge in a more flexible way.\n\t\t\n\t\t# Keep only the first entry we care about\n\t\tnewOrderings = []\n\t\t\n\t\t# First try to reverse the ordering\n\t\tfirstIndex = 0\n\t\tfor index in reversed(range(0, len(orderings))):\n\t\t\ttry:\n\t\t\t\tnewOrderings.append(orderings[index] + orderings[firstIndex + 1])\n\t\t\texcept IndexError:\n\t\t\t\tnewOrderings.append(orderings[index] + orderings"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\t_orderings_str = \"\".join([str(a) for a in orderings])\n\t\tif orderings == []:\n\t\t\treturn None\n\n\t\telse:\n\t\t\treturn ''.join(_orderings_str)\n\n_MergeOrderings = _legacy_mergeOrderings\n\n\ndef _flattenOrderings(orders):\n\t\"\"\"Flatten the given list of orders so that it has the correct orderings\"\"\"\n\tif not orders:\n\t\treturn []\n\n\tresult = []\n\tfor o in orders:\n\t\ttry:\n\t\t\tresult.extend(_flattenOrderings(o))\n\t\texcept"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n# Import Zope 2.0 compatible interface module\nfrom zope.interface import Interface, implementer\n\n__all__ = ('provides', 'implements')\n\nclass Provides:\n\t\"\"\"Provides interfaces and interfaces which provide implementations\"\"\"\n\t\n\tdef implements(self, interface):\n\t\t\"\"\"Implements a particular interface.\n\t\t\n\t\tI.e. a ``Provides`` object must return at least one interface.\n\t\t\"\"\"\n\t\n\t\n\tdef implementsInterface(self, interface, implementer):\n\t\t\"\"\"Implements a particular interface on the given :class:`Interface`.\n\t\t\n\t\t@param interface: The interface the interface should use"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\nclass IFunction(object):\n\t\"\"\"\n\tAn interface for the `@zope.interface.interfaces` decorator.\n\t\n\tIf the given object implements `__self__`, only the object will be passed to the wrapper\n\tobject.\n\t\"\"\"\n\tname = None\n\tdescription = None\n\tparameters = None\n\t\n\tdef __init__(self, obj):\n\t\t\"\"\"\n\t\tCreate a new `IFunction`.\n\t\t\n\t\t:type obj: object\n\t\t:rtype: IFunction\n\t\t\"\"\"\n\t\tself.obj = obj\n\t\t\n\tdef get(self, name, default=None):"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\tdef __init__(self, location, description, interfaces):\n\t\tobject.__setattr__(self, \"__location\", location)\n\t\tobject.__setattr__(self, \"__description\", description)\n\t\tobject.__setattr__(self, \"__interfaces\", interfaces)\n\t\tobject.__setattr__(self, \"__id\", location)\n\t\tobject.__setattr__(self, \"__provides\", ())\n\t\tobject.__setattr__(self, \"__deprecated\", False)\n\t\t\n\tdef __getattr__(self, name):\n\t\t# pylint:disable=redefined-builtin\n\t\tinterface = getInterface(self.__location, name)\n\t\tif interface"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t_interface_declarations = {\n\t\tb'Z-PIDL:Zope,IDL:CFinder'\n\t}\n\t\"\"\"\n\t\n\t\n\tThe interfaces directly provided by the IDeclaration objects are returned instead. For details see the documentation in the Zope\n\tDocFile above.\n\t\n\t\"\"\"\n\n\t\n\tdef __init__(\n\t\tself, iface, name=None, allowUnicode=True, allowUnicodeStr=True, allowAnything=True\n\t):\n\t\tself._object = iface\n\t\tself._cache = {}\n\t\tself._has_interface = True\n\t\t\n\t\tif allow"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\ndef interfacesWithoutExtendedInterfaces(interfaces): # pylint:enable=redefined-builtin\n\t\"\"\"\n\tReturn all interfaces and not extended interfaces\n\t\"\"\"\n\ndef _interfacesRecursive(interfaces): # pylint:disable=redefined-builtin\n\t\n\t\"\"\"\n\tRecursive return all interfaces, or their parents\n\t\n\tThe value returned is an immutable dictionary, one for each interface type that we\n\twant and the values have access to.\n\t\"\"\"\n\treturn dict.fromkeys(interfaces.iterate_own([IDeclaration]), False) \\\n\t\t\t\ndef _interfacesRecursiveRecursive(interfaces, **kwargs): # pylint:disable"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\tdef directlyProvidesAsObject(self, object): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn an interface object directly provided with `object` as `a` and\n\t\t``b``.\n\t\t\n\t\tSee also:\n\t\t\t`inheritedFrom`\n\n\t\t:param object: object\n\t\t\"\"\"\n\t\n\tdef inheritedFrom(self, object): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tResolve any of the interfaces that this class references to.\n\t\t\n\t\tSee also: \n\t\t\t`inheritedFrom`\n\t\t\n\t\t:param object: object\n\t\t\"\"\"\n\t\n\tdef inheritedWithout"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t\n\t@classmethod\n\tdef interface(cls, interface):\n\t\t\"\"\"\n\t\tReturn the provided interface.\n\t\t\n\t\tArguments:\n\t\t\tinterface: The interface to resolve the interface from in the AST.\n\t\t\n\t\tReturns:\n\t\t\tThe interface to resolve the interface from in the AST.\n\t\t\n\t\tRaises:\n\t\t\tTypeError: The interface can't be resolved with provided\n\t\t\t\tinterface type.\n\t\t\"\"\"\n\t\treturn interface\n\n\nclass Interface(ast.NodeVisitor):\n\t\"\"\"\n\tA node visitor for InterfaceType and InterfaceTypes.\n\t\n\tInterfaceType is a type (eg. types or interfaces) that is"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, obj, interfaces, ifaceDict):\n\t\t\"\"\"\n\t\t\n\t\t:param obj: the object the interface is supposed to be given by\n\t\t:param interfaces: the interfaces to inspect (list of `~zope.interface.interfaces.Interface`)\n\t\t:param ifaceDict: the interfaces to inspect (dict in the order of the interfaces and interface types)\n\t\t\"\"\"\n\t\tself._obj = obj\n\t\tself._interfaces = interfaces\n\t\tself._ifaceDict = ifaceDict\n\t\n\tdef getInterfaceDict(self):\n\t\t\"\"\"Return the interfaces used by the given interfaces.\n\t\t\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\treturn object\n\t\n\t@classmethod\n\tdef getInterfacesByType(cls, type): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the interfaces through `type`.\n\t\t\n\t\tThe type of the interfaces is defined by `type`.\n\t\t\n\t\t@param type: an `IDeclaration` type (i.e. `IDeclaration`).\n\t\t@return: an instance of the `IDeclaration` type.\n\t\t\"\"\"\n\t\t\n\t\ttry:\n\t\t\t# pylint: disable=no-member\n\t\t\treturn cls.interfacesByType(type)\n\t\texcept TypeError: \n\t\t\treturn cls.interfacesBy"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\treturn lambda iface, name: iface.declarations.get(name, None)\n\t\n\n@interface\nclass Class(Attribute):\n\t\"\"\"\n\tA ``__init__()`` attribute that should be provided in Python-style code.\n\t\"\"\"\n\tdeclarations = {}\n\tnamespace = \"zope:\"\n\tdef __init__(self, name, bases, namespace, **kwargs):\n\t\tfor base in bases:\n\t\t\tif isinstance(base, Interface):\n\t\t\t\t# pylint:disable=redefined-builtin\n\t\t\t\tif base.name == name:\n\t\t\t\t\tself.declarations[name] = base."}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\nclass MiniSizeList:\n\tdef sum(self, lists, sum):\n\t\tresult = [0] * len(lists)\n\t\tfor i in range(0, len(lists)):\n\t\t\tresult[i] = 1\n\t\t\tfor j in range(0, sum):\n\t\t\t\tresult[j] = (1 if i+j <= len(lists)-1 else 0) ^ (1 if j+j <= len(lists)-1 else 0)\n\t\treturn result\t\t\t\n\tdef get_sum_with_base(self, base, sum):\n\t\tresult = 0\n\t\t"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n#\tReturns: class with minimum subset of base classes\n#\tdef orderedMinSubset(class1, class2):\n\t\tfor value1, value2 in [(tuple(x) for x in c.__bases__[:]) for c in classes]:\n\t\t\tif isinstance(value1, base) and isinstance(value2, base):\n\t\t\t\treturn value1\n\t\treturn None\n\t\n\t\n\t\n\t\n\n#\tfor base in classes:\n#\t\tprint base\n#\t\tprint len(sortedClasses(base.__bases__))\n#\t\tsortedClasses(base).sort(key=lambda class:"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\t\n\tif len(classes) > 1:\n\t\traise TypeError('This function should take at most one argument')\n\t\n\tdef combine(classes):\n\t\treturn combineSingleClass(classes, 'in', lambda x: type(x) in bases.keys())\n\t\t\n\treturn combine(classes)\n\n# this is needed in order to get the proper class attribute\ndef combineSingleClass(classes, attribute, function):\n\tdef wrapper(self, *classes):\n\t\tif any(func not in x for x in classes):\n\t\t\treturn self\n\t\t\n\t\twith memoize(function):\n\t\t\tclassObj"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\treturn sorted(map(lambda base: min(len(base), 3), classes),\n\t\t\tkey=lambda a: a.name)\n\n\ndef listBases(cls):\n\t\"\"\"\n\tList all the base classes of the given base classes\n\t\"\"\"\n\t\treturn [obj for obj in cls.__bases__]\n\n\nclass BaseClassList(listBases):\n\t\"\"\"\n\tList base classes in the given list of base classes\n\t\"\"\"\n\t\n\n\n\n\n\n\n#\n# Base classes for all class types supported by our subclass\n#\n\n#\n# This version of this class is"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\tif classes == []:\n\t\t\treturn classes\n\t\telif not len(classes) == len(classes):\n\t\t\treturn [classes[0]]\n\t\t#\n\t\t# Find the minumum number of bases\n\t\t# (ie. if there is one base classes)\n\t\t#\n\t\t#\tFor each base type\n\t\t#\n\t\tfor classIndex, baseType in enumerate(sorted(classes)):\n\t\t\t#\n\t\t\t#\tFor each base type\n\t\t\t#\n\t\t\t#\tIf no bases in the list are minimally equal to a\n\t\t\t#\tminumum type, then the first"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\tinstances = [n for n in classes if n not in ALL_BASE_CLASSES]\n\t\tif len(instances) > 0:\n\t\t\treturn instances.index(ALL_BASE_CLASSES.index(classes[0]))\n\t\telse:\n\t\t\treturn ALL_BASE_CLASSES.index(classes)\n\treturn ALL_BASE_CLASSES.index(classes)\n\nclass CroppedClass(object):\n\t\"\"\"\n\t\tA class to be cropped, as the classes are not required to map onto some sort \n\tof data so far (eg. where the base images are more complicated)"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\n\tdef _merge(classes, order):\n\t\tfor a in classes:\n\t\t\tif (a.ordered == order):\n\t\t\t\treturn a\n\n\t\treturn classes\n\n\treturn sorted(_merge(cls_mro, cls_mro), key=classes.sort)\n\n\n@unique(base=list)\ndef _sorted(list_of_ordered_classes):\n\tif list_of_ordered_classes is None:\n\t\treturn sorted(default_ordered_classes)\n\telse:\n\t\treturn sorted(list_of_ordered_classes, key=cls_mro.sort)\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\tbases, newbases = zip(*classes, 1)\n\t\tseen = {}\n\t\tfor base in bases:\n\t\t\tif base not in seen:\n\t\t\t\tseen[base] = 1\n\t\t\tbases.append(base)\n\t\t\tbase = newbases\n\t\n\treturn reduce(OrderedSet.__add__, bases)\n\n\ndef main():\n\timport inspect\n\timport math\n\timport copy\n\n\tclass C(object):\n\t\tdef __init__(self):\n\t\t\tpass\n\t\n\t\tdef __init__(self, a, b):\n\t\t\tself.a = float(a)\n\t\t\tself.b"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\tclasses.sort(key=lambda c: not (int(c.tag) < len(classes) and c.tag not in ['BASE']))\n\t\n\tdef getCommonOrder(classes):\n\t\t\"\"\"\n\t\tReturns common base classes from a sorted list\n\t\t\"\"\"\n\t\tassert len(classes) > 0\n\t\ti = classes.index(max([str(i)] + list(cls) for cls in classes))\n\t\tassert i >= 0\n\t\treturn [classes[idx] for idx, cls in enumerate(classes) if cls[i] >= len(classes)]\n\t\n\tdef getValid"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\treturn reduce(lambda c, n:\"-\"+c, classes, [])\n\n\ndef base_type(type_name):\n\t\"\"\"\n\tGet the basic structure of an int type based on the type name, and\n\treturn the proper type\n\t\"\"\"\n\tif not(type_name):\n\t\traise Exception(\"Invalid type name: %s\" % type_name)\n\n\ttype = int(type_name)\n\tif type in classes_base.types:\n\t\treturn classes_base.types[type]\n\telse:\n\t\tdebug.log(\"Unknown type %s"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\n\tclass Property(object):\n\t\t\n\t\tdef isValid(self):\n\t\t\t\"\"\"\tBoolean value to indicate that this Property is valid. Will\n\t\t\tbe checked for validity by looking at the.properties file.\n\t\t\tNote that this is only meant to be used by default.\n\t\t\t\"\"\"\n\t\t\tfor p in self.properties:\n\t\t\t\t(type, propname) = p.split('=')\n\t\t\t\tif not (type =='string' and propname in ['name', 'description']):\n\t\t\t\t\traise ValueError, \"Property %s in %s is not a string or a %s\" %(self."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\treturn self.data.attrNamesAndDescriptions if all else self.data.namesAndDescriptions\n\n\t\nclass PrototypeAttribute(object):\n\t\"\"\"\n\tAttribute of a Prototype object. A given object is a string containing the name of the protocol.\n\n\n\tExample::\n\n\t\tattribute =  {\n\t\t\t'sname': 'testInterface',\n\t\t\t'defName': ('testInterface', 'TestInterface'),\n\t\t\t'descr': 'interface description',\n\t\t\t'min': 32,\n\t\t\t'description': 'interface description'\n\t\t}\n\t\"\"\"\n\tpass\n\nclass Compressed"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\treturn self._node.attributesNameAndDescriptions(self.name,\n\t\t\t\t\t\t\t\tself.description)\n\ndef checkAttributeValues(self, node):\n\t\"\"\"\n\tCheck interface attribute values and return the dictionary.\n\t\"\"\"\n\tif node.hasAttribute('name'):\n\t  name = node.getAttribute('name')\n\t  description = node.getAttribute('description')\n\t  if name:\n\t\t\t\n\telse:\n\t  name = None\n\t  description = None\n     \n\treturn name,description,node\n\t#raise Exception, \"AttributeValues not defined for %s, %s\" % (self.name"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tmethods = self\n\t\tname = None\n\t\tdescription = None\n\t\tfor f in methods:\n\t\t\tif not all and method in (c.__name__ for c in methods):\n\t\t\t\tcontinue\n\t\t\tif not name: # only attributes specified by class are available.\n\t\t\t\tname = f.__name__ \n\t\t\tif not description: # attributes specify descriptions of a method\n\t\t\t\tdescription = getattr(self, f.__name__)\n\t\t\telif not all and self.descriptions and f.description \\\n\t\t\t\t\tand f.__name__ in self.descriptions:\n\t\t\t\tdescription = getattr(self, f.__name__)"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\nclass Attribute(object):\n\t\"\"\"\n\tRepresent a global attribute.\n\n\tInterface attributes define attribute attributes as instance attributes\n\t\"\"\"\n\t__slots__ = [\"name\", \"is_defined\", \"class_name\"]\n\t__order__ = [\"is_defined\", \"name\"]\n\n\tdef __init__(self, name, is_defined=None, class_name=None):\n\t\t\"\"\"Constructor\n\t\t\n\t\tAttributes name, is defined as class attributes, attributes\n\t\tor attributes as an instance attributes.\n\t\t\n\t\tAll arguments are optional, if given, are available.\n\n\t\t@param name:"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t    \tif all: print \"    - All attributes\"\n\t    \tallName = [\"type\", \"interface\", \"description\", \"data\", \"attribute\", \"data_type\", \"attribute_class\"]\n\t    \telse: print \"    - One or more attributes (if defined)\"\n\t    \tallAttrList = allName\n\t    \tallDescription = [\"description\" if not \"interface\" in all else \"interface.description()\" if \"interface\" in all else \"Attribute\", \"attributeClass\" if not \"interface\" in all else \"Descriptor\"]\n\t    \t\tif \"data\" not in all"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tnamesAttributeNames = \"attributeNames\" in self.__slots__\n\t\tnamesAttributeDescriptions = \"attributeDescriptions\" in self.__slots__\n\t\tnamesAttributeNamesAndDescriptions = \"attributeNamesAndDescriptions\" in self.__slots__\n\n\treturn {\n\t\tnamesAttributeNames: namesAttributeNames,\n\t\tnamesAttributeDescriptions: namesAttributeDescriptions,\n\n\t}\n\n\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom unittest import skipUnless\n\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\tself._all = all\n\n\t\tself._attributes = []\n\t\tself._propertyNames = set()\n\t\tself._propertyNames.update(self._listAttributes())\n\t\tself._propertyNames.update(\n\t\t\tself._typeNamesAndDescriptions(all))\n\n\tdef __add__(self, other):\n\t\treturn self._filterAttributes() + other._filterAttributes()\n\n\tdef __getitem__(self, index):\n\t\treturn self._filterAttributes()[index]\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tIterate over property values\n\t\t\"\"\"\n\t\treturn self._filter"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\treturn self._interfaceAttributes(\n\t\t\t\ta=False,\n\t\t\t\tb=True,\n\t\t\t\tc=False,\n\t\t\t\td=False,\n\t\t\t\te=False,\n\t\t\t\tf=False,\n\t\t\t\tg=False,\n\t\t\t\th=False,\n\t\t\t\ti=False,\n\t\t\t\tj=False,\n\t\t\t\tk=False,\n\t\t\t\tl=False,\n\t\t\t\tm=False,\n\t\t\t\tn=False,\n\t\t\t\to=False,\n\t\t\t\tp=False,\n\t\t\t\tq=False,\n\t\t\t\tr=False,\n\t\t\t\ts=False,\n\t\t\t\tt=False,\n\t\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t\n#\t\tret = {}\n#\t\tif all:\n#\t\t\tret = OrderedDict()\n#\t\t\tret[\"names\"] = []\n#\t\t\tfor type in dir(self._base_api):\n#\t\t\t\tif any(name in type.__doc__ for name in attr[type]):\n#\t\t\t\t\tret[\"names\"].append(name)\n#\t\t\treturn ret\n#\t\tret[\"names\"] = [str(name) for name in list(attr.keys()) if name!= \"__module__\"]\n#\t\tif \"desc\" not in ret:\n"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn self._attr\n\n\tdef __getattr__(self, attr): # pylint:disable=redefined-builtin\n\t\ttry:\n\t\t\treturn getattr(self._obj, attr)\n\t\texcept AttributeError:\t\t\t\n\t\t\traise AttributeError(attr)\n\n@tf.custom_op(True)\nclass Misc(object):\n\t\"\"\"\n\tSpecialization wrapper around a python object which is useful for\n\tthe object intensive Python objects, which can be used to find/remove\n\tattributes of objects.\n\n\tInstead of the standard Python methods or Python properties, you can use\n\tthese"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn self._names.fget\n\tdef names_with_data(self, *args): # pylint:enable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the attribute names of all the given interfaces,\n\t\tby name, (first occurrence of the attribute from each name).\n\t\tIf multiple attributes exist for the given interface name,\n\t\treturn a list instead.\n\t\tAny given attribute will raise KeyError when multiple\n\t\tinterfaces exist for the given interface name.\n\t\t\"\"\"\t\n\t\ttry:\n\t\t\tif len(args) > 1:\n\t\t\t\tif not (len(args) == 1 and"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn self._attr_names\n\n\tdef attrvalues(self): # pylint:disable=redefined-builtin,redefined-outer-name\n\t\"\"\"\n\tReturn the values of all the attribute names.  Use attrvalues(all=True) to return\n\tattributes.\n\t\"\"\"\n\t\tassert self._attr_names is not None, self._attr_names\n\t\tret = [getattr(self, attr) for attr in self._attr_names]\n\t\treturn ret\n\n\tdef is_global(self):\n\t\t\"\"\"\n\t\tCheck whether the interface has the attribute 'global'.\n\t\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn self.interface.names\n\tdef names_all(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\t\tReturn the attribute names defined by the interface.\n\t\t\"\"\"\n\t\tnames = \" \".join([n for n in self.interface.names if n in self.name])\n\t\tself._return_if_all = all\n\tdef iter_names(self, all=False): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tReturn the attribute name iterator\n\t\t\"\"\"\n\t\treturn self.interface.iter_names(self.name"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\n#################################################################################\n# Main\n#################################################################################\n\ndef main():\n\tprint(' '.join(sorted(getters())))\n\nif __name__ == '__main__':\n\tmain()# -*- coding: utf-8 -*-\n\n# Copyright (c) 2017 Matt Martz <matt@sivel.net>\n#\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\nfrom __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\treturn [attr.name for attr in self.attrs() if not attr.is_builtin]\n\tdef uniq_named(self): return self.attrs()\n\ndef isinstance(obj): return obj.is_instance()\n\ndef isclass(obj): return obj.is_class()\n\n\n\n\nclass IAttr(object):\n\n\t__slots__ = \"_doc\", \"_name\", \"type\", \"doc\"\n\tdef __init__(self, doc, name):\n\t\tself.name = name\n\t\tself.doc = doc\n\n\tdef name(self): return self."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\n# NOTE:\n# For now, only supporting the interfaces which have an attribute with a __getattr__ hook.  You'll need more info.\n# TODO - more info on this...\n@Attributes(interface='Interface')\nclass Interface(Element):\n\t\"\"\"\n\tBase interface object for accessing data from interfaces.\n\tThe interface object is not meant for \"interfaces\", but only for \"interfaces where interfaces\" are used in this code, as an extensible API for creating the attribute type.  For example, you would use a __getattr__ from a specific class or method of the object (such as"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\t# Note: we only use this for testing on a specific interface...\n\t\tif isinstance(self, Interface) or not self._is_implemented():\n\t\t\treturn [self.attribute]\n\t\t# Note: this is really a list-of-dict interface?\n\t\treturn self._is_implemented(all=all)\n\n\t@since(\"\")\n\tdef attributes(self): # pylint:disable=redefined-builtin,unused-argument\n\t\t\"\"\"\n\t\tReturn the names of all of this interface's attributes.\n\t\t\"\"\"\n\t\treturn [getattr(self, attr) for attr in self.attribute"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\tif hasattr(self, 'names'): return list(self.names)\n\tdef add(self, name):\n\t\t\"\"\"\n\t\tReturn a new attribute that is added to this object.\n\t\t\"\"\"\n\t\tif self.name == name: return self\n\t\tif name in self.names: self.names.remove(name)\n\t\tnew = self.__class__(name)\n\t\tnew.__setattr__('self', name)\n\t\treturn new\n\nclass _AttributeGetterAPI():\n\t\"\"\"\n\tThe AttributeGetterAPI provides a special 'attribute names' method\n\tfor which the interface name"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\ndef classes_properties(all=False): # pylint:disable=redefined-builtin\n\treturn ClassProperties(all=all)\n\ndef classes_methods(all=False): # pylint:disable=redefined-builtin\n\treturn ClassMethods(all=all)\n\ndef class_methods(all=False): # pylint:disable=redefined-builtin\n\treturn ClassMethods(all=all)\n\ndef _build_attributes(class_name, attributes):\n\tif class_name in attributes:\n\t\tif len(attributes[class_name][0].split(\"/\")) == 2:"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n@implementer(IDictInterface)\nclass IDict(Interface):\n\t\"\"\"\n\tRepresenting dictionary values\n\t\n\tThis object can be used only when:\n\t\t* *dictionary is nested from the dictionary it has attributes (or values\n\t\t\tof the form: *dict*, where one or more entries consist of\n\t\t\tan attribute, and an attribute/value tuple).\n\t\t* *dictionary has a single 'key' key in its __all__ attribute,\n\t\t\totherwise an AttributeError can be raised.\n\t\"\"\"\n\t#\n#\tdef __repr__(self):\n\t#\t\treturn"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif not output:\n\t\toutput=input\n\tif not output and'' in sequence:\n\t\toutput=\"\"\n\tif output:\n\t\tif '(' in sequence and ')' in sequence:\n\t\t\traise Ice.Exception, \"Declariant separator not supported\"\n\t\telse:\n\t\t\tsequence=sequence.split(', ')\n\tif 'I' in sequence:\n\t\tlist=[]\n\t\tfor i in sequence:\n\t\t\tlist.append(i.split('='))\n\t\tlist=[a for a in list if len(a)>=2]\n\t\tfor i in list:\n\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\tif(sequence == 'Union'):\n\t\t\treturn tuple(flatten([_validatetuple(a) for a in sequence]))\n\t\tif(sequence == 'Struct') or (sequence is None):\n\t\t\treturn list(flatten([a for a in sequence if a!= None]))\n\t\telse:\n\t\t\treturn [ _validatesequence(a) for a in sequence]\n\n\n# ---------------------------------------------------------------------\n# Adafruit Davis:\n#    (C) Valley Khan, http://www.avstix.net\n#    (C) Valley Khan, University of V"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif not output:\n\t\treturn sequence\n\t\t\n\ttry:\n\t\toutput.insert(0, (len(sequence)!= 4 and sequence or None))[0]\n\t\tif sequence:\n\t\t\toutput.insert(0, (output[4] is not None and output[4] or None) and output[4].endswith('')])\n\t\t\n\t\telse:\n\t\t\toutput.insert(0, (output[4] is not None and output[4] or None) and output[4] == '')\n\t\t\n\t\treturn ''.join(output)\n\t\n\texcept:\n\t\ttry:\n\t\t\treturn ''."}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif not output:\n\t\toutput = _normalizeargs(sequence)\n\t\n\ttry:\n\t\tout = [x.normalize() for x in sequence]\n\texcept KeyError:\n\t\tpass\n\telse:\n\t\tout = [_normalizeargs(x, output) for x in sequence]\n\t\n\treturn out\n\n\t\t\n\t\ndef _get_param_fromdoc(parameters):\n\t#TODO: Check parameters before returning?\n\treturn parameters.get(NO_PARAMETER_VAR, None)\n\n\ndef _get_param_fromdoc_arg(parameter):\n\t#TODO: Check parameter"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\tself.parser = ArgumentParser(description='Normalize declaration arguments')\n\t\tself.parser.add_argument('-f', '--field', required=True, help='field to normalize')\n\t\tself.parser.add_argument('-i', '--identifier', required=True, help='identifier')\n\t\tself.parser.add_argument('-v', '--verbose', action='store_true', help='be verbose')\n\t\tself.parser.add_argument('-e', '--error-level',  metavar='ERRORLEVEL', help='e-mail level')\n\t\tself.parser.add_argument('-d"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\t# TODO: This should only be used for simple cases when they are \n\t\t# just a single interface. In particular, if it's a single\n\t\t# interface, then there is no need to normalize the\n\t\t# interface to an interface for that interface. This might seem\n\t\t# worth it!\n\t\tret = []\n\t\tif not output:\n\t\t\toutput = []\n\t\tfor spec in sequence:\n\t\t\tif hasattr(spec, \"Normalize\"):\n\t\t\t\tfor component in spec:\n\t\t\t\t\tcomponent = component.Normalize(returntype=spec.returntype)\n\t\t\t\t\tif returntype in (set"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\t# Normalize declarations based on type and type specifications.\n\t## FIXME: This should not be hardcoded, as no reason to force them to\n\t## be consistent and not always possible.\n\t# \n\t# Note, to prevent infinite recursion, I really don't believe we\n\t## will need to add it if it wasn't a problem.\n\t#\n\t# First, convert the first, because we want to replace all elements\n\t# where in the C++ implementation (or so it's the end of the\n\t# object model it shouldn't be used). So basically try"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif isinstance(sequence, ClassDefinition):\n\t\tclassDefinitionName, moduleName = sequence.name.split('.')\n\telse:\n\t\tclassDefinitionName = moduleName = None\n\t\n\t# Check that the parameters are of the correct type\n\tif len(args) < len(classDefinitionName) + 1: \n\t\traise InvalidArgument(\"Incorrect number of arguments to normalize: \" + \\\n\t\t\tstr(len(args)) + \" - \" + str(len(classDefinitionName)))\n\t# And then convert to a list of instances of the class, and finally\n\t# append the original"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t\n\tif isinstance(sequence, basestring):\n\t\tsequence = [sequence]\n\tif issubclass(sequence, types.MethodType):\n\t\tdef wrapper(self, *args, **kwds):\n\t\t\tdef _fixargs(*args):\n\t\t\t\tif args[1] is None:\n\t\t\t\t\targs[1] = []\n\t\t\treturn _fixargs\n\t\treturn wrapper\n\n\telif isinstance(sequence, types.StringType):\n\t\tSequence = types.MethodType(_normalizeargs, sequence)\n\n\n\tif output is None:\n\t\tresult = []\n\telse:\n\t\tresult = _normalizeargs(output, sequence"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n# Import here only to avoid import errors if there are\n\t# non-standard dependencies.\nimport CN3\n\n# Try to import CN3 if available\nif _c_optimizations_available():\n\treturn CN3\n\n\n# Try to import CN3 with an incompatible package.\nif not hasattr(CN3.init, \"C\"):\n\ttry:\n\t\traise ImportError\nexcept:\n\tpass\n\n# Try to import CN3 with a wrong package name.\nif not hasattr(CN3.init, \"N_INVALID\"):\n\traise"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\treturn True\n\t\ndef _c_solver_available():\n\t\"\"\"\n\tReturns the C solver, if available, otherwise\n\ta false value.\n\t\n\tIf the solver module doesn't appear to be able to be\n\tfound, this raises a ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\ndef _c_optimization_available():\n\t\"\"\"\n\tReturns the C optimization module, if available, otherwise\n\ta False value.\n\t\n\tIf the optimization module doesn't appear to be able\n\tto be found, this raises a ImportError.\n\t\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\t__import__(\"c_optimizations\").optimize_available = False\n\t\ttry:\n\t\t\treturn __import__(\"c-optimizations\").c_optimizations\n\t\texcept ImportError as error:\n\t\t\tif not error.args[0].startswith(\"c_optimizations.\"):\n\t\t\t\traise error\n\n#######################################################################\n\nclass COptimizer(object):\n\t\"\"\"\\\n\tClass implementing a simple C-algorithm (or c_optimizations) and optimization.\n\t\n\tThis function implements the basic operations like optimizations and\n\toptimizations on a class, and a few useful attributes.\n\t\n\tThis function can be"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\nDEFAULT_OPTIMIZATIONS = [\n\t\"libcudnn3.cudnn3_rnn_basic_ops\",\n\t\"libcudnn3.cudnn3_rnn_pool_impl_rnn_ops\",\n\t\"libcudnn3.cudnn3_rnn_conv2d_ops_rnn_ops\",\n\t\"libcudnn3.cudnn3_rnn_conv2d_block_ops_rnn_ops\",\n\t\"libcudnn3.cudnn3_rnn_conv2d_block_op_rnn_ops\",\n\t\"libcudnn3.cudnn3_rnn_pool"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\tif numpy.distutils.misc.find_config_var('MOLECULEMING_ENABLED'):\n\t\t\tmod = __import__('MOLECULEMING_API')\n\t\telse:\n\t\t\tmod = __import__('MOLECULEMING_API')\n\t\n\telse:\n\t\tmod = __import__('MOLECULEMING_API')\n\n\tif mod not in ('numpy','scipy'):\n\t\traise ImportError('\"MOLECULEMING_API\" is not available.')\n\n\treturn mod\n\n\n__all__ = ['build_py']\n#!/usr"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\tfrom scapy.all import *\n\telse:\n\t\t\t# don't try for anything about anything else, just for the\n\t\t\t# current implementation: do it manually...\n\t\t\traise ImportError(\n\t\t\t\t\"To use optimizations, please install a higher version.\"\n\t\t\t)\n\ndef _setup_debug_level(c, debug_level):\n\t\"\"\"\n\tSetup a DebugLevelWrapper instance for all optimizations.\n\t\n\tThis is only used when debugging this module or not.\n\t\"\"\"\n\n\tif debug_level >= 2 and c.optimization_enabled and \\\n\t\t(c"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\treturn _get_optimization_module()\n\ndef _c_optimization_available():\n\t\"\"\"\n\tReturn whether the optimization modules were requested for\n\tthis implementation.\n\n\tThis is a handy to check whether the optimization modules\n\trequire that they are available for that interface.\n\t\"\"\"\n\t\n\t\n\treturn True\n\ndef enable_optimization(optimization):\n\t\"\"\"\n\tEnable the optimization module and its configuration.\n\t\n\t\"\"\"\n\t\n\t\n\tif not hasattr(optimization, '__name__'):\t\n\t\tlog.warn(\"Optimization in use of ObjectiveSolver is currently \" + \\\n\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\nif _c_optimizations_available():\n\timport numpy as np\n\treturn True\n\n### MAIN ###\n\n# Check if the version of scipy is available.\ntry:\n    _c_optimizations_available()\nexcept ImportError:\n    if 'NUMPY_SETUP_WARNING' in _sys.modules:\n        warnings.warn('scipy is missing, but the module'\n                    'is missing, and it is required.')\n    \n\n### VERSION ###\n\n__all__ = ['gen_versions','Version'] + ( ['gen_version','__version__'] +\n                             _c_optim"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\ttry:\n\t\t\ttry:\n\t\t\t\timport optim\n\t\t\texcept ImportError:\n\t\t\t\traise ImportError\n\t\n\telse:\n\t\t# If the optimizations are required,\n\t\t# return \"builtin\" (i.e, if it exists) or None\n\t\t# instead.\n\t\ttry:\n\t\t\treturn _c_optimizations['builtin']\n\t\texcept KeyError:\n\t\t\treturn None\n\n_c_optimizations_available()\n\t\n\t\nif _os_crayon_platform == 'darwin' and _os_osx_platform == 'win32':\n\t_c_optimizations = ['c"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif (distutils.spawn.find_executable('c') and\n\t\tdistutils.spawn.find_executable('c++') and\n\t\tdistutils.spawn.find_executable('gcc') and\n\t\tdistutils.spawn.find_executable('m') and\n\t\tdistutils.spawn.find_executable('msp') and\n\t\tdistutils.spawn.find_executable('mrt') and\n\t\tdistutils.spawn.find_executable('muk') and\n\t\tdistutils.spawn.find_executable('mukle') and\n\t\tdistutils.spawn.find_executable('mus"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\tis_python = sys.version_info[0] == 3\n\t\tif is_python:\n\t\t\t# If Python 3, it returns 3.\n\t\t\treturn sys.version_info[1] == 3\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn sys.implementation.eval('USE_PYTHON')\n\t\t\texcept:\n\t\t\t\treturn False\n\treturn _should_attempt_c_optimizations\n\n\ndef _use_c_impl(impl_or_func, *args, **extra):\n\treturn not isinstance(impl_or_function, CCompiler.Executable)\n\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\ndef _use_c_impl():\n\t\"\"\"\n\tReturns whether we're on CPython and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n#if __name__ == '__main__':\n\n## This file is part of Invenio.\n## Copyright (C) 2007, 2008, 2010, 2011 CERN.\n##\n## Invenio is free software; you can redistribute it and/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn _USE_C_IN_PY\n\telse:\n\t\treturn _USE_C_FUTURE\n\ndef _get_py_optimized(opt, val, opt_dict):\n\t\"\"\"\n\tA decorator for setting the C optimized optimizations. This function replaces\n\tthe optimized optimizations that were supplied by the user, but that would\n\tnot take into account the option being set again. This allows certain optimizations\n\tto be set using the c_optimizations or c_shared optimization machinery.\n\t\n\tParameters\n\t**opt: dictionary with optimizations.\n\t**val"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn _use_c_impl\n\n_USE_C_IMPORTER = _determine_prefer_c_optimizations()\n\n\n#\n# Internal routines\n#\n\nimport io.StringIO\nimport re\nimport sys\n\nfrom pydispatch._i18n import _\nfrom _cython import (assert_extension_flag, is_extension_scope,\n                       is_cython_extension)\n\ndef _get_from_file(fname, is_cython=False):\n\ttry:\n\t\tfile = io.StringIO.StringIO(fname)\n\t\treturn file\n\texcept AttributeError"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\tif not py_implementation_enabled:\n\t\t\tassert False, 'pip is not installed', 'Could not use C optimizations, please install it'\n\t\telse:\n\t\t\treturn True\n\t# end _should_attempt_c_optimizations\n\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturns the boolean value for the c_compiler in use.\n\t\"\"\"\n\t\treturn py_implementation_enabled and 'c' in find_executable('python')\n\t# end _use_c_impl\n\n\ndef _load_dist_packages():\n\t\"\"\"\n\tLoad"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn True\n\t\t\t\t\n#=============================================================================================\r\n# Imports\r\n#=============================================================================================\r\nimport wx\r\nfrom ete2 import Config\r\nfrom ete2.ui.common import CommonUtils\r\nimport _common_utils as cu\r\n\r\n# ----------------------------------------------------------------------------\r\n# Create the window\r\n#=============================================================================================\r\nclass _Window(wx.WindowBase):\r\n    \"\"\"\r\n    The main window for the ete2 platform.  Will call the\r\n    base class base class for this class to instantiate the panel.\r\n\r\n    :param parent: The parent window.\r\n    :param title: The title text to display in"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn _USE_C_OPTIMIZATION in os.environ and \\\n\t\t\t'PYPYOPC' in os.environ\n\n@_should_attempt_c_optimizations\ndef _use_c_impl():\n\treturn _USE_C_NOOPC!= '1'\n\n@_should_attempt_c_optimizations\ndef _use_py_optimized():\n\treturn _USE_PYPYOPC\n\n@_should_attempt_c_optimizations\ndef _use_python_optimized():\n\treturn _USE_PYTHONOPC\n\ndef set_pure"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn os.environ['PYPY_INSTALL_C'] is not None\n\ndef _initialize_c_impl(p, **kwargs):\n\timport _use_c_impl\n\tp.__dict__[\"c_lib_path\"] = _use_c_impl\n\tdel p._use_c_impl\n\treturn p\n\ndef _init_cpp_impl(p):\n\timport _use_cpp_impl\n\treturn _use_cpp_impl\n\n_py2_implementation = python_implementation(_should_attempt_c_optimizations)\n_cpp"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\twith _patch_c_impl(PURE_PYTHON) as mock_c_impl:\n\t\t\tfrom pymplexceptions import CMPyDeprecationWarning\n\t\t\treturn mock_c_impl.mock_calls\n\n\tdef _should_c_optimizations():\n\t\t\"\"\"\n\t\tReturn a true value if we should attempt to use the C optimizations.\n\t\t\n\t\tThis takes into account whether we're on PyPy and the value of the\n\t\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\t\"\"\"\n\t\twith _patch_c_impl(P"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\treturn use_c_python\n\t\n\n# For the test suite, the following globals aren't used by any test\n# modules themselves.\nuse_c_impl = {'PURE_PYTHON': None,\n\t\t\t\t     'USE_CLANG_EXC': None,\n\t\t\t\t     'USE_PYPY': None,\n\t\t\t\t     'USE_CUDA_EXPERIMENTAL': None,\n\t\t}\n\n# This is used to avoid having to overwrite the environment's\n# variables\nuse_cudnn = {}\nuse_ptx = False # Use cuDNN if it fails to import\ncudnn_enabled"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\tn += 1\n\ndef _c_error_message(error_message=None):\n\t\"\"\"\n\tAn exception raised during optimization.\n\t\"\"\"\n\n\t\tglobal _c_error_messages\n\n\t\tif error_message:\n\t\t\tif error_message == \"optimizing method\":\n\t\t\t\traise OptimizerException(\"optimizing method is not yet implemented, but it \" \\\n\t\t\t\t\t\t\t\"is currently not supported.\")\n\t\t\telse:\n\t\t\t\traise OptimizerException(error_message)\n\n\treturn n\n\nclass OptimizerException(Exception):\n\tpass\n\ndef _c_error_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\nclass _c_optimizations_required(object):\n\t\"\"\"\n\tA decorator that is used to prevent unnecessary calls to\n\tC++ optimizations.\n\n\tThis class is used to prevent unnecessary calls to\n\tC++ optimizations by simply evaluating \"optimizations\" to set\n\t'options' in the function body if the parameters are not listed in\n\t'options'.\n\n\t@type: bool\n\n\t@return: A C++ optimizations function, which\n\t\tprovides the \"options\" argument.\n\t@rtype: tuple\n\n\t\"\"\"\n\t@classmethod\n\tdef _c_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n#def _c_optimizations_missing():\n#\t\"\"\"\n#\tThe opposite of `_c_optimizations_required`.\n#\t\"\"\"\n#\n##\n#\t_c_optimizations_required = _c_optimizations_required()\n#\n##\n\n#def _c_optimizations_required(all_optimizations, *args, **kwargs):\n#\t\"\"\"\n#\tA decorator which wraps a decorator for all optimizations in the \n#\t`pychess.chess_base.chess_base.optimizations.optimizations_required"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n\"\"\"\n* A function\n\n* _add_argument(fn,\n\t#\t\t#\t\t#\n\t#\t#\n\t#\t# The following parameters are passed to the function\n\t#\t__call__\n)\n\n\ndef _c_opt_actions():\n  \"\"\"\n\tThe opposite of `_c_optimizations_optional`.\n\t\"\"\"\n\t\n\"\"\"\n* A function\n\n* _add_argument(fn,\n\t#\t\t#\n\t#\t# The following parameters are passed to the function\n\t#\t__call__\n)\n\n\n\"\"\""}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\tpass\n\t\n# coding: utf-8\nimport hashlib\nimport os\nimport urllib2\nimport socket\nimport struct\nimport httplib\nimport re\nimport json\nimport time\nimport traceback\nimport traceback\nfrom datetime import datetime\n\nfrom.common import InfoExtractor, BaseInfoExtractor\nfrom..compat import (\n    compat_HTTPError,\n    compat_urlparse,\n    compat_urllib_parse_urlencode,\n    compat_parse_qs,\n    compat_urlparse_urlparse,\n)\nfrom..utils import (\n    encodeFilename,\n    encodeFilename_compat,\n)\n\n\nclass RssIE(InfoExtractor):\n   "}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\nclass _Optimizer(object):\n\t\"\"\"\n\tA class for performing the different operations on the argument\n\tspecified, which makes things easier to debug.\n\t\"\"\"\n\n\tdef __init__(self, op, *names, **_kwargs):\n\t\t\"\"\"\n\t\tInitialize a new Optimized object.\n\n\t\tParameters\n\t\t----------\n\t\top : any\n\t\t\tThe argument to optimize, a string.\n\t\tnames : iterable\n\t\t\tA string or iterable of strings that specify the allowed\n\t\t\tnames (optional). Each element of name must be in the\n\t\t\tallowed names of the Optimizer.\n\t\t\t("}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\nclass _c_optimizations_required(_c_optimizations):\n\tdef __init__(self, context):\n\t\tpass\nsuper(_c_optimizations_required, _c_optimizations_required).__base__.__defaults__ =  (\n\t\t__defaultParams_not_allowed,\n\t\t_c_optimizations)\n\n\nclass _c_optimizations_optional(object):\n\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\nclass _c_optimizations_optional_not_allowed(object):\n\tdef __init__(self,"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\tfor i in range(len(_args) - 2):\n\t\t\tif _args[i] is not None:\n\t\t\t\t_args[i + 1] = None\n\n\tif _check_args_not_empty(_args)!=\"_c_optimizations_required\":\n\t\t\tlog.error(\"Function doesn't return any values.\", _func_name='c_optimizations_required', exc_info=True)\n\n\t\t# Add the input parameters to the return value.\n\t\tfor i in range(len(_args) - 2, -1, -1):\n\t\t\tif _args"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\treturn False\n\treturn False\n#-----------------------------------------------------------------------------\n\ndef _c_optimizations_required():\n\t\"\"\"\n\t\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\treturn True\n#-----------------------------------------------------------------------------\n\n# TODO: _saves/reads out of _files, _directories in the way, rather than asks for that.\n# We may want to set _directories directly, which is not actually needed for things like\n# _directories or _pathfiles within an object that is only defined in this module.\n\n__all__ = ['savez_config_string', 'load',"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t\n    #def _c_optimizations_required():\n\tpass\n\n#\n# For convenience below this function is no longer used.\n#      \n_c_optimizations_default = _c_default.c_optimizations_added\n\n#def _c_default_factory(cls):\n#\t\"\"\"\n#\tReturns a factory function defined by the default C type.\n#\t\"\"\"\n#\t_c_default_factory = type(_c_default.new_c_type())\n#\treturn _c_default_factory\n\n#def _get_function_"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\treturn CONFIG['USE_C']['OPTIMIZATIONS'] if is_c_impl(config) else 0\n\n\ndef _parse_with(cfg):\n\t\"\"\"\n\t\tParse a C configuration file in a dict. This is a list of key/values that \n\t\tcannot be specified in the C config file itself, which are then left \n\t\tover in the given order.\n\t\t\n\t\tThe keys and values that are not allowed are the same as the keys and values of\n\t\teach other, and the values that are allowed are the same as the values of\n\t\tthe keys and values of each other."}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn _c_optimizations_required()\n\ndef build(module=None):\n\t\"\"\"\n\tCreate the given module.\n\t\"\"\"\n\t\n\treturn _create_module_from_config(module, ['PURE'], ('PURE_PYTHON',))\n\n#---- functions ---\n\n#---- _use_c_impl --- a hook for use by code generator\n\n\nimport time\nimport os\nimport math\n\ndef sloveness(data, weight):\n    # This is in the implementation of the standard L2 normality problem, \n    # given both the training samples and"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\n\tc_optimizations_required= 1 # in case of \"C\" optimizations.\n\n\tfrom _ctypes import c_function, c_borrowed_ctype, pointer, Structure, Structure, void_p, c_long, c_int\n\n\tif c_function.restype == c_void_p:\n\n\t\tc_optimizations_required = (0==_c_optimizations_required) # for C\n\telse:\n\t\t\tc_optimizations_required=(1==_c_optimizations_required)\n\n\n\n\treturn c_optimizations_required\n\n\n\ndef"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n#\t#\tset up the C compiler\n#\t\tcompiler_type = 'none'\t# set up the c compiler\n#\t\tcompiler_name = 'libpq'\t# set up py-c++ compiler\n#\t\tcompiler_version = '3.11.0'\t# override default PurePython version\n#\t\tcompiler_implementation = \\\n\t\t\t\t\t\t\t\t\t\t\t\t\tget_compiler_type()\n#\t\tif (compiler_implementation == 'none') or (compiler_implementation is None):\t# pragma: no cover\n\t\t#\t\traise ValueError, 'Missing"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\tif _USE_C_IMPLIMATE:\n\t\t\t# Use C optimizations first\n\t\t\treturn _c_impl.USE_C_IMPLIMATE\n\t\telse:\n\t\t\t# Use C optimizations\n\t\t\treturn _c_impl.USE_C_IMPLIMATE\n\t\n\t\nc_optimizations = _USE_C_IMPLIMATE\n\ndef _c_optimizations_supported():\n\t\"\"\"\n\tReturn a boolean if the C optimizations are supported or not.\n\t\n\tThis will return a boolean when the current :class:`libpq.core.core_core` impl \n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n_c_optimizations = getattr(_c_optimizations_module, 'c_optimizations')\n_c_optimizer_module = _c_optimizations['PURE_PYTHON']\n\t\n_c_optimizeizations_in_module = getattr(_c_optimizeizations_module, 'c_optimizations_in_module')\n\t\n\tglobal _c_optimizations_in_module\n\tif not callable(_c_optimizations_in_module) and not _c_optimizations_in_module.is_static() and _c_optimizations_in_module.is_"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn (get_c_optimizer(), CUDA_OPTIMIZATIONS_REQUIRED)\n\ndef _use_c_impl(fnames):\n\t\"\"\"\n\tSet a single function to be used on CPU-compiles.\n\t\"\"\"\n\t\n\tfor f in fnames:\n\t\t_use_c_impl[f] = 1\n\t\n# \ndef _check_all_function(func):\n\t\"\"\"\n\tReturn ``True`` if all required functions are available.\n\t\n\tAll functions are used, but no more specific to that function.\n\t\n\tExceptions:\n\t\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\n# \t\t\n# This is the `numpy` test. For this reason, it has to be checked against numpy.\n# \t\t\n# \t\t\ndef _use_numpy_impl(a):\n\tfrom numpy.testing.utils import (assert_, assert_allclose, assert_array_almost_equal,\n\t\tassert_raises, assert_array_equal, array, b, place=place)\n\tfrom numpy.core._multiarray_tests import (assert_no_overwrite, _simple_new_call,\n\tassert_raises, _multiarray_test_func,"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\t\n\treturn _use_c_impl(sys.platform==\"darwin\")\n\n\nclass _c_runtime_environment_dependencies(object):\n\t\"\"\"\n\tFor use in :mod:`distutils.sysconfig` and :mod:`distutils.pyconfig`.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\ttry:\n\t\t\timport plib\n\t\texcept ImportError:\n\t\t\traise SystemError(\"Missing 'plib' Python library.\")\n\n\t\tkwargs.setdefault('pre_compile', getattr(_plib, 'pre_compile', False))\n\t\tkwargs.setdefault('post"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn True\n\n## \n# --- The C implementation which can be integrated with PURE_PYTHON, is currently an error. \n# --- \n# --- An error occurred while trying to call `eval` will be raised when an exception is thrown.\n\nclass _Exception(Exception):\n\t\"\"\"\n\tFor errors in `eval` if the expression of a given condition can not be found.\n\t\"\"\"\n\t\n\tdef __init__(self, expression):\n\t\tself.expr = expression\n# Copyright (c) 2010 University of Maryland.\n# \n# Permission"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n# Reset all bins in hist\ndef reset_all():\n\t\"\"\"\n\tReset all bins to a single value.\n\t\n\tCurrent context is reset to a single value.\n\t\"\"\"\n\t\n###\n\n###\n\ndef _read_from_file(path):\n\t\"\"\" Returns the contents of a certain file.\"\"\"\n\t\n\ttext = _read_into_file(path, 'text')\n\treturn text.split('\\t')\n\n\n###\n\ndef _read_into_file(path, text):\t\n\t\"\"\" Reads in a file and returns the contents of it.\n\t\n\t"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\nclass Histogram(object):\n\t\"\"\"\n\tPlot histogram.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the histogram\n\t\t\"\"\"\n\t\t\n\t\tself.x = [0, 0]\n\t\tself.y = [0, 0]\n\t\tself.bins = [0]\n\t\tself.init_value = None\n\t\tself.bin_centers = [None]\n\n\tdef init_value(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInitialise the histogram value.\n\t\t\"\"\"\n\t\t\n\t\tself.init_value = numpy.zeros"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t   \tself.current_context = { \n\t\t    'bins': None\n\t\t      }\n\t \t\n\t   \t\n\t\t\t# Initialize empty values\n\t   \t\n\t   \tself.current_context['bins'] = [0,0]\n\t   \t\n\t   \t# For one iteration\n\t   \t\n\t\t# Set initial size of histogram\n\t\tself.current_context['bins'][0] = 25\n\t\tself.current_context['bins'][1] = 200\n\t\tself.current_context['bins'][2] = 100\n\t\tself.current_context['bins'][3] = 200\n\t\tself.current_context['"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\tself.initial_value=dict()\n\t\tself.make_bins=False\n\t\t\n\tdef make_new_minima_(self):\n\t\tif self.getMinImage() is None:\n\t\t\tself.make_new_minima_=numpy.zeros(self.shape, dtype=numpy.float32, order='F')\n\tdef getMinImage(self):\n\t\treturn self.getMinImage()\n\tdef get_new_minima(self, iters=1000, min_=0.0):\n\t\tif min()<min_:\n\t\t\tmin_=min\n\t\t"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\tfor item in self.__dict__.keys():\n\t\tif item == \"_initial_value\":\n\t\t\tself.reset_initial_value()\n\t\telif item == 'bins':\n\t\t\tif self.__dict__.has_key(item):\n\t\t\t\t#print \"Bins in \"+self.__dict__[item][\"bins\"]\n\t\t\t\tfor i in range(len(self.__dict__[item][\"bins\"])-1):\n\t\t\t\t\tself.__dict__[item][\"bins\"][i] = 0\n\t\t\telse:\n\t\t\t\tprint \"Could not assign histogram value to histogram\"\n\t\t\t\treturn\n\t\t\t\t\n\t\tif self.__"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t    \n\tdef update(self, bins):\n\t\t\"\"\"\n\t\tUpdate the histogram.\n\t\t\n\t\t**Parameters**:\n\t\t\n\t\t* *bins* : Dictionary mapping int to float in the form\n\t\t  {'min': 0.0,'max': 1.0}, or an integer or \n\t\t   float of the minimum and maximum value of the histogram,\n\t\t  where:\n\t\t  - min < 0 (default)\n\t\t  - max > 1 (default)\n\t\t  - default > 0 (default)\n\t\t\n\t\t**Returns**:\n\t\t\n\t\t* *hist_values* : The histogram values, which are the histogram\n\t\t   values in the range [-1"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\tb[self._current_ctx] = initValue(*self._initial_values)\n\tdef reset_value(self, *bins, *value):\n\t\tb[self._current_ctx] = initValue(*self._initial_values)(bins, value)\n\tdef reset_bins(self, *bins, *bins_value):\n\t\t# For the brewer's defaults, we cannot really reset between bin boundaries\n\t\t# so we can only set one of those bins.\n\t\tif \"bins\" not in self._current_ctx:\n\t\t\tself._current_ctx."}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n#\t\tprint \"Reset histogram\"\n\n        self.histogram = dict()\n        self.histogram_update()\n\n#\t\t#self.histogram_update()\n\n        print(\"Current bin histogram is reset.\")\n\treturn self\n        \n\n#        self.hist_update()\n        \n        if self.output_frequency % 2 == 0:\n            self.hist_update2()\n\n\n#\t\tprint \"Intial values for histogram:\"\n#\t\tfor k, v in self.histogram.items():\n#\t\t\tprint k, self.hist[k]\n#\t\t\tself.hist.pop"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\t\n\tself._initial_value = {}\n\t\n\t# Remove the bins (if provided).\n\tif self._bins is None:\n\t\tself._initial_value['bins'] = []\n\telse:\n\t\tself._initial_value['bins'].extend(self._bins)\n\t\t\n\t\n\t# This needs to be implemented by subclasses, and not per-subclass.\n\t# It should be reimplemented by subclasses.\n\t# The default is the *default* one for any initial value\n\t# of the histogram.\n\t# This is necessary in cases where a histogram does not have a default initial"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tclass _hist_init(object):\n\t\tdef __init__(self, index=[], initial_value=[]):\n\t\t\tself.index = len(index)\n\t\t\tself.initial_value = initial_value\n\t\t\tself.bins, self.bins_init = initialize_histogram(initial_value)\n\n\tdef __init__(self, current_context):\n\t\ttry:\n\t\t\tself._hist_init(self, current_context)\n\t\texcept:\n\t\t\tself._init_context_to_zeros_hist()\n\n\tdef _init_context_to_zeros_hist(self"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tdef _is_valid(point):\n\t\tif not isinstance(point, Node) or type(point) is not Node:\n\t\t\treturn False\n\t\t\n\t\tfor dim in point.dim:\n\t\t\tif (isinstance(dim, tuple) and len(dim) == 2) or\\\n\t\t\t\t   (isinstance(dim, int) and dim > 0 and not _is_valid(dim)):\n\t\t\t\treturn True\n\t\t\n\t\treturn False\n\t\n\t# this function exists for backwards compatibility\n\t# so can be removed in 0.5\n\tdef to_csv_csv(self, separator=\",\", prefix=\"\","}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\tif not header: self.write_rows([\n\t\t\t(\"ID\", self.ID),\n\t\t\t(\"type\", \"Point\"),\n\t\t\t(\"value\", [self.graph.point[0], self.graph.point[1]])\n\t\t])\n\t\tfor row in self.write(separator):\n\t\t\tself.write(row)\n\t\n\t# TODO: Use write() method for a more explicit set of outputs and outputs\n\tdef write(self, row):\n\t\tif self.is_empty():\n\t\t\treturn\n\t\t\n\t\tdata = OrderedDict()\n\t\t\n\t\tdata[\"ID\"] = format_id("}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tself._to_csv()\n\twriter = StringIO()\n\trow = self._header + separator.join([str(x) for x in tuple(self._points).values()])\n\twriter.write(row.encode())\n\treturn writer.getvalue() + \"\\r\\n\"\n\n@_doc(repr(Point))\ndef _pprint(points):\n\t\"\"\"\n\tHelper for ``Points`` plotting functionality.\n\t\n\tReturns a Pandas Dataframe, which is the same format as\n\t``numpy.array()`` that can be written to disk and accessible via\n\tthe :"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\tfrom math import ceil\n\n\tself._output = Table()\n\t\n\tif not header: header = [\"Points\", \"Coordinate Size\"]\n\n\tfor point in self.points:\n\t\tpoint.points = self._points = list(point.points)\n\t\tif not isinstance(point.coordinates, (list, tuple)):\n\t\t\tpoint.coordinates = (point.coordinates, )\n\t\tpoint.coordinates = sorted(tuple(point.coordinates))\n\t\tpoint.coordinates = [(point.coordinates[i], point.coordinates[j]) for (i, j) in enumerate(point.coordinates)]\n\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tif header is None:\n\t\theader = self._separator\n\t\n\toutput = \"\\n\".join([to_csv(p) for p in self._points])\n\tif self.header is None:\n\t\theader = self.__class__.__name__\n\toutput += separator+\"{0}\\n\".format(header)\n\treturn output\n\t\t\n        # Amara, universalsubtitles.org\n#\n# Copyright (C) 2011 Participatory Culture Foundation\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\treturn self.to_csv_list(self.iter_points(), separator)\n\t\t\n\tdef iter_points(self):\n\t\ttry:\n\t\t\treturn (o for o in \n\t\t\t\t\t  self.list_of_points() if \n\t\t\t\t\t  len(o.vertices) == len(self.vertices))\n\t\texcept AttributeError:\n\t\t\traise ValueError(\"list of points not iterables\")\n\t\n\tdef all_vertices(self):\n\t\t\"\"\"return all vertices at the end of the list\"\"\"\n\t\treturn self.vertices\n\t\n\tdef all_to_csv(self, fileobject, delimiter=\","}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\tresult = \"\"\n\t\n#\treturn result\n#\t\"\"\"\n#\t>>> g = Graph.from_dataframe(results) ## \n#\t\"\"\"\n#\t>>> result = g.to_csv(sep=\"\") \t ## \n#\t\"\"\"\n#\t>>> import StringIO \n\nclass ToCSV(object):\n    \"\"\"\n    From :class:`graphlib.graph` to a new :class:`Graph`.\n\n    \"\"\"\n    def __init__(self, results):\n        self.results = results\n    \n    def to_csv(self, separator=\";\", header=None):\n\t\"\"\"\n\t.."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\tif header is None:\n\t\t\theader = self.csvheader\n\t\treturn csv.writer(self.output, delimiter=separator,\n\t\t                        skipinitialspace=True, \n\t\t                        newline=\"\\n\")\n\t\n\t@staticmethod\n\tdef from_csv(reader=None, header=None):\n\t\"\"\"\n\tReads a list of points from a CSV, into the graph.\n\t\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\t*header*, if not ``None``, is the first string of the output\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\ndef to_html(graph, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_html is not used.\n\t\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to HTML.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header* if not ``None``, is the first string of the output\n\t(new line is modified automatically), else None is built up\n\tfrom the lines as of 2008.\n\t\"\"\"\n\t\ndef from_html(graph, separator=\",\", header="}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t\n\tif header is None:\n\t\theader = lena.output_table\t\n\tif isinstance(header, string_types):\n\t\ttry:\n\t\t\theader = header.split(',')\n\t\t\theader = header[0];\n\t\t\theader = [x.strip() for x in header]\n\t\t\t\t\n\t\texcept TypeError:\n\t\t\theader=[header]\n\theader = \",\".join(header)\n\tresult = []\n\tfor e in header:\n\t\td = tuple([x.strip() for x in e.split(\"=\").split(\",\")])\n\t\td = tuple([d[0]."}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\treturn np.where(coord_name == 'X', 0, -1, 0).shape[0]\n\nclass AxisLabel(object):\n\tdef __init__(self, axis, name='AxisLabel', offset=(0, 0)):\n\t\t\"\"\"\n\t\tInitialize AxisLabel.\n\t\toffset is a tuple of four positive floats representing the\n\t\toffset of the current axis in the axis label.  This offset can\n\t\tbe passed, in which case there is no offset.  You must do\n\t\tnot call this constructor directly, instead, modify the\n\t\taxis label and offset.  See"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n\tcoord_file_path = coord_name.split('coord1')\n\tcoord_file_path_len_str = len(coord_file_path[0])\n\tcoord_file_path = coord_file_path_len_str * coord_file_path[2:]\n\tif len(coord_file_path)!= 3:\n\t\traise ValueError(\"Wrong number of coord files.\")\n\tcoord_file_path = \" \".join([coord_file_path[i:i + coord_file_path_len_str]\n\t\t\t\t\t\t\t\t\t\t\tfor i in"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\treturn self._cache.get_err_indices(coord_name)\n\n\tdef _get_err_vals(self, coord_name, key):\n\t\tself._cache.get(key, {}).update({key: [],\n\t\t\t\t'required': False,\n\t\t\t\t'service_type': None})\n\n\t\treturn _values_to_indices(value or [])\n\nclass _LogReader(object):\n\t\"\"\"\n\tA reader for log files stored in a file.\n\t\"\"\"\n\tdef __init__(self, filename, format):\n\t\t\"\"\"\n\t\tThe file can contain"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\treturn self._error_indices[coord_name]\n\t\n\tdef get_error_indices(self, coord_name):\n\t\"\"\"\n\t\tLookup the error indices corresponding to a CubeCoord.\n\t\t\n\t\t@param coord_name: The name of the dataset column to lookup\n\t\t@type coord_name: str\n\t\t@return: the full error indices corresponding to a given CubeCoord\n\t\t@rtype: L{tuple} of int\n\t\t\"\"\"\n\t\tresult = []\n\t\tfor (error_coord, idx) in self._error_indices.items():\n\t\t\tresult.append("}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\tout = self._error_indices[coord_name]\n\t\tif out!= -1:\n\t\t\treturn out\t\t\t\n\tdef get_indices(self):\n\t\t\"\"\"\n\t\tGet slice of error indices corresponding to the array.\n\t\t\"\"\"\n\t\treturn self._error_indices[:]\n\n\tdef get_min_size(self):\n\t\t\"\"\"\n\t\tGet minimum error size.\n\t\t\"\"\"\n\t\treturn self._min_size[0]\n\n\tdef get_max_size(self):\n\t\t\"\"\"\n\t\tGet maximum error size.\n\t\t\"\"\"\n\t\treturn self._max_"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\tif coord_name not in self.coord_dict:\n\t\t\treturn []\n\n\t\treturn self.coord_dict[coord_name]\n\nclass InsertionMatrix:\n\n\tdef __init__(self, matrix_name):\n\t\tself.matrix_name = matrix_name\n\n\tdef generate(self, num_genes, num_vals, num_sites, num_seqs):\n\t\t\"\"\"\n\t\tGenerate a list of unique insertions for a given matrix\n\t\t@param num_genes:\tnumber of genes\n\t\t@param num_vals:\tnumber of values\n\t\t@param num"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\tif coord_name not in self.data:\n\t\t\traise ValueError(\"There is no coordinate named {}.\".format(coord_name))\n\t\tcoord = self.data[coord_name]\n\t\t\n\t\tif coord.startswith('[C]'):\n\t\t\treturn self.__coordIndices[coord_name]\n\t\telse:\n\t\t\treturn []\n\t\n\tdef __str__(self):\n\t\treturn '\\tname:\\n' + ', '.join([ '{}={!r}, dtype={!r}, dims={!r}'.format(x[0], x[1], dim) for x in self.__coordIndices.values"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\tc_index = ccrs.GeogCS(self.bounds.values())\n\t\treturn c_index\n\n\tdef get_code_idx(self, coord):\n\t\t\"\"\"\n\t\tGet coordinates associated with a coordinate.\n\t\t\"\"\"\n\t\tcoords = self.map_to_code(coord.as_dict())\n\t\tif coords is None:\n\t\t\treturn (None, )\n\t\telse:\n\t\t\treturn coords\n\n_coord_aliases = {\n\t'dears': 'long',\n\t'altitude': 'altazt',\n\t'decond': 'h"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._coord_dim[coord_name]\n\t\texcept IndexError:\n\t\t\treturn []\n\t\t\n\tdef _get_err_index(self, index):\n\t\t\"\"\"\n\t\tGet error index corresponding to a given index.\n\t\t\"\"\"\n\t\tif self._coord_dim.exists(index): \n\t\t\treturn np.argmin(self._coord_dim[index])\n\t\n\tdef __str__(self):\n\t\tif self.num_err == 0.:\n\t\t\treturn \"{} - No errors\".format(self.name)\n\t\telif self.num_err > 1.:"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\tout = np.where(self.dim == coord_name)[0]\n\t\treturn out\n\n\tdef _get_coord_coords(self, coord_name, coord):\n\t\t\"\"\"\n\t\tGet coordinates of a coordinate.\n\t\t\"\"\"\n\n\t\tif self.dim in ['x', 'y']:\n\t\t\tdim = coord if coord in self.coord_names.ravel() else self.coord_names[0]\n\t\telif self.dim in ['z', 't']:\n\t\t\tdim = coord if coord in self.coord_names.ravel() else self.coord_names[1"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\twith self.output_info[self.__class__.__name__] as out:\n\t\t\tfor index, field in enumerate(self.fields):\n\t\t\t\tnew_field = Field(name=field)\n\t\t\t\tif index == len(self.fields) - 1:\n\t\t\t\t\tnew_field[\"x_low\"] = {}\n\t\t\t\telse:\n\t\t\t\t\tnew_field[\"x_low\"] = self.values[index][0]\n\t\t\t\t\tif len(self.values) > index:\n\t\t\t\t\t\tself.values[index] = [(\"\", -1)]\n\n\t\tsuper().__update_context(out)\n\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tif \"E\" not in context.errors:\n\t\t\tcontext.error = {}\n\t\ttry:\n\t\t\tself.graph = _to_graph(self.graph, self.value, self.node_name, self.error, self.name)\n\t\texcept AssertionError as exc:\n\t\t\traise ValueError(exc)\n\n\tdef destruction(self):\n\t\t\"\"\"Returns all the edge labels and the edge data, in\n\t\tthe returned format:\n\n\t\t\t{edge.name: (\"E\",), edge.data: (label, index)}\n\t\t\"\"\"\n\t\treturn {node: (self."}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\t\n\tdef __init__(self, graph, index):\n\t\tself.__graph = graph\n\t\tself.__index = index\n\t\t\n\t\tself.__graph_attributes = None\n\t\t\n\t\tself.__graph_fields = {}\n\t\tself.__graph_index = 0\n\t\t\n\t\t#self.__update_to_graph(graph)\n\t\n\t\n\tdef __setitem__(self, attr, value):\n\t\tif attr not in self.__graph_attributes:\n\t\t\tself.__graph_attributes[attr] = self.__graph.__dict__\n\t\tself.__graph_attributes[attr].update(value)\n\t\t\n\tdef __getitem__("}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tdef __init__(self, graph, key_type, values=None): \n\t\tself.graph = graph\n\t\tself.key_type = key_type\n\t\tself.values = values\n\t\t\n\tdef __getstate__(self):\n\t\tassert self.graph.__dict__.has_key(self.key_type)\n\t\treturn {\"graph_type\": self.graph.__dict__[self.key_type], \"value_type\": self.values[self.key_type]}\n\t\n\tdef __setstate__(self, state):\n\t\tassert state[\"graph_type\"] in (\"toCSV"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\n\tdef __init__(self, value, line, column, row):\n\t\tsuper(ToCSV, self).__init__(value, line, column, row)\n\n\t\t# Convert (or copy) to string values.\n\t\tif self.has_key(value):\n\t\t\tvalue = copy.deepcopy(self[value])\n\n\t\t\t# Get the field, if we can, and index.\n\t\t\tvalue = [e[0] for e in value.items()]\n\t\t\tif len(value) == 2:\n\t\t\t\tvalue = value[0]\n\n\t\t\t# Copy subcontext if it's not"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\tgraph = self._graph\n\t\t#print context.error_index\n\t\tindex = 0\n\t\terror_names = []\n\t\tself._forces = {}\n\t\tfor x in range(self._debug.GetLevel()):\n\t\t\tfor y in range(len(graph.node[x])):\n\t\t\t\tself._forces[x][y] = {\n\t\t\t\t\t\"error\" : 0,\n\t\t\t\t\t\"low\" : 0,\n\t\t\t\t\t\"high\" : 0,\n\t\t\t\t}\n\t\t\t\tgraph.node[x][y].keys.append(\"error_E_low\")\n\t\t\t\tself._"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, error_values):\n\t\t\"\"\"\t\n\n\t\t:param (list) error_values: An array of error indices which can\n\t\t\t\t\t\t\tbe used to select an error index which is returned.\n\t\t\t\t\t\t\t\n\t\t:return: None\n\t\t\"\"\"\n\n\t\tself.error_values = error_values\n\t\t# The default indices used to select an error index\n\t\tself._error_indices = []\n\n\tdef __del__(self):\n\t\t\"\"\"\n\t\tDelete the output of *GraphGraph.__del__()\n\t\t\"\"\"\n\n\t\tdel(self.error_values[0])\n\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tdef __init__(self, context, errors=None):\n\t\t\"\"\"\n\t\tAdd a context to the graph.\n\t\t\"\"\"\n\t\tGraph.__init__(self)\n\t\t#self.graph = Graph()\n\t\tself._errors = { }\n\t\tself._errors[\"\"] = errors or self\n\n\tdef add(self, name, value, index=0):\n\t\tassert len(value) == 1, \"error: expected one value\"\n\t\tassert len(self._errors[\"\"]) == index, \"error: number of errors not found\"\n\t\tif isinstance(value["}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\t# this function changes the values in self.getVarName().\n\t\tself.removeValuesFrom(context.error)\n\t\tself.valueToList(context.value.tolist())\n\t\t\n\t\twith open(\"graph.txt\", \"w\") as outfile:\n\t\t\tfor index in range(len(self.valueToList()) - 1):\n\t\t\t\tfor j in range(2):\n\t\t\t\t\tif self.valueToList()[j] in [\"X\", \"T\"]:\n\t\t\t\t\t\tj += 1\n\t\t\t\t\t\toutfile.write(\"[{} {}]\\n\".format(index, j))\n\t\t\n\t\tcontext.value ="}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\tself.errors = {\n\t\t\"x_low\": (float, dict),\n\t\t\"y_low\": (float, dict),\n\t\t\"error_E_low\": (lambda value: (1.0*value)/(1.0*len(value)-0.001))\n\t}\n\t\"\"\"\n\tThe names of \"x\", \"y\", \"z\" are called form \"x\", \"y,z\". If\n\tused as an input to the graph, field \"x\", is converted to a floating\n\tpoint, and \"z\" is converted to a float"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\thistogram_desc = \"\"\"\n\tComputes the integral for the given bins.\n\t\"\"\"\n\t_integral = []\n\t_integral_val = 0\n\t\n\tfor x,y,label in bins:\n\t\tx_label = bin_label_to_label(x)\n\t\ty_label = bin_label_to_label(y)\n\t\ttry:\n\t\t\n\t\t\tif len(bin_name_for_node(label))==2:\n\t\t\t\tbin_id = label[0].split('.')\n\t\t\t\t\n\t\t\t\tbin_name_for_node(label[0])\n\t\t\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\tvalues = np.array(edges, dtype = np.intc)\n\t\tvalues = values.astype(\"intc\")\n\t\treturn values.min(axis = 0) - values.max(axis = 0)\n\n\ndef integral_range(binvals):\n\t\"\"\"\n\tCompute a :py:class:`.Interval` based on the interval binvals passed as arguments.\n\n\t*binvals* is assumed to be an iterable of values of the form\n\t:py:class:`.histogram` description.\n\tTheir format is defined in :class:`.histogram` description.\n\t\n\tThe valid"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n#\tdef histogram2(bins, edges, width):\n#\"\"\"\n\t#\tCompute and store histogram values at the given bins.\n#\tTheir format is defined in :class:`.histogram` description.\n#\t\"\"\"\n#\n#\tdef histogram(bins, edges, width):\n#\t\t(histogramx, histy), (histogramx_points, histy_points) = histogram(\n#\t\t\tbin(bins, 0, width), width)\n#\t   \n#\t\n\tdef histogram2_points(bins, edges, width):\n\t\"\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\t# Set the bounds\n\tbounds = zip(*range(0,len(bins),3))\n\n\t# Normalize the integral\n\tbins = [int((i*step)+step) for i in bins]\n\n\t# Create the integral mesh\n\tmesh = []\n\n\t# Loop over all bins\n\tfor i,bins_ in enumerate(bins):\n\t\t\n\t\t# Initialize the histogram with the integral value at all of the given bins as a tuple\n\t\tif bins[i] < bounds[i]: # No need to extend the interval\n\t\t\tmesh.append((i, bins_[i]))\n\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tbins = [ (bin,(edges[u-1]-edges[0])) for u in bins ]\n\t\n\t\n\tintegral = zeros( bins )\n\t\n\tfor i in range( len(bins) ):\n\t\tintegral[bins[i][1]].append( bin**2. )[0] + bins[i][0] - bins[i][1] - bins[i][0]\n\t\n\treturn integral\n\ndef smooth(bins, weight):\n\t\"\"\"\n\tUpdate the weights of a specific histogram.\n\t\n\t*bins* contain values, and *weight* form the mesh\n\tfor"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n#\tI have not used a docstring yet\n#\tI have not tried it anymore because I took better speed!\n\tif bins is None and edges is None:\n\t\tbins, edges = histogram(edges)\n\t\n\tbins, edges, density, density = _calculate_bins(bins, edges)\n\t#\n\t# Check the histogram against the integral. If the integral\n\t# value is zero the integral should be zero, otherwise it should be equal\n\t# to the integral. The correct integral value is `density`\n\t# (instead of `density` is not zero, but `"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tdef scale(b, v, s):\n\t\t\"\"\"\n\t\tscale a histogram with a factor (or a scalar) s.\n\t\t\n\t\t*b* is the histogram\n\t\t:param b: the histogram\n\t\t:param v: the value to be scaled\n\t\t:param s: the bin width\n\t\t\"\"\"\n\t\treturn b * v * s\n\t\n\tdef integral_scale(b, v, s):\n\t\t\"\"\"\n\t\tscale a histogram with a factor (or a scalar) s.\n\t\t\n\t\t*b* is the histogram\n\t\t:param v: the value to be scaled\n\t\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tif type(bins) not in(list, tuple):\n\t\tbins = [bins]\n\telse:\n\t\tbins = list(bins)\n\t\n\tfor bin in bins:\n\t\tf = interp1d(bin, hist, kind=interp1d.eval_discrete)\n\t\tif bins == None:\n\t\t\th, bins = zip(*f(np.repeat(edges[::-1], hist.shape)))\n\t\t\tf = interp1d(bin, h, kind=interp1d.eval_discrete)\n\t\treturn np.asarray(f(bins))\n\t\n@_trace"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\tbins = bins or np.arange(-len(bins) + 1, len(bins) + 1)\n\t\tif isinstance(hist, np.ndarray):\n\t\t\tbins = bins[0:len(bins + 1)]\n\t\tbins = np.asarray(bins).reshape(-1,)\n\t\tedges = edges or [[],] * len(bins)\n\t\tedges[1] = bins.astype('float')\n\t\treturn hist(*(edges[:,0:len(edges)]).T)\n\n\tdef mean(bins, edges):\n\t\t\n\t\t\"\"\"\n\t\tCompute a histogram representing the mean for this interval"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t\n\tnbins=len(bins)\n\tscale=(1/nbins)*edges.min()\n\treturn scale\n\n\"\"\"\n\n.. image:: images/tools/math.py\n\nmath is the natural arithmetic operation, i.e. sum, min etc.\n\"\"\"\n\ndef sum(bins,edges):\n\t\"\"\"\n\tCompute the sum of an array of values.\n\t*bins* are of the form: (x,y) where x is the array of x\n\tand y is the array of y. For instance, we sum(0.5) = 5"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t# test if the sequence of a FillRequestElement have a\n\t# FillRequestElement in it's elements,\n\t# and the element is empty\n\ttry:\n\t\tfelements = flatten_elements_and_dicts(\n\t\t\tflatten_elements_and_dicts(seq))\n\t\tfill_req_elem = next(filts_of_types.FillerType(\n\t\t\tfill_type = felements.element))\n\texcept KeyError, err:\n\t\twritemsg_level('!!! Ignoring input Element: %s!' % err)\n\t\treturn False\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n#\tprint \"is_fill_request_seq: %s\" % is_fill_request_seq(seq)\n#\tif is_fill_request_seq(seq):\n#\t\t\traise ValueError(\"Can't convert %s into FillRequestSeq\" % is_fill_request_seq(seq))\n#\treturn\n\n\"\"\"\n\tReturn true if the item is a Source or Copy source\n\tthat has a \"FillRequest seq\" attribute\n\t(in between source and destination element)\n\tand the destination element \"FillRequest\" element.\n\t\"\"\"\n\ndef"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n#\tif isinstance(seq, Fuzz) or isinstance(seq, SourceSequence):\n\t\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_fill_request_seq_str(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeqStr.\n\t\n\tTrue only if it is a FillRequestSequence element or contains at least one such,\n\tand it is not a SourceSequence.\n\t\"\"\"\n\t\n#\tif isinstance(seq, Fuzz) or isinstance(seq, SourceSequence):\n\t\t\treturn True\n\telse:"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\tif seq.tag == \"fill-request-sequence\":\n\t\treturn True\n\telse:\n\t\treturn False\n\nclass FillRequestSeq(object):\n\t\"\"\"\n\tA Source or a FillRequest element.\n\t\"\"\"\n\tdef __init__(self,\n\t\ttype,\n\t\tdata,\n\t\ttext,\n\t\tseq,\n\t):\n\t\t\"\"\"\n\t\tBasic check for *seq* element\n\t\t\"\"\"\n\t\tself.type = type\n\t\tself.text = text\n\t\tself.seq = seq\n\t\tself.src_name = data.get(\"name\")"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tif not isinstance(seq, FillRequest):\n\t\traise TypeError(\"Expected a fill request object: %s\" % type(seq))\n\tfor r in seq:\n\t\tif not isinstance(r, FillRequest): continue\n\t\tyield r\n\t\t\n\nclass FillRequestSequence(object):\n\tdef __init__(self, first=None, last=None):\n\t\tif not first or not last: raise ValueError\n\t\tself.first = first\n\t\tself.last = last\n\nclass FillRequestSeq(object):\n\tdef __init__(self):\n\t\tself._count = []\n\t\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t# assert seq.type in [fill_request_type.SourceDataTypes.Source, fill_request_type.FillRequestType.SourceData]\n\t\n\treturn seq.type == fill_request_type.SourceDataTypes.FillRequestSeq\n\ndef is_fill_request(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\n\tTrue only if it is a FillRequest element\n\t\n\tNote that the source element has to be defined before they are"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tvalid = False\n\tfor item in seq:\n\t\tif not valid:\n\t\t\tvalid = True\n\t\ndef get_fill_request_seq(seq):\n\tif isinstance(seq, type(str)):\n\t\treturn FillRequestSeq(seq, *args, **kwargs)\n\traise AssertionError('Unable to parse fill request. seq is %s.  seq is %s.' % (str(seq), str(type(seq))))\n\t\ndef fill_request_and_source_seq_from_list(source_seq_list, source_seq):\n\tfor i in"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t # We must check that the elements of this array live in the given\n\t # order.\n\t i = 0\n\t while hasattr(seq,'_sources') and not hasattr(seq,'_sources_end'):\n\t\t i += 1\n\t return seq.has_attr('sources',i,seq.sources) and seq.has_attr('sources',i,1,seq.sources_end)\n\n\tdef get_fill_request_object(self, fill_request, **kwargs):\n\t\to = FillRequestSeq()\n\t\to.__dict__.update(fill_request.__dict__)\n\t\t#if the"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\tif isinstance(seq, FillRequestSeq):\n\t\t\t\treturn True\n\treturn False\n\ndef get_data_from_file(infile):\n\treturn read_all(infile)\n\ndef get_data_from_file_in_folder(folder, filename):\n\tfor fname in os.listdir(folder):\n\t\tif not os.path.isdir(os.path.join(folder,fname)):\n\t\t\tyield fname\n\t\t\tcontinue\n\ndef get_data_from_file_with_default_name(fname):\n\tfn = os.path.join(folder,"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\t\n\t# If there is an element of a sequence, we will use this for the current element\n\t# so we assume it has a ContentInfo element.\n\treturn IsElementPresent(SeqElement, seq)\n\t\n\ndef find_request_seq(seq):\n\t\"\"\"\n\tTest whether *seq* has an ElementPresents element\n\tor contains at least one such, and it is not a Source sequence.\n\t\n\tTrue only if it is a Source element\n\tor contains at least one such,\n\tand it is not a ContentInfo element.\n\t\"\"\"\n\t\n\t\n\treturn seq."}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn isinstance(obj, FilledRequest) and not isinstance(obj, Request)\n\n\n#!!!TODO:\n#!!!: These are just functions that can be used from the\n#!!! 'fill' and'request' modules\n\n#!!!TODO: The following function is not fully implemented.\n\ndef generate_fill_request(request):\n\tfill_request_el = request.params.get(\"fill_request\")\n\trequest_code = request.params.get(\"request_code\")\n\trequest_context = dict(request.context, _request=request"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t # The default fill attribute is'request'.\n\t if hasattr(obj, 'fill') is None:\n\t\t obj.fill = getattr(obj,'request')\n\telif hasattr(obj, 'fill'):\n\t # The default fill attribute is'request'.\n\t if hasattr(obj, '__fill'):\n\t\t obj.__fill__()  # Fill out all parameters in the module\n\treturn hasattr(obj,'request') or obj.request\n\n\ndef is_fill_request_module(module):\n\t\"\"\"\n\tObject contains a module and the right name.\n\t\"\"\"\n\t # The default fill attribute is '"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\n\n\nclass fill:\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\tfill: \n\t\t\tFill objects in this view.\n\t\t\"\"\"\n\n\t\tsuper(fill, self).__init__(**kwargs)\n\n\nclass request:\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\trequest: \n\t\t\trequest: \n\t\t\t\tRequest objects.\n\t\t\"\"\"\n\n\t\tsuper(request, self).__init__(**kwargs)\n\n\nfill.type ='request'\nfill.function = FillExpression(fill_fn='fill_request')\nfill.element = request()\nfill."}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\n\t\n\tfill = None\n\t\n\trequest = None\n\n\tassert_type_of(obj, list)\n\tassert_type_of(fill, string_types)\n\n\tdef fill_e():\n\t\tif True:\n\t\t\tobj.fill(fill)\n\n\tfill_e.__doc__ = (\"\"\"\\\n\n\t\tObject contains methods 'fill' and'request'.\n\t\t\"\"\")\n\n\trequest_e = None\n\n\tobj.fill_e()\nfrom setuptools import setup, find_packages\nimport sys\nimport shutil\nimport os\nimport argparse\nimport subprocess\n\ndef load_"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn obj.get('fill') =='request'\n\ndef is_fill_request_el_file(obj):\n\t\"\"\"\n\tObject contains file.\n\n\tArguments:\n\t\tobj (class): The class whose elements are of interest.\n\t\"\"\"\n\t\treturn obj.get('fill') =='request.file'\n\nclass FileCheck(object):\n\t\"\"\"\n\tA class for checking files.\n\t\"\"\"\n\t__author__ = 'Kerry-Feng'\n\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2006-"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\ndef is_update_request_el(obj):\n\t\"\"\"\n\tObject that contains a 'update_request', 'update_task_state',\n\t'update_state'\n\t\"\"\"\n\treturn  obj.is_update_request()\n\ndef is_fill_task_state_el(obj):\n\t\"\"\"\n\tObject that contains all values/boolean values/string values\n\tof the 'fill' and \"request\".\n\t\"\"\"\n\t\ndef is_update_state_el(obj):\n\t\"\"\"\n\tObject that contains a 'update_state' in"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\tif obj.hasContents():\n\t\t\treturn True\n\t\treturn False\n\tdef get_request_value(obj):\n\t\treq_dict =obj.propdict()\n\t\tif req_dict.has_key('fill'):\n\t\t\tval = req_dict['fill']\n\t\telse:\n\t\t\tval = ''\n\t\treturn val\n\tdef get_request_data(obj):\n\t\trequest_dict =obj.propdict()\n\t\tif request_dict.has_key('request'):\n\t\t\tvalue = request_dict['request']\n\t\telse:\n\t\t\tvalue = 'default"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn isinstance (obj, FillRequestElement)\n\n\tdef fill (self, o):\n\t\t\"\"\"\n\t\tMethod for creating a new fill request, that will fill the input buffer\n\t\t\"\"\"\n\t\tif isinstance (o, Fill):\n\t\t\tself.fill_element (o)\n\t\telse:\n\t\t\tself.fill_element (o.obj)\n\n\t##############\n\t# Other Functions #\n\t##############\n\t\n\t@staticmethod\n\tdef get_element(obj):\n\t\t\"\"\"\n\t\tReturns a new element, that can be used to add a new element,\n\t\tit will return"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\tcls = obj.__class__\n\t\tt2 = []\n\t\tif cls in ['obj','string'] and not is_executable_class(t2):\n\t\t\tif verbose: print(\"Checking {} is ExecutableClass.\".format(t2))\n\t\t\t\t\t\t\n\t\t\tif cls in ['obj','string']: \n\t\t\t\tt2 = [t2]\n\t\t\tis_executable_or_input = False\n\t\treturn is_executable_or_input\n\t\n\n\n###\n#\n### These classes generate the request\n###\n###\n## Functions/functions for creating the return\n### objects and calling the"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t\treturn True\n\n\tdef fill_request(self, request):\n\t\"\"\"Fill the given request with the given instance of the class.\n\t\tReturns the newly filled request.\n\t\"\"\"\n\t\t# Do something to fill a request with, in the form of an HTTP request.\n\t\t# The return value is a tuple (True, String, String) where the first element is True, the second is whether the request is to be shown at a specific position in the list of lines.\n\t\t# Since the request is constructed as XML, this function will write out the contents of the HTML tree (html_tree"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\tcmd = ('python', '-c','-f','-t',obj,'-r','/tmp','-l','/tmp/tmp.log','./tmp','$HOME','-t',obj)\n\t\treturn (run_exe(cmd))\n\t\ndef make_dir(name):\n\t\"\"\"\n\tCreates directory for object and runs it by executing a command\n\t\"\"\"\n        run_shell = get_var(make_var('run_shell', '0'))\n        run_exe = get_var('run_exe', '0')\n        cmd = ('perl', run_shell, name"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\tret = False\n\t\tif (obj['type']==\"Executable\"):\n\t\t\tif not isinstance(obj['cmdline'],list)==True:\n\t\t\t\tret = False\n\t\treturn ret\n\n\tdef get_execution(object, name):\n\t\treturn object[\"execution\"]\n\n\tdef execute_file(obj):\n\t\treturn obj[\"file\"]\n\n\tdef execute_shell(obj):\n\t\treturn obj[\"shell\"]\n\n\tdef create_script(obj):\n\t\treturn obj[\"text\"]\n\n\tdef execute_file_with_args(obj):\n\t\treturn obj[\"file\"]\n\n\tdef execute"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\ndef is_run_link(obj):\n\t\"\"\"\n\tObject contains executable method 'link'\n\t\"\"\"\n\t\ndef is_run_obj(obj):\n\t\"\"\"\n\tObject contains executable method 'obj'\n\t\"\"\"\n\treturn type(obj)(obj.id)!= type(None)\nimport pymysql\nimport getpass\nimport re\nimport MySQLdb\nimport hashlib\nimport time\nimport datetime\nimport os\nfrom optparse import OptionParser\n\nfrom collections import OrderedDict\n\nfrom t0mm0.util import config\n\nclass DBManager(object):\n    conn = None\n\n   "}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn len(obj.binfiles) > 0\n\nclass Entry(object):\n\tdef get_script(self):\n\t\treturn \"/bin/sh\"\n\nclass ConfigObj(object):\n\n\tdef __init__(self, filename):\n\t\tself.filename = filename\n\t\tself._config.read(filename)\n\n\tdef __getattr__(self, name):\n\t\treturn self._config[name]\n\n\n\tdef __setattr__(self, name, value):\n\t\tself._config[name] = value\n\t\n\tdef get_binfiles(self):\n\t\treturn filter(None,"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn not isinstance(obj, RunCommand) and hasattr(obj, 'run')\n\n\n\tdef can_run(self):\n\t\t\"\"\"\n\t\tReturns is instance method method for this instance that has can_run\n\t\t\treturns whether there are any available executables\n\t\t\"\"\"\n\t\treturn not isinstance(self.path, DirectoryEntry) and hasattr(self, 'run_dir')\n\n\n\tdef can_run_dirs(self):\n\t\t\"\"\"\n\t\tReturns is instance method method for this instance that has can_run_dirs\n\t\t\treturns whether there are any available executables\n\t\t\"\"\"\n\t\tif self."}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn(is_run.is_executable(obj))\n\t@staticmethod\n\tdef get_run_el(obj):\n\t\tif obj: return obj.run\n\t\telse: raise RuntimeError(\"Run is a null object\")\n\n\"\"\"\n\nimport sys\nimport os\nimport json\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n\n\nfrom django.conf import settings\nfrom django.test import TestCase\nfrom django.utils.encoding import force_unicode\nfrom django.utils.translation import ugettext_lazy as _\n\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn 'run' in dir(obj)\n\nclass Binary(object):\n\t\"\"\"\n\tBinary object.\n\t\"\"\"\n\t\n\tdef __init__(self, source):\n\t\tself.bin = Binary(source)\n\t\t\n\tdef get_file_basename(self, name):\n\t\t\"get name of file type. may be one of the methods in src (bin/src.bin or x64/x86/x64.bin)\"\n\t\tif name.startswith('src'):\n\t\t\treturn name[3:].replace('bin', 'x64')\n\t\telse:\n\t\t\treturn name"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn bool(obj.__class__.__module__.split('.')[-1])\n\nif not hasattr(os, 'geteuid'):\n\traise Exception\n\n# python3 does not have os.geteuid when euid!= 0 (Python 3\n# seems not to be installed properly - use python2.executable)\n\ndef euid():\n\ttry:\n\t\treturn os.getuid()\n\texcept OSError:\n\t\tuid = os.getuid()\n\t\tlogging.warning(\"The euid value returned was 0, using zero.\")\n\t\treturn int(uid) - 0xffffffff\n\ndef is"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn isinstance(obj, (File, Instance))\n\ndef is_run_inst(obj):\n\t\"\"\"\n\tObject contains executable method 'run'\n\t\"\"\"\n\t\tif isinstance(obj,'ObjectWithProps'):\n\t\t\treturn obj.run,obj.source\n\t\telse:\n\t\t\treturn False,obj\n\ndef remove_dir(path,file_name):\n\t\"\"\"\n\tRemove directory 'file_name', or all files 'file_name'\n\t\"\"\"\n\t\tfolder = os.path.abspath(path)\n\t\twhile os.path.exists(folder):\n\t\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t\treturn isinstance(obj, RunElf)\n\n\tdef is_elf_program(self, object):\n\t\t\"\"\"\n\t\tObject contains executable method 'elf program'.\n\t\t\"\"\"\n\t\treturn isinstance(object, ElfProgram)\n\n\tdef is_elf_program_header(self, object):\n\t\t\"\"\"\n\t\tObject contains executable method 'elf program header'.\n\t\t\"\"\"\n\t\treturn isinstance(object, ElfProgramHeader)\n\n\tdef is_elf_program_header_with_name(self, name):\n\t\t\"\"\"\n\t\tObject contains executable method 'elf program header with name"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn getattr(obj, \"fill\", None) in [\"fill\", None, \"compute\"]\t\n\n\ndef get_line_length_map(a, b):\n\t\"\"\"\n\tDictionary with line length as the value.\n\t\"\"\"\n\t\tlst = {}\n\t\tfor k, _o in enumerate(b.split(\"\\n\")):\n\t\t\tif a!= b or _o == \"\":\n\t\t\t\tcontinue\n\n\t\t\tif a is None or b is None or _o in [\"\", \"\", \"-\"]:\n\t\t\t\tlst[k] = 1\n\t\t\telse:\n\t\t\t\tlst[k] = lst[k"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn False\n#!/usr/bin/env python\n\nimport sys\nimport os\nimport argparse\nimport numpy\nimport pandas as pd\n\nclass RF_base_parameters():\n    def __init__(self):\n        self.inputs = None\n        self.outputs = None\n        self.variables = None\n\n    # -------------------------------------------------------------------------\n    \n    def set_inputs(self):\n        self.inputs = args[0]\n        self.outputs = args[1]\n        self.variables = None\n        \n    def set_output_variables(self):\n        self.variables = args[2]\n\nif __name__ == \"__main"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn (\n\t\t\tisinstance(obj, (Type, Tuple, List)),\n\t\t\tobj.func is not None\n\t\t)\n\n\ndef clear(obj=None):\n\t\"\"\"\n\tClears all 'data' objects to 'obj'.\n\t\"\"\"\n\tif obj is None:\n\t\treturn\n\tobj.func = obj.__func__\n\treturn\n\n# The regular expression used to split lines.\n#\n# This regular expression is used to substitute variables,\n# such as \"x =  y\", where each x and y consists of a set of\n# unique lines, the"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn None\n\ndef process_element_type_element(attrs_list, element, t, el_str, t_element):\n\t\"\"\"\n\tElement type element in a generic element set.\n\t\"\"\"\n\t\tattr_name = element.attrib[\"name\"]\n\t\ttype_element = element.attrib[\"type\"]\n\t\tval_type = element.attrib[\"value\"]\n# TODO(satemb): check if something's wrong before the\n\t# element type has been determined. The same thing:\n\t# * the object could have 'type' attribute but it can't get"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\tt = obj.is_fill()\n\t\tif(t):\n\t\t\tfill_size = t.size\n\t\telse:\n\t\t\tfill_size = 0\n\t\tfill = zeros(fill_size, dtype=t.dtype)\n\t\tis_computed = not obj.is_constant()\n\t\tfill[isnan(fill)] = 1.0\n\t\tcompute = zeros(fill_size, dtype=t.dtype)\n\t\treturn asarray((obj._obj, obj.args, obj.var, fill,\n\t\t\t\t\t t.type, t.shape, obj.base, is_computed))"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\nclass fill(object):\n\tdef __init__(self):\n\t\tself.fill = None\n\t\tself.compute = None\n\t\t\n\tdef check_fill(self, obj, oparg):\n\t\t\n\t\tif not isinstance(obj, ir.Array):\n\t\t\tobj = numpy.asarray(obj)\n\t\t\tif obj.dtype!= obj.dtype:\n\t\t\t\traise ValueError(\"Invalid type of fill arg: \" + str(obj.dtype))\n\t\t\t\t\n\t\tif self.fill!= oparg:\n\t\t\traise Exception(\"Incompatibly expected to have type \" + str(type(o"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn isinstance(obj, Cube) and isinstance(obj.array, IPy.Symbol) and \\\n              obj.is_fill() and not obj.array.is_free()\n\n\tdef is_fill_free(obj):\n#\t\treturn not isinstance(obj, IPy.Symbol) and \\\n\t#        not isinstance(obj.array, IPy.Symbol)\n#\t\treturn not isinstance(obj, IPy.DataArray) and \\\n#\t\tnot isinstance(obj, IPy.DataArray2D) and \\\n#\t\tnot isinstance("}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\t# get a name of the calling object\n\tname = obj.__name__\n\n\t# get the attributes of the resulting object.\n\tattrs = {}\n\tallattrs = set(name + \"_attrs_lst\")\n\tanyattrs = set(attrs.keys())\n\tret = getattr(obj, allattrs)\n\n\tif len(allattrs & anyattrs) == 0:\n\t\tif not hasattr(obj, \"func\"):\n\t\t\treturn getattr(obj, thisclass_name)\n\t\telse:\n\t\t\tprint \"object is not executable\"\n\t\t\treturn getattr(obj, thisclass"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\treturn obj.fill, obj.compute\n\tdef _is_fill_compute(obj):\n\t\treturn obj.fill or obj.compute\n\tdef is_constant_node(obj):\n\t\treturn obj.constant or obj.constant_value\n\t\n\tdef is_free_function_node(obj):\n\t\treturn obj.free or obj.free_value\n\t\n\tdef is_global_variable_node(obj):\n\t\treturn obj.var or obj.is_variable\n\t\n\tdef isdefined(obj):\n\t\treturn obj.is_constant_node(obj)\n\t\n\tdef"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t\ttry:\n\t\t\tassert( obj.__class__ == (Object,dict,type) )\n\t\texcept AssertionError:\n\t\t\t\treturn False\n\t\treturn True\n\tdef fill(self):\n\t\t\"\"\"\n\t\tReturns False if there is only one file to return and has no \n\t\tdata stored.\n\n\t\tReturns True if there are no data in the object, False otherwise.\n\t\t\"\"\"\n\t\treturn self.array.has_key(self.fill_key)\n\tdef compute(self):\n\t\t\"\"\"\n\t\tReturns True if there are no data in the object, False otherwise\n\t\t\"\"\"\n\t"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\td = {}\n\t\ttry:\n\t\t\td.update(d1)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\td.update(d2)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tdel d1['key']\n\t\texcept TypeError:\n\t\t\tpass\n\t\ttry:\n\t\t\tdel d['value']\n\t\texcept TypeError:\n\t\t\tpass\n\t\treturn d\n\ndef intersection (src_list, dst_list, level=0):\n\t\"\"\"\n\tReturn a dictionary with items from *src_list* are not nested in *dst"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\nimport sys\n\nimport logging\nif sys.version_info[0] >= 3:\n\timport builtins\nelse:\n\timport __builtin__ as builtins\n\n# For Python 3 support\nif sys.version_info[0] >= 3:\n\tbasestring = str\nelse:\n\tbasestring = basestring\n\t\nclass IntersectionError(Exception):\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\ndef _remove(d1, d2):\n\tif d1 == d2:\n\t\treturn d1\n\n\tfor key in set(d"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\tif level!= -1:\n\t\t\tif d1.has_key(d2):\n\t\t\t\tresult = mergedict(d1[level], d2[level])\n\t\t\telse:\n\t\t\t\tresult = d1[level]\n\t\telse:\n\t\t\tresult = d2.items()\n\t\tresult = sorted(result, key=lambda x: x[0])\n\t\treturn result\n\t\n\tdef difference(d1, d2, level=-1):\n\t\t\"\"\"\n\t\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\t\n\t\t*level* sets the"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\treturn dict(d1.items() - d2.items())\n\ndef intersection(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from *d1* contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1, if a key is present both in *d1* and *d2* but\n\thas different values,\n\tit is included into the intersection.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\tif not isinstance(d1, dict) and not isinstance(d2, dict):\n\t\t\traise TypeError('a and b can only be dictionaries')\n\n\t\td2 = copy.deepcopy(d2)\n\t\tfor k, v in d2.items():\n\t\t\tif isinstance(v, (list, tuple, dict)): return d[k], d2.get(k)\n\t\t\telif k in d and k not in d2:\n\t\t\t\td[k] = d2.get(k) or v\n\t\treturn d2\n\ndef normalize_path(path):\n\tif"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\t\n\td1 = deepcopy(d1)\n\td2 = deepcopy(d2)\n\t\n\tres = {}\n\t\n\tfor key in sorted(d1.keys()):\n\t\tif d1[key]!= d2[key]:\n\t\t\tres[key] = False\n\t\t\n\tif level == -1:\n\t\td1['children'] = list(d2)\n\tfor key in d1.keys():\n\t\tif key not in d1:\n\t\t\tres[dict(key)] = list(d1[key])\n\t\t\tcontinue\n\t\tfor child in d1[key]:"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\ndef intersection(d1, d2, key=None):\n\t\"\"\"\n\tReturns a dictionary with keys from *d1*, not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the intersection.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1*"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\tif level == -1:\n\t\t\treturn {}\n\t\tassert type(d1) == dict, \"Dictionary expected\"\n\t\tassert type(d3) == dict, \"Dictionary expected\"\n\t\tassert type(d2) == dict, \"Dictionary expected\"\n\t\tassert type(d1) == type(d3), \"Dictionary expected\"\n\t\tassert type(d2) == type(d1), \"Dictionary expected\"\n\t\treturn d1.copy()\n\t\n\tdef difference_sublists(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\n\n\tif level!= -1 and level < 0:\n\t\traise ValueError(\"level must be non-negative\")\n\tif level < 0:\n\t\tlevel = -1\n\t\n\td1_or_d2 = d1.copy()\n\twhile d1_or_d2[-1]!= None and d1_or_d2[level]!= None:\n\t\td2_or_d1.pop()\n\t\td2_or_d2.pop()\n\tfor key in d1.keys():\n\t\td1_or_d2[key] ="}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t\tresult = {k:v for k,v in d1.items() if v not in d2.items()}\n\t\tfor key in result:\n\t\t\tif key in d2:\n\t\t\t\tresult[key] = d2[key]\n\t\t\t\treturn result\n\telse:\n\t\treturn result\n\ndef less_than(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items less than *d1*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\t"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\t\n\tif self.isClosed():\n\t\traise Exception(\"can't open histogram. Please call draw() first. You can use add() method.\")\n\t\n\tself.points.append((coord, weight))\n\t\t\n\t# fill histogram edges\n\t\n\tself.clear()\n\tself.line(0, 0, self.pointSize(coord)[0])\n\t\t\t\n\n\t# clear line, line segment\n\tfor i, pt in enumerate(self.points):\n\t\tpt = pt[0]\n\t\tpt = self.points[i]\n\t\tself.line(pt[0], pt[1],"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\nif hasattr(self, 'isscalar'):\n\t# If the object has an explicit __setitem__ method, we need to call it\n\t# directly.\n\tself.value, = self.value, None\nelse:\n\tself.value = self.value[0], None\nself.weight = weight\n\t\n\tself.value = _array2value(self.value)\n\tif self.weight is not None:\n\t\tself.isscalar = self.isscalar(self.weight)\n\telse:\n\t\tself.isscalar = 0\n\n\ndef _get_size"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t # TODO: This should be handled outside the code!\n\t #        raise Exception('Please contact the author of the'+ \\\n #                 'Cortex module.')\n\t self._add_histogram(coord, color=weight)\n\t self.update()\n\n\tdef _add_histogram(self, coord, **kwargs):\n\t \"\"\"\n\t\tCreate a histogram of the specified *coord*\n\t\tin the given *weight* with the given *weight*.\n\t\t\"\"\"\n\t # Make sure the weight is between 0 and 1\n\t new_weight = [min(0, x) for x in range(1, len"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\timport sys\n\tfrom time import time\n\n\n\tglobal width\n\tglobal height\n\twith open(sys.argv[1], \"w\") as f:\n\t\tf.write(\"#\\n\\n\")\n\n\t\tfor r,c in self.ranges:\n\t\t\tf.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(r,c,width,height,c,float(float(r)/c)*weight))\n\n\n\tprint str(time() - start) + \" seconds\\n\"\n"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\tif not isscalar(coord):\n\t\tcoord = [ coord ]\n\treturn histogram2d(\n\t\tself.histogram(*coord), \n\t\twidth=weight,\n\t\tvertical=True, \n\t\ttranspose=True,\n\t\tpad=0,\n\t\tedgecolor='none',\n\t\t**kwargs\n\t)\n\nif __name__ == \"__main__\":\n\t\"\"\"The main function.\"\"\"\n\t\n\n\tif len(sys.argv)!= 3:\n\t\traise Exception(\"Usage: fill [<coord/weight>] <weights <axis>\")\n\tcoords = args.coords\n\tbinwidths"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif (coord[4] > 1e5 and coord[4] < 1e-5):\n\t\t\traise ValueError('The first histogram edge must be >= 1e5.')\n\t\t\n\t\t# Make sure *weight* is within a given *range*.\n\t\t# Since all grid ticks in the histogram will all be within a half\n\t\t# of the total width, adjust the range accordingly.\n\t\tif weight <= 0:\n\t\t\traise ValueError('Weight should be a number between 0 and 1.')\n\t\t\n\t\t# Make sure the histogram is really sorted at a given frequency.\n\t\t# Note that when the histogram are"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\traise NotImplementedError()\n\nclass Histogram(Bin):\n\t\"\"\"\n\tA histogram class representing histogram bins.\n\t\n\tHistograms have the following public attributes:\n\t\txtype: type of histogram (histogram-like instance)\n\t\tcount: number of points in input histogram (1 if 1). This is not\n\t\tto be used by all other histories, and is useful for generating\n\t\ta histogram of the histogram-like.\n\t\tbins: array representing the number of histogram bins.\n\t\ttype: the type of histogram, as a tuple\n\t\tcount: the number of distinct points in the interval"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\tif self.hist:\n\t\treturn self.hist.getHist(*coord)\n\telse:\n\t\treturn self.hist.getHist(*coord,weight=weight)[1]\n\n\nclass F(BaseColor):\n\t\"\"\"\n\tLightweight color class.\n\t\n\tFor example, the color class can be created to create a color that is\n\ta :class:`LinearColor`, :class:`Color` and any :class:`Color`, so that\n\tthe color is within the range, the color range is (0.0, 1.0), and the\n\tcolor is ("}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif not weight:\n\t\t\tself.histogram.addToCell(coord+weight, len(self) + 1 )\n\tdef renumber(self, coord):\n\t\"\"\"\n\tRenumber histogram at *coord*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tif not weight:\n\t\t\tself.histogram.addToCell(coord, len(self) )\n\tdef renumberCells(self):\n\t\"\"\"\n\tRenumber cells at *self*.\n\t\n\tReturns True if any cell with *coord* renumber, otherwise False.\n\t\"\"\"\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\tfor a in xrange(self.nbhisto()):\n\t\t\tself.axHist[a].fill(coord[0] + weight * coord[1], 0)\n\n\n\t#_______________________________________________________________________\n\tdef _plotHist(self, histogram, pos, title=''):\n\t\ttry:\n\t\t\thist = float(hist);\n\t\t\tif pos < len(self._histHist):\n\t\t\t\thist = self._histHist[pos];\n\t\t\t\treturn pos + 1, hist\n\t\t#except:\n\t\t\t#raise TypeError, \"invalid hist range!\"\n\t\t# raise ValueError, \"invalid hist range"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif isinstance(labels, dict):\n\t\t\tif isinstance(labels, str):\n\t\t\t\tlabels = [labels, ] * len(labels)\n\t\t\tfor label in labels:\n\t\t\t\tif label not in labels:\n\t\t\t\t\traise ValidationError({'$$': label, 'key': labels[label]})\n\t\telse:\n\t\t\tkeys = [labels[key] for key in labels]\n\t\t\tvalues = [labels[value] for value in range(len(keys))]\n\t\t\terrors = [labels[key] for key in keys]\n\t\t\tassert len(errors) == 0\n\t\t\terrors = set(errors"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tdef check_pattern(msg):\n\t\t\t\"\"\"\n\t\t\tValidates the pattern, taking into account empty strings and characters.\n\t\t\t\"\"\"\n\t\t\tfor _regex_match in itertools.permutations(msg):\n\t\t\t\tvalid_msg = _regex_match.pattern\n\t\t\t\tif valid_msg is not None:\n\t\t\t\t\treturn valid_msg\n\t\t\traise ValidationError(msg)\n\n\t\terrors = dict()\n\t\tfor label in labels:\n\t\t\tlabel = _validate_labels(label)\n\t\t\tlabels[label] = \"\"\n\t\t\t# Empty string matches the regex\n\t\t\tif not label: continue\n\t\t\t#"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif any(not isinstance(key, str) for key in labels):\n\t\t\traise ValidationError(\n\t\t\t\t'labels should be a dictionary [{0}]'.format(type(labels)))\n\t\telse:\n\t\t\tfor label, val in labels.items():\n\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\n\t\t\t\t\tlabel_matches = re.findall(key, val)\n\t\t\t\t\tassert label_matches, '{0} is not a valid label'.format(label)\n\t\t\t\t\tlabel_matches = [t for t in label_matches if t == label]\n\t\t\t\texcept ValueError:\n\t\t\t\t\t# We do not support the regex for"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif not isinstance(labels, dict):\n\t\t\traise ValidationError(\"keys, value given in `labels` must not be a dict\")\n\t\tfor key in labels:\n\t\t\tif not key in labels[key]:\n\t\t\t\traise ValidationError(\"label {} does not contain key {}.\".format(labels[key], key))\n\t\tfor key in labels:\n\t\t\tif labels[key]!= labels[key]:\n\t\t\t\traise ValidationError(\"labels (matched in the label must have the same labels) with the key {}\".format(key))\n\n\tdef _validate_not_empty(values):\n\t\t\"\"\"\n\t\tCheck"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif labels is None:\n\t\t\treturn None\n\t\t\n\t\tfor k in labels.keys():\n\t\t\tif k not in VALID_LABELS:\n\t\t\t\traise ValidationError(\"Label '%s' is not a recognized label\"%k)\n\t\t\n\t\tfor val in labels.values():\n\t\t\tif val not in VALID_LABELS:\n\t\t\t\traise ValidationError(\"Label '%s' is not a recognized label\"%','.join(VALID_LABELS))\n\t\t\n\t\treturn labels\n\n\nclass ValidationException(Exception): pass\n__all__ = [k for k, v in globals().items() if not k.startswith('__')]\n\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tassert len(labels) > 0\n\n\tdef _validate_multiplexer(data, label):\n\t\tif isinstance(label, string_types):\n\t\t\tdata = label(data)\n\t\telse:\n\t\t\tlabel = {\n\t\t\t\t'$': label,\n\t\t\t\t\"'\": False,\n\t\t\t}\n\t\tif isinstance(data, dict):\n\t\t\tdata = dict(data)\n\t\telse:\n\t\t\tdata = label\n\t\tfor v in data:\n\t\t\tif v.endswith(\"\\\\\"):\n\t\t\t\tv = v.replace(\"\\\\\", \"\")\n\t\t\telif v.startswith(\"$\"):\n\t\t\t\tv ="}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tdata = dict(labels)\n\n\t\tfor k, v in six.iteritems(data):\n\t\t\tif v is None:\n\t\t\t\traise InvalidFieldError(k, v, \"data\")\n\t\tsuper(ChoiceField, self).__init__(*args, **kwargs)\n\t\t\n\t\tfor label, regex in data.items():\n\t\t\tregex_errors = re.compile(regex, re.UNICODE)\n\t\t\tfor i in itertools.chain(range(0, len(data), 2), repeat=2):\n\t\t\t\tre_regex = re.compile(regex_errors, regex)\n\t\t\t\tif re.match"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tassert isinstance(labels, dict) and 'labels' in labels),\\\n\t\t        'labels should be a dictionary'\n\t\tassert all('key' in label for label in labels), \\\n\t\t        'labels[%s] should contain a dict' % labels\n\t\tfor (key, label) in labels.iteritems():\n\t\t\tassert not label.lower().startswith(key.lower())\n\t\terrors = {}\n\t\ttry:\n\t\t\tvals = list(labels.values())\n\t\texcept TypeError as err:\n\t\t\tassert str(err) == \"Expected list, but is a tuple!\"\n\t\t\tcontinue\n\n\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tkeys_exp = dict((key, self._valid_label(label)) for label in labels.keys())\n\t\tif not (all_is_dict(keys_exp) and\n\t\t\t\tall_is_dict(keys_exp.get(label, None)) == all_is_dict(values_exp)):\n\t\t\traise ValidationError(errors.messages.keys_invalid)\n\n\tdef _valid_label(self, label):\n\t\tif not isinstance(label, string_types):\n\t\t\tlabel_type = type(label)\n\t\t\tif label_type == type(None): #"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t\tif isinstance(labels, dict):\n\t\t\tfor label in labels:\n\t\t\t\tif label.startswith('@'):\n\t\t\t\t\tassert re.match('[a-zA-Z]', label.replace('$', ''))!= None, \"Value \" + label + \" is not an integer or string\"\n\t\t\t\telif label.startswith('$'):\n\t\t\t\t\tassert re.match('[a-zA-Z]', label.replace('$', ''))!= None, \"Value \" + label + \" is not an integer or string\"\n\t\t\t\telif label.startswith('$:'):\n\t\t\t\t\tassert re.match('[a-zA-Z]', label.replace('"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t\n\tresources = list()\n\t\n\t\n\tfor resource_name, resource_info in resources.iteritems():\n\t\tres = resource_info.get('name').get('regex')\n\t\tif res:\n\t\t\tresources.append(re.compile(res))\n\t\n\treturn resources or set()\n\ndef _get_rels_per_user_per_project(_user):\n\t\"\"\"\n\tGet the list of rels per user.\n\t\n\tReturns:\n\t    (list): the list of rels per user, in which the list\n\t\t                 contains the keys given as values.\n\t\"\"\"\n\t"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n    _regex = re.compile('\\\\Z$')\n\n_name_regex = []\n\"\"\"\nReturn a list of compiled regular expressions that are to be used as names.\n\"\"\"\n\ndef _is_valid_name(name, valid_name):\n\t\"\"\"\n\tChecks if the given name is a valid Krake resource naming pattern\n\tand throws an exception if it cannot. \n\t\n\tArguments:\n\t    (string): the name of the Krake resource being checked\n\t\t\tas a regular expression, given as string\n\t    (string): a regular expression or identifier\n\t    (string): the regular expression"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\tre_regex = re.compile(r'^(?:^[a-z_][a-z0-9$-]+)*$')\n\t\treturn re_regex\n\n\ndef _parse_zk_config(zk_file):\n\t\"\"\"\n\tParse the specified Kafka config and return a ZooKeeperClient-like object\n\t\"\"\"\n\tif not zookeeper_in_file([zk_file], zk_file):\n\t\traise ValueError(\"zk_file is not a valid Kafka config at the moment\")\n\n\tzk = _get_zk_client(zk_file)"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n    def __init__(\n\t\tself,\n\t\tname,\n\t\tresource_type,\n\t\tresource_name\n\t\t):\n\t\t\"\"\"\n\t\tInitializes the name and resource type.\n\t\n\t\tArgs:\n\t\t    name (str) - The the name of the resource to convert.\n\t\t    resource_type (str) - The resource type to convert.\n\n\n\t\t    resource_name (str) - The resource name to validate against.\n\t\t    Raise:\n\t\t    InvalidKrakeResourceNameError, if the resource name is invalid or\n\t\t    not registered with the Krake service.\n\t\t\"\"\"\n\t\tself.name = name\n\t"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t    \n\ttry:\n    \t\treturn re.compile(\n\t\t\tr'(?P<res_id>[-._0-9eE]{3})'\n\t\t\t'(?P<resource_name>[-._0-9-eE]{3})(?P<ext>[a-z]+)')\n\t\n\texcept AttributeError:\n\t    raise RuntimeError(\n\t\t\t'Unable to compile a resource name pattern: {}'\n\t\t\t.format(resource_name_patterns.get(resource_name_regex_pattern_type, resource_name_regex_pattern_type), resource_name)\n\t\t\t\n\t\nclass UnknownK"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t return r'^((?P<resource_name>[^:]+)(?:-(?P<resource_name_type>\\w+))?)?$'\n\ndef _get_krakes_name():\n\t\"\"\"\n\tbuild a regular expression that matches the contents of the\n\tKrake resources.\n\t\n\tReturns:\n\t    (str): The Krake name.\n\t\"\"\"\n\t\treturn r'^((?P<resource_name>[^\\/]+(?<!\\/))\\?)+$'\n\ndef _get_krake_url_root():\n\t\"\"\"\n\tbuild the regular expression that matches the contents of"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n_RESOURCE_NAME_REGEX=re.compile(r'^\\s+(\\w+\\.?(com|org))*')\n\ndef _has_extension_resource_name():\n\t\"\"\"\n\tCheck that the requested resource name is valid.\n\t\n\tReturns:\n\t    (bool): True if the requested resource name is valid, and False if\n\tan invalid resource name is given to the function.\n\t\"\"\"\n\t\nclass ResourceNameValidator(Validator):\n\t\n\t\"\"\"\n\tValidates a resource name against the defined pattern.\n\t\n\tReturns:\n\t    (boolean): True if the resource name is legal, and"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\nimport re\nimport os\nfrom distutils.core import _version\n\n# Default values for resource name regex\nRESOURCE_NAME = 'KrakeResource'\n# Default values for resource name path regex\nRESOURCE_PATH = 'VNC_RESOURCE'\n# Default values for resource name type regex\nRESOURCE_TYPE = 'VNC_SERVICE'\n# Default values for resource name directory regex\nRESOURCE_DIR = r'\\S+_\\S+\\.\\S+$'\n# Default values for resource resources regex\nRESOURCE_RESOURCES = r'\\S+_\\S+'.join([x.rstrip('.'), RESOURCE"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\nclass KrakeFileResourceResolver(RebaseResourceResolver):\n\t\n\tdef find_resource(self, filename, suffix=None):\n\t\t\"\"\"\n\t\tCreate a Krake-specific resource file from the provided URL,\n\t\tand return the :class:`KrakeFileResource`.  If None is passed,\n\t\tit will generate a random name.\n\t\t\n\t\t:param filename: the URL we would like to validate\n\t\t:type filename: str | unicode\n\t\t:param suffix: the Krake-specific resource suffix pattern\n\t\t:type suffix: str\n\t\t:return: the parsed"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n    def _get_resource_name_default():\n\treturn _get_resource_resource_name_regex()\n    \n    \n    def _get_resource_name():\n\t\"\"\"\n\tReturns the regex to a regular expression that can be used to\n\tvalidate the resource names.\n\t\n\tReturns:\n\t    (re.Regex): the normalized regular expression\n\t\"\"\"\n\treturn _get_resource_re()\n    \n    \ndef _get_resource_name_regex_default():\n\treturn _get_resource_resource_name_regex()\n    \n    \n    def _get_resource_name():\n\treturn _get_"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\tif not isinstance(value, basestring):\n\t\t\traise ValidationError(\"Expected a string, got a %r\" % type(value))\n\t\tif not re.match(validRegex, value):\n\t\t\traise ValidationError(\"Valid regex '%s' is not a valid regular expression\" % value)\n\treturn value\n\ndef validate_integer(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not representable as an integer.\n\t\"\"\"\n\n\n\ttry:\n\t\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif re.match('^\\d+$', value):\n\t\t    raise ValidationError('Remember to include a \"-\" when a value is an int.')\n\t\n\telif value[:4] in (('-0', '-1'), ('-'):\n\t\traise ParseError('Invalid string format. Is the format correct for [+].')\n\ndef validate_number(number):\n    \"\"\"\n\tValidate the given value against the corresponding numeric value.\n\t\n\tArgs:\n\t    number: the string to validate\n\t\n\tRaises:\n\t    NumberParseError: if the given value is not valid.\n\t\"\"\"\n\t\n\tif re.match('^\\d+',"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\tregex = re.compile(\"[a-zA-Z]*\")\n\t\n\tif regex.match(value):\n\t\traise ValidationError(\"%s is not equal to %s\" % (value, regex.sub(\"\", value)))\n\telse:\n\t\treturn value\n\t\nclass FleetScanData(object):\n\n\tdef __init__(self, service_type, data):\n\t\tself.data = data\n\t\n\t@staticmethod\n\tdef from_dict(d):\n\t\treturn FleetsSiteData(d['Data'])\n\nclass FleetsSiteData:\n\tFIELDS = ['service"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_email(value):\n\t\"\"\"\n\tValidate that the given value is a valid ElasticSearch email address.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_email_valid(value):\n\t\"\"\"\n\tValidate that the given value is a valid ElasticSearch email address.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not valid based on the regular expression.\n\t\"\"\"\n\t\ndef validate_email"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef test_validate_range_and_range_max():\n\t\"\"\"\n\t@param value: the range to test\n\t\n\t@throws InvalidRangeError: if the ranges are invalid, a range error is raised\n\t\"\"\"\n\ndef validate_value_or_range_min():\n\t\"\"\"\n\tValidate the given value against the range specified for min\n\tvalidation.\n\t\n\tArgs:\n\t    value: the range to validate\n\t\n\tReturns:\n\t    the range to test\n\t\n\tRaises:\n\t    InvalidRangeError: if the range is invalid, a range error is raised\n\t\"\"\"\n\t\n\nclass Valid"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tif value is not None and re.search(r'^\\d+\\.\\d+', value):\n\t\t\n\t\traise ValidationError(message=_(\"Invalid integer\"), code=code.INVALID_CHAR)\n\t\n\ndef validate_dict(dict_input):\n\t\"\"\"\n\tValidate the dictionary input object against the corresponding regular expression.\n\t\n\tArgs:\n\t    dict_input: the dictionary input object\n\t\n\tReturns:\n\t    A dictionary.\n\t\"\"\"\n\t\n\t\n\tif dict_input is not None and isinstance(dict_input, dict):\n\t\t\treturn dict_input\n\t\n\tif dict_input is not None:\n\t\t\n\t\t\n\t\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\t\n\tif pattern is None:\n\t\t\n\t\tif re.match(r'^[A-Za-z0-9_-]{16}$', value) is None or re.match(r'^[a-z0-9_-]{3}$', value) is None:\n\t\t\traise ValidationError(\"Invalid input: %s\")\n\t\t\n\tif pattern is not None and len(value) < 5:\n\t\traise ValidationError(\"Invalid input: %s\")\n\t\t\n\treturn value\n\ndef validate_type(type_):\n\t\"\"\"\n\tValidates the given type against the corresponding regular expression.\n\t\n\tArgs:\n\t    type_:"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validator(s):\n\t\"\"\"\n\tValidate the given value against the regular expression specified.\n\t\n\tArgs:\n\t    s: the string to validate\n\t\n\tRaises:\n\t    ValidationError: otherwise the value supplied is invalid.\n\t\"\"\"\n\t\n#\t\tif s is None:\n\t#\t\t\traise validators.ValidationError(s)\n\t#\t\telse:\n\t#\t\t\traise validation.ValidationError(s)\n\n\n#-----------------------------------\n#\n#   Value Validation\n#\n#-----------------------------------\n\ndef validate_value_1(val):\n\t\"\"\" \n\t\n\tValidate the given"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n@receiver(POST_RECURSIVE_THEG_VALUE, sender=CustomValue)\ndef validate_value_post_redirection(sender, instance, *args, **kwargs):\n\t\"\"\"\n\tThis is where the method called, after the given instance has been successfully \n\tvalidated.\n\t\n\tArgs:\n\t    sender: the object of the first receiver to pass to the redirection process.\n\t    instance: the object of the second receiver to pass to the redirection process.\n\t    *args: positional arguments for the redirection process.\n\t    **kwargs: keyword arguments for the redirection process.\n\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\tif re.match(regex, value):\n\t\t\traise ValidationError(\"Unrecognized regular expression\")\n\ndef run(path):\n\ttry:\n\t\tos.system(path)\n\texcept OSError as e:\n\t\tif e.errno!= errno.ENOENT:\n\t\t\traise\n\t\t\n\ttry:\n\t\timport json\n\t\timport itertools as it\n\texcept ImportError:\n\t\traise ValidationError(\"JSON Error\")\n\ndef generate_json(path):\n\t\"\"\"\n\tGenerate JSON files and directories for a given file path. \n\t\n\tArgs:\n\t    path: the path to generate the JSON files and directories"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tfor key in key.split(':'):\n\t\t\tif re.match(r'^[a-zA-Z0-9_\\-]\", key):\n\t\t\t\traise ValidationError(key)\n\nclass Validator(object):\n\t\"\"\"Validates and returns the string to be used in an XML element (unvalidated).\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\nclass Invalid(object):\n\t\"\"\"Validates and returns the string to be used in an XML element (unvalidated)."}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\tassert(key.startswith(\"\\\\^\"))\n\t\tassert(re.match(\"^$$\", key))\n\t\ttry:\n\t\t\tkey = int(key, 10)\n\t\t\tassert(key == 0)\n\t\texcept ValueError:\n\t\t\t# Some keys cannot be integers or greater\n\t\t\treturn\n\t\tif(type(key) == int):\n\t\t\traise ValidationError(\"integer cannot be of type 'int'\")\n\treturn key.upper()\n\t\n\t\n## The actual file path of the file (or the dir above of a python script).\ndef open_path(path):\n\tif(type(path)"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\nclass ValidationError(Exception):\n\t\"\"\"\n\tValidate that the input arguments match the regular expression.\n\t\n\tRaises:\n\t    TypeError: if the input arguments are not of the correct type and the given key does not match it.\n\t\n\tNote:\n\t    * Invalid keys may be ignored.\n\t\n\t\"\"\"\n\t\nclass StringType(BaseType):\n\t\"\"\"\n\tValidate the format of the string value.\n\t\n\tThe optional key argument may be specified during construction of a regular expression.\n\t\n\tRaises:\n\t    ValidationError: if the string is not valid.\n\t\"\"\"\n\t\nclass RegexType(BaseType):\n\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n### Functions to generate key validation\n\ndef valid_json(v):\n\t\"\"\"\n\t\tVerify that the given value is a correct datatype.\n\t\n\tArgs:\n\t    v: the desired json representation\n\t\n\tReturns:\n\t    A boolean, true if the given value is valid, false otherwise\n\t\"\"\"\n\n        if type(v)!= str:\n            raise ValidationError(u\"The given value should be of type string.\")\n\t\n\tif v not in (0, None, u''):\n\t\traise ValidationError(u\"The given value should be of type bool.\")\n\t\n\treturn True\n\ndef validate_bool("}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n    def get_key(key):\n\t    return re.match('^([^\\n]+)', key)\n\n    def validate_key_and_length(key, maxlen):\n\t    if len(key) > maxlen:\n\t        raise ValidationError('key lengths do not match: '+key)\n        return key\n\n    def gen_keys(config, keys):\n        \tif any([k not in keys for k in keys]):\n            \traise ValidationError(\"keys not allowed on config: \"+config)\n    def gen_options(config, options):\n        \tif any([k not in options for k in options]):\n            \traise ValidationError(\"options not"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\ndef validate_int(integer):\n\t\"\"\"\n\tValidate the given integer against the corresponding regular expression.\n\t\n\tArgs:\n\t    integer: the integer to validate\n\t\n\tRaises:\n\t    ValidationError: if the given integer is not a valid int.\n\t\"\"\"\n\t\ndef validate_uuid(uuid):\n\t\"\"\"\n\tValidate the given string against the corresponding regular expression.\n\n\tArgs:\n\t    uuid: the UUID to validate\n\t\n\tRaises:\n\t    ValidationError: if the given string is not part of the valid UUID.\n\t\"\"\"\n\t\ndef valid_value(value):\n\t\"\"\"\n\tValidate a value"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\t# parse the regular expression\n\tm = _re_valid_regular.search(key)\n\t\n\tif m:\n\t\t# If it is a regular expression, parse it again\n\t\tif not m.group(0):\n\t\t\traise ValidationError(m.group(0))\n\t\treturn m.group(0)\n\t\n\t# Not a regular expression, try to match the regular expression\n\tm = re.search(re.compile(key), re.IGNORECASE)\n\tif m:\n\t\traise ValidationError(m.group(0))\n\t\n\traise ValidationError(key)\n\t\t\t\ndef"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\ndef build_key_regex(key):\n\t\"\"\"\n\tBuild regular expression to a regular expression.\n\t\n\tArgs:\n\t    key: the string that will be interpreted as a regular expression.\n\t\n\tRaises:\n\t    ValidationError: if the given key isn't conform to the regular expression.\n\t\"\"\"\n\t\ndef is_key_valid(key):\n\t\"\"\"\n\tCheck if the given key is valid and all of its components\n\tmatches.\n\t\n\tArgs:\n\t    key: the string that will be interpreted as a regular expression.\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\tif key[-1] in '0123456789':\n\t\traise ValidationError(\"Valid characters are allowed in the key names and underscores\")\n\telif key[-1]!= \"'\":\n\t\traise ValidationError(\"Key must be in the form {0}\".format(value))\n\n\ndef validate_str(str):\n\t\"\"\"\n\tValidate a string against the regular expression given\n\tby the user (see above).\n\t\n\tArgs:\n\t    str: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the string is not conform to the regular expression.\n\t\"\"\"\n\t\n\t\n\t# The expression must contain only lowercase"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif (type(key) == str) and (len(key) > 255):\n\t\t\tassert (len(key) > 253) or (len(key) > 254), \"Invalid input.\"\n\t\n\t\tif (not re.match(pattern, key)):\n\t\t\traise ValidationError(\"Invalid pattern.\" + key)\n\t\n\t\n\treturn key\n\ndef validateString(s):\n\t\"\"\"\n\tValidate the given string to conform to the regular expression.\n\t\n\tArgs:\n\t    s: the input string\n\t\n\tRaises:\n\t    ValidationError: if the given string is not conform to the regular expression.\n\t"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tfor (field_name,field_value) in manifest_dict.items():\n\t\tif field_name in'required': continue\n\t\tif first_level:\n\t\t\tif field_name not in'required' and field_value is not None:\n\t\t\t\tcontinue\n\t\tdict_dict = dict((field_name,field_value))\n\t\n\treturn dict_dict\n\t\n\n# TODO: Add a check function for this function\ndef generate_initial_observer_schema(schema_name, deployment_name, config_name,\n\t\t\t\t\tlast_level_schema_name=None,\n\t\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tdef _flatten_schema(schema):\n\t\n\t\treturn {\n\t\t\t'description': '',\n\t\t\t'id': 'None', # The name of the resource\n\t\t\t'id_type': 'None',\n\t\t\t'name': None, # The human-readable identifier of the resource\n\t\t\t'kind': 'None', # The kind of resource\n\t\t\t'measure_id': None,\t# the description of each specular resource in the schema file\n\t\t\t'measure_name': None, # The human-readable name of each specular resource in the schema file\n\t\t\t'version': None"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\t# Generate default_schema_dict for all of the different ways in\n    # this list\n\tdefault_schema_dict = {}\n\t\n\t# Check if this is the first level of a dict\n\tif first_level:\n\t\tdefault_schema_dict, default_schema_dict = generate_default_schema_from_type_dict(\n            manifest_dict, 'Kubernetes')\n\t\t\n\t# Replace the default_schema_dict with a specific mapping of the\n\t# default_schema values to the object type\n\tdefault_schema_dict['type'] = default_schema_dict\n\t\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tdef copy_missing_data(missing_dict, dictionary):\n\t\tfor element in missing_dict:\n\t\t\tif element!= dictionary:\n\t\t\t\tcopy_missing_data(element, dictionary)\n\t\t\n\t\tdictionary = copy(dictionary)\n\t\tfor key in missing_dict:\n\t\t\tif key in missing_dict:\n\t\t\t\tdictionary[key] = missing_dict[key]\n\t\t\n\t\tfor name, value in missing_dict.iteritems():\n\t\t\tif not isinstance(value, dict) and not isinstance(name, str):\n\t\t\t\tcopy_missing_data(name, value)\n\t\n\t\t\t\n\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n    def generate_default_pod_schema(manifest_dict=None, first_level=False):\n\n        def get_default_pod(name, resource):\n            try:\n                v = [item[name] for item in resource.metadata['values']]\n            except KeyError:\n                _log.error('Invalid default pod at', name)\n                return None, None\n            for _item in v:\n                if _item == None:\n                    continue\n                _item = _item.copy()\n                if first_level:\n                    _item = _item[None]\n                _item[\"metadata\"][name] = _item\n               "}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tmetadata_dict = manifest_dict.copy()\n\t\n\tmetadata_dict['first_level'] = True if first_level else False\n\tmetadata_dict['unique_field_types'] = [\n\t\t'name',\n\t\t'schema',\n\t]\n\n\t# The metadata of all the required fields are defined in the `fields` dict\n\tfields_present = [\n\t\t'full_template_url',\n\t\t'template_url',\n\t\t'full_template_version',\n\t]\n\tfields_missing = [\n\t\t'full_template_id',\n\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\ndef generate_default_observer_schema_list():\n\t\"\"\"\n\tThe function is called recursively to generate part of a default ``observer_schema``\n\tfrom part of a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tThe values of the identifiers are copied directly from the ``manifest_dict``.\n\t\n\tThe first level dictionary is changed to the empty one.\n\t\"\"\"\n\t\ndef generate_default_action_schema_dict():\n\t\"\"\"\n\tThe function is called recursively to generate part of a default ``action_schema``\n\tfrom part of"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tdict = dict((k, parse_spec(v)) for k, v in manifest_dict.iteritems() if v)\n\t\n\tkeys_to_new = list()\n\t\n\tfor k, value in sorted(dict.iteritems()):\n\t\tdict_new = dict((k, parse_spec(v)) for k, v in dict.iteritems() if v)\n\t\tfor key in v:\n\t\t\tdict_new[key] = parse_spec(dict_new[key])\n\t\t\n\t\tkeys_to_new.append(k)\n\t\n\tvalues = []\n\t\n\tfor k in keys"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t\n\tmanifest_dict_iter = iter(manifest_dict.items())\n\tunique_items = set(manifest_dict.iterkeys())\n\t\n\tfirst_level_dict = {}\n\tfor item in manifest_dict_iter:\n\t\tkey, val = item\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif key not in unique_items:\n\t\t\tunique_items.add(key)\n\t\t\tfirst_level_dict[key] = val\n\t\telse:\n\t\t\tif first_level:\n\t\t\t\tcontinue\n\t\n\tunique_items -= unique_items\n\t\n\treturn dict((k"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\n\t# first level is set so we can add metadata in the mapping (if this is already\n\t# the case, it is not in fact of a set of metadata items to use with a\n\t# single-level schema\n\tdef _add(item):\n\t\tkey = item.metadata_key\n\t\tdefault_value = item.default_value\n\t\tif isinstance(default_value, (list, dict)):\n\t\t\tdefault_value = default_value[0] if isinstance(default_value, list) else default_value\n\t\titem.metadata_value.add(key, default"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\tfor field in response:\n\t\tif field.get('type') == 'object':\n\t\t    observer_field = field['childKey']\n\t\t    if observer_field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observer_field] = {}\n\t\t\n\t\tlatest_value = latest_last_applied_manifest(field)\n\t\tlatest_last_added = latest_last_applied_manifest_value(field)\n\t\tlatest_updated = latest_last_applied_manifest_list_from_resp(\n\t    last_applied_manifest[last_applied_manifest_field"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\tlast_applied_manifest_dict = {}\n\tupdate_last_applied_manifest_list_at_resp(response, last_applied_manifest_dict, observer_schema)\n\t\n\treturn last_applied_manifest_dict\n\n##################################################################################\n\tdef update_last_applied_manifest(last_applied_manifest, observer_schema):\n\t\t_logger.info('Updating the OBP')\n\t\tif not last_applied_manifest:\n\t\t\tlast_applied_manifest = []\n\t\t\n\t\t_logger.debug('Observation: %s', observer_schema)\n\t\tlast_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tupdate_resp = []\n\t\tfor datum in last_applied_manifest:\n\t\t\tif datum in observer_schema:\n\t\t\t\tupdate_resp.append(datum)\n\t\tif not update_resp:\n\t\t\treturn None\n\t\n\tdef get_latest_version(self) -> str:\n\t\t\"\"\" Returns the contents of the latest version of the object \"\"\"\n\t\treturn self.__version\n\tdef get_version(self) :\n\t\t\"\"\" Returns the contents of the version information \"\"\"\n\t\treturn self.__version\n\tdef set_latest_version(self, version: str) :\n\t\t\"\"\""}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n    @classmethod\n    def is_complete(meta):\n        \"\"\"Check whether ``last_applied_manifest`` is complete\n\n        Args:\n            meta (dict): complete ``metadata`` of ``last_applied_manifest`` being\n                updated.\n            \n        Returns:\n            bool: ``True`` if complete, ``False`` otherwise\n\n        This function should be safe to use in conjunction with the ``is_full_list``\n        or ``full_list`` methods, but if you want to know it is completely\n        complete, call the ``full_list()`` method from ``full_list`` instead.\n\n        \"\"\"\n        return (meta and isinstance"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t        # first validate list of observer_schema\n\t        observed_fields = get_required_fields(\n\t\t\t                            observer_schema)\n\t        for field in observed_fields:\n\t\t      \t    if not has_required_field(field):\n\t\t\traise ValueError(\"Not a required field\")\n\t\t\n\t\tfor observer in observation.ObservedFields.values():\n\t\t      \t    self.__validate_list(observer, response)\n\n\t        # now update any new fields in observer\n\t        for field in observer_schema:\n\t\t      \t    if field in observer:\n\t\t        \t    new_value = update_only_on"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# Get the first result for each field\n\t\tfor field_name in fields_dict.keys():\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif field_name not in observer_schema:\n\t\t\t\tobserver_schema[field_name] = {}\n\t\t# Update the first results\n\t\tif not last_applied_manifest:\n\t\t\tresponse = None\n\t\telse:\n\t\t\tresponse = last_applied_manifest\n\t\t\n\t\t# Append the response to the list of results\n\t\tif response:\n\t\t\tif"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\t# check that we got something...\n\t\ttry:\n\t\t\tresponse[last_applied_manifest] = len(response[last_applied_manifest])\n\t\texcept KeyError:\n\t\t\t# maybe some fields are empty or non-empty (like the one which\n\t\t\t# in turn isn't included in the response, which is\n\t\t\t# a little mesaky...)\n\t\t\tpass\n\t\t\n\tif observer_schema is not None and response is not None:\n\t\t# grab the last update notification (this will make it a little bit \n\t\tlast_update_notification = None\n\t\tfor i in range("}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tfor field in last_applied_manifest_list:\n\t\t\tif field[1] not in observer_schema:\n\t\t\t\tfield[1] = current_version\n\t\t\t\tfield[0] = field[1]\n\t\t\t\t_update_last_applied_manifest_list_from_resp(field)\n\t\tobserver_schema = last_applied_manifest_list\n\n\ndef delete_last_applied_manifest_list_from_resp(\n\t\"\"\"\n\tDelete the last applied manifest list from the response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tif last_applied_manifest:\n\t\t\tfor key, value in last_applied_manifest.iteritems():\n\t\t\t\tif value is None:\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tif value == observer_schema:\n\t\t\t\t\t_update_response(key, value)\n\t\t\t\telse:\n\t\t\t\t\tlast_applied_manifest_dict_for_object(key, value)\n\n\ndef list_latest_modified_on(query, datefield=\"\", when=None):\n\t\t\"\"\"\n\t\tList the ``LatestModifiedOn``.\n\t\t\n\t\tArgs:\n\t\t\tquery (Query): Query string from which to query\n\t\t\tdatefield"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t        current_state = 'latest_applied'\n\t        for item in last_applied_manifest:\n                if item[\"state\"] == current_state:\n                    current_state = item[\"state\"]\n\t        if current_state == 'latest_applied':\n\t\t    item[\"state\"] = 'latest'\n\t\telse:\n\t\t    if current_state == 'pending':\n\t\t\t    item[\"state\"] = 'pending'\n\t\t    elif current_state == 'waiting_for_next':\n\t\t        item[\"state\"] = 'pending'\n\t\t        if self.config.debug:\n\t\t\t    logger.debug(item)\n\t\t\tself.last"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\ndef get_last_applied_manifest_list_from_resp(\n\t\"\"\"\n\tReturns a generator of last_applied_manifests from the response\n\t\n\tThe generator returns a list which contains a 'last_applied_manifest' set\n\tfor each field value in the response.\n\n\tArgs:\n\t    last_applied_manifest (dict): a ``last_applied_manifest``\n\tRaises:\n\t    KeyError: If the observed field is not present in the kubernetes response\n\t\n\tThis function go through all observed fields, and return the last_applied_manifest\n\tobject for the given field and its"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tresponse = response\n\twith open(\".last_applied_manifest.json\") as f:\n\t\tlast_applied_manifest = json.load(f)\n\t\n\treturn last_applied_manifest\n\n\ndef update_latest_pod_version_dict():\n\tversion_info = get_latest_pod_latest_version()\n\tif version_info is not None:\n\t\tdate_added = version_info.get(\"date_added\", None)\n\t\tlatest_version = get_latest_pod_latest_version(date_added)\n\t\tlatest_pod = version_info"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t  \n\t\n\t# If we're a last-applied version before the 'last_applied_manifest',\n\t# this will be the entire partial document\n\tlast_applied_manifest = dict([(k, v) for (k, v) in last_applied_manifest.iteritems() if \n\t    (not isinstance(v, (unicode, list, set, float)) and v!= v)])\n\t\n\tif last_applied_manifest.has_key(observer_schema[observer_schema.keys()[0]]):\n\t\traise KeyError(str(observer_schema[observer_schema[observer_schema."}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t# Obtains the field from the partial response\n\tresponse['last_applied_manifest'] = {}\n\tfor field in _response_fields:\n\t\tif field not in last_applied_manifest:\n\t\t\tobserver_schema[field] = None\n\t\tif isinstance(last_applied_manifest[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tobserver_schema=None,\n\t\t\tresponse=dict((k, last_applied_manifest[field][k]) for k in range(len(last_applied_manifest)))\n\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    \n\t\n\t# Build a partial dict from JSON encoded dictionary\n\tpartial_dict = {}\n\tfor field in ('last_applied_manifest',):\n\t\tif field in observer_schema:\n\t\t\tpartial_dict[field] = observer_schema[field]\n\t\telse:\n\t\t\tpartial_dict[field] = str(datetime.datetime.utcnow())\n\t\n\t    \n    # Get a partial dict from the HTTP response\n\tresponse_dict = json.loads(requests.get(\n\t'https://raw.githubusercontent.com/OpenShift/OscClient/' + \\\n\tstr("}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest, observer_schema, response\n\t\t)\n\t\n\tdef update_last_applied_manifest_dict_from_resp(\n\t\tself, last_applied_manifest, observer_schema, response\n\t):\n\t\t\"\"\"\n\t\tUpdates partial ``last_applied_manifest`` dict from the response.\n\t\t\n\t\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t        observer_schema = observer_schema or []\n\t        # get latest one first\n\t\tlatest = first = last_applied_manifest.get('last_applied_manifest_version', '0')\n\t\t# check to see if the current schema is still present, or if not,\n\t        # if so, raise error if not\n\t        if not latest:\n\t\t        raise KeyError(\"Missing latest_modified set in Kubernetes response.\")\n\t\t# this is most likely an update\n\t\tif not latest.get('modified'):\n\t\t\traise Exception('Missing modified set in Kubernetes response.')\n\t\t# check if"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t     \n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = []\n\n\t\t# Only update latest applied information if any.\n\t\tif field in last_applied_manifest.keys():\n\t\t\tupdated_last_applied_manifest = \\\n\t\t\t\tlast_applied_manifest[field] + [last_applied_manifest[field]]\n\n\n\t\t\tif (not observer_schema[field].index(last_applied_manifest[field]))\\\n\t\t\t\tor (not last_applied_manifest[field]):\n\t\t\t\tif"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t\n\tif last_applied_manifest is None:\n\t\traise KeyError(\"No values in last_applied_manifest\")\n\t\n\tassert observer_schema is not None\n\t\n\tfor name, value in observer_schema.items():\n\t\tif name in last_applied_manifest:\n\t\t\tcontinue\n\t\tlast_applied_manifest[name] = value\n\t\n\tlast_applied_manifest['last_applied_manifest'] = last_applied_manifest['last_applied_manifest'].copy()\n\t\t\n#\treturn last_applied_manifest\n\n\ndef update_last_applied_manifest_list_from_resp"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\tif last_applied_manifest:\n\t\t\traise KeyError(\"Cannot update last_applied_manifest fields \"\n\t\t\t\t\t- last_applied_manifest is mandatory for Kubeflow response generation\")\n\t\t\tlast_applied_manifest = collect_json(last_applied_manifest,\n\t\t\t\t\t              data_field=['last_applied_manifest'])\n\n\t\tif observer_schema:\n\t\t\tobserver = {'actor': str(observer_schema['actor'])}\n\t\t\tobserver['type'] = 'update'\n\t\t\tresponse = collect_json(observer, data_field=['last_applied_manifest'])\n\t\t\n\t\tif response:\n\t\t"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\tschema = {\n\t\t\t'message': {\n\t\t\t\t'description': \"Base resource\",\n\t\t\t\t'type':'string',\n\t\t\t\t'type': 'object',\n\t\t\t\t'objectProperties': {\n\t\t\t\t\t'status': 'complete',\n\t\t\t\t\t'description': \"A resource description that can be \"\n\t\t\t\t\t\t\t\"added to the node/nodes. This should describe \"\n\t\t\t\t\t\t\t\"what resource to expose.\",\n\t\t\t\t\t'version': 0\n\t\t\t\t}\n\t\t\t},\n\t\t\t'schema': {\n\t\t\t\t'description': \"Base resource schema\",\n\t\t\t\t'type': 'object',\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\t_meta_path_schema = os.path.join(app.path,'meta.yaml')\n\t\tif not os.path.exists(os.path.dirname(os.path.dirname(\n\t\t\t\tos.path.dirname(__file__)))) and \\\n\t\t\tos.path.exists(os.path.join(app.path, _meta_path_schema)):\n\t\t\ttry:\n\t\t\t\tos.makedirs(os.path.dirname(os.path.dirname(os.path.dirname(\n\t\t\t\t\tos.path.dirname(__file__))))\n\t\t\texcept OSError as exc"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\tschema = {\n\t\t\t'name': 'default-observer-schema'\n\t\t}\n\n\t\tfor resource in app.resources:\n\t\t\tschema['paths'] = {\n\t\t\t\t'schema': {\n\t\t\t\t\t'type': 'object', 'properties': {\n\t\t\t\t\t\t'name': resource.name\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tapp.resources.append(schema)\n\n\tdef get_object_by_resource_name(self, resource_name):\n\t\t'''\n\t\tFind the object in the namespace given by ``resource_name``.\n\t\t\n\t\tArgs:\n\t\t    resource_"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\twith open(spec.manifest, 'r') as f:\n\t\tdefault_schema = next(iter(f), None)\n\n###############################################################################################################################\nclass DefaultObserverController(object):\n\t\"\"\"\n\tAbstract base class implementation of the default observer controller\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): the application for which to create a\n\t        default observer controller\n\t\"\"\"\n\t\n\tdef __init__(self, app):\n\t\tif app.config['default_observer']['name'] == 'kube-proxy':\n\t\t\tself._app = _get_kube_"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\n\tdefault_observers_schema = dict()\n\t\n\t\n\tfor path in get_k8s_client_resources_from_dir(\"default.json\"):\n\t\t# if k8s_client_resources_path is None, raise.\n\t\tif not os.path.basename(path).startswith(\"Default Resource\") or path not in default_observers_schema.keys():\n\t\t\tlogger.debug(\"default.yml: Default resource {} not present; generating default\".format(path))\n\t\t\tnew_schema = create_default_resource_json(path)\n\t\t\tdefault_observers"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\t@app.errorhandler(KrakesException, 'kubernetes.statuscode')\n\t@app.errorhandler(KrakesException, 'kubernetes.resourcekind')\n\tdef custom_observer_error_handler(error):\n\t\tprint(error.details)\n\t\tprint(config.describe(\"Error\"))\n\n\tfrom kraken.api import get_default_pod_schema, get_pod_data\n\tfrom kraken.exceptions import CronException\n\tfrom kraken.types import Namespace, Event, EventKind, ResourceKind, \\\n\t\tProperty, Object"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\t\n\tfor resource in sorted(app.get_resources(KINCK_RESOURCE_PATHS)):\n\n\t\tif resource.get('type') == 'kube-api-schema':\n\n\t\t\tfor resource_type in KINCK_RESOURCE_PATHS[resource]['types']:\n\t\t\t\tif (len(KINCK_RESOURCE_PATHS[resource]['spec']['resources']) <= 2 and\n\t\t\t\t\t\tresource.get('type') == resource_type):\n\t\t\t\t\tcontinue\n\n\t\t\t\tif resource.get('type') in ('kubernetes-env', 'kube-api') and \\\n\t\t\t\t\t\tresource.get('spec'):\n\t\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tconfig = {\n\t'resources': [\n\t\t {\n\t\t\t\t'kube-master': os.environ.get('KERBEROS_MASTER'),\n\t\t\t\t'kube-executor-address': os.environ.get('KERBEROS_EXECUTOR_ADDRESS'),\n\t\t\t\t'kube-master-instance': os.environ.get('KERBEROS_MASTER_INSTANCE'),\n\t\t\t\t'kube-master-cluster-address': os.environ.get('KERBEROS_MASTER_CLUSTER_ADDRESS'),\n\t\t\t\t\n\t\t\t\t# Kubernetes resource names (e.g. \"http://master.cluster.name/v1"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\ndef get_config_file_uri(default_config):\n\t\"\"\"\n\tParses the specified Kube config file path, returning an URI that includes\n\ta URL name for all Kubernetes resources that may already be registered\n\twith the app specified.\n\t\n\tArguments:\n\t    default_config (str): The path to the default Kube config file path\n\t\"\"\"\n\trelative_location = default_config\n\tif os.path.isabs(default_config):\n\t\trelative_location = os.path.dirname(default_config)\n\t\tdefault_config = os.path.join("}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\tif os.path.exists(\"/var/www/cassandra_datasources/custom_schema/common.yml\"):\n\t\t\tcommon.logger.info(\"Found custom observer schema schema schema: %s\", common.kv.DEFAULT_CRON_SCHEMA)\n\t\t\twith open(\"/var/www/cassandra_datasources/custom_schema/common.yml\") as f:\n\t\t\t\tcommon.kv.DEFAULT_CRON_SCHEMA = yaml.load(f)\n\t\t\t\t\n\t\tif not common.kv.get(\"custom_observer_schema\"):\n\t\t\tlogger.warning(\"Creating custom observer schema schema"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\treturn self._apply(formatParamClause, sql, params)\n\t\n\tdef _apply(self, formatParamClause, sql, params):\n\t\tif isinstance(sql, compat.text_type):\n\t\t\tsql = sql.encode(params.get('charset', 'utf8'),'replace')\n\t\tif params.get('in_style', ''): # special case for special IN style params\n\t\t\tsql = util.escape(sql) # no escaping\n\t\t\n\t\tfor key in fmtParamClause.findall(sql):\n\t\t\tif key.startswith(\"?\"):\n\t\t\t\tkey = key[1:]\n\t\t"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\treturn _convert_sql(\n\t\t\tparams, format=format, paramnames=paramnames)\n\t\n\tdef format_id(\n\t\tself, identifier_name, max_length=6, omit_none=False,\n\t\texclude_col=None, col_name=None, verbose=False,\n\t\tis_id_col=False):\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\t\tthe in-style parameters.\n\t\t\n\t\t*identifier_name*\n\t\tThe name of the identifier (eg: \"user_id_\") to be converted to"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\t_dbparams = {\"in_style\": True} if type(sql) == type(str()) else {'in_style': True}\n\t\n\t_params = _dbparams.setdefault(\"params\", {})\n\t\n\tif paramstyle:\n\t\tparamstyle = paramstyle.lower()\n\t\tif paramstyle == 'all':\n\t\t\tparamstyle = ['none']\n\t\telse:\n\t\t\tparamstyle = tuple(paramstyle)\n\t\n\ttry:\n\t\tdbparams = _params[\"sql\"]\n\texcept KeyError:\n\t\tpass\n\t\n\t# convert to lists\n\tin_list = [iter"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\treturn format_table(*map(parse_output, format_query.split(\"|\")))\n\n\ndef _format_table(sql, format):\n\t\"\"\"\n\tHelper function that formats SQL to a table.\n\t\n\tThis function uses the table name, such as ``x;`` to determine the\n\tsql format to use, and for each result return the appropriate SQL.\n\tThe table format is normally specified by the format string.\n\t\n\tThe format string :class:`str` is assumed to always return the\n\tdata without having all columns name the same as the column name\n\tfor a statement"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tdef get_formatted_sql(sql, params, **kw):\n\t\t\"Get a formatted SQL query.\"\n\t\tc_sql = sql.encode(\"utf8\", errors=\"replace\")\n\t\tsql = c_sql + params\n\t\tresult = sql_from_dict(result)\n\t\treturn result\n\t\n\tdef parse_parameters(params, values):\n\t\t\"Parse a parameter list to a tuple.\"\n\t\t\n\t\tif isinstance(values, Mapping):\n\t\t\tout_style = params[\"out_style\"]\n\t\t\tout_fmt = params[\"out_fmt\"]\n\t\t\t\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\t(db, dbParamMap) = _formatsqlParam(dbParamMap)\n\t\n\tout = _sqlparamFormat(sql, params)\n\t\n\treturn (out, db, dbParamMap)\n\t\n\ndef _formatsqlParam(dbParamMap):\n\tsqlParam = 'SELECT %s FROM %s'\n\tsqlParam = sqlParam % (';'.join('?' for s in dbParamMap), dbParamMap)\n\treturn sqlParam.upper()\n\t\ndef _sqlparamFormat(sqlString):\n\t\n\t\"\"\"\n\tFormat the SQL query to use in the in-"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\t\n\tin_style = sql.sql.in_style\n\t\n\t\n\tif sql.sql.isscalarlike(sql.sql_param(sql.sql_param(sql.sql_query(sql.sql_result())))):\n\t\treturn (\"(CAST(''{}' : VARCHAR('%s') as FLOAT)\"\n\t\t\t .format(sql.sql_param(sql.sql_result()), in_style)))\n\n\tif  sql.sql.iscollectionlike(sql.sql_param(sql.sql_param(sql.sql_result()))) and sql.sql.sql_param"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tif isinstance(sql, Table):\n\t\tsql = sql.to_pymysql()\n\n\telse:\n\t\tsql = sql.to_pymysql()\n\t\n\tformatted_rows = []\n\t\n\t\n\tif len(sql.db) > 1:\n\t\ts = sql.db[0]\n\t\t\n\t\t# Check if the database contains some values for\n\t\t# columns in the query before the SQL query is run\n\t\t# to be executed.\n\t\tif isinstance(s.params, Mapping):\n\t\t\tparams = {}\n\t\t\t\n\t\t\tfor k in params.keys():\n\t\t\t\tif s.params."}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\n\t# TODO: add back compatibility with in_style-based classes\n\tdef __init__(self, sql, params=None):\n\t\tself.sql = sql\n\t\tself.params = params or {}\n\t\tself.in_style = False\n\t\tif self.params:\n\t\t\tif not isinstance(self.params, Mapping):\n\t\t\t\traise TypeError(unicode(self.params))\n\t\t\tself.convert()\n\t\t\tself.params.update(self.params.default_values.items())\n\t\telse:\n\t\t\tself.in_style = True\n\t\t\tself.convert()\n\n\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\t\n\tparams = _convert_params(params)\n\treturn (\n\t\tsql_to_kwargs(\n\t\t\tparams,\n\t\t\tdefaults,\n\t\t\tuse_builtin_map=use_builtin_map,\n\t\t\tmapfunc=format_map,\n\t\t),\n\t\t(\n\t\t\tsql_to_kwargs(\n\t\t\t\tparams,\n\t\t\t\tdefaults.get(key, key) for key in (\n\t\t\t\t\t'sql', 'param', 'positional', 'named'),\n\t\t\t\t\t'dns', 'default', 'desc', 'alias'\n\t\t\t\t)\n\t\t\t),\t*params.values(),\n\t\t"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\treturn self.sql(sql, many_params, convert_type=convert_type)\n\t\t\n\tdef sql(self, sql, many_params=None, convert_type=None):\n\t\t\"\"\"\n\t\tConstruct and return an SQL query.\n\t\t\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL statement to be queried\n\t\t*many_params* (:class:`~collections.abc.Iterable`) contains each set of\n\t\tconverted parameters (*params*).\n\t\t\n\t\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc."}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\treturn self._convert(sql, *many_params.keys(), **many_params.values())\n\t\n\tdef format(self, *sql, **many_params):\n\t\t\"\"\"\n\t\tConvert an SQL query into a formatted SQL query.\n\t\t\n\t\t-       *sql*: the SQLAlchemy query object that will be formatted. Should be a one-element\n\t        element list where each element will be a dictionary to be passed to\n\t\t:meth:`formatall`.\n\t\t-       *many_params*: mapping of keys to values to be converted into a tuple\n\t        containing the converted parameters (tuple of in-"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\tif types is str or types is bytes:\n\t\t\tin_style = 'IN_'\n\t\t\tmany = 'M'\n\t\telif types is Sequence:\n\t\t\tin_style = 'INS'\n\t\t\tmany = 'MAR'\n\t\telse:\n\t\t\traise ValueError('Invalid sql parameter types : (%s)' % (types,))\n\t\t\n\t\tparams = copy.deepcopy(self.params)\n\t\t\n\t\tfor name, param in six.iteritems(params):\n\t\t\tif param['type'] in ('STR', 'FLOAT', 'BIT', 'VARBINARY', 'VARINTEGER', 'VARSMALLINT', 'VAR"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\treturn _table_formatmany(\n\t\t\tself.__repr__(),\n\t\t\tparams.convert(params.in_style, params.in_db, params.db, params.params))\n\t\n\tdef format_oneparam(self, values, params):\n\t\t\"\"\"\n\t\tConverts the values for the parameter values to format with the \n\t\tin-style parameter values supplied.\n\t\t\n\t\t*values* (:class:`float`) is the amount of values for the \n\t\t\"parameter values\", which is a multiple of the limit for the \n\t\tvalues to be returned. A single value, if specified on the \n\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\tformatdict = {\n\t\t\t'sql': self.sql,\n\t\t\t'params': self.params,\n\t\t\t'in_style': self.in_style\n\t\t\t}\n\t\treturn [formatdict[r] for r in [self.params, self.in_style]]\n\t\t\t\n\n# Python 2 & 3 compatibility\n\ndef getparam_type(obj, paramtype):\n\t\"\"\"Return the input parameter type, as string, e.g. 'double_precision' (with double precision).\n\tIf we get a type without a type converter, that we can't return\n\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\traise NotImplementedError(\"getmany\")\n\n\tdef getone(self, key):\n\t\t\"\"\"\n\t\tThis method handles a single query result with one of the\n\t\tfollowing:\n\t\t\n\t\t- :attr:`.SQLResults.fetchone` return :class:`list` of values if no key is\n\t\tgiven.\n\t\t- :attr:`.SQLResults.fetchone`.\n\n\t\tIt is the equivalent to the calling ``fetchone()``.\n\t\t\n\t\tSee :meth:`get` for more information on :class:`namedtuple` and \n\t\t:meth:`getone`.\n\t\t\n\t\t:param key: Query key\n\t\t\n\t\t.. note:: Only the"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t    \tparams = iter(self._get_data())\n\t        out, params = [], param[:]\n\t        while True:\n\t\t    if params:\n\t\t    params.pop()\n\t\t    param = []\n\t\t    out.append(self.parse_param(param))\n\t\t    if not params: break\n\t\t    params.extend(self.convert_to_python_parameter(param) if param else [])\n\t\t    for param in params:\n\t\t        if param == None: raise Exception(\"Empty parameter given.\")\n\n\t\t        # Do nothing if there are no parameters\n\t\t        out.append(self.format_parameters_for(param, out,params"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\tif not self.query:\n\t\t\traise ValueError('The sql query must be provided before the :class:`Query` instance.')\n\t\tqstr = u''\n\t\tif self.is_literal: qstr += u'IS NOT NULL'\n\t\telif isinstance(self._query, self._query._QueryBase):\n\t\t\tif not self.name and self.parameters:\n\t\t\t\tqstr += u' '.join(['%s: %s' % item for item in self.parameters])\n\t\t\telse:\n\t\t\t\tqstr += u'[{}]'.format(self._query)\n\t\telse:\n\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n    \nclass SQLParams(DictLike):\n\tdef __init__(self, **params):\n\t\tDictLike.__init__(self, **params)\n\t\t\t\t\t\t\n\t\tclass SQLParams.in_style:\n\t\t\n\t\t\tdefault=None\n\t\t\n\t\t\t# Format the params as a sequence of the form `(name, value)` pairs\n\t\t\t# Example: (`0, 2, 3, 4)`, `[1, 2, 3]`\n\t\t\tparams_values = [str(p_value) for p_value in params.values()]\n\t\n\t\t\tparams_str = \", \".join(\"%s=%s\" %"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\t\n\tdef _formatparams():\n\t\toutparams = []\n\t\t\n\t\tfor paramname, paramtype in params.items():\n\t\t\tparamval = paramtype(db_ctx, paramname)\n\t\t\t\n\t\t\t# Convert parameter name string to datatype strings\n\t\t\tif isinstance(paramval, str):\n\t\t\t\tif paramname in db_ctx.get_param(db_ctx._name, paramname):\n\t\t\t\t\tparamval = params[paramname]\n\n\t\t\telif isinstance(paramval, int):\n\t\t\t\t# Increment outparams counter\n\t\t\t\toutparams.append((paramname, paramtype(db_ctx"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tif self.path is not None:\n\t\treturn True\n\n\twarnings = []\n\tfor ocfla_node in self.getObjs():\n\t\tif ocfla_node.validate(path):\n\t\t\twarnings.append( ocfla_node )\n\t\n\tif not warnings: return True\n\telse: return False\n\n#####\n# Path OCFL object.\n\n\ndef paths_ocfla( self ):\n\t\"\"\"\n\tReturn list of OCCCFL's. \n\t\"\"\"\n\t\n\t\n\tif self.root is None: return []\n\n\tocfalfiles"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\terrors, warnings = self._validator.validate(path,\n\t\tfileutil.find(path),fileutil.find(os.path.dirname(path)),False)\n\t\n\t\n\treturn errors[0]\n\ndef is_valid(path):\n\t\"\"\"\n\tCheck which object (or root object) is valid.\n\t\n\tRaises a FileCheckError if object is not valid.\n\t\"\"\"\n\t\t\n\t\n\t\n\terrors, warnings = self._validator.validate_valid(path,fileutil.find(path),False)\n\n\t\n\t\n\tif errors:\n\t\treturn errors[1]\n\t\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\t# Validate base type for file (including file_path).\n\tif path.is_file() and path.is_dir():\n\t\tif not os.path.exists(path.join(self.base_dir, \"src\")):\n\t\t\treturn False\n\t\treturn True\n\tif not path.is_file() and not path.is_dir():\n\t\treturn False\n\t\n\tif os.path.isfile(path.join(self.base_dir, \"__init__.py\")):\n\t\treturn False\n\tif not os.path.exists(path.join(self.base"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\tif not path.startswith(self.pyfsRoot):\n\t\t#TODO: Handle relative files\n\t\treturn False\n\t\t\n\twith open(os.path.relpath(path), self.pyfsRoot) as f:\n\t\tif path.startswith(self.pyfsRoot):\n\t\t\t#TODO: Handle relative files\n\t\t\tpath = path[len(self.pyfsRoot):]\n\t\t\n\t\ttry:\n\t\t\tjson.loads(f.read())\n\t\texcept:\n\t\t\treturn False\n\t\t\n\t\treturn True\n\t\n\t\n\tdef get_version(self):\n\t\ttry:\n\t\t\twith open(\""}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tif path[0] == '.':\n\t\tname, ext = os.path.splitext(path)\n\t\tif name:\n\t\t\ttry:\n\t\t\t\tself.validate_file(path)\n\t\t\texcept ValidationError:\n\t\t\t\tname = name[path.rfind('.') + 1:]\n\t\treturn name is not None\n\t\nclass OSCFL(_OSCFL):\n\t\"\"\"\n\tOSCFL instance and object.\n\t\n\t\"\"\"\nclass _OSCFL_objects(ListType):\n\t\"\"\"\n\tOSCFL `items` lists instances of `PyFuseOSCFL` objects."}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\tif not '.' in path:\n\t\treturn False\n\t\n\twith pyfs_lib import fs\n\tfs.validate_filesystem_root(path)\n\t\n\treturn True\n# ----------------------------------------------------------------------------\n# cocotbed.py -- Cocotbed Framework\n# Copyright (c) 2009--2019 Nicola Ille\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions \n# are met:\n#\n# * Redistributions of source code must retain the above copyright \n# notice,"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\tif not filePathIsValid(path):\n\t\treturn False\n\t\t\t\t\n\tdef getDirectoryContents(self, directory, recurse=0, check=not_none):\n\t\t\"\"\"\n\tGet all the attributes and data of a directory\n\t\"\"\"\n\n\t\tdirContents = os.listdir(directory)\n\t\treturn [s for s in dirContents\n\t\t\t if not check(s)]\n\t\t\n\tdef getChildrenList(self, path, recurse):\n\t\tdef children(items):\n\t\t\tfor i in items:\n\t\t\t\tyield i\n\t\t\t\n\t\tchildren = []\n\t\tfor directory in children(path):"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\tglobal _root_module, _root_file, _root_name\n\t\n\tif type(path) is str:\n\t\tpath = str(path)\n\t\n\tif os.path.isdir(path) and not self.parse_module(path):\n\t\tprint (\"Error: module for path: '\" + path + \"' has no \" + \"required elements\")\n\t\tprint (\"Error: invalid file path or PyFS object: \" + path)\n\t\treturn False\n\t\t\n\t_root_module = path\n\t_root_file = path\n\n\tif not os.path.isfile(path"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\n\tfor fname, fntname in six.iteritems(OCF_FILE_MAPPING):\n\t\tif path.name == fname:\n\t\t\t# check if already exists\n\t\t\texists = True\n\t\t\tfor fn, fnamepath in OCFL_FILE_MAPPING[fname].iteritems():\n\t\t\t\tif fn.name == fnamepath:\n\t\t\t\t\texists = True\n\t\t\t\t\tif not hasattr(self, '_cache'):\n\t\t\t\t\t\traise ocferror.OcfLibError(\"Found duplicate OCFL file %r, instead, check OCFL_FILE_MAPPING.\", fn)\n\t\t\t\t\t\n\t\t\t\t# check if file"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n#\tCheck that an OCFL type exists.\n\tvalidTypes = list(set([\n\t\t\"OCDFL\", \"OCEF\", \"OCEI\", \"OCIF\", \"OFF\", \"OS\", \"OCIF-MRI\",\n\t\t\"COEF\", \"OBFS\", \"PO\", \"RIM\", \"NRO\", \"PRO\", \"RE\", \"_MRI\", \"SUB\", \"SC\",\n\t\t\"SE\", \"TH\", \"SO\", \"THT\", \"SR\", \"TR\", \"TRR\", \"BDI\", \"BDI-"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\tif prefix:\n\t\t\tprefix ='' + prefix\n\telse:\n\t\tprefix = ''\n \n\t\tif len(self.errors) > 0:\n\t\t\tsuffix = '-' * len(self.errors) \n\t\telse:\n\t\t\tsuffix = ''\n\t\treturn self.error_str % suffix\n\n\tdef valid(self):\n\t\t\"\"\"\n\t\tValidate status type.\n\t\t\"\"\"\n\t\treturn True\n\n\tdef output(self, prefix=\"\"):\n\t\t\"\"\"\n\t\tGenerate the string, without prefix.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.output_str = str(self."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\tif self.is_valid():\n\t\t\treturn self._status[len(prefix):]\n\t\t\n\tdef _status(self, format_str, prefix='', prefix_len=len(prefix_str)):\n\t\t\"\"\"\n\t\tReturn (status, status_str) of validator status.\n\t\t\n\t\t@param format_str: str. Should be an str, containing the validating format.\n\t\t@param prefix: str. Prefix before a description is considered.\n\t\t\"\"\"\n\t\tself._check()\n\n\t\tr = re.compile('^\\s*(\\d*\\.?\\d*)*')\n\t\n\t\tif self"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn \"\".join([prefix, self.validator])[:8]\n\t\"\"\"\n\tReturns (string) parsed validator config\n\t\"\"\"\n\tdef status_data(self):\n\t\treturn self.validator.status\n\n\t# -- General callbacks -----------------------------------------------------------\n\tdef post_conf(self):\n\t\t\"\"\"\n\t\tReturn a dict of validator post-configuration options. Each\n\t\tkey in the dict is the option name and the corresponding value.\n\t\tTo parse the option name and value, use the format\n\t\t{\n\t\t'key': <string>,\n\t\t'value': <string>\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\tif prefix:\n\t\t\ts = prefix +' '\n\n\t\ts = s.replace('?', '-')\n\t\ts = s.replace('- ', '/')\n\t\treturn s\n\n\t# Return parsed json as json string\n\tdef json_to_str (self, data):\n\t\"\"\"\n\tReturn parsed json string from the given data.\n\t\"\"\"\n\t\ts = []\n\t\tfor el in data:\n\t\t\titem = el.split('=')\n\t\t\tkey = item[-1]\n\t\t\tval = item[0]\n\t\t\ts.append((key, val))\n\t\treturn json.dumps"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\ndef get_status(item):\n\t\"\"\"\n\tReturn validator status, for given item.\n\t\"\"\"\n\t\t\n\t\n#\tdef inner_status(status, status_msg, text=0):\n\tdef inner_status(status, status_msg, text=0, text_len=40):\n\t\tif text > status_msg:\n\t\t\ttext_len = 40\n\t\t\ttext = status_msg\n\t\tif text > status:\n\t\t\ttext_len = 40\n\t\t\ttext = int(status, 16)\n\t\tif text > status_msg:\n\t\t\ttext_len ="}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn '<%sx%s %s>.' % (prefix or '').strip(), (self.valid and 'Xs' or 'S', self.invalid_status or '')\n\nclass StatusString:\n\t\"\"\"\n\tA string which contains some validation status.\n\t\"\"\"\n\tvalid = None # the status that should be validated/checked (may only be set for status check)\n\tinvalid_status = None # the status that should be not validated/checked\n\tstatus_regex = None # a function to be used to parse the status (must be one of'success', 'fail',"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn '%s:\\t%s\\n' % (prefix, self.validator.status())\n\ndef error_str(prefix=''):\n\t\"\"\"\n\tReturn string of validator error level, with optional prefix.\n\t\"\"\"\n\t\treturn '%s: ERROR\\t%s\\n' % (prefix, self.validator.error_level())\n\ndef fatal_str(prefix=''):\n\t\"\"\"\n\tReturn string of validator error level, with optional prefix.\n\t\"\"\"\n\t\treturn '%s: FALCON\\t%s\\n' % (prefix, self.validator.fatal"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\tif isinstance(self.status[0], str):\n\t\t\treturn '%s%s%s%s, %s' % (self.status[0], os.linesep,\n\t\t\t\t(self.status[1]-1, self.status[2]), self.status[3], self.prefix)\n\t\telse:\n\t\t\treturn '%s%s%s%s, %s' % (self.status[2], os.linesep,\n\t\t\t\t(self.status[1]-1, self.status[2]),\n\t\t\t\tself.status[3],\n\t\t\t\tself.prefix)"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn \"\\n\".join(\"%s %s\" % (x[0].capitalize(), x[1]) for x in sorted(self.error))\n\n\nclass Error(Validator):\n\t\"\"\"\n\tValidate all the errors for a variable.\n\t\"\"\"\n\n\tdef __init__(self, variable, error=[]):\n\t\tValidator.__init__(self, error=error)\n\t\tself.variable = variable\n\n\tdef error(self):\n\t\treturn []\n\n\tdef __call__(self, var):\n\t\tresult = self.variable.validate(var, None)\n\t\tif isinstance(result,"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t\treturn \"%s: %s: %s\" % (prefix, \"Valid\", self._status)\n\t\t\n\tdef add_file(self, path):\n\t\t\"\"\"\n\t\tAdd the file to the dict\n\t\t\"\"\"\n\t\t# TODO: Handle empty files\n\t\tprint(\"Path %s already exists\" % path)\n\t\tself._path_to_file[path] = True\n\t\tprint(\"Setting file %s to %s\" % (path, self._file))\n\t\tself._file_to_file[path] = True\n\t\tprint(\"Adding file %s\" % path)"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\n\t\n\tprefix = prefix + '\\t' if prefix is None else \" \" + prefix + \"\\n\"\n\t\n\treturn self.__print(\"\")\n\n@configurable\ndef get_logger(log_dict, context='logging'):\n\t\"\"\"\n\tTry to get the logging settings.\n\t\"\"\"\n\t\n\tif log_dict.has_key('logging') is True:\n\t\treturn log_dict.get('logging')\n\t\n\tlogging_config = {\n\t\t'tries': 0,\n\t\t'status': 'FATAL',\n\t\t'filters': {\n\t\t\t'status': 'filter',"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn '%s:%s:%s' % (self._status_prefix(), self._version_no_display_str(),\n\t\t             (self._msg_prefix() if prefix is None else prefix))\n\n\tdef __str__(self):\n\t\treturn '%s:%s' % (self._status_prefix(), self._version_no_display_str())\n\n\tdef __bool__(self):\n\t\treturn True\n\n\tdef _status_prefix(self):\n\t\treturn self._status_prefix_\n\n\tdef __repr__(self):\n\t\treturn '<%s:%s %s status: %"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn '%s: %s' % (self.method, self.message_string(prefix=prefix, indent_level=4))\n\n\t@property\n\tdef method(self):\n\t\treturn self._method\n\n\t@method.setter\n\tdef method(self, method):\n\t\tself._method = method\n\n\tdef message_string(self, prefix=''):\n\t\t\"\"\"\n\t\tReturns message string from log file.\n\t\tArguments:\n\t\tprefix -- Message prefix string, optional\n\t\t\nReturns:\n\tstring\n\t\"\"\"\n\t\tif self._args is not None:\n\t\t\treturn"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\n\t\n\t# Create a basic string representation of validation log.\n\t# TODO : check if this is a valid log\n\ts = prefix +'' + 'Validation log:' + str(self)\n\treturn s\n\n\nclass ValidationLog:\n\t\"\"\"\n\tClass for parsing and writing log files.\n\t\"\"\"\n\tdef __init__(self, data=''):\n\t\t\"\"\"\n\t\tInitialize the object.\n\n\t\t@param data : file-like object containing validation log files.\n\t\t\"\"\"\n\t\tself.__data = data\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn string representation"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn prefix + 'ERROR_MODEL NOT FOUND: %s\\n' % self.get_name()\n\n\nclass InvalidLogError(Error):\n\n\tdef __init__(self, message):\n\t\t\"\"\"\n\t\tInitialization of the error for validation log.\n\t\t\"\"\"\n\t\tsuper(InvalidLogError, self).__init__(message)\n\t\tself.msg = message\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\treturn message or format is invalid.\n\t\t\"\"\"\n\t\treturn super(InvalidLogError, self).__str__()\n\n\nclass InvalidNameError(Error):\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn self.status +'-'+ prefix\n\nclass ValidationLog(Configuration):\n\t\"\"\"Base class for all validation and validation logic.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.validation_mode = 'valid'\n\t\tself.validation_log_format = '%(asctime)s -'+ time.asctime().isoformat() +'- %(name)s: %(message)s'\n\n\tdef configure(self):\n\t\tif self.validation_mode:\n\t\t\tfor file_name in self._filenames:\n\t\t\t\tif not file_name == '.':\n\t\t\t\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t     return self._status_str.format(prefix)\n\n\tdef _set_status_str(self):\n\t\t\"\"\"\n\t\tSave message in status file.\n\t\t\"\"\"\n\t\tstatus_path = self._status_str\n\t\tstatus_file = open(status_path, 'w')\n\t\tstatus_file.truncate()\n\t\tstatus_file.close()\n\n\tdef _add_status(self):\n\t\t\"\"\"\n\t\tAdd validation status file to stdout file\n\t\t\"\"\"\n\t\tlog_path = self._status_str\n\n\t\tif os.path.isfile(log_path"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn self.log_type(prefix) +'validation result'\n\n\tdef get_type_string(self):\n\t\treturn str(self.log_type())\n\nclass test_Log:\n\t\"\"\"\n\tTests the validation log parser\n\t\"\"\"\n\n\tdef __init__(self, tests_dir, log_type, tests_count):\n\t\tself.tests_dir = tests_dir\n\t\tself.log_type = log_type\n\t\tself.tests_count = tests_count\n\n\tdef get_tests_str(self, test_set):\n\t\treturn"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn \"Logging with {}\".format(','.join(\n\t\t\t\t(k,(str(v)+prefix) for x,v in self.logger.items())))))\n\nclass LogWriter(object):\n\t\"\"\"\n\tWrite logs to a temporary file (with a timestamp) using the logger object.\n\n\tYou will create a log file in the temp folder named before using this.\n\t\"\"\"\n\tdef __init__(self, filename, format=None):\n\t\t\"Creates a log entry (temp file name)\"\n\t\tself.filename = filename.strip('/') + '/'\n\t\tif format is not None"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t\treturn '%s'% prefix.upper()\n\n\nclass OldestRegistrar(object):\n\t\"\"\"\n\tAn object wrapping a registry of registered Oldest registrations.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._registered_registrates = {}\n\n\tdef register(self, registry_entry):\n\t\t\"\"\"\\\n\t\tAdd a registered value to the registry and mark it as valid or\n\t\tvalidated.\n\n\t\tThis method takes care of updating the registry.\n\t\tThe value of a registry entry is not checked.\n\t\tIf it has a valid value then it should also"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn identifier.upper() == self.class_name.upper()\n\n\tdef check(self, target):  # pylint: disable=unused-argument\n\t\ttry:\n\t\t\tfor line in self._iterlines():\n\t\t\t\tif line is None:\n\t\t\t\t\tcontinue\n\t\t\t\tself._line = line.rstrip('\\n')\n\t\t\t\tif self._line not in target.tokens:\n\t\t\t\t\tcontinue\n\t\t\t\tif self.match_comment(line):\n\t\t\t\t\tprint(line.strip('\\n'), file=self._target)\n\t\t\t\t\treturn True\n\t\texcept GeneratorExit:\n\t\t\treturn False\n\n\tdef"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn True\n\n\tdef get_next_id(self):\n\t\t\"\"\"\n\t\tReturn next id based on next value from the instance.\n\t\tWhen the new value is entered, returns None\n\n\t\tTODO: Use a separate function\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_previous_id(self):\n\t\t\"\"\"\n\t\tReturn previous id based on previous value from the instance\n\t\t\"\"\"\n\t\tpass\n# This file is part of Indico.\n# Copyright (C) 2002 - 2012 European Organization for Nuclear Research (CERN).\n#\n# Indico is free software"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn True\n\n\tdef get_data(self):  # pylint: disable=unused-argument\n\t\ttry:\n\t\t\treturn self.data\n\t\texcept AttributeError:  # Python 2, use self.metadata.get(identifier) instead\n\t\t\ttry:\n\t\t\t\treturn self.metadata[identifier]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\telse:\n\t\t\treturn {}\n\n\tdef get_metadata(self, identifier, include_metadata=True):  # pylint: disable=unused-argument,dangerous-default-value\n\t\ttry:\n\t\t\treturn dict(\n\t\t\t\tidentifier=identifier,\n\t\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\tif isinstance(identifier, tuple):\n\t\t\treturn check_identifier(identifier)\n\t\treturn identifier in self.__dict__\n\t\n\tdef check_identifier(self, identifier):\n\t\tif (not isinstance(identifier, six.string_types) and\n\t\t\t\tidentifier not in self.__dict__):\n\t\t\traise InvalidIdentifier(\"Invalid key '{0}' for {1}\".format(identifier, self.__class__.__name__))\n\t\n\t# ----------------------------------------------------------------------------------------------------------------\n\t\ndef register(name, base_version=None):\n\t\"\"\"\n\tRegister a new version of the base class\n\n\tA version of a base instance should always"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn is_identifier_valid(identifier, identifier_string)\n\n\ndef is_identifier_valid(identifier):\n\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn bool(identifier.isalnum())\nfrom django.core.mail import send_mail\nimport logging\nimport os\n\nfrom app import models\nfrom app.util import get_instance_or_none\n\nlogger = logging.getLogger(__name__)\n\nclass EmailHandler(models.BaseHandler):\n    \"\"\"\n    This class handles emails\n    \"\"\"\n    def get("}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\ndef build_path(path):\n\t\"\"\"Return path with given path if it exists and False otherwise.\"\"\" # pylint: disable=unused-argument\n\tif not path:\n\t\treturn None\n\tif os.path.isdir(path):\n\t\treturn path\n\telse:\n\t\treturn None\n\ndef parse_log(logfile):\n\t\"\"\"\n\tParse a text file's lines as a python string.\n\t\"\"\"  # pylint: disable=unused-argument\n\tloginfo = list()\n\tfor line in logfile.readlines():\n\t\tline = line.strip()\n\t\tif"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn identifier in SUPPORTED_VOCABULARY\n\nclass VocabularyItem(object):\n\n\tdef __init__(self, corpus, vocab_size):\n\t\tself.corpus = corpus\n\t\tself.vocab_size = vocab_size\n\n\tdef add_next_entry(self, entries):\n\t\t# type: (List[VocabularyItem]) -> None\n\t\tassert entries is not None\n\t\tstart = 0\n\t\tfor entry in entries:\n\t\t\tstart = start + 1\n\t\t\tself.add_word(entry)\n\t\tself.add_end_of_sentence"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn identifier.upper() in self.VALID_IDENTIFIERS\n\n\nclass Id(object):\n\t\"\"\"\n\tId is the type that uniquely identifies a type.\n\t\n\t:cvar TYPE_ENUM: The value associated with type.\n\t\"\"\"\n\t# ID types can be set in either `type_names` or `enum_names`.\n\t# Note that currently only id types are supported.\n\t# Use the same function if you're using the specific one of these.\n\tTYPE_ENUM = 0\n\tTYPE_UNSPECIFIED = 1\n\n\tID_NAME_MAP = {"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\n\n\tdef validate(self):  # pylint: disable=no-self-use\n\t\treturn True\n\n\tclass InvalidIdentifierException(Exception):\n\t\t\"\"\"\n\t\tRaises this exception for the identifier validation process.\n\t\"\"\"\n\t\n\n\nclass IdentifierValidator(DefaultValidator):\n\t\"\"\"\n\tValidator that validates the provided identifier to its values.\n\t\"\"\"\n\tdef validate(self, identifiers):\n\t\t\"\"\"\n\t\tValidates the identifier to all lower case, including case, e.g.\n\t\tone_two, a_b_c_d\n\t\t\"\"\"\n\t\tvalidated_identifier ="}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t\treturn (\n\t\t\tidentifier in self.base.valid\n\t\t\tor identifier in self.base.valid_invalid\n\t\t)\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\t# Verify and set up the requirements\n\trequired_requirements = None\n\tif self.spec_version is not None:\n\t\tif self.spec_version and extracted is None:\n\t\t\trequired_requirements = [self.spec_version]\n\t\telif self.spec_version:\n\t\t\trequired_requirements = [\"{}\".format(self.spec_version)]\n\n\tif required_requirements is not None:\n\t\tself.validate_required_requirements(required_requirements)\n\n\t# Validate a set of specifications\n\ttry:\n\t\tvalidation = InventoryRequirement(validate_requirements=[self._"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tinventory['spec_version']\t\t\n\t\ndef get_inventory(inventory_name_or_key):\n\t\"\"\"\n\tAttempts to retrieve a given inventory.\n\t\n\tIf inventory_name is not passed, attempts to retrieve only the inventory\n\tfound through get_inventory_by_name()\n\t\"\"\"\n\ttry:\n\t\treturn inventory[inventory_name_or_key]\n\texcept KeyError:\n\t\t# This is not a required inventory that's not in the inventory dict \n\t\tpass\n\ndef gather_inventory(template='', variables={}, spec_version=False, extract_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tspec_version = self.spec_version\n\tspec_name = self.name\n\tif not spec_name:\n\t\traise Exception('No SpecName for %s, was provided' % (self.name))\n\tinvalid_spec = False\n\tif parse_version(version=spec_version, spec=spec_name):\n\t\tif spec_name in inventory.specs:\n\t\t\tinvalid_spec = True\n\t\t\tself.spec_version = spec_name\n\t\t\treturn\n\t\telif extract_spec_version:\n\t\t\tspec_type = inventory.specs[spec_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tspecs = [item.lower() for item in self.specs]\n\tself.specs = [str(x) for x in specs if x in ('type','version')]\n\t\t\n\t\n\ttry:\n\t\tself.validate_spec_version(unpack_spec_version)\n\texcept ValidationError as e:\n\t\tprint(\"Invalid version or spec version: %s; version %s\" % (e.__str__(), e.error_dict))\n\t\t\nclass Validator(object, metaclass=ABCMeta):\n\t\"\"\"\n\tThe base class for validation.\n\t\"\"\"\n\n\tdef __init"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tif not isinstance(self.spec_version, tuple):\n\t\treturn inventory.not_a_string\n\t\n\tif extract_spec_version is True:\n\t\treturn inventory.is_extracted_spec_based(self.spec_version)\n\telse:\n\t\tassert False\n\t\n\n#\n# These tests have been put in the api_methods in this file. The \"test\" class is in this module\n#\n\n# NOTE:\n# If you want to use this file, please use one of the following:\n# * client -> client_mock\n#  * json ->"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\tif not isinstance(self.spec_version, (Version, Tuple)):\n\t\t\tself.fail(\"Specifications %s must be a version tuple or list of versions.\" % self.spec_version)\n\t\telse:\n\t\t\tfor spec_name, spec_config in self.spec_version:\n\t\t\t\tif not validate_spec:\n\t\t\t\t\tself.fail(\"Specifications %s does not validate, but it doesn't exist.\" % self.spec_version)\n\t\t\t\tif not validate_version(spec_config['version']) == spec_config['version']:\n\t\t\t\t\tself.fail(\"Specifications"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tif hasattr(self,'spec_version') and self.spec_version is not None:\n\t\t\t# If version is specified then return the spec version.\n\t\t\treturn self.spec_version\n\n\t# Otherwise just return the spec_version and return None\n\treturn None\n\t\t\ndef get_hostname(inventory):\n\tif not inventory:\n\t\treturn 'localhost'\n\treturn inventory.hostname\n\nif __name__ == '__main__':\n\timport optparse\n\tparser = optparse.OptionParser()\n\tparser.add_option('-d', '--host', type='string', help='"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tif not unpack_spec_version:\n\t\treturn None\n\t\n\tversion = self.spec_version;\n\n\tprint 'validate called'\n\t\n\tif (1 <= version <=4 and ((version-4)<int(unpack('v'))) or (version-1<int(unpack('i'))) or (version-2>=1 and (version-2<int(unpack('l'))) or (version-3>=0) or (version-4<int(unpack('k'))) or (version-5>=1 and (version-5<int(unpack('l'))) or (version-6)<"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\tspec_type_string = self.spec_version(inventory.get('type'))\n\tif not spec_type_string:\n\t\treturn\n\t\n\tif self.spec_version is None:\n\t\tif self.spec_version == 1:\n\t\t\traise InventoryError('Invalid version specified in inventory')\n\t\telse:\n\t\t\traise InventoryError('Invalid version specified in inventory: {}'.format(\n\t\t\t\tself.spec_version))\n\t\n\tif self.spec_version not in inventory:\n\t\tinventory[self.spec_version] = {}\n\t\n\tif self.include_deps:\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif self.spec_version is None:\n\t\t\treturn inventory\n\n\t\tif extract_spec_version:\n\t\t\tif self.spec_version is None:\n\t\t\t\traise InvalidFormatException(\n\t\t\t\t\t'Expected spec_version to be `None` but'+\n\t\t\t\t\t'missing, as spec.version was not set'  + '\\n' +\n\t\t\t\t\t'type value specified'  +\n\t\t\t\t\t'\\n' +\n\t\t\t\t\tself.spec_version\n\t\t\t\t)\n\n\t\t\tif ':' in self.spec_version:\n\t\t\t\ttry:\n\t\t\t\t\tspecs = self.spec_"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\tfor file in manifest_files:\n\t\tmanifest, digest = utils.getJsonFromFile(file.fn, check_digests=True)\n\t\t# Check digest only if they've already been used\n\n\t\tif not digest and digests_used:\n\t\t\twritemsg(\"\\tChecking file '%s' is now used\\n\" % file.fn, noiselevel=-1)\n\t\t\tcontinue\n\t\tif not os.path.isfile(file.fn):\n\t\t\t# Check if file is not present or a different hash.\n\t\t\ttry:\n\t\t\t\tdigests_used.add("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tmanifest_files = get_list_of_files(manifest_files, path='./' or './')\n\t\tself.get_digests_by_type('md5').check(manifest_files, digests_used)\n\n\ndef get_filepaths_of(digests_used, path='./')\n\tfor path in get_list_of_files(digests_used, path='./', path='./'):\n\t\tif is_file_file(path):\n\t\t\treturn (os.path.join(path, './' + digest) for digest in digests_used)"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\n\n\tdigest_cache = {}\n\tfor filename in manifest_files:\n\n\t\tdigest_path = os.path.join(os.path.dirname(__file__), digests_path_name(filename))\n\t\tprint(\"Checking digest file: %s\" % digest_path)\n\t\twith open(digest_path, 'r') as f:\n\t\t\tdigest = digest_cache.setdefault(str(int(f.readline().strip() or '')), {})\n\t\t\tif not digest.setdefault(filename, {}):\n\t\t\t\tmsg = \"No entry matching %s for file %s\" %"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\t# check if already installed\n\t\tneed_to_install_digests = self.__listHacker()['digests']\n\t\tif not digests_used:\n\t\t\tlogger.warning('Could not load digests from manifest and use hatcher')\n\t\t\treturn need_to_install_digests\n\t\tfor file_path in manifest_files:\n\t\t\tif need_to_install_digests and digests_used.get(file_path):\n\t\t\t\t# already installed?\n\t\t\t\tlogger.info(\"Skipping digests for %s, %s (already installed)\", digests_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tfor file_name in manifest_files:\n\t\t\tif not os.path.exists(file_name):\n\t\t\t\tprint \"Checking %s, using %s.\" % (file_name, digests_used)\n\t\t\t\toutput, returncode = run_cmd(\n\t\t\t\t\t\"git log --decorate --format=%cd %s --\",\n\t\t\t\t\t['git', 'log', '-1',  file_name],\n\t\t\t\t\treturn_stderr=True)\n\t\t\t\tif returncode!= 0:\n\t\t\t\t\traise error(\"Checksums not present: %s\" % output)\n\n\tdef checkout(self, git"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tfor name in manifest_files:\n\t\t\tchecksums_used = []\n\t\t\ttry:\n\t\t\t\tfiles = list(digests_used)\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\t\tfor n in files:\n\t\t\t\tdigests_used.append(ManifestDigest(manifest=self, name=name, n=n[\"n\"]))\n\t\t\tself.digests.append(Digest(manifest=self, name=name, n=manifest_files[-1], digests_used=digests_used))\n\n\n\tdef check_digest(self, name):\n\t\t\"\"\"\n\t\tDetermines the digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\twith open(manifest_files, 'r') as json_file:\n\t\t\tjson_manifest = json.loads(json_file.read())\n\t\t\n\t\tdigests_used = set(manifest_json.get('digests', []))\n\t\tprint \"found {} hashes of manifest, checking {} digests\".format( digests_used, len(digests_used) )\n\t\tprint \"manifest files: {}\".format(json_manifest['files'])\n\t\tfor h in json_manifest['hashes']:\n\t\t\tif not h in digests_used:\n\t\t\t\treturn False\n\t\n\tdef get_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tfor manifest_file in manifest_files:\n\t\t\tfound = self.md5sum(manifest_file).hexdigest()\n\t\t\tif (not found in digests_used and found!= manifest_file +'' or\n\t\t\t\t\tmanifest_file == manifest_file[:-3] +'' or\n\t\t\t\tmanifest_file[0:2] =='md5.'):\n\t\t\t\tdigests_used.append(manifest_file)\n\n\ndef get_manifest(context, manifest_data):\n\t\"\"\"\n\tRetrieve a manifest containing the given data and return the data.\n\t\"\"\"\n\n\tif not"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\tfound_digests_present = True\n\t\tfor f in manifest_files:            \n\t\t\tfound = False\n\t\t\tfor digest in f['files']:  \n\t\t\t\tif digest in digests_used:\n\t\t\t\t\tfound = digest\n\t\t\t\t\tbreak\n\t\t\tif not found:\n\t\t\t\tself._log(\"Not all the files are needed to satisfy the requested digests\")\n\t\t\t\tfound_digests_present = False\n\t\t\t\tbreak\n\t\t\t\n\t\tif not found_digests_present:\n\t\t\tself._log(\"No missing input digests found in manifest\")\n\t\t\tfound_digests_present = False\n\t\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t\t# Create directory manifests with the correct permissions\n\t\t# that would be added\n\t\tfor md_file in manifest_files:\n\t\t\tdigests = os.listdir(os.path.join(self._destination_path, self._md_file))\n\t\t\tif digests:\n\t\t\t\tdigests_used.extend(\n\t\t\t\t\tset(dir_digests(dir_digests(md_file))) for dir_digests in digests\n\t\t\t\t)\n\n\tdef check_digests_available(self, manifest_files):\n\t\t\"\"\"\n\t\tCheck the digest of any file that should"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tif prior.isValid(self):\n\t\traise InventoryValidatorException(\"Not prior version of inventory object %s inventorories.\" % prior.fullName())\n\n\t\n\tif prior.isValid(self):\n\t\treturn prior.absoluteVersion() - self.absoluteVersion()\n\nclass InventoryValidatorException(Exception):\n\t\"\"\"\n\tException raised when the type of exception is raised.\n\t\"\"\"\n\nclass InvalidVersionException(InventoryValidatorException):\n\t\"\"\"\n\tException raised when there's a problem parsing the inventory,\n\te.g. the valid version is not valid.\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\tif prior.inventory_obj is not None:\n\t\t\traise AssertionError(\n\t\t\t\t\"%s version of %s cannot be used because it has an invalid inventory object.\\\n\t\t\t\tCan only check prior objects with valid inventory type: inventories,prior,and/or prior_history and prior_database are mutually exclusive.\"\n\t\t\t\t% (self._meta.resource_name, prior.inventory_type, prior.uuid)\n\t\t\t)\n\t\t\n\t# Returns the result of check_version()\n\tdef check_version(self, prior):\n\t\t\"\"\"\n\t\tCheck if specified version of inventory object matches the"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\nif PriorityValidator is not None:\n\tclass ProdValidator(Validator):\t\n\t\tdef __init__(self, prior = None):\n\t\t\tsuper(ProdValidator, self).__init__()\n\t\t\tself.prior = prior\n\n\t\t\tself.validate_as_prior_version(priprior)\n\n\tclass VersValidator(Validator):\n\t\tdef __init__(self, prior = None):\n\t\t\tsuper(VersValidator, self).__init__()\n\t\t\tself.prior = prior\n\n\t\t\tself.validate_as_prior_version(priprior)\n\n\nclass TileValidator(Validator):"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n#\t\tif prior == InventoryValidator.DEFAULT_PROPRIETARY:\n#\t\treturn\n#\t\telse:\n#\t\treturn\n\n\tdef process_inventory(self, inventory):\n#\t\twith open('inventory_valid.json') as fp:\n#\t\t\tinventory_data = json.load(fp)\n#\t\t\tself.assertEqual(inventory_data.has_key(\"prior_version\"), \"True\")\n#\t\t\tself.assertEqual(inventory_data.has_key(\"version\"), False)\n\n\tdef check_is_is_prior_"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\n\tdef check_inventories_compatibility(self):\n\t\t\"\"\"\n\t\tChecks that the InventoryValidator objects in this InventoryValidator\n\t\tobject are allowed to be used correctly without prior version\n\t\tinformation.\n\t\t\t\n\t\t\tThis routine will use the provided prior as the initial object in\n\t\t\tself as the PriorityValidator.\n\t\t\"\"\"\n\n\t\t# Check that prior is a valid prior version of self\n\t\tif prior is None or prior is self:\n\t\t\traise NoInitialisedError(\"No prior object selected for %s\" % str(self))\n\t\t\n\t\t# Check that prior is not an InventoryValidator object\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\nclass InventoryVersionError(Exception):\n\t\"\"\"\n\tClass for all exceptions that are thrown by this module.\n\t\"\"\"\n\tdef __init__(self, message):\n\t\tself.message = message\n\tdef __str__(self):\n\t\treturn self.message or self.message + \" are not a valid inventory version.\"\nclass InventoryValidationError(Exception):\n\t\"\"\"\n\tClass for all exceptions that are thrown by this module.\n\t\"\"\"\n\tdef __init__(self, message):\n\t\tself.message = message\n\tdef __str__(self):\n\t\treturn self.message"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\ndef print_inventory(inventory, verbose=False):\n\t\"\"\"\n\tPrint inventory to stdout.\n\t\n\tThe output is a 2-tuple of strings with the output from the\n\tcheckin program informing what the inventory was found and the return\n\tvalue of this check.\n\t\n\tArgument 'inventory' is an InventoryValidator object and the\n\tvariable'verbose' determines whether or not to print the output.\n\t\"\"\"\n\t\t\n\t\n\t\n\tif verbose:\n\t\t# Don't print inventory if the command takes a'verbose=True' argument\n\t\tif verbose is True:\n\t\t\tprint('[checking"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\terror_msg = self.assert_invalid_inventory_version(prior)\n\t\n\tif not prior.check_valid_inventory_version(self):\n\t\traise AnsibleError(\"Invalid prior version of %s\" % prior.name)\n\t\n\treturn prior\n\ndef validate_inventory_version(self):\n\t\"\"\"\n\tCheck that the version of the provided Inventory is between the\n\tcurrent inventory version and the required version.\n\t\"\"\"\n\t\n\t\n\tversion = self.get_inventory_version()\n\tassert version > self.get_expected_inventory_version()\n\treturn version\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tif prior.version == None:\n\t\traise Exception(\"An object with an'version' attribute must be \"\n\t\t\t\t\"'None' or an InventoryValidator object.\")\n\tif prior.version.version_major < 3:\n\t\traise Exception(\"A prior version of {version} should be \"\n\t\t\t\t\"a prior < 3.\".format(version=prior.version))\n\tif prior.version.version_minor < 2:\n\t\traise Exception(\"A prior version of {version} should be a prior \"\n\t\t\t\t\"major < 2.\".format(version=prior.version))\n\tif"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t\n\tlog.debug('Validating the input prior for inventory {0}...'.format(self))\n\tfor inventory_data in prior.get_data(validate_as_prior_data = True):\n\t\tif str(inventory_data)!= '{0}:{1}'.format(self._id, self):\n\t\t\tlog.error('You must own a valid InventoryValidator object for the next generation!')\n\t\t\traise InvalidInventoryError('Invalid prior data') \n\t\t\t\n\tif not prior.validate_data(validate_empty_inventory()):\n\t\t\t\tlog.debug('No input prior')\n\t\t\t\traise"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tstate_mappings = { }\n\tfor (dirpath, dirnames, filenames) in os.walk(inventory_path):\n\t\tfor filename in filenames:\n\t\t\tif filename[0] == '.':\n\t\t\t\tcontinue\n\t\t\tif filename not in version:\n\t\t\t\tcontinue\n\t\t\tstate_mappings.setdefault(os.path.join(dirpath, filename), []).append(filename)\n\t\n\treturn state_mappings\n\t\n\ndef get_version_mapping(inventory, version):\n\t\"\"\"\n\tGet a map of (path, prefix), for version in the current working\n\tdirectory.\n\t\n\tThis assumes that"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\tmapd = {\n\t\t\t\t'main':\t\t'inventory',\n\t\t\t\t'main.d':\t\t 'inventory',\n\t\t\t\t'main-version': \t'inventory',\n\t\t\t\t'tags':\t\t'inventory',\n\t\t\t\t'tasks':\t\t'inventory',\n\t\t\t\t'tasks.c':\t\t'inventory',\n\t\t\t\t'tasks.m':\t\t'inventory',\n\t\t\t\t'tags.m':\t\t'inventory',\n\t\t\t\t'tags-version':\t'inventory',\n\t\t\t\t'tasks-version':\t'inventory',\n\t\t\t\t'tags.m':\t\t'inventory"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tpath_map = {}\n\t\n\t# Get new path maps for future lookup\n\tnew_get_last_file_hash = sha1(inventory.inventory.inventory_source).hexdigest()\n\tcur_path_map = {}\n\t\n\t# Iterate by version number using the current version\n\t# (get first version of each file for all files)\n\tfor i in range(1, len(inventory.inventory.inventory_source)):\n\t\titem = inventory.inventory.inventory_source[i]\n\t\t\n\t\tif item['hash'] == new_get_last_file_hash:\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\ndef file_paths_in_state(state_path):\n\t\"\"\"\n\tGet a map of logical path in state to files on disk for version in state.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may not include references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\ndef file_paths_in_state_with_version(version):\n\t\"\"\"\n\tGet a map of logical path in state with version.\n\t\n\tReturns a dictionary: logical_path_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tout_map = {}\n\t\n\tfor module_path, content_files in get_content_files():\n\t\tcontent_files = get_files_with_metadata(inventory, module_path, content_files)\n\t\tif version not in content_files:\n\t\t\tcontinue\n\t\t\n\t\tlogical_path_in_state_map = get_logical_path_map(inventory, module_path, os.path.basename(module_path))\n\t\tout_map[logical_path_in_state_map] = set(content_files)\n\ndef get_files_with"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\ndef get_inventory_path_map(inventory, version):\n\t\"\"\"\n\tGet a map of path, logical path(s) which are relative to inventory.\n\t\n\tReturns a dictionary of path -> set\n\t\"\"\"\n\t\ndef get_hosts(inventory):\n\t\"\"\"\n\tReturn a list of Host objects representing hosts in a particular Inventory.\n\t\n\tArgs:\n\t\tinventory (a Inventory of L{Inventory})\n\t\n\tReturns:\n\t\tL{Hosts}\n\t\"\"\"\n\t\ndef get_hosts_in_inventory(inventory):\n\t\"\"\"\n\tReturn a list of Host objects representing"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\ttry:\n\t\t\titems = inventory.get_inventory_with_files_and_version(inventory.inventory_hostname, version)\n\t\texcept Exception as e:\n\t\t\tlogger.error(\"Error getting the inventory with version %s with id %s\", str(version), inventory.inventory_hostname)\n\t\t\traise e\n\t\t\n\t\tif len(items) > 0:\n\t\t\treturn items\n\n\t\treturn set()\n\n\ndef get_content_files_and_version(inventory, version):\n\t\"\"\"\n\tGet files and version from inventory for version in inventory.\n\t\n\tReturn list: (content"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tlogical_path_in_state = frozenset(get_all_paths_from_inventory(inventory, version))\n\t\n\treturn logical_path_in_state\n\ndef get_all_paths_from_inventory(inventory, version):\n\t\"\"\"\n\tReturn a set of path in state to files on disk:\n\t\n\t- If the path does not follow any rules in the inventory, returns the entire\n  \tstate.\n\t- If the path does match any paths in inventory, returns the entire state\n\t\"\"\"\n\tlogical_paths = list_of_path_patterns_logical_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t\n\tcontent_files = {}\n\t\n\t# Get existing content paths first.\n\tfor path in inventory.content_folders():\n\t\t# Skip non-existent files if version is not found.\n\t\tif not os.path.exists(path):\n\t\t\tcontinue\n\t\t# Skip empty files.\n\t\tif not os.path.isdir(path):\n\t\t\tcontinue # Skip empty folders in the inventory.\n\t\n\t# Loop through all path paths, removing duplicates.\n\tfor path in inventory.content_folders():\n\t\tif not path.startswith('~'): continue\n\t\t# Skip empty files.\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\ndef is_content_file(content_filename, content_path):\n\treturn (not os.path.exists(content_filename) or \n\t\t   not os.path.exists(content_path) or \n\t\t   stat.S_IFMT(os.stat(content_filename).st_size) == os.stat(content_path)[stat.ST_SIZE])\n\ndef is_inventory_available(inventory):\n\ttry:\n\t\timport resourcemanagement \n\t\tresourcemanagement\n\texcept ImportError:\n\t\tpass\n\ttry:\n\t\timport openshift\n\n\t\tresourcemanagement ="}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\t# Validate the fixity.\n\tcheck_fixity(self.fixity, manifest_files)\n\t\ndef load_resource(filepath, loader=lambda x: None):\n\t\"\"\"\n\tLoads a resource from a file.\n\t\n\t@raises C{ValueError} if the file can not be parsed from a file.\n\t\"\"\"\n\tif not isinstance(filepath, basestring):\n\t\traise ValueError('File %r can only be parse from paths' % filepath)\n\t\n\tdef safe_read(stream):\n\t\t\"\"\"\n\t\tReturn the object given the stream object, as a file\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t# Checks that the fixity is present and works\n\tif not os.path.isfile(fixity):\n\t\t# In case of non-existing files, check for missing.\n\t\texisting_paths = validate_dir_tree(dir_tree(fixity))\n\t\tif not len(existing_paths):\n\t\t\traise RuntimeError, \"Missing directory tree: missing \" \\\n\t\t\t    \"file or structure path: \" + fixity\n\t\t\n\t\t# If the fixity has no directory, check for existing files.\n\t\tif len(existing_paths):\n\t\t\texisting_paths = validate_dir_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\ndef convert_manifest(file_paths, include_deps_whitelist=False):\n\t\"\"\"\n\tConvert the files into a new manifest file with the corresponding\n\tdependencies, if provided.\n\t\n\t:param list file_paths:\n\t:param bool include_deps_whitelist:\n\t\"\"\"\n\tif not file_paths:\n\t\treturn []\n\n\tmanifest_file = open(file_paths[0], \"r\")\n\n\tmanifest_lines = manifest_file.readlines()\n\t\n\twhile manifest_lines:\n\t\tline = manifest_lines.pop(0)\n\t\tif not"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\tif str(fixity[0])!= 'v':\n\t\traise InvalidFileError(\"Fixity doesn't conform with a Valid fixity. (v) {0}\" \n                                .format(str(fixity)))\n\t\n\tcheck = True\n\tfound = False\n\tfor manifest_file in manifest_files:\n\t\tif str(manifest_file) in fixity:\n\t\t\t\n\t\t\tpath = manifest_file \n\t\t\tfound = True\n\t\t\t\n\t\t\tif path in list_files: \n\t\t\t\t\n\t\t\t\tprint(\"Duplicate: {0}\", path)\n\t\t\t\traise InvalidFileError(\"Duplicate paths in manifest. Not allowed"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t\n\t# Validate in-memory inventory.\n\tfor i in range(1, len(manifest_files)):\n\t\tfilename = manifest_files[i].filename\n\t\tif os.path.isfile(filename) == False:\n\t\t\treturn \"Invalid file: '%s'\" % filename\n\t\ttry:\n\t\t\tin_memory_file(filename, fixity)\n\t\texcept Exception as e:\n\t\t\treturn \"Invalid file: '%s': %s\" % (filename, str(e))\n\t\t\n\t\t# Clean up the directory if it exists\n\t\ttry: os.remove(filename)\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\ndef file_countfile(resource, target, file):\n\t\"\"\"\n\tReturn the file count from the supplied file if the resource's location\n\tis in the archive directory and it doesn't already exist.\n\t\n\tThe target is the path used for the file index or an URL pattern that is passed\n\tin to the inventory method in the ``inventory`` method\n\t\n\t**note**: A ``file`` may contain ``/' ``\\\\`` or '\\\\.`` within the archive\n\t**\t**\t-``'s\n\t**\t\t\t-``'#``\n\t**\t\t\t-``#``"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n#\tThis file doesn't need to be processed, even though we are not interested\n\t# in the fixity file.  It's just a bad idea to check all the paths\n\t# of the files in the manifest.\n\tfilename = manifest_files[0][0]\n\tfor child_file in manifest_files[1:]:\n\t\twith open(child_file, \"r\") as f:\n\t\t\tfor line in f:\n\t\t\t\tif not line.startswith(\"#\"):\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tcontinue\n\t\t\t\n\t\tif filename in line:\n\t\t\tcontinue\n\t\t\t\t\n\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tmissing_files = []\n\twhile missing_files:\n\t\tmissing_files = manifest_files[missing_files.pop()]\n\t\tif len(missing_files) == 0:\n\t\t\treturn\n\t\tmissing_files.sort()\n\t\t\n\t\tfor i in missing_files:\n\t\t\tv = os.path.join(bundle_root_dir, i)\n\t\t\tif os.path.isfile(v): \n\t\t\t\tif (os.path.isdir(v) and os.path.isfile(v + \".yaml\")\n\t\t\t\t):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tself.validate_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\nclass Packages(object):\n\n\tdef __init__(self, args):\n\t\tself._file_list = []\n\t\tself._manifest_files = []\n\t\tself._manifest = None\n\t\tself._manifest_files_to_check = []\n\t\tself._pack_list = []\n\t\tself._file_list_to_check = []\n\t\tself._manifest_list = []\n\t\tself._manifest_file_list_to_check = None\n\t\tself._packages_per_category = [None] * len(args)\n\t\tself._file_count += 1"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tfor fname in manifest_files:\n\t\t\t\n\t\t\tif not os.path.exists(fname):\n\t\t\t\traise Exception(\"Cannot find \" + fname)\n\t\t\t\t\n\t\t\ttry:\n\t\t\t\twith open(fname, \"r\") as f:\n\t\t\t\t\treturn (fixity, re.search('(.+?) = (.+?)$', f.read()).groups().replace('[', '').replace(']', ''))\n\t\t\texcept Exception,e:\n\t\t\t\tpass\n\t\n\t\t\ndef pack(m):\n\t\"\"\"\n\tPacks data and writes it to file\n\t\"\"\"\n\tf = open(m) + '.packer'"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tos.mkdir(path)\n\t\treturn os.listdir(path)\n\n\t@retry(backoff=2, tries=3, retry_sleep=30, backoff=2, retry_on_exception=True)\n\tdef make_directory(dBaseDir):\n\t\treturn os.path.join(dBaseDir, dBaseDir)\n\n\ndef get_contentfiles(dBaseDir):\n\t\"\"\"\n\tReturn the files in `dBaseDir` in a directory\n\t\"\"\"\n\n\tret = []\n\tfor path in os.listdir(dBaseDir):\n\t\tpath"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t    \n    def get_file_list(path, ext):\n\t\t    \n        if not os.path.exists(path):\n\t\t\n\t\t    \tfile = open(path)      \n\t\t    \tlines = file.readlines()  \n            file.close()\n            \n                for i in range(1,len(lines)):\n\t\t    \tsplitline = lines[i]\n\t\t\t    \tif splitline.startswith(ext):            \n\t\t\n\t\treturn file_list[splitline]\n\nclass Settings:\n    \"\"\"\n\tConfigure user settings for the application.\n   \t\"\"\"\n\n    def __init__(self, application, config):\n\t\"\"\""}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t path = os.path.abspath(path)\n\t\tfolders = []\n\t\tfor root, dirs, files in os.walk(path):\n\t\t\tfor d in dirs:\n\t\t\t\tif (d == \"\" or d == '.') and (d.endswith('~') or d.strip('.') not in exclude):\n\t\t\t\t\tfolders.append(os.path.join(root, d))\n\t\treturn folders\n\n\ndef make_directory(path, makedir=True):\n\t\"\"\"\n\tCreate a new directory.\n\t\"\"\"\n\tif not makedir:\n\t\tos.makedirs(path)\n\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tfiles_list = []\n\t\tfor file in listdir(path):\n\t\t\tfiles_list.append(os.path.join(path, file))\n\t\treturn files_list\n\t\"\"\"\n\tRecursive descent.\n\t\"\"\"\n\t\ndef files_find(path, folder):\n\tfiles = []\n\tfolder_list = os.listdir(folder)\n\tfor file in files_list:\n\t\tif file.startswith(folder):\n\t\t\tfiles.append(os.path.join(folder, file))\n\t\tif file.endswith(folder):\n\t\t\tfiles.append("}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\treturn os.listdir(os.path.abspath(path))\n\ndef file_is_readable(file_id):\n\t\"\"\"\n\treturn `False` if the given file is readable, `True` if it is not\n\t\"\"\"\n\treturn os.path.isfile(os.path.join(config.REPO_ROOT, file_id))\n\ndef list_files():\n\t\"\"\"\n\tList all files in repo and dirs\n\t..note::\n\n\t\tUsage::\n\n\t\t\tfiles_list REPOS_FULL_FILES\tLIST\t\tFile List\n\t\"\"\"\n\t\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\n\n\tfiles_list = [ f for f in glob(path) ]\n\treturn files_list\n# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for license information.\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is regenerated.\n# --------------------------------------------------------------------------\nfrom typing import Any, AsyncIterable, Callable, Dict, Generic, Optional, TypeVar, Union\nimport warnings\n\nfrom azure"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tf = sorted(os.listdir(path))\n\t\treturn list(filter(lambda f: f.endswith('.py'), f), f)\n\n\ndef find_module_in_path(path, paths):\n\t\"\"\"\n\tFind `paths` from the given path.\n\t\"\"\"\n\t\tfor path_ in paths:\n\t\t\tif path_ in path:\n\t\t\t\tf = getattr(importlib.import_module(path_), path_)\n\t\t\t\tif callable(f):\n\t\t\t\t\treturn f\n\n\ndef install(paths):\n\t\"\"\"\n\tInstall `paths` into the path search path by"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\ndef _extract_data(dir_path, d):\n\t\"\"\"\n\tTake the data dir as a list\n\t\"\"\"\n\t\n#\t\tprint d[\"path\"]+\" \"+str(d[\"data\"])\n\t\n\t\t\t\n\t# now check which files are present in the dir\n\tfiles = list()\n\tfor f in os.listdir(dir_path):\n\t\tif f.endswith(\".json\"):\n\t\t\tfiles.append(f)\n\t\t\n\t\t\tif len(files) > 0:\n\t\t\t\tall_files = get_all_files(dir_path+\"/\"+f)\n\t\t\t\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tpath = [ p for p in path if p!= '/', ]\n\t\treturn [ f for f in os.listdir( path ) if os.path.isfile(os.path.join(path, f)) ]\n\tdef files_match(pat, files):\n\t\treturn fnmatch.fnmatch(files, pat)\n\treturn files_listfrom django.conf import settings\n\nfrom apps import views\nfrom apps.forms import FormField\nfrom apps.utils import get_data\nfrom apps.models import User\nimport json\n\n\nclass Index(views.Module):\n    def get(self"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t\tfiles = []\n\t\tif files == '': return files\n\t\tfor file in files:\n\t\t\tif os.path.isdir(path + '/' + file):\n\t\t\t\tfiles.append(file)\n\t\t\telse:\n\t\t\t\tfiles.append(file)\n\t\treturn files\n\n#\n#\tFile list for a file.\n\n\tfiles_list_file = files_list(file)\n\n\tfile_list_name = None\n\tif files_list_file!= None:\n\t\tfile_list = files_list_file\n\t\tassert(file_list.count(b'./"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tfiles1 = []\n\t\n\tfor xml_filename in files:\n\t\tfile1 = File(xml_filename)\n\t\tfiles1.append(file1)\n\t\n\t# Group the groups\n\tfor files_group, files1 in enumerate(files1):\n\t\tfilename = files1[files_group]\n\t\t\n\t\tif filename.split('.')[-3] == 'zip':\n\t\t\tfilename = 'zip' + filename\n\t\t\n\t\tfile_basename_list = filename.split('-')\n\t\t\n\t\t#print('filename=', filename)\n\t\tfilename_list = _group_files(filename_list, file"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tfor f in files:\n\t\tname = os.path.basename(f)\n\t\tdata = _split_file(f)\n\t\tdata['name'] = name\n\t\tdata['zip'] = os.path.dirname(f).replace('.zip','')\n\t\tdata['xml_filename'] = \"%s.xml\" % name\n\t\tdata['files'] = _group_files_by_xml_filename(f, xmls, data)\n\t\tfor sub_file, sub_file_data in data.items():\n\t\t\tname_sub_file = name.replace('."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tdata = {}\n\t\n\t\n\txml_directory = xml_filename.split(\"/\")\n\tfor directory in xml_directory:\n\t\tif directory[-1] == \"/\":\n\t\t\tdirectory = directory[:-1]\n\t\txml_directory.remove(directory)\n\t\tdirectory = directory[:-3]\n\t\t\n\t\troot, file_name = os.path.split(directory)\n\t\tif root == \"common\":\n\t\t\tfile_name +=\".xml\"\n\t\tif root == \"commons\":\n\t\t\tfile_name +=\".xml.xml\"\n\t\n\t\tdata[file_name] = {\n\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tgroupFilesByXmlFileName = {}\n\tfor filename in [xmls, groups]:\n\t\tfile_id = groupFilesByXmlFilePath(source, filename)\n\t\tgroupFilesByXmlFileName[id] = file_id\n\t\n\t# add list of files to dictionary to reduce the order\n\t\n\tgroupFilesByXmlFiles = {}\n\t\n\t# iterate over the list of files to be uploaded with each.xml file\n\t\n\tfor filename, fileobj in files:\n\t\t\n\t\tif type(fileobj) == type([]):\n\t\t\t\n\t\t\t# ensure that the file exists\n\t\t\t\n\t\t\tif os.path.exists(fileobj):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tif os.path.isabs(xml_filename):\n\t\tsource = os.path.join(source, os.path.basename(xml_filename))\n\ttag = 'filelist_by_xml_filename'\n\t\n\tfor file in files:\n\t\tpath = os.path.join(xml_filename, file)\n\t\ttry:\t\t\n\t\t\twith open(path, 'rb') as fh:\n\t\t\t\tdata = fh.read()\n\t\t\t\tif len(data)!= 0:\n\t\t\t\t\tdata = data.decode()\n\t\t\t\t\tif xml_filename == file:\n\t\t\t\t\t\treturn"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\t# Get all files in the zip of the XML files\n\tfile_names = [filename for filename in files if filename.endswith('.xml')]\n\tfiles_names = [filename for filename in files if filename.endswith('.xml.1')]\n\t\n\t# Copy files into a new folder if necessary\n\tfor x in range(len(file_names)):\n\t\toutput = os.sep.join(files_names[:x])\n\t\tif not os.path.exists(output):\n\t\t\tos.makedirs(output)\n\t\txfn = zip(*[os.path.join(output,"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tgrouped = {}\n\t\n#\tprint files\n\txml_files = files[:]\n\t\n#\tfor filename in files:\n\t#\tprint filename,\n\twith open(os.path.dirname(source), 'r') as source_file:\n\t    xml_root = etree.parse(source_file, parser=XMLBuilder)\n\t\n\t    for xml_filename in xml_files:\n\n\t     \t#\tprint xml_filename,\n\t\twith open(os.path.join(xml_root, filename), 'r') as source_file:\n\t  \n\t\t     \t# get the file"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\t# get the XML files with the desired basename\n\txmls = [ file for path in files if os.path.isfile(path) is not None \n\t\t\tfor file in _group_files_by_xml_filename(source, xmls, path)]\n\t\n\t# open the zipfile and concatenate them\n\tsource_zipfile = zipfile.ZipFile(xmls[0], \"w\", zipfile.ZIP_DEFLATED)\n\tsource_zipfile.write(_to_xml_filename(xml_filename))\n\n\t# return the results\n\treturn source_zipfile.namelist()\n\n# Copyright"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t\n\tif isinstance(xml_filename, basestring):\n\t    xml = xml_filename\n\t    with open(xml, 'r') as fh:\n\t    \tdata = fh.read()\n\t    \tfh.close()\n\t    \treturn str(data)\n\t    \treturn ''\n\t    \t\n\telse:\n\t    xml = str(xml_filename)\n\toutput = {}\n\tfiles = [os.path.basename(x) for x in files]\n\tfor filename in files:\n\t    fullpath = os.path.join(config.data_dir, filename)\n\t    fullpath = fullpath.replace('."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\n\tdata = {}\n\t\n\tdata['name'] = files\n\t\n\tfor xml_filename in files:\n\t\txmls.append(source_xmls.path(xmls, xml_filename)[0])\n\t\tfile_basename_path = xmls[xmls.index(xml_filename)]\n\t\t\t\t\n\t\t\t\t\n\t\t\t# Create a new file path by writing it to disk\n\t\t\tfilename = filename_file.path(xmls)\n\t\t\tfile_basename_path.mkdir()\n\t\t\tfile_basename_path.join(dir_structure.get_tmp_dir(), file_basename_path"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t # Detamos e nas especial, tamb\u00e9m os filen diret\u00f3rios que n\u00e3o podes podes os especificados\n\t teste = get_este(file_path)\n\t teste = teste.strip()\n\t\t\n\t if (not (prefix[:2] == prefix[1:])):\n\t\t\tfor line in file(file_path):\n\t\t\t\tif (prefix[:2] == prefix[1:]) and\\\n\t\t\t\t\t    (prefix[2:].find(line) == -1):\n\t\t\t\t\t# Se a especificada dos pa"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\treturn _match_file_by_prefix(prefix, file_path)\n\t\n\tdef match(self, name):\n\t\t\"\"\"\n\t\tIdentify if `name` belongs to a package by a given name\n\n\t\tRetorna `True` para documentos pertencentes a um pacote.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tname : str\n\t\t\tPackage name\n\t\tReturns\n\t\t-------\n\t\tbool\n\t    \"\"\"\n\t\treturn _match_file_by_prefix(name, self.filenames)\n\t\n\tdef _match_resource_in_package(self, prefix, file_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif not prefix:\n\t\treturn True\n\t\n\treturn any([match_file_by_prefix(prefix, f) for f in file_path.split(\"/\") if match_file_by_prefix(f.lower(), prefix)!= \"http://\"])\n\n\ndef find_package_dir(package_dir):\n\t\"\"\"\n\tGet full full path of `package_dir`\n\tAssumes folder name must match\n\t\n\tParameters\n\t----------\n\tpackage_dir : path\n\t    Path where package should be searched\n\tReturns\n\t-------\n\tpath : str\n\t    Full path to"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\tif not prefix in package_files:\n\t\tmodule = pkg.module()\n\t\tmsg = \"No such file: %s\" % prefix\n\t\traise ValueError(msg)\n\t\n\ttry:\n\t\tmodule = module.__dict__[prefix]\n\t\t\n\t\tlog.debug(\"Loitero diverso de idieda.\")\n\t\tlog.debug(\"Convierte para documentos\")\n\t\tlog.debug(\"O par\u00e1metro s\u00e3o os valores por ponto Python\")\n\t\tif isinstance(file_path, unicode):\n\t\t\tfile_path = file_path.encode(\""}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\ttry: file_path=os.path.abspath(file_path)\n\texcept UnicodeEncodeError: pass\n\t\n\tfor filename in os.listdir(file_path):\n\t\t\n\t\tfound_in = False\n\t\t\n\t\tfor p in pattern.findall(filename):\n\t\t\t\n\t\t\tfull_filename=p[:-len(p)]\n\t\t\t\n\t\t\tfor p_name in pattern.split(p):\n\n\t\t\t\tif file_path.startswith(full_filename) and p_name.endswith(p):\n\t\t\t\t\n\t\t\t\t\tif re.search(p_name, file_path):\n\t\t\t\t\t\ti_match = True\n\t\t\t\t\t\t\n\t\t\t\t\t\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\tif prefix!= '/':\n\t\tprefix += '/'\n\t\n\tif file_path.lower().endswith('json')!= False:\n\t\tdata = json.loads(open(file_path, 'r').read())\n\n\t\tif data.get('ok', False):\n\t\t\tprint(\"Documento \" + file_path+ \" exists.\")\n\t\t\treturn True\n\n\t\telif data.get('info', False)!= False:\n\t\t\tprint(\"File \" + file_path+ \" does not contain a document type.\")\n\t\t\treturn False\n\t\t\n\t\telif data.get('link', False)!= False:\n\t\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\t\n\tif prefix in document_packages:\n\t\t\n\t\tpackage_info = []\n\t\tfor dir in document_packages[prefix]:\n\t\t\tif prefix!= \"/\":\n\t\t\t\tpackage_info.append(\"  %s\" % file_path+dir)\n\t\t\n\t\treturn any(\n\t\t\tmatch_file_by_prefix(package_info, dir)\n\t\t\tfor package_info in _valid_files(file_path)\n\t\t)\n\t\t\t\n\treturn True\n\t\t\t\n### Ajustes e o campo arquivo dado no formato\ndef match_file_by_suffix(suffix,"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\tif file_path.startswith(prefix):\n\t\t\treturn True\n\t\t\n\t\treturn False\n\ndef match_file_by_suffix(suffix, file_path):\n\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `suffix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tsuffix : str\n\t    Filename suffix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\tif"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tm = re.match('[\\w]+$', prefix)\n\t\n\tif m:\n\t\treturn True\n\t\n\treturn False'''\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\n\ndef read_file_by_prefix(prefix):\n\t\"\"\"\n\tRead file with a prefix\n\t\n\tReads in each line of a file.\n\t\n\tReturns \n\t-------\n\tlist of tuple\n\t  Each element in the list is a tuple, `(filename, lineno, function)`.\n\t\n\tExample\n\t-------\n\t-----\n\t>>> import os, string, re\n\t>>> data = open('foo', 'r').read()\n\t>>> match_file_by_prefix('\\\\foo\\\\*', data)\n\t[('<file foo>> 1.0', 20, 1, '<function foo()"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\ndef read_text_file(text_file):\n\twith open(text_file) as f:\n\t\tdata = f.read()\n\treturn data\n\n\ndef split_string(text):\n\t\"\"\"\n\tSplits given text line into a list.\n\n\tParameters\n\t---------\n\ttext : str or unicode\n\t\tInput input text\n\n\tReturns\n\t-------\n\tlist\n\t    List of split lines for text\n\t\"\"\"\n\t\ndef _check_len(line):\n\t\"\"\"\n\tCheck if `line` is longer than `MAX_WIDTH_LENGTH`\n\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tif not files:\n\t\t\n\t\treturn []\n\t\n\t\n\tsuffix = '.' + files[-1]\n\tbase_path = root_path(prefix)\n\t\n\tfiles.append(get_filename(base_path, suffix))\n\t\n\treturn files\n\t\ndef get_filename(base_path, suffix):\n\t\n\tif not suffix:\n\t\tsuffix = ''\n\n\telse:\n\t\t\n\t\tsuffix = suffixes[suffix]\n\t\n\tfname = '%s%s' % (base_path, suffix)\n\t\n\treturn fname\n\t\ndef root_path(prefix, file_ext):\n\t\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tdef get_filenames_by_pkg(prefix, files): \n\t\tfiles = [f for f in files if f.startswith(prefix + '.')]\n\t\treturn list(set(files))\n\t\t\n\tfilenames = [os.path.basename(f) for f in files]\t\n\t\n\tfiles = []\n\tfor f in files:\n\t\tfiles.append(get_filenames_by_pkg(prefix, f))\n\t\t\n\treturn list(set(files))\nif __name__ == '__main__':\n    print \"Definando\"\n    \n    parser = ArgumentParser(description=\"Cujos"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfilenames = []\n\t\n\t\n\tfor path in files:\n\t\tif os.path.isfile(prefix + '/' + path):\n\t\t\tif not os.path.isdir(prefix):\n\t\t\t\tos.chdir(dir_path)\n\t\t\tfilenames.append(os.path.basename(prefix + '/' + path))\n\t\t\tif prefix in ['doc', 'doc/']:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tfilenames.append(os.path.basename(prefix + '/' + path))\n\n\t\tfiles.remove(path)\n\t\n\treturn filenames\n\ndef get_file_names(prefix,"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\t# Se captura la excepci\u00f3n, para cortar la excepci\u00f3n, para a esta ejecuta de pagina\n\texcept IndexError:\n\t\tprint 'Error'\n\n\treturn [os.path.basename(os.path.realpath(f)) for f in files]\n\t\ndef get_metadata(filename, directory):\n    try:\n        metainfo = json.loads(open(os.path.join(directory, filename)).read())\n        return metainfo['metadata']\n        #return metainfo\n    except (ValueError, SyntaxError) as e:\n        print 'Error"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\tfiles = set(files)\n\n\t\tfor filepath in files:\n\t\t\tfor prefix in files:\n\t\t\t\tfname = prefix + os.path.basename(filepath)\n\t\t\t\tif os.path.isfile(fname):\n\t\t\t\t\tyield fname, os.path.basename(filepath)\n\t\t\t\telse:\n\t\t\t\t\tyield fname, ''\n\n\ndef read_document(filename):\n\t\"\"\"\n\tRetorna um documento.\n\tSeu fileo deve ser um arquivos do ESC.\n\t\n\tParameters\n\t----------\n\tfilename : str\n\t  equipement os arquivos,"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfrom.document import Document, DocumentTypes, DocType\n\tdocument_types = DocumentTypes()\n\tdocument_types.update(DocumentTypes())\n\tdocument_types.update(DocType())\n\t\n\tprefix_list = prefix.split('/')\n\t\n\tprefix_list = map(prefix_list.strip, prefix_list)\n\t\n\treturn (f.strip() for f in files if f.partition('.doc')[0].upper() == prefix).sort()\n\t\t\ndef check_for_documents_glob(prefix, files, prefix_glob):\n\t\n\t# Elimina dos globes"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tfiles = []\n\t\n\t\n\tif files is None:\n\t\tfiles = get_filenames([prefix])\n\n\tfor f in files:\n\t\tfiles.append(f)\n\n\treturn files\n\t\ndef select_filenames_by_suffix(doc, prefix, files, suffix):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `suffix`\n\t\n\tParameters\n\t----------\n\tdoc : document\n\t\tSource Document\n\tprefix : str\n\t    Filename prefix\n\tfiles"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\t\n\tdir_path = join(join(env.root, prefix))\n\t\n\tif not exists(folder_path):\n\t\tos.makedirs(folder_path)\n\n\tfor f in files:\n\t\tfilename = join(folder_path, f)\n\t\tprint('Reading: %s' % filename)\n\t\tif isdir(filename):\n\t\t\tyield join(prefix, filename)\n\t\telif isfile(filename):\n\t\t\tfilename_glob = [os.path.join(folder_path, pattern) for pattern in \n\t\t\t\t        ('.*.tar', '*.tar.gz', '*.tgz',"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tret = []\n\t\t\n\t\n\t\n\tfor filename in files:\n\t\ti = filename.rfind(prefix)\n\t\tpath = filename[:i]\n\t\tif path!= prefix:\n\t\t\ti = path.rfind(prefix)\n\t\t\tbasename = path[i + 1:]\n\t\t\tret.append(os.path.join(prefix, basename))\n\t\n\treturn ret\n# -*- coding: UTF-8 -*-\n\"\"\"Tests for the Pages.py module\"\"\"\n\n#from py_pandora import Pandora\n#from py_common import Mock\n#from py_common import mock"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\titems = os.listdir(folder)\n\t\n\t\n\tfiles = []\n\t\n\tfor item in items:\n\t\n\t\tfilepath = os.path.join(folder,item)\n\n\t\tif(item.lower().find('_data/')!= -1):\n\t\t\t\n\t\t\txml = open(filepath,'r').read()\n\t\telse:\t\t\n\t\t\txml = open(filepath,'wb').write()\n\t\t\t\n\t\t\t\n\t\t\tif(item.lower().find('metadata/')!= -1):\n\t\t\t\txml.write(item)\n\t\t\t\n\t\t\titems = file(filepath).readlines()\n\t\t\n\t\tfiles.append(filepath)\n\n\t\t\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t\n\tlogger.debug(\"Folder {}\".format(folder))\n\t\n\t\n\txml = utils.read_xml(os.path.join(folder, \"products.xml\"))\n\t\t\t\n\t\n\t\n\tpackage_tree = ET.parse(xml)\n\t\n\tlogging (package_tree)\n    \n\t    package_index = 0\n\t#Get root_folder_data = utils.get_root_folder_data()\n\t\n\t    root_folder_data = root_folder_data.strip().split('/')\n\t\n\t    for root_file in root_folder_data:\n\t\tpackage_index += 1\n\t\tpath ="}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\treturn {\n\t\t\t'package': FolderGetXml('/home/hodberg'),\n\t\t\t'packs': sorted(FolderGetXml('/home/hodberg/packs')),\n\t\t\t'packages': [PackageGetXml('/home/hodberg/package/%s' % pkg) for pkg in FolderGetXml('/home/hodberg/packs/')]\n\t\t\t}\n\t\n\n\n\ndef explore_packs(folder, packs):\n\t\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups packages based on the package name.\n\t\n\tParameters\n\t----------\n\tfolder :"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tdef _recursive_explore(folder):\n\t\"\"\"Recursive function to explore a package/folder looking for it\"\"\"\n\t\n\t\t# Expand to get relative path\n\t\tpath = folder\n\t\tfor item in os.listdir(path):\n\t\t\tpath = os.path.join(path, item)\n\t\n\t\t# We also want to make folders non-folders\n\t\tif not path.startswith(folder):\n\t\t\treturn {}\n\t\n\treturn {'data':[]}\n\t\n\ndef explore(data):\n\t\"\"\"Generate package/folder explore data\"\"\"\n\t\n\t# Get the xml\n\tpkg ="}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t#print \"Opening folder\", folder\n\topen(folder,'rb').close()\n\tdata = open('PKGDATA.json')\n\tdict = json.load(data)\n\n\t#print \"Opening dict\", dict\n\treturn dict\n\n\n#TODO: Add better code (just run this in to the package file): \n\n\ndef _get_package(package):\n\twith open(package, 'r') as pkg_data:\n\t\tdata = pkg_data.read()\n\t\treturn data\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n\ndef _get_xml_file"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\ndef import_data(fname):\n\t\"\"\"\n\tRead the data from the tar.gz and process it.\n\t\n\tParameters\n\t----------\n\tfname : str\n\t    Filename in tar filename\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfname = os.path.join(DATA_DIR, fname)\n\t\n\t# Read the metadata from the tar file and extract it\n\t\n\tf = pkg.tar.gz.open(fname)\n\tdata = []\n\t\n\t\n\t\n\tfor line in f:\n\t\t\n\t\tif line[0:2] == \"File \" :\n\t\t\tname = line"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t    \n\tif not os.path.isdir(folder):\n\t\tos.mkdir(folder)\n\t\n\tdata = glob.glob('%s/*.xml'%folder)\n\t\n\t#print data\n\t\n\tfor data in data:\n\t\ttry:\n\t\t\tinfo = pyupm.package(data)\n\t\t\t\n\t\t\t# add all the dependencies and dependencies to the dictionary\n\t\t\tfor dep_data in info.dependencies:\n\t\t\t\tprint dep_data.relative \n\t\t\t\tdep_data.dependency = [d for d,f in dep_data.dependency if f.startswith('__')]\n\t\t\t\n\t\t\t# write the package data to"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\treturn { }\n#\n# Copyright 2013 Jonathan Peirce\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\ttry:\n\t    _folder = os.path.dirname(_folder)\n\texcept:\n\t    raise ValueError(\"Folder %r not found\" % (folder))\n\t\n\tif (_folder == \".\"):\n\t    return {}\n\t\n\treturn _folder\n\ndef file_from_text(folder, file_name):\n\t\"\"\"\n\tLoad the XML from a path and return the data\n\t\"\"\"\n\t\n\tif os.path.isfile(folder) and os.path.getmtime(folder) < os.path.getmtime(file_name):\n\t\treturn load_package_from_data(folder, file"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# create new list of packages' data\n\tpackages = dict()\n\t\n\t# get the name of the folder\n\tfor root, _, files in os.walk(folder):\n\t\t\n\t\t# create a root element\n\t\troot_element = ET.Element('data')\n\n\t\t# add element to root\t\n\t\troot_element.set('filepath', files[__name__])\n\t\t\n\t\t# append the elements to the root element\n\t\troot_element.append(root_element)\n\n\t\t# get the number of files\n\t\tcount = 1\n\t\tfor filename in files:\n\t\t\t\n\t\t\t# check if"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(file_folder):\n\t\t# Inicializa um pacote.\n\t\timport time\n\t\t# Proprimeo o tipo da arquivo\n\t\timport pacotes\n\t\t# Arquivo o tipo da pacote\n\t\timport pacotes.nuevo\n\t\t\n\t\timport numpy as np\n\t\t# Inicializa um arquivo\n\n\t\timport glob\n\n\t\tasset = pacotes.pacote_get(file_folder, directory_name=[prefix], asset_type='asset')\n\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\timport pandas as pd\n\timport datetime\n\timport re\n\n\tfolder_paths = []\n\t\n\tfor f in glob.glob(file_path):\n\t\tfilename = os.path.join(prefix, f)\n\t\tif not os.path.exists(filename):\n\t\t\tcontinue\n\t\t\n\t\t\n\t\tpath = re.sub(\".txt\", \"-\", filename)\n\t\t\n\t\tfolder_paths.append(path)\n\t\n\tif len(folder_paths) > 0:\n\t\treturn {str(prefix): dataset.create_from_pandas(folder_paths[0])}\n\telse:\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\timport xml.etree.ElementTree as xml\n\n\t\twith open(\"/tmp/\"+prefix+\"_\"+file_path+\".xml\",\"r\") as arquivo_root:\n\t\t\troot = xml.etree.ElementTree(root)\n\t\t\troot.make_namespace(prefix)\n\t\t\troot.writexml(arquivo_root)\n\t\t\troot.writexml(filename)\n\t\t\ttry:\n\t\t\t\treturn xml.etree.ElementTree(root).getroot()\n\t\t\texcept:\n\t\t\t\traise \n\t\texcept:\n\t\t\traise\t\t\n\t\t\n##################################################################\n#!/usr/bin/python\n# -*- coding:"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tfrom datetime import datetime\n\tfrom io import StringIO\n\t\n\timport numpy as np\n\t\n\timport os\n\n\tfolder = os.path.join(file_folder, prefix)\n\t\t\n\tfile = open(file_path, \"r\")\n\tdoc = minidom.parseString(file.read())\n\txml_file = StringIO()\n\t\t\t\n\t\n\tfor package in doc.childNodes:\n\t\t\n\t\tif package.nodeType == node.ELEMENT_NODE and isinstance(package, node.ELEMENT_NODE):\n\t\t\t\n\t\t\tif package.tagName == \"packages\":\n\t\t\t\tpackages = package.getElementsByTagName(\"package_reference\")\n\t\t\t\t\t\n\t\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tdef _extract_filename(filename):\n\t\tparts = filename.split('/')\n\t\tfilename = parts[-1]\n\t\t\n\t\tfor part in parts:\n\t\t\tfilename = path.join(filename, part)\n\n\t\treturn filename\t\t\n\n\treturn {'asset': _extract_asset_doc,\n\t\t\t'rendition': _extract_rendition_doc,\n\t\t\t'packages': _extract_packages_doc,\n\t\t\t'file_folder': _extract_file_folder_doc,\n\t\t\t\t}\n\n\ndef _check_doc_folder(base_path, doc_folder):\n"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\tlogging.info('Inicia o arquivo das arquivos do pacote: `{}` o arquivo '\n\t\t   'endere\u00e7os.'.format(prefix))\n\t\tlogging.info('La estructura para estructura do arquivo {}: '\n\t\t   'Se a\u00f1adir n\u00e3o \u00e9 excepto alse\u00e7\u00e3o de sua/em de pacote '\n\t\t  'se a pacote \u00e9 exce\u00e7\u00e3o.')\n\t\tif prefix not in os.listdir(file_folder):\n\t\t\tlogging.error('Arquivo \"{}\" n"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\t# Crea\u00e7\u00e3o os tipo de arquivo da partiba do folder\n\tif not file_path.endswith('/'):\n\t\tfile_path += '/'\n\t\n\tasset = '%s/%s' % (prefix, file_path)\n\tif file_path.startswith('https://docs.trading-data.org/api/'):\n\t\tasset = 'https://docs.trading-data.org/enquiries' + asset\n\tif prefix =='market':\n\t\treturn _market_to_asset(asset)\n\telse:\n\t\treturn Asset(assets=["}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tbase_dir = os.path.abspath(os.path.split(file_path)[0])\n\t\n\t\n\tif prefix in os.listdir(base_dir):\n\t\t\n\t\t\n\t\tif file_folder is None: return\n\t\t\n\t\ttry:\n\t\t\n\t\t\tf = open(os.path.join(base_dir, prefix), 'r')\n\t\t\t\n\t\t\tcontent = f.read()\n\t\t\tf.close()\n\t\t\t\n\t\t\tpackage = package_from_xml(re.sub('(\\\\\\w)[\\\\\\s]*', '#'*len(re.sub('(\\\\\\w)[\\\\\\s]*','', content)), package"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tfilename = os.path.join(prefix, filename)\n\t\n\t\n\tif isinstance(file_folder, basestring):\n\t\tfile_folder = os.path.join(prefix, file_folder)\n\t\n\tif os.path.exists(file_folder):\n\t\tfname = os.path.join(prefix, file_folder + \".json\")\n\t\twith open(fname, \"r\") as f:\n\t\t\tresult = read_json(f)\n\t\tif \"version\" in result:\n\t\t\treturn result[\"version\"]\n\t\telse:\n\t\t\treturn None\n\t\t\n\t\n\t\n\traise ValueError"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\twith open(prefix + file_path, \"r\") as file_:\n\t\tlines = file_.readlines()\n\tnumproses = np.fromfile(prefix + file_path, formats='>d')\n\t\n\tfile_files = []\n\tfor i in range(2,numproses[0]):\n\t\tfile_files.append(prefix + file_path + '/' + str(i) + '_'.join(lines[2:8]))\n\n\treturn {'file_files': file_files}\n\ndef _eval_dataset(prefix, filename):\n\t\"\"\""}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\tself.set_file_path( file_path )\n\n\tprint (\"\\n--- New rendition ---\")\n\n\tif self.get_current_language()!= self.en:\n\t\tprint (\"\\n--- %s %s\" % (self.get_current_language(), self.en))\n\n\tif self.get_current_language() == lang:\n\t\tprint (\"\\n--- %s in english\" % lang)\n\telse:\n\t\tprint (\"\\n--- %s in esperanto\" % lang)\n\nclass TextToImage(object):\n\tdef __"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t    if 'en' in lang and self.remove_extension(file_path, lang, False):\n#\t\treturn\n\n\tfrom Axel.utils.misc import write_file, read_file\n\n\tif lang!= '':\n\t  self.doc.append([\"add\", [lang], \"Axel file at %s\" % file_path])\n\t  self.doc.append([\"add\", [self.path]])\n\telse:\n\t  add_rendition(self.path, (self.lang,), file_path)\n\n\tif self.remove_extension(file_path, lang, False"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t    \"artigo02_en.pdf\"\n\t    self.add_picture(lang, file_path)\n\n\t    \"artigo04.pdf\"\n\t    self.add_picture(lang, file_path)\n\n\t    \"artigo.pdf\"\n\t    self.add_picture(lang, file_path)\n\n\t    \"artigo.pdfx\"\n\t    self.add_picture(lang, file_path)\n\n\t    self.add_picture(lang, file_path)\n\n\t    langs = [\"artigo02.pdf\", \"artigo02."}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\t# create Russian-style English text file\n\t\tfile = r\"\"\"\n\t$PATH_ID=$(RENDITION(\"Russian-style\",\"artigo02\"))\n\t$PATH_FILE=$(RENDITION(\"Russian-style\",\"artigo02-en.pdf\"))\n\n\t\"\"\"\n\t\tfile = open(file_path, \"rb\")\n\t\tdata = file.read()\n\t\tfile.close()\n\t\tself.generate_data(data, lang)\n\t\tfile.close()\n\n\tdef build_index(self, doc):\n\t\"\"\"\n\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\tfile_path = file_path + \"_R7U_R13.pdf\"\n\n\treturn self.path_of_rendition(lang, file_path)\n\n\nclass EarlyNLP(LangLinter):\n\t\"\"\"\n\thttps://www.google.com/en/e/en-en-language-language-language-language-language/index.html\n\te.g., Early's nlp file renderer with support for NLP.\n\t\"\"\"\n\tdef __init__(self, language):\n\t\tLangLinter.__init__(self, None"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n    def generate_index(self, target_path, langs, languages):\n        \"\"\"\n\t{\n\t    \"target\": \"artigo.html\",\n\t    \"files\": \"artigo-en.html\",\n\t}\n        \"\"\"\n\n        path = \"%s.html\" % target_path\n\n        files = {}\n        languages_path = self.find_output_in_path(path)\n        languages_path = languages_path.replace('\\\\', '/')\n        languages.keys().sort()\n        self.get_templates_and_generate_index(self.get_content_dirs(), path, langs, languages"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n\n\tfile_name = os.path.basename(file_path)\n\tif lang == \"de\":\n\t\tif file_name == \"./de.pdf\":\n\t\t\treturn 1\n\t\tif file_name == \"./en.pdf\":\n\t\t\treturn 1\n\t\tif lang == \"de\":\n\t\t\treturn 1\n\t\tif lang == \"en\":\n\t\t\treturn 1\n\t\n\tif file_name in self.resources:\n\t\tself.resources[file_name] = []\n\t\n\tfile_path = os.path.join(self.root_path, file_name)\n\t\n\tassert"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t    \n\t\tpass from __future__ import print_function, absolute_import\n\ntry:\n    import json\nexcept ImportError:\n    import simplejson as json\nfrom pprint import pprint\n\nfrom django.core.exceptions import ObjectDoesNotExist, ValidationError\nfrom django.http import Http404\nfrom django.response import Http404\nfrom django.utils.translation import ugettext as _\n\nfrom sentry.models import (\n    Alert, AuditLogEntry, Release, ReleaseFile\n)\nfrom sentry.plugins.base import (\n    Plugin,\n    PluginError,\n    TeamsPlugin,\n)\nfrom sentry.utils"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\n#\tget_current_doc()\n\t#\tif not os.path.exists(file_path):\n\t#\t\tprint \"Error: File %s not found!\" % file_path\n\t#\t\treturn None\n\t\n\text_file = os.path.splitext(file_path)[1]\n\ttry:\n\t\t#\tprint \"Opening PDF %s\" % file_path\n\t\tpdf_file = open(file_path, 'rb')\n\texcept IOError:\n\t\tprint \"Error opening PDF %s, make sure you have permission to view the given"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t    \n\n\tdef add_subentry(self, lang, file_path, is_new=False):\n\t\t\"\"\"Returns:\n\t \t \n\t\t    Erro no esa modificado se pueden en formato artificao:\n\t\t@param lang\t(str)\t\n\t\t@param file_path : \t\t\t\n\t\t@param is_new : \t\t\t\t\n\t\t\"\"\"\n\t\tif file_path not in self.entries: \n\t\t\t# Retorna la redicacion entrada \n\t\t\tself.entries[file_path] = SubInfo(\n\t\t\t\t"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tf = codecs.open(file_path, \"w\", \"utf-8\")\n\t\tf.write(basename)\n\t\tf.close()\n\t\n\tdef get_data(self):\n\t\t\"\"\"\n\t\tget_data() -> returns the original data\n\t\t\"\"\"\n\t\treturn self.__get__()\n\t\t\n    # -----------------------------------------------------------------------------------------------------------------------\n    def __get__(self, instance, *args, **kwargs):\n        \"\"\"\n\t\tget_data() -> returns data in dictionary\n\t\t\"\"\"\n        return self.getData()\n\t\t\n    # -----------------------------------------------------------------------------------------------------------------------\n    # return the value which was set on the instance variable:"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t    \n\t\t\n\t\tprint(\"Add asset: \", basename)\n\t\treturn_value = self.app.put('/assets/', {'basename': basename, 'filePath': file_path},\n\t                    headers=self.default_headers, **self.headers)\n\t\t\n\t\t#print(\"File: \", file_path)\n\t\treturn_value.status_code\n\t\tprint(\"Returncode: \", return_value.status_code)\n\t\tprint(\"Exit status code : \", return_value.status_code)\n\t\tprint(\"Create permissions: \", return_value.headers['X-Content-Type'])\n\t\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t    \n\tprint \"===============================\"\n\tprint \"add_asset:\", basename, file_path\n\n\t# add files from this directory to the asset tree\n\tfor root, dirs, files in os.walk(os.path.abspath(file_path)):\n\t\tfor f in files:\n\t\t\tif f.startswith(\"Artigo\") and \"artigo02-gf03.tiff\" in f:\n\t\t\t\tprint f\n\t\t\t\tadd_directory(os.path.join(root, b\"Artigo02_gf03.tiff\"), os.path.join(root, b"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tif not self.images:\n\t\t\tself.images = []\n\t\t\tprint \"Warning: Cannot find images!\"\n\t\telse:\n\t\t\timage = loadImage(basename)\n\t\t\tself.images.append(image)\n\n\tdef add_files(self, artigo02_dirs, artigo02_names, artigo02_filepaths):\n\t\t\"\"\"\n\t\t{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t    \n    def process_file (self, filename, content):\n        if not os.path.exists(filename):\n           continue     \n\tif not os.path.isfile(filename):\n\t\treturn\n            \n        with open (filename) as f:\n\t\t\n            asset = self._create_asset(filename, f)\t\n        \n            return self.process_image_file (asset, content)\n            \n        \n    def _create_asset (self, path, path_list):\n        '''\n        Create an artigo from a list of paths.\n\n        The path_list is a list of tuples in the form (directory,\n        file name"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tif not os.path.isdir(basename):\n\t\t\tos.makedirs(basename)\n\n\t\tfilename = os.path.basename(file_path)\n\t\tif not os.path.isfile(filename):\n\t\t\twith open(filename, 'wb'):\n\t\t\t\tpass\n\t\t\t\n\t\tprint filename\n\t\tself.asset_folder = os.path.dirname(os.path.realpath(__file__)) + '/'\n\t\tself.assets.append(Asset(\n\t\t\tbasename,\n\t\t\tfilename\n\t\t))\n\t\t\n\t\tprint'made' + basename\n\n\t@staticmethod\n\tdef from_file(file_"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\tif basename not in self.assets:\n\t\t\traise ValueError(\"There is no asset named {0}\".format(basename))\n\t\tasset = self.assets[basename]\n\t\tif asset.__class__ == AssetDB2:\n\t\t\traise ValueError(\"Cannot add file with name of {0}\".format(basename))\n\t\tself.files.append({\"name\": basename, \"file_path\": file_path})\n\n\tdef get_assets(self):\n\t\treturn self.files\n\t\t\n\tdef to_file(self):\n\t\t\"\"\" Creates a new file \"\"\"\n\t\t\n\t\t# print \"to_file\""}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n\t# Check if file exists\n\t\tif not os.path.exists(file_path):\n\t\t\tprint(\"File path '\"+file_path+\"' not found. Please change file name.\")\n\t\t\treturn \n\t\tprint(\"Asset exists. Please wait.\")\n\t\treturn False   \t\t\n\t\t\t    \n\t\n\t\n\t# Check if directory exists\n\t\tif not os.path.exists(self.folder_path):\t\t\n\t\t\tprint(\"Directory path '\"+self.folder_path+\"' not found. Please change folder_path.\")\n\t\t\treturn \n\t\tprint(\"Directory exists. Please wait.\")\n\t\treturn"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\timage = self.get_image(basename, file_path)\n\t\tif image is None:\n\t\t\ttry:\n\t\t\t\tos.makedirs(\"%s.jpg\" % self.config[\"images\"][\"imgpath\"])\n\t\t\t\timage = file_path\n\t\t\texcept OSError as ose:\n\t\t\t\tprint \"Oops! I can't create temporary directory for %s\" % basename\n\t\t\t\tprint ose\n\t\t\t\n\t\treturn image\n\t\t\n\tdef add_dir_to_image(self, basename, directory_name):\n\t\t\"\"\"\n\t\t\"\"/path/image/path.jpg\" -> \"/path/"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n\n\tnew_source_directory = os.path.join(\"/home/xgugli/expedido/media/images/artigo02-gf03\")\n\tif not os.path.exists(new_source_directory):\n\t\tos.makedirs(new_source_directory)\n\n\tsource_filename = basename+\".tiff\"\n\tsource_path = os.path.join(\"/home/xgugli/expedido/media/images/artigo02-gf03-%s.tif\" % basename)\n\tsource_path = os.path"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n    def _explore_metadata(files):\n\"\"\"\n   return packages' metadata in one dictionary format and lists them\n\tin the specified list of files.\n\n\tParameters\n\t----------\n\tfiles : dict\n\t\t[dict,str]\n\n\tReturns\n\t-------\n\tdict\n\n\t\"\"\"\n        return dict((key, _explore_metadata(os.path.join(zip_path, k))) fork, \\\n\t\t         (file, val) in files.iteritems() if os.path.isfile(file))\n\n    def _explore_packages(packages):\n\t\"\"\"\n\tExp"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\treturn yaml.safe_load(open(zip_path).read())\n\t\n\ndef create_package_from_files(path):\n\t\"\"\"\n\tCreate Package from given file name\n\t\n\tCreate Package from given path name.\n\t\n\tParameters\n\t----------\n\tpath : str\n\t    path name which is to be created\n\t\"\"\"\n\t\n\t\n\treturn package.package.create(path)# -*- coding:utf-8 -*-\nimport itertools\nimport base64\nimport json\nimport logging\n\nfrom django.conf import settings\nfrom django.core.cache import cache\n\nfrom..base"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tfiles = glob(zip_path)\n\tret = {}\n\tif files:\n\t\tfor fn in files:\n\t\t\twith open(fn, 'r') as data:\n\t\t\t\tdata = data.read()\n\t\t\t\tret[fn] = data\n\t\n\treturn ret\n\ndef zip_to_dict(zip, name, directory):\n\t\"\"\"\n\tGet package data from zip.\n\t\n\tParameters\n\t----------\n\tzip  : str\n\t    name of file\n\tname   : str\n\t    unique identifier of package\n\tdirectory : str\n\t    path to directory\n\t\n\tReturns\n\t-------"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\t\n\t\t\n\t\twith open(zip_path, 'rb') as unzipped_file:\n\t\t\tunzipped_data = unzipped_file.read()\n\t\t\t\n\t\treturn unzipped_data\t\t\n\t\t\n\ndef _explore_file_list(file_list, files_by_name):\n\t\n\t\"\"\"\n\tGiven a set of files and a list of file name, explore those files.\n\n\tParameters\n\t----------\n\tfile_list : list\n\t    list of files\n\tfiles_by_name : list\n\t    list of file names\n\n\tReturns\n\t-------\n\tdata in dict format"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\ndef _explore_directory(directory):\n\t\"\"\"\n\tGet files by their xml basename and returns data in dict format.\n\t\n\tGroup files by their XML basename and return data in dict format.\n\t\n\tParameters\n\t----------\n\tdir : str\n\t    where /path/to/directory\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\ndef _get_pkg_from_data_files(data_files):\n\t\"\"\"\n\tGets packages from data files\n\t\n\tGroups files by their XML basename and return data in dict format.\n\t\n\tParameters\n\t----------\n\tdata_files"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\t# Get file list from zip file\n\tlist_files = os.listdir(zip_path)\n\n\t\n\t# Expand file names into a dict of package-name to package-data\n\tpackage_names = {}\n\n\tfor name in list_files:\n\t\tpackage_names[name] = _expand_file(os.path.join(zip_path, name))\n\t\n\treturn package_names#-*- coding:utf-8 -*-\n\nimport json\nimport logging\nimport time\nimport re\nimport urllib\nimport urlparse\nfrom collections import namedtuple\nfrom..errors import *"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path, 'r') as zip_data:\n\t\t\tdata = map(lambda path: zip_data.read(path), zip_data.namelist())\n\n\t\treturn data\n\t\n\n@app.before_request\ndef _save_config():\n\tapp.logger.debug('Configuring zip file: %s', zip_path)\n\tapp.logger.debug(datetime.utcnow())\n\n\tif 'config_dir' in os.environ:\n\t\tapp.logger.debug('Using config_dir')\n\t\tshutil.copy2(app.config"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\nclass _WsgiCvn:\n\t\n\t\"\"\"This helper function scans the folder given the input zip filename.\n\t\n\tA file name (str) is returned if the zip file name is not given\n\tand a path is given which is returned.\n\t\n\tFor example::\n\t>>> print _WsgiCvn('foo.bin.tar.gz')\n\t{'foo.bin.tar.gz.tar.gz': 'ar'}\n\t\"\"\"\n\t\n\tdef __init__(self, zip_path):\n\t\tself.file_path = zip_path\n\t\t\n\tdef _get_package_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t\n\tif zip_path.endswith(\".zip\"):\n\n\t\t#return data to be loaded from zip file\n\t\t\n\t\tfile_list = [\n\t\t\t#xml_files\n\t\t\t['data/mapfile/mapfiles/mapFile_mapfile.xml',\n\t\t\t 'data/maps/mapfile/mapfile_mapfile_mapfile_mapfile_mapfile.xml',\n\t\t\t 'data/maps/mapfile/mapfile_mapfile_mapfile_mapfile_mapfile_list.xml',\n\t\t\t 'data/maps/mapfile/mapfile_mapfile_mapfile_mapfile"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n#\tfrom numpy import *             # Make list return dictionary\n#\tfrom gpyl import GPyL, GPy\n#\timport string, os\n#\n#\tfor name, xml_name in os.walk(zip_path):\n#\t\tassert len(xml_name) == 6\n#\t\twith open(xml_name[0]) as file_handle:\n#\t\t\tyield file_handle\t# make dictionary and write\n\t\n\ndef _add_sub_elements(sub_list):\n\t\n\t## make list of strings\n\tsub_list_of_"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\toutput = []\n\t\tfor root, dirs, files in os.walk(zip_path):\n\t\t\tif 'kml' in dirs:\n\t\t\t\tkml_file = os.path.join(root, 'kml')\n\t\t\t\tif os.path.exists(kml_file):\n\t\t\t\t\tkml_file_path = os.path.join(root, 'kml', 'kml_data.kml')\n\t\t\t\t\toutput.append(kml_file_path)\n\t\t\t\telse:\n\t\t\t\t\tkml_file_path = os.path.join(root, 'kml')\n\t\t\t\t\tif os.path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\traise NotImplemented\n\t\n\tfrom openpyxl.writer.excel import load_workbook\n\tfrom openpyxl.writer import XlsxWriter\n\timport os.path\n\tprint 'Loading files in the zip folder... ', zip_path\n\t# Read in the zip for the user\n\tfile_list = []\n\n\tprint 'Selecting files to check...'\n\t\n\tfor file_path in zip_path.split('/'):\n\t\tprint '\\n', file_path\n\t\t\n\t\tpath = os.path.dirname(file_path)\n\t\tif not (path.startswith('http://') or path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tfiles = []\n\tfor z in range(len(zip_path)):\n\t\tzip_file = os.path.join(zip_path, zip_file)\n\t\tif file_is_open(zip_file):\n\t\t\tfor file in os.listdir(zip_file):\n\t\t\t\tfname = os.path.join(zip_file, file)\n\t\t\t\tif not file_is_open(fname):\n\t\t\t\t\tcontinue\n\t\t\t\tnid = file_name_to_nid(filename)\n\t\t\t\tfiles.append((nid, file))\n\t\telse:\n\t\t\tlogging."}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\tfilenames = []\n\t\tfor f in os.listdir(zip_path):\n\t\t\tif f.endswith(\".\" + file_extension):\n\t\t\t\tfilenames.append(os.path.join(zip_path, f))\n\t\treturn filenames\n\t\n\tdef get_files_by_name(self, file_name, file_extension, files_list=[]):\n\t\t\"\"\"\n\t\tGet the files in all files of the file_name that we will add\n\t\tfor the `file_extension` in the zip file.\n\t\t\n\t\tTo get only the files in the file_name that we will then we"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\tif os.path.isfile(zip_path) == False :\n\t\t\twith zipfile.ZipFile(zip_path, 'r') :\n\t\t\t\t# this is a path\n\t\t\t\tfilename = os.path.basename(zip_path)\n\t\t\t\tpathlen = len(filename)\n\t\t\t\tif pathlen == 0 :\n\t\t\t\t\t# zip.ZipFile could raise a ValueError exception.\n\t\t\t\t\traise ValueError('zip.ZipFile returns False, but file path does not exist')\n\t\t\t\n\t\t\t\t# open the zip file\n\t\t\t\tzip_file = zip.open(filename,'rU')\n\t\t\t\t# read the file\n\t\t\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\tfrom cStringIO import StringIO\n\n\t\tzip_dir = os.path.split(zip_path)[-1]\n\t\tzip_file = zipfile.ZipFile(StringIO(zip_path), 'r')\n\t\t#print zip_dir\n\t\t\n\t\tfiles = zip_file.namelist()\n\t\tfilelist = zip_file.filelist\n\t\t\n\t\tif zip_dir == '.':\n\t\t\treturn files\n\t\n\tif zipfile.is_zipfile(zip_path):\n\t\tfor file in files:\n\t\t\tfiles_list_from_zipfile(file)\n\telse:\n\t\tfiles_list_from_zipfile(zip_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\t\n\t# Make sure the input path is a file\n\tif not isinstance(zip_path, basestring):\n\t\traise TypeError(\"Input path should be a file path\")\n\t# Make sure we have a zip archive (if it doesn't already exist)\n\tzip_file = zipfile.ZipFile(zip_path)\n\t# Get the entries within the zip\n\tfor entry in zip_file.infolist():\n\t\tif len(entry.filename) > 0:\n\t\t\t# Add the path to the list\n\t\t\tzip_file.writestr(entry.filename, entry.file)\n\t\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\tresult = []\n\t\twith zipfile.ZipFile(zip_path, mode=\"r\") as zip_and_namelist:\n\t\t\tfor zip_file in zip_and_namelist:\n\t\t\t\twith zip_file.open(mode='r') as f:\n\t\t\t\t\tif len(f) == 0:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tresult.append(list(map(float, f.read())))\n\t\treturn result\n\t\n\t@staticmethod\n\tdef _get_file_name(path):\n\t\treturn os.path.basename(path)\n\t\n\t@staticmethod\n\tdef _split_list_in"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tfrom django.contrib.auth.models import User\n\n\tfrom django.dispatch import receiver\n\tfrom django.utils importencoding\n\n\tlist_of_files = [\n\t\t('1.jpeg', '<a href=\"/static/file/a3.jpg\">A3</a>'),\n\t\t('1.ps', '<a href=\"/static/file/a3c.jpg\">A3 C</a>'),\n\t\t('1.tif', '<a href=\"/static/file/a3d.jpg\">A3 D</a>'),\n\t\t('1.tif', '<a href=\"/static"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\tfile_list = []\n\t\tif(zipfile.is_zipfile(zip_path)):\n\t\t\tif(open(zip_path).read().endswith('1337.zip')):\n\t\t\t\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\t\t\t\t#zip2.ZipFile will raise error if ZipFile is not in memory\n\t\t\t\t\tfor entry in zip_file.namelist():\n\t\t\t\t\t\tfile_list.append(entry)\n\t\t\t\t\t#zip2.unzip is broken in python2.5\n\t\t\t\t\tzip_file.extractall(zip_path)\n\t\t\t\twith open"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tcontents_list = content.splitlines()\n\t\tfor contents in contents_list:\n\t\t\tcontents_list = contents.strip('\\n')\n\t\t\tif contents_list:\n\t\t\t\tcontents = '<s>' + contents_list + '</s>'\n\t\t\tcontent = re.sub('st','w:st=\"' + contents + '\"', content)\n\treturn content\n\n\ndef remove_prefixes(text):\n\tcontent = ''\n\twith open(os.path.join(settings.CONTENT_DIR,'prefix_preprocessors.content'),'r') as f:\n\t\tfor line in f:\n\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\ts = content.split(\"\\n\")\n\t\ttry:\n\t\t\tfor word in s:\n\t\t\t\tword = \"w:%s\" % word\n\t\t\treturn s[0]\n\t\texcept ValueError:\n\t\t\treturn \"\"\n\treturn \"  \"\n\n\ndef _convert_doc(content):\n\t\"\"\"\n\tConverts a string in RTD to a list of words, and a single list of words (or single words for attributes).\n\t\"\"\"\n\n\twords = []\n\ti = 0\n\twhile i == 0:\n\t\tword = \"\"\n\t\titem = content[word].split()"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tif content is None:\n\t\t\treturn content\n\t\tif content == None or content == u'w:st=\"' or content is None:\n\t\t\treturn content\n\t\treturn re.sub(r'(^\\w)\\.(default|default).*', '', content)\n\ndef unescape_prefix_w(content):\n\tif content is None:\n\t\treturn None\n\tif content == u'w:st=\"' or content is None:\n\t\treturn None\n\treturn re.sub(r'.*\\.(default|default).+', u'w:', content)\n\ndef fix_namespaces("}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tfor line_no, line in enumerate(content):\n\t\t\tline = line_no + 1\n\t\t\tline_no_new = 0\n\t\t\twhile line_no_new < len(line):\n\t\t\t\tnew_line = line[line_no_new]\n\t\t\t\tif line_no < len(line):\n\t\t\t\t\tline_no_new += 1\n\t\t\t\tline_no_new += 1\n\t\t\t\tif line[line_no_new] == '}':\n\t\t\t\t\tline_no_new += 1\n\t\t\t\telse:\n\t\t\t\t\treturn line_no_new\n\n\t# This method replace the"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\n#\t\tout = '\\n'.join(str(c) for c in content)\n\t#\n\t#\t\tif os.sep in out:\n\t#\t\tout = out.split(os.sep)\n\t#\t\tfor c in out:\n\t#\t\t\tout = out[0] + c + \"\\n\"\n\t#\n\t#\t\tout = out.split(\"\\n\")\n\t#\n\t#\t\tfor c in out:\n\t#\t\t\tif c[0] == content[0]:\n\t#\t\t\t\tcontent["}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tcontent = markdown(content, output_format='xhtml11')\n\t\tcontent = re.sub(r'^w:st=(.*?)$', r'w:{st}', content)\n\t\tcontent = re.sub(r'^w:st=\"?\"', r'w:{st}', content)\n\t\tcontent = re.sub(r'^(w:{st})(?<!\\w){st}$'.format(st='_prefix'), r'|', content)\n\t\tcontent = re.sub(r'^(w:{st})(?<!\\w){st}$'.format(st='_prefix'), r'|',"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tif \".\" in content:\n\t\t\tstart, end = content.split(\".\")\n\t\t\tif start.lower() == \"w:st\":\n\t\t\t\t# w:text-block(s)\n\t\t\t\tst_namespace = end[1]\n\t\t\t\tif not st_namespace in w_namespaces:\n\t\t\t\t\tw_namespaces[st_namespace] = {}\n\t\t\t\tw_namespaces[st_namespace][end[1:]] = content\n\t\t\telse:\n\t\t\t\tm = [\"w:st namespace missing for namespace '{}' \"\n\t\t\t\t\t\"because of conflicting prefixes to '{}'\"\n\t\t\t\t\t\" in prefix '{}' \".format(st"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tm = _re_namespace_prefix_w.match(content)\n\t\tif m is None:\n\t\t\treturn m.group(1)\n\t\telse:\n\t\t\treturn m.group(1)\n\t\t\n\tdef text_to_indent(self, markup):\n\t\t\"\"\"\t\n\t\tConvert text with a specified indentation level.\n\t\t\"\"\"\t\n\t\t# Note:\n\t\t#  - you may want to change this when we have more than one new sentence in a document\n\t\t#  - you may set N_INDENT_SIZE.\n\t\t#  - the content of this is then"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\tres = ''\n\t\tres += content.replace('w:', '')\n\t\tfor char in u'\\w:st':\n\t\t\tres += char\n\t\treturn u''.join(res.split())\n\ndef strip_namespace_prefix(content):\n\t\"\"\"\n\tStrip namespace prefix from ``content``\n\tIf no namespace prefix is specified, ``content`` will be removed.\n\t\"\"\"\n\t\tres = remove_prefix_prefix(content)\n\t\treturn u''\n\n@contextmanager\ndef temp_prefix_dir(ns):\n\t\"\"\"\n\tContext manager to manage temporary files under ``"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\n\tif content.find('w:st') >= 0:\n\t\tcontent = content.replace('w:st=\"', 'w:st=' + content[:content.find('w:st')])\n\treturn remove_namespace_prefix(content)\nfrom django.http import HttpResponse\nfrom django.http import HttpResponseRedirect\nfrom django.template import RequestContext, loader, Context\nfrom django.conf import settings\nfrom django.shortcuts import redirect\nfrom django.db.models.signals import post_save\nfrom django.core.urlresolvers import reverse\nfrom django.contrib.auth.decorators import login"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tif \"newest\" in pubdate_xpaths:\n\t  return match_pubdate(node, pubdate_xpaths[\"newest\"])\n\telse:\n\t  return None\n\n\ndef process_author(node):\n\t   if \"first\" in node: \n\t\t     return \"first author\"\n\t   return None\n\ndef process_title(node):\n         if \"first\" in node: return \"first title\"\n\t   return None\n\ndef match_title(node, text):\n       return process_title(node, text)\n\ndef match_contributor(node, text):\n\n#\t\tif"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tfor t in regex_pubdate_xpaths:\n\t\ttry:\n\t\t\tmatch = t.search(node_pattern % pubdate_xpaths)\n\t\texcept AttributeError:\n\t\t\tcontinue\n\t\t\n\t\tif not match:\n\t\t\tt.close()\n\t\t\tcontinue;\t\t\n\t\t\n\t\tif match.group(1) == pubdate_xpaths[-1]:\n\t\t\treturn t\n\t\telse:\n\t\t\treturn None;\t\t\n\ndef clean_dir(path):\n\t\"\"\"\n\tClean path if is absolute\n\t@param path: the root path\n\t@retval: the cleaned path"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\ttitle     = \"Match\"\n\tif node is None:\n\t\ttitle    = \"\"\n\tif 'pubdate-date' in node.attrib:\n\t\tpubdate = parseXML(node.attrib['pubdate-date'])\n\t\ttoday = str(strptime(pubdate.xpath('/date').text, '%Y-%m-%dT%H:%M:%SZ'))[-1]\n\t\ttoday += str(today.replace('-', ''))\n\t\ttitle += \" [%s]\" % today\n\tif 'pubdate' in node.attrib:\n\t\treturn parseXML(node.attrib['pubdate'])['publication-"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tmatch = None\n\t\tlast = 0\n\t\tpublist = [node.find(pubdate_xpaths[last_offset:])]\n\t\tfor node in nodetext.split('\\n'):\n\t\t\tm = _RE_PUB_DATE.match(node)\n\t\t\tif m:\n\t\t\t\tpubdate_xpaths[last_offset:last] = [m]\n\t\t\t\tlast += 1\n\t\t\telse:\n\t\t\t\tpubdate_xpaths[last_offset:last] = [node]\n\t\t\t\tmatch = None\n\t\t\tlast += 1\n\t\treturn match\n\telse:\n\t\treturn"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\tmatch = None\n\t\t# Parse each node\n\t\txpaths = xpath(node)\n\t\tfor xpath in xpaths:\n\t\t\t# Compare with xpath\n\t\t\tmatch = match_pubdate(xpath, pubdate_xpaths)\n\t\t\tif match:\n\t\t\t\treturn match\n\n\t\t# No match, try in reverse order\n\t\tfor i, dirname in enumerate(xpaths):\n\t\t\t# If last item in a xpath exists, compare with that\n\t\t\tif dirname==dirname_re_xpaths:\n\t\t\t\treturn match\n\t\t\t# Add one to match\n\t\t\txpaths.append"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\t# get the first match for the publication date\n\tmatch_xpaths = [ p for p in pubdate_xpaths if p.date == node.date ]\n\n\t# now check each match as the first found match\n\tfor match in match_xpaths:\n\t\tif node.location == match.location:\n\t\t\treturn match\n\n# called when we have the year for the article\ndef year_match(node, yearyear):\n\tmatch_type = 'year'\n\tif node.date == node.date:\n\t\tmatch_type = 'first-match'"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tmatches = search.get_author_pubdates_for_node(node)\n\t\n\tif matches is not None:\n\t\treturn matches[0]\n\t\n\treturn None\n\ndef get_date_from_pubdate(node, pubdate):\n\t\"\"\"\n\tReturns the first match in the pubdate_list from the node\n\t\"\"\"\n\t\n\tif not isinstance(node, Node):\n\t\treturn None\n\t\n\tif str(pubdate)[:5] in pubdate_xpaths:\n\t\treturn pubdate_xpaths[str(pubdate)[:5]]\n\t\n\treturn None"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\treturn node.id[0] if node.id else None\n\ndef match_link(node, link_xpaths):\n\t\"\"\"\n\tReturns the text match of link\n\t\"\"\"\n\t\treturn node.id[0] if node.id else None\n\ndef parse_tag(html):\n\t\"\"\"\n\tGiven html, return a dictionary of name -> (name, tag)\n\t\"\"\"\n\t\ttree = xml.dom.minidom.Document()\n\t\troot = tree.childNodes.getchildren()\n\t\tnames = dict()\n\t\tfor node in root:\n\t\t\tif node"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\t# Get all the match\n\tmatches = soup.find_all('a')\n\t# Get only the date in first match\n\tdates = matches[0].find_all('dd').find_all('span')\n\t\n\t# First match in each match\n\tdates = dates[0][0].replace('January ', '').replace('February ', '').replace('March ', '').replace('April ', '').replace('May ', '').replace('Jun ', '').replace('July ', '').replace('August ', '').replace('September ', '').replace('October ', '').replace('"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\treturn node.findall('pubdate')[0].text\n\n\n######################################################################\n# main\n######################################################################\nif __name__ == \"__main__\":\n\t\n\t## parsing usage\n\tusage = \"\"\"\n\tUsage:\n\t ./build/simple.py -s <sourcename> -d <directory> -p <prefix>-l <linkname-1>\n\t      or\n\t ./build/simple.py -s --addtodir <newdir> --addtoprefix <prefix>-l --addtolink <newlink> --addtolink.log\n\t      or\n\t ./build/simple.py -"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tnumbers = issue.contents\n\t\tsuppl = issue.contents.text\n\ndef _extract_date_and_title(issue):\n\t# Extract the first non numerical date and the remaining titles.\n\ttry:\n\t\tdates = _extract_date_list(issue, '.*')\n\texcept:\n\t\treturn None\n\t# Extract the first numerical issue title.\n\ttry:\n\t\ttitles = _extract_title_list(issue, '.*')\n\texcept:\n\t\treturn None\n\n\treturn dates[0], titles[0]\n\ndef _extract_timestamp"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tnumber = None\n\t\tsupplment = []\n\t\tfor field in issue.text:\n\t\t\tif field == \"Original Insertion Number\" or field == \"Reference Name\":\n\t\t\t\tnumber = field\n\t\t\telse:\n\t\t\t\tcontinue\n\t\t\tsupplment = field.split(' ')\n\t\t\tif len(supplment) == 1:\n\t\t\t\tnumber = supplment[0]\n\t\t\t\tsupplment = []\n\t\t\telse:\n\t\t\t\tsupplment.insert(0, number)\n\t\t\tsupplment.append('1/12')\n\t\t\tnumber = None\n\t\treturn number, supplment\n\n\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tissue_title = issue.title.text\n\t\tissue_url = issue.url\n\n\t\ttry:\n\t\t\tnumber_value = int(issue.xpath(\n\t\t\t\t'issue-item/p[@id=$id]//p[@href]')[0])\n\t\t\tsuppl = int(issue.xpath(\n\t\t\t\t'issue-item/span[@class=\"suppl\"]')[0])\n\t\texcept:\n\t\t\traise ValueError('Could not determine number of value of'+ issue_title)\n\n\t\treturn issue_number_value, sprokit_suppl or None, sprokit_issue_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\t# The numeric identifier of the issue\n\t\tissue_identifier = _get_xml_attribute(issue, 'issue-identifier', None)\n\n\t\t# The integer of the issue\n\t\tissue_number = _get_xml_attribute(issue, \"issue-number\", None)\n\n\t\t# The total number of the issue\n\t\tnumber_and_suppl = issue['num-suppl']\n\n\t\t# The optional unit if applicable\n\t\tsuppl_unit = _get_xml_attribute(issue, \"suppl-unit\", \"\")\n\n\t\treturn (number_and_suppl, sf_unit, sf_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n# TODO:\n# Add the following as an optimization\n# - Find the number from the series\n# - Try the following one (with a list of numbers)\n#        1. Split on [ ] and remove [ ]\n#        2. Search each individual and find each number\n#        3. Do it. This is the way it finds the number and suppl and the\n#            number of issues.\ndef _perform_opensearch_issue(doc):\n\t# Parse the issue list\n\tissue_list = xml.find('issue')\n\tnum_issues = len("}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n# Author: Gabriel\n# Date:   Oct  5 18:23:50 GMT\n\nif __name__ == '__main__':\n\tissues_list = read_issue_xml(\".//issue/issueList.xml\")\n\n\tfor issue in issues_list:\n\t\tfor entry in issue.find_all(\"sub-information/issue\"):\n\t\t\tissue.getparent().writexml(entry)\n\n\"\"\"\nSupport for Verisure Cloud Water Level Air (Depth Sensor) devices.\n\nFor more details about this platform, please refer to the documentation at\nhttps://"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tissue_number = None\n\t\tif issue.text and issue.text not in get_numbers(issue.text):\n\t\t\tif issue.text!= 'issue title':\n\t\t\t\tissue_number = get_number_from_issue_text(get_issue_text(issue.text))\n\t\t\tif issue.text +'(' + issue.text + ')' in issue.text:\n\t\t\t\tissue_number = issue_number + ', '\n\t\tif issue_number:\n\t\t\tif issue_number not in get_numbers(issue_number.text):\n\t\t\t\treturn '', issue_number\n\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\tcontents = issue.find(fixxpath(r'//div[contains(@class,\"issue-number\") and contains(@id,\"number\")]/div//cite'),\n\t\t\t\t\t\t\tfixxpath(r'//html/body/td[contains(@src,\"issue-number\")]//text()),\n\t\t\t\t\t\t\tfixxpath(r'/div[@class=\"issue-number\"]//a/text()',\n\t\t\t\t\t\t\t\tfixxpath(r'//@class=\"issue-comment\"')\n\t\t\t\t\t\t\t)\n\t\ttry:\n\t\t\tnum = int(contents.get('value'), 16)\n\t\texcept (ValueError, AssertionError, IndexError):\n\t\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\ndef issue_number_and_supplment(line):\n    \"\"\"\n    Extract the possible values for number and suppl from the contents of line.\n    Return (number_and_supplment, suppl, number_of_copies)\n    \"\"\"\n    line = line.strip('\\n\\r')\n    number, suppl, number_of_copies = line.split()\n    return (number, suppl, number_of_copies)\n\n\ndef handle_protein_output_to_dict(dict):\n    \"\"\"\n    Function used to extract information from protein output.\n\tThe dictionary has a"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t\n\ttry:\n\t\t# If the issue has already been extracted, remove everything and return.\n\t\tissue = int(issue)\n\t\tnumber = int(re.findall('^([0-9]+)[A-Z0-9\\-]\", issue)[0])\n\t\tsuppl = re.findall('^([0-9]+)[A-Z0-9\\-]\", issue)[0]\n\t\tnumber = number + suppl[0] + suppl[1]\n\t\tfor num, value in enumerate(issue.split()):\n\t\t\tif str(number)+'_sub' in str(value):"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\treturn pprint.pformat(self.content, stream = indent, sort_keys = True, indent_chars = \"    \", indent = indent)\n\ndef to_json(obj):\n\t\"\"\"\n\tConverts a Json object to a Python object used by the JSONSerializer to json serializable entities.\n\tThis includes formatting fields needed to serialize json output.\n\n\t:type obj: Json\n\t:returns: A converted Json Object\n\t\"\"\"\n\t#print(\"to_json: obj: \", obj)\n\treturn json.dumps(obj, indent = 2)\n\n\n\ndef load_"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tself._print(indent=indent)\n\t\twith indent_lines(indent+1):\n\t\t\treturn self._indent_line(\"  \", indent+1, self.__class__.__name__)\n\n\t# Override some methods to add a copy of any additional methods\n\t# to the method that will be called.  Also, add some special functions\n\t# that return their own output.  For example, if you modify an existing\n\t# version, you would write::\n\n\t\tdef _log(self, msg, level=\"  \", flush=True):\n\t\t\tself._log_func(msg,"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tdef _text(*args, **kwargs):\n\t\t\toutput = unicode.__new__(type(self))\n\t\t\toutput.write(_text_argstring%args)\n\t\t\tif indent:\n\t\t\t\toutput.write(indent+_text(_text_argstring%args))\n\t\t\tif debug:\n\t\t\t\toutput.write(_format_argstring%args)\n\t\t\treturn output.getvalue()\n\n\t\treturn _text\n\telse:\n\t\tpretty = pretty_to_printer(__file__, _text,_pretty, indent)\n\ndef stringFormatter(name, *args):\n\t\"\"\"\n\tReturn"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tout = ''\n\t\tindent = self._indent()\n\t\tout += (indent * \" \"*indent).join(self.__dict__.keys()) + \" \"\n\t\t\t\n\t\tindent = self._indent()\n\t\tspace = self.indentSep\n\t\tline = self._line()\n\t\tfor key in list(self.__dict__.keys()):\n\t\t\tindent = out + (' '*indent).join(key.split())\n\t\t\tline = indent + (' '*indent).join(val)\n\t\t\n\t\tout = indent + line\n\t\tif debug:\n\t\t\tout += indent\n\t\n\tdef __str__(self):\n\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\tout = []\n\tout.append(\"<%s for %s>\" % (indent * self.indent, self.__class__))\n\t\n\tfor k in dir(self):\n\t    v = getattr(self, k)\n\t    if v is not None:\n\t\tout.append(\"  %-30s : %s\" % (k.title(), v()))\n\tif debug: out.append(\"  >\")\n\treturn \"\\n\".join(out)\n\nclass CppLexical(CppLexical) :\n    \"\"\"\n    A class that manages an \"Lexical\" data structure"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tif hasattr(self, \"name\"):\n\t\t\treturn self.name\n\t\treturn \"object %r\\n\" % self\n\n\tdef asList(self, inList, level=0):\n\t\t\"\"\"\n\t\tMake a nicely formatted copy of self.\n\n\t\tLevel arguments can be used to control which object to output. If there are\n\t\tmore than one level of nesting you will not output the same tree, so\n\t\tto return just a string with all level levels of nesting (0, 10,\n\t\t2, or 3). If there are no level of nesting you will get the same"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\ts = PrettyPrinter(indent, debug).pformat(self)\n\t\treturn s\n\n\t#------------------------------------------------------------------- #\n\t#\n\t#  Getter/Setter functions\n\t#\n\n\t#-------------------------------------------------------------------\n\tdef add(self, other):\n\t# Check if self is not a self and if so, return the value.\n\t# If not, return '' if self and other are the same.\n\tif type(self)==type(right):\n\t\tif self!=right:\n\t\t\treturn 'right'\n\t\treturn 'right'\n\t\n\t# Check if other is a proper type for this"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\n\tout = \"\"\n\tif debug:\n\t\tout += \"<indent: \" + str(indent) + \">\"\n\tout += (\"   \" + str(self))\n\tif debug:\n\t\tout += \"<indent: \" + str(indent) + \">\"\n\tout += (indent * \"   \")\t\t\t\n\t\t\n\tif self.sort_value:\n\t\tout += \"   \" + str(self)\n\t\t\n\tif isinstance(self.value, basestring):\n\t\tout += \"    \" + self.value\n\t\t\n\tif isinstance(self.value, list):\n\t\tout += \""}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tlines = []\n\t\tfor chunk in self.text:\n\t\t\tlines.append(indent*indent+chunk)\n\t\treturn '\\n'.join(lines)\n\nclass SimpleParser(Parser):\n    _parse_error = ValueError(\"Invalid input\")\n\n    def __init__(self, input=io.BytesIO()):\n        self.reset()\n\n        super(SimpleParser, self).__init__(input)\n\n    def reset(self):\n        self._stack = [None, None]\n\n    def parse(self):\n        return ''.join(self._stack)\n\n    def reset_lineno(self):\n        self._stack ="}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t\tif debug or not self.is_terminal():\n\t\t\treturn \"%s%s(%s)\" % (indent*'..', self.__class__.__name__,\n\t\t\t\tself.id)\n\t\telse:\n\t\t\treturn repr(self)\n\nclass ScaledStrFormatter(ScalarFormatter):\n\t\"\"\"\n\tA subclass of :class:`ScalarFormatter` that replaces its value\n\twith a pretty representation.\n\t\"\"\"\n\tformatters = dict(\n\t\tname = pretty,\n\t\tid = id,\n\t\tcolor = r'rgb(%(start)s,(%(end)s)+%(start)"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\tif len(args) == 2 and (not args[1].is_Symbol):\n\t\t\t pass\n\t\telif len(args) == 1:\n\t\t\t absp = args[0]\n\t\t\t if absp.is_Symbol:\n\t\t\t\tabsp = absp.subs(arg for arg in absp.args if (not arg.is_Symbol))\n\t\telse:\n\t\t\t absp = args \n\t\t\t\tabsp = absp.subs(arg for arg in args if arg.is_Symbol)\n\t\t\t\n\t\tabsorbed = {arg:1 for arg in absp if not arg & arg<"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t        return [arg.absorb() for arg in args]\n\t\n\tdef plus(args):\n\t    return [arg.plus() for arg in args]\n\t\n\tdef minus(args):\n\t    return [arg.minus() for arg in args]\n\t\n\tdef power(args):\n\t    return [arg.power() for arg in args]\n\t\n\tdef factorial(args):\n\t    return [arg.factorial() for arg in args]\n\t\n\tdef power_with_absorbance(args):\n\t    aborbance = args[0].lower()\n\t    if aborbance.startswith(\"ab\"): \n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\t#print(args)\n\t\t\n\t\targs = [arg for arg in args if arg in self.absorb]\n\t\t\n\t\t#print(self)\n\t\treturn args\n\n\tdef __call__(self, *args):\n\t\targs = self.absorb(*args)\n\t\t\n\t\t#print(self, args)\n\t\treturn (args[0]) | args[1]\n\n\tdef __repr__(self):\n\t\treturn \"absorb()\"\n\n###################################################################################\n\nclass UnbsorbError(Exception):\n\tdef __init__(self, message):\n\t\tself.message ="}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\n\t    ### Make A * -1\n\t    A = [i.apply_operator(self._negate) for i in args]\n\t    ### Make B * -1\n\t    B = [-1.0 for i in args]\n\t    ### And C = A, C = B\n\t    A = sum([i.apply_operator(self._absorb) for i in A])\n\t    ### And D = B, C = A\n\t    \t ### Get D from A\n\n\t    \t### Make B & C = 0\n\t    B = [B[i] if i else i for i in A]\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t     \n\t    \n\n\t    \t\n\n\t    \n\t        # Compute absorption/negative absorption\n\t        AB = args[1]\n\t        for i in range(1, len(args)):\n\t        if 1 <= i <= len(args):\n\t            if A <= args[i]:\n\t\t                AB = args[i]\n\t\t\t   \n\t \t                if absorb(AB):\n\t                AB = (AB - 1) * A\n\t\t\n\t        # Compute negative absorption\n\t        AB = args[2]\n\t        for i in range(1, len(args)):\n\t        if 1 <= i <= len"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\t\n\t\n\n\tif args[0] is None:\n\t    return [self]\n\telse:\n\t    return [self, args[0]]\n\n \ndef absorption(self, a_min, a_max):\n\t\"\"\"\n\tGiven two expressions, return absorption and positive absorption.\n\tUsage: (a | b) = -a + b\n\t\n\tReturns the absorption and positive absorption.\n\t\"\"\"\n\t\n\n\tif self == a_min:\n\t    return (-a_min)\n\telif self == a_max:\n\t    return (-a_max"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t    \n\t\n\tif len(args) == 1:\n\t\treturn self.apply(args[0])\n\treturn self.apply(args[0].absorb(args[1:]))\n        \n\n\tdef solve(self, expr):\n\t\t\"\"\"\n\tGiven an expression, return the solution of the form\n\t\tabsorption with negative absorption, negative absorption, and positive absorption\n\t\t\n\t\tThis assumes there is a *square* in its domain of domain B to be\n\t\tadjacent and the absorption is at the boundary of the domain B.\n\t\t\n\t\tYou must call"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\nclass Interpolation(object):\n    \"\"\"\n    Interpolation algorithm.\n\n    This class implements the interpolation algorithm, which is a helper class\n    for :class:`Adafruit_GPIO.inter` and :mod:`Adafruit_GPIO`.\n\n    See the `inter` section for more information.\n\n   .. autoclass:: inter\n\n   .. automethod:: parse\n\n    \"\"\"\n    \n    def __init__(self, **kwargs):\n        self._parse = None\n        self._is_valid = None\n        self._is_negative_absorbed = None\n        self._is_interpolated = None\n        self._is_"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\t    return [A(arg) for arg in args] + [B(-arg) for arg in args if arg!= arg]\n\n\tdef invert(self, args):\n\t\"Invert the arguments.\"\n\tif self.inversion in (1.0, 2.0):\n\t    return [A(~a) for a in args] + [B(~a) for a in args]\n\telse:\n\t    A = args[0][0]\n\t    B = args[2][0]\n\t    C = args[1][0]\n\t    return A.invert(B, C)\n\n\tdef invert"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\ndef invertAB(self, *args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying inverse absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Inverse_absorption\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A\n\t    = A\n\t    = A & (~B | B) = A\n\t    = A & (~B & B) = ~B\n\t\n\tNegative absorption::\n\t\n\t    A & (~B | A) = A &"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\treturn hook.on(self)\n\n\nclass HookType(object):\n\t\n\t\"\"\"\n\tType of Hooks.\n\t\n\t:param name: Name of the hook.\n\t:type name: str\n\t:param hooks: Map of (func, method, parameters, help) tuples.\n\t:type hooks: dict\n\t:return: Map of (func, method, parameters, help) tuples, where func is the\n\t    new function in place of the registered method.\n\t:rtype: dict\n\t\"\"\"\n\t\n\tdef __init__(self, name, hooks):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\tdef decorator(func):\n\t\treturn lambda key: None\n\treturn decorator\n\t\t\n\t@on\n\tdef add_handler(self, hook, handler):\n\t\t\"\"\" Adds an existing handler to the registry.\n\t\t\n\t\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\t\t\n\t\tReturns:\n\t    Callable: Modifies the specified hook to the registry.\n\t\t\"\"\"\n\t    \n\t@on\n\tdef remove_handler(self, hook, hook_instance):\n\t\t\n\t\n\t@on\n\tdef deregister_handler(self, hook):\n\t\t\"\"\" De"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\tdef wrapper():\n\t    \traise NotImplementedError('should be implemented in Manager.register_event()')\n\treturn wrapper\n\treturn wrapper\n\nclass Manager:\n\t\"\"\"\n\tManager class for creating and saving Event objects.\n\t\n\tManagers:\n\t * _event: the event object to save with the event's name as it will\n\t\t         be stored.\n\t\t* _db: the database object that manages the event. This is a\n\t\t         dictionary that manages attributes for a particular\n\t\t         object.\n\t\"\"\"\n\t\n\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\""}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\tif self.handler is None: raise Exception(\"No handler registered for %s\" % hook)\n\t\tself.handler = hook\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.handler is not None:\n\t\t\thook = getattr(self.handler, self.hook_name)\n\t\t\treturn functools.wraps(hook)\n\n\t\traise TypeError, \"Hook type %s not found\" % self.hook_name\n\n\tdef __call__(self, f):\n\t\tself.register(f)\n\t\treturn f\n\n\n## Register the dispatcher in the factory\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n        def hook_register_handler(handler_type, hook_attribute):\n            if hook_attribute == hook:\n                hook.register_listener(hook_type, hook_attribute)\n\t\t\t\n        return hook_register_handler\n                \n\n# #START_LICENSE###########################################################\n#\n# This file is part of the Environment for Tree Exploration program\n# (ETE).  http://ete_bimstain.eu/page_result/showed/wont/the_obsolete_building_theory\n# \n# ETE is distributed under the GNU General Public\n# License as"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \n\tdef on_new_app(app, registry):\n\t\tapp.logger.debug(\"_Registering listeners for \" + app.name)\n\t\tfor handler in registry:\n\t\t\tif handler not in self._handlers:\n\t\t\t\tself.dispatch(handler, None)\n\t\treturn handler\n\t\n\treturn on_new_app\n\t  \ndef unregister(application):\n\t\"\"\"\n\tUnregister the appropriate handler registered on `application`.\n\t\n\tArgs:\n\t    application (Application): App object to unregister.\n\t\n\tReturns:\n\t    None\n\t\n\tRaises:\n\t    TypeError : When one of `Application` subclass of"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\tdef on_event(self, event):\n\t\t\tself.on_update(event)\n\t\treturn on_event\n\n\tdef on_update(self, event):\n\t\t# Get the event to add to the registry, and register it\n\t\tevent.add_callback(self)\n\t\tevent.execute()\n\n\tdef on_delete(self, event):\n\t\t\"\"\"\n\t\tClean up the event list and any listeners on that event removed.\n\t\t\n\t\tArgs:\n\t\t\tevent (Event): Event to clean up the event list and any listeners.\n\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\""}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\tfrom...common import PluginInterfacePlugin, Plugin\n\n\tclass HandlerHookType(PluginInterfacePlugin):\n\t\t\"\"\"\n\t\tClass that registers the \"hook\" type for all plugins in this\n\t\tplugin type.\n\t\t\"\"\"\n\n\t\tdef on_load(self, plugin):\n\t\t\tself._onRegister(hook, None, self.PluginOptions)\n\t\t\n\t\tdef on_unload(self, plugin):\n\t\t\tself._onUnregister(hook, None, self.PluginOptions)\n\t\t\n\t\tdef on_option(self, _, option):\n\t\t\tself._onRegister(hook, None, self.Plugin"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \treturn hook.on(self, hook)\n\n\tdef listen(self, listener):\n\t\t\"\"\"\n\t\tRegisters listener for the specified listener.\n\n\t\tArgs:\n\t    listener (ListenerType): Handler to be registered.\n\t\t\n\t\tReturns:\n\t    callable: Decorator for registered listeners.\n\t\tRaises ValueError if the event handler already registered.\n\t\t\"\"\"\n\n\t    \tif not self.dispatcher:\n\t\t\traise ValueError(\"Listener type not registered\")\n\n\t    \tself.dispatcher.on(self.dispatcher.addListener(self,listener))\n\n\t\treturn listener\n\n\nclass EventDispatcher:\n\t\"\"\"\n\tA"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t    \tself._registry[hook.name.lower()] = self\n\t\n\t@classmethod\n\tdef unregister(cls, name):\n\t\tif name not in cls._registry:\n\t\t\tHookType.add(name)\n\t\t\tHookRegistry.register(name)\n\t\thook = cls._registry[name]\n\t\tHookRegistry.remove(hook)\n\t\tHookRegistry.on(hook)\n\t\t\n\tdef __call__(self, hook):\n\t\ttry:\n\t\t\tself._registry[hook.name.lower()] = super(HookRegistry, self).__call__(hook)\n\t\texcept KeyError as error:"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t \n\tprint \"Creating config...\"\n\tconfig = {\n\t  \"username\": \"admin\",\n\t  \"password\": \"admin\",\n\t  \"connection_timeout\": \"5\",\n\t  \"database\": \"production\",\n\t  \"port\": \"2181\",\n\t  \"database_host\": \"localhost\",\n\t  \"database_port\": \"2379\",\n\t  \"database_user\": user,\n\t}\n\t\n\t# Set up the database if none has been supplied\n\tif config.has_key(\"database\"):\n\t\tconfig[\"database\"] = config[\"database\"][:]\n\t\n\t# Setup the etcd_port if not supplied"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\tconfig = dict(host=user, port=etcd_port)\n\t\n\tdefault_config_json = base_config(**config)\n\treturn config\n\ndef default_config(data):\n\t\"\"\"\n\tCreates a default configuration instance of a parameter dictionary.\n\t\n\tArgs:\n\t    data (str or dict):  data to populate a parameter dictionary object with\n\t\n\tReturns:\n\t    dict: an instance of type dict.\n\t\"\"\"\n\t\n\tif type(data) is str:\n\t\t\n\t\t\ttry:\n\t\t\t\n\t\t\t\tdata = json.dumps(data, ensure_ascii=False)"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tconfig = {}\n\t\n\t\n\t# add in the default values\n\tdefaults = {\n\t\t\t'user': user,\n\t\t\t'tername': 'Dark Tape',\n\t\t\t'startpename': 'Amazon',\n\t\t\t'sharekey': os.environ['ETCD_KEY'],\n\t\t\t'domainname': 'Mr.'+etcd_host,\n\t\t\t'regexp': '(.+?)([\\S\\-\\.]+@(.+?\\.))?(\\s+)?@.*/',\n\t\t\t'timestamp': str(datetime.datetime.now().strftime(\"%Y\"))}\n\n\t\t# config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tdefaults = {\"user\": user, \"etcd_host\": etcd_host, \"_etcd_port\": etcd_port}\n\t\n\t\n\treturn config.Config(defaults)\n\ndef log_config(title, message):\n\tdef log_func(x):\n\t\tprint(\"%s\"%x, end=\"\\r\")\n\t\tlogging.debug(message, end=\"\\r\")\n\t\n\treturn config.Config(log_func, title, message)\n\ndef build_base_config(defaults):\n\t\"\"\"\n\tCreates a base configuration with some simple parameters, which have a '"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\tcfg = {\n\t\t\t\"debugMsgSize\":10,\n\t\t\t\"logLevel\":logging.DEBUG,\n\t\t\t\"logLevel2\":logging.INFO,\n\t\t\t\"logLevel3\":logging.INFO,\n\t\t\t\"logLevel4\":logging.INFO,\n\t\t\t\"logLevel5\":logging.INFO,\n\t\t\t\"logLevel6\":logging.ERROR,\n\t\t\t\"logLevel7\":logging.ERROR,\n\t\t\t\"logLevel8\":logging.ERROR,\n\t\t\t\"logLevel9\":logging.ERROR,\n\t\t\t\"logLevel10\":logging.ERROR,\n\t\t\t\"logLevel"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\treturn {}\n    \n    def get_default(self, key, value):\n        \"\"\"\n        Gets the value for a key 'value' in a dictionary.\n        \n        Args:\n\t    key (str): the name of the field to get the value for.\n\t    value(str): the value to look up.\n     \n        Returns:\n\t    value: the value associated with the key 'key' on the dictionary. This will be overwritten\n\t\t\t                with 'value'.\n \n        Examples:\n            s3_upload(\"/files\", \"/files/foo.txt\", \"text/plain\")    \n\t    print(s3_upload.__class__)    \n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\t# Initialization\n\tconfig = configparser.ConfigParser()\n\tconfig.read(os.path.join(\n\t\tos.path.dirname(os.path.realpath(__file__)), \"etcd_configs.ini\"))\n\t\n\t# Setup the TetCD server\n\tetcd = Tetcd(etcd_host, etcd_port)\n\n\t# Create a basic database, set this to a dictionary of defaults\n\tdb = etcd.get_database()\n\tdb.set_database('test')\n\t\n\tdb.admin.command('DROP DATABASE IF EXISTS public;')\n\tdb"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\t\n\tenv = os.environ\n\t\n\tenv.update({ \"TENANT_ID\": user, \"TENANT_NAME\": user, \"APPLICATION_ROOT\": environment[\"APPLICATION_ROOT\"].rstrip(\"/\"), \"AUTH_HOST\": etcd_host, \"TENANT_PORT\": etcd_port })\n\t\n\t# now create a config file\n\treturn load_user_config_file(user)\n\ndef get_user_config(user):\n\t\"\"\"\n\tFinds out all the configuration data for the given user by get a config object\n\tin the user's configuration file using the get_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\tconfig = {\n\t\t\t'user': user,\n\t\t\t'tecd_host': etcd_host,\n\t\t\t'etcd_port': etcd_port\n\t\t}\n\t\treturn config\n\n\ndef login_user(user, etcd_host, etcd_port):\n\t\"\"\"\n\tLog in using the given credentials in the given  user and  etcd host.\n\t\n\tArgs:\n\t    user (str): the login name to login to the user.\n\t    etcd_host (str): the etcd host to log in with.\n\t    etcd_port (int): the port"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t \treturn {\"user\" : user, \"etcd_host\" : etcd_host, \"etcd_port\" : etcd_port}\n\n\tdef get_local_config(self, user, etcd_host=\"localhost\", etcd_port=2379):\n\t\t\"\"\"\n\tRetrieves the default configuration in the database in the local database.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt == datetime.datetime.now():\n\t\treturn self._fromutc(dt)\n\t\n\treturn self._fromutc(dt.replace(tzinfo=None))\t\t\n\n\nclass _datetime_to_iso(self):\n\tdef _toutc(self, dt):\n\t\t\"\"\"\n\t\tReturn an :class:`datetime.datetime` instance with only the\n\t\tUTC timestamp as the argument to `datetime.mktime`\n\t\t\"\"\"\n\t\treturn dt.astimezone(LOCALEPO_UTC)\n    \n\nclass _datetime_to_tz(self):\n\tdef _toutc(self"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif not isinstance(dt, datetime):\n\t\t\traise TypeError(\"Expected a :class:`datetime`\")\n\t\n\t\tif dt == self.get_datetime():\n\t\t\treturn self\n\t\telif dt.utcoffset() is not None:\n\t\t\treturn _fromutc(DT.parse(dt - dt.utcoffset()))\n\t\telse:\n\t\t\treturn None\n\n\tdef _fromstr(self, tstr):\n\t\t\"\"\"\n\t\tTakes a string of time in a given timezone.\n\t\t\n\t\tSince we do not have the same timezone objects directly, the\n\t\tdatetime object will be returned, so for this purpose, we"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# The current timestamp, which is used to keep our old datetime\n\told_dt = datetime.datetime.now()\n\t\n\t# If it's ambiguous or \"fold\" it, use \"fold\"\n\tif (self.time[0] and not self.time[0].fold_in or \\\n\t   ((dt.date().year%100!= 0) and (dt.date().year>12))):\n\t\treturn dt.replace(tzinfo=UTC)\n\telse:\n\t\tdt = _fromutc(dt)\n\t\t# Now, apply the change\n\t\treturn dt."}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tt = dt.replace(pytz.timezone('UTC'), tzinfo=pytz.utc)\n\t\n\t\treturn tz.localize(t).astimezone(pytz.timezone('UTC'))\n\t\n\tdef apply(self, context):\n\t\tif not context:\n\t\t\treturn\n\t\tcontext = _fromutc(context)\n\t\n\t\tif getattr(self.dt, 'tzinfo', None) == context.dt.tzinfo:\n\t\t\treturn self.dt\n\t\t\n\t\treturn context.apply(context)\n\t\n\tdef __format__(self, format_):\n\t\tself.apply(format_)\n\t\treturn frappe._format_datetime("}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt is None:\n\t\t\treturn self.new_tzinfo()\n\t\t\t\n\t\t# If the datetime is on an ambiguous timezone, that is the\n\t\t# start of the given time zone.\n\t\tif dt.tzinfo is None or (dt.tzinfo.utcoffset(self.datetime) > (self.datetime - dt).tzinfo.utcoffset(DTSTimeZone())):\n\t\t\tself.new_tzinfo = self.new_tzinfo.replace(tzinfo=self.datetime)\n\t\t\treturn self.new_tzinfo\n\t\n@implementer(ISBN)\nclass NullTimezone(tzinfo):\n\t\"\"\"A"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tstart = dt.utcoffset()\n\tend = dt.dst()\n\tif ( end is False ):\n\t\tstart = dt.timetuple()\n\t\n\tutc = datetime.datetime( dt.year,dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond )\n\tiso8601 = pytz.timezone('Europe/Helsinki')\n\tunixtime = UnixTime(start, utc=utc)\n\treturn iso8601.localize(unixtime)\n\ndef _fromutcdatetime(dt):\n\t\"\"\"\n\tGiven a user-"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzinfo is not None:\n\t    return dt + _fromutc(dt.tzinfo)\t\n\n\tif _isnull(dt):\n\t    return self._default_fromutc(dt)\n\t\n    def _default_fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates the\n\ttimezone-aware datetime in a new timezone.\n    \n    :param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n    def _fromunicode(self, string, encoding):\n    \t\"\"\"\n\tGiven an"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n# If the given timezone is a \"fold\" state, we do not need to check for\n# the ambiguous datetime's validity. That would be pretty bad for time\n# as an ambiguous datetime would result in a naive representation\n# of an ambiguous datetime. So instead of doing this, we also use a very\n# strict check to guard against ambiguity for a time period, but not for\n# time-zones.\n\tif dt.tzinfo is not None:\n#\t\t_fromutc = _tzinfo(_fromutc(dt.astimezone(_tzinfo(None)))).datetime\n#\t\tif"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# Determine the first date / time pair in the \"fold\" state,\n\t# and see if it's in the end date range\n\tfold = self.fold_state & (dt.date()+dt.timedelta())\n\t\n\t# Now we take the closest time.\n\tclosest = dt - self.start_dt\n\t\n\t# If we can't find a close at this point, then we fall through\n\t# on the next day\n\tif closest == dt:\n\t\treturn dt\n\t\n\t# Then the time-delta will be normalized and normalized\n\t# (and possibly normalized) in"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\n\t# Convert the string, returning the timezone.\n\tts = datetime.datetime.fromtimestamp(float(dt, 16)):\n\n\t\n\t# Get the timezone.\n\ttzTime = tskDateTime.TskTime.getTimeTsk(self._dt)\n\t\n\tif ts!= tzTime.ts:\n\n\t\t# If we are at the end of every valid day (it is our start of an\n\t\t# hour), we can't know if we should keep a valid day. We do this in\n\t\t# order to avoid race conditions. If we are in an exact same time,\n\t\t"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n#\t<class'relativedelta_object' ','relative_attribute' >\n\t#       ... \t<attribute> = <class 'timedelta' />\n\t\ndef normalized_date(self):\n\t\n\t\"\"\"\n\t\tReturn a normalized version of this object represented entirely using\n        integer values for the relative attributes.\n\t\t\n\t\t>>> relativedelta(days=1.5, hours=2).normalized_date()\n\t\trelativedelta(days=+1, hours=+14)\n\t\t\n\t\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n#\t<class '"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\t# Note that you have to create a function that should always return\n\t\t# a timedelta or an object with a numeric value, as\n\t\t# this is a python function.\n\t\treturn self._to_datetime(self.delta).normalized()\n\t\n\tdef delta(self):\n\t    \"\"\"\n\tReturn a version of this object represented entirely using a\n\ttime object for which value is a :class:`datetime.timedelta`.\n\n\t    See :func:`~relativedelta.microseconds_per_second` for a usage \n\t    description of the amount of microseconds to perform.\n\n\t:return:\n\t    Returns a :class:`"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\tif (self.__class__ == relativedelta and\n\t\t\tself.__dict__.has_key('days')):\n\t\t\ttry:\n\t\t\t\treturn self.__dict__['days'].toordinal()\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\traise AttributeError(\"Class '%s' has no 'days' attribute! Use 'days' instead.\" % self.__class__)\n\n\t# __new__ is available in `dateutil.relativedelta.__new__`\n\tif not hasattr(relativedelta, '_new'):\n\t\trelativedelta._new = relativedelta._new_from_relativedelta\n\n\treturn relativedelta._new(self)\n\n\nclass timedelta"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t    \n\tdef normalized_to_seconds(self):\n\t\t\"\"\"\n\t\tReturn a version of this object represented entirely using seconds\n\t\tvalues for the relative attributes. The returned string is of the type specified by the `days` parameter.\n\t\t\n\t\t>>> relativedelta(days=1.5, hours=2, minutes=2).normalized_to_seconds()\n\t\t'14:32'\n\n\t\t:return:\n\t    Returns :class:`dateutil.relativedelta.relativedelta` object.\n\t\t\"\"\"\n\t    \n\tdef days_long(self):\n\t\t\"\"\"\n\t\tReturn the length and time of each weekday"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\treturn relativedelta(self.days, self.hours)\n\t\n\tdef from_date(self, date):\n\t\t\"\"\"\n\t\tReturn a date or a datetime from a datetime instance\n\t\tcontaining the given date.\n\t\t\n\t\tThis will generally only create and modify `from_datetime`,\n\t\twhich will change the timezone on demand.\n\t\t\n\t\t:param date:\n\t\t\tA datetime as returned by the ``get_date`` method of this object.\n\t\t\n\t\t:return:\n\t\t\tA :class:`dateutil.relativedelta.relativedelta` representation\n\t\t\"\"\"\n\t\treturn from_datetime(self, date)\n\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\tif self.days!= None:\n\t\t\treturn relativedelta(days=self.days)\n\t\telif not self.days:\n\t\t\treturn relativedelta(days=-1)\n\tdef __mul__(self, other):\n\t\tif isinstance(other, dateutil.relativedelta):\n\t\t\treturn NotImplemented\n\t\tif not isinstance(other, relativedelta):\n\t\t\traise TypeError(\"cannot multiply by a non-type\")\n\t\tif self.days:\n\t\t\treturn relativedelta(days=other.days)\n\t\telse:\n\t\t\treturn relativedelta(hours=+other.hours)\n\tdef __lt__(self, other):"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t    return self._normalized\n\t\n\tdef normalized_as_seconds(self):\n\t\"\"\"\n\tReturn the relative attributes of this object's \n\tspecific time (relative date in seconds) expressed in\n\tmicroseconds. If any of the current objects derive based \n\ton offset, returns None.\n\t\n\t.. attribute:: hours\n\t\tAn integer containing the seconds relative to be\n\t\taccessed by this object.\n\t:type hours: int\n\t:param hours: Length of the relative attribute, if multiple is \n\t\tspecified, the returned object has the \n        :meth:`normalized_as_seconds`"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\treturn datetime.timedelta( days = int(self.days) )\n\t\n\tdef __str__(self):\n\t\treturn self.localized()\n\t\n\tdef __len__(self):\n\t\treturn len(self.localized())\n\t\n\tdef __iter__(self):\n\t\tfor value in self.get_localized():\n\t\t\tyield value\n\t\traise StopIteration\n\nclass day(object):\n\tdef localized(self):\n\t\treturn self.timetuple().timetuple()\n\t\n\tdef __init__(self, year, month, day=None, hour=None, minute=None, second=None):\n\t\tself"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\treturn type(self)(relative=self)\n\t\n\t@classmethod\n\tdef normalized(cls, offset=None):\n\t\"\"\"\n\tReturn a version of this object represented entirely using float values\n\tfor the relative attributes.\n\t\n\t>>> relativedelta(hours=None,minutes=2).normalized()\n\trelativedelta(hours=+1,minutes=14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\n\t\tif offset is None:\n\t\t\toffset = dateutil.relativedelta.relativedelta()\n\n\t\toffset.__class__ = datetime.timedelta\n\t\t"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\treturn self._months\n\t@months.setter\n\tdef months(self, value):\n\t\tself._months = value\n\t@property\n\tdef years(self):\n\t\treturn map(int, filter(lambda y: 1<=y, self))\n\t@years.setter\n\tdef years(self, value):\n\t\tself._years = value\n\t@property\n\tdef month(self):\n\t\treturn map(int, filter(lambda y: y>31 and y<=21, self))\n\t@month.setter\n\tdef month(self, value):\n\t\t"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\nimport types\n\t\nclass TimezoneError(Exception): pass\nclass GMTError(Exception): pass\n\n\ndef format_timedelta(tdelta):\n    return \"{0} h:{1} M {2} s\".format(tdelta.days, tdelta.seconds, tdelta.microseconds)\n\n\nclass UnknownTimeZoneError(TimezoneError): pass\n\nclass TimezoneValueError(Exception): pass\n\nclass UnknownTimezoneError(TimezoneError): pass\n\nclass NotAFixedOffsetError(TimezoneError): pass\n\nclass UnresolvableNameError(Exception): pass\n\nclass InvalidTimezoneError(Exception"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n_new_tzname = \"\"\"Jun 27, 2006\n\n\tThis was a Julian Standard Weekday in Python 2. It has an option to\n\tallow you to specify a specific timezone, which in turn, means\n\ta full timezone. For example, assuming a US/Eastern America\n\twhere the FED format differs from what we expect (but you probably\n\twant one of these as \"Europe/Madrid\"). In this case you may want to\n\tuse the --force option before your program to modify the timezones.\n\t\n\tThe \"force\" option allows you to specify a specific"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef bdecode(s,default=None,encoding=\"ascii\"):\n\t\"\"\"\n\tDecodes unicode strings\n\t\"\"\"\n\tif not isinstance(s,basestring): s = s.decode(encoding)\n\treturn s\n\ndef decode(s, encoding=\"ascii\"):\n\t\"\"\"\n\tDecodes unicode strings\n\t\"\"\"\n\treturn unicodify(unicodify(s,default=None,encoding=encoding))\n\t\t\t\t\n\t\t\n\nclass Encoder:\n\tdef __init__(self, encoding=\"ascii\"):\n\t\tself.source=None\n\t\tself.sourceEncoding = encoding"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\tdef _checkunicode(s):\n\t\t\t\"\"\" Return a bool (bytes, bytes, etc...) where bytes.bytes == s and chars.bytes == s are the string representation of the string\n\t\t\t\t\"\"\"\n\t\t\ts = s and bytes(s, 'latin-1') or bytes('')\n\t\t\treturn chr(a) in s and s or s\n\t\t\n\t\n\tdef tzname_in_python3(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 3\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t    return (namefunc(bytes.lower())).decode('latin1')\n\telse:\n\t    return (namefunc('ascii'))(bytes.lower())\n\ndef utcfromtimestamp2localtime(seconds):\n\t\"\"\" convert UTC time value from seconds to localtime in local timezone\n\t\n\tutcfromtimestamp2localtime(seconds) -> localtime value \"\"\"\n\tif seconds!= 0:\n\t\treturn timesince(localtime(seconds))\n\telse:\n\t\treturn datetime.datetime.utcnow()\n\ndef localtime(datestr, localtime=tz.tzutc()):\n\t\"\"\" return time"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef to_str(name):\n\t\"\"\"\n\tReturn a unicode representation of a name, to make it easier to see it\n\twith argparse.  This seems to only be used for things that don't seem to match.\n\t\n\tTo have a valid way to make the output:\n\t  a) Convert name to unicode if it hasn't already been replaced by a single character, or to use a single character\n\t\t     (eg. \"ABC\") => \"ABC\"\n\t  b) Convert name to unicode if it has already been replaced by a single character, or to use a single character\n\t\t     (eg. \""}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef utcoffset_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\tutcoffset() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\ndef tzlocal_to_utc(tzname_tuple):\n    \"\"\"\n\tReturns datetime.tzinfo.utcfromtimestamp\n\n\ttimelib.tzname\n    \"\"\"\n\n\tresult = tzname_tuple[0]\n\tdst_hour = tzname_tuple[1] - (timelib.name2codepoint[result])\n\tdst_minute = tzname_"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\treturn namefunc.encode(pyio.strobject)\n#\n# Copyright 2016, Google Inc.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t # This will only work on 2-2 and 3-3 compatible Python 2\n\t if name is not None:\n\t\t # We cannot handle str objects.\n\t\t return namefunc(name)\n\t else:\n\t\t return namefunc\n\telse:\n\t # We need to coerce in Python 3 to bytes\n\t return bytes(name)\n\ndef to_python(namefunc=to_unicode, no_unicode=False):\n\t\"\"\"\n\tConverts a python string to a python string\n\t\n\t\tnamefunc is the name of the Python program\n\n\tReturns a Python string\n\t\"\"\"\n\t # The function we define the new Python"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\nclass Pickle:\n\t\n\tdef __init__(self):\n\t\tif sys.platform == 'win32':\n\t\t\tfile = open(os.path.join(os.getcwd(), 'pickle_2'), 'rb')\n\t\t\tdata = PickleWriter(file)\n\t\telif sys.platform == 'darwin':\n\t\t\tfile = open(os.path.join(os.getcwd(), 'pickle_2'), 'rb')\n\t\t\tdata = pickle.load(file)\n\t\telse:\n\t\t\tdata = pickle.load(open('pickle.cpickle'))\n\t\tself.data = data\n\t\n\t"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\n####################################################################\n####################################################################\ndef is_release():\n\t\"\"\"\n\tCheck if we need to make a release build.\n\t\"\"\"\n\tif False:\n\t\tlogger.debug(\"[*] Checking for release build.\")\n\t\treturn False\n\n\tif True:\n\t\tlogger.debug(\"[*] Creating release clean folder.\")\n\t\tos.makedirs(os.path.join(addondir,'release', 'clean'))\n\n\ttry:\n\t\tf = open(os.path.join(addondir,'release', 'clean','release'), 'w')\n\t\tf.close()\n\n"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\n@cachedMethod(cacheable=True, cache_timeout=60*24*30)\ndef find_version(query='ALL', limit=100):\n\t\"\"\"\n\tFind any documents matching specified query (case-insensitive) into a list.\n\t(This will return documents that meet specified criteria)\n\tLimit:\tNone to search all.\n\t\"\"\"\n\t\n\t# use cache for performance\n\tfrom config import config_settings\n\tfrom utils import normalize, get_cache_timeout\n\n\t# get doc id for each version\n\ttry:\n\t\tquery = normalize(query)"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\t_logger.debug(\"Opening get_versions() in the 'data_path')\")\n\t\ttry:\n\t\t\tf = io.open(self.data_path, \"r\", encoding=\"utf-8\")\n\t\t\tout = f.read()\n\t\t\tf.close()\n\t\t\tout = out.decode('utf-8')\n\t\texcept IOError, err:\n\t\t\t_logger.debug(str(err))\n\t\t\treturn None\n\n\t\t_logger.debug('get_versions() returning: %s' % out)\n\n\t\tversions = out.splitlines()\n\t\tif len(versions)"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t \n\tversion_file = get_env_var('PYMOLXS_VERSION')\n\tpackage_dir  = os.path.join(path.dirname(__file__), os.pardir, 'lib')\n\tname                        = get_env_var('PYMOLXS_NAME')\n\tdescription                    = get_env_var('PYMOLXS_DESCRIPTION')\n\tcopyright                        = get_env_var('PYMOLXS_COPYRIGHT')\n\tmaintainer                         = get_env_var('PYMOLXS_MAINTAINER')\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\tversion = check_output([\"git\", \"describe\", \"--tags\", \"-v\", \"--always\", \"--dirty\", \"--always-on-failure\", \"--match\", \"^0\\+.*\\+$\", \"--dirty\"])\n\tif re.search(\"(?P<version>\\d+\\.\\d+)/.*$\", version):\n\t\tversion = version.replace(\"+\", \"\")\n\t\tversion = re.sub(r'[^0-9]', \"\", version).strip()\n\t\tif version and len(version) > 40:\n\t\t\tversion = version[:10]\n\t\tversion = version + \" \" + check_output([\"git\","}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\ttry:\n\t\t\toutput = get_file_from_path(config['versions_file'])\n\t\texcept:\n\t\t\t# There's no current version. \n\t\t\treturn get_default()\n\n\t\t# Check output for new file format.\n\t\tif output.endswith('.new'):\n\t\t\toutput_new = get_files(output)\n\t\t\tif len(config['versions_file']) > 0:\n\t\t\t\tversions = output_new[config['versions_file'].rfind('.')+1:]\n\t\t\t\tversions[config['versions_file'].rfind('.')+1:] = config['versions_file']\n\t\t\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\tversion_map = get_version_map()\n\n\tif version_map is None:\n\t\t\traise Exception(\"could not get version information from the server\")\n\t\treturn version_map\n\n\tcurrent = datetime.now()\n\n\tversions = [\n\t\tdict_from_date_string(current, i) for i in get_ver_nums(version_map)]\n\n\treturn versions\n\n\ndef get_versions_for_package(package):\n\t\"\"\"\n\tGet a list of versions for a specific package.\n\t\"\"\"\n\t\tversions = get_versions()\n\t\tif package in"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\tversions = webnotes.conn.sql(\"\"\"select v.name, v.version from `tabVersion Info`\n\t\t\twhere docstatus = 1 and ifnull(v.prev, 0) = %s\n\t\t\tand docstatus < 2\"\"\", as_dict=1, as_dict=1)\n\n\t\tif versions:\n\t\t\treturn versions[0]\n\t\telse:\n\t\t\treturn default_v.get_dict(\"version\")\n\n\tdef is_frozen(self, doc, user):\n\t\tif self.get_version()!= get_version(doc, user):\n\t\t\treturn 1\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\ndef get_version_from_files(filename, version):\n\t\"\"\"\n\tGet a version from a file.\n\t\"\"\"\n\twith open(filename, 'r') as infile:\n\t\tfor line in infile:\n\t\t\tif line.startswith('version '):\n\t\t\t\tversion = line[2:].split('=')[1].strip()\n\t\t\t\treturn version\n\t\treturn default_version\n\nfrom eve.test import eq_\nfrom eve.test.decorators import eve_test_func, login_required\nfrom eve.test.mixins import BaseEveTest\n\n\nclass"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t\ttry:\n\t\t\tc = get_current()\n\t\t\tassert(c)\n\t\t\treturn (c.get(\"version\") or c.get(\"version_number\", \"0\"))\n\t\texcept Exception,e:\n\t\t\tprint \"\\nERROR! Failed to get current version information: %s\"%e\n\n\nclass VersionManager:\n\t\"\"\"\n\tManager class.\n\t\n\tClass used to create versions with new versions\n\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize manager.\n\n\t\tParameters:\n\t\t\tfilename : absolute filename of file\n\t\t\tformat : format of version ("}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# FIXME: We shouldn't be allowed to merge the pieces into\n\t\t# parts/vcs/flags; they are messing with either git or hg+git\n\t\t# messages; it is ugly and impossible for github URLs to be resolved.\n\t\tif pieces[0].startswith(u('git')) or pieces[0].startswith(u('hg+')):\n\t\t\tif style == \"pep440\":\n\t\t\t\tstyle = \"pep440\"\n\t\t\telse:\n\t\t\t\tstyle = style + \"pep440\"\n\t\treturn style\n\texcept EnvironmentError:\n\t\traise EnvironmentError(\"unable to compute version\")\n\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# FIXME: this code should really just be rewritten\n\t\t# for IronPython\n\t\tfor i in range(len(pieces) - 1):\n\t\t\tpieces[i] = re.sub(\"\\{(.*?)\\}\", \" \", pieces[i])\n\t\telse:\n\t\t\tpieces[0] = re.sub(\"\\{(.*?)\\}\", \" \", pieces[0])\n\t\t\tpieces[0] = pieces[0].replace(\"\\*\", \" * \")\n\t\t\tpieces[1] = pieces[1].replace(\"\\*\", \" * \")\n\t\treturn pieces\n# -*- coding: utf-8 -*-\nimport"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# First, compute our rendered version\n\t\t# * or the full hex version, with or without the pre-release-1\n\t\t# * representation.\n\t\tmd = str(build_version(*pieces[:4])).strip()\n\telif style is None:\n\t\t# Set the style to 'pep440' or 'pep440__pre.'. \n\t\t# Then use git describe on the top of the source tree.\n\t\tmd = git_describe(pieces)\n\telse:\n\t\t# TAG version extraction.\n\t\ttag = str(style)\n\t\tmd = git_describe_long"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# Create a new \"best version\", using the given style\n        # BUTNOT EVER IT IS GENERATED BY values: Pre-Alpha 9 #\n        # with opensource; per http://www.owl.de/python/file-pre.html, a copy of this\n        # document is created by adding `pack your new style` to the end of this\n        # variable. This is where you can put a \"dirty\" version of this\n        # tree, otherwise you'll always get an \"not here\" Exception:\n        # https://github.com/sphinx-doc/sphinx/blob\n       "}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# FIXME: there is a bug that requires the\n\t\t# \"git rev-parse\" to be bumped to something more likely\n\t\t# than to have a real time work.\n\t\tif pieces[\"error\"]:\n\t\t\tif pieces[\"error\"].endswith(\"%%d.%%d\" % pieces[\"error\"]):\n\t\t\t\treturn {\"version\": \"unknown\", \"full-revisionid\": pieces.get(\"long\"), \"dirty\": None}\n\t\t\telse:\n\t\t\t\traise ValueError(\"unable to extract version from tag\")\n\t\telse:\n\t\t\tfull = qualify(pieces)\n\t\t\tfull = full.lstrip"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# Defaulting to no style, so no style is requested\n\tpieces = {} if pieces is None else pieces\n\tstylesheet = get_stylesheet(style) or 'bootstrap.css'\n\treturn render_to_string(style, pieces, resolve_stylesheet=True)\n\nrender = render_string\nimport logging\nimport os.path\nimport re\nimport sys\n\nfrom autotest.client.shared import error, utils\nfrom virttest.utils_test import libvirt_xml\nfrom virttest import env_process\n\n\ndef run_virsh_migration(image, params,"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# FIXME: This is ugly, but it makes sense to use the parent's\n\t\t#              \"right\" attribute.\n\t\treturn style.replace(\n\t\t\t\" : \",\n\t\t\t\" : \" : \",\n\t\t\t\" : \", \" : \"\n\t\t)\n\ndef get_git_tag(pieces, tag_name):\n\t\"\"\"Return the tag in the given name.\"\"\"\n\t# Branch-based tags do not have a tag yet.\n\tif pieces[\"closest-tag\"]:\n\t\t# Source tarballs (built by setup.py sdist) are tag-1.0 and\n\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# override default formatting methods and override the fill\n\t\t# method.\n\t\tpieces = super(StyledBlockTemplate, self).render(pieces, style)\n\t\tpieces.update({\n\t\t\t'class':'styled-block',\n\t\t\t'data-style': 'data-style-%s' % style[1],\n\t\t\t'data-name': style[2],\n\t\t})\n\t\treturn pieces\n\tdef as_table(self):\n\t\t\"\"\"\n\t\tGiven a style template, return a string representing the\n\t\tstyled block.\n\n\t\tThis is a convenience for building a table"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\t# If there is a previous transition from a change, see if we are\n\t\t# changing this part.\n\t\tnew_pieces = []\n\t\tfor i in range(pieces[0].start(), pieces[0].end()):\n\t\t\tif pieces[i].name and pieces[i].name!= pieces[i-1].name:\n\t\t\t\tif style.get('rebase')!='move':\n\t\t\t\t\tnew_pieces.append(pieces[i])\n\t\t\t\telse:\n\t\t\t\t\tnew_pieces.append(pieces[i])\n\t\tstyle['pieces'] = old_map(new_pieces"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t\n# first, check if we're inside a color. If it's already one, just copy it from\n\t# the top to the bottom of the rendered version. If not, tag\n\t# the first match with hope.\n\t\n\t\n# For \"old\", we're using the upstream version\n\tif pieces[\"closest-tag\"] < min(pieces):\n\t\thtml = render_line(pieces, style)\n\telse:\n\n\t\t# If we're on a tags.Tag\n\t\thtml = render_line(pieces, style), []\n\t\t\n\t\t# If we one point inside a tag, use"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n# We prefer a -inu-suffix that includes more.\n# There's no risk of modifying the output of clang when including\n# standard c++ headers. So we do our best by creating _minor_version.py, which\n# we use for both.\nif os.path.exists(_minor_version_py):\n\tfor i in range(len(_minor_version_py.split('.'))):\n\t\t_minor_version_py = _minor_version_py + \"._%d\" % (i + 1,)\n\n\n# Use more strictly rather than a full hex"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\treturn''.join(pieces[1:])\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2016 Chris Caron\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\n#############################################\n#                WARNING                   #\n#############################################\n\nfrom __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\n\nANSIBLE_METADATA = {'metadata_version': '1.1',\n                   'status':"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\t# use -if-match,/ or similar\n\t\tfull_path = ''\n\t\tfor i in range(2):\n\t\t\tfull_path = full_path + '/' + pieces[i].strip().replace('/', '.')\n\t\treturn full_path\n\n\tdef parse_sha1(buf):\n\t\treturn base64.b64decode(buf).rstrip('\\n')\n\n\tdef parse_base64(buf):\n\t\treturn base64.b64decode(buf)\n\n\tdef get_commits_from_sha1(buf):\n\t\tm = re.match(r'^Commit:"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n# Returns the version string for the given string\ndef get_version(pieces):\n\treturn getattr(pieces, \"version\", \"unknown\")\n\ndef render_pep440(pieces):\n\treturn XL_PLAINSLATE[\"render_pep440\"].strip()\n\n# Generate output\ndef run_pep440(pieces):\n\tfrom version import get_version\n\twrite_git_version = False\n\n\ttry:\n\t\tfd, fname = tempfile.mkstemp(\".git\")\n\n\t\tf = open(fname, \"w\")\n\t\tf.write(\"version = %s\\"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\tassert len(pieces) == 1\n\t\treturn '%s%s' % (pieces.pop(), pieces.pop())\n\n\ndef render(pieces, style):\n\t\"\"\"\n\tRender the given path.\n\t\"\"\"\n\t\timport re\n\n\t\t# Build paths inside the project like this: os.path.join(BASE_DIR,...)\n\t\tpath = os.path.abspath(os.path.join(BASE_DIR, pieces.pop()))\n\t\tos.path.join(path, 'README.md')\n\t\tfor p in pieces:\n\t\t\tif os.path.dirname"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\t# This regex works without arguments\n\t\tmatch = _RE_PATTERN_ALPHABETAIN.search(pieces)\n\t\tif match:\n\t\t\treturn prefix + clean_sentence(match.group(2))\n\t\treturn \"\"\n\n\n# CDATA tags\nCDATA_TAGS = {\n\t'born': ['born', 'born'],\n\t\"moon\": [\"moon\", \"moon\"],\n\t\"bake\": [\"bow\", \"bow\", \"bow\"],\n\t'baby': ['baby', 'bag', 'burgers', 'burgers', '"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\treturn \".\" in pieces.get(\"closest-tag\", None)\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tIf there is a current PEP440 file, This will write to a formatting string\n\tbefore the tags. Our goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tcan unify tags, you should have a \"tag\" that shows up as \"tag 0.gHEX\"\n\tif there are no tags.\n\t\"\"\"\n\tif pieces[\"closest-tag\"]:\n\t\trender_pep440_pre(**pieces)\n\telse"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\t\treturn \".\"\n\t\treturn \"+\"\n\treturn \"+\"\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tBuild up version string, with post-release \"local version identifier\".\n\t\"\"\"\n\t# now build up version string, with post-release \"version\".\n\treturn _version.render(pieces, version=\"local\")\n\ndef render_pep440_pre(pieces):\n\t\"\"\"\n\tBuild up version string, with post-release \"local version identifier\".\n\t\"\"\"\n\t# now"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\tif '+' in pieces.get('+', {}) and not pieces['+' in pieces[''}]:\n\t\t\treturn '.' + pieces['+']\n\t\treturn None\n\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\n\t Our goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t\n#     if len(pieces) == 1:\n#         return \".\" + pieces.first() + \".yad\"\n#     return \".\".join(pieces)\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tBuild up version string, with post-release \"local version identifier\".\n\t\"\"\"\n\timport re\n\n\ttry:\n\t\timport git_version\n\t\tfull_version = git_version.version_string()\n\t\tgit_full_full = full_version.strip()\n\texcept ImportError:\n\t\tpass\n\n\ttry:\n\t\timport git\n\t\t"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tp = [str(c) for c in commands]\n\t\tp.append('sphinx')\n\t\tp.append(os.path.abspath(args[0]))\n\t\tif cwd:\n\t\t\tp.append(\"--cwd=%s\" % cwd)\n\t\tif verbose:\n\t\t\tp.append(\"-v\")\n\t\tif hide_stderr:\n\t\t\tp.append(\"-r\")\n\t\tp.append(str(env).strip())\n\t\tif PACKAGERO_SUBVERSION:\n\t\t\tp.insert(0, PACKAGERO_SUBVERSION)\n\t\trun_subprocess("}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tp = subprocess.Popen(commands,\n\t\tcwd=cwd,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=devnull,\n\t\tuniversal_newlines=True,\n\t\tenv=env,\n\t\tbufsize=1\n\t\t)\n\t\tout, err = p.communicate(input=command)\n\t\treturn out\n\nscript_help = \"\"\"\nUsage:./check_docker.py [-d <directory>] [-h]\n\"\"\"\n\ndef help_test():\n\tprint \"\"\"\n\tRun check_docker with the passed -d directory.\n\t\"\"\"\n\tprint \"\"\"\n\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tstart_time = time.time()\n\t\ttry:\n\t\t\tcode = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, cwd=cwd, verbose=verbose, stderr=subprocess.STDOUT)\n\n\t\texcept OSError as e:\n\t\t\tif hasattr(e, 'errno') and e.errno == errno.ENOENT:\n\t\t\t\t# There's no.cmd file and can't be installed by that system\n\t\t\t\treturn None\n\t\t\telse:\n\t\t\t\traise\n\n\t\treturn code\n#\n# This Source Code Form is subject to the terms of the Mozilla"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tassert isinstance(commands, list)\n\t\tp = [to_native(c) for c in commands]\n\t\tp[0] = str(p[0]) # append -s for windows purpose\n\t\tif hide_stderr:\n\t\t\tp[0] = str(p[0] or '')\n\t\tif hide_stdout:\n\t\t\tp[0] = str(p[0] or '')\n\t\ttry:\n\t\t\tp[1] = subprocess.Popen(args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\tif"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tif cwd:\n\t\t\tcwd = os.path.abspath(cwd)\n\t\telse:\n\t\t\tcwd = os.getcwd()\n\t\tp = Popen(commands, close_fds=True, stdout=PIPE, stderr=PIPE, env=env, cwd=cwd)\n\t\t(stdout, stderr) = p.communicate()\n\t\tassert p.returncode == 0, \"Command '%s' failed with status code %d\" % (commands, p.returncode)\n\t\tif verbose:\n\t\t\tprint stdout\n\t\tif hide_stderr:\n\t\t\tprint stderr\n\t\tif cwd and not is_python"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tp = self._null_subprocess(commands, args, cwd, verbose, hide_stderr, env)\n\t\tif env and env.has_key('TERM'):\n\t\t\t# On win32 this calls Popen(command, shell=False)\n\t\t\t# On win64 this calls Popen([command] + args, env=None)\n\t\t\tp.stdout.write(str(p.stdout.read()))\n\t\t\tp.stdout.flush()\n\t\treturn p\n\n\tdef kill(self, id):\n\t\t\"\"\"\n\t\tStop the given pid.\n\t\t\"\"\"\n\t\tp = self._null"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\trun_subprocess(commands, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\texcept Exception as e:\n\t\tprint(\"failed to execute command - error: %s\" % e)\n\t\tif display:\n\t\t\tprint(args)\n\t\telse:\n\t\t\tprint(str(e))\n\t\tsys.exit(1)\n\nif __name__ == \"__main__\":\n\tsys.exit( run_command([r'/usr/bin/'+__dir__]) )\n\n# -*- coding: utf-8 -*-\n# H"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tassert isinstance(commands, list)\n\t\t# Don't call str() on non-Unix systems\n\t\tif not isinstance(cwd, str):\n\t\t\tcwd = os.getcwd()\n\t\tif hide_stderr:\n\t\t\tprint(\"Called with'stderr' to indicate that the command being executed was a standard command\")\n\t\tif env is None:\n\t\t\tenv = os.environ\n\t\tenv['CPPPATH'] = env['PATH']\n\t\tif cwd:\n\t\t\tenv['PATH'] += ':' + cwd\n\t\tlog.info(\"Running %s\",''.join(commands))\n\t\treturncode ="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t\n\tcwd = os.getcwd()\n\tstdout = None\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\telse:\n\t\tcwd = str(cwd)\n\t\n\tp = None\n\tfor char in commands:\n\t\tif verbose:\n\t\t\tprint \"Running command '%s'\", char.command\n\t\tif not is_string(char):\n\t\t\tcontinue\n\t\tif cmd_exists(char, cwd):\n\t\t\tp = subprocess.Popen(char.command, shell=True, stdout=stdout)\n\t\telse:\n\t\t\tprint \"Couldn't find command '%s'"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tp = subprocess.Popen(list(commands), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\t\tout, err = p.communicate()\n\n\t\t# the p.returncode, stdout, and stderr attributes are as for os.popen()\n\t\tret = p.returncode\n\t\tif ret:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Command exited with exit code\", p.returncode)\n\t\t\traise SystemExit(p.returncode)\n\t\treturn out as _out, err as _err\n\telse:\n\t\treturn run_command_as_list(commands, args,"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# Save the configuration to a ConfigParser instance.\n\t\tconfig = VersioneerConfig()\n\t\t# create a standard read-only version.\n\t\t# See https://docs.python.org/3/howto/copying.yaml for copying.\n\t\t# NOTE: This started at https://github.com/warner/python-versioneer/issues/8472\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"\"\n\t\tconfig.parentdir_prefix = \"\"\n\t\tconfig.versionfile_source = \"\"\n\t\tconfig."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\ndef _finalize(config):\n\t\"\"\"finalize -- finalizes the VersioneerConfig and returns to the\n\ttop level of _versioneer.py.\"\"\"\n\troot = get_root()\n\tcfg = VersioneerConfig()\n\tcfg.VCS = \"git\"\n\tcfg.style = \"pep440\"  # git-2.0.format\n\n\ttry:\n\t\tcfg.get_source_as_commitment = cfg.git_full_name\n\t\tcfg.tag_prefix = cfg.versionfile_source\n\t\tcfg.parentdir_prefix = cfg.versionfile_build\n\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# Make sure we are in'setup.py' rather than'setup.py build'\n\t\timport os\n\t\timport sys\n\t\tfrom subprocess import CalledProcessError\n\t\troot = os.path.realpath(os.path.dirname(__file__))\n\t\tif os.path.exists(os.path.join(root, '.git')):\n\t\t\t# import git\n\t\t\tdir = os.path.join(root, \".git\")\n\t\t\tif os.path.exists(dir):\n\t\t\t\t.command_obj['config']        = DirConfig(dir, True, False, True)\n\n\t\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n        # _version.py\n        cfg = VersioneerConfig()\n\t\tconfig = cfg.get_config()\n\n\n\t\t# remember the format of the version string.\n        cfg.VCS = \"git\"\n\t\t# style=0, no #style on Wgit --filter=-i440\n\t\tcfg.style = \"\"\n\t\t# section-width: 4\n\t\tcfg.section_width = \"4\"\n\n\t\t# suppress options from config files\n\n\t\t# save to a file when \"setup.py versioneer\" was created"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\n\t\n\t# I don't know why 'extragues' is relevant here, but if it breaks,\n\t# what you should do without making any changes for it.\n\textensions = {\n\t\t'simple-release':[],\n\t\t'alabaster':'sphinx.ext.autodoc',\n\t\t#'sphinx.ext.viewcode': 'api.ext.viewcode',\n\t\t#'sphinx.ext.githubpages':'sphinx.ext.githubpages',\n\t\t'sphinx.ext.mathjax':'mpl_mathjax.mpl_mathjax',\n\t\t'recommonmark"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\tfpath = os.path.join(os.environ['HOME'], '.git', 'config')\n\t\twith open(fpath) as f:\n\t\t\tfor line in f.readlines():\n\t\t\t\tline = line.strip()\n\t\t\t\tif line.startswith('-e'):\n\t\t\t\t\tcontinue\n\t\t\t\tif line.startswith('-o'):\n\t\t\t\t\there = line[5:].strip()\n\t\t\t\t\tif os.path.isdir(here):\n\t\t\t\t\t\there = os.path.abspath(here)\n\t\t\t\t\ttry:\n\t\t\t\t\t\toption, val = line.split('=')\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\toption = arg\n\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n        # _version.py\n        cfg = Config(_version.get_config_var('LONG_VERSION'),\n                     long_description=get_config_var('LONG_DESCRIPTION'),\n                     maintainer=get_config_var('MAINTAINER'),\n                     maintainer_email=get_config_var('MAINTAINER_EMAIL'),\n                     url=get_config_var('LINK_URL'),\n                     classifiers=get_config_var('CLASSIFIERS'),\n                     cmdclass={'test': LocalVCSCommand,\n                                    'install':"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\n# Don't modify anything here, except in PEP8\n# TODO(unknown): Add test to recompile these.\nget_config.converters['bool'] = lambda x: bool(x)\nget_config.converters['int'] = lambda x: int(x)\nget_config.converters['datetime'] = lambda x: datetime.datetime.fromtimestamp(float(x))\nget_config.converters['path'] = lambda x: os.path.expanduser(os.path.expandvars(os.path.expanduser(x))).replace(\"\\\\\", \"/\")\nget_config.converters['"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# these strings will be replaced by git during git-archive\n\timport git\n\twith open(os.path.join(git.GitRevisionFilename(__file__), \".git\"), \"a+\") as f:\n\t\tf.write(git.get_config())\n\tos.chdir(cwd)\n\n\nclass NotThisMethod(Exception):\n\tdef __init__(self, message):\n\t\tself.message = message\n\nclass VersioneerConfig:\n\tdef __init__(self):\n\t\tself.versiondir = None\n\t\tself.verbose = False\n\t\tself.full = False"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t\t# Create, populate and return the VersioneerConfig() object.\n\t\treturn VersioneerConfig(_version)\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\tdef __init__(self, method):\n\t\tself.method = method\n\n\tdef __call__(self, f):\n\t\ttry:\n\t\t\tresult = getattr(self.method, f)\n\t\t\tif result is None:\n\t\t\t\traise NotThisMethod(\"Not completed due to an invalid method %r\" % self.method)\n\t\texcept AttributeError:\n\t\t\t#"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tif vcs.vcs()!= \"git\":\n\t\t\traise ValueError(\"'git' provider needs 'git' handler\")\n\n\t\treturn vcs._vcs_handler(method)\n\treturn decorate\n\n\nclass Repository(object):\n\t\"\"\"\n\tRepository object which represents a repository\n\t\"\"\"\n\n\tdef __init__(self, url=None):\n\t\t\"\"\"\n\t\tCreate Repository\n\n\t\t@param url A valid git url\n\t\t\"\"\"\n\t\tself._url = url\n\n\tdef __str__(self):\n\t\treturn \"Repository <{url!r}>\".format(url=self._url)\n\n\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tvcs.register(method, handler)\n\t\t\treturn handler\n\t\treturn wrapper\n\treturn decorator\n\ndef unregister_vcs_handler(vcs, method):  # decorator\n\t\"\"\"\n\tUnmark a method from a handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(handler):\n\t\t\tvcs.unregister(method)\n\t\t\treturn handler\n\t\treturn wrapper\n\treturn decorator\n#!/usr/bin/env python3\n# $Id$\n\"\"\"\nThis is a unit test from the original test suite.\nIt doesn't test the data structure,"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(f):\n\t\t\tvcs.register(f.__name__, f)\n\t\t\treturn f\n\t\tvcs.register(method.__name__, wrapper)\n\t\treturn method\n\t\"\"\"\n\tdef decorate(f):\n\t\tvcs.register(f.__name__, f)\n\t\treturn f\n\treturn decorate\n\ndef get_version():\n\t\"\"\"\n\tGet the version number of the git repository.\n\t\"\"\"\n\t\twith open(versionfile_source) as f:\n\t\t\treturn f.read().strip()\n\texcept EnvironmentError:\n\t\treturn \"unknown\"\n\n# --"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tvcs.register(method, wrapper)\n\t\t\treturn func\n\t\treturn wrapper\n\n\treturn mark_as_handler_decorator(wrapper)\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport codecs\nimport os\nimport re\nimport socket\n\nfrom ansible import constants as C\nfrom ansible.errors import AnsibleError, AnsibleFileNotFound\nfrom ansible.module_utils._text import to_bytes, to_native, to_text\nfrom ansible.plugins.connection import ConnectionBase\nfrom ansible.playbook.attribute import Attribute, FieldAttribute\nfrom ansible.utils"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef decorate(func):\n\t\t\t@functools.wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tvcs.register(method, func, *args, **kwargs)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn decorate\n\t\"\"\"\n\tdef wrapper(func):\n\t\tvcs.register_vcs_handler(vcs, method)\n\t\treturn func\n\treturn wrapper\n# -*- coding: utf-8 -*-\n\"\"\"\n    Copyright (C) 2012-2018 University of Dundee & Open Microsense\n    All rights reserved.\n\n    This program"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef inner(func):\n\t\t\tvcs.add_handler(method, func)\n\t\t\treturn func\n\t\treturn inner\n\n\t@register_vcs_handler('svn+ssh')\n\tdef _add_handler(vcs_handler):\n\t\tvcs.add_handler('ssh', SVNHandler)\n\n\t@register_vcs_handler('svn+git')\n\tdef _add_handler(vcs_handler):\n\t\tvcs.add_handler('git', GitHandler)\n\n\t@register_vcs_handler('git+svn')\n\tdef _add_handler(vcs_handler):\n\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tasync def decorate(func):\n\t\t\tasync def wrapper(*args, **kwargs):\n\t\t\t\tvcs.register_vcs_handler(func, *args, **kwargs)\n\t\t\treturn update_wrapper(wrapper, func)\n\t\treturn decorate\n\t\"\"\"\n\tasync def decorate(func):\n\t\treturn func\n\treturn decorate\nclass BaseWidget(object):\n    \"\"\"Base class for all widgets.\n\n    All Widgets should implement the `init_widget()` method.\n\n    \"\"\"\n\n    def __init__(self):\n        self._widget_data = {}\n\n    def init_widget(self, widget):\n       "}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tobj = vcs\n\t\t\tif not vcs is None:\n\t\t\t\tobj = func(obj)\n\t\t\treturn obj\n\t\treturn wrapper\n\treturn vcs_handler\n\nvcs_handler = register_vcs_handler(vcs, 'install')\nimport matplotlib\nimport matplotlib.cm as cm\nimport matplotlib.lines as mlines\nimport numpy as np\nfrom matplotlib.pyplot import *\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d import Axes3DCollection3D\nfrom"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(f):\n\t\t\t@wraps(f)\n\t\t\tdef wrapper2(self, *args, **kwargs):\n\t\t\t\tself.status = VCSStatus.get(method, 5)\n\t\t\t\treturn f(self, *args, **kwargs)\n\t\t\treturn wrapper2\n\n\t\treturn wrapper\n\n\t@register_vcs_handler('git', 'annotate')\n\tdef git_annotate(self, **files):\n\t\tfrom.gitmsg import GitMsg\n\t\treturn GitMsg(self._repo, files=files)\n\treturn GitMsg\n# coding=utf-8\nfrom"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif vcs.register_repository:\n\t\t\t\ttry:\n\t\t\t\t\tfunc = vcs.register_repository(method)\n\t\t\t\t\tfunc_was_a_valid = True\n\t\t\t\texcept AuthException as e:\n\t\t\t\t\traise AuthException(\"{a}\".format(a=e)) from e\n\t\t\t\texcept Exception as e:\n\t\t\t\t\traise AuthException(\"Error while registering VCS:\", e) from e\n\t\t\t\telse:\n\t\t\t\t\tif func_was_a_valid:\n\t\t\t\t\t\tdel vcs.__dict__[method]\n\n\tA vcs can also be imported from a module or"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t    \n    def check_object_info(self, object_id):\n\t\"\"\"\n\tChecks if an object exists with the given ID.\n\t\n\tReturns:\n\t    None\n\t\"\"\"\n\t    \n    def _add_object_to_storage(self, object_id):\n\t\"\"\"\n\tAdds an object to the storage.\n\t\n\t\"\"\"\n\t    \n    def _add_object_from_storage(self, object_id, new_object_id):\n\t\"\"\"\n\tAdds an object to the storage that should contain this object. \n\t\n\t\"\"\"\n\t    \n    def delete_object(self, id):\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\tlog = self.get_logger\n\t\tlog.debug(\"Validating hierarchy...\")\n\t\tcheck_hierarchy_object_exists(self._instance, self._storage_root)\n\t\tnum_objects = len(self._storage_root)\n\t\tbest_object = self._storage_root[num_objects - 1]\n\t\tif best_object.get_parent_object():\n\t\t\tobj_count = len(self._storage_root[:best_object.get_parent_object().get_parent_object().root_object_index])\n\t\t\tlog.debug(\"Num parents {"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n    def show_message(self, message, level=0):\n\t\t\"\"\"\n\t\tShow a message box showing level number 'level'.\n\t\t\n\t\tThe message should be shown in a separate place.\n\t\t\n\t\tParameters:\n\t\t    message - a string containing a message of interest\n\t\t    level - the string defining the level, will be shown with a level appended\n\t\t    show_warnings - should messages of warning if the level is >= 0\n\t\t\"\"\"\n               \n\t\tif level.startswith(' ') or level in ('ERROR', 'WARNING'):\n                        # In debug mode, ignore the entire message if it's not valid\n                        if not self.validate"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\t# Validate that the given objects are valid\n\t\tif validate_object(objects_to_check=validate_objects):\t# pragma: no cover\n\t\t\treturn num_objects\n\t\t# Otherwise, raise an exception\n\t\tif validate_objects:\n\t\t\tmissing = [o for o in objects if o not in self.objects]\n\t\t\tif len(missing):\n\t\t\t\treturn 0\n\t\t\t# If the objects is contained in other objects, raise an exception\n\t\t\telif show_warnings:\n                raise AssertionError(\"Missing %d objects for this tree: %s\", len(missing), ', '.join(missing"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n        # Check hierarchy first...\n        if self.validate_hierarchy(validate_objects, check_digests, show_warnings):\n            if not show_warnings:\n                raise AttributeError(\"Invalid hierarchy.\")\n\n        from spyderlib.sobjs.archive.base_tree import BaseTreeNode\n\n        if self.validate:\n            return self.__validate(base_tree_name='Storage',\n                                    base_folder=self,\n                                    child_folder=self,\n                                    child=object_type, method='validate',\n                                    validate_hierarchy=validate_hierarchy)\n        else:\n            return self.__tree_valid(show_warnings="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\t\n\tglobal _current_root\n\tglobal _newstyle_object\n\t\n\t_current_root = None\n\t_newstyle_object = None\n\t\n\tif self._current_root is not None and self._current_root.get_children_count() > 0:\n\t\t_current_root = self._current_root\n\t\t_newstyle_object = self._current_root.get_children()[0]\n\t\t\n\t\tif len(str(self._current_root)) > 100:\n\t\t\t# no more than 100 objects will be checked\n\t   \tnum_objects_found = 0\n\t   "}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\t\n\tif check_digests:\n\t\tself.check_checksums = check_digests\n\t\t\n\tif show_warnings:\n\t\tprint \"Validating hierarchy...\"\n\t\t\n\troot = self.get_data_by_structure()\n\t\n\t#print \"Validations found:\", len(root)\n\t\n\t#for i, item in enumerate(root):\n   \t\tif show_warnings and str(i+1) == \"objects checked\":\n                print \"Item \", i + 1, \"is\", validation_objects, \"checked\"\n\tnum_objects = len(root)\n\t\n\tif validate_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t if not (validate_objects and check_digests and show_warnings):\n\t\t    self.warning('Unrecognised number of objects to continue checking.')\n\t    if check_digests:\n\t\t# check against the directory tree\n\t    num_objects = int(os.listdir(self.location))\n\t    good_objects = 0\n\t    try:\n\t\t# build a dictionary from the root of the directory tree\n\t    # get all possible objects in the directory tree\n\t    good_objects_list = {}\n\t    good_objects_keys = sorted(good_objects)\n\t    for key in good_objects_keys:\n\t\tkey ="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n#\treturn_object = []\n#\t\n#\tdef validate_tree(self, validate_root=False):\n#\t\tprint(\"[INFO] Attempting to validate parent or leaf {0}\".format(self.full_path))\n    \n    def validate_tree2(self, validate_data=[], validate_objects=True, check_digests=True, show_warnings=False):\n\t\n        if validate_objects:\n            objects_data = validate_data\n        else:\n            objects_data = []\n \n        if check_digests and not check_data:\n            _logger.info(\"["}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n   \tif not validate_objects:\n\t    assert not show_warnings\n            message('Errors found to be validated:\\n%s\\n' % (_get_msgs(check_digests))))\n            raise error_dialog('Error Checked', 'Validation must be supplied', show_warnings=show_warnings)\n        num_objects = self.validate_hierarchy_num()\n        good_objects = len(check_trees)\n        assert good_objects == num_objects\n\n   \tif num_objects not in self.max_entries:\n\t    message('Number of objects to validate: %s\\n' % (self"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n#        raise Exception('Not implemented yet')\n\n        # create a new root\n        os.mkdir(os.path.join(os.path.abspath(os.path.dirname(__file__)),'../../'))\n\n        # set up configuration\n        self.configuration = self.configuration_template(name = 'ocfl', default = os.path.join(os.path.dirname(__file__),'../../../OCLL.xml'), default_value = os.path.join(os.path.abspath(os.path.dirname(__file__)), '../../etc/ocfl/ocfl.conf'), default_"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tif os.path.exists(self.root_path):\n\t\t  raise ValueError('Storage root path %s already exists.')\n\n\t\tdata_root = self.root_path\n\t\tos.makedirs(data_root)\n\t\twith open(self.root_path, 'wb') as file_object:\n\t\t  pass\n\n\t#\n\tdef create_dataset(self, index_dir, file_path, data_filename, options):\n\t\"\"\"\n\tCreate an OCFL dataset.\n\t\n\t@param index_dir: The root directory of the storage root.\n\t@param file_"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\tif self._oclfl:\n\t\t\tself._oclfl.initialize()\n\t\n\tdef cleanup(self):\n\t\t\"\"\"\n\t\tTells the OCRF to clean the storage storage root as is.\n\t\t\"\"\"\n\t\tself._oclfl = None\n\n\tdef clean(self):\n\t\t\"\"\"\n\t\tChecks whether ocrfl is in use and writes it to self._oclfl.\n\t\t@raise OCRFError: If not.\n\t\t\"\"\"\n\t\tif self._oclfl == None:\n\t\t\traise OCRFError, \"Error: ocrfl not"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n#\t\tprint \"\\n\"\n#\t\t\n#\t\t# initialize a new OCFL storage root\n#\t\t# check, we have an existing storage root for this new OCFL\n#\t\tstorageRootName = self.storageRootName\n#\t\tprint \"%s: storage root already exists\" % (storageRootName)\n\t\n#\t\t# initialize the storage root\n#\t\tif not self.folderExists():\n\t#\t\tself.mkdir(self.storageRootName)\n#\t\tself.mount(\"%s/%s\" % (self.storage"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n# Create or replace a record with the contents of a CFL FITS file.\n# This takes the parameters of CFL_FILE to read.\n# To do so, use ocflpack/otfpack on a string with the following list:\n# [...\n# ['H2','O3','3X','O4','O5','O6','O7','X3','O2','O1','O0','2','O3','G']....\n# ['H','O','3','X','O16','O17','O18','O19','X3"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\nroot = Root(sys.argv[1]).initialize()\n\nprint(\"Setting the data source type to pyfits\")\n\nsys.exit(main(root) == 0)from unittest import TestCase\n\ntry:\n    import simplejson as json\nexcept ImportError:\n    import json\n\nfrom couchdbkit.exceptions import NotFound\nimport couchdbkit.tests\nfrom celery import register_signal\nfrom django.conf import settings\n\nfrom elasticsearch_dsl import Search\n\n\n@register_signal\nclass FakeCouchDBKey():\n    pass\n\n\nclass TestSearch(TestCase):\n\n    def setUp"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\nclass OclStorageRoot(fs.Root):\n\t\"\"\"\n\tCreate or update OCLStorageRoot\n\t\"\"\"\n\t\n    @classmethod\n\tdef reset(klass):\n\t    fs.Root.reset()\n\nclass Filesystem(fs.Object):\n    \"\"\"\n\tFS based storage root.\n\t\"\"\"\n\tdef __init__(self, filename):\n\t    self.__filename = filename\n\t    self.fs = fs.FileSystem.open(self.__filename)\n       \n\tdef write(self, *args, **kwargs):\n\t    return self.fs.filesystem.write(self.__filename, *args,"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\n\tdef getRoot(self):\n\t\n\t\t# Create and setup logging\n\t\tself.logger = logging.getLogger()\n\t\tself.logger.info(\"Opening serial port and logging in...\")\n\n\t\tself.serial = serial.Serial(self.portName, baudrate=9600, parity=serial.PARITY_OCTO, closefd=1, timeout=10)\n\t\tself.serial.open()\n\t\tself.logger.info(\"Creating OCFL storage root.\")\n\n\t\tself.__setup_log()\n\n\tdef isConnected(self):\n\t\n\t\t# Check connection\n\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\nclass OCFLStorageRoot():\n    \"\"\"\n    Define a new storage root class that records all the OS-specific actions performed by the \n    operating system. Used by the OS-specific actions in the OCFStorage class.\n    \"\"\"\n\n    def __init__(self):\n\t\"\"\"\n\tThe object containing the storage root class.\n\t\"\"\"\n        self.version = 3\n\t\nclass OCFStorage(object):\n    \"\"\"\n    The general processing of an OCF FL storage system.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialized the main storage object.\n\n        Note that there is no OCFStorage"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\t# create\n\t\tself.root = createRoot()\n\n\t\tself.storageRoot = StorageRoot(self.root)\n\n\tdef get_config(self):\n\t\treturn self.storageRoot.get_config()\n\n\tdef get_storageRoot(self):\n\t\t'''return the storage root for this file'''\n\t\treturn self.storageRoot\n\n\tdef get_object(self, objId):\n\t\treturn self.storageRoot.get_object(objId)\n\n\tdef create_config(self, data):\n\t\t'''create the config file'''\n\t\treturn self.storageRoot"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\tif version == \"0\":\n\t\t\treturn \"0\"\n\t\telif version.startswith(\"0.0.0\"):\n\t\t\treturn version[7:].lstrip(\"0.0.0.\")\n\telse:\n\t\treturn version\nimport numpy as np\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = pd\nimport numpy as np\nimport pandas\n\n\ndef get_nrow(d, column):\n    \"\"\"\n    Returns the next n-row from d where the column is the row\n    \"\"\"\n    return d[column]\n\n\ndef get_ncol(d, column):"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\tif version == 0:\n\t\treturn 0\n\telif version not in PATTERN_VERSIONS:\n\t\treturn 0\n\telif '-' in version:\n\t\treturn version\n\telse:\n\t\treturn PATTERN_VERSIONS[version]\n\n\"\"\"\nIf an optional pattern is not matched, the function is assumed to match the\nfirst version only. Otherwise the function is assumed to match the first version.\n\"\"\"\ndef is_required_version(version):\n\treturn version.split('-')!= ['?'][0].split('~')[0]\n\n\"\"\"\nIf the input version is not required,"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\treturn version + b':'\n\t\n\n# Next version has to be a zero-prefixed hash, therefore the only version\n# that it uses to store the hash can be\n_version = None\n\ndef parse_version(v):\n\t\n\t# check version\n\tif not _version:\n\t\treturn (int(v), None)\n\t\n\t# check version version\n\tv = v.decode(\"utf8\")\n\tif len(v)!= 4:\n\t\treturn (int(v), None)\n\telif v[0]!= _version:\n\t\n\t\treturn (int(v),"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tmajor, minor = [int(n) for n in version.split('.')]\n\texcept ValueError:\n\t\tsys.stderr.write(\n\t\t\t'ERROR: Cannot parse version from:'+ repr(version) + '.\\n'\n\t\t\t'Check that format is supported.\\n'\n\t\t\t'Format must be one of \"0\", \"1\", or \"2\" (or none).')\n\t\tsys.exit()\n\n\tv = int(major) << 40\n\tv &= ~major\n\t\n\tif v!= _RE_HEX_FORMAT:\n\t\traise ValueError"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\tversion = version[len(\"0x\"):version.find(\"-\")]\n\n\treturn version\n\n\ndef get_versions():\n\t\"\"\"\n\tReturn a string giving current versions available in the source\n\tserver.\n\t\"\"\"\n\tversions = os.scandir('versions/')\n\treturn versions\n\n\ndef parse(config):\n\t\"\"\"\n\tReturn a (version,versionlist,versionslist) tuple, where version is one of the\n\tmajor/minor/patch, separated by space.\n\t\"\"\"\n\tif not os.path.exists('src'):\n\t\treturn ('src',"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\ndef check_match_length(ver):\n\t\"\"\"\n\tReturn True if there is exactly one match.\n\t\n\tRaises IndexError if ver is not present in version.\n\t\n\t@return: True if there is an incomplete match.\n\t\n\t@raise IndexError: The first match of ver.\n\t\n\t\"\"\"\n\t\ndef check_match(version):\n\t\"\"\"\n\tMatch version, return version or None.\n\t\n\tRaises IndexError if the version was not present in version or nothing\n\t\n\t@param version: version identifier\n\t@param verify_version: verify that version has the expected format\n\t\n\t@return:"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\tversion = str(version)\n\t\t_number = version.split('-')[1]\n\t\t_match = _number_to_version._match('^' + _number + '|' + \\\n\t\t\t\t\t\tversion + '|' + _number_to_version._match(''))\n\t\tif not _match:\n\t\t\treturn '0'\n\n\ndef _match(pattern):\n\t\"\"\"\n\tCompare `pattern` to a number.\n\t\n\tRaises a ValueError for no match, and returns `version`.\n\t\"\"\"\n\t\tversion = str(pattern).split('-')[1]\n\t\t_number = version.split"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n#\tif(version[0]=='0' and version[1]=='*'):\n#\t\treturn version\n#\tif(version[0]=='*'):\n#\t\tnew_version=version[1:]\n#\t\treturn new_version\n#\telse:\n#\t\treturn version\n\n#\tif(not version==\"\":\n\t\n\treturn version\n\t\n\nif __name__=='__main__':\n\t''' This function is to test code for the next version.'''\n\tprint(\"Test Code Version: \"+VERSION);# -*- encoding: utf-8 -*-\n# Copyright ("}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\tm = re.match(r\"(?i)^(?!#)\\s*[\\s\\S]+\", version)\n\t\n\t\n\tif m:\n\t\n\t\t\n\t\treturn int(m.group(1))\n\t\n\telse:\n\t\tprint \"No version identifier for version: \" + version\n\t\n\tg.current_version = None\n\treturn None\n\n\ndef start_from_revision(revision_id, current_revision):\n\tglobal g\n\t\n\t\n\tfor key in [\"author\", \"maintainer\", \"title\"]:\n\t\tif revision_id in key:\n\t\t\tif key in g:\n\t\t\t\tif"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t\n\tfor i in _version_pattern.split('.'):\n\t\ttry:\n\t\t\tmodule = __import__(version)\n\t\texcept ImportError, e:\n\t\t\traise e\n\t\telse:\n\t\t\ttry:\n\t\t\t\tversion = getattr(module, \".\".join(i.split('.')[:-1]))\n\t\t\texcept AttributeError:\n\t\t\t\traise Exception(\"Couldn't import %s\" % version)\n\t\n\tif hasattr(version.LooseVersion, \"alpha\"):\n\t\tversion.LooseVersion = version.LooseVersion.alpha\n\telse:\n\t\tversion.LooseVersion = version.LooseVersion.alpha\n\ndef get"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n # This function is here to be called whenever the process handles\n # a version upgrade.\n\tdef handle_version_upgrade_error(self, message):\n\t\tif len(message) > 0:\n\t\t\tlogger.error(message[0] + '\\033[93m' + message[1])\n\t\telse:\n\t\t\tlogger.error('version upgrade failed!')\n\ndef main():\n\t\"\"\"\n\tProcess input from the repository, if necessary, and validate it\n\tif necessary.\n\t\"\"\"\n\ttry:\n\t\tsetup_logger()\n\t\t\n\t\tif not is_installed('"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\nclass Inventory(object):\n\t\"\"\"\n\tInventory base class.\n\t\n\tThese are methods to handle inventory (creating, removing, listing)\n\t(in the appropriate \"public\" form).\n\t\n\tInventory is designed to be used directly.  There are both public and\n\twritable products (products are \"public\", and other things are \"writable\").\n\t\"\"\"\n\t\n\tdef create_root_object(self):\n\t\t\"\"\"\n\t\tCreate an empty instance (if it does not exist yet)\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef create_validations_in_database(self):\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\ndef validate_version_inventories_non_empty(version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tUnnecessary, do not provide any pre and post processing here to\n\tvalidate it, but we will need to check them at every valid version.\n\t\"\"\"\n\t\ndef validate(input, inventory, root):\n\t\"\"\"\n\tValidate the input.\n\t\n\tInput is expected to be a list. The returned version is not necessarily\n\tequal to itself except at least the required version.\n\t\n\tinput should be a list. A list"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tinvalid_data_directory_paths = [\n\t\t\"^.*/manifest\\.xml$\", # should have a valid root manifest.xml.\n\t\t\"^manifest\\.xml$\", # should have at most one manifest.xml. It's a file.\n\t\t\".svn$\",\n\t\t\".svn/manifest.xml\",\n\t]\n\t\n\tversions_to_check = []\n\tfor path in version_dirs:\n\t\tif not os.path.isdir(path):\n\t\t\tcontinue\n\t\tfor (path2root, path2item) in os.walk(path):\n\t\t\tfor filename in version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tassert(not version_dirs), \"Inventory requires version_dirs to be a sequence of version dirs\"\n\t\n\tversion_sums = []\n\t\n\tfor path in version_dirs:\n\t\tfor filename in sorted(get_versions_in_path(path), key=validate_version_inventories_key):\n\t\t\tversion_sums.append([filename, path, version_dirs_in_path(path), 0])\n\t\n\tassert(len(version_sums)>0, \"Empty version entries in directory: \"+str(sum(version_sums)))\n\ndef get_versions_in"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tfor root_dir in version_dirs:\n\t\tversions = get_root_inventory_files(root_dir)\n\t\tif len(versions)!= 4:\n\t\t\tprint (\"Error: Bad Mandrillv inventory version file (version) in v4.\\n\" + \\\n\t\t\t\t\t\"Make sure you have 4 and 5 installed versions.\")\n\t\t\treturn\n\n\t\tfiles = check_sh_contains_v4_entries(root_dir)\n\t\tif len(files)!= 4:\n\t\t\tprint (\"Error: V4 inventory file has not atleast 4 entries in file \\\"{"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\tfor directory_path in version_dirs:\n\t\t\tdirectory_base = os.path.basename(directory_path)\n\t\t\tif directory_base in self.content_digest_dir:\n\t\t\t\t\n\t\t\t\t# The version directory must be in version sequence so that\n\t\t\t\t# we can check if the repository corresponds to a version and\n\t\t\t\t# that it matches.\n\t\t\t\tif not os.path.isdir(directory_path):\n\t\t\t\t\traise ValueError(\"version_dir '%s' not in %s\" % (\n\t\t\t\t\tdirectory_path, self.content_digest_dir))\n\t\t\t\n\t\t\tif not os."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\n\tdef get_inventory_from_version(self, version_dir, version=None):\n\t\t\"\"\"\n\t\tThe version is used to find out if version was specified as a \"1\", 2, 3...\n\t\t(the other two version SHOULD have an inventory if so)\n\t\t\"\"\"\n\t\n\n\t\t# check the existence\n\t\tif len(version_dirs) == 1:\n\t\t\t# we haven't found a version for this directory\n\t\t\traise Exception(\"No inventory for version: {0}\".format(version))\n\t\t# check the hash\n\t\tif not self.check_sha1"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tversion_dir_pairs_str=\"\"\"\t[]\n\t[] Version, [0.1.2.3]\t\tVersion 1\n\t[] Version 2, 0.1.2\t\tVersion 2\n\t[] Version 3, 1.1.2\t\tVersion 3\n\t\"\"\"\n\t\n\tfor root_dir, files in version_dirs:\n\t\tversion_dir_pairs = str(root_dir).split(\"/\")\n\t\tfor file in files:\n\t\t\tfile_info = os.path.join(os.path.dirname(file), file_info)\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n# TODO: make this class less strict\n\tdef __init__(self, version_dirs=None, file_versions_from_dirs=None):\n\t\tsuper(VersionInventory, self).__init__(version_dirs=version_dirs, file_versions_from_dirs=file_versions_from_dirs)\n\n\t@staticmethod\n\tdef _validate_version_inventories(versions, inv_file_hash):\n\t\tfor version in versions:\n\t\t\tversion_digest = Blocker.validate_blocker_version(fs.blocker_read(inv_file_hash,"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n###\nclass OCFLStorageException(Exception):\n\t\"\"\"\n\tBase exception class for all storage-related exceptions.\n\t\"\"\"\n\ndef abort_and_retry(func):\n\t\"\"\"\n\tDecorator to abort an OCFL Storage exception and retry\n\t\"\"\"\n\tdef retry(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tRetry the given function\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself._call_retries.append(self._retry_func)\n\t\texcept AttributeError:\n\t\t\tself._retry_func = func\n\t\traise self\n\treturn retry\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t        if '/' in path:\n\t\treturn path[:path.find(\"(\")]\n\t    return \"object\"\n\t\n\tif len(path)==0:\n\t\treturn \"root\"\n\n\tbase, extension = os.path.splitext(path)\n\tfor basefile in base.split(os.path.sep):\n\t\t    parts = path[len(basefile)+1:]\n\t\t    if parts[-1:]==extension:\n\t\t    parts.append(extension)\n\t\t \n    return base + \"object\"from __future__ import print_function, absolute_import\n\n\n\"\"\"\nTest that two types of data types"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\ttry:\n\t\t\tstatvfs(path).stat()\n\t\texcept IOError:\n\t\t\treturn 'root'\n\t\treturn 'object'\n\n\"\"\"\nThis module is for the main of the module. This should inherit from the\nCommandBase class. \n\nCommands:\n\tinit\tcreates a new storage root\n\tadd\tcreates an existing OCFL File\n\tedit\tremoves an existing OCFL File\n\tremove\tremoves an existing OCFL File\n\"\"\"\nclass CommandBase(object):\n\t\n\tdef __init__(self, options):\n\t\tself.options = options\n\t\t\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\tpaths = paths.split('.')\n\t\tif path in ('', '..'):\n\t\t\treturn 'root'\n\t\tif path in ('', '..', '//'):\n\t\t\treturn 'object'\n\t\tif path[0] == '':\n\t\t\treturn 'file'\n\t\tfor p in paths[1:]:\n\t\t\tpaths.pop(0, p)\n\t\treturn 'file'\n# Copyright 2013-2020 Lawrence Livermore National Security, LLC and other\n# Spack Project Developers. See the top-level COPYRIGHT file for details.\n#\n# SPDX-License-Identifier: (Apache-2.0"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\tif not path:\n\t\t\troot = root = None\n\t\t\tobject = None\n\t\t\tfile = None\n\t\t\tfor p in path.split('/'):\n\t\t\t\tif p.islower():\n\t\t\t\t\troot = p\n\t\t\t\t\tobject = p\n\t\t\t\t\tfile = p\n\t\t\t\t\tbreak\n\t\t\t\telif p.isupper():\n\t\t\t\t\troot = root.lower()\n\t\t\t\t\tobject = object.lower()\n\t\t\t\t\tfile = file.lower()\n\t\t\t\t\tbreak\n\t\t\tif root:\n\t\t\t\treturn root\n\t\t\telse:\n\t\t\t\treturn object\n\t\telse:\n\t\t\treturn type(path)\n\t\n\t\"\"\" A string with Nam"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\tif path == '*':\n\t\t\treturn os.getcwd()\n\ti = [0]\n\twhile i[0] == \"/\":\n\t\ti.pop(0)\n\t\n\twhile i[0] == \"/\":\n\t\ti.pop(0)\n\t\n\twhile i[0] == \"file\":\n\t\tif type(path) == types.StringType or isinstance(path, str):\n\t\t\tpath = path.strip()\n\t\ti.append(\"<* \\\"%s\\\"\" % path)\n\treturn \"root\"\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCopyright (c)"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t    \n    def check_paths(paths):\n        \"\"\"\n\tChecks the result of an OCFL CheckPaths call (check_paths()).\n\t\n        \n        Return value (True if the result of an OCFL CheckPaths is a file)\n\t\t(False if there is a check. False if something went wrong).\n\t\t\"\"\"\n        errors = False\n        if os.path.getsize(paths)!= 1:\n            errors = True\n            return False\n            \n        return True\n    \n    def check_files(files):\n\t\"\"\"\n\tCheck file access to the database of a file.  This function also creates\n\ta new entry in"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\n\t# We are assuming an inventory directory looks like an ontology entry...\n\t# So we have to check...\n\tif path == \"root\" or path == \"object\" or path == \"file\" or path == \"other\":\n\t\treturn \"root\"\n#\t\traise NotImplementedError(\"Only use this method for OO data\")\n\t\n\t# But if this looks like an ontology in a directory like OO_data.py,\n\t# look for the type of this...\n\tif path == \"root\" or path == \"object\" or path == \"file\":\n\t\tif"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\toecfl_type = oecfltypes[path]\n\t\n\tif oecfl_type:\n\t\ttry:\n\t\t\treturn oecfl_type.get_type_info(path)\n\t\texcept TypeError:\n\t\t\treturn 'root'\n\tif oecfl_type:\n\t\t\n\t\tif oecfl_type.get_type_info(path) == 'root':\n\t\t\treturn 'object'\n\t\tif oecfl_type.get_type_info(path + 'inventory') == 'file':\n\t\t\treturn 'file'\n\t\tif oecfl"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t    found = 0\n\t    for root, dirs, files in os.walk(path):\n\t        if not files:\n\t\t        continue\n\t        if path.startswith(root):\n\t   \t\t\t  found = 1\n\t        if \"root\" in dirs:\n\t    \t    found = 2\n           \t\t    dirs.remove(\"root\")\n\t   \t\tif not dirs:\n\t        continue\n\t\tif path.startswith(directories[\"object\"]):\n\t   \t\t\t  found = 3\n\t        if not dirs:\n\t        continue\n\t\treturn dir_type(path)\n\t\t\n\treturn \"root\"\n\t\n\ndef find_file_type(filename):"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tbugzilla_params = [\n\t\t\t'name',\n\t\t\t'url',\n\t\t\t'comment',\n\t\t]\n\t\tfor bug_id in bug_ids:\n\t\t\tvalues = params[bug_id]\n\t\t\tdata = urllib.urlencode(values)\n\t\t\tbugzilla_params.append(data)\n\t\tif bugid == \"BZ_BUG_ID\":\n\t\t\tfor k in bug_ids:\n\t\t\t\tbz_body = urllib.urlencode({'name': k})\n\t\t\t\tbz_code, bz_version = get_bugzilla_bugs_and_version(bz_body, bug_id"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tself.log(3, 'amend_bzparams: params: %s', params)\n\t\tresults = []\n\t\tfor bug_id in bug_ids:\n\t\t\tbug = self.get_bug(bug_id)\n\t\t\tbug = (bug and 1 or 0, bug)\n\t\t\tbug = bug[0]\n\t\t\tif not params.has_key('name') or not params['name']:\n\t\t\t\tparams['name'] = bug_id\n\t\t\tif not isinstance(bug['properties'], dict):\n\t\t\t\tbug['properties'] = {}\n\t\t\tbug.update(params['"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tif bug_ids:\n\t\t\tself._process_bug_ids(bug_ids)\n\t\t\n\t\tparams.update({\n\t\t\t'action': 'amend',\n\t\t\t'startnum': self.repo,\n\t\t\t'repoid': self.reponame,\n\t\t\t'bz_branch': self.bzbranch\n\t\t})\n\t\treturn params\n\n\tdef set_bzparams(self, bzparams):\n\t\tself.bugzilla_params = bzparams\n\n\tdef _process_bug_ids(self, bug_ids):\n\t\tBugzillaBugid.process_bug_ids("}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\tissue_date = get_date_diff(params.get('issue_date'), params.get('created_at'))\n\t\nclass Bugzilla(BaseBugzilla, BugzillaParameters):\n\t\"\"\"\n\tA Bugzilla class provides interface to all other Bugzilla systems, e.g.\n\tBugzilla bugs. Bugzilla can be created and updated\n\t\"\"\"\n\n    def get_bug_dict(self):\n\t\t\"\"\"\n\t\tRead the bugzilla parameters from the config file\n\t\t\n\t\t:rtype: dict\n\t\t:return: dict with the parameter names which are set and the value\n\t\t   "}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\tbugzilla = Bugzilla.bugzilla(bugzilla_username=params.bugzilla_username)\n\tif bug_ids:\n\t\tbug_id = bug_ids[0]\n\t\tparams['id'] = bug_id\n\tparams['version'] = '1.0'\n\treturn params\n\nclass Bugzilla(Postfixing):\n\t'''\n\tThe bugzilla server\n\t'''\n\n\tBugzilla_username = 'cielo.com'\n\t\n\t\n\t#TODO: Support new bugzilla version?\n\tdef send_request(self, bugs):\n\t\t'''\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\tissue_list = None\n\t# get the new issue\n\tif issue_list is None:\n\t\tissue_list = []\n\t\t\n\t# open in the user interface as well\n\tusername = self.gconf_client.config['gconf_login']\n\tuser_password = self.gconf_client.config['gconf_password']\n\tif username is None or user_password is None:\n\t\traise GConfError(\"No GitHub username and password provided\")\n\n\t# get the new issue\n\tif issue_list is None:\n\t\tissue_list ="}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tself.logger.info(\"Amend Bugzilla params: <%s>\" % unicode(params))\n\t\t\n\tdef amend_cc(self, bug_id, target_milestone, bug_idstring, ccc_from, ccc_to):\n\t\t\"\"\"\n\t\tAmend the CC\n\t\t\"\"\"\n\t\tself.logger.info(\"Amend CC: <%s> @ %s\" % (bug_id_to_cstring(bug_id), bug_idstring))\n\t\t\n\tdef amend_cc_to(self, bug_id, target_milestone, bug_id"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tparams_in_the_commit = bug_ids + ['commit']\n\t\tparams_of_the_commit = params.copy()\n\t\tfor bug_id in bug_ids:\n\t\t\tparams_of_the_commit.pop('bugid[%s]' % bug_id)\n\t\tfor param in params_in_the_commit:\n\t\t\tif params_of_the_commit[param].startswith('repository:'):\n\t\t\t\tif'repo' in params_of_the_commit[param]:\n\t\t\t\t\tparams_of_the_commit[param] = 'branch(%s)'"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tself.abbr.append('params')\n\t\tfor x in params:\n\t\t\tself.abbr.append(str(x))\n\t\t\t\n\tdef get_params(self):\n\t\tself.close(self.repo)\n\t\tparams = self.abbr\n\t\titempat_param = []\n\t\tparams.append(self.branch)\n\t\tparams.append(self.comment)\n\t\tparams.append(self.comment2bugnum)\n\t\tparams.append(self.revision)\n\t\tparams.append(self.bug_number)\n\t\tparams.append("}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\tself.add_param(params, bug_ids)\n\nclass GetbugsURL(Base):\n\turl = 'http://bugs.sourceforge.net/bugset/'\n\t_table_args = {\n\t\t'page' : 'none'\n\t}\n\n\n\tdef __init__(self, args=None):\n\t\tBase.__init__(self, args=None)\n\t\tself.add_table(BugzillaBugset, 'bugzillabugs')\n\t\tself.add_param(1, 'id')\n\t\tself.add_param('bug')\n\t\tself._table_"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\tresult = []\n\t\tfor key,value in nodes:\n\t\t\tif key=='tag:yaml.org,2002:str':\n\t\t\t\tfor n in deep_merge_nodes(value):\n\t\t\t\t\tresult.append((key,n,deep_merge_nodes(value)))\n\t\t\telse:\n\t\t\t\tresult.append((key,value,))\n\t\treturn result\n\n\n\ndef merge(source,filename,file_type='binary',line_format='%#5s',depth_limit=0):\n\n\t\"\"\"\n\tMerge the nodes given in a dictionary of (key, value) pairs and write the"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\t\n\tresult = collections.defaultdict(set, deep_merge_nodes)\n\t\n\tfor i, elem in [iter(nodes) for nodes in result.values()]:\n\t            \n\t    for k,v in elem.items():\n\t            \n\t    result[k|i]=result.get(k,set())\n\t        if i >= len(nodes):\n\t        nodes[i] = set()\n\t        nodes[i].add(k)\n\treturn result\n\t\n# ==============================================================================\n#\n#                                  \n# ==============================================================================\n\n@memoize\ndef load_node_tag(tag):\n\t\n\ttry"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\t#if isinstance(nodes, str):\n\t#\t\tnodes = [nodes]\n\t\n\t#for t in nodes:\n\t\t#if str(t) not in deep_merge_nodes(t.nodes):\n\t\t\t#\tDeepMergeError(\"configuration section \" + t.tag + \" \" + str(t.node) + \" is not deep merge.\")\n\t\t#\t\tnodes.append(t)\n\n\t#if node_whitelist_and_exclude_non_MappingNodes(nodes, deep_merge_nodes):\n\t\t#\tDeepMergeError(\"The following are dictionaries"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\t\t# convert the configs to a list of lists of tuples\n\t\tlst = []\n\t\t\n\t\t\n\t\t# for every combination of the nodes specified,\n\t\tfor knode in nodes:\n\t\t\t# each of these elements is a tuple of key:value (or lists of values)\n\t\t\t# for which the key is not present.  All values are lists\n\t\t\tlst.extend(deep_merge_nodes([\n\t\t\t\t\tv[0]\n\t\t\t\t\tfor v in knode.values()  # list of two-tuples to convert to lists, each of the 3 lists are keyed by the corresponding value (which"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\tret = []\n\t\ttry:\n\t\t\tif isinstance(nodes, list):\n\t\t\t\tfor v in nodes:\n\t\t\t\t\t# convert nodes to strings for easy access\n\t\t\t\t\tif type(v) is int:\n\t\t\t\t\t\tret.append('{0}_{1}'.format(v, nodes[v]))\n\t\t\t\t\telse:\n\t\t\t\t\t\tret.append('{0}_{1}'.format(v, nodes))\n\t\texcept BaseException:\n\t\t\tpass\n\t\t\n\t\treturn ret\n\t\n\t@classmethod\n\tdef _merge_nodes(cls, nodes):\n\t\tret = []\n\t\tfor v in nodes:\n\t\t\tret.append({"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n    def merge_node(node):\n        tags = []\n        # First, we go through the nodes, in the order they occur.  If we\n        # are processing more than one (or all) node, we just use the value for the\n        # keys we want to process.  If it's a mapping, then we can't take the\n        # whole bunch of subsections further into consideration, so as to allow up to\n        # 8 elements of the subsections.  For example, to deal with this, we\n        # could use a node value from a borgmatic configuration data structure.  A \""}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    # First, iterate through all nested borgmatic options.\n\n\tfor node_option_index, node in enumerate(nodes):\n\t    child_node = nodes[node_option_index]\n\t    node = deep_merge_node(node_node, deep_merge_nodes(child_node))\n\n\t# Second, try to find a matching configuration file.\n\n\t# The first, find the first configuration section that is matched by the configuration line.\n\tindex = 0\n\tnode_match = re.search('r\"(.*)\"\\s*\\*\\|', node['tag'])\n\tfor line in"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t   \n\tnew_list = []\n\tfor node, value in six.iteritems(nodes):\n\t    if isinstance(node, six.string_types):\n    \t    new_list.extend(deep_merge_nodes(value))\n\t    else:\n\t\tnew_list.append(DeepMergeNode(node, value))\n\treturn new_list\n\n\ndef check_config_file_exists(filename):\n\tif os.path.exists(filename):\n\t    return True\n\telse:\n\t    return False\n\n\ndef copy_dict_file(dict_file, template):\n\t\"\"\"\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tdeep_merge = None\n\tfor node in nodes: \n\t\tif isinstance(node, tuple) and len(node) == 2:\n\t\t\tnode, deep_merge = deep_merge_nodes(node)\n\t\telif isinstance(node, str):\n\t\t\tnode = node.split(\".\", 1)\n\t\n\treturn deep_merge\n\n######################################################################\n#####################################################################\n# Function to write the values the yaml config to a file.\n\ndef write_values_to_file(filename, values, encoding=sys.getdefaultencoding(), pretty=False):\n\t\"\"\"\n\tGiven a filename,"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tnew_dict = {}\n\tfor (key, value) in nodes.items():\n\t    if isinstance(value, dict):\n\t        new_list = deep_merge_nodes(value.flatten())\n\t            if new_list:\n\t                nodes = OrderedDict({k:new_list for k in values})\n\treturn nodes\n\n# ==============================================================================\n# main\n# ==============================================================================\n# ==============================================================================\nif __name__ == \"__main__\":\n\tmain()\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Sep  7 22:39:01 2014\n\n@author"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\tusage = \"usage: %prog [options]\"\n\n\n\targuments = parser.parse_args(arguments)\n\tparser.exit()\n\n\n\ndef test():\n\tparser = argparse.ArgumentParser() \n\tparser.add_argument(\"input\", type=argparse.FileType(\"w\"))\n\tparser.add_argument(\"-m\", \"--mode\", type=int, \n\t        help=\"the mode to use\")\n\tparser.add_argument(\"-c\", \"--clean\",\n\t        help=\"clean the created profile\",\n\t        action=\"store_true\",\n\t        default=False)\n\tparser.add"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\narguments_str = ''\nif arguments:\n\targuments_str += arguments\n\tArgumentParser(add_help=False).parse_args(arguments_str)\n\n\"\"\"\nThis module adds some more utilities for creating a Keras VTK widget.\n\"\"\"\n\nimport json\nimport os\n\nfrom django.core.urlresolvers import reverse\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom kolibri.core.content.models import KolibriObject\n\ndef add_static_url_to_url(url):\n    \"\"\"\n    Add a static URL to"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=__doc__.split('\\n')[0],\n\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\tprog=argparse.get_prog_name(arguments[0]),\n\t\t\tdescription=arguments[1],\n\t\t\tepilog=arguments[2],\n\t\t)\n\n\t\tcommandline_argument_list = parser.parse_args(arguments)\n\n\t\treturn parser\n\ndef run_query(query_file):\n\t\"\"\"\n\tIt queries a JSON-file with a hash of data, and returns the json-formatted result.\n\n\tIn this function"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ntry:\n\tparser = ArgumentParser(\"python create-static-tables.py\")\n\tparser.set_defaults(func = parse_args)\t\nexcept BadArgument:\n\tparser.print_help()\n\tparser.print_version()\n\tsys.exit(2)\n\ndef main():\n\t\"\"\"\n\tmain method of this script is all here, but with some sanity checking of arguments and usage. \n\t\n\tArgs:\n\t\toptions - this is the options object as returned by parser.parse_args()\n\t\"\"\"\n\tprint(\"create-static-tables.py"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='Get information about the specified directory.')\t\n\n\tparser.add_argument('-d', '--directory',\n\t\t\t\t\t\tdefault=DEFAULT_HOME_DIR,\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\thelp='path to the directory. Defaults to DEFAULT_HOME_DIR')\n\t\n\tparser.add_argument('-p', '--project',\n\t\t\t\t\t\tdefault=PROJECT,\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\thelp='the project name/package ID to create/remove')\n\n\tparser.add_argument('-t', '--type',\n\t\t\t\t\t\tdefault='package',\n\t\t\t\t\t\tchoices=('project"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tusage = \"usage: %prog [OPTIONS] [TARGET]\"\n\t\tparser = ArgumentParser(description=usage)\n\t\tparser.add_argument(\"-c\", \"--configure\", dest=\"configure\",\n\t\t\thelp=\"configure/build the build environment.\", metavar=\"COMMAND\")\n\n\t\tparser.add_argument(\"-P\", \"--path\", dest=\"path\",\n\t\t\thelp=\"path to the build program.\", metavar=\"PATH\")\n\n\t\tparser.add_argument(\"-s\", \"--size\", dest=\"size\", type=int,\n\t\t\thelp=\"size of the build program.\", metavar=\"SIZE\")\n\n\t\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\targs = parse_arguments()\n\tprint \"Args: [\" +str(args.command)]\n\treturn args\n\n\n\"\"\"\ndef parse_arguments_with_default_arguments():\n\tparser = argparse.ArgumentParser();\n\t\t\n\tfrom sfepy.base.selectivity.base_selectivity import BaseSelectivity\n\timport sfepy.base.selectivity.sfe as sfe\n\n\tparser.add_argument('command', type=str, choices=['add_domain', 'add_domain_subtraction', '"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = ArgumentParser(description='Parse arguments and run an EDFLM test job')\n\t\tparser.add_argument(\"input_filename\", type=str, help=\"The Illumina test test Illumina input file\")\n\t\tparser.add_argument(\"-l\", \"--logfile\", type=str, help=\"Log file name (stdout, stderr)\")\n\t\tparser.add_argument(\"-v\", \"--version\", type=str, help=\"Log version of this script (default=3.17)\")\n\t\tparser.add_argument(\"-V\", \"--verbose\", action=\"store_true\","}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument('-c', '--cache', dest='cache', action='store',\n\t\t\t\t\t\t  default='false', nargs='?',\n\t\t\t\t\t\t  const='true', help='Output the cache to.')\n\t\tparser.add_argument('--cache-dir', dest='cachedir', action='store',\n\t\t\t\t\t\t  default='data',\n\t\t\t\t\t\t  const='true', help='Output an output file in the same directory/file')\n\t\tparser.add_argument('--output-file', dest='outputfile', action='store',\n\t\t\t\t\t\t  default='output.txt', help='Output a file in"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\n\t# Parse the required arguments\n\targuments = parser.parse_args(arguments)\n\n\n\t# If the given option -k flag is not valid, it throws an error.\n\tif arguments.options is not None and arguments.options.get(\"k\"):\n\t\tprint(\"ERROR: Missing option '\" + arguments.options.get(\"k\") + \"'\")\n\t\texit(2)\n\n\t# If the given option -v flag is not valid, it throws an error.\n\tif arguments.options is not None and arguments.options.get(\"v\"):\n\t\tprint(\"ERROR: Missing option"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument('--list', action='store_true','store_true')\n\t\tparser.add_argument('--show', action='store_true')\n\t\tparser.add_argument('--dry-run', action='store_true')\n\t\tparser.add_argument('--output', dest='output', default=\"./output\",\n                            type=str,\n                            help='Set the output path for debugging purposes')\n\tdef parse_args(parser):\n\t\targs = parser.parse_args()\n\t\treturn args\n\t\treturn args.list and not args.show\n\t\t#return args."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t # Make a copy so we can add them back later on to handle it appropriately\n\t sanitized_parser = copy.deepcopy(parser)\n\t sanitized_parser.prog = argparse.get_prog_name(\"parse_flags_%s\" % parser.prog)\n\t\t\n\t return sanitized_parser.parse_args()\n\t\n\nclass Family(object):\n\t\"\"\"\n\tRepresents an Family and a reference to it (or the parent of it)\n\t\"\"\"\n\tdef __init__(self, name, parents_family=None):\n\t\tself.name = name\n\t\t\n\t\tself.children = {'Family': self"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\toptions = parser.parse_args()\n\treturn options\n\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser(description=\"A simple command line tool for setting up the project using git.\")\n\t\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\", help=\"be verbose\",\n\t\ttype=str, metavar=\"verbose\", default=None\n\t)\n\t\n\tparser.add_argument(\n\t\t\"-r\", \"--recursive\", help=\"return all subdirectories with all of them.\"\n\t)\n\t\n\targs = parser.parse_args()\n\t\n\tif not os"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument('--crack', action='store_true', default=False, help=\"Enable to crack the path in the CWD\")\n\t\tparser.add_argument('--use', '-r', default='git', help=\"Use git to look up git files (default: git)\")\n\t\tparser.add_argument('--repo', '-r', default=None, help=\"Git repo to use (default: git)\")\n\t\tparser.add_argument('--tag', '-t', default=None, help=\"Branch tag\")\n\t\treturn parser.parse_args()\n\nif __name"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser = parser.parse_args(sys.argv[1:])\n\t\treturn parser.parse_known_args()\n\n\ndef do_help(parser, args):\n\tif args.quiet:\n\t\tparser.print_help()\n\t\treturn 2\n\n\tif args.verbose:\n\t\tsys.stdout.write('\\033[1;32m\\033[1;34m Usage:\\033[0m\\n')\n\telse:\n\t\tsys.stdout.write('\\033[0;32m\\033[0m')\t\t# Print help text \n\t\timport argparse\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\t# If this is a positional argument, we use a single space. The first section\n\t\t# is the set of flags to pass to the parser.\n\t\tparser.add_argument('--foo', dest='foo_id', nargs='+', help='The id of the foo.')\n\t\tparser.add_argument('--bar', dest='bar_id', nargs='+', help='The id of the bar.')\n\t\tparser.add_argument('--baz', dest='baz_id', nargs='+', help='The id of the baz.')\n\t\t\n\tdef parser(self, namespace, args):\n\t\targs ="}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef main(argv = None):\n\t\"\"\"\n\tParse the command line options, starting with command line options.\n\t\"\"\"\n\n\t# Create an object of type \"ArgumentParser\"\n\targv, parser = parser_helpers.get_parser_arguments(argv)\n\targs = parser.parse_args()\n\t\n\t# Store the command line flags that were provided\n\targs.parser = parser\n\tif args.verbose:\n\t\tprint_parser_usage()\n\t\tprint_parser_help()\n\n\t# Let's see if there is a file open whose name has the argument\n\tif"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tif not parser.add_argument(\"--type\", dest=\"type\", action=\"store\",\n\t\t\t choices=[\"simple\"], default=\"simple\"):\n\t\t\t\tparser.print_help()\n\t\t\t\treturn None\n\t\t# Add the --type option at the front to catch \"simple\" syntax\n\t\tparser.add_argument(\"--type\", dest=\"type\", action=\"store_true\",\n\t\t\t\tdefault=False, help=\"Only write out type (like cargo).\")\n\t\targs = parser.parse_args()\n\t\treturn args\n\ndef parser_settings(parser):\n\treturn parser.parse_args() if"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument(\"--config\", type=str, default=\"\", help=\"Path to configuration file. Default: configuration file path.\")\n\t\tparser.add_argument(\"--config-file\", type=str, default=\"\", help=\"Path to configuration file. Default: database file.\")\n\n\tdef flags_to_conf(flags, parser):\n\t\tconf_file = flags.config_file\n\t\tif conf_file.endswith(\".conf\"):\n\t\t\tfd = open(conf_file, \"r\")\n\t\t\tconf_content = fd.read().replace(config_file, \"\")\n\t\t\tfd.seek"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef flags_from_flags(flags):\n\t\"\"\"\n\tGiven each flag as returned by argparse.parse_args(), return those flags in any string,\n\twhich may or may not be present.\n\t\"\"\"\n\n\t\ndef parser_description(description):\n\t\"\"\"\n\tGiven a argparse.ArgumentParser instance that describes its usage as returned by argparse.ArgumentParser\n\tinstallations from argparse/config/main.py.\n\t\"\"\"\n\treturn description\n\n\n\ndef main():\n\t\"\"\"\n\t\n\tParse options and parse command-line options:\n\t\tparse_args()\n\n\t\tParse the command"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\targv = unparsed_arguments\n\targ_parser = ArgumentParser()\n\tsub_parser = subparser = argparse.ArgumentParser()\n\tsub_parser.add_argument(\"arg\", metavar=\"SUBCOMMAND\")\n\tsub_parser.add_argument(\"--verbosity\", help=\"verbosity level or verbose message\", metavar=\"v\")\n\tsub_parser.add_argument(\"args\", metavar=\"ARGS\")\n\n\treturn arg_parser.parse_known_args(argv)\n\n\nif __name__ == \"__main__\":\n\t# if this test fails, print usage and exit\n\t# else print usage"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\t# If user has specified a namespace, parse it recursively.\n\tif args and len(args) > 0 and 'name' in args:\n\t\t\n\t\t# If the user defined an optional namespace, parse it recursively.\n\t\tif args:\n\t\t\tif args[0] not in ns.namespaces:\n\t\t\t\targs.insert(0, 'global')\n\t\t\t\tns = ns.namespaces['global']\n\t\t\t\n\t\t\tsubparser = ns.subparser(args[0], parser_name=args[1])\n\t\t\t\n\t\t\t# Pass along any unknown options -- make sure they exist,\n\t\t\t# and convert to"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tsubparser = getattr(argparse, args[0], None)\n\t\tif subparser is None:\n\t\t\tsubparser = globals()[\"GLOBAL_NAMESPACE\"]\n\t\targs = subparser.parse_known_args(*unparsed_arguments)\n\t\treturn args\n\t\n\treturn parse_arguments(*args)\n\ndef configure_logging(logfile_path):\n\t\"\"\"\n\tInitializes the console logger, and write all output logs to the console log.\n\t\n\tParameters:\n\t\tlogfile_path (str): where the output should be written to.\n\t\"\"\"\n\t\tsys.stderr = log.get_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\nparser = argparse.ArgumentParser(description=\"parse the arguments\")\nparser.add_argument(\"args\", nargs=argparse.REMAINDER)\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"-f\", \"--for\", required=True)\ngroup.add_argument(\"--no-for\", required=True)\nsubparser = parser.add_subparsers(title=\"command-line arguments\")\n\n# The --namespace argument, so we can look in the arguments with a space:\nparser.add_argument(\"--namespace\", \"-g\")\n\ncmd ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\nparser = argparse.ArgumentParser(description=\"Extract information from a file. By default it extracts information from files named as.mo files.\")\n\t\nif __name__ == \"__main__\":\n\tparser.add_argument('mo', help='Path to.mo file')\n\targs = parser.parse_args()\n\tif args.mo == None or len(args.mo) < 1:\n\t\tprint(\"missing.mo file, please specify the file name\")\n\t\texit()\n\tprint(\"extract\", args.mo)\n\"\"\"This file contains classes and functions to interface with\na"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='Parse arguments passed by --%s: %s',\n\t\t\tepilog=description,\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t\t)\n\t\t# Add required arguments to the parser object.\n\t\tparser.add_argument('-v', '--verbose',\n\t\t\taction='store_true',\n\t\t\tdest=\"verbose\",\n\t\t\tdefault=False,\n\t\t\thelp=\"print debug messages\",\n\t\t)\n\t\tparser.add_argument('-n', '--netrc',\n\t\t\taction='store_true',\n\t\t\tdest=\"netrc\",\n\t\t\tdefault="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(description='Create and register a function to manage an argument parser.\n\t\"\n\t\"Args:\t\"\n\t\"Options:\t\"\n\t\"Arguments:\tOptions.\n\tOptions.\n\t\"Subcommand line arguments:\n\t\tArguments[0] (the argument name)\t-- a positional argument\n\t\tArguments[1] (the argument definition)\t-- a positional argument\n\t\"Subcommand options:\n\t\t\tOptionally, all subcommands and their arguments or options.\n\t\t\"Options.\" (This is also an \"instance attribute\" -- this allows"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\t_options, unparsed_arguments = [], unparsed_arguments\n\t\tfor arg in unparsed_arguments:\n\t\t\toption, = opt_to_namespace(arg)\n\t\t\tif option.dest not in arguments_help:\n\t\t\t\traise argparse.ArgumentError(argparse.SUPPRESS)\n\t\t\targ.dest = arguments_help[option.dest].dest\n\t\t\tif option.store:\n\t\t\t\targ.nargs = [str(_x) for _x in option.nargs]\n\n\treturn _options, unparsed_arguments\n\ndef _get_all_subparser_names(subparsers, args, action_name"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\t# Command argument \"name\" is always the empty string\n\toptions = {}\n\targv = unparsed_arguments\n\t\n\targs = parse_arguments(argv)\n\t\n\tfor k, v in argv.items():\n\t\t\tif k.lower() == 'global':\n\t\t\t\tname = \"global\"\n\t\t\telif k =='subcommand':\n\t\t\t\tsubcommand = v\n\t\t\telif k == 'name':\n\t\t\t\tname = v\n\t\t\telif k in ['command-line-arguments', 'command-line-argument','subcommand', 'name']:\n\t\t\t\traise Exception(\"Command line argument not"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tif not unparsed_arguments:\n\t\t\treturn {}\n\n\t\tparsed_arguments = {}\n\n\t\tparsed_arguments['argument_list'] = [name.strip(' \\t \\t\\r\\n\\r ') for name in unparsed_arguments[0]]\n\t\tparsed_arguments['argv'] = [argv for argv in unparsed_arguments[0]]\n\t\tparsed_arguments['default'] = unparsed_arguments[0].default\n\t\tparsed_arguments['help'] = {\n\t\t\t'action' : unparsed_arguments[0].help,\n\t\t\t'version' : unparsed_"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=\"Perform subparser subparser(s).\"\n\t\t\t\t\t \"  Must also be called from the subparser parser.\")\n\t\n\t\n\tsubparser_names = set()\n\tsubparser_parser = None\n\t\n\t# The type of subparser to be used for all argument values. This gets set by subparser\n\t\n\tdef process(self, subparser):\n\t\t\"\"\" \n\t\tSubparser processing.\n\t\t\n\t\tArguments:\n\t\t--parser: The argparse subparsers parser instance.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Make sure the parser instance exists and have all values set.\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\trepository = args_to_argv(args)\n\t\n\tsubparsers = tuple(sorted(r.name for r in subparsers if r.name.startswith('args')))\n\t\n\tparsed = [name for name in unparsed_arguments]\n\t\n\tfor subcommand_name in subparser_names():\n\t\tsubparser = subparsers.get(subparser_name)\n\t\tsubparser_args = parse_subparser_arguments(subparser, subcommand_name)\n\t\t\n\t\tif len(subparser_args) == 0 and subcommand_name in repository:\n\t\t\tparsed.append(argparser.parse_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\ndef parse_subparser(subparser_name, subparsers):\n\t\"\"\"\n\tGiven a positional argument (a string) and a list of arguments that are not\n\tsubparser names, return a namedtuple of the args.\n\t\n\tArgs (a dict mapping from subparser name to a parsed namespace of args, a list\n\tof remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\t\t\n\tsubparser = subparsers[subparser_name]\n\trequired = []\n\tif len(subparser.args) == 1:\t# Single argument\n\t\trequired = args\n\telse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tparsed_args = []\n\tsubparsers['subparser_arguments'] = {None: parse_argument}\n\tif unparsed_arguments[0]:\n\t\tparsed_args.append(unparsed_arguments[0])\n\t\tsubparsers['subparser_arguments'].update(ungrouped(unparsed_arguments[0].split(' ')))\n\t\n\treturn parsed_args, subparsers\n\nclass Subcommands(object):\n\t\"\"\"\n\tGiven the namespace of arguments and a parent parser list containing subparser names and\n\targuments this function will return an subparser instance, and has the methods"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n#\tparser of all required subparser arguments.\n\tsubparser_args = []\n\tfor arg_dict in unparsed_arguments.values():\n\t\t#print '--subparser arg= %s' % arg_dict['name']\n\t\tparser = argparse.ArgumentParser(\n\t\t\tprog='subparser_subparser',\n\t\t\tdescription='Subparser used when setting up the subparser.'\n\t\t)\n\t\t\n\t\t#the subparser name, must be globally unique\n\t\tparser.add_argument ('--name', metavar='subparser_name', help='The name of the main subcommand')\n\t\t\n\t\tsub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tsubparser_dict = {}\n\t\n\t\n\tfor parser in subparsers:\n\t\tsubparser = parser.add_subparsers(metavar = '-x', dest = 'command', help='subparser handler')\n\t\tfor arg in unparsed_arguments:\n\t\t\tsubparser_dict[subparser.parser_name.split('.')[0]] = \\\n\t\t\t\t\tArgumentParser(title=argparse.SUPPRESS)\n\t\t\t\n\t\tfor subparser_name in unparsed_arguments:\n\t\t\tsubparser = subparser.add_parser(subparser_name, \n\t\t\t\thelp = 'argument {0} not optional'.format"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\ndef _remove_namespace(subparser):\n\treturn [arg for arg in subparser.option_strings if arg.lstrip_prefix() == \"\"][0]\n\ndef _parse_command_line_and_arguments(subparser, args):\n\tif not args.parse_command_line:\n\t\treturn None, None, None\n\t\n\t# Get subparser name\n\tsubparser_name = _remove_namespace(subparser)\n\t\n\t# Parse subparser arguments from subparser name\n\targ_count = 0\n\tfor arg in args:\n\t\targ_count +="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tsubparser_names = []\n\tsubparser_names += [name.replace(\".\", \"_\").replace(\" \", \"_\") for name in get_subparser_names() ]\n\t\t\n\tsubparser_args, subparser_kwargs = parse_subparser_arguments(unparsed_arguments, subparsers)\n\tsubparser_names += [name.replace(\".\", \"_\").replace(\" \", \"_\") for name in get_subparser_args()]\n\tsubparser_kwargs += [\n\t\tname for name in subparser_names\n\t\tif hasattr(subparser, name)\n\t]\n\t\n\tdef parse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser.add_subparsers(**parser_kwargs(unparsed_arguments, subparser))\n\t\n\treturn (None, [])\t\n\t\t\ndef _get_default_options(sys_path):\n    \"\"\"\n\tConstruct the default behavior for the sys.path.  If it's a standard Python\n\tpath, then do not modify sys.path\n\t\"\"\"\n    import sys, imp\n\tif not sys_path:\n\t\tsys.path.insert(0, os.path.expanduser('~/.git/pygit"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tsubparser_arguments = [arg[1:] + (\".\" if arg.startswith(\"-\") else arg) for arg in unparsed_arguments]\n\t\tparser_subparsers = [subparser + \"-\"]\n\t\t\n\t\t# In the same way, any subparser named \"parser_subparsers\" is not found\n\t\t# in the argument parser that were defined in the commandline\n\t\t# or parsed by the command line, so we need it:\n\t\tparser_subparsers = [f for f in parser_subparsers if f is not None]\n\t\t\n\t\t# Construct a sequence of subparser_actions for each"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = subparsers.add_parser(\"build\", help=\"build documentation for a language\")\n\t\tparser.add_argument(\"lang\", type=str, nargs=\"+\", help='Language to build docs.')\n\t\treturn parser\n\n# -*- coding: utf-8 -*-\n\"\"\"\n    pygments.lexers.cpp\n    ~~~~~~~~~~~~~~~~~~~~~~~\n\n    Lexers for C++ language (C) source code.\n\n    :copyright: Copyright 2006-2015 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\nfrom pygments.lexer import Lexer, RegexLexer,"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\n\tparser = subparsers.add_parser(\"get_data\", help=\"Get raw data.\",\n\t\tdescription = \"Returns raw data from Google API. All JSON encoded JSON\",\n\t\tusage = \"usage: get_data <path_or_url>\")\n\n\tparser.set_defaults(func=get_data)\n\n\tparser.add_argument(\"query\", type=str,\n\t\tchoices = [\"Get data\", \"Run command\"])\n\n\tparser.add_argument(\"path_or_url\", type=str,\n\t\thelp=\"Searching API paths with <http://address/ or <"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n#\tGet all sub parsers\n\tparser = build_parser('commands/cli_commands.py', 'help', 'commands/cli_commands_help.py')\n\t# Set default actions\n\tparser.add_argument('command_command', help='Command to help')\n\tparser.add_argument(dest='command_function', help='Function name')\n\tparser.add_argument(dest='command_id', help='ID of command')\n\tparser.add_argument('-s', dest='enable_help',\n                        help='Enable help commands', action='store_true"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n    def parse_args(opts):\n    \tif len(opts) == 2:\n    \t\t\topts = getopt.getopt(opts, 'hhp:o:r:b:m:t:s:n:o:p:')\n    \t\t\tif ((len(opts)!= 2 and not opts[0] == \"-c\") or\n\t\t\t(len(opts)!= 1 and (not opts[0].startswith(\"-c\") or opts[0].startswith(\"-h\") ))):\n#--------------------------------------------------\n\t    opts += [\"help\", \"help1\", \"read\", \"help\"]\n\t    return opts\n    \t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\thelp = argparse.ArgumentParser(prog=\"./run_tests\",\n\t\tdescription=\"Build and run all unit tests\")\n\n        # the argument parser has the following options:\n\t\tparser = help.add_subparsers(title=\"Arguments\", description=\"These take the form: -c <config>\")\n\t\tparser.add_argument('-c', '--config', default='', type=str,\n\t\thelp=\"Where configuration to use\")\n\t\tparser.add_argument('-o', '--output', default='-', type=str, \n\t\thelp=\"Where to output the results\")\n\t\tparser.add_argument('-"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\tparser = ArgumentParser()\n\tparser.description = \"Process the commands passed into Python.\"\n\tparser.add_argument('-h',\n\t\t\t'--help',\n\t\t\thelp=\"Show this help message and exit.\",\n\t\t\tdest = 'help')\n\t\n\tparser.add_argument('-q',\n\t\t\t'--quiet',\n\t\t\taction=\"store_true\",\n\t\t\thelp=\"Do not print this help message and exit.\",\n\t\t\tdest = 'quiet')\n\n\tparser.add_argument('-s',\n\t\t\t'--stdin',\n\t\t\taction=\"store_true\",\n\t\t\tdefault=None,\n\t\t\thelp"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef add_argument(parser, *args, **kwargs):\n\tparser_class = kwargs.pop('parser_class')\n\targparsers = parser.add_subparsers(**kwargs)\n\n\t# Add argument parsers\n\trequired_args = list(set(kwargs.pop(REQUIRED_ARG, [])))\n\tfor a in REQUIRED_ARGS:\n\t\trequired_args.remove(a)\n\n\tif type(required_args) is not list:\n\t\traise ValueError('Required argument must be a list.')\n\tfor a in required_args:\n\t\t# Add the parser object"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = ArgumentParser(\n\t\t\tadd_help=True,\n\t\t\tdescription=\"Build a top-level parser.\")\n\n\t\tcommand_group = parser.add_argument_group('command', description=\"Manage command groups.\\nThe default is to build documentation from the given source.\")\n\n\t\tcommand_group.add_argument(\"-v\", '--verbose', action='store_true',\n\t\t\thelp=\"enable verbose mode. (default=False).\")\n\t\t\n\t\tcommand_group.add_argument(\"-t\", '--tree', dest='root',\n\t\t\thelp='specify a source tree. (default: root)')\n\t\t\n\t\tcommand_"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\nclass Parser(BaseParser):\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\tself._parser_factory_args = []\n\t\tif _verbose:\n\t\t\tprint '*'+ self.__class__.__name__\n\t\t\n\t\tif self._output_dir is not None:\n\t\t\toutput_dir = self._output_dir\n\t\t\tif _verbose:\n\t\t\t\tprint 'output directory (' + output_dir + ')'\n\t\t\tself._parser_factory_args.append('-o')\n\t\t\toutput_dir = os.path.dirname(output_dir)\n\t\telif _verbose:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Convert a bam file to a tab-separated tab-delimited tab-delimited file.')\n\t\tparser.add_argument('-S', '--shift-count', type=int, default=1, help='number of shift to trim, 0 for whole file')\n\t\tparser.add_argument('--bin-count', type=int, default=1, help='number of bins to extract from each file')\n\t\tparser.add_argument('infile', nargs='+', help=\"Informative tab-separated tab-delimited file paths\")\n\t\treturn parser.parse_args"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tdef flatten(l):\n\t\tnewl = []\n\t\tfor element in l:\n\t\t\tif element is None:\n\t\t\t\tnewl.append(element)\n\t\t\telse:\n\t\t\t\tflatten(element)\n\t\t\t\tnewl.append(element)\n\t\treturn newl\n\t\n\t\n\tdef merge_to_subtree(tree2):\n\t\t\"\"\"\n\tConvert node lists into subtree lists and return a list of a list of the\n\tinstances corresponding to the merged node lists. Tree_of_type is a tuple that\n\trepresents the node lists with a subtree node, and tree_of"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\t\n\t# convert nested key tuples into an ordered list\n\titems = [(key, value) for key, value in items]\n\tnew_items = []\n\t\n\tfor d in nodes:\n\t\t# get rid of non-ScalarNode\n\t\tfor k in d.keys():\n\t\t\tif d[k] in nodes.keys():\n\t\t\t\tdel d[k]\n\t\t\n\t\ttag = '.'.join(d.keys())\n\t\t\n\t\titems += [\n\t\t\t(\n\t\t\t\tScalarNode(tag, value=value, key=key, key_type=KeyType.TAG_STRING)\n\t\t\t)"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n    def make_key_list(nodes):\n\t'''\n\tCreate a list of sorted, deterministic keys on the node list.\n\n\t>>> d = deep_dict({'a': {'b': 'c'}})\n\t>>> nodes = d.make_key_list(nodes, ('a', 'b'))\n\n\tA single key can be serialized as a file, and all keys are sorted on its\n\telement by the key specified in the yaml representation of that key.\n\t\n\t>>> keys = d.make_key_list('keys', ('key1', 'key2'))\n\t\n\tA key"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tresult = []\n\t\n\tif len(nodes) > 1:\n\t\tresult.append(merge_nodes(nodes))\n\t\n\tfor node in nodes:\n\t\t#print node.tag\n\t\t#print type(node.value)\n\t\t\n\t\tif type(node.value)!= ScalarNode:\n\t\t\tresult.append((node.tag, node.value))\n\t\n\tresult.reverse()\n\t\n\treturn result\n\n\n\n# -*- coding: utf-8 -*-\n# This file is a part of beets.\n# Copyright 2016, Adrian Sampson.\n#\n# Permission is hereby granted"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\tmerged_data = []\n        nodes.append((node.tag, node))\n\t\tmerged_data.extend(deep_keyed_merge_nodes(node.values))\n\t\tfor key, value in nodes[0]:\n\t\t\tif isinstance(value, BaseScalarNode):\n\t\t\t\tmerged_data.append((key, value))\n\t\treturn nodes[0] + tuple(merged_data)# -*- coding: utf-8 -*-\n\"\"\"\n/***************************************************************************\nName                 : DB Manager\nDescription          : Database manager class with QGIS\nDate                 : May 23, 2011\ncopyright            : ("}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    \n\tmy_dict = {}\n\t#my_dict = deep_merge(nodes, deep_merge_nodes(my_dict))\n\t\n\tfor key, value in my_dict.iteritems():\n\t    if isinstance(value, MappingNode):\n\t\tmy_dict[key] = deep_merge(value, deep_merge_nodes(my_dict))\n\t\n\t#print \"my_dict\"\t\t\n\treturn my_dict\n\ndef find_by_tag(node):\n\t\"return the best path with the node that matches the tag.\"\n\t\n\tfor key, values in my"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t    if isinstance(nodes[0], tuple):\n\t    for node in nodes:\n\t        for i in node.value:\n\t        del node.value[i]\n\t    return nodes[:]\n\n\tif isinstance(nodes[0], Node):\n\t    value = nodes[0].value.copy()\n\telse:\n\t    (value, ) = nodes[0].value.copy(), nodes[0].value\n\td = deep_merge_nodes([value])\n\treturn [d]\t\n\n\ndef recursive_iter_and_merge_nodes(nbunch, nodes=None):\n\t\"\"\"\n\tGiven"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n#-----------------------------------------------------------------------------\n# Code\n#-----------------------------------------------------------------------------\n\ndef deep_merge_dict(data):\n\t\"\"\"\n\tPerform deep merge of the given dict and return a dictionary of dictionaries.\n\t\"\"\"\n\n\tnode_dict = dict(copy.deepcopy(data))\n\tfor key, value in node_dict.items():\n\t\tnode_dict[key] = deep_merge(value)\n\treturn node_dict\n\n\n#-----------------------------------------------------------------------------\n# Code\n#-----------------------------------------------------------------------------\n\ndef load_yaml_node(source_str): # pylint: disable=redefined-builtin\n\t\"\"\"\n\tGiven a configuration data file ("}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\t\n\tdef process_scalar(node):\n\t    if node!= None:\n\t\tfor name, (value, _key) in node.iteritems():\n\t\tif name == '':\n\t\t\tcontinue\n\t\t#print >>sys.stderr, \"Processing\", name, \"of\", node['_id']\n\t\tvalue = convert_to_scalar(value, default=None, key=node['_id'])\n\t\tif value!= None:\n\t\t    return True\n\t\telse:\n\t\t    return False\n\n\tdef process_mapping(node):\n\t\tfor base_dict, (value, _key) in node.iteritems():\n\t\t   "}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n#        nodes = []\n\t\n#        for k in deep_merge_nodes(nodes):\n\t\t# \t\tfor v in nodes:\n\t    # \t\t\tdeepmerge.merge_nodes(v)\n\t\t# \t\t#print deepmerge.nodes\n        \n        return nodes\n    \n    \"\"\"\nUnit tests that check that the `check_for_duplicates` code works as intended\nby both the `list_duplicates` and `get_duplicates` functions.\n\"\"\"\nimport json\nfrom mock import Mock, patch\n\nfrom xmodule.modulestore.tests.factories import CourseFactory\nfrom xmodule.modulestore.xml"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = ArgumentParser(argument_usage=argparse.SUPPRESS, prog=\"main\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", default=False)\n\t\tparser.add_argument(\"-q\", \"--quiet\", dest=\"quiet\", default=True)\n\n\t\t# Arguments may or may not be specified.\n\t\targs = parser.parse_args(arguments)\n\n\t\t# Run the script.\n\t\tcall_main(parser.prog, args.verbose, args.quiet)\n\"\"\"\nCopyright 2011-present Robert Foord\n\nThis file is part of python"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\nparser = ArgumentParser(prog='python', description='Python script to build documentation')\n\t\n\t# add the arguments to the parser\n\tfor arg in arguments[1:]:\n\t\tparser.add_argument(arg, help=arg)\n\t\n\t# set appropriate parser defaults\n\tfor arg in parser.parse_args(arguments[0:]):\n\t\tparser.set_defaults(**{arg: argv[arg]})\n\t\n\t# add command line specific stuff to the parser and return it\n\tfor arg in parser.parse_args(argv[1:]):\n\t\tprint(arg)\n\t\tsys.exit(os."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ndef get_argparser(parser):\n\t\"\"\"\n\tReturns a `ArgumentParser` instance matching the arguments in `argv`. Any missing\n\targuments are returned.\n\t\"\"\"\n\tparser_method = arguments[0]\n\tparser_method = getattr(arguments[1], parser_method, getattr(argument) for argument in arguments[2:])\n\treturn parser_method()\n\ndef is_option_option(arg, default):\n\t\"\"\"\n\tReturns True if the given argument has the given default argument; False otherwise.\n\t\"\"\"\n\treturn (arg not in ('-h',"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\n\tif arguments:\n\t\t# parse command line options\n\t\tparser = ArgumentParser(prog=\"main\", description=\"Runs command-line arguments.\")\n\n\t\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"If input file is found or not, do not check if the file exists.\")\n\t\tparser.add_argument(\"-o\", \"--output\", action=\"store_true\", help=\"Run command-line (stdout) and stderr to the standard output instead of stdout.\")\n\t\tparser.add_argument(\"-p\", \"--path\", action=\"store\", help=\"Write the path to"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = ArgumentParser(description=\"\"\"A module for storing the information stored in the config file specified in the config file. The options are run as :py:func:`config_handler --args`\"\"\")\n\t\t\n\t\tparser.add_argument(\"args\", metavar=\"CONFIG_PATH\", nargs=\"*\", type=str, default=None)\n\t\t\n\t\ttry:\n\t\t\tcfg = ConfigParser.ConfigParser()\n\t\t\tcfg.read(CONFIG_FILE)\n\t\texcept ConfigurationError as e:\n\t\t\traise ConfigurationError(e)\n\n\t\treturn parser, cfg\n\t\n\tdef get_logger(self):\n\t\t\"\"\"\n\t\tReturn the"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\ndef parse_file(input, filename, parser, encoding='utf-8'):\n\t\"\"\"\n\tGiven input a name of a file, parse the argument list.\n\t\"\"\"\n\tparser.print_help()\n\tparser.add_argument('--in', action ='store', nargs=1, dest = '/source/', metavar = 'filename', help='input file to parse', metavar='filename')\n\tparser.add_argument('--out', action ='store', nargs=1, dest = '/output/', metavar='filename', help='output file', metavar='filename')\n\tparser."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\toption_lists = []\n\t\ttry:\n\t\t\tfor option, value in arguments:\n\t\t\t\tif option in parser_option_list:\n\t\t\t\t\toption = parser_option_list[option][0]\n\t\t\t\telse:\n\t\t\t\t\tparser_option_list.append(option)\n\t\t\treturn option_lists\n\t\texcept IndexError:\n\t\t\traise UsageError(\"The following arguments to the argparse module are required:\\n\\\n\t\t\t\"  python vcfparse.py [options]\\n\\n\"\n\t\t\t\"  python vcfparse --help\\n\\n\"\n\t\t\t\"  python vcf"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Perform CouchDB operations on the given host.')\n\t\tparser.add_argument('-s', '--service',\n\t\t\thelp='The name of the CouchDB service.',\n\t\tname = \"CouchDBCommand\")\n\t\tparser.add_argument('-t', '--tag', default = '*', help='The tag name for the action.')\n\t\targs = parser.parse_args()\n\t\treturn parser\n\n\ndef is_command_line_reparsed_from_args(method_list_arguments):\n\t\"\"\"\n\tGiven a method-list argument,"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\nimport argparse\nimport os\nimport sys\n\n\ndef find(name, *subpaths):\n\t\"\"\"\n\tFind a particular name in the specified paths, with given ``os.path.expanduser(name)`` if any.\n\t\"\"\"\n\tpath = os.path.expanduser(name)\n\tfor subpath in subpaths:\n\t\tif os.path.exists(path + subpath):\n\t\t\treturn subpath\n\t# Fall back to using the default paths\n\tfor root, dirs, _ in os.walk(path):\n\t\tfor filename in dirs + (os.path.join("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\targuments = parse_arguments_with_arguments(*arguments)\n\t\tif \"--help\" in arguments:\n\t\t\tprint(_(\"Usage:\") + \" %s [options for %s]\" % (args[0], len(arguments)))\n\t\tif \"--verbose\" in arguments:\n\t\t\targuments = arguments[1:]\n\t\treturn arguments\n\n\tif \"--config\" not in sys.argv:\n\t\tsys.argv.append(\"--config=config.py --version\")\n\n\tfor arg in sys.argv:\n\t\tif arg.startswith(\"--\"):\n\t\t\ttry:\n\t\t\t\texec(arg[2:], globals(),"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser_flags = []\n\t\tfor o in parser.option_strings:\n\t\t\tparser_flags.append(getattr(parser, o, ''))\n\t\treturn parser_flags\n\treturn parser_flags\n@parser_flags('add --dry-run --force --test --verbosity=N --no-report --silent -v')\ndef command_add(parser, args):\n\t\"\"\"\n\tCreate a command-line tool to add the specified options.\n\tArgs:\n\t\tparser: A argparse.ArgumentParser subclass.\n\t\targs (argparse.Namespace): The argument specification. Defaults to an"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tcmdLineArgParser = getattr(parser, 'parser')\n\t\treturn cmdLineArgArgParser.parser_flags(parser.add_argument_group('additional arguments'))\n\ndef parse_arguments_config(argv=None):\t\n\t\"\"\"\n\tParses flags from the argument parser. The command line flags are the command-line flags\n\tused by `argparse <http://docs.python.org/3/library/argparse.html>`_ when calling add_argument_group(). \n\t\"\"\"\n\tparser = parse_command_line_flags(argv)\n\tparser.add_argument"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument('--version', action='version',\n\t\t\thelp='Print the version number and exit.', \n\t\t\tversion=0)\n\t\targs = parser.parse_args()\n\treturn args\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--version', action='version',\n\t\t\thelp=('Print the version number and exit.', \n\t\t\t\tversion=0))\n\targs = parser.parse_args()\n\tsys.exit(0 if args.version==0 else 1)\n\nif '__main__' == __name__:\n\tparser ="}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\targparser = parser.parse_args()\n\tif len(argparser.args) < 1:\n\t\tparser.print_help()\n\t\tprint \"    or, eg, --help\"\n\t\treturn\n\t\n\tif \"--help\" in argparser.args:\n\t\tprint \"    \"\n\t\tprint \"    use the following flags:\"\n\t\tprint \"\"\n\t\tfor x in range(0, len(argparser.args), 3):\n\t\t\tprint \"--\", argparser.args[x],\n\t\treturn\n\nif __name__ == \"__main__\":\n\tsys.argv="}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\treturn \" \".join(parser.option_strings) if parser.option_strings else \"\"\n\n# Returns the list of flags that are applicable to the current tool in this\n# file that can be used to run this script\ndef parse_options():\n\timport argparse, os\n\tparser = argparse.ArgumentParser(description=\"run all tool flags given in sys.argv\")\n\tparser_flags(parser)\n\treturn parser.parse_args()\n\n#--------------------------------------------------------------------------\n# Load the file, which is the program name. Only run if the program exists. \n\n\ndef load_file("}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\t\n\tparser.add_argument(\n\t\t'-o',\n\t\tdest='file',\n\t\tmetavar='FILE',\n\t\tdefault=None,\n\t\taction='store',\n\t\tversion=get_version(),\n\t\thelp='The file to check. Should be specified via the command line argument. If not given, the default file is used as the file name from the command line arguments.')\n\ndef check_output(args):\n\t\"\"\"\n\tChecks the file exists.\n\t\"\"\"\n\tfor line in args.args:\n\t\tif os.path.isfile(line):\n\t\t\treturn True"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument(\"-c\", \"--config\", help=\"config file\", default=\"/etc/openshift/openshiftrc.conf\")\n\t\tparser.add_argument(\"-l\", \"--labels\", help=\"list of labels of the labels\")\n\n#!/usr/bin/env python\n#\n# Copyright (C) 2011 The Android Open Source Project\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument('-b', '--b', metavar='PATH',\n\t\t\t\t\t\taction=\"append\",\n\t\t\t\t\t\thelp=\"search a directory for files matching the given regex\")\n\t\tparser.add_argument('-f', '--follow', metavar='PATH',\n\t\t\t\t\t\taction=\"append\",\n\t\t\t\t\t\thelp=\"find files by path\")\n\t\tparser.add_argument('-g', '--glossary', metavar='PATH',\n\t\t\t\t\t\taction=\"append\",\n\t\t\t\t\t\thelp=\"find files by path\")\n\t\tparser.add_argument('-i', '--input', metavar='PATH',\n\t\t\t\t\t\taction=\"append\",\n\t\t\t\t\t\thelp"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tparser.add_argument(\"-a\", \"--addlist\", action=\"store_true\", required=False, help=_(\"Add options to the user list\"))\n\tparser.add_argument(\"-e\", \"--editlist\", action=\"store_true\", required=False, help=_(\"Edit options for the user list\"))\n\tparser.add_argument(\"-r\", \"--regex\", action=\"store\", type=str, action=\"store\", type=str, help=_(\"Regex to use (default: {0})\"), dest=\"regex\")\n\tparser.add_argument(\"-o\", \"--output\", action=\"store\", type"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\treturn''.join('--' if flag.isupper() else''--' for flag in parser.option_strings)\n\ndef process_arguments(args=None, config=None):\n\t\"\"\"\n\tParse command-line arguments and return a list of the command line arguments.\n\n\tIf args is provided, only args may contain arguments.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='Run a command based on config.')\n\t\tparser.add_argument(\n\t\t\t'region', metavar='REGION', required=True,\n\t\t\thelp='The regeion number,"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\timport subprocess, os\n\t\timport sys\n\t\tfrom Borgmatic import borgmatic_parser\n\t\tparser = borgmatic_parser.BorgmaticParser()\n\n\t\tcmd = '{borgmatic} --help'.format(borgmatic=os.environ.get('BORGATOMAIN', 'bash'))\n\t\tparser.usage = parser.format_epilog(cmd)\n\t\tsys.exit(cmd)\n\tdef parse_borgmatic_args():\n\t\t\"\"\"\n\t\tParse borgmatic command-line arguments. Command-line arguments can be strings.\n\t\t\"\"\"\n\t"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tscript = os.path.join(\n\t\tos.path.dirname(os.path.realpath(__file__)),\n\t\t\".bash_completion\",\n\t)\n\t\targs = shlex.split(sys.argv)\n\t\targs.append(\"-d\")\n\t\treturn script + (b\"\\n\".join(args))\n\ndef parse_and_evaluate(script):\n\t\"\"\"\n\tParse out the bash script to ensure it properly runs. Then, evaluate\n\tall of the arguments.\n\t\"\"\"\n\tb = bash_completion()\n\texec(b, {}, {}, [script])"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tprint(\"*** Running %s ***\\n\" %(blb.name))\n\t\tblb.run()\n\t\t#print(\"*** Returning %s ***\\n\" %(blb.name))\n\t\t#print(\"*** Borgmatic command output follows:\")\n\t\t#print(\"\\t\".join([str(arg) for arg in blb.output]))\n\t\t#print(\"*** Completed\\n\")\n\n\ndef main():\n\tparser = create_parser()\n\tparser.add_argument(\"-c\", \"--commands\", default=\"\", help=\"commands that are found\")\n\tparser."}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\nclass BorgmaticCompletionPrinter(object):\n\tdef _parse_input(self, input):\n\t\t\"\"\"\n\t\tParse input for borgmatic completion.\n\t\t\"\"\"\n\t\t(options, args) = self._split_basic_arguments(input)\n\t\treturn BorgmaticCompletionPrinter(options)\n\n#===============================================================================\n# Command line options parser\n#===============================================================================\nclass CommandLineOptions(object):\n\tdef __init__(self, usage, parent=None):\n\t\t\"\"\"\n\t\tCreate and fill with optparse options. This class will create a new\n\t\tparser for usage and all of"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\tusage = 'Usage: bash -c command_line arguments...'\n\t\n\tparser = ArgumentParser(prog='bash', prog_name='bash')\n\tparser.add_argument('-c', '--command-line', dest='command_line', default='.', nargs='*',\n\t\t\t\t\t\taction='store', help='The command-line arguments to be added')\n\t\n\t# command-line arguments\n\tparser.add_argument('files', nargs=argparse.REMAINDER)\n\t\n\t# parsing arguments\n\targs = parser.parse_args()\n\t\n\tif options.command_line:\n\t\tparser"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\timport string\n\t\timport sys\n\t\timport subprocess\n\t\tcmd_completion_args = [sys.executable, \"./Borgmatic\", \"--help\"]\n\t\tif \"-V\" in sys.argv:\n\t\t\tcmd_completion_args.append(\"--verbose\")\n\t\tif \"--quiet\" in sys.argv:\n\t\t\treturn subprocess.check_output(cmd_completion_args)\n\t\tf = string.popen(cmd_completion_args, \"w\")\n\t\tfor line in f:\n\t\t\tif '-f' in line and \"-borgmatic\" in line:\n\t\t\t\tsubprocess.call("}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tfrom borgmatic import bash\n\t\tbfile = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"borgmatic.py\")\n\t\tcode = bash(\"bash %s %s\"%(bfile,os.path.basename(bfile)),\" \")\n\t\treturn bash(code)[1]\n\n#--------------------------------------------------------------------\nclass BorgmaticCli:\n\tdef __init__(self, argv, borgmatic=True, borgmatic_config=None, runlevel='default', profile=2, profile_color=None):\n\t\tself._"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tborgmatic = os.path.basename(sys.argv[0])\n\t\tcommand = [\"bash\"]\n\t\tif borgmatic in COMMAND_BORGMATIC:\n\t\t\tcommand.append(\"--bootstrap\")\n\t\tif borgmatic in COMMAND_BSUB_BORGMATIC:\n\t\t\tcommand.append(\"--config=\" + CONFIG_BORGMATIC.get(borgmatic))\n\t\tif borgmatic in COMMAND_BSUB_BSUB_BGEN:\n\t\t\tcommand.extend(CONFIG_BSUB_BGEN[borgmatic])\n\t\tif borgmatic in"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\treturn _bash_completion_script(args)\n\n\ndef create_and_save_borgmatic():\n\t\"\"\"\n\tCreate and save a borgmatic instance.\n\t\"\"\"\n\t\timport os\n\n\t\t# Get the first of the arguments\n\t\tif(args == \"-o\"):\n\t\t\treturn _create_borgmatic_script()\n\t\telse:\n\t\t\treturn _save_borgmatic_script(args) \n\n\t\ndef make_and_load_borgmatic(arguments):\n\t\"\"\"\n\tCreate and load a borgmatic instance.\n\t\"\"\"\n\t\t"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Run from borgmatic.\")\n\t\tparser.add_argument( \n\t\t\t\t'-u', help='output the original bash command, if it exists', \n\t\t\t\tdest = 'args', nargs='?', default='$BORGMAN_COMMAND', required = True, nargs='?')\n\t\targs = parser.parse_args()\n\tprint \"Processing borgmatic command...\"\n\tborgmatic_command = eval(args.args)\n\tborgmatic_command(locals())\n\tprint \"Done.\"\n\nif __name__ == \"__main__\":\n\tbash"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=\"Convert the CWD format for a PRESUB project to a PRESUB project\")\n\t\tparser.formatter_class = argparse.ArgumentDefaultsHelpFormatter\n\t\targ_dict = parser.parse_args(unparsed_arguments)\n\t\treturn_dict = {}\n\t\tfor name, value in arg_dict.items():\n\t\t\treturn_dict[name] = parser.parse_arg(name, value)\n\t\treturn return_dict\n\n\treturn parse_arguments\n\ndef get_project_config():\n\tfor subparser in get_sub"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=subcmd_opt)\n\t\tname_group_grp = parser.add_mutually_exclusive_group(required=True)\n\t\tname_group_grp.add_argument('-f', '--failfast', action='store_true', help=\"If specified, will raise an exception for premature termination of the job.\")\n\t\tname_group_grp.add_argument('-C', '--config', action='store', type=str, default=\"\", help=\"Default config file to use (to use the first argument, e.g. 'config.yml'. \" +"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\treturn {}\n\n\ndef main(args = None):\n\t\n\tif args is not None:\n\t\tparse_args(*args)\n\n\ndef get_all_args():\n\t\"\"\"\n\tParse the command line arguments and return this dictionary.\n\t\"\"\"\n    \n\t\n\treturn parse_args()\n\n\ndef get_global_args():\n\n\n\treturn make_global_parser()\n\ndef get_parser(subparser = None):\n\t\"\"\"\n\tParse the command line arguments and return the :class:`ArgumentParser` object.\n\t\"\"\"\n\t\t\n\t\ttry:\n\t\t\tsubparser = subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparsed_options, _ignored_arguments = parse_subparser_config_options(*unparsed_arguments)\n\n\t\tif \"global\" in parsed_options or \"global\" in parsed_options.global_config_options():\n\t\t\tglobal_config = dict(**parsed_options.global_config_options())\n\t\t\tglobal_config.update(global_config.clone())\n\n\t\treturn global_config\n\"\"\"\nUnit tests for the CourseOutlinePage objects.\n\"\"\"\nimport unittest\nfrom datetime import datetime\n\nfrom django.contrib.auth.models import User\nfrom django.test."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tglobal arguments\n\t\tglobal namespace, commands\n\t\targuments=dict(parser=argparse.ArgumentParser(prog=argv[0], description='An alias for this command line'))\n\t\t\n\t\t# If this script was called without arguments, run this command just prior to sub command.\n\t\tif len(argv) == 1:\n\t\t\t# Set up sub command object.\n\t\t\tsub_command = arguments[str(id(sub_commands))]\n\t\t\tsub_arguments= getattr(sub_command, 'arguments', [])\n\t\t\tsub_arguments.append(getattr(sub_command, 'global_variables', {}))"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t if unparsed_arguments:\n\t\t\treturn unparsed_arguments[0]\n\t\telse:\n\t\t\tparser = argparse.ArgumentParser(prog=\"python\", add_help=False)\n\t\t\tparser.epilog = \"\\n\\n\"\n\t\t\tparser.add_argument(\"-l\", \"--level\", type=int, default=0, help=\"Verbose level\")\n\t\t\tparser.add_argument(\"-r\", \"--root\", type=str, default=None, help=\"Root folder\")\n\t\t\tparser.add_argument(\"-r\", \"--remote\", action=\"store_true\", default=False, help=\"Use the"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n#\tfrom argparse import Namespace\n#\tfrom os.path import basename,dirname,isfile\n#\tfor arg_name,subparser_cls in unparsed_arguments:\n#\t\targ_type,value = arg_name.split('-')\n#\t\tsubparser = getattr(parser, subparser_cls)\n#\t\tif any([hasattr(subparser, arg_name) for arg_name in (subparser.__doc__,)):\n#\t\t\tprint('%s is of type: %s, but does not contain any argument.' %(arg_name, type(subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\tparsed_arguments = Namespace()\n\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\tif '--%s' % i in arg:\n\t\t\tparsed_arguments.global_subparsers[arg.replace('--%s' % i, '', 1)].global_subparser_name_count += 1\n\treturn parsed_arguments\n# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparsed_arguments = []\n\tparser = SubtaskArgumentParser(subparsers=[], description=subparsers[0])\n\tparser.parse_args(*parsed_arguments)\n\treturn parser\n\n\n\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2015 - 2020 Detlev Offenbach <detlev@die-offenbachs.de>\n#\n\n\"\"\"\nModule implementing the general interface to a plugin interface.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport os\n\nfrom E5Scripting.CommandInterface import CommandInterface\n\n\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(\n                # conflict_handler='resolve', # if given multiple subparsers are used as the options for subparser\n\t# formatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\t # usage='%(prog)s [args] [<subcommand>]', # Help message for this parser\n\tdescription=('Generates a namespace with subcommands that can be used, to configure the set of namespaces and sets those subcommands (that is, can be empty).  To specify subcommands, pass in the namespace name and arguments.  For example, if the namespace specifies a namespace for a subparser of Carbon"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\nclass ArgParseError(Exception):\n\t\"\"\"\n\tException indicating that a parse failed. This is called for errors in an\n\tinvalid command-line options.\n\t\"\"\"\n\n\nclass DuplicateSubparserError(ArgParseError):\n\t\"\"\"Raised when a subcommand name specified in a list of subparser name conflicts with another one.\"\"\"\n\n\nclass ConfigException(Exception):\n\t\"\"\"\n\tException indicating that config is invalid. This is called for errors in an\n\tincorrect command-line options.\n\t\"\"\"\n\n\nclass NonDefaultDefaultError(ParamError):\n\t\"\"\"\n\tException indicating that"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\targs       = {}\n\tsub_commands = {}\n\t\n\t# Make the argument parser into subparsers\n\tfor group in parser_group_names(parsed_args):\n\t\tif not parser_name_has_command(group):\n\t\t\tparser = groups[group]\n\t\t\tsub_commands[group] = parser.argument\n\t\t\targs[parser_name_has_command(group)] = []\n\t\t\n\t\t# Parse the rest of a nested parser.\n\t\tfor subparser_name in parser_namespace(parsed_args[group]):\n\t\t\tif parser_name_has_command(sub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\nclass Subparser(argparse.ArgumentParser):\n\t\"\"\"\n\tSubparsers must implement `add_argument()`, and `count()`.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(Subparser, self).__init__(*args, **kwargs)\n\t\tself.add_argument('--repository', help='''Repository URL''')\n\n\tdef print_help(self, subcommand_list):\n\t\t\"\"\"\n\t\tDisplay help about --repository. All subcommands will print on stdout\n\t\t\"\"\"\n\t\tif subcommand_list is None:\n\t\t\treturn self.print_list([])"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tsubparser_list = []\n\t\n\tfor parser in unparsed_arguments:\n\t\tsubparser = parser.subparsers[parser.name]\n\t\t\n\t\tfor subsubparser in subparser_list:\n\t\t\t\n\t\t\tfor arg in parser.parse_subparsers():\n\t\t\t\tif arg.dest == subsubparser.name:\n\t\t\t\t\tsubparser.add_parser(arg.parser_name, parser)\n\t\t\t\n\t\t\tsubparser.description = parser.description_str\n\n\t\t\t# The description should be parsed as a separate parser, otherwise the full parsed namespace\n\t\t\t# will need to be used.\n\t\t\t\n\t\t\tsub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\n\t# parse the args, pull out any subparser from it.\n\tsubparser_names = []\n\t\n\tfor subparser in subparsers:\n\n\t\ts = subparser['subparsers']\n\t\t\n\t\targs = []\n\t\t\n\t\tif 'help' in s:\n\t\t\targs.append(subparser['type'].split(':')[0]) # remove the help flag\n\t\t\n\t\twhile len(args) > 1:\n\t\t\ts.pop()\n\t\t\t\n\t\t\t# all remaining subcommands should go in the same scope:\n\t\t\tif ':' not in s[0]:\n\t\t\t\targs.append(subparser['type'].split(':')["}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\tparsed_subparser = []\n\t\tfor argument in unparsed_arguments:\n\t\t\targument_namespace = {}\n\t\t\tself.add(argument)\n\n\t\t\tsubparser = subparsers.add_parser(\n\t\t\t\targument,\n\t\t\t\thelp=\"Subparser for the given action\",\n\t\t\t\tprog='subparser:' + argument\n\t\t\t)\n\n\t\t\tself.add(subparser)\n\n\t\t\tsubparser.set_defaults(action=\"store\")\n\t\t\tsubparser.add_argument(\"action\", nargs=2, action=argparse.Action, help=\"Action under which the action should take the action from\")\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\t# The common arguments for this subparser\n\tsubparser_name_arg = None\n\n\tsubparser_args = []\n\tsubparser_defaults = []\n\n\tfor arg in unparsed_arguments:\n\t\tif arg == \"--repository\":\n\t\t\tsubparser_name_arg = 'pulser'\n\t\t\tpass\n\t\telif arg == \"--source-location\":\n\t\t\tsubparser_args.append(parser.get_source_location_arg(subparser_name_arg))\n\t\telif arg == \"--source-location-url\":\n\t\t\tsubparser_args.append("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\nclass ArgumentParser(object):\n\t\"\"\"\n\tThe main entry point for the argparse module.\n\n\t:arg parser: the ArgumentParser object\n\t:arg option_strings: An optional string with commands to parse. The optional string can be used to set\n\t\tthe default value of a command. If the parser does not define a command, it is automatically\n\t\tunparsed, and an error is raised. Defaults to the empty string\n\t\t(``'')\n\t:type option_strings: ``str``\n\t:arg subparsers: A dictionary where the keys are the subparsers names and the values are the"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\treturn (\n\t\tsubparser(subparsers[0]),\n\t\t# Subparser name, which maps subparser name to parser\n\t\t(subparser) for subparser, arguments in unparsed_arguments[::-1]\n\t\tfor subparser, args in parser_parameters_re.named_args(remaining_arguments).items()\n\t\tif len(args) > 0\n\t)\n\t\t\nclass OptionParser:\n\tdef __init__(self, description, epilog):\n\t\tself.description = description\n\t\tself.epilog = epilog\n\t\tself.options = dict([(k, v) for k,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t\n\tparser = subparsers.add_parser(\n\t\t'--repository', default=DEFAULT_RCS_REPO, \n\t\thelp='which repository to use')\n\tparser.add_argument('-p','--name', default='', help='repository to use or file with namespace prefix')\n\t\n\tparser.add_argument('--force-update', action='store_true',\n\t\thelp='also update the namespace')\n\tparser.add_argument('file', nargs='+', type=str, help='XML tree filename')\n\n\treturn unparsed_arguments, parser, parser.print_help()\n\t\n\t\ndef get"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = subparsers.add_parser(\"parse\", description=\"Build package package documentation from \" + package_name)\n\t\tparser.add_argument(\"\")\n\t\tparser.add_argument(\"package_name\", help=\"Package name\")\n\t\tparser.add_argument(\"--doc_type\", help=\"Show documentation about package type\")\n\t\tparser.add_argument(\"--docs\", help=\"Show documentation about package docs: %s\" % get_docs())\n\t\tparser.set_defaults(func=do_help)\n\t\tparser.add_argument(\"--verbose\", help=\"Show verbose mode, do a full"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n# parse the commandline arguments\nparser = ArgumentParser()\nparser.add_argument('-d', dest='base_dir', help='Directory with models (excluding models from this project) files')\nparser.add_argument('-l', dest='output_dir', help='Directory within input data dir')\nparser.add_argument('-t', dest='tags', help='Tags of models to be included in results')\n\n# parse the command line arguments\nargs = parser.parse_args()\n\n# get a list of the models to add the new\nmodels_list = load_model_list"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\timport argparse\n\t\tparser = argparse.ArgumentParser(description=DESCRIPTION)\n\t\tparser.add_argument('--version', action='version',\n\t\t\tversion=' '.join(VERSION_SHORT))\n\t\tparser.add_argument('--root', help='The root level build of the repository root',\n\t\t\taction='store', default=ROOT)\n\t\tparser.add_argument('--debug', action='store_const', const=True, default=False,\n\t\t\tname=\"print debugging logging\")\n\t\tparser.add_argument('-v', action='version', version=''.join(VERSION_SHORT))\n\t\t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Manage and manage all users in the system\")\n\t\tparser.add_argument(\"--admin\", action=\"store_true\", help=\"Indicate whether this session admins to the system\")\n\t\tparser.add_argument(\"--admin-url\", help=\"URL containing the admin user\",\n\t\t              default=\"https://api.github.com/users/\")\n\t\tparser.add_argument(\"--admin-token\", help=\"Token to use for authenticating the admin\",\n\t\t              default=\"\", type=str)\n\t\tparser.add_argument(\"--org\", help=\"Organization Id\", default"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\t# Create subparser\n\tparser=subparsers.add_parser(\"list\", help=\"List commands\")\n\n\tparser.add_argument(\"command\", help=\"Command to execute\")\t\t\t\n\tparser.add_argument(\"host_name\", help=\"Host name\")\n\tparser.add_argument(\"host_number\", help=\"Host number\")\n\t\n\t# Create the subparser\n\treturn parser\n\t\t\n\nif __name__ == \"__main__\":\n\tprint(\"Run this example\")\n\tparser=make_parsers()\n\tparser.print_help()\n\t\n\n# -*- coding:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Utility to build a python library for python.')\n\t\n\tparser.add_argument(\n\t\t'python_path',\n\t\thelp='Python code to compile to',\n\t\ttype=str,\n\t\tnargs=1,\n\t\tdefault=os.path.abspath(os.path.expandvars(env.PYTHON_PATH)))\n\t\n\tparser.add_argument(\n\t\t'--extra-dirs',\n\t\thelp='Extra directories (may include environment variables), e.g. --extra-dirs=\"/opt/python/lib\"' +\n\t\t'"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\nparser = subparsers.add_parser(description=__doc__, \\\n\t\t\t\t formatter_class=argparse.RawDescriptionHelpFormatter, \\\n\t\t\t\t description=description, )\nparser.add_argument(\"-v\", \"--verbose\", help=\"Verbose\", default=False, type=bool)\nparser.add_argument(\"-i\", \"--input-file\", help=\"Input file to generate list of commands\", default=None)\nparser.add_argument(\"-d\", \"--debug\", help=\"Debug\", action=\"store_true\", default=False)\nparser.add_argument(\"-A\", \"--add\", help=\"Add action to add"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef make_test_subparsers():\n\t\"\"\"\n\tBuild a top-level subparsers and its subparsers are used to test the parser\n\tand all its subcommands.\n\t\"\"\"\n\t\ndef make_build_parser():\n\t\"\"\"\n\tBuild a build parser, providing the parser name, and subparsers\n\t\"\"\"\n\t\ndef make_help_parser():\n\t\"\"\"\n\tBuild a help parser and the help subcommand\n\t\"\"\"\n\t\t\n\t\ndef usage():\n\thelp_text.print_help()\n\t\n\ndef main():\n\t\n\tif len(sys.argv) > 1:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\tp = OptionGroup(parser, \"Make\", \"Create top level parser.\")\n\t\tp.add_option(\"--dry-run\", dest=\"dry_run\", action=\"store_true\", default=True)\n\t\tp.add_option(\"--git-only\", dest=\"git_only\", action=\"store_true\", default=False)\n\t\tp.add_option(\"-x\", \"--noisy\", dest=\"use_gitlyy\", action=\"store_false\", default=True,\n\t\t            help=\"don't run under git-only\")\n\t\tp.add_option(\"-L\", \"--"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"name\")\n\tparser.add_argument(\"target\")\n\tparser.add_argument(\"command_options\")\n\tparser.add_argument(\"--verbosity\", \"-v\", type=int, choices=[0, 1, 2, 3], default=2, help=\"Verbosity level\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable verbose mode\")\n\tparser.add_argument(\"--force\", action=\"store_true\", help=\"Skip dependency dependency checks\")\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='pygisti',\n    version='2.0.0',\n    description='An anagry application for Google Calendar',\n    long_description=open(\"README.rst\").read(),\n    author='Darren Dong',\n    author_email='darren@dong.husu.de',\n    license='New BSD License (see LICENSE)',\n    packages=find_packages(),\n    zip_safe=False,\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n       "}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tyield wapilib.wapi_wrapper(response, 'paging', max_results)\n\t\n\tdef is_authenticated():\n\t\t\"\"\"\n\t\tRaises authenticated access exception in non-current user.\n\t\tReturns True if is_authenticated is invoked in current user. \n\t\t\"\"\"\n\t\tif is_logged_in():\n\t\t\tif not is_admin():\n\t\t\t\traise UserNotLoggedInException('Not authorized to access this page')\n\t\t\telse:\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\t\n\t\n\tdef get_items(request):\n\t\t\"\"\"\n\t\t\tGets the item list in"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tif max_results == 1:\n\t\t\tif response is None:\n\t\t\t\ttry:\n\t\t\t\t\timport json\n\t\t\t\texcept ImportError:\n\t\t\t\t\traise ImportError(\"WAPI JSON module is not available. Use --help to see this list.\")\n\t\t\tresponse = json.loads(response)\n\t\telif max_results < 1:\n\t\t\traise Exception(\"Maximum number of objects to be returned in one page. The \" + repr(max_results) + \" arguments might be less than the \" + repr(min(response.keys())))\n\n\t\tfor obj in response[\"objects\"]:\n\t\t\ttry:\n\t\t\t\tyield obj"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t    # Get objects to iterate\n\t    n_entries = 0\n\t    # Walk pages\n\t    for page in response:\n\t        n_entries += 1\n\t    # If we have more than max_results we can iterate\n\t    if max_results >= len(page):\n\t    #          break\n\t\n\t# Iterate pages\n\tfor page in response:\n\t\tyield page\n\t    # Remove the last page in the pages\n\t    if page and page[-1:] == page[-max_results:] :\n\t    yield page[:-max_results]\n\t\n\t# Clean up\n\tyield response[-1:]\n\n################################################################################\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\tyield response\n\t\tpage_range = response[0].get(\"pages-range\")\n\t\t\n\t\tyield \"Page-Range: %s\" % page_range\n\t\n\t\tfor row in response[1:]:\n\t\t\tyield \"Page: %s\" % row\n\t\t\n\t\tdel response[:]\n\t\t\n\t\t# Add max results for last page\n\t\tfor item in yield paginator.page(max_results, offset=1):\n\t\t\tresponse.append(dict([(i.name, i.get_text().replace(\"&nbsp;\", \" \"), i.get_attr().replace(\"&nbsp;\", \" \"))) for i in item]))"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t \n\tif not response:\n\t\traise gen.Return.GetEmptyResult(0)\n\tif max_results <= 1:\n\t\treturn\n\tcurrent_page = request.server.getPage(max_results)\n\tfor page in range(1, max_results):\n\t\tnext_response = next(current_page)\n\t\twhile next_response:\n\t\t\tyield next_response\n\ndef api_response_object(item, objectid):\n\t\"\"\"\n\tReturns WAPI response page without given object by id\n\t\n\tArgs:\n\t    item (Object): WAPI item by id"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t\n\tdef fetcher(url, max_results=1000):\n\t\t# Get the HTML code for the page\n\t\tresponse = urllib2.urlopen(url)\n\t\t#print(response.read())\n\t\tif response.info().get('charset') == \"utf-8\":\n\t\t\ttry:\n\t\t\t\tbuffer = response.read()\n\t\t\t\twhile buffer:\n\t\t\t\t\tprint(buffer)\n\t\t\t\t\tbuffer = buffer.replace('\\n','').replace('\\t','')\n\t\t\t\t\tresponse.seek(buffer_offset)\n\t\t\texcept:\n\t\t\t\tprint \"Cannot process webpage\"\n\t\telse:"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\t\n\twhile True:\n\t\t# get the first page\n\t\tresponse = next(response)\n\n\t\t# get number of items\n\t\tresponse.page = int(response.page)\n\n\t\t# iterate over results\n\t\tfor object in response:\n\t\t\t\n\t\t\t# calculate the response for this object\n\t\t\tresponse.page += 1\n\n\t\t\t# make an endpoint that is required to read an object\n\t\t\turi = \"self/{0}\".format(object.identifier)\n\t\t\tquery = \"\"\"\n\t\t\t{0}\n\t\t\tobject[name=\"{1}\"]\n\t\t\t{2}\n\t\t\t{3}\n\t\t\t{"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t  \n\n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t\"\"\"\n\t  \n\t  \n\t  \n\t  \n\t\"\"\"\n\t  \n\t  \n\t  \n\t  \n\t  \n\t  \n\t\"\"\"\n\t\"\"\"\n\twapi.WAPI.PAGER.page(response, max_results)\n\n\t\"\"\"\n\treturn response as list.\n\t\"\"\"\ndef get_list(request, max_results):\n\t\"\"\"\n\tReturns WAPI"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\ttry:\n\t\t\tfor page_index in range(len(response)):\n\t\t\t\tyield _Page(response[page_index])\n\t\texcept:\n\t\t\t# Error response from WAPI.\n\t\t\tyield response\n\t\t\treturn\n\tif max_results < 0:\n\t\tprint \"You must supply a max_results value.\"\n\t\treturn\n\tpage = paginator.page(max_results)\n\tif page.has_next():\n\t\tprint \"WAPI page at: \"+str(page.url)+\": \"+str(page.get_data())\n# Copyright (C) 2006"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\t\n\tsize = float(size)\n\tif '.' in size:\n\t    size = float(size.split('.')[0])\n\n\tif not size:\n\t    return size\n\t\n\tresult = ''\n\tdigits = 1\n\tfor digit in (',', '.', '-', '*', '1'):\n\t\tdigits *= digit\n\t\tbreak\n\t\n\tif result.endswith('.00'):\n\t\tresult = result[:-1]\n\t\n\telse:\n\t\traise Exception(\"invalid value provided for size: %r\" % result)\n\t\n\tfor digit in (10, '0', '1'):\n\t\tresult += \".\" + digit\n\t\t\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tsize_value = to_bytes(size)\n\t\tif size_value in b\"kKM\":\n\t\t\treturn 10**((size_value // 10**10 * 1000) / 1000)\n\t\telif size_value // 10**10 == 100 * size_value / 10**10:\n\t\t\treturn 1 << (size_value // 10**10 * 1000 / 100)\n\t\telse:\n\t\t\treturn size_value\n\t\nclass WakeupException(ValueError):\n\t\"\"\"\n\tBase for Wakesup exceptions.\n\t\n\tFor now I can't figure out how to implement the"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    \n\t    m = re.match(r'\\d+(\\.\\d+)?M(\\d+)?s', size)\n\t    \n\t    return int(m.group(1))\n\t    \n    def byte_to_size(size: str, bit_length: int):\n\t\"\"\"\n\tConvert human readable file size to byte size.\n\t\n\tConvert result to a human readable value (eg: '100000M')\n\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '1000K')\n\t    bit_length: An integer representing a human readable file size (eg: '1000M')\n\t\n\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\ttry:\n\t\t\timport gabr.fileutil.base as base\n\t\texcept ImportError:\n\t\t\tfrom gabr.utilities import base\n\n\texcept IOError:\n\t\tpass\n\telse:\n\t\ttry:\n\t\t\treturn base.human_readable_filesize(size)\n\t\texcept ValueError:\n\t\t\tpass\n\n#------------------------------------------------------------------------------\ndef get_user_dir(path: str) -> str:\n\t\"\"\"\n\tReturn the user directory path given an absolute path to the python installation.\n\t\"\"\"\n\treturn os.path.expanduser(os.path.expandvars(os.path."}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t    size = size_from_str(size)\n\t    return int(math.floor(float(size)/1024) * 1024)\n\t    except ValueError:\n\t    return 0\n\n@lru_cache(maxsize=400)\ndef size_from_bytes(size: str)->int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\t\n\tParameters:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\t\n\t\tExamples::\n\t\n\t        >>> size_to_bytes(\"500K\")\n\t        500\n\t       "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tif size.strip().endswith('.'):\n\t\t\tsize = size[:-1]\n\t\tif len(size)!= 16:\n\t\t\traise ValueError(\"'%s' is not a valid human readable file size\" % size)\n\n\t\tbytes = 0\n\t\twhile bytes < 1024 * 1024:\n\t\t\tbytes = bytes + size.lower()\n\n\t\treturn '%.2f byte' % bytes\n\t\n\ndef size_to_bytes_float(size: str) -> float:\n    \t\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\treturn str(size)\n\n\tdef from_bytes(bytes: bytes) -> int:\n\t\"\"\"\n\tConvert data to human readable number.\n\t\n\tArgs:\n\t    bytes: The data to convert\n\t\t\n\t\tReturns:\n\t    An approximation of the data.\n\t\n\tRaises:\n\t    ValueError: An unknown error occurred\n\t        \"\"\"\n\n\t\treturn int(bytes)\n\n\tdef to_bytes(num: str) -> bytes:\n\t\"\"\"\n\tConvert human readable number to bytes.\n\t\n\tThe values returned will contain a leading 0 bytes.\n\t\n\tArgs:\n\t    num: The number to convert\n\t\t\n\t\tReturns:"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tif size == '1K':\n\t\t\tsize = 'K'\n\t\treturn size\n\t\n\tdef get_bytes(size: str) -> str:\n\t\ttry:\n\t\t\tbytes = _f_size_to_bytes(size, 'B')\n\t\texcept IOError as e:\n\t\t\ttry:\n\t\t\t\tbytes = _f_size_to_bytes(size, 'KB')\n\t\t\texcept Exception as e:\n\t\t\t\ttry:\n\t\t\t\t\tbytes = _f_size_to_bytes(size, 'MiB')\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tpass\n\t\treturn bytes"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\tfrom math import floor, ceil\n\t\tsize = long(size)\n\t\tif isinstance(size, basestring):\n\t\t\tsize = floor(size) / 1024\n\t\telif isinstance(size, (long, int)):\n\t\t\tsize = ceil(size / 1024)\n\t\tif size > 1024 * 1024:\n\t\t\traise ValueError(\"Value will be rounded up to 1000k, not %1g\" % size)\n\t\treturn size\n\t\n\t\n\tdef read_from_stream(self, f, block_size=65536):\n\t\tr = 0\n\t\twhile True:\n\t\t\tr += len("}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\n\tif isinstance(size, bytes):\n\t\tsize = size.encode('hex').replace(':', '|.')\n\n\treturn size\n\n\ndef human_readable_time_to_bytes(seconds: float) -> str:\n\t\"\"\"\n\tConvert human readable time according to unix timestamp\n\t\"\"\"\n\n\tminutes, seconds = divmod(seconds, 60)\n\n\tmicroseconds, nanoseconds = divmod(minutes, nanoseconds)\n\n\treturn \"%d:%02d:%02d\" % (microseconds, nanoseconds, seconds)\n\n# -*- coding: utf-8 -*-\n\n# Form implementation"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\ndef _dictsum_same(dicts):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum_same([({'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 10, 'b': 4}\n\t\"\"\"\n\t\ndef _keysum(dicts):\n\t\"\"\"\n\tCombine lists of dictionaries.\n\t\n\t>>> _keysum([[{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 9, 'b': 4}\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\n\tif __name__ == '__main__':\n\t\t# a = []\n\t\t# b = []\n\t\t# for it in iteritems(dicts):\n\t\tfor it in iteritems(dicts):\n\t\t\tfor k, val in it.iteritems():\n\t\t\t\tif k not in ('a', 'b', 'defaults'):\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tfor index in range(len(val)):\n\t\t\t\t\tval[index] += val[index]\n\t\t\t\tif 'name' not in val:\n\t\t\t\t\tcontinue\n\n\t\t\t\tfor k in ('default', 'order'):\n\t\t\t\t\tif val[index] == val[k"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\treturn map(lambda d: (d[0], sum(d[1]), d[2]))\n\t\n\tdef _getdictname(n1, n2):\n\t\treturn '{0}({1}, {2})'.format(n1, n2, n1)\n\t\n\tdef _getdicts(dct):\n\t\tn1, n2 = dct[0].keys(), dct[0].keys()\n\t\treturn [self._getdictname(n1 + d, n2 + d) for d in dct]\n\t\n\ttry:\n\t\ti = 0\n\t\tfor dct in sorted"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\ndef _count(dictlist, iterable):\n\t\"\"\"\n\tReturn the number of items in *dicts* that match the given dictionary.\n\t\n\t>>> _count([1, 3], [4, 5], [6, 7])\n\t8\n\t\"\"\"\n\t\n\tsums = 0\n\tfor k, g in _dictlist:\n\t\tif len(list(g.items(dictlist[k]))[0]) > 0:\n\t\t\tfor d in list(g.values(dictlist[k])):\n\t\t\t\tsums += g[d]\n\t\t\t\tif len(list(g."}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\tdicts_sorted = [dictionary for dictionary in dicts if dictionary is not None]\n\t\tall_keys = sets(dicts.keys())\n\t\tall_keys_and_values = zip(all_keys, dicts.values())\n\t\tresult = []\n\t\tfor dictionary in dicts_sorted:\n\t\t\tresult.extend({'a': dictionary.count()})\n\t\t\tfor key, value in all_keys_and_values:\n\t\t\t\tif result[0][key] == value:\n\t\t\t\t\tresult[1] = value\n\t\treturn tuple(result)\n\t\t\n\tdef _dicttodict(self, key"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tif not d.has_key(k):\n\t\t\t\td[k] = v\n\t\t\telse:\n\t\t\t\td[k] += v\n\t\ndef _iterlist(l):\n\ti = 0\n\twhile i < len(l):\n\t\tyield l[i]\n\t\ti += 1\n\t\n\n#-----------------------------------------------\n#\n# \tTBD\n#-----------------------------------------------      \n\ndef _tuplesum(*tupledicts):\n\tif len(tupledicts) > 1:\n\t\td ="}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\ndef keydict(*dicts):\n\t\"\"\"\n\tReturn a mapping of (key, value) pairs for every key in the first dict.\n\t\"\"\"\n\treturn dict(((_value, _key) for _value in dicts)\n\n__all__ = ['update', '_make', 'keys','count']\n\nclass DictSummer(object):\n\t\"\"\"\n\tDictSummer combines list/dict values of a dictionary object.\n\t\"\"\"\n\t\n\tdef __init__(self, *dicts):\n\t\t\n\t\tassert len(dicts) <= 2\n\t\tself.dicts = dicts\n\t\t"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n## \t\n## \t\n## \t\n\t\n## \tret = { }\n\t\n## \tfor d in dicts:\n## \t\tif isinstance(d, list):\n## \t\t\tret.update(self._dictsum([d[k] for k in d]))\n\t\telse: # must be a dict\n\t\t\tret.update(d)\n## \t\t\n## \treturn ret\n## \n##\n## def _dictmerge(self, a, b):\n## \t\"\"\"\n\tMerge dictionaries supplied by other lists.\n\t\"\"\"\n## \t"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\t\n\tdef combine_dicts(dicts1, dicts2):\n\t\tfor d in dicts1:\n\t\t\tyield d\n\t\t\tfor d in dicts2:\n\t\t\t\tyield d\n\t\t\tyield d\n\t\n\tdicts1 = sorted(dicts1, key=_dictsum, reverse=True)\n\tdicts2 = sorted(dicts2, key=_dictsum, reverse=True)\n\treturn combine_dicts(dicts1, dicts2)\n\t\ndef _create_dict(keys, vals):\n\tvalues = []\n\tfor key in keys:\n\t\tvalues.append({key: float(vals["}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\tdicts = _dictmerge(*dicts)\n\t\tl = []\n\t\tfor a in dicts:\n\t\t\tl = l + [a]\n\t\treturn reduce(lambda acc, b : acc + b, l)\n\nclass _DictMerger:\n\t\"\"\"\n\tA wrapper class for merging dictionary data.\n\t\t\n\t>>> _DictMerger(OrderedDict([('a', 1), ('b', 2), ('c', 3)]).items())\n\t([('a', 1), ('b', 2), ('c', 3)])\n\t\"\"\"\n\tdef __init__(self, *dicts):\n\t\tfor"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\tfor i in range( len(url_args) ):\n\t\t\tp = url_args[i].replace('+', '%20')\n\t\t\turl = url.replace('+', '%20')\n\t\t\turl = url.replace('+', '%40')\n\t\t\turl = url.replace('+', '%20')\n\t\t\t\n\t\treturn url[:-1]\n\treturn ''\n\n\ndef _add_url_args2(url_str, url_args):\n\t\"\"\"\n\tAdd any custom string URL items into args\n\t\"\"\"\n\t\tfor i in range( len(url_args) ):\n\t\t\turl = url_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\turl_args = replace_url_args(url_args)\n\t\turl_args = url_args.replace('?', urllib.unquote(url_args))\t\t\t\t\t# remove any %20 after the %20\n\t\turl_args = url_args.replace('+', '%21')\t\t\t\t\t# replace %23 with a space and add space to url_args\n\t\turl_args = url_args.replace(':', '%23')\t\t\t\t\t# replace %29\n\t\turl_args = url_args.replace('*', '%29')\t\t\t\t\t# replace %29\n\t\t# Make sure all params are"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\ndef _replace_url_vars(url, url_vars):\n\t\"\"\"\n\tReplace any custom string URL variables with values in args\n\t\"\"\"\n\t\n#\tfor key in param_keys:\n#\t\t\told_key = key.strip()\n#\t\t\tif key in new_key:\n#\t\t\t\tparams['url_' + key] = new_key[key]\n#\t\t\telse:\n#\t\t\t\tparams[key.strip()] = original_vals[key.strip()]\n\n\n#\treturn params    # return original url_args\n\n\ndef get_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\tfor key in URL_KEYMAP.keys():\n\t\t\tif url_args.has_key(key):\n\t\t\t\turl_args[key]=url_args[key].replace(\"&%s\"%key, urllib.quote(str(url_args[key])))\n\n\t\treturn url\n\n\treturn str(url_args)\n\n\ndef get_url(url):\n\treturn _replace_url_args(url, _url_values['url'].keys())\n\ndef _url_values(dict_url):\n\tif dict_url[3]=='':\n\t\tdict_url2"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\tfor s, v in url_args.items():\n\t\t\tif '${' in s:\n\t\t\t\ts = s.replace('${'+s+'}', v)\n\t\t\telse:\n\t\t\t\ts = s.replace('${{'+'+s+'}', v)\n\treturn url\n\ndef get_host():\n\tif env.get('HTTP_HOST') is not None:\n\t\treturn env.get('HTTP_HOST')\n\ndef parse_header():\n\ttry:\n\t\theaders = urllib2.build_opener(urllib2.HTTPCookieProcessor(environ={'HTTP_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\ndef _replace_url_args_dict(url_args_dict):\n\t\n\tfor key,value in url_args_dict.items():\n\t\tif (key,value) in args:\n\t\t\targs[(key,value)]=value\n\ndef _get_default_api_version():\n\n\treturn requests.Session()\n\n\n# Add more URL resources here:\n# https://www.theforeman.com/forum/?forumidid=16\n\n# End of domain requests\n\ndef _process_kwargs(json_obj):\n\t\n\t# Get a full dict:\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n#\turl_args = parse_url_args(url_args)\n\t\ndef _replace_path_args(path, path_args):\n\t\"\"\"\n\tReplace any custom string path items with values in args\n\t\"\"\"\n\t\ndef _replace_query_args(query, query_args):\n\t\"\"\"\n\tReplace any custom string query arguments with values in args\n\t\"\"\"\n\t\n#\tquery_args = parse_query_args(query_args)\n\t\ndef _to_list_dict(obj, data):\n\t\n\tfor key in obj:\n\t\tif not"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\toriginal_dict = dict(url.items())\n\t\twith open(url, \"r\") as f:\n\t\t\tfor line in f:\n\t\t\t\tkey,value = line.strip().split(\"=\",1)\n\t\t\t\toriginal_dict[key] = value\n\t\treturn original_dict\n\n\nclass Url(object):\n\n\t\"\"\"\n\t\tClass responsible for handling URLs generated by Website.py\n\t\t@param str url: Link URL (string) or string (list of Strings)\n\t\"\"\"\n\n\tdef __init__(self, url):\n\t\t\"\"\"\n\t\t  \n\t\t@param str url"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n#\tif url.lower().endswith(u'/') or url.lower().endswith(u'/'):   # allow special characters here\n\t\t#        url = url.replace(' ',u'_')\n\tif url.lower().startswith(u'/') or url.lower().startswith(u'/'):\n\t return url_args\n\n\t# Get the normalized list of URLs\n\tfor key, value in url_args.iteritems():\n\t\t# Only convert values that match regular expression\n\t\tif re.match('^' + re.escape(key), value):\n\t\t\t# remove trailing slashes\n\t\t\turl"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n#\t\tif type(url) == list:\n\t\tfor item in url:\n\t\t\t_replace_url_args(item, url_args)\n\t\treturn url_args\n\t\n\telse:\n\t\t_replace_url_args(url, url_args)\n\ndef add_new_item(session):\n\t_logger.debug( 'get_item_list(session,url)', session)\n\tif 'new_item' in session:\n\t\t#TODO:\tcheck if new_item is present in the list\n\t\tif session['new_item']:\n\t\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tif not val: # Value is None (not str or None)\n\t\t\treturn False\t# Value is None\n\t\tif not isinstance(val, basestring): # Not a string (somewhat the above\n\t\t\tfrappe.throw(_(\"Value must be of type str\"))\n\n\t\treturn True\n\ndef scrub_urls(urls: Iterable) -> List[str]:\n\t\"\"\"\n\t\tRemoves and re-suburls from a list of string URLs and return\n\t\ta list of sanitized urls in the same manner as the list of strings in\n\t\turl1\n\t\thttps://gist.github.com/"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn not val in (\"\", None)\n\ndef make_dict_type(data: Mapping[Any, None]) -> Dict[Any, Any]:\n\t\"\"\"\n\tConvert a dict to a string.\n\t\"\"\"\n\t\treturn pickle.dumps(data, -1)\n\n\ndef find_unique_dicts(dic: Dict[Any, Any]) -> Iterator[Dict[Any, Any]]:\n\t\"\"\"\n\tTry to find unique values within a dictionary, returning\n\ta list.\n\t\"\"\"\n\tseen_dict_values = []\n\tfor key, values in dic.items():"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tif val == u\"\":\n\t\t\treturn None\n\t\treturn val is None\n\n\t@property\n\tdef has_empty_value(self):\n\t\treturn self.__val!= u\"\"\n\n\t@property\n\tdef is_empty_string(self):\n\t\treturn self.__val == u\"\"\n\nclass IntegerData:\n\n\t\"\"\"\n\tAn integer data structure.\n\t\"\"\"\n\t__slots__ = [\"num\", \"type\", \"scale\", \"value\", \"default\", \"extra_data\"]\n\n\tdef __init__(self, val: Union[int, float, str],"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\n\t\n\tif isinstance(val, str):\n\t\treturn val\n\n\treturn isinstance(val, bool)\n\ndef is_not_none(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents not None.\n\t\"\"\"\n\t\n\t\n\tif isinstance(val, str):\n\t\treturn False\n\n\treturn isinstance(val, bool)\n\ndef is_list(val: str):\n\t\"\"\"\n\tCheck if a string is a list.\n\t\"\"\"\n\n\t\n\n\treturn isinstance(val, bool)\n\n\ndef is_tuple(val: str):\n\t\"\"\""}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val is None or isinstance(val, str)\n\n# Copyright (C) 2011 Open Society Institute\n#\n# This program is free software; you can redistribute it and/or modify it\n# under the terms of the GNU General Public License Version 2 as published\n# by the Free Software Foundation.  Please review the following information to\n# ensure the GNU General Public License version 2 requirements will be met:\n# http://www.gnu.org/licenses/old-licenses/gpl-2.0.html.  To assist\n# your convenience, please contact:\n#"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tif val == NULL_STR and not force:\n\t\t\t# TODO: We should get a better message instead of printing it\n\t\t\treturn False\n\t\telif type(val) == str:\n\t\t\treturn val == NULL_STR\n\n\t\treturn False\n\ndef is_bool_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a True/False value.\n\t\"\"\"\n\t\tif val in [ON_BOOLEAN_VALUE, ON_BOOLEAN_TRUE]:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n# TODO: It's a bit silly"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tret = False\n\t\tfor v in val:\n\t\t\tret = ret and v.strip()\n\t\treturn ret\n\n\tdef encode(self):\n\t\t\"\"\"Encodes and returns the values as bytes.\"\"\"\n\t\treturn self.encode_value(force_bytes(self).decode('ascii', 'ignore'))\n\n\tdef decode(self):\n\t\t\"\"\"Decodes and returns the unicode strings as bytes.\"\"\"\n\t\treturn self.decode_string(force_bytes(self).decode('ascii', 'ignore'))\n\n\tdef encode_value(self, val: any, encoding='utf-8') ->"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn val and val.lower() in ('', \"null\", \"'null'\")\n\n\tdef is_comma(val: str) -> bool:\n\t\"\"\"\n\tCheck if a comma-delimited string represents a 'None' value.\n\t\"\"\"\n\t\tif '|' in val:\n\t\t\traise ValueError('Comma delimited values must not be enclosed in double quotes.')\n\t\telse:\n\t\t\treturn val.strip().lower() == \"none\"\n\n\tdef is_numeric(val) -> bool:\n\t\t\"\"\"\n\t\tCheck if a number represents a non-numeric value.\n\t\t\"\"\"\n\t\treturn float("}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\treturn False if val is None else val is NOT_NULL_VALUE\n\n\tif isinstance(val, basestring):\n\t\treturn True if val == NOT_NULL_VALUE else bool(val)\n\n\treturn False\n\ndef get_data_or_exception_type(row: Row) -> Type[Any]:\n\t\"\"\"\n\tReturn a type that represents the passed in row.\n\n\tIf there is no existing data, return the default type.\n\t\"\"\"\n\t\ttry:\n\t\t\tdata_type = row.data_type\n\t\texcept (RowDoesNotExist, IndexError):\n\t\t\tdata_type"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t\tif val is not None:\n\t\t\tstr_val = str(val)  # type: Any\n\t\telif val is None:\n\t\t\traise ValueError(\"null\")\n\t\telse:\n\t\t\treturn None\n\treturn True\n\nclass RollTimer(threading.Timer):\n\t\"\"\" This is the Timer which polls the RollTimer to the correct number of seconds\n\t\"\"\"\n\n\tdef __init__(self, callback):\n\t\t\"\"\" Constructor\n\t\tArguments:\n\t\t\tcallback (RollTimer): callback to call\n\t\t\"\"\"\n\t\tthreading.Timer.__init__(self)\n\t\tself."}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\nclass ParseFlags(argparse.Action):\n\t\"\"\"\n\tA parser for parsing flags that are not applicable in this process (they may be useful).\n\t\"\"\"\n\t\t\n\tdef __call__(self, parser, namespace, values, option_string=None):\n\t\t\"\"\"\n\t\tParses an option string from `values` and `namespace` to the values for a given namespace.\n\t\t\"\"\"\n\t\t# Validate the namespace, and if it's a valid argument.\n\t\t# Any values with a space are considered invalid.\n\t\tif namespace:\n\t\t\tif not re.findall(\"[\\w.](_["}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\tflagstring = ''\n\t\tfor attr in dir(parser):\n\t\t\tif attr not in (optparse._Store.NO_ARGUMENTS, optparse._Store.IN_ARG):\n\t\t\t\tflagstring += '{}{} '.format(attr, optparse._Store.NO_ARGUMENTS)\n\t\treturn flagstring\n\nclass ArgCheck(object):\n\t\"\"\"\n\tChecks that the supplied argparse.ArgumentParser (if any) has the required\n\targuments.\n\t\"\"\"\n\tdef __init__(self, argparser):\n\t\t\"\"\"\n\t\t@param argparser: A argparse.ArgumentParser instance,"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\nparser.add_argument('-v', '--verbose',\n\tdest='loglevel', type=int, default=logging.INFO,\n\taction='store',\n\thelp='set loglevel of logging messages')\n\t\t\nparser.add_argument('input_folder', type=str)\n\n# TODO: set the logger name once\nlogger = argparse.ArgumentParser(\n\tdescription='Run PuMPS.py within '\n\t'the PukuML format in the PuML format')\n\t\n\n# The following is a dictionary that may be assigned to any parameter as a value. If you"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\topts, args = parser.parse_known_args()\n\tif len(opts) > 0:\n\t\t# we only need the first (arg) argument, or a single long option, otherwise, the args will be empty\n\t\treturn parse_flags(parser, opts[0])\n\ndef check_parser_args(parser, argv):\n\tif hasattr(parser, 'get_args'):\n\t\targlist = parser.get_args()\n\telse:\n\t\targlist = None\n\treturn args, arglist\n\ndef check_args_are_empty(args, arglist):\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\tparser.add_argument('--no-input', '-n', action='store_false', dest='interactive',\n\t\thelp=\"Don't prompt for input to continue, rather than entering them again\")\n\n\tparser.add_argument('--interactive', '-i', action='store_false', dest='interactive',\n\t\thelp=\"Don't prompt for interactive input in interactive mode, rather than exiting\")\n\n\treturn parser.parse_args()\n\n# --- General Functions ----#\n# -----------------------------------------------------------------\ndef file_exists(path, exists = True):\n\t\"\"\"\n\tCheck whether a file exists"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef parse_commandline_args(parser):\n\t\"\"\"\n\tParse the flags, such as ``sys.argv``.\n\t\"\"\"\t\t\t\t\n\t\ndef args_for_cli(cmd_args, parser):\n\t\"\"\"\n\tParse the command-line command-line argument list.\n\t\"\"\"\n\t\t\n\t\n\tparser.add_argument('-v', '--verbose', '-v',\n\t\t\taction='store_true',\n\t\t\tdefault=False)\n\t\t\t\t\t\t\ndef __init__(options, args):\n\t\"\"\"\n\tConstructor, parses the arguments and parses the command-line arguments.\n\t\n\t:param"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\t# Parse flags, which is the last one before the first element.\n\t\tpos_arg = parser.add_mutually_exclusive_group()\n\t\tpos_arg.add_argument(\"--x\", dest=\"x\", help=\"Print x-coordinates in points.\",\n\t\t\taction=\"store_true\")\n\t\tparser.add_argument(\"--y\", dest=\"y\", help=\"Print y-coordinates in points.\",\n\t\t\taction=\"store_true\")\n\t\tpos_arg.add_argument(\"--z\", dest=\"z\", help=\"Print z-coordinates in points.\",\n\t\t\taction=\"store_true\")"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\ndef read_flag(fname):\n\t\"\"\"\n\tConvert flag path to a python name.\n\t\"\"\"\n\tl_file = open(fname, 'r')\n\ts_file = open(os.path.join(DIR_PATHS[\"--root\"], fname), 'r')\n\tl_flags = l_file.read().encode(\"utf-8\")\n\ts_file.close()\n\t\n\ts_file = open(os.path.join(DIR_PATHS[\"--root\"], fname) + '.toml_flag', 'r')\n\tl_toml"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\n\tparser.add_argument('-o','--output_dir', required=True,\n\t\t   help='output directory')\n\n\tparser.add_argument('-c','--config', required=False,\n\t\t   nargs='+',\n\t\t   help='the source configuration')\n\tparser.add_argument('-r','--config_file', required=False,\n\t\t   help='path to the source configuration file')\n\tparser.add_argument('-s','--source', required=False,\n\t\t   help='the source directory')\n\tparser.add_argument('-q','--quiet', required=False,\n\t\t   action='store"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\nparser.add_argument('-f', '--get-data', dest='get_data',\n\t\taction=\"store_true\", help=\"returns the data as a JSON string\")\t\nparser.add_argument('-d', '--download', dest='get_download',\n\t\taction=\"store_true\", help=\"returns the data as a CSV string\")\nparser.add_argument('-k', '--apikey', dest='apikey',\n\t\taction=\"store\", help=\"the API key\")\nparser.add_argument('-o', '--limit', dest='limit',\n\t\ttype=int, help=\"specify"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t    \n\t# Remove any non-existing entries (from the set, or from the list, \n\t# recursively call checkRecursivePathnames(processed))\n\tentries = set(processed)\n\t\n\tfor entry in entries:\n\t\tif not os.path.exists(os.path.join(path_name, entry)):\n\t\t\n\t\t\treturn False\n\t\n\tfor subentry in entries:\n\t\tif os.path.exists(os.path.join(path_name, subentry)):\n\t\t\n\t\t\treturn False\n\t\n\treturn True\n\t\ndef checkRecursivePathnames(processed, path_names_list"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n#\tif verbose: \n\t#\t\tprint \"Processing %s\" % path_name \n\t\n\tif verbose: \n\t\tprint \"Processing %s\" % path_name\n\t\n\tif os.path.isdir(path_name):\n\t\tfor dirpath, dirnames, filenames in os.walk(path_name):\n\t\t\tfor filename in filenames:\n\t\t\t\t# the file is a directory - process the entry \n\t\t\t\tif os.path.normpath(filename)!= os.path.abspath(dirpath):\n\t\t\t\t\tif verbose:\n\t\t\t\t\t\tprint \"File does not match the path or file\"\n\t\t\t\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\tif verbose:\n\t\t\tprint('Processing processed set %s' % processed)\n\t\tif path_name in processed:\n\t\t\tpass\n\t\telse:\n\t\t\tprocessed.add(path_name) # Add the entry to set\n\t\t\t# Now loop through the set and add the processed set\n\t\n\treturn True # True if the path has been processed already\n# -*- coding: utf-8 -*-\n\"\"\"Common classes for the data model\"\"\"\n\n# Copyright (c) 2012 Universidade Federal de Madrid\n\n# This file is part of CKAN.\n\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\tif verbose:\n\t\t\tdebug('Processed', path_name)\n\t\tprocessed.add(path_name)\n\t\treturn True\n\t\t\ndef get_path_for_path(path_name, level):\n\t\"\"\"\n\tSearch for a path name and find a proper path\n\tand return the path to it, or ``None`` if there is none.\n    \n    Args:\n\t    path_name: Path name in the search path\n\t    level: Sections to check\n\t\n\tReturns:\n\t    The path to the path found in the search path\n\t\n\tRaises:\n\t    None if path not found in the search"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tassert os.path.isfile(path_name)\n\t\tassert sys.platform == \"win32\"\n\t\t\n\t\tfiles = os.listdir(path_name)\n\t\t\n\t\tentries = 0\n\t\tfor file_ in files:\n\t\t\tif verbose!= True: print \"Processing\", file_, \"is processing.\"\n\t\t\t# try:\n\t\t\tprocessed_dir = path_name + \"_\" + file_.split(\".\")[-1]\n\t\t\tif not os.path.isfile(processed_dir):\n\t\t\t\tif verbose == True:\n\t\t\t\t\tsys.stderr.write(\"Couldn't find processed directory: \" + processed"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\tfound_paths = set()\n\tdirs_processed = set()\n\twhile not path_name.startswith('.') or path_name not in processed:\n\t\tfound_paths.add(path_name[1:])\n\t\tif len(found_paths)!= 1:\n\t\t\tverbose_output('Path is empty!')\n\t\t\tbreak\n\t\telse:\n\t\t\tfound_paths = found_paths.pop()\n\treturn found_paths == dirs_processed\n\t\n\ndef path_has_already_got_here(path_name, verbose=False):\n\t\"\"\"\n\tCheck"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t    if processed and path_name not in processed:\n\t\t    return False\n\t        processed_file = processed.pop(0)\n\t        if not os.path.isdir(os.path.dirname(processed_file):\n\t\t\t\t\t\t    if verbose:\n                                print >> sys.stderr, \"Produced path: \" + processed_file + \" but not in the set.\"\n\t\t\telse:\n\t\t\t\t\t\t\t\t\tprint >> sys.stderr, \"Produced path: \" +  processed_file + \" that is not a directory.\"\n\t       else:\n                              # If it's an absolute path then move and use the set.\n\t\t\t\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\tpathnames = []\n\t\tif isinstance(processed, basestring):\n\t\t\tprocessed = loaded.load(processed)\n\t\ttry:\n\t\t\tfor pathname in processed.keys():\n\t\t\t\tpathnames.append(os.path.realpath(pathname))\n\t\texcept OSError:\n\t\t\tpass\n\t\t# make sure the dir actually has its proper path\n\t\ttry:\n\t\t\tos.path.exists(os.path.dirname(pathnames[0]))\n\t\texcept OSError:\n\t\t\tpass\n\t\tif verbose:\n\t\t\tprint \"Processing directory %s\" % pathnames[0]\n\t\tlogger"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t    \tcheck_is_file_or_directory(processed)\n\t\t\n\t\tcur_entry_list = set([entry for entry in processed if entry.is_file])\n\t\tcur_entry_list = list(set(cur_entry_list))        \n\t\tcur_entry_list = list(set(cur_entry_list))\t\t\n\t\t\n\t\tif verbose:\n\t\t\tprint(\"Found %i new entries\" % len(cur_entry_list))\n\t\tif check_is_directory(path_name):\n\t\t\tcheck_files_in_directory(path_name)\n\t\telse"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\t\n\t# check if the path name has not already been visited (if so, raise an exception)\n\tvisited = visited_names(path_name)\n\t\n\t# check if the path name has already been processed\n\tif not visited:\n\t\t# check if the path cannot be processed\n\t\tif verbose and verbose_exception(path_name, silent=False):\n\t\t\t# use default\n\t\t\traise PathError(\"The specified path is not a file or folder. \"\n\t\t\t\t\t\t\t       \"Please specify path_name with --path-only.\")\n\t\t\n\t\t# if the path name hasn't been processed yet\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tvertices3D = []\n\t\n    for v in vertices:\n\t\tvertex3D = tuple([vertex3tuple(v_list) for v_list in v])\t\n\treturn vertex3D\n\t\t\t\n\n#end def loop\t\n\n#end def loop\n\n\n\n\n\n#end class verticesToGrid\n\nclass verticesGrid:\n\t\"\"\"\n\tA vertexGrid wraps a list with vertices and edges that are lists\n\tof vertices and edges.\n\t\"\"\"\n    def __init__(self, vertices, edges):\n\t\tself.vertices = vertices\n\t\t\n        self.edges = edges\n\n    #return"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tvertices = np.asarray(vertices)\n\t\n\tDAB = np.array(points[:,0])\n\tABC = np.array(points[:,1])\n\tBCD = np.array(points[:,2])\n\tCDA = np.array(points[:,3])\n\t\n\treturn tuple(np.asarray([DAB,ABC,BCD,CDA]))\n\ndef face3tuple(face):\n\t\"\"\"\n\treturn 3 tuples for each face which has a face vertices connected to the face. The face vertices are the opposite way to face. The face should be followed by three"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\tD = [[], [], []]\n\t\ti = 0\n\t\tfor vertex in vertices:\n\t\t\tif vertex[0] in vertex3list:\n\t\t\t\tD[-1] = vertex[1]\n\t\t\t\tD[-2].append(vertex2tuple(vertex))\n\t\t\telse:\n\t\t\t\tD[-1].append(vertex)\n\t\t\t\tD[-2].append(vertex3tuple(vertex))\n\t\t\ti += 1\n\t\treturn D\n\t\n\ndef point3tuple(vertices):\n\t\"\"\"\n\treturn 3 tuples for each point of the polygon, which can be constructed to add extra vertices if needed"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tdef _vertices2Vertices(vertices):\n\t\t\"\"\"\n\t\tdef _verticesToVertices():\n\t\t\t\"\"\"\n\t\t\treturn vertices\n\t\t\t#A    B    C     -> of vertices\n\t\t\t#A    D    E     -> oriented vertices with vertices A\n\t\t\t#A    G    G    B    C     -> of vertices A\n\t\t\t#G    D    G    B    D    E     -> oriented vertices with vertices B\n\t\t\t#A    E    G    G    B    D    E     -> oriented vertices with vertices B\n\t\t\t\"\"\"\n\t\t\tvertices"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\treturn vertex3tuple(vertices)\n\ndef vertex4tuple(vertices):\n\t\"\"\"\n\treturn 4 points for each vertex of the polygon. This will include the vertex and the 3 points on both sides of the vertex::\n\t\n\tpolygon with vertices \"ABCD\"\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 4tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\ndef vertex2tuple(vertices):\n\tif vertices:\n\t\treturn tuple(vertices[i] for i in range(len(vertices)))\n"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\tresult = []\n\tfor v in vertices:\n\t\tif isinstance(v, Vect3):\n\t\t\tresult.append((v.x, v.y))\n\t\telse:\n\t\t\tresult.append((v, v-1))\n\treturn result\n\t\t# -*- coding: utf-8 -*-\n\nimport logging\n\nfrom django.db import models\nfrom django.utils import timezone\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_time_range_in_timezone(start_time, stop_time):\n    \"\"\"\n    \n    :rtype: datetime.datetime\n    \"\"\""}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\treturn ((np.array(vertices) % 4)).astype(int) * 2 + (np.array(vertices) % 3)).astype(int) * 2 + (np.array(vertices) % 2)).astype(int) * 2 + (np.array(vertices) % 4)).astype(int) / 2\t\t\t# B\n\ndef vertex3tuple3(*vertices):\n\t\"\"\"\n\t\treturn 3 points for each vertex of the polygon, which will contain the 2 points of the vertices::\n\t\t\n\t\tpolygon with vertices ABCD\n\t\tWill return\n\tDAB, ABC, BCD"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\t\n\t#CBA-XYZ is a 2D Pointing Cube of DAB\n\t#CA    C    D    = of vertex C\n\t#CD    D    = of vertex D\n\t#BBD, CBA, CDC-> in vertices ABCD, CDBC-> in vertices BCD, CDC-> in vertices BCD, CDC-> in vertices BCD, etc.\n\tB = coo_matrix(vertices, dtype=float)\n\tCB = coo_matrix(vertices, dtype=float)\n\tDC = coo_matrix(vertices, dtype"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\ndef addPoint(d, v):\n\t\"\"\" \n\treturn vertex2dvector3(vertices)\n\t\"\"\"\n\t\n\tif len(v) == 0:\n\t\treturn v\n\t\ndef addPointList(l,r):\n\t\"\"\" \n\treturn 2d vectors \n\t\"\"\"\n\n\t\n\tvertex2dvector3 = []\n\tvertex2dvector3.append([0,0,0],[0,0,0,0])\n\tfor s in l:\n\t\tvertex2dvector3.append(sum(v[i] for i,v in enumerate(s"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\ndef matrix3coords(vertices, matrix, periodicity, periodicity2):\n\t\"Convert vertices (a, b, c, d) to matrix3 coordinates : to return vertex coordinates of a to B and C -> of the periodicity\"\n#\tprint vertices, matrix, periodicity\n\t#Create a new 1d tuple, with the vertices A,B,D,C,D->\n\tvertexlist=[]\n\tif len(vertices)==1:\n\t\tvertexlist.append([vertices[0], vertices[0], vertices[2]])\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\ndef str_to_int(string: str) -> int:\n\t\"\"\"\n\tConvert a number to an integer, using the given alphabet.\n\t\n\tIf the input is less than or equal to zero, the output will be an integer greater than the input number.\n\t\"\"\"\n\t\n\tassert len(string) >= 0\n\tnumber += (len(string) - 1)\n\t\n\tif (number < 0):\n\t\treturn (-1) * (1 - alphabet[0])\n\t\t\n\tif (alphabet[number]!= length(string)):\n\t\traise ValueError(\"Input is not a valid number\")"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\ndef bytes_to_int(bytes: \"bytes\") -> int:\n\t\"\"\"\n\tConvert a bytes string to an int\n\t\"\"\"\n\ttry:\n\t\treturn int(bytes, \"hex\")\n\texcept:\n\t\traise ValueError(\"bad length!\")\n\ndef string_to_int(s: str) -> str:\n\t\"\"\"\n\tConvert a string to an int\n\t\"\"\"\n\treturn int(s, \"hex\")\n\ndef int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n#\t\tstring = number+1\n\t\t\n\tif padding is None:\n\t\tpadding = number\n\n\t# Now we can encode the string with pad, then the remaining one.\n\t# We have to handle all upper-case cases.\n\tassert padding > 0\n\t\n\t# Make sure the number gets decremented\n\tnum_remaining = len(number)\n\n\t# Iterate through all digits\n\tfor digit in xrange(num_remaining):\n\t\tchars = [c for c in alphabet if number[c]!= digit]\n\t\tstring = \"\".join(chars).rjust(pad)\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tnumber = int(number, 16)\n\t# We have a number such that 2**number-1 == 0.1.\n\t# If first digit is 0 or more, then the last digit is usually first.\n\tif not (1 <= number <= 2**number):\n\t\tnumber += 1\n\tnumber_first_digit = number % 2\n\t\n\t# Build a sequence of 256-bit unsigned ints.\n\tsequence = list('0123456789abcdef')\n\t\n\t# The number is the number of bits left in the sequence.\n\tlast_bit = 0\n\t\n\t# If the last bit"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tout = \"\".join([str(i) if i > 0 else '-' for i in alphabet])\n\tif padding is not None:\n\t\tout = alphabet + padding\n\treturn out\n\ndef get_alphabet(alphabet_size: int):\n\talphabet = []\n\tcurrent_index = -1\n\tfor i in range(alphabet_size):\n\t  current_index = i + 1\n\t  current_alphabet = alphabet[current_index].lower()\n\t  if current_alphabet == alphabet_size:\n\t\t\talphabet.append(current_alphabet)\n\treturn \"\".join(alphabet)\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tinteger_digits = []\n\tif padding is None:\n\t\tpadding = -1\n\tfor i in range(0, number):\n\t    integer_digits.append(str(i) * (alphabet + padding))\n\treturn ''.join(integer_digits)\n\ndef float_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n        \"\"\"\n        Convert a number to a string.\n\t\t\n\t\tThe output has the most significant digit first.\n        \"\"\"\n       \n\tinteger_digits = []\n\tfor i in range(0"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tnum_chars = len(number)\n\tpos_length = number.find(alphabet)\n\tif pos_length > 0:\n\t\tnum_chars = num_chars, pos_length\n\t\n\twhile num_chars > 0:\n\t\tnum_chars -= 1\n\t\n\t# If `padding` is None (e.g., no extra space) then leave padding in\n\t# order to reduce the size of the string in word length.\n\tif padding is None:\n\t\tpadding = min_len\n\n\t# The padding amount to ensure at most one space character has been left\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\talphabet = set(alphabet)\n\tif not padding:\n\t\tassert number == 0, \"Can't convert negative ints only when padding is set.\"\n\t\tpadding = -3 - number\n\tresult = alphabet.difference(number)\n\tresult.sort()\n\t# add padding\n\tif padding is not None:\n\t\tresult.insert(padding)\n\treturn \"\".join(result)\n\t\ndef dict_to_hash(dictionary: dict) -> str:\n\t\"\"\"\n\tReturn a hashed version of the input dictionary.  Used for hashing strings.\n\t\n\tThis is based on"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\ndef str_to_number(num: str, alphabet: List[str], padding: Optional[int] = None) -> int:\n\t\"\"\"\n\tCreate a number ready to have a given number, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\ndef int_to_string_array(num: str, alphabet: List[str], padding: Optional[int] = None, numchars: int=1) -> List[int]:\n\t\"\"\"\n\tCreate an array with the given number of chars ready for use.\n\t\"\"\"\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\talphabet.sort()\n\t\tstring = alphabet[number % 4]\n\t\tif padding:\n\t\t\tstring += \" \" * padding\n\t\t\tstring += alphabet[-number % 4]\n\t\treturn string.rstrip()\n\na = int_to_string(10, alphabet=[\"a\"])\na_max = int_to_string(10, alphabet=[\"a\", \"b\", \"d\", \"j\", \"l\"])\n\ndef generate_random_str(length: int) -> str:\n\t\"\"\"\n\tCreate a random string with 'a' but only 'n', which means"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\tif len(register_value) > len(register_number) + 1:\n\t\t\t# Register value is truncated after the number of registers.\n\t\t\treturn 0\n\t\tvalue = register_value[:register_number]\n\t\tentry_name = register_number + '_' + value.replace(' ','_')\n\t\tentry_name = entry_name.strip()\n\t\tif not entry_name:\n\t\t\traise Exception, 'No entry name defined'\n\t\tdata = 'REGISTER' + entry_name + '_' + register_value\n\t\treturn data\n\t\t\t\ndef _replace_register_value(flow"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\t\n\t\tregister_number_to_register_key = dict(\n\t\t\t\t\n\t\t{\n\t\t\t\t\t\t\t'start_value_out': register_number,\n\t\t\t\t\t\t\t'value_out': register_value\n\t\t\t\t\t\t\t}\n\t\t\t\t\n\t\t)\n\t\tlogger.debug('Updating flow_parameters to %s', register_number_to_register_key)\n\t\t\n\t\tflow_params_to_update = {\n\t\t\t\t\t\t\n\t\t\n\t\t'id': register_number, \n\t\t\t'value in flash_value_in': register_value, \n\t\t\t'start_value_out': register_number, \n\t\t\t'tart_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\ndef _write_register(flow_params, register_number, register_value, flow_params_to_filename):\n        flow_filename = flow_params_to_filename.replace(\"_\" + str(register_number), \"\")\n        #filename = _find_flow_template('flow_flow_{}'.format(flow_params['flow_number']))\n        #filename = flow_filename+'\\n'\n        _write_template(\"register_flow_template\", flow_params)\n        flow_filename = flow_params_to_filename\n\n\n        _write_template(\"registers.json\", register_value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\t_key = {\n\t\t\"RegisterNumber\": register_number,\n\t\t\"RegisterValue\": register_value\n\t}\n\n\treturn _base._flow_map[flow_params['RegisterNumber']](payload=_key, params=_params, authKey=authKey, paramsKey=paramsKey)\n\t\n\n######################################\n@_base_flow_plugin.route('/get_config', methods=['GET'])\ndef get_config():\n\t\"\"\"\n\tGet settings from request.\n\t\n\tReturns:\n\t\tA dictionary containing settings of the flow\n\t\"\"\"\n\t\n\ttry:\n\t\treturn _"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\t# Check if register_number is string\n\tif not register_number.isdigit():\n\t\t\traise NameError(\"\\\"'{}' has to be string!\"\n\t\t\t.format(register_number))\n\t\t\n\t# Check value\n\tif register_number == '':\n\t\traise NameError(\"value '{}' doesn't exist!\".format(register_number))\n\t\t\n\t# Update registered value\n\tcurrent_values = dict()\n\t\t\n\t# Get values for register number\n\tfor key in register_number.split('_'):\n\t\tif key:\n\t\t\tcurrent_values[key] = register_value\n\t\t\n\t\telse:"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tfor key, value in flow_params.items():\n\t\tif'register_number' == value:\n\t\t\tflow_params[key] = register_number\n\t\telse:\n\t\t\tflow_params[key] = register_value\n\t\n\t\t\n\n\n# Removed from flow_dict but only if register not provided\ndef _get_input_from_register_params(flow_params):\n\tinput_params = {}\n\tfor key, value in flow_params.items():\n\t\tif key =='register':\n\t\t\tif value is not None:\n\t\t\t\tinput_params["}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tfor key in flow_params.__dict__:\n\t\tnew_register_number = register_number + key\n\t\tif new_register_number == register_number:\n\t\t\treturn\n\ndef _get_params_from_config():\n\treturn {\n\t\t'stop': 'ipflow-top'\n\t}\n\n#-------------------------------------------------------------------------------\n# Process the log file\n#-------------------------------------------------------------------------------\n\ndef save_log(filename=None):\n    \"\"\"\n    Write to the log directory or file if necessary\n    \n    If a file name was specified it will be returned and\n    a log will be written to the given filename"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\tprint \"Rewriting {register_number} with register value {register_value}...\".format(\n        register_number=register_number, register_value=register_value\n    )\n\t# replace value in dictionary with register number\n\tfor i, entry in enumerate(flow_params):\n\t\tfor (k, v) in enumerate(entry.keys()):\n\t\t\tentry[k] = (register_value, register_number)\n\n#\trefill\n\tfor key in flow_params.keys():\n\t\tif key in flow_params:\n\t\t\tvalue = flow_params"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\t\n\t\tregister_number = int(register_number) if register_number is not None else 999999\n\t\t\n\t\t# replace by reg\n\t\t# remove this key\n\t\t# from parameter name\n\t\tname = form.name\n\t\tparameters = {'regnum': register_number, 'nname': name, 'param': form.param.value().decode('hex')}\n\t\t\n\t\t# reinitialize flow variables\n\t\tflow_params['regnum'] = register_number\n\t\tflow_params['name'] = name\n\t\tflow_params['param'] = parameters\n\t\t\n\t\n\t\n\t\t# remove"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tentry = params.get(registration_number).entry_by_entry_uuid()\n\t\tif entry == None:\n\t\t\t_logger.error('Could not find value for %s - %s was not found',\n\t\t\t            params.get(entry_uuid), register_value)\n\t\telse:\n\t\t\t_logger.error('Will replace %s with %s', params.get(entry_uuid), \n\t\t\t                register_value)\n\texcept IndexError:\n\t\t_logger.error(\n            'Could not find value for %s - %s was not found in the"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\treturn value.replace('.', args)\n\t\n\ndef remove_comma(value):\n\t\"\"\"\n\tRemoves white space from the given string. \n\n\t\"\"\"\n\t\n\treturn value.replace(',', args)\n\ndef remove_non_space(value):\n\t\"\"\"\n\tRemoves non space characters from the given string. \n\n\t\"\"\"\n\t\ndef unescape_double_quotes(value):\n\t\"\"\"\n\tConverts double quotes to the real double, if necessary.\n\n\t\"\"\"\n\t\n\tvalue = value.replace('\"', '\\\\\"')\n\treturn value\n\n\ndef escape_quotes(value):\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\ndef escape(val):\n\t\"\"\"\n\tShorthand for escape calls; replaces the previous double-dash with\n\t\" \".\n\t\"\"\"\n\t\n\t\n\t# Instructions here:\n    ## The string itself (string or list of strings) is escaped according to the rules:\n    \n    def escape_to_str(val):\n        \"\"\"\n        Replace string with the same string replaced \n        by an underscore (without the dash)\n        \"\"\"\n        if type(val)!= type(\"\"):\n            val = str(val)+\"&\"+repr(val)\n        else:\n            val = val.replace(\"_\",\"\\\\\\\\\")\n        return val"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n\treturn value.replace('.', arg)\n\t\t\t\ndef strip_name(value, name):\n\t\"\"\"\n\tReturns the value of a dotted name, e.g., \"MyClass.MyClass\".\n\tConverts \"ClassName.\" to \"MyClass.\"\n\tIf value is one of the following types, strip it out of the tuple:\n\t\n\t(a, b), (a, \"Class1\", \"ClassName1\")\n\t\n\tOtherwise, strip() is a function that takes a value and makes it\n\tan object of myclass. MyClass instances convert to \"ClassNameClass.MyClass\""}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\tfor i in range(len(value)):\n\t\t\tvalue = value[i]\n\n\t\tstring = re.sub(r'(?<!\\\\)\\(.*)\\\\?', r'\\1', value)\n\t\tstring = re.sub(r'\\)\\)', r'\\1', string)\n\t\tstring = string.replace('\\\\', '\\\\\\\\')\n\t\tarray[arg] = string\n\n\treturn array\n\n\"\"\"\n# This function is only needed for the \"use python -m\" script because this\n# code is not intended to be run with interactive scripts\n\"\"\"\n\n# Function is used for testing purposes only\n\"\"\""}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\treturn value.replace('.', arg)\n\tdef replace_dash(value, arg):\n\t\treturn value.replace(',', arg)\n\tdef replace_backslash(value, arg):\n\t\treturn value.replace('\\\\\\\\', arg)\n\tdef replace_tilde(value, arg):\n\t\treturn value.replace('\\'', arg)\n\tdef get_env_var(name, default):\n\t\treturn os.environ.get(name, default)\n\tdef generate_install_script(filename, install_options):\n\t\treturn ''\n\tdef install_python_distribution(self, dist"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\tvalue = value.replace('.', ',')\n\t\treturn arg.replace('\"', '').replace('\\n','')\n\ndef replace_camelcase(value, arg):\n\t\"\"\"\n\tReplaces all values of CamelCase into an underscore and spaces \n\t\"\"\"\n\tresult = value.title()\n\tresult = re.sub('([a-zA-Z0-9_.]*)(\\.?)', '/', result)\n\tresult = re.sub('([a-zA-Z0-9_.]*)(\\.?)', '_', result)\n\treturn result\n\ndef parse_date_string"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n\tre_pattern = re.compile(r'^'+arg+'.*'.decode(sys.getdefaultencoding(), 'utf8'))\n\treturn re_pattern.sub(' ', arg)\n\ndef isValid(value, arg):\n\tif arg=='n':\n\t\treturn False\n\treturn True\n\ndef load_from_files(inputdir):\n\timport json\n\n\tfor filename in os.listdir(inputdir):\n\t\t\n\t\tif isValid(filename, 'data/') and os.path.isdir(inputdir + filename):\n\t\t\n\t\t\tfor (section, datafile) in"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\n\tvalue = value.replace('.', '\\u00A0')\n\treturn value\n\ndef replace_hyphen(value):\n\t\"\"\"\n\tReplaces all dashes (hyphen), converts dashes to underscores\n\t\"\"\"\n\tvalue = value.replace('-', '')\n\treturn value\n\t\ndef replace_dot(value, arg):\n\t\"\"\"\n\tReplaces all dots (dots), converts dots to underscores\n\t\"\"\"\n\tvalue = value.replace('.', '.')\n\treturn value\nfrom __future__ import absolute_import, division\n\nfrom collections import Iterable\n\nfrom.base import (\n   "}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\tif callable(arg):\n\t\t\targ = arg(arg.replace('.', '$$'))\n\n\t\tvalue = value.replace('$', '$$$')\n\t\targ = arg.replace('$', '$$$')\n\t\targ = arg.replace('$($)', '$$$')\n\t\targ = arg.replace('$(\\\\*', '$$$')\n\t\targ = arg.replace('$(:)', '$$$')\n\n\t\treturn value.replace(arg + '$$', arg)\n\n\tdef expandfile(self, s):\n\t\tvalue = ''\n\t\tfor word in re.findall('\\$\\((.*)\\)$', s):"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t\treturn re.sub(\"%([^\\/]+\\.)\", replace_dots, value)\n\ndef replace_colon(value, key):\n\t\"\"\"\n\tReplaces all items in key with value, in case the key has commas only\n\t\"\"\"\n\t\treturn re.sub(\"%[\\[]\" + key + \"]\", replace_colon, value)\n\nclass CliError(Exception):\n\tdef __init__(self, err):\n\t\tself.err = err\n\tdef __str__(self):\n\t\treturn \"%s: %s, %s\" % (self.err, traceback.format_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tinstances = []\n\t\tfor k in dir(super):\n\t\t\tv = getattr(super, k)\n\t\t\tif isinstance(v, types.MethodType) or issubclass(v,types.GeneratorType):\n\t\t\t\tinstances.append(v)\n\treturn instances\n\t\t\nclass SubTest(unittest.TestCase):\n\t\n\t\t@classmethod\n\t\tdef setUpClass(cls):\n\t\t\t\tsuper(SubTest, cls).setUpClass()\n\t\t\n\t\t@classmethod\n\t\tdef tearDownClass(cls):\n\t\t\t\tsuper(SubTest, cls).tearDownClass()\n\t\t@classmethod\n\t\tdef make_instance(cls, type):\n\t\t\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tcls = super(BaseClass, cls).subclasses()\n\t\tcls().update(cls.subclasses())\n\t\treturn cls\n\tdef update(self, instances):\n\t\t\"\"\"\n\t\tUpdates instance attributes on this class\n\t\t\"\"\"\n\t\tfor instance in instances:\n\t\t\tif instance.__dict__.get(\"name\", None) in self.classes():\n\t\t\t\tinstance.__dict__.update(self.classes()[instance.__dict__[\"name\"]]())\n\t\tsuper(BaseClass, self).update(instances)\n\tdef list(self):\n\t\treturn list(self.subclasses()) + list(self"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tsubclasses = [c for c in cls.__subclasses__() if issubclass(c, cls)]\n\t\tfor cls2 in subclasses:\n\t\t\tyield cls2\n# pylint: disable=missing-docstring\n\n_HEADER_TEMPLATE = \"\"\"\n// Copyright 2015 Google Inc. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tsubclasses = set()\n\t\tfor base in inspect.getmro(cls):\n\t\t\tsubclasses |= base.__subclasses__()\n\t\treturn subclasses\n\n\nclass ModuleLoader:\n\t\"\"\"\n\tThis class can load the modules of an instance of a class. When a submodule\n\tis loaded, any instance created (class, method, and function) is\n\tinstantiated with the module's attribute ``_modules``. The latter one\n\twill give the instances it will be in, and any instance\n\tthat will be unloaded (class) will remain unloaded when the class\n\tdoesn't inherit from it."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tcls.subclasses = []\n\t\tcls.bases = []\n\tcls.subclasses.append(type(cls))\n\treturn cls.subclasses\n\ndef recursiveList(parent):\n\t\"\"\"\n\tReturn the list of objects of type parent which does not inherit from\n\t\"\"\"\n\t\tdef _recursive(parent_klass, parent_bases):\n\t\t\tresult_types = parent_klass, parent_bases\n\t\t\tfor result_type, bases in result_types:\n\t\t\t\tfor base in bases:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tyield base\n\t\t\t\t\texcept NotImplementedError as e:\n\t\t\t\t\t\tprint \"subclasses of"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.__subclasses__()\n\n\nclass SubClass(object):\n\t\"\"\"\n\tA class describing a SubClass.\n\n\t:example:\n\n\t>>> sub = SubClass()\n\t>>> sub.subclasses\n[]\n\n\t\"\"\"\n\n\tdef __init__(self, id_, type_, value, name, is_set=False):\n\t\t\"\"\"\n\t\tClass constructor.\n\n\t\t:type id_: int\n\t\t:type type_: Union[str, int, bool]\n\t\t:type value: Any\n\t\t:type name: str\n\t\t:type is_set: bool\n\t\t\"\"\"\n\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\titems = []\n\t\tfor base in cls._get_subclasses():\n\t\t\titems.extend(base.subclasses())\n\t\treturn items\n\tdef all_subclasses(cls):\n\t\treturn []\n\tdef get_subclasses(cls, key=None, raiseonerrors=True):\n\t\tsubclasses = cls._get_subclasses()\n\t\tif key is None:\n\t\t\treturn subclasses\n\t\tfor c in subclasses:\n\t\t\tif key is None or key.is_private():\n\t\t\t\tif isinstance(c, subclass):\n\t\t\t\t\tyield c\n\t\t\t\telse:\n\t\t\t\t\tif c is not None:\n\t\t\t\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tif cls.__newsubclassname__ is None:\n\t\t\tclassname = cls.__module__.split('.')[-1]\n\t\t\tif '.' in classname:\n\t\t\t\tclassname = classname.split('.')[-1] + '.'\n\t\t\t__init__.__superclasses__(cls,classname,subclassname)\n\t\t\tfor subcls in cls.__subclasses__():\n\t\t\t\tyield subcls\n\t\t\tcls.__superclasses__(cls,classname)\n\t\treturn cls.__newsubclassname__\n\n\n@_decorators.register\nclass __prepare__(object):\n\t\"\"\"A class that can be implemented in __init__() as needed\"\"\"\n\tdef __new__("}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\tclasses = dir(cls)\n\t\tdef getsubclass(name):\n\t\t\tif name.startswith('_'):\n\t\t\t\traise RuntimeError((cls, name))\n\t\t\treturn cls.__dict__[name]\n\t\tif len(subclasses) == 0:\n\t\t\treturn getsubclass('T')\n\t\t\t\n\t\tsubclasses = list(subclasses)\n\t\t# Sort by order, and return class hierarchy\n\t\treturn [getsubclass(name) for name in subclasses\n\t\t\t\tif name in cls.__dict__.keys()][::-1]\t\t\n\t\t\n\tdef __repr__(self):\n\t\ts = \"\"\n\t\tfor c in self.__bases"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\treturn cls.mro()\n\ndef is_class(cls):\n\t\"\"\"\n\tReturn True if the class (or class that it belongs to) is a subclass of the\n\tcurrent classes (as opposed to the base classes)\n\t\"\"\"\n\t\treturn cls in cls.__mro__\n\ndef is_base_class(cls):\n\t\"\"\"\n\tReturn True if the class is a base class of its\n\tsubclasses (as opposed to the base classes)\n\t\"\"\"\n\t\treturn issubclass(cls, metaclass)\n\ndef is_decorator(cls):\n\t\"\"\"\n\tReturn True"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\tnum = 0\n\tfor c in alphabet:\n\t\tnum += ord(c) - 97\n\treturn num\n\n############################################################################################\n#\t\tGold -\t\tAuthor:\t\tKathmandu\n############################################################################################\n\nclass Gold(object):\n\t\"\"\"\n\tCreate a Gold matrix-like structure.\n\t\n\tThe class uses Gold's \"Morphological Algorithms\" to create the\n\tstandard golds, such as the \"Nuclear Modelling\" (e.g., \"Vigter\")\n\twith Houston's algorithm and"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\t# Split the string to the highest significant digit. A possible\n\t# case - all letters (i.e., a-z, \"a\"), the remaining letters (i.e.,\n\t# \"i\"), and whatever other characters (letters between them) will be\n\t# converted to uppercase.\n\tdigits = string.split(\"a-\")\n\tdigits = [int(digit) for digit in digits]\n\t\n\twhile ((len(digits) % 2) == 1):\n\t\tdigits.append(0)\n\treturn digits[0]\t\n\ndef string_to_string"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n# ######\n# Get an index of the string, the result of the find_non_negative_word function, and the sequence of the strings.\n# ######\ndef find_non_negative_word(text, maxlen=None, lowercase=False):\n\talphabet = string.upper()\n\tsequences = string.lower()\n\tif len(sequences) > maxlen:\n\t\t# there is less than 5 sequences\n\t\tsequences = sequences[maxlen:]\n\n\tif lowercase:\n\t\tsequences = sorted(sequences)\n\n\tsequences = [c for c in sequences if alphabet[c]"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tnumber: int = 0\n\tfor a in alphabet: number = number ^ ord(a)\n\treturn number\n\t\t\n\ndef str_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tnumber: str = ''\n\tfor a in alphabet:\n\t\tnumber = number ^ ord(a)\n\treturn number\n\t\n\timport os\nfrom datetime import datetime\ntry:\n    from urlparse import urljoin"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\tstring = string.upper()\n\t\tfor char in string:\n\t\t\tnum = ord(char)\n\t\t\tif num < 0x10000:\n\t\t\t\tsys.stderr.write(\"Unprintable character: \" + str(char) + \"...\")\n\t\t\t\tsys.exit(1)\n\t\t\tif num!= 97:\n\t\t\t\tsys.stderr.write(\"Illegal character: \" + str(alphabet) + \"...\")\n\t\t\t\tsys.exit(1)\n\treturn ord(string[0])\n\n\ndef string_to_hex(string: str) -> str:\n\t\"\"\"\n\tConvert"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tnumber = -99\n\tfor n in string:\n\t\tif not(n in alphabet):\n\t\t\traise TypeError(\"alphabet length must be a non-negative integer.\")\n\t\tnumber += n\n\t\t\n\treturn number\n\t\t\ndef parse_dict(data) -> dict:\n\t\"\"\"\n\t\tTake a dictionary and returns a key/value dict.\n\t\tOnly keys that start with '-' will be used, others not allowed to use\n\t\tnames.\n\t\"\"\"\n\tdict_data = {}\n\t\n\tfor key in data:\n\t\tif ',' in key: \t# First letter of the dictionary"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\ndef is_number(number: str) -> bool:\n\treturn all(map(isNumber, number.upper())) \n\n\n\ndef find_letter(string: str) -> List:\n\t\"\"\"\n\tFind all the letters from strings.\n\t\n\tEach valid character is represented by 0, 1, 2, 3, 4, 5, 6 or None.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\ndef get_word(string: str) -> str:\n\t\"\"\"\n\tGiven a string, try find_letter(string). \n\t\n\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\tnumber = 0\n\t\tfor c in alphabet:\n\t\t\tnumber = number * int(string[number % len(string) + 1:], 2)\n\t\t\tnumber = abs(number)\n\treturn number\n\ndef string_to_double(string: str) -> double:\n\t\"\"\"\n\tReturn a double based on the input, if the input is a sequence of digits.\n\t\"\"\"\n\t\tif (len(string) < 3\n                or string[0] in string[1:]\n                    or string[0] in string[1:3]\n                    or (string[0] == '"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\ts = str(string)\n\t\tif not s.strip().isdigit():\n\t\t\ts = str(string_to_int(s))\n\t\treturn int(s)\n\ndef string_to_number(string: str, alphabet: List[str], split: bool=True) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\ts = str(string)\n\t\tm = string_to_number(s, alphabet)\n\t\t#s = str"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n\tdef number_to_string(num: int, letter: str) -> str:\n\t\t\n\t\treturn '%0' % (alphabet[num-alphabet.index(letter)-1] + (num-alphabet.index(letter)))\n\n\tfirst = int(string) - len(string)\n\tsecond = first // 2\n\tthird = int(first) + len(string) % second\n\tfourth = first % third\n\tfifth = first % third\n\t\n\treturn first + '0' + string_to_int(string[:fifth], alphabet) + string[fifth"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t    \n\t# get json file from the zip file\n\twith open(destination_path, 'r') as r:\n\t\tdata = r.read()\n\t\tdata = data.decode('utf-8')\n\t\t\n\t\t# get contents of package zip file\n\t\twith open(destination_path, 'r') as r:\n\t\t\tdata = r.read()\n\t\tdata = data.decode('utf-8')\n\t\t\n\t\t# parse into a directory Path\n\t\tname = data.rstrip(\"/\").split(\"/\")\n\t\t\t\n\t\t# create Arch Linux repo\n\t\trepo = ArchLinux(url, name["}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\twith open(destination_path, 'wb') as destfile:\n\t\tdestfile.write(urllib.request.urlopen(url).read())\n\t\n\twith tarfile.open(destination_path, \"r|gz\") as tar:\n\t\ttar.extractall(destination_path)\n\t\n\t\n\treturn destination_path \n\n# @package archive_utils\n# @brief Class to perform operations on archive files of.tar.gz\n# @param src_dir: top-level directory where the archives are stored and unpacked\n#\t\t(must already exist)\n#\t\t(must"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\tdef pathextract(package_name, url, destination_path):\n\t\t\"\"\" Extract an archive, extract.tar.gz to extract \"\"\"\n\t\tarchive = requests.get(url)\n\t\tarchive.content = archive.content.decode('utf-8')\n\t\t\n\t\ttar_gz = tarfile.open(os.path.join(destination_path, package_name), 'r:gz')\n\t\ttar_gz.extractall(path=destination_path)\n\t\ttar_gz.close()\n\t\t\t\n\t\t\nfrom itertools import groupby\nfrom collections import namedtuple\nimport logging\nfrom collections."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\turl = \"api2/v4/files/\" + destination_path\n\t\tresp = requests.get(url)\n\t\tif resp.status_code == 200:\n\t\t\tdata = resp.json()\n\t\telse:\n\t\t\traise ArchlinuxError(\"failed to download tar.gz archive from \" + url)\n\t\tif data[\"data\"] is None:\n\t\t\traise ArchlinuxError(\"file is not present on disk\")\n\t\tname = data[\"data\"][\"name\"]\n\t\tfilepath = data[\"data\"][\"full_path\"]\n\t\tfilepath = os.path.basename(filepath)\n\t\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\tif not os.path.exists(destination_path):\n\t\traise Exception(\"Invalid destination path %s\" % destination_path)\n\t\n\tarchive = subprocess.check_output([\"tar\", \"--table\", \"DESC\"], \n\t\tstdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tcontent = []\n\tfor line in archive.replace(\"\\r\\n\",\"\"):\n\t\tpackage_line = line.rstrip().split(' ')\n\t\tif len(package_line)<2:\n\t\t\traise Exception(\"Invalid line %s\" % line)\n\t\tpackage_line = package_line"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\n\treq = requests.get(url + \"/archive.tar.gz\")\n\tif req.status_code == 200:\n\t\treturn destpath + \"/\" + url.split(\"/\")[-1]\n\telif req.status_code == 404:\n\t\traise urllib.error.HTTPError(\"Archive does not exist.\")\n\n\t# Not a normal file.\n\traise urllib.error.HTTPError(\"Unable to download tarball.\")\n\n\ndef extract_tar(srcpath, destination_path):\n\t\"\"\"\n\tGiven an absolute path to a destination path, return the extracted tarball.\n\t\"\"\"\n\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\tdef run_tar(self, url, destination):\n\t\t\"\"\"\n\t\tGiven an url, extract and extract the.tar.gz archive\n\t\twhich contains 'DESC' file for each package.\n\t\t\n\t\tArgs:\n\t    url: url of the.tar.gz archive to download\n\t    destination: the folder to extract the archive \n\t\n\t\t\n\t\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\t\"\"\"\n\t\t\n\t\t\n\t\twith open(\"dir/\" + destination + \"/\" + url, 'w') as tmpf:\n\t\t\tfor package in self.download_packages:\n\t\t\t\tif not package in"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t    \n\turl = \"%s/core/extra/comm\" % destination_path\n\tif not os.path.isfile(url):\n\t\turl = open(url).read()\n\t\tprint 'url is not a file at %s' % url\n\treturn url\n\t\t\n@requests.get(url, stream=True)\ndef tarFileDownload(url, destination_path):\n\t\"\"\"Downloads a single file from a TAR archive.\n\t\n\tArgs:\n\t    url: the url to download\n\t\t       tar.gz tar file.\n\t\t       If a URL is present then all files should"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t\n\tif destination_path.endswith('.tar.gz'):\n\t   extract_url = destination_path\n\telse:   \n\t       extract_url = os.path.join(os.path.dirname(destination_path), 'tar.gz')\n\t\t\n\ttar = None\n\tif os.path.exists(extract_url):\n\t\ttar = tarfile.open(extract_url, 'r:gz')\n\tif not tar:\n\t\treturn None\n\t\t\n\tfor name in tarfile.open(tar.name, 'r:gz'):\n\t\t#print('Extracting File: {0}'."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\ndef retrieve_tar_gz(url, destination_path):\n\t\n\tlogger = logging.getLogger('TarGurlFetch')\n\tlogger.info(\"Retrieving archive tar file %s from %s\", url, destination_path)\n\ttar = requests.get(url)\n\t\n\tif tar is None:\n\t\tlogger.error(\"Could not download URL %s from GURL\" % url)\n\t\traise Exception('Could not download URL %s from the GURL' % url)\n\t\n\tlogger.info('\\nArchive: %s' % tar.status_code)\n\t\n\ttar"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\ndef check_MacOS():\n\tif not os.path.isfile(\".test_macos.out\"):\n\t\treturn False\n\t\n\telse:\t\n\t\tif os.stat(os.path.path.join(test_app, \"testing_macos.out\")).st_size == 2225:\n\t\t\treturn True\n\n\treturn False\n\t\n\t\nclass MacOSGenerator:\n\t\n\t#The MacOS generator used to check whether a device is macOS.\n\t#It allows to set the device to 'MACOSX'\n\t#and the directory that the OS is in.\n\t_"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\tif os.name =='mac':\n\t\t\tos.environ['MACOSX_DEPLOYMENT_TARGET'] = '1132'\n\t\t\tos.environ.update(os.environDefault)\n\t\t\treturn True\n\t\tif os.name == 'win32':\n\t\t\tos.environ['MACOSX_DEPLOYMENT_TARGET'] = '1133'\n\t\t\tos.environ['MACOSX_DEPLOYMENT_VERSION'] = '10.8'\n\t\treturn False\n\t\nclass AppVersionMissing(Exception):\n\tpassimport six\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom tensorflow"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\tif os.access(\"/etc/os-release\", os.R_OK):\n\t\tos.uname(\"uname -a\").close()\n\t\treturn True\n\telse:\n\t\n\t\treturn False\n\t\t\n\t\ndef set_user_locale():\n\n\t\"\"\"\n\t\tSet the localization to 'en_US'\n\t\t\n\t:return: True\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\n\ts = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\ts.connect((\"10.255.255.128\", 4100))\n\t\n\t\n\t\n\t\treturn True\n\t\n\t\n\t\t\n\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\ndef checkMacOS(isMAC=True,isOS=True):\n\t\"\"\"\n\tChecks the macOS\n\t\n\t:param isMAC: whether to check the macOS\n\t:type isMAC: bool\n\t:param isOS: whether this is a MacOS\n\t\nReturns\n\t:Boolean: True or False\n\t\"\"\"\n\tif os.name == 'posix':  # Mac OS\n\t\tmacOS = \"/dev/config/\"\n\telse:\n\t\tmacOS = \"/bin/maccheck\"\n\n\tif not isMAC:\n\t\tcheckMacOS()\n\n\treturn"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\ndef run(args):\n\t\"\"\"\n\tCheck if the os is supported\n\t\n\t:param args: list of argument tuples\n\t:type args: List[Tuple]\n\t:return: True if the OS was supported\n\t:rtype: bool\n\t\"\"\"\n\t\n# TODO: Move this to something more generic.\ndef check_linux_kernel():\n\t\"\"\"\n\tChecks if the system is linux\n\t\n\t:return: True if linux\n\t:rtype: bool\n\t\"\"\"\n\t\n\t\n\ndef check_macOS():\n\t\"\"\"\n\tChecks if the system is macOS\n\t\n\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\nclass MacOS(object):\n\tdef __init__(self, *args, **kw):\n\t\tpass\n\n\tdef check_supported(self, options):\n\t\tfor option in options:\n\t\t\tif os.name.startswith('osx:'):\n\t\t\t\treturn True\n\n\tdef check(self):\n\t\treturn self.check_supported(options)#!/usr/bin/env python\n# Copyright 2014 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport glob\nimport logging"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\tif os.name =='macOS':\n\t\t\t return True\n\t\telse: \n\t\t\t\treturn False\n\"\"\"\n#    if os.name == 'nt':\n\t# \t\t# On windows, the os on Unix is Mac, but on Linux it can't be anything.\n\t# \t\treturn os.name == 'nt'\n\t# \t\treturn \"win32\"\n\t# \t\treturn os.name == 'posix'\n\t\t\ndef isMac(path):\n\t\"\"\"\n\tChecks if a path is a macOS\n\t\"\"\"\n\t\n\tif os.name == '"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\ndef run_if_os_is_mac_linux():\n\treturn True\n\t\n\n\ndef check_if_path_exist(path):\n\tpathstr = None\n\tif os.path.isfile(path):\n\t\tpathstr = path\n\t\ndef check_if_file_already_exists(path):\n\t#This function checks if the path is already exist\n\t\n\tif not os.path.isfile(path):\n\t\treturn\n\tif os.path.getsize(path) < 500000:\n\t\treturn\n\t\n\tpathstr = path\n\tif not os."}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\nif __name__ == \"__main__\": \n    __init__(sys.argv)\n    main()\n \n#\n# Copyright 2009-2017 Splunk, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\n\treturn (os.uname()[0] == \"Darwin\")\t\n\t\t\t\t\t\tfrom.models import Author, Entry, Slack, User\nfrom django.http import Http404\nfrom django.template.response import TemplateResponse\nfrom django.shortcuts import get_object_or_404\nfrom django.core.urlresolvers import reverse\nfrom django.contrib.auth.decorators import login_required\nfrom django.shortcuts import redirect\n\nfrom socialauth.backends import AuthBackend, BaseOAuth2\nfrom socialauth.backends.oauth import OAuthBackend\nfrom oauth2_provider import models, providers\n\nfrom socialauth"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tpatterns = [\n\t        ('(?:a|b)', r'Arf', 'arf', r'arf'),\n\t        # ('(?:a|b)', r'Arf_d', 'arf_d'),\n\t        # ('(?:a|b)', r'Arf_b', 'arf_b'),\n\t\t\t\t\t\t# ('(?:a|b)', r'Arf_c', 'arf_c'),\n\t\t\t\t\t\t('#(?:a|b)B', r'Arf_b_d', 'arf_b_c', 'help')  \n\t\t\t\t\t\t]\n\t"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tregex_keys2 = {}\n\t\n\n\tfor fn, fn2, fn2l, fn2n, fn2l2n, fn2h in item.items():\n\t\tif fn is not '' and fn2n == 'c':\n\t\t\tcontinue\n\t\tkey = fn2l2n.split(\".\")\n\t\treg_key = key[1]\n\t\t\n\t\tregex_key = [fn.upper()+\"_\"+key[0]]\n\t\treg_match = re.match(\".*\",reg_key,re.IGNORECASE)\n\t\t\n\t\t#print(fn2l,reg_"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tregex_keys = map(lambda x: x[0], regex_keys)\n\t\n\tregex_keys.sort(key=lambda s: re.search(r'a(\\d+)', s).group(0), reverse=True)\n\t\n\toutput_path = os.path.join(CONFIG.get('tools.cache.dir'),\n\t                        '{}_{}_out'.format(hash(item.get('_id')), hash(regex_keys[0])))\n\t\n\tif not os.path.exists(output_path):\n\t\tos.mkdir(output_path)\n\t\n\tregex_keys"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tregex_keys = [(re.compile(f), re.compile(re.escape(s))) for f, s in regex_keys]\n\tregex_dict = {key: regex_dict[str(key)] for key in regex_keys}\n\t\n\n\tif not any([regex_dict[str(key)] for key in regex_keys]):\n\t\traise ParserError(\"Keys in strings should be regex strings\")\n\t\n\treturn regex_dict\n\n\n\ndef parse_dict_from_rules(rule_dict, rules, replace=True):\n\t\"\"\"\n\tConvert rule_dict"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tresult = {}\n\t\n\t\n\tregexes = []\n\tfor entry in regex_dict_from_keys(item):\n\t\tregexs.extend(re.findall(entry['pattern'], regex_dict_from_keys(entry['keys'])))\n\t\tresult.update(regexes)\n\t\n\treturn result\n\t\n\ndef remove_rule_keys(regexs, item):\n\t\"\"\"\n\tRemove the keys contained by the key dict in the dict.\n\t\n\tArgs:\n\t    regexs: regexs the keys should be removed\n\t\"\"\"\n\tregex_keys = [entry['pattern'] for"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\tfor key in regex_keys:\n\t\t\tregex_dict[key] = re.compile(key)\n\t\n\t#return regex_dict\n\t\n\tdef convert_regex(regex_dict, pattern):\n\t\tresult = regex_dict.get(regex_dict['a'])\n\t\tif result:\n\t\t\tregex_dict = re.compile(pattern)\n\t\t\treturn regex_dict\n\t\telse:\n\t\t\traise ValueError('%s. Cannot convert regex to regex' % regex_dict['a'])\n\t\n\treturn result\n\t\t\n\nclass FileMap(dict):\n\tdef __new__(self, data):"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\trules['*.cpp'] = list(re.findall('''^.*-?(a|b|c|o|d|g|j|h|k|l|m|n|o|p|q|r|s)$', item['name']))\n\t\n\trules['*.h']   = regex_dict(rules['*.h'])\n\t\n\trules['*.m']    = regex_dict(rules['*.m'])\n\trules['*.n']      = regex_dict(rules['*.n'])\n\trules['*.p']     = regex_dict(rules['*.p"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t    # replace all special keys\n\titem['a1'] = re.sub('(?m)^[^a-zA-Z_\\w]*$', '', item['a'].rstrip())\n\titem['b1'] = re.sub('(?m)^[^b-zA-Z_\\w]*$', '', item['b'].rstrip())\n\t\n\treturn get_strings(regex_keys)\n\t\t\n\ndef match_keys_in_string(matches, item):\n\t\"\"\"Takes a list of regexes and returns whether there is an object match with the match keys.\n\t\n\tArgs:\n\t    matches: iterable of regexes that"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\tmatch = re.compile(item['a'])\n\t\n\tif match.group(2) == \"arf\" or match.group(2) == \"bark\":\n\t\tkey_to_convert = item['h']\n\t\tconvert_keys = [key_to_convert[x].replace(\"'\", \"\\\\'\") for x in match.group(1).split(\",\")]\n\telif match.group(2):\n\t\tkey_to_convert = item['c']\n\t\tconvert_keys = [re.sub(x, \"\\\\'\", string.lower(key)) for x in match."}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t\n\trule_regex = re.compile(item['regex'])\n\t\n\t# Replace all the keys in the dict with the regex pattern \n\tfor key in rule_regex.findall(item['cpp']):\n\t\trule = dict()\n\t\tfor match in rule_regex.finditer(item['cpp'], item):\n\t\t\trule[match.start():match.end()].append(key)\n\t\titem = match.group()\n\t\n\t\n\treturn itemimport csv\nimport io\nimport datetime\nimport pandas as pd\n\nimport ujson as json\nfrom ujson.decoder import JSONDecodeError\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\nclass Parser_Remove_Quoted_quote:\n\t\"\"\"\n\t\tRemove quotes from given name.\n\t\"\"\"\n\tdef __init__(self, remove):\n\t\tself.remove = remove\n\t\t# Regex for quotation. \n\t\tself.regex = '([\\\"\\'][^\\s\"\\']+)'\n\tdef remove_quotes(self, text):\n\t\tmatch = re.search(self.regex, text)\n\t\tif match!= None:\n\t\t\tself.remove = match.group(1)\n\t\t\treturn match.group(0)\n\t\telse:\n\t\t\treturn text\n\tdef parse"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\nclass RemoveQuote(object):\n\t\"\"\"\n\tRemove quote from the supplied name.\n\t\"\"\"\n\t\nclass ParseArgs(object):\n\t\"\"\"\n\tProcess arguments passed by the calling program.\n\t\"\"\"\n\tdef __init__(self, args):\n\t\t\"\"\"\n\t\tRead and process list from STDIN\n\t\t\n\t\t:type args: argparse.Namespace\n\t\t:type args: argparse.Namespace\n\t\t:rtype: list\n\t\t\"\"\"\n\t\tself.args = args\n\t\t\n\t\t# Remove leading quotes.\n\t\tself.args.replace('\\\"', '')\n\t\t\n\t\t# Remove trailing quotes.\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ndef parse_quote(q):\n\tr\"\"\"\n\t\tParse the given quote. If it's an empty string, then return False. Otherwise,\n\t\treturn the whole string.\n\t\t\"\"\"\n\t\tif not q and len(q) == 0:\n\t\t\treturn False\n\t\t\n\t\treturn \"\".join(q)\n\t\n\tdef find_quotes(str):\n\t\tr\"\"\"\n\t\tReturn a list of all quote characters which are inside an escaped\n\t\tstring.\n\t\t\"\"\"\n\t\tq = []\n\t\tfor c in str:\n\t\t\tif len(c) <= 2:\n\t\t\t\tr\"\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\tname = name.strip().replace('\"', '')\n\t\t\n#\tdef remove_string(self, string):\n#\t\t\"\"\" Remove an arbitrary string (string is a python list or list-like).\n\t#\t\t\"\"\"\n#\t\tassert not type(string) == list or type(string) == str, str\n#\t\treturn ''.join( list(re.findall(r'\\{(\\w+):\\,?(%s)' % string, item) ) for item in string )\n\t\t\n\tdef get_quote_name(self):\n\t\treturn self.__name\n\t\t\t\n\tdef"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\tresult = [name]\n\t\tfor i in range(len(name)):\n\t\t\tname = name[1:]\n\t\t\tmatch = re.match('\\s*\\s*\\(')\n\t\t\tif match:\n\t\t\t\tidx = 0\n\t\t\t\twhile idx < len(name)-1:\n\t\t\t\t\tif name[idx] == '{':\n\t\t\t\t\t\tresult.append(name[idx+1:])\n\t\t\t\t\t\tresult.append(',')\n\t\t\t\t\tidx += 1\n\t\tresult = ''.join(result)\n\t\treturn result\n\n\nfrom subprocess import Popen\nfrom sys import exit\nif len(sys.argv)!= 3:\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ndef quote (text, quote = False):\n\t\t'''\n\t\tReturn the specified text without quotes.\n\t\t\n\t\t:type text: str\n\t\t:param text: the line containing text to extract the quote\n\t\t:type quote: bool\n\t\t:param quote: whether the text should be safely quoted\n\t\t:rtype: str\n\t\t:return: the text quoted\n\t\t:rtype: str\n\t\t:return: the text without quotes\n\t\t\t\n\t\tExamples:\n\t\t\n\t\t>>> a=re.compile(\"\\A(.*?) B (\")\n\t\n\t\"\"\"\n\t\ndef replace (text, in_str, quote"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\n\t\n\tp = re.compile( r\"(&|\\\\)(?![a-zA-Z0-9\\$&\\[\\]\\(\\)\\[\\],/?) +([^;]*)\", re.I )\t\n\tl = list()\n\tfor l in p.sub( r\"\\1\\2,\", name ):\n\t\tl = re.sub( r\"\\1\\2,\", \", \", l )\n\t\n\t\ndef remove_spaces_from_name(name):\n\t\"\"\"\n\t\tRemove space from a name.\n\t\t\"\"\"\n\t\t\n\t\tfor r in re.findall( r\".*? \\w"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ndef removeQuote(fname):\n\t\"\"\"\n\tRemove quotes from the given filename.\n\t\"\"\"\n     \n\tpattern = re.compile(\"\\\"([^'\\\"]*['\\\\']\")\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\n\t\n\tf = open(fname + \".txt\",\"r\")  \n\tout = f.read()\n\t\n\tret, tmp = re.split(pattern,tmp)\n \tf.close() \n \ttmp = re.sub('(?i)\\W+', '', tmp.lower())\n\t \n\t\n\t\tif not out.startswith(\" \"):\n\t\t\treturn \"Failed to remove invalid quotes.\"\n\t\n  "}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\treturn re.sub('^\"'+quote_quote+\"|,\",\"', name)\n\t\t\n\tdef get_quote_from_name(self, name, regex_type):\n\t\ttry:\n\t\t\treturn re.search(self.quote_regex,name).group(1)\n\t\texcept:\n\t\t\treturn None\n\t\t\n\tdef remove_quote_from_name(self, name):\n\t\treturn re.sub('^\"'+quote_quote+\"|,\",\"', name)\n\n\tdef __str__(self):\n\t\treturn self.name_replace(self.name)\n        \n# ===============================================================================\n#"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\nclass Cog(object):\n\t\"\"\"\n\t\n\t\t\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.name = \"dog\"\n\t\n\tdef get_string_from_url(self, url):\n\t\turl = re.sub(\"https://.*\\.(gif|image|png)$\", \"\", url)\n\t\tif url.startswith(\"http\"):\n\t\t\turl = \"http://\" + url\n\n\t\treq = urllib2.Request(url)\n\t\tr = urllib2.urlopen(req)\n\t\tout = \"\"\n\t\twhile r.info().get_header(\"content-type\","}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tplatform = ''\n\tdef __new__(cls):\n\t\td = type.__new__(cls)\n\t\tif d.__module__ in ('string', 'unicode', 'range', 'input', 'inputfile'):\n\t\t\td.__module__ = d.__module__\n\t\t\td.__dict__.update(d.__dict__)\n\t\treturn d\n\t\n\tif not (is_string(platform) and is_py3k():\n\t\tif platform.find('posix') < 0:\n\t \t\tp2 = 'posix'\n\t\telse:\n\t\tp2 = ''\n\telse:\n\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\ndef re_split(pattern, repl, split_lines):\n\t\"\"\"\n\tRegular expression matching of the group-by pattern.\n\t\n\tpattern: pattern to split on in, e.g. '/home/someone/bin'\n\t\"\"\"\n\t\n\tm = re.match(pattern, repl)\n\tif m:\n\t\treturn m.groups()\n\telse:\n\t\treturn repl\n\ndef split_shell(shell_line):\n\t\"\"\"\n\tSplits a shell line\n\t\"\"\"\n\treturn re_split(shell_line, r\"SHELL\")\n\ndef"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\tif not isinstance(platform, (text_type, binary_type)) and \\\n\t\t\tnot sys.platform.startswith('-') \\\n\t\t\tand platform!= sys.platform[:-1]:\n\t\t\traise ValueError('Invalid platform name: %s (%s)' % (platform, sys.platform))\n\t\tif platform is None:\n\t\t\tret = shlex.split(command).split()\n\t\t\t# The argument list is empty for py2/py3 compatibility.\n\t\t\tplatform = 'this'\n\t\t\tfor value in ret:\n\t\t\t\tif platform == 'POSIX':\n\t\t\t\t\tplatform = value\n\t\t\t\telif platform."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tif platform == 'this':\n\t\tshlex.split = shlex.split\n\telif platform == 'posix':\n\t\tshlex.split = shlex.splitAndEval\n\telse:\n\t\t#TODO: Support cases specific to UNIX, as it is required\n\t\t#\tbut not as expected\n\t\tprint \"WARNING: unsupported platform '%s'. Currently supported only on unix platforms\" % platform\n\t\treturn []\n\t\n\targs = shlex.split(input)\n\t\n\tif len(args) == 0:\n\t\tprint \"Syntax: shlex.split() [-l|-a -o PATH...]\"\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\nclass CommandSplitCmdError(Exception):\n\tpass\n\nclass Command(object):\n\t\"\"\"\n\tAbstract base class.\n\t\n\t@type name: str\n\t@param name: command name as string (required)\n\t@type description: str\n\t@param description: description as string, empty to clear description string\n\t\t\n\t@cvar regex: text regular expression (default is %s)\n\t\n\t@cvar allow_shell_alias: Allow shell aliases\n\t@type allow_shell_alias: bool\n\t\"\"\"\n\t\n\tdefault_argv_string = \"%s%s\" % (name"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n__revision__   = \"$Id$\"\n__date__    = \"$Date$\"\n__author__ = \"Brad Holkner\"\n__copyright__ = \"Copyright (c) 2005-2008 by Brad Holkner\"\n__license__   = \"Web License\"\n__contact__     = \"brad.holkner@gmail.com\"\n__license__   = \"\"\n\n#==============================================================================\n# System Imports\n#==============================================================================\nimport os\nimport re\nimport subprocess as sub\nfrom collections import defaultdict\nimport itertools\nfrom io import open, close\nimport sys\nimport threading"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\ttry:\n\t\t\treturn shlex.split(args)\n\t\texcept Exception, e:\n\t\t\tlog.error(\"shlex module called %s: %s\", e, str(e))\n\t\t\treturn [args]\n\n\nclass _CmdSplitter(object):\n\tdef __init__(self, args):\n\t\t# add command to list\n\t\tself.args = args\n\n\tdef split(self):\n\t\t\"\"\"splits args into an old args list\"\"\"\n\t\tcmd = self.args\n\t\t# remove the first occurence\n\t\tif cmd[len(self.args):] == cmd:\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\nclass MultiprocessOptionParser (OptionParser):\n\t\n\t\"Multi-platform variant of optparse.OptionParser for commandline\"\n\t\n\tdef __init__ (self,*args,**kwargs):\n\t\t\"Initialize MultiProcessOptionParser.\"\n\t\tOptionParser.__init__(self,*args,**kwargs)\n\t\tfor key, val in kwargs.items():\n\t\t\tif key.startswith(\"cmd\"):\n\t\t\t\tself.add_option(key[6:],val)\n\n\nclass PlatformString (object):\n\t\"String for single-platform versions.\"\n\tdef __init__ (self,major,minor,patch):\n\t\tself.major="}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tif (PY2:\n\t\tfrom shlex import split as split\n\telse:\n\t\tfrom shlex import split\n\n\nclass _SplitType(Enum):\n\t\"\"\"\n\tTypes for splitting\n\t\"\"\"\n\tsplit = None \n\t\"\"\"\n\tSplitter used on command line.\n\t\n\tNot implemented\n\n\t\"\"\"\n\tdef split(self):\n\t\t\"\"\"\n\t\tSplit the command line (e.g..) into a list of elements\n\t\t\"\"\"\n\t\tif self.split == 0:\n\t\t\tself.split = 0\n\t\treturn self.split\n\n\tdef split_lines"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tdef __init__( self, argv, cwd=None ):\n\t\tif len( argv ) == 0:\n\t\t\treturn\n\n\t\tassert( len( argv ) == 2 )\n\t\tassert( 'args' in argv )\n\t\tif len( argv ) == 1:\n\t\t\tshlex.Split( argv[0] )\n\t\telif any( re.match( r'\\.py$', p ) for p in argv[1:] ) :\n\t\t\treturn\n\t\telse:\n\t\t\tassert( len( argv ) == 2 );\n\t\t\traise ValueError( 'no command-line arguments supplied for \"%s\"'"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress(archive_path):\n\treturn zipfile.ZipFile(archive_path)\t\n\n\ndef main(arg):\n\t\"Print out some preset parameters\"\n\tif arg == \"--version\":\n\t\tprint \"Version: %s\" % gdk_version\n\telif arg == \"--src\":\n\t\tsrc = arg\n\telif arg == \"--archive-path\":\n\t\tdest = arg\n\telse:\n\t\traise Exception, \"invalid arguments\"\n\n\tarchive_path_dst = os.path.join(dest, arg)\n\t\n\t# check if there's a directory"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef uncompress_archive(archive_path, destination):\n\t\"\"\"\n\t\tUncompress a file archive. Returns the file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\tif not os.path.exists(archive_path):\n\t\treturn archive_path\n\t\n\trepo_url = os.path.join(destination, os.path.basename(archive_path))\n\t\n\tif os.path.exists(repo_url):\n\t\tprint \"Archive is already uncompressed\"\n\t\treturn"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\tdef compress(self, repositoryurl, destlocation):\n\t\ttry:\n\t\t\tcmd = 'zcat --force -o %s %s' % (destlocation, repositoryurl)\n\t\t\tprint cmd\n\t\t\trc = subprocess.call(cmd)\n\t\t\treturn rc\n\t\texcept:\n\t\t\tprint 'Failed to compress repository %s' % (repositoryurl)\n\t\t\treturn None\n\t\n\n\tdef parse(self, repositoryurl):\n\t\trepo = self.decompress(repositoryurl)\n\t\tif repo is None:\n\t\t\treturn repositoryurl\n\t\trepourl = urllib.request.urlunparse"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\tif arch == u\"xz\":\n\t\t    cmd = \"xz {0} -d {1}\".format(archive_path, archive)\n\t\t    p = subprocess.Popen(cmd, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n\t\t    if p.wait() is None:\n\t\t\traise Exception(\"Unable to read archive {0}\".format(archive))\n\t\t    zx_url = p.stdout.readlines()\n\t\t    self.log_error(\"xz: {0}\".format(ip_list))\n\t    return self.url\n\n\tdef uncompress_archive_xz_files(self,"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef remove_temp(path):\n\t\"\"\"\n\tFunction to remove the tar.gz file with a certain suffix.\n\t\"\"\"\n\treturn os.remove(path)\n\t\t\nfrom __future__ import print_function, absolute_import\n\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom dateutil.rrule import rrule, RRuleSet, OWLRule\nfrom mock import patch\nfrom pyquery import PyQuery as pq\n\nfrom django.db import IntegrityError\nfrom django.utils import timezone\n\nfrom sentry.models import Project, Group, ProjectStatus, GroupStatus"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\tarchive_path = archive_path.replace('http://', 'https://')\n\t\n\t# Extract the file url path if it exists.\n\tif checkIfURLPathExist(archive_path):\n\t\tfilename = tempfile.mktemp()+\".orig\"\n\t\tfd, tmpFile = tempfile.mkstemp()\n\t\t\n\t\tos.fsync(fd)\n\t\tos.remove(filename)\n\t\t\n\t\tsubprocess.call([\"gzip\", archive_path])\n\t\t\n\t\twith open(tmpFile, 'w'):\n\t\t\traw_dir = downloadArchive(archive_path, tmpFile)\n\t\t\n\t\treturn raw_dir"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t\n\twith open(archive_path, 'rb') as archive_handle:\n\t\tdata_block = archive_handle.read()\n\t\tdata = 'from tar import %s\\n' % data_block\n\t\tdata = data.encode()\n\t\tsubprocess.check_call(['zip', '-d', archive_path], stderr=subprocess.STDOUT)\n\t\treturn url_for('tarball_build')\n\t\n\t\ndef tarball_clean(archive_path):\n\t\"\"\"\n\tDeletes a non-empty directory specified as archive_path. \n\t\n\tThis is a helper function that removes a previously"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef get_archive_url(archive):\n\t\"\"\"\n\t\tExtract a file, and extract it from the archive_path.\n\t\t\n\t\tThis assumes that all files in archive_path contain \"a\" character.\n\t\t\"\"\"\n\t\n\t\n\tarchive_url = archive_path_abspath(archive)\n\t\n\treturn url_to_archive_url(archive_url)\n\t\n\ndef delete_archive_url(url, delete_dir = True):\n\t\"\"\"\n\t\tDeletes a url.\n\t\t\n\t\tIf the directory path is the same as the archive path,\n\t\tit will be deleted.\n\t\"\"\""}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\tassert repo_path.isdir()\n\t\tlocal_repo_url = 'http://archive.local'\n\t\t\n\t\t# Get the file contents\n\t\tarchive_content = subprocess.run(\n\t\t\tself._get_file_url(archive_path),\n\t\t\tstdout=subprocess.PIPE).communicate()[0]\n\t\t\n\t\t# Ensure that the archive origin url is not on the path where we need it.\n\t\tlocal_repo_url = local_repo_url[len(local_repo_url)-1:]\n\t\n\t\t# Return the file url where we need to compress the local url to the new archive"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\ndef get_archive(repo_path, archive_path):\n\n\t\tcmd = [\n\t\t\t'cd',\n\t\t\t'{0} && uncompress 1 {1}'.format(repo_path, archive_path)\n\t\t]\n\t\t\n\t\tlog.debug(' '.join(cmd))\n\t\t\n\t\tp = subprocess.Popen(\n\t\t\tcmd,\n\t\t\tstderr=subprocess.STDOUT,\n\t\t\t stdout=subprocess.PIPE\n\t\t)\n\t\ttext = str(p.communicate()[0])\n\n\t\t#if text.find('404 Not Found')!= -1:\n\n\t\t#\twith open(archive_path"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n# TODO: Implement a file by just reading all files\ndef read_file(path, path_join, path_name=\"config.txt\", verbose=True):\t\n\t\t# TODO: What should I help?\n\t\tself._logger.info(\"Reading file: %s\", path)\n\t\twith self._open_file(path) as fp :\n\t\t\t\n\t\t\t# Build the data for a new file\n\t\t\tdata = fp.read()\n\t\t\tdata_dict = json.load(data, object_hook=OrderedDict)\n\t\t\tif (verbose):\n\t\t\t\tself._logger.debug(\"New"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\timport shlex\n\t\tcommand = shlex.split(cmd)\n\t\toutput = []\n\t\tfor file in command:\n\t\t\toutput.append(file)\n\t\tcommand = list(set(output))\n\t\tprint(\"file name list:\\n\"+str(command))\n\t\tfiles = []\n\t\tfor filename in command:\n\t\t\t#print(filename)\n\t\t\tf = open(filename,'r')\n\t\t\tlines = f.readlines()\n\t\t\tf.close()\n\t\t\t#print(lines)\n\t\t\twhile len(lines) > 0:\n\t\t\t\t#print(lines[0])\n\t\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\toutput = subprocess.check_output(\"git describe --tags --match all,plain\")\n\t\t#print output\n\t\tfilenames = output.split(\",\")\n\t\treturn filenames\n\t\t#return filenames.sort()\n\tdef extract_files_from_files(self, filenames):\n\t\t#print \"extracting files from \" + filenames\n\t\tfor filename in filenames:\n\t\t\tf = open(self.working_dir + \"/\" + filename, 'r')\n\t\t\tfile_contents = f.read()\n\t\t\tf.close()\n\t\t\tos.remove(self.working_dir + \"/\" + filename"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\tall_files = subprocess.check_output(('git', 'ls-files', '--no-dir', '--pretty=format:%h,%an <%ae>'))\n\t\treturn [f.strip() for f in all_files.decode('utf-8').strip().split('\\n') if ',' in f]\nclass CommandLine:\n\tdef name(self):\n\t\treturn \"tar xzf \"\n\n\tdef get_name(self, path, local_name, local_remote_version):\n\t\t# TODO: We should probably use a local file instead, which can generate too much"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef getDiffListFromFile(file):\n\t#Open the file and get all the diffs\n\tcontent = file[0]\n\tif content.lower() == \"diff\": \n\t\tf = open(\"Diff\",\"r\")\n\t\tlines = f.readlines()\n\t\tcurrent_diff = \"\"\n\t\tfor line in lines:\n\t\t\t#If the source is a comment, remove it\n\t\t\tif line.startswith(\"Comment: \"):\n\t\t\t\tcurrent_diff += line\n\t\tprint \"Diff: \", current_diff\n\t\tif (current_diff == \"\"):\n\t\t\tprint \"No change detected.\""}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\nclass git:\n\t\"\"\"\n\tA local git clone (git clone).\n\tThe implementation here is that this class creates a clean git directory and\n\tuses it to checkout the version and repo.\n\t\"\"\"\n\tdef __init__(self, repo_url):\n\t\tself.repo_url = repo_url\n\t\tself.user = ''\n\t\tself.password = base64.b64(os.urandom(16)).decode()\n\t\tself.repo = 'git://github.com/' + self.repo_url\n\t\t# git command will throw an exception if the command fails"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\nclass BlazeFuse(object):\n  \"\"\"\n    A class to help fast parsing the git output if the git configuration file has changed or if the configuration file has changed in the Git repository.\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    if \"--force\" in kwargs.keys():\n      force = kwargs[\"force\"]\n    else:\n      force = True\n  \n  def list_files(self):\n    \"\"\"\n      List all the files it has been changed to.\n    \"\"\"\n    cmd = (\"git --no-pager | awk '{print $4}' > {}\").format(GIT_CHANGE"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\tcommand = ['git', 'ls-files', '-s']\n\t\tcommand.append(source)\n\t\treturn subprocess.check_output(command).split(\"\\n\")\n\ndef get_all_file_filenames(directory = None):\n\toutput = get_dir_of_files_on_directory_using_arguments(directory)\n\treturn_csv = []\n\tfor file in output:\n\t\tif not file:\n\t\t\toutput.append(file[0])\n\t\t\treturn return_csv\n\t\toutput_file = file[0].split('_')[0]\n\t\treturn"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\nif __name__ == '__main__':\n\tdef parse_options():\n\t\timport csv, sys\n\t\tparser = argparse.ArgumentParser(description=script_description, epilog=script_epilog)\n\t\tparser.add_argument('--git-dir', type=str, default=os.path.join(\"..\", \"git_dir\"), help=\"local path to git directory\")\n\t\tparser.add_argument('--ignore', type=str, default='CVS |'+''.join(os.listdir(\"./\"))),\n                        help=\"list of ignored files, e.g \" + \"CVS"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef GetFilenames(filenameSet,ignoreFile):\n\t\"\"\"\n\tRead the list of filenames from the file name file set and return them as a list.\n\t\"\"\"\n\t\n\t\n\t\n\tfilenames=getFilenames(filenameSet,ignoreFile)\n\n\tfor f in filenames:\n\t\tprint \"\\t\", f, \"\\n\"\n\ndef ReadFilenames(FILENAME_FILE,ignoreFile =  False,excludeFiles = []):\n\t\"\"\"\n\tReturns a list of files from the supplied file name. Excludes files defined by a list of files in\n\tignoreFile.\n\t\"\"\"\n\t\t\n\t\n\t\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\treturn False\n\n\tdef has_object_method(self):\n\t\t\"\"\"\n\t\tCheck whether the class that contains this object has the\n\t\tobject type.\n\t\t\n\t\tReturns:\n\t    True if the object type matches\n\t\t\"\"\"\n\t\treturn True\n\n\tdef type_string_match(self):\n\t\t\"\"\"\n\t\tCheck whether the string has an object type and that it uses\n\t\tthe given string (string), including the object identifier\n\t\t\n\t\tReturns:\n\t    True if matching\n\t\t\"\"\"\n\t\treturn True\n\n\tdef has_function_method(self):\n\t\t\"\"\"\n\t\tCheck whether a function has"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\ttype_id = len(os.path.splitext(os.path.basename(filename)))\n\t\tif type_id > 2:\n\t\t\tlogging.warn(\"The module 'type_%s' is a type with less than 2 file name\", filename)\n\t\treturn False\n\t\n\tdef has_compatible(self, path):\n\t\t\"\"\"\n\t\tCheck if the supplied path matches the current file type version.\n\t\t\n\t\tArgs:\n\t    path: Path to match\n\t\t\"\"\"\t\n\t\tif re.search(\"\\\\*.type$\", path):\n\t\t\treturn True\n\t\treturn False\n\t\n\tdef get_supported_"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\nif os.path.extsep is None:\n\t# This module does not provide a filename separator, the user will not need \n\t# this. Please do not use this method.\n\tfpaths_in_filename = tuple(map(os.path.basename, glob.glob('._*.py')))\n\tfpaths_out_filename = os.path.join(os.getcwd(), 'test.zip')\n\n\ndef isfile(filename):\n\tfor ext in EXTENSIONS:\n\t\tif filename.endswith(ext):\n\t\t\treturn True\n\treturn False\n\t\n\t\ndef"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\tfname, fname = os.path.split(os.path.realpath(__file__))\n\t\tif fname.endswith('.py'):\n\t\t\treturn False\n\t\n\t# Return False if the given filename is not a type\n\treturn True\n\ndef check(filename, filename2=None, ignore=None, ignore2=None, verbose=False):\n\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\t    filename2: File type for which this function matches\n \tReturns:\n\t    True if the filename is a type and is a file,"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n    def _test_support_type(self, filename):\n\t# check if a match is supported from the filename type\n\tif isinstance(filename,type):\n                try:\n\t\treturn filename.lower() == type(self).__name__\n\t\t\t\texcept:\n\t\t\treturn True\n\t\t\n\treturn False\n\t\t\n\n\t\t\t\ndef validate_python(file, expected):\n\t# test file\n\tpython_files = os.listdir(file) \n\texpected_files =  [f for f in expected if os.path.splitext(f)[1][-3:] == '.' ]\n\tif test_support"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\n\tfor ext in os.path.splitext(filename):\n\t\tif ext.lower() in TYPES:\n\t\t\tif os.path.splitext(filename)[1] == \".py\":\t\t\t\t\t\n\t\t\t\treturn True\n\t\t\t\t\n\t\telse:\n\t\t\treturn False\n\n\ndef set_filename(filename):\n\n\t\"\"\"\n\t\tSet the filename, if it exists\n\t\"\"\"\n\t\n\t\n\tif not os.path.isfile(filename):\n\t\n\t\ttry:\n\t\t\tos.rename(filename, filename + \".py\")\n\t\texcept OSError, e:\n\t\t\tprint e\n\t\t\t\n\t\t\t\n\t\nclass FileType(object"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\tmodule_data = os.path.splitext(filename)\n\t\tif module_data[0] in [\".\",\"..\"]:\n\t\t\treturn True\n\t\telif module_data is not None and module_data[0] is None:\n\t\t\treturn True\n\t\telse:\n\t\t    #if filename contains a \"file\" extension (ex: foo.cpp) that\n\t\t\t# matches _.*_pymodule*\n\t\tif module_data is not None \\\n\t\t\tand filename[-3:]=='_pymodule':\n\t\t\treturn True\n\t\treturn False\n\n\n\t#\n\tdef _load_compiled_"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t    \n\t# If not filename is a string we return False here\n\tif filename!= \"test.xml\":\n\t\t\treturn False\n\t\t\n\t# Search for any.xml file extensions\n\tfilename_exts = [x.split(\".\")[-1] for x in filename.split(\".\")]\n\tfor dir_extension in filename_exts:\n\t\t\n\t\tif len(dir_extension) == 0:\n\t\t\treturn False\n\t\t\n\t\t# Check to see whether the file is a file extension\n\t\tif  re.search(\".xml\",dir_extension):\n\t\t\treturn True\n\t\t\n\t\t# Is the extension a _html extension"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n    class BuiltinObject(object):\n\t\tpass\n\t\t\nclass Glob(BuiltinObject):\n        if sys.version_info >= (3, 0):\n                    # Python 3.x doesn't have a way to parse a module with the name\n                    # that matches the type name. It returns a list for all\n                    # matching types, but for each one type only a string\n                    # (e.g. \"foobar\") is required. This means that the type name\n                    # may get printed out but must be included in the output.\n                    # e.g. brython\n                    def __repr__(this):\n\t\treturn"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\ndef supports_type(filename):\n\t\"\"\"\n\t\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    True if the file is supported, False otherwise\n\tRaises:\n\t    NameError, TypeError, ValueError\n\t\"\"\"\n\t\ndef is_valid_filename_type(filename):\n\t\"\"\"\n\t\tChecks if the filename is a type that this module supports.\n\t\t\n\t\tArgs:\n\t    filename: File to match\n\t\tReturns:\n\t    True if the filename is a valid file type (not a type)\n\t\tRaises:\n\t    NameError, TypeError"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\tfrom lib import time, string_to_durationstr\n\t\n\ttime_str, unit = string_to_durationstr(time)\n\tdt = None\n\tif unit == 'weeks':\n\t\tdt = time_str\n\telif unit == 'days':\n\t\tdt = time_str.replace(day=1)\n\telif unit =='seconds':\n\t\tdt = time_str\n\t\n\tif dt == None or dt.hour <= 0 or dt.minute <= 0 or dt.second <= 0:\n\t\traise ValueError(\"A time is given by time string '%s'\" %"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\t# We expect a float as the \"always\" parameter, but we want to pass it to\n\t# datetime.timedelta so we are not only parsing one timestamp per second.\n\t# For any reason, we don't allow a timedelta to be parsed, since\n\t# datetime.timedelta() doesn't accept \"never\" in the meantime\n\t# value being converted to minutes\n\tdt=datetime.timedelta()\n\t\n\tif dt.minute > 0:\n\t\tdt=dt+dt.minute\n\t\n\tif dt.hour > 23:\n\t\tdt=dt+dt.hour+3600\n\t\n"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\n\tdef __init__(self, frequency):\n\t\tif frequency == \"\":\n\t\t\tself.seconds = None\n\t\telse:\n\t\t\tself.minutes = _parse_frequency(frequency)\n\t\t\tself.seconds = _parse_frequency(frequency, 1)\n\t\t\tif is_number(self.seconds) or is_float(self.seconds): \n\t\t\t\tself.minutes, self.seconds = _parse_frequency(self.seconds)\n\n\tdef __str__(self):\n\t\treturn \"datetime.timedelta(%s) %s\" % (self.seconds, self.minutes)\n\n\tdef"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\tif len(frequency) == 0:\n\t\traise ValueError(\"frequency is None.\")\n\n\tif isinstance(frequency, str) and len(frequency) > 0:\n\t\treturn datetime.timedelta(days=int(frequency) * 7)\n\t\n\traise TypeError(\"frequency must be a string or a time\")\n\ndef parse_time(text):\n\t\"\"\"\n\tTurn a time in three formats with a frequency and return a\n\tdatetime.timedelta instance.\n\t\n\t>>> parse_time(\"15:30\")\n\tdatetime.timedelta(minutes =15, seconds = 30)\n\t\n\t\"\"\"\n"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\n\n# @brief This function contains the list of intervals used before a conversion. This is an example of how to create a timedelta.\ndef get_intervals_by_frequency(freq, time):\n\t\"Get a list of intervals for a frequency (string) with a number and a unit of time (string)\"\n\t\n\tstart = datetime.datetime.fromordinal(time - 1)\n\treturn [interval for interval in TimeRange.intervals(start, end=len(interval) * 1.5, freq=freq, unit=TimeUnit.MILLORS) if not interval.is_valid"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\tif frequency is None or (\"never\", frequency) == ('never', None):\n\t\t\tfor unit_of_time, dt in TIME_UNITS_OF_TIME[1:]:\n\t\t\t\tif not dt <= datetime.datetime.now():\n\t\t\t\t\traise ValueError(\"'%s' must be later than %s\" % (frequency, dt))\n\t\t\t\treturn dt\n\telif len(frequency) == 1:\n\t\treturn datetime.timedelta(days=frequency[0])\n\telse:\n\t\traise ValueError(\"Frequency cannot be parsed\")\n\ndef set_and_cache_time(number, unit=\"\"):\n\t"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\tfrequency = datetime.timedelta()\n\t\ttry:\n\t\t\tparsed = str(frequency).replace('.', '').split(' ')\n\t\t\tif parsed[0] == \"-\":\n\t\t\t\tfrequency = parsed[1]\n\t\t\telse:\n\t\t\t\tif len(parsed) < 2:\n\t\t\t\t\traise ValueError\n\t\t\t#print parsed\n\t\texcept IndexError:\n\t\t\traise ValueError\n\t\t\n\t# Return the timedelta instance.\n\treturn None if frequency else datetime.timedelta(weeks=frequency)import os\nfrom functools import reduce\n\nimport pandas as pd\nimport pytest\n\nimport pyspn\nimport pyspn.base as bfn"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\t#If we're using the full-form time format, return None\n\t\t#If we don't have a full-form time format yet, return None\n\t\tif not isinstance(frequency, basestring):\n\t\t\traise ValueError, \"Frequency must be a string\"\n\t\t\n\t\t\n\t\tif freq.find('days') >= 0:\n\t\t\treturn datetime.timedelta(int(freq[freq.find('days')+1:freq.find('days')]))\n\t\t\n\t\telif freq.find('hours') >= 0:\n\t\t\treturn datetime.timedelta(int(freq[freq.find('hours"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t\n\tres = None\n\t\n\toffset = timedelta(days=2*time.day-1)\n\tif isinstance(freq, str):\n\t\tres = datetime.time(freq)\n\telse:\n\t\tif time.is_string(freq):\n\t\t\tres = datetime.timedelta(seconds = int(time))\n\t\t\tif isinstance(res, datetime.timedelta):\n\t\t\t\tif res.seconds >= 1:\n\t\t\t\t\traise ValueError(\"'time' is out of sync with'secs' parameter\")\n\t\telse:\n\t\t\tres = 0\n\t\t\n\tif res > 0:\n\t\traise ValueError(\"Given"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\tfrequency = None\n\t\tif frequency == \"always\":\n\t\t\tfrequency = datetime.timedelta(weeks=3)\n\t\telif frequency == \"never\":\n\t\t\tfrequency = datetime.timedelta()\n\t\telif frequency == \"daily\":\n\t\t\tfrequency = datetime.timedelta(days = 1)\n\t\telse:\n\t\t\traise ValueError(\"Bad frequency string %r.\" % frequency)\n\t\treturn frequency\n\n\n@functools.total_ordering\nclass DateHour(object):\n\t\"\"\"\n\tA date that represents the hour in minutes. An integer value represents the\n    hour. For example, '1st', will be"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n# Function to determine if the host is the localhost\ndef checkIPHost(host):\n\ttry:\n\t\tif socket.gethostbyname(host) == 0:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception:\n\t\treturn False\n\n# Function to check the host is the localhost of the server\ndef checkHostname(host):\n\ttry:\n\t\tif socket.gethostbyaddr(host) == 0:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception:\n\t\treturn False\n\n# Function to check if the host is the"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef check_host(host, port):\n\twith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as socket_socket:\n\tsock = socket_socket.bind((host, port))\n\tsock.setblocking(0)\n\treturn True\n\n\"\"\"\n\tInitializes the SNIP address and port.\n\t\"\"\"\n\ndef initialize_snip(ip, port):\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ts.connect((ip, port))\n\treturn True\n\n\n\n\n\"\"\"\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\thostIP = socket.gethostbyname(host)\n\t\t\n\t# Check if the host is the localhost...\n\tif hostIP in [hostIP]:\n\t\tif(checkIfLocalHostIsSatisfiedIP():\n\t\t\treturn True\n\telse:\n\t\treturn False\n                \n\t\n\t# We got here...\n\treturn True\n\ndef checkIfLocalHostIsSatisfiedIP():\n       \n\tfor x in range(4,10):\n\t\t\n\t\ttry:\n\t\t\t# Get the address of the localhost of the application\n\t\t\tserver,addr = socket.getaddrinfo((HOST, 0),"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef check_localhost(host):\n    \"\"\" This is the generic check for the localhost host, but the local host is in the inventory\"\"\"\n    return True\n\ndef check_testnet(address, username, password):\n    \"\"\" This is the base test for the tests in the inventory module on localhost\n\t:param address: the remote host\n\t:param username: the username/password\n\t:param password: the password\n\t\"\"\"\n    ip = socket.gethostbyname(address)\n    try:\n        testnet = socket.gethostbyaddr(ip)[0]\n    except:\n        syslog_error(\"Hostname"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ttry:\n\t\t\tsocket.gethostbyname(host)\n\t\texcept socket.gaierror as e:\n\t\t\tprint \"Host\", repr(host), \" is not available\\n\", e.args\n\t\t\treturn False\n\texcept socket.herror as e:\n\t\tprint \"Host\", repr(host), \" is not up and running\\n\", e.args\n\t\treturn False\n\ndef valid_host(hostname):\n\t\"\"\"\n\t\tChecks if the host is a localhost\n\t\n\t\t:param hostname: Check of the host\n\t\t:return: True if a given host is a localhost\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef getSocket():\n        \"\"\"\n\tReads the socket from stdin\n\t\n\t:returns: The current socket\n\t\n\t\"\"\"\n\ttry:\n\t\t\treturn socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\t\t\t \n\t\texcept IOError, e:\n\t\t\tif e.errno in (errno.EADDRINUSE, errno.EAFNOSUPPORT):\n\t\t\t\treturn socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\n\t\texcept OSError, e:\n\t\t\tif e.errno not in (errno.EADDRINUSE, errno.EAFNOSUPPORT):\n\t\t\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t\n\tcheck_host(host)\n\treturn True\n\t\ndef check_host(host):\n\t\"\"\"Check if the host is the localhost and the port for the given host\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost and the port for the given host\n\t\"\"\"\n\t\t\n\t\n\tport = None\n\thost = host.split(\":\")[0].strip()\n\t\t\n\tsocket.setdefaulttimeout(2)\n\ttry:\n\t\tbind = (host, 4500)\n\t\tport = (host.split(\":\")[1])[0]\n\t\t\n\t\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n#\t\ttry:\t\t\t\t\n#\t\t\tretVal = socket.gethostbyname_ex(host)\t\n#\t\t\treturn retVal\n#\t\texcept:\n\t\t#\t\treturn False\n\t\nclass DS_TailCheck(object):\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor for the TailCheck class\n\t\t\n\t\t:param host: The host IP and Port (if passed, they will be checked \n\t\t\tonce if not)\t\t\t\n\t\t\"\"\"\n\t\tself.host = None\n\t\tself.port = None\n\t\t\n\t\t#Check if the socket is valid"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t# Open connection\n\taddress_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\taddress_sock.bind(('', 0))\n\taddress_sock.close()\n\t\n\ttry:\n\t\t# Read all lines and figure out whether the host is localhost\n\t\tfor line in address_sock.recvfrom(1024):\n\t\t\tif not line: break\n\t\t\tline = line.strip()\n\t\t\tif len(line) < 10: continue\n\t\t\tif line.startswith(\"127.0.0.1\"):\n\t\t\t\treturn False\n\t\n\texcept socket."}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n# The port that the daemon socket connects to\ndef sge_hostname (sock_address) :\n\t\"\"\"\n\tChecks if the host is localhost\n\t\n\t:param sock_address: The IP address is the ip address to connect to.\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n# Get the hostname of the server that's running the daemon\ndef sge_exec (\"hostname\") :\n\t\"\"\"\n\tReturns a list of all the available ports currently connected \n\t\n\t:param sock_address: The IP address is the ip address to connect to.\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\nclass PatternMatcher(object):\n\tdef __init__(self, match_matcher):\n\t\tself.match_matcher = match_matcher\t\n\n\tdef match(self, path_fragment):\n\t\treturn self.match_matcher.match(path_fragment)\n\n\tdef find(self, path_fragment):\n\t\tif self.match_matcher.find(path_fragment):\n\t\t\treturn self.match_matcher.fglob(path_fragment).groups()\n\n\ndef _remove_prefix(path_fragment):\n\t\"\"\"Removes the :prefix and :postfix from the given path fragment"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\tif isinstance(paths, str):\n\t\t\tpaths = paths.split()\n\t\tpatterns = paths # flatten\n\telse:\n\t\tpatterns = patterns.replace('*', r'\\*')\n\tfor pattern in patterns:\n\t\tif len(pattern) > 3:\n\t\t\tre_path_pat = re.compile(pattern)\n\t\t\tfor pattern_segment in patterns[1:]:\n\t\t\t\tre_path_pat = re_path_pat.sub('', pattern_segment).strip()\n\t\t\tif re_path_pat is None:\n\t\t\t\tcontinue\n\t\t\tpatterns_re = re_path_pat."}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\tif isinstance(path, basestring):\n\t\t\tfragments = list(util.find(path, pat))\n\t\telse:\n\t\t\tfragments = list(filter(None, util.iter_path(path)))\n\n\t\t# NOTE: don't use the original regex, need a more explicit regex\n\t\tmatch = None\n\t\tfor i in range(len(fragments)):\n\t\t\tif fragments[i] is not None:\n\t\t\t\tmatch = re.search(fragments[i], fragments[i+1])\n\t\t\t\tif not match:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\treturn list(flatten"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t    def __init__(self):\n\t\tself.find = []\n\t\n\tdef get_paths(self,path):\n\t\tp = subprocess.Popen([\"find\",path],shell=True)\n\t\t#p = subprocess.Popen(os.environ['PATH']+\"/root/script.py\")\n\t\t#p = subprocess.Popen([\"find\",path,'',p.pid.pidf],shell=True)\n\t\tp = subprocess.Popen([\"find\",path,'*',p.pid.pidf],shell=True)\n\t\t#return p.communicate()[1][:-1].split()\n\t\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\t# get all the paths\n\n\tpath_fragments = list(os.path.abspath(__file__.rsplit(os.sep, 2)))\n\tpath_patterns, glob_patterns = set(), set()\n\n\t# filter the paths\n\n\tfor path_fragment in path_fragments:\n\t\tpath_pattern = []\n\t\tif not path_fragment:\n\t\t\tcontinue\n\t\t\n\t\tfor pattern in pattern_set:\n\t\t\tpattern_pattern.append(pattern)\n\t\t\tglob_pattern = re.compile(pattern)\n\t\t\tpath_pattern.append(glob_pattern)\n\t\tpath_patterns"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\nclass PathPatterns(object):\n\t\"\"\"\n\t\tPath patterns can be used for arbitrary wildcard syntax in the regex pattern syntax. A wildcard pattern can also be used to match files with optional glob wildcards.\n\t\tA path pattern can be passed to `--find`.\n        \n        The behavior is controlled by:\n\t\t\n            - `--find` uses `fnmatch`_ module to look for files in paths and their names in pattern.\n\t\t\t\n            - `--any` uses `fnmatch`_ module to match the files (using wildcards).\n\t\t\n\t\tThe behavior is controlled by:\n\t\t\t- `--all` uses `fnmatch` module to only"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\tpatterns = re.compile('\\$').findall(paths[0])\n\t\treturn patterns\n\n\ndef find_paths(*seq, recurse=True, *path):\n\t\"\"\"\n\tGiven `seq`, turn each path in the sequence.\n\tIf `recurse=True`, then search recursively through path(subpath).\n\t\n\tReturns a list of paths matching the patterns found (glob). If recurse\n\tis False, then yield from recursive call.\n\t\"\"\"\n\tpattern = re.compile('\\$').findall(seq)\n\tfor path in path:\n\t\tyield pattern\n\t\tif recurse is"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\treturn list(chain(re.split,self.paths) if self.paths else list(chain.from_iterable(self.paths)))\n\tdef glob_path(self, pattern):\n\t\t\"\"\"\n\t\tGiven a path fragment, return the first glob, not just glob wildcard.\n\t\t\"\"\"\n\t\tpattern = re.sub(r'^%s'%(re.escape(pattern)), '', pattern, flags=re.I)\n\t\treturn pattern\n\tdef glob(self, pattern, *patterns):\n\t\t\"\"\"\n\t\tGiven a pattern to test, return all matching paths in `patterns`."}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, matches=None, patterns=None):\n\t\tself.matches = []\n\t\tself.patterns = []\n\t\tself.patterns_ = patterns if patterns is not None else []\n\t\tself.path_count = 0\n\t\n\tdef match(self, path):\n\t\tfor p in path:\n\t\t\tself.matches.append(re.compile(p))\n\t\t\tself.match_path.add(p)\n\t\tself.path_count += 1\n\t\n\tdef _find(self):\n\t\t\"\"\"Finds self matching Path for each pattern\"\"\"\n\t\tfor p"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\tif not isinstance(fragments, list):\n\t\t\tfragments = [fragments]\n\t\telse:\n\t\t\ttry:\n\t\t\t\tfragment_re = re.compile(fragments[0])\n\t\t\texcept ValueError, e:\n\t\t\t\traise Exception('Invalid path fragment pattern: %s' % str(fragments[0]))\n\t\t\tfor fragment in fragments[1:]:\n\t\t\t\tfragments.remove(fragment_re.sub('', fragment))\n\n\n\tdef do_match(self, pattern, flags = 0, match_patterns = None):\n\t\"\"\" Find any path where the given pattern matches the given pattern and return all matching paths"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n#get version\ndef rungit():\n  path = os.getcwd()\n  git_version = subprocess.Popen(['git', '--version'] + sys.argv[2:], stdout=subprocess.PIPE).communicate()[0]\n  path = os.path.realpath(os.path.join(path, '..', '..')) \n  \n  if 'linux' in sys.platform:  # linux\n    path = os.path.realpath(os.path.join(path, '..', 'linux'))\n    \n  if os.path.islink(path) or not os.path.exists(path):\n    if 'linux' in"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t    \n\tif \"python\" in sys.argv:\n\t\t#os.exewalk(sys.argv[2])\n\t\t#os.execl(sys.argv[2])\n\t\treturn True\n\telse:\n\t\t#os.path.basename(sys.argv[0]) #to_str()\n\t\treturn False\n\n\ndef to_str(f_name):\n\treturn f_name + '.' + platform.python_implementation()\n\ndef to_int(f_name):\n\treturn f_name + '.' + platform.python_implementation() + 1\n\n\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef GitCommand(command):\n\t\"\"\"\n\texecute using git bash\n\t\n\t:param command: path to git command to execute\n\t\"\"\"\n\treturn subprocess.Popen(\"git bash %s/bin/git\" % command, shell=True) == None\n\n\"\"\"\n\nSolve the euclidean distance using GIS\n(https://en.wikipedia.org/wiki/Solve_the_euclidean_distance).\n\nIn order to implement your algorithm we will first pass the input data to the solver\n\n:returns: tuple\n\"\"\"\n\nimport re\nimport numpy as np\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef write_log(f,msg_type):\n\t'''\n\twrite the log to a file\n\t'''\n\tf.write(msg_type+'\\n')\n\ndef get_cmd_line_list(options):\n\ttry:\n\t\tcmd_line_list_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../data/GitExec_GitCmd_options.txt')\n\t\tcmd_line_list = []\n\t\twith open(cmd_line_list_file) as f:\n\t\t\tfor line in"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n#TODO - research when using regex for version in git, not using regex for current version...\n\tdef get_versions(path):\n\t\ttry:\n\t\t\tcommand = 'git'\n\t\t\tcommand = \"%s %s | grep %s > %s\" %(command, os.path.abspath(path), path, path)\n\t\t\tout = utils.run_cmd(command, capture_output=True, print_stdout=True)\n\t\texcept (IOError, os.error), why:\n\t\t\tprint \"could not get versions...\"\n\t\t\tif why.errno == 1:\n\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\nclass WindowsGitBash(object):\n\n\tdef __init__(self, win_client):\n\t\t\"\"\"\n\t\tthe parameters\n\t\t:param win_client: a Unix gitbash shell\n\t\t\n\t\t\"\"\"\n\n\t\tself.command = \"gitbash\"\n\t\tself.args = ''\n\t\tself.win_client = win_client\n\n\n\t\n\t\n\n\tdef run_command(self):\n\t\t\"\"\"\n\t\tRuns the command you use\n\n\t\t:return: True if a pipe is added, else False\n\t\t:rtype: bool\n\t\t\"\"\"\n\n\t\t# Open the pipe\n\t\tpipe = os."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n    \tfrom subprocess import Popen\nelse:\n\t\timport subprocess\n\t\t\n\texe = ['git', '-c', \"git.exe\", \"-c\", r'git.exe $< ',`cwd\"+\"/\"+os.path.split(os.path.realpath(sys.argv[0]))[1:], \n\t\t\t'-v','-c', r'git.bat']\n\t\n\t\ndef git():\n\t\t\t\"\"\"\n\t\t\treturns the script to run git\n\t\n\t\t\t:return: tuple of process and exit code\n\t\t\t\"\"\" \n\t\t\texe = ['git', '-c', r'git.exe', '-c"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\ndef get_sub_dir():\n\t\"\"\"\n\tpath: returns this directory\n\t:return: path of this sub directory\n\t\"\"\"\t\t\n\tif sys.platform == 'linux':\n\t\tpath = os.path.abspath('~/')\n\telif sys.platform == 'darwin':\n\t\tpath = os.path.abspath('~/Library/Preferences/Local/share/gitbash')\n\telse:\n\t\traise RuntimeError('unknown platform')\n\n\treturn path\n\ndef get_gitbash():\n\t\"\"\"\n\treturn the current gitbash working directory\n\tcall the following command at"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\nclass gitbash():\n\t\"\"\"\t\tmain() returns False on success, exit on failure\n\t\"\"\"\n\t\t\n\t\ndef main():\n\ttry: \n\t\tif os.access('C:\\\\Windows\\\\git.exe', os.R_OK):\n\t\t\tgitbash()\n\t\telse: \n\t\t\tprint 'Please create C:\\\\Windows\\\\git.exe first.'\n\texcept:\n\t\tprint 'Error: ', sys.exc_info()[0]\n\t\tsys.exit()\n\t\t\n    else: \n\t\tgitbash()\n\treturn true\n\nif __name__ == '__main__': main()\n\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t\t\n\t\nimport os.path\n\ndef get_file(path, default=False) :\n\t\n\treturn os.path.join(os.path.dirname(__file__), path)\n\ndef open_shell_command(command, default=False):\n\t\n\tp = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid)\n\tstdout,stderr = p.communicate()\t\n\n\tif p.returncode == 0:\n\t\tif default == True :\n\t\t\treturn stdout.strip(os.linesep."}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\nconf_file = open('config.yml','w')\nconf_file.write('# Generated by autmake.py - do not edit\\n\\n')\n\t\nconf_file.write('---\\n')\nconf_file.write('\\n')\nconf_file.write('target.yml: autogenerated\\n')\nconf_file.write('---\\n')\nconf_file.write(['\\n'])\nfor i in template_vars:\n\tconf_file.write(i + ': ')\nconf_file.write('\\n')\n\n\nconf_file.close()\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\t\n\tconfig = yaml.load(open(config_filename,'r'))\n\t\n\t# TODO: make sure that your config file is valid.\n\tif not config or not any('target_dir' in item for item in config.keys()):\n\t\treturn\n\t\n\ttarget_dir = config['target_dir']\n\n\tdest = 'config_generated_%s' % config['target']\n\tif not os.path.isdir(dest):\n\t\tos.makedirs(dest)\n\t\t\t\t\n\t# Copy all existing files into the target directory.\n\tdest = os.path.join(target_"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\n\tf = open(targetConfigFilename,'r')\n\tconfigs = yaml.load(f)\n\tif configs.keys():\n\t\tfor config in configs.keys():\n\t\t\tf.write(str(config)+ '\\n')\t\n\t\t\n\nclass CvFile:\n    def __init__(self, config_dict_file):\n        self.configs = {}\n        self.config_dict = {}\n        for config_line in config_dict_file:\n            # parse and split the line into fields\n            (key, value) = config_line.split(\": \", 1)\n            #print('"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef outputToYAML(target_config_file):\n\tyaml.dump(getConfig(), file=target_config_file)\n\ndef inputToYAML(target_config_file):\n\tyaml.load(target_config_file)\n\t\n\ndef main():\n\tdef createTarget(target_config_file):\n\t\ttarget = createConfig(target_config_file)\n\t\t\t\n\t\tif not target[0]:\n\t\t\treturn\t\t\n\n\t\ttarget_name = target[0]\n\t\ttarget_file = open(target_config_file).read()\n\t\ttarget_file"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef writeFile():\n\t\"\"\"\n\tWrite target config YAML to file.\n\t\"\"\"\n\t\n\t\ndef errorHandler(error):\n\t\"\"\"\n\tProcess a simple error handler and return an error code, the corresponding message.\n\t\"\"\"\n\t\ndef writeFailure(*args, **kwargs):\n\t\"\"\"\n\tWrite a failure to the specified file.\n\t\"\"\"\n\t\ndef getTargetConfigFileName(targetConfigDir, configFilename):\t\n\t\"\"\"\n\tReturn target config YAML file path + targetConfigDir + configFilename (if it exists).\n\t\"\"\"\n\t\ndef loadTargetConfigFile"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef readConfig(configfilename, overwrite=False):\n\t\"\"\"\n\tGiven a target config filename and rendered config YAML, return its content. \n\tThis is a static routine to be called from the core module of this module's\n\tmodule's _read().\n\t\"\"\"\n\t\n\ttry:\n\t\twith open(configfilename, \"rt\") as f:\n\t\t\tconfig = yaml.safe_load(f)\n\t\t\t\n\texcept Exception as e:\n\t\tprint(e)\n\t\tsys.stderr.write(\"Unable to load config YAML, aborting..\\n\")\n\t\tabort(1"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n#-----------------------------------------------------------------------\n# Setup\n#-----------------------------------------------------------------------\nROOT = os.getcwd()\nCONFIG = os.path.join(ROOT, \"build\", \"config.yml\")\n\n# Create some path based on the target path\nTARGETS = [x for x in os.listdir(\"build\") if x!= '']\nfor path in TARGETS:\n\tif not os.path.isdir(path):\n\t\tos.makedirs(path)\n\n# Save configs to file\nwith open(CONFIG, encoding=\"utf-8\") as f:\n\tconfig_data = json.load(f)\n\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef make_dirs(config):\n\t\"\"\"Make all dirs in a specified target config yaml, and return the path to that directory\"\"\"\n\tfor config in config.config.all('dirs'):\n\t\tconfig_path = os.path.join(config.dir, config.name)\n\t\tif not os.path.isdir(config_path):\n\t\t\tif not os.path.exists(config_path):\n\t\t\t\tmake_dirs(config)\n\t\t\treturn config_path\n\ndef load_config(filename, config_list):\n\tconfig = config_list['config']\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t  \n\tfilename = target.config_filename\n\tpath = os.path.dirname(filename)\n\tos.makedirs(path)\n\tdata = {}\n\tf = open(filename, \"w\")\n\tdata['config'] = template.render(target.config_configuration_data)\n\tf.close()\n\t\n\tfor parent, dirs, files in os.walk(path):\n\t\tfor f in files:\n\t\tos.unlink(os.path.join(parent, f))\n\t\tfor filename in files:\n\t\t\tpath = os.path.join(parent,"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\ndef write_target():\n\t\"\"\"\n\tWrite an individual source file into the current directory. Then write out all the files.\n\t\"\"\"\n\t\ndef validate(tree):\n\t\"\"\"\n\tFor every root of the tree, validate the path that is going to be.git/config/file.xml\n\tThen try to find it relative to root directory.\n\t\"\"\"\n\t\ndef process(targetPath, templateFilePath):\n\t#TODO: make this work as a utility function\n\tdirPath = None\n\t\n\tif not os.path.exists(targetPath):\n\t\tprint \"Parent"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\t\n\tdef __init__(self, script):\n\t\tself.script_output = script\n\t# Copyright 2009-2015 Rumma & Ko Ltd\n# License: BSD (see file COPYING for details)\n\nimport os\nimport random, string, sys\nfrom cStringIO import StringIO\nfrom ConfigParser import SafeConfigParser\n\nfrom lino_xl.lib.addresses import _email_split\nfrom lino_xl.lib.base import Base, _\nfrom lino_xl.lib.config import get_config\nfrom lino_xl.lib.event import Event, NewId\nfrom"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef create_ssh_list(script):\n\t\"\"\"\n\tcreates list of ssh commands.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef main():\n    \"\"\"\n\tmain():\n\t\"\"\"\n\t__settings__['SSHHOME'] = \"/root/.ssh/\"\n\n\t__settings__['SSHHOST'] = '127.0.0.1'\n\t\n\t__settings__['SSHPORT'] = 22\n\tglobal __settings__['SSHLOGIN']\n\tglobal __settings__['SSHPASSWD']\n\tglobal __settings__['SSHTYPE']\n\n\t__settings__['USERCOMMANDSTRING']"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n# run a command.  This uses a subprocess to run a single SSH session\n# that can have an extra line to read\n#\n\ndef run_ssh_command(script, timeout=60, ignore_errors=False, stdin='/dev/null'):\n\t# ssh_cmd does not like this.\n\tif ignore_errors:\n\t\terror_msg = \"ignoring: error in script\"\n\t\traise RuntimeError, error_msg, sys.stderr.read()\n\n\treturn subprocess.Popen(shlex.split(script),\n\t\t\t\tstdin=stdin,\n\t\t\t\tstdout=sys."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\nclass ScriptConvert(object):\n\t\"\"\"\n\tconverts a script to one line command.\n\t\"\"\"\n\tdef __init__(self, script, script_type=None):\n\t\t\"\"\"\n\t\t:param script_type: a type\n\t\t:return:\n\t\t\"\"\"\n\t\n\tdef convert(self, output_filename, command):\n\t\t\"\"\"\n\t\tConvertes the given script to one line command.\n\t\t\"\"\"\n\t\tf = open(os.path.join(output_filename, script_type), 'r')\n\t\tscript_type = self.script.get_text()."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\nclass CmdCommand:\n\t\"\"\"\n\tabstract base for cmd command.\n\tThe only command should be implemented with this command type.\n\t\"\"\"\n\t@abc.abstractmethod\n\tdef run(self, cmd):\n\t\t\"\"\"\n\t\trun a command on the current script\n\t\t\"\"\"\n\t\n\t@abc.abstractmethod\n\tdef save_script(self):\n\t\t\"\"\"\n\t\tsave a script to be saved\n\t\t\"\"\"\n\t\n\t@abc.abstractmethod\n\tdef add_cmd(self, cmd):\n\t\t\"\"\"\n\t\tadd a command to the history\n\t\t\"\"\"\n\t\t\t\n\t@abc."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\t\n\ndef runit(script, args, ret=None):\n\t\"\"\"\n\texecute an ansible script and pass it to a cmd.\n\t@param script:\n\t@param args:\n\t@param ret:\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\t\n\tp = subprocess.Popen(args, stdout=subprocess.PIPE, stdin=None, \n\t\t  stderr=subprocess.PIPE)\n\t\n##\trunit(\"from lib2ssh import ssh_config_wrapper\", args)\t\n\t\t\n\twhile p.poll() is None:\n\t\tdata = p.stdout.readline()\n\t\tif not data"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\ndef __str__(script):\n\t\"\"\"\n\tchecks for proper commands and prints errors if necessary\n\t\"\"\"\n\t\n\tif script.cmd[0] =='ssh':\n\t\tpass\n\telse:\n\t\tprint 'unexpected command:',script.cmd[0]\n\ndef parse_script(script):\n\t\"\"\"\n\tParses a script.\n\t\"\"\"\n\t\n\tif not script:\n\t\traise ValueError('Script requires a script with the script command')\n\n\targs = script.cmd[1:]\n\tif '--no-script' in args and not args[0].startswith(\"echo\"):"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\t\n\t# get the script, which may include quotes around, convert newlines\n\tscript =''.join(textwrap.wrap(script,  width=80,  break_long_words=False)) \n\t\n\tsh = script.split('\\n')\n\t\n\t# get the command, which must start with a newline, convert that\n\tcommand =''.join(sh[:-1])\n\t\n\treturn command\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport hashlib\nimport json\nimport logging\nimport sys\nimport time\nimport uuid\nimport hashlib\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\nclass Script:\n\t\"\"\"\n\tThis class wraps around python scripts.\n\n\tThis class implements the python scripts to execute the script.\n\tEach python script must define one of the following functions on\n\tthe same object.\n\t\n\t.. code-block:: python\n\t\n\t\n\t\tpython = Script(\"echo '{\"string\":\"hello\"},\"echo '{string:\"world\"}' | python $SHELL''\")\n\t\n\t:param script:\n\t:param kwargs:\n\t:return:\n\t\"\"\"\n\tdef __init__(self, script, **kwargs):\n\t\tself.script = script\n\t\tself.kwargs"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n# ------------------------------------------------------------------------------------------------------------------\ndef main():\n\t# gets input from the user \n\t# inputs:\n\tinp = open(sys.argv[1])\n\tlines = textwrap.wrap(inp, 78, initial_indent=' ')\n\t\n\t# inputs:\n\tfor i in range(1,len(lines)):\n\t\t# gets the script, removes trailing '\\n', but prevents trailing '\\n' from command line\n\t\tscript = lines[i][0:line_len].rstrip('\\n')\n\t\tif inp.read(len(script)) == script:\n\t\t\t# if the script"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ndef run(func, args, extra_env=dict()):\n    \"\"\"\n\tRun a function in a process.\n\t\"\"\"\n\timport subprocess\n\t\n\targs = [str(x) for x in args]\n\tcommand_list = subprocess.list2cmdline(args)\n\tsubprocess.Popen(func, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=extra_env).communicate(command_list)\n\"\"\"This is the class that holds the classes and functions\"\"\"\n\nimport os\nimport platform\nimport sys\nimport signal\nimport subprocess\nimport threading\n\nimport"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, func, extra_env=None):\n\t\n\t\tself.func = func\n\t\tself.extra_env = extra_env\n\t\t\n\t\tself.args = self.func(args)\n\t\t\n\tdef __exit__(self, etype, evalue, traceback):\n\t\tif self.args and type(self.args[0])!= str:\n\t\t\traise subprocess.CalledProcessError(2, self.args[0])\n\t\treturn 0\n\t\nclass CppTestSubprocess(subprocess.Popen):\n\tdef __init__(self, args, env=None, **kwargs):\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\tos.environ['PYTEST_PYTHON'] = func\n\t\tsubprocess.run([args] + extra_env.get('args', ''))\n\n\ndef main():\n\t\"\"\"\n\tRun the function in a sub-process\n\t\"\"\"\n\ttry:\n\t\tcmd_line_list = []\n\t\tfor arg in sys.argv:\n\t\t\targ = arg.replace('-', '_')\n\t\t\targ = arg.replace('.', '_')\n\t\t\targ = arg.replace('.', '/')\n\t\t\tcmd_line_list.append(arg)\n\t\tcmd_line =''.join(cmd_line_list)"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ndef Run():\n\tos.popen('bash {}'.format(args),\n\t              'r').write(os.getenv('SHELL', ''))\n\tif args.find(\"which\") > 0:\n\t\tos.execv('which '+args,args)\n\tos.execv('clear' if args.strip('clear')=='0' else '')\n\ndef Clean(target):\n\timport shutil\n\tfor i in target:\n\t\tif i.split('.')[-1] == 'c':\n\t\t\tshutil.rmtree(i)\n\n\n\ndef main():\n\tglobal process_manager\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\ndef run(args=None, extra_env=None, debug=False):\n\t\"\"\"\n\tRun some function in a sub-process.  \n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run. The first argument to the first \n\t\t\targument must be ``func.__name__``.\n\targs : str\n\t    Any additional command line arguments to be passed in\n\t\t\tthe first argument to ``subprocess.run``.\n\t\t\t\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\tdebug : boolean\n\t    If true,"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\tprint('Running %s: %s, args=%s, extra_env=%s' %\n\t\t\t(func.__name__, func.__doc__, args, extra_env))\n\t\tret = subprocess.call((func.__name__, args), **extra_env)\n\t\tif ret!= 0:\n\t\t\traise Exception('subprocess failed (%s)' % ret)\n\nclass SubProcessRunner(object):\n\t\"\"\"\n\tRuns one or more functions in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be a module that is"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\nclass Function(object):\n\tdef __init__(self, func, arguments, extra_env, stdout=None):\n\t\tself.func = func\n\t\tself.args = arguments\n\t\tself.extra_env = extra_env\n\tdef run(self, source=None):\n\t\tstdout_fd = None\n\t\tif not source:\n\t\t\tsource = self.func(self.args, self.extra_env)\n\t\tstdout, stderr = subprocess.run(source, self.args, self.extra_env)\n\t\tstderr_fd = None\n\t\tif stderr:\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t    main = args\n\t    assert func.__name__ == func.__module__, (\n\t        \"Can't execute function '{0}'\".format(func))\n\t    def wrapper(*args, **extra_env):\n\t\tret = subprocess.run(func, args, **extra_env)\n\t\tret = [line for line in ret if line.strip()]\n\t\tif not ret: ret = None\n\t\treturn ret\n\t\n\twrapper.__name__ = func.__name__\n\twrapper.__doc__ = func.__doc__\n\twrapper.__dict__.update(extra_envs)\n\twrapper.__doc__"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\targs = args.split(' ') # Split up arguments\n\tfunc(*args)\n\n@Command('python')\ndef main(): \n    \"\"\"Run a function in a sub-process.\n\n\t\"\"\"\n\tif __name__ == '__main__': \n\t\tprint('python', 'a')\n\t\n\t\nif __name__ == '__main__':\n    main()\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n#"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t\n\n\ndef run(func, args, extra_env=None):\n\n\tif not isinstance(func, str):\n\t\tif not isinstance(args, str):\n\t\t\traise TypeError(\n                \"Only strings acceptable to '{}' \".format(func)\n\t\t\t)\n\t\tfunc = str(func)\n\n\t# Create a temp path to the command\n\ttemp_path = \"{project_name}.func\".format(project_name=os.environ['project_name'])\n\n\t# Create a temporary file handle\n\twith open(temp_path, 'w') as f:\n\t\tf.write"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tif name not in environment.config:\n\t\t\treturn Error(\"No environment variable supplied\")\n\t\tif re.search(\"default value (.*)\", environment.config[name]):\n\t\t\treturn Error(\"Env variable specified for environment variable %s has no value\" % name)\n\t\treturn environment.config[name][0]\n\ndef env_option_parser():\n\tparser = Parser(prog=\"env\", description=\"Get the value of environment (default is used with the environment variable NAME, VALUE) or default value. The variable NAME and VALUE should be the same.\"\n\t\t\t\t\t\t\t \" Multiple paths are allowed (variable"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tdef __init__(self, matcher):\n\t\tlogger = getLogger(type(self).__name__)\n\t\tlogger.info(\"Making a value for matcher: {}\".format(self.name))\n\n\t\tif matcher is None:\n\t\t\tmatcher = matcher_util.parse_config_data(\"m2m_matchers/emptyMatcher\")\n\t\tself.matcher = matcher\n\t\tlogger.info(\"Using: {}\".format(str(self.matcher)))\n\n\n\tdef __get_default(self):\n\t    # logger.debug(\"Calling: get_default {}\".format(self))\n            \n\t    # Get"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\ndef get_matcher():\n\ttry:\n\t\treturn MMatcher()\n\texcept NameError:\n\t\terror_msg(\"ERROR: No matcher found! Try the get_matcher_by_name function with a name.\")\n\t\treturn None\n\ndef get_parser():\n\treturn argparse.ArgumentParser(description=\"Get the value of variables from a matcher.\")\n\n\ndef run_command(args, parser):\n\tprint_verbose = True\n\tparser.print_help()\n\n\tprint_verbose = False\n\ttry:\n\t\targs.append(sys.argv[1])\n\t\tparser"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n# Set up default values\n_default_setting = { \n\t\"%s_value\"    : None,\n\t\"%s_default\"   : \"value\"\n}\n\n# Initialize global default values\n_default_setting = {}\n\nclass DefaultConfig:\n\tdef __init__(self, matcher, name, value):\n\t\tself._matcher = matcher\n\t\tself._name = name\n\t\tself._value = value\n\t\t\n\tdef set_strv(self, strv):\n\t\tself._name = strv\n\t\tself._value = self._matcher.get_property("}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\tenv_config_obj = EnvironmentConfigObj(environment)\n\tconfig_values = env_config_obj.get_config_values()\n\tdefault_value = env_config_obj.get_default_variable_value()\n\n\t# If the variable is defined in environment and the default value is not None, make the value\n\t# in env_config_obj\n\tif (config_values is not None and value is None and config_values is not None) or (default_value is not None and value!= None):\n\t\tresult = False\n\n\t\t# Parse the variable"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\ndef extract_match(environment, matcher):\n\t\n\tmatch = matcher.extract(environment)\n\t\n\tif match is None:\n\t\traise Error(\"Match could not be extracted.\")\n\t\n\tif len(match) == 0:\n\t\treturn default_value\t\n\t\t\n\tmatch = [m.group(1) for m in match.split(None) if m]\t\t\n\t\n\tif len(match)!= 1 or not os.path.isfile(path):\n\t\traise Error(\"Invalid %s file\" % path)\t\n\t\n\tvalue_list = []\n\t\n\tpath_index = path.rfind"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\ndef get_key_matcher(env, matcher, **kwargs):\n\t\"\"\"\n\tReturn the value from environment key matcher containing a matcher containing a specified name and an optional default value.\n\t\n\tIf the variable is not defined in environment and no key name is provided, an Error is raised.\n\t\"\"\"\n\t\ndef run(env, matcher, **kwargs):\n\t\n\ttry:\n\t\tget_value_matcher(env, matcher)\n\texcept Exception as error:\n\t\tif kwargs:\n\t\t\tprint(\"Exception occured\")\n\t\t\tprint(error)\n\t\telse:\n\t\t\traise error\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\n\n\ndef get_environment(matcher):\n\tif matcher.config.get(section_name, None) is None or matcher.config.get(section_name + '_default', None) is None:\n\t\n\t\t# print('getting environment of matcher with name \\'%s\\'' % matcher.find('%s'))\n\t\tglobal_dict = {'variable name_of_the_matcher': matcher.config.get(section_name, None), 'default': '0', 'default value_of_the_matcher': '0'}\n\t\t\n\t\t# If the variable is not defined in environment and no"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\ndef get_config_value_from_file_in_directory(path, matcher):\n\tconfig_path = os.path.join(path, matcher)\n\t\ndef get_config_value_from_file_in_files(config_path, files):\n\ttry:\n\t\treturn config_file_exists(config_path)\n\texcept IOError as e:\n\t\tprint e\n\t\treturn None\n\ndef read_config_values(config_file):\n\ttry:\n\t\tconfigs = json.load(open(config_file))\n\t\treturn configs\n\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t\tprint \"Making sure the environment is configured to return the key and value from the environment.\"\n\t\tconfig_file = \"MANGLED_ENVIRONMENT\"\n\t\tvalue = os.getenv(config_file, default=env)\n\t\tif value:\n\t\t\treturn value\n\n\t\tif matcher.name == '*':\n\t\t\treturn env\n\t\t\n\t\t# get default value from environment\n\t\ttry:\n\t\t\tvalue = os.getenv(matcher.value, \"none\")\n\t\texcept:\n\t\t\tprint \"MangLED version doesn't return default value for variable %s!\" % matcher.value\n\t\t\treturn"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\n\tbase = 'http://localhost:5000/v1.0/' \n\t\n\tif isinstance(image_href, str):\n\t\tparts = urllib.parse.urlparse(image_href)\n\telif image_href.startswith(base):\n\t\tparts = urllib.parse.urlparse(image_href)\t\n\telif isinstance(image_href, (list, tuple)):\n\t\tparts = image_href\n\telse:\n\t\traise ValueError(\"Invalid image href: %s\" % image_href)\n\t\n\tparts.path = image_href[len(base):]\n\t\n\tnetloc ="}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\tif not image_href.startswith('http'):\n\t\traise ValueError('Invalid image href:'+ image_href)\n\t\n\traw_url = image_href[:-5]\n\tparsed_url = urllib.parse.urlparse(raw_url)\n\tnetloc = 'data:' + parsed_url.netloc\n\tuse_ssl = parsed_url.scheme == 'https' or 'http'\n\t\n\timg = parsed_url.path\n\t# print parsed_url.path \n\treturn ('image', netloc, None, False, False, raw_url, use_ssl)\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\n\tparts = urlparse(image_href)\n\n\tif not parts.path:\n\t\traise ValueError('image href is not an absolute path (e.g. /path/to/some/file)')\n\n\turl_parts = parts.path.lstrip('/')\n\tif not url_parts[0] == '/':\n\t\turl_parts_len = len(url_parts)\n\t\tif url_parts_len > 1:\n\t\t\traise ValueError('image href cannot have a slash, use url as separator')\n\n\tif not parts.netloc:\n\t\traise ValueError('image href cannot have an"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\turl = image_href\n\tnetloc = query_netloc(netloc)\n\tuse_ssl = urlparse.urlparse(url).scheme == 'https'\n\treturn url, netloc, (use_ssl if use_ssl else None)\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ndef parse_url(image_href):\n\t\"\"\"\n\tParse an image url into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\tif image_href.lower().find('(') == -1:\n\t\traise ValueError('image_href is not well-formed, but '\n\t\t'the image_href must start and end with \"(/)\"')\n\tif image_href.lower().find('/') > -1"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\tparts = urllib.parse.urlparse(image_href)\n\tif parts[0] == 'https':\n\t\treturn (parts[1], parts[5], parts[6])\n\telse:\n\t\treturn (parts[0], parts[1], parts[6])\n\ndef checkParams(params):\n\tif '/s/$' not in params: params['/s/'] = \"\"\n\n\tif params[params.index('/s') + 1:] == 'S':\n\t\treturn params\n\telif params[params.index('/s') + 1:] == 'C':"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\ttry:\n\t\t\tdata = urllib.parse.parse_qs(image_href)\n\t\texcept (TypeError, ValueError):\n\t\t\traise ValueError(\"Invalid request\")\n\t\tnetloc = urllib.parse.urlsplit(data['netloc'])[2]\n\t\tuse_ssl = True\n\t\t\n\t\tparts = urlparse.urlparse(netloc)\n\t\t\n\t\t# We only pass the netloc if netloc was passed in via the image_href\n\t\tif not parts[2]:\n\t\t\turl_parts = urlparse.urlparse(url_parts[2])\n\t\t\tnetloc = url_parts[0]\n\t\t\t\n\t\t\tif url_parts.scheme"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\tif not image_href:\n\t\traise ValueError(\"url cannot be empty\")\n\t\n\tif image_href.startswith('/'):\n\t\tparts = urllib.parse.urlparse(image_href)\n\t\timage_id = parts[1]\n\t\tnetloc = parts[0]\n\t\tuse_ssl = parts[2]\n\t\n\telif image_href.endswith('/'):\n\t\tparts = urllib.parse.urlparse(image_href)\n\t\timage_id = parts[1]\n\t\tnetloc = parts[0]\n\t\tuse_ssl = parts[2]\n\t\n\telse:\n\t\traise ValueError"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\t\n\t# if there is no netloc but there are no usessl info in the urls (http or https),\n\t# we use the default one (https) if that is the case\n\tif not use_ssl and netloc:\n\t\tuse_ssl = False\n\t\n\tparts = urlparse.urlsplit(image_href)\n\t\n\turl = urlunsplit((parts[4], parts[3], parts[2], parts[1], parts[0]))\n\t\n\treturn url, use_ssl\n\"\"\"\nSupport for a Netatmo Controller, from Netatmo.\n\nFor more details about this platform"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\nexcept ImportError:\n    # Probably the url had been removed.  The first part of the\n    # image href that could be either an image url or an\n    # image id.\n    \n    raise ValueError(\"Could not retrieve the image\")\n\ndef get_image_parts(base_url, img_type):\n    \"\"\"\n\tDetermines a piece of images from a url and a base URL.\n\n\t:param base_url: base URL that is the base URL for a image\n\t:param img_type: type of image for which a part of images are to be added\n\t:returns: a"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tassert isinstance(input_list, list)\n\tif not isinstance(input_list,str):\n\t\tinput_list.append([])\n\n\tfor elem in input_list:\n\t\tif elem.endswith(os.sep):\n\t\t\telem = elem[:-1]\n\t\tif len(elem) < 4:\n\t\t\tbreak\n\t\telse:\n\t\t\tassert elem[-1] == os.sep\n\t\t\troot, extension = os.path.splitext(elem)\n\t\t\tif os.path.isdir(root):\n\t\t\t\tyield root, os.path.expanduser(elem[:-1] +"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tdef recurse_string(input_list):\n\t\tinput_list = [x for x in input_list if x!= os.curdir]\n\t\tnext_path = os.path.join(os.path.dirname(input_list[-1]), input_list[-1])\n\t\tif next_path == '' or next_path.startswith(current_path):\t\t\t\n\t\t\traise TypeError, \"No such pathname: %s!\" % input_list[-1]\n\t\n\t\treturn input_list[:-1]\n\t\n\tdef remove_paths(input_list, ignore):\n\t\t\"\"\"Remove"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\ti = 0\n\t\tnum_chars = 0\n\t\tdef remove_sep(d):\n\t\t\ti = i + 1\n\t\t\treturn d.count(os.sep)\n\t\tdef path_split(path):\n\t\t\treturn path.split(os.sep)\n\t\tdef path_join(path):\n\t\t\treturn path[:-1]\n\t\tdef remove(l, r):\n\t\t\tif i == num_chars:\n\t\t\t\ti = 0\n\t\t\t\tl = \"\"\n\t\t\telse:\n\t\t\t\ti = i + 1\n\t\t\t\tl = l.lstrip(os.sep)\n\t\t\td"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\toutput_list = []\n\t\tfor line in input_list:\n\t\t\tif line[0] == '\\n' or line[0] == '\\r':\n\t\t\t\toutput_list.append(line)\n\t\t\t\tcontinue\n\t\t\t\n\t\t\ttail = os.path.splitext(line)[-1]\n\t\t\tinput_list.remove(tail)\n\t\t\t\n\t\t\ttemp_output_file = tempfile.NamedTemporaryFile(suffix='.nc', delete=False)\n\t\t\t\n\t\t\t# Get paths and directory\n\t\t\tpaths_list, _, paths = os.walk(temp_output_file.name, topdown=True)\n\t\t\t\n\t\t\tfor"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n### Load the files to make a list.\n\t#for filepath in input_list:\n\t# \tfpath = os.path.join(ROOT_PATH, filepath)\n\t# \tif len(fpath) > 20:\n\t# \t\tbreak\n\t### Now loop through the input list\n\tfile_list = []\n\tfor line in input_list:\n\t\tif line.startswith(r'\\<'):\n\t\t\tcontinue\n\t\tfile_list.append(line)\n\treturn file_list\nimport sys\nimport os\nimport getopt\nimport time\nimport"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\t\n\t\ndef remove_seperators(input_list):\n\t\n\t\"\"\"\n\tRemove seperators from a list of strings\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    List of String\n\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\t\t\n\t\n\t\t\n\t\n\t\n\t\t\n\t\n\t\t\n\t\n\t\t\n\t\tdef remove_os_seperator(x):\n\t\t\n\t\t\tresult = []\n\t\t\t\n\t\t\tfor char in input_list:\n\t\t\t\n\t\t\t\tif char == '.':\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tresult.append(char)\n\t\t\n\t\t\t\tif len(result) > 1:\n\t\t\t\t\n\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tdef is_str(self, string):\n\t\t#TODO: what is this\n\t\t\"\"\"If a string has already been checked, return False. Otherwise\n\t\treturn True if the string is already checked, else return True if \n\t\tthe string has at least one 'path' character.\n\t\t\n\t\tArgs:\n\t    string: string to check\n\t\tReturns:\n\t    False If a string has already been checked\n\t\t\"\"\"\n\t\t#TODO: what to check\n\t\t\n\t\tif len(string) > 0:\n\t\t\tif string[0] == \".\": #TODO: what should check?\n\t\t\t\treturn True\n\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\t\n\tall_strings = []\n\t\n\t\n\t\n\tfor string in input_list:\n\t\tif string.endswith(os.sep):\n\t\t\tall_strings.append( string[:-1] )\n\t\telse:\n\t\t\tall_strings.append( string )\n\t\n\tif all_strings == []:\n\t\traise TypeError(\"Invalid input list passed.\")\n\t\t\n\tif any(string[i] for string in all_strings):\n\t\traise TypeError(\"Input list length has different number of characters than \" + \\\n\t\t\t\"the last character. If you expect list length to contain only \\\n\t\t\tcharacters, your list"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n# Check if the input list is a file (eg. single-character files)\n\tif not os.path.isfile(input_list):\n\t\t\traise TypeError\n# Find the last character in string list\n\tinput_list = str(input_list).rfind('\\n')\n\t# Convert to list\n\tif input_list < 0:\n\t    raise TypeError\n\t# Extract the last character from text list\n\ttext_list = input_list + 1\n# Check if the given string is one of the end of the string\n\tif text_list > input_list:\n\t    raise"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\n\ndef removeNthosseperation(input_list):\n    \"\"\"\n    Remove trailing os sepertators\n    \n    Args:\n\t    input_list: a list of string/string pairs\n\n    Returns:\n\t    A processed list of strings\n    \n    Raises:\n\t    TypeError\n\t\"\"\"\n       \n    def removeNthosseperationChar(x):\n        result = re.findall(x[0].strip(), x[1].strip())\n        return result\n\n    return list(removeNthosseperation(input_list))\n\n\ndef removeNTHosseperator(input_list):\n    \"\"\"\n    Remove trailing"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\tmatch = re.match(pattern, s)\n\t\tif match:\n\t\t\tregex = \"^\"+match.group(1)\n\t\t\tpattern = \"^\"+match.group(2)\n\t\t\treturn re.compile(regex)\n\t\telse:\n\t\t\traise IOError(\"Not allowed to process {}\".format(s))\n\n# Create a regex pattern from the given string\ndef regex_pattern(s):\n\t\n\tmatch = re.match(pattern, s)\n\tif match:\n\t\treturn match.group(1)\n\telse:\n\t\traise IOError(\"Not allowed to process {}\".format("}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\tregex_patterns = re.compile(r'(%s)((%s)+)([\\w\\.]+)$' % (regex, regex))\n\tdef _convertToRegex(self, in_string):\n\t\tregex = re.compile(regex_patterns)\n\t\treturn regex.sub('\\1', in_string)\n\"\"\"\n\tThis method removes all lines that are not alphanumeric\n\tin one line of input.  If regex is found, then return all lines\n\tin regex.  Returns None on error, so if we don't match by default\n\tthe result is None\n\t\"\"\"\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef replacePattern(inputString, replacement) :\n\t\"\"\"\n\tThis method converts the given string to a regular expression.\n\t\n\t@type inputString: string\n\t@param inputString: String to be transparented\n\t@type replacement: string\n\t@param replacement: New pattern in input string\n\t\"\"\"\n\t\ndef replaceNonRepealChar(inputString) :\n\t\"\"\"\n\tThis method converts the given input string to a regular expression with a non-repeal character\n\t@type inputString: string\n\t@param inputString: String to be transparented\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n#\tdef regex_pattern_to_regex(self, string):\n#\t\t\treturn re.compile(regex)\n#\t\n#\t\tdef convert_to_regex(self, message):\n#\t\t\treturn re.compile(self.regex_pattern_to_regex(message))#!/usr/bin/env python\n\n# coding: utf-8\n\n\"\"\"\nCopyright (c) 2010 Joachim Karpathy (johnparpathy@gmail.com)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\t# Check for regex patterns\n\t\tfor pattern, replacement_type in [('~',  re.compile(r\"^(?:^|\\s*)+\"), re.DOTALL) ]:\n\t\t\treturn re.sub(pattern, replacement_type, string)\n\t\t# Check for a string which already matches\n\t\tif pattern in re.split(r'\\s+', string):\n\t\t\treturn re.match(pattern, string)\n\t\"\"\"\n\t\tThis method adds a regex pattern that matches any string\n\t\"\"\"\n\t\treturn re.compile('(^|\\s*)+')\n\t\"\"\"\n\tdef normalize(self"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\nclass Dna(object):\n\tdef __init__(self):\n\t\t# regex pattern\n\t\tself.patterns = {\n\t\t\t\"a\" :  \"(?P<num>\\d{8})$\",\n\t\t\t\"bb\" :  \"(?P<num>\\d{8})$\",\n\t\t\t\"cc\" :  \"(?P<num>\\d{8})$\",\n\t\t\t\"dd\" :  \"(?P<num>\\d{8})$\",\n\t\t\t\"bbb\" :  \"(?P<num>\\d{8})$\",\n\t\t\t\"bbc\" :  \"(?"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef getRegex(pattern):\n\t'''\n\t\tThis method converts a list of regex patterns that will look for a regular expression \n\t\tgiven\n\t'''\t\t\n\tif(len(pattern)==0):\n\t\traise ValueError('None of the regex pattern cannot be empty')\n\treturn str(re.compile(pattern))\n\t\ndef test():\n\t'''\n\t\tThis method tests the program\n\t'''\n\tprint('This python script test program')\n\tcmd = \"c3dte_1.py\"\n\t#cmd = \"c3dte_2.py\"\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef parse(text, flags = 0, use_regexp = True, use_regexp_flags = 0, use_regexp_re = False):\n\t\"\"\"\n\tConverts the given string to regex pattern\n\t\"\"\"\n\t\n\t\n\tpattern = r'''(?x)(?:\n        (?P<pattern>[^\\:]+)\n        (?P<flags>[^\\:]+)\n    )$'''\n\tmatch = re.search(pattern, text)\n\ttry:\n\t\tpattern = re.compile(pattern, flags)\n\texcept Exception as ex:\n\t\tex.args = sys.exc_info()\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\nimport logging\n\n\n#logging.basicConfig(level=logging.DEBUG)\n\"\"\"\nThis method reads the pattern and adds it to an internal object.\n\"\"\"\ndef parse_regex(pattern, name_length):\n\tpattern1 = re.compile(pattern)\n\treturn parser.parse(pattern1.sub(pattern2, pattern1)).get_match().get_string()\n\n\"\"\"\nThis method will returns the result of the command\n\"\"\"\ndef get_values(command, log=None):\n\tresult_list = []\n\toutput = logging.getLogger(\"\")\n\tfor"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\n\t\n\tdef convert(self, pattern):\n\t\tresult = re.findall(pattern, self.s)\n\t\treturn ''.join(result)\n\n\t@staticmethod\n\tdef convert2Regex(pattern, regex):\n\t\tpattern = '|'.join(pattern.split())\n\t\tregex = re.compile(pattern, re.DOTALL).search\n\n\tdef read(self):\n\t\twhile 1:\n\t\t\tres = self.file.read(2 * self.chunksz)\n\t\t\tif res is not None:\n\t\t\t\tbreak\n\t\t\tself.file.seek(0)\n\nclass Src:"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tsys.stdout.flush()\n\t\tsubprocess.check_call(sys.argv[1:], stdout=subprocess.PIPE)\n\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#!/usr/bin/python\n\n\"\"\"\n\nCopyright (C) 2014-2015 Victor Olaya.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef execute(args):\n\t\treturn subprocess.call(args, shell=True)\n\"\"\"\ndef run(argv):\n\t\treturn subprocess.call(argv)\n\"\"\"\ndef is_running():\n\treturn subprocess.call(subprocess.list2cmdline(sys.argv)) == 0\n\n\ndef start(argv=None):\n\tif argv is None:\n\t\targv = sys.argv\n\tif not os.path.exists(argv[0]):\n\t\traise RuntimeError(\"The executable '%s' is not running. Please specify a path to the executable.\" % argv[0])\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef shellCmd(args,env):\n\t\"\"\"\n\tExecute command(s).\n\tUses shell command(s), and tries to execute the given shell command(s).\n\t\"\"\"\n\t\n\t\t\n\t#Check the version.\n\tif \"V0.0\" in env[env.index(\"VERSION\")+1:]:\n\t\treturn runShellCommand(args,env.pop(env.index(\"VERSION\")+1))\n\telse:\n\t\treturn runShellCommand(args,env.pop(env.index(\"VERSION\")+1))\n\ndef command():\n\t\"\"\"\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\n\t\n\toutput = subprocess.check_output(command)\n\t\n\n\t# Read in intensities of length and length of sample\n\tt = []\n\t# If the sample has more than one satellite in each saturation region\n\t# make the list of the remaining satellites\n\twhile(len(t) > Saturation_Region_Length - 1 ):\n\t\tt.append(sample[0].lower())\n\t\t\n\t# If the sample has only one satellite, use the list of all saturation\n\t# sites and remove them from the sample list\n\tfor s"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\twith subprocess.Popen(cmd.split(), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True) as proc:\n\t\t\tretcode = proc.wait()\n\t\t\tif retcode:\n\t\t\t\traise CommandError(\"Command '%s' failed with exit status %d\" % (cmd, retcode))\n\t\t\tout, err = proc.communicate()\n\t\t\treturn out, err\n\n\tdef kill_processes(self, processes=None, kill_pid=None):\n\t\t\"\"\"\n\t\tKills or terminates processes. Runs the command with as much"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tsubproc = subprocess.Popen([ \"python\", \"wget\", \"-q\", \"http://localhost:4001/root/index\"], stdout=subprocess.PIPE)\n\t\tproc = subproc.stdout.readline().split(\"\\t\")\n\t\tstdout.close()\n\t\tproc = [ x.split(None, 1) for x in proc ]\n\t\t#proc = map( int, proc )\n\t\tif len(proc)>12: \n\t\t\tprint \"[error] Error parsing sub-processes. Please check the logs for details\"\n\t\toutput, error = proc[3]\n\t\ttry:"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef get_result(text, timeout):\n\t\"\"\"\n\tParse the output of the given command\n\t\"\"\"\n\treturn subprocess.check_output([\"ssh-keygen\", \"-L\", text])\n\ndef parse_command(cmd):\n\t\"\"\"\n\tParse the output of the given command\n\t\"\"\"\n\treturn cmd.split(' ')\n\ndef main(argv):\n\t\"\"\"\n\tParse and run the given command.\n\n\tArgs:\n\t\targv (List or SingleLine(arg))\n\t\t\t\tList of arguments received from the command file\n\t\"\"\"\n\n\tcmd = None"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tassert isinstance( args, basestring ), 'Unknown (s).'\n\t\tfor arg in args.split( ',' ):\n\t\t\tcmd = ''\n\t\t\tif arg.strip() or '-' in arg:\n\t\t\t\tcmd = arg +''\n\t\t\t\tcmd[-1] = '-'\n\t\t\telif arg.strip():\n\t\t\t\tcmd = arg.strip()\n\t\t\t\tcmd.pop(0)\n\t\t\t\tcmd[-1] = cmd[-1] +''\n\n\t\t\tself.call(cmd)\n\n\tdef call(self, cmdList):\n\t\texec(cmdList, self.shell)\n\n\tdef exit("}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\nclass Popen:\n\t\"\"\"\n\t\tSimple class to do a subprocess.\n\t\"\"\"\n\t\nclass PopenContext:\n\t\"\"\"\n\t\tThis class is useful for working with Popen functions.\n\t\"\"\"\n\t\n# class PopenContext:\n#\t\"\"\"\n#\tClass to handle the environment variable substitution of executables\n#\t\"\"\"\ndef expand_commands(args):\n\t\"\"\"\n\t\tExpand a list of command strings before running them.\n\t\"\"\"\n\t\ndef run_command_subproc(cmd, shell=True):\n\t\"\"\"\n\t\tExecutes the given command(s).\n\t\"\"\""}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef main():\n\t\n\targs = sys.argv[2:]\n\tif args:\n\t\tfor arg in args:\n\t\t\ttry:\n\t\t\t\tpipe = subprocess.Popen(filter_command(arg, os.environ), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\texcept OSError as e:\n\t\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\t\traise e\n\t\n\tp = subprocess.Popen([sys.executable, '-c','--verbose','-C', \n\t\t\t\t\t\t'drawtext','-b:x','--no-pager', '-L', \n\t\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t return True\n\t\n\nfrom ecotools.ecosystem import (\n    Ecosystem,\n    EcosystemError,\n    EcosystemException,\n    EcosystemPlugin,\n    EcosystemPluginError,\n    EcosystemExceptionData,\n    EcosystemNotIP\n)\n\nimport re\nimport json\nfrom.common import InfoExtractor\nfrom..compat import compat_str\nfrom..utils import (\n    compat_urllib_parse,\n    remove_start,\n    int_or_none,\n    str_to_int\n)\nfrom..compat_urllib import urlencode\nfrom..utils import (\n   "}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\n\t\tpass\n\n\tdef test_ipv4address(self):\n\t\n\t\tself.assertEquals(ipaddress.IPv4Address('10.10.10.10'), '10.10.10.10')\n\t\n\t\n'''\n\tTest if IPv4 address type\n'''\n\n'''\n\ttest:\n\t1000000000000000000000000000:10::10\n\t160000000000000000000000000000\n\t16:000000000000000000000000000:20::10\n\t20000000000000000000000000000:20::10\n\t2:00000000000000000000000000000\n\t2:00000000000000000000000000000:2::10\n\t"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n    def test_simple_ipv4_addresses(self):\n        \"\"\"\n            Test if IPv4 address or not\n               \n\t\"\"\"\n        \n        self.assertTrue(len(IP('127.0.0.1')) >= 24)\n        self.assertTrue(len(IP('1.1.1.1')) >= 27)\n        self.assertTrue(len(IP('1.1.1.1')) >= 38)\n        self.assertTrue(len(IP('1.1.1.2')) >= 19)\n        self.assertTrue(len(IP('1.9.9.8')) >= 40)"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n##\tdef testIpv4Address(self):\n\t\n\tdef test_ipv4Address_invalid_address(self):\n\t\t\n\t\ttry:\n\t\t\tip = ipaddress.IPv4Address('0.0.0.0')\n\t\t\tself.assertTrue(False)\n\t\t\t\n\t\t\tip = ipaddress.IPv4Address('10.0.99.0/24')\n\t\t\tself.assertFalse(True)\n\t\t\t\t\t\n\t\t\tip = ipaddress.IPv4Address('192.168.1.1')\n\t\t\tself.assertTrue(True)\n\t\t\t\n\t\t\tip = ipaddress.IPv4Address('192.168.1.1"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t \n\tdef testV6(self):\n\t\n\t\tip = ipaddress.IPv6Interface('+0200::')\n\t\t\n\t\tself.assertEqual(ip.network, \"8000::/126\")\n\t\tself.assertEqual(ip.broadcast, None)\n\t\tip = ipaddress.IPv6Interface(['00:00:01:01:02'])\n\n\t\tself.assertEqual(ip.network, \"8c00::/127\")\n\t\tself.assertEqual(ip.broadcast, \"fe80::0\")\n\t\tip = ipaddress.IPv6Interface('ff00::0')\n\t\t\n\t\tself.assertEqual(ip.network, \""}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n#===============================================================================#\n\tclass TestIPv6(unittest.TestCase):\n\t\t\"\"\" Test IPv6 address constructor. \"\"\"\n\t\t\t\n\t\t#===============================================================================#\n\n\t\tclass TestIPv6(unittest.TestCase):\n\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\tipaddress.IPv6('1.1.1.1')\n\t\t\t\tassert False\n\t\t\texcept AttributeError:\n\t\t\t\tassert True\n\t\t\telse:\n\t\t\t\traise AssertionError('Should have raised ValueError')\n\n\t\t#===============================================================================#\n\n\t\tclass TestIPv6Bad(unittest.TestCase):\n\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\tipaddress.IPv6('')\n\t\t\t\tassert False\n\t\t\texcept AttributeError:\n\t\t\t"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\ndef test():\n    assert_almost_equal(parse_netmask(b'255.255.255.0'), 0.0)\n\ndef test_4():\n    assert_almost_equal(parse_netmask(b'1.1.1.1'), 0.0)\n\ndef test_5():\n    assert_almost_equal(parse_netmask(b'255.255.255.255'), 1.0)\n\ndef test_6():\n    assert_almost_equal(parse_netmask(b'\\x01'), 1)\n\ndef test_7():\n    assert_"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\tv4_ipv4_1 = ipaddress.IPv4('1.1.1.1')\n\t\tv4_ipv4_2 = ipaddress.IPv4('1.1.1.2')\n\t\t\n\t\tv4_ipv4_5 = ipaddress.IPv4Address('5.5.5.9')\n\t\tv4_ipv4_6 = ipaddress.IPv4Address('2001:4860:4860:4860::101')\n\n\t\tv4_ipv4_7 = ipaddress.IPv4Interface('2001:4860:4860::102')\n\t\tv4"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n    def test_01_ipv4_address(self):\n\t\t\"\"\"\n\t        Test IPv4 address\n\t    \n\t       ..................................................................................\n\t        IPv4 address\n\t\t\"\"\"\n\t\toutput = ipaddress.IPv4Interface('::1/128')\n\t\tself.assertEqual(output.address, '1.1.1.1')\n\t\n\tdef test_02_ipv4_address(self):\n\t\t\"\"\"\n\t        Test IPv4 address\n\t    \n\t       ..................................................................................\n\t        IPv4 address\n\t\t\"\"\"\n\t\toutput = ipaddress.IPv4Interface('::1/128')"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\nclass testBase(object):\n\t\"Test base class for IPv4 object\"\n\t\n\tdef testAddress(self):\n\t\t\"\"\"\n\t\tTest what happens when working with IPv4 properly\n\t    \t\t\n\t\n\t      \n\t      \n\t      \n\t      \n\t       ........................................................................\n\t \n        \"\"\"\n\t\t\n\t\taddr = ipaddress.ip_address(\"\",\"192.168.15.255\")\n\t\tself.assertRaises(ipaddress.NetworkX.AddressValueError, lambda:addr.isValid())\n\t\t\n\tdef testPort(self):\n\t\t\"\"\"\n\t\tTest what happens when working with IPv4 properly\n\t    \t\t\n\t      \n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, principals, subClassOf=rdflib.RDFS.PathRef):\n\t\n\t\"\"\"\n\tFind 'roots' for'subClassOf'\n\t\"\"\"\n\t\n\tp = principals[0]\n\t\n\tif not p.parents:\n\t\t\n\t\tp.parents = graph\n\t\n\tfor p in p.parents:\n\t\t\n\t\tif p.name == 'property':\n\t\t\tp.prop = p.value\n\t\t\tp.property = p.type\n\t\t\t\n\t\t\t#print \"find roots for \" + p.name\n\t\t\tif p.children:\n\t\t\t\tfor"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\n\t## Construct the roots in a way which provides proper ordering.\n\ttreeToOrder = set([RDFS.subClassOf])\n\tfor node in tree:\n\t\tif not node.isRoot():\n\t\t\tif node.isRDF():\n\t\t\t\tif node.getChildren() is not None:\n\t\t\t\t\tfor other in node.getChildren():\n\t\t\t\t\t\ttreeToOrder.update(other.getChildren())\n\t\t\t\telse:\n\t\t\t\t\ttreeToOrder.add(node)\n\t\t#elif node.getChildren() is not None:\n\t\t\t #for other in node.getChildren():\n\t\t\t\t #tree"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n#from lxml import etree\n\n#from rdflib import GraphRDF, URIRef, XSD, XSDSubject\n\ndef check_uri_subset(graph, subject):\n\t\"\"\"\n\tFind all roots in some sort of transitive hierarchy.\n\t\n\tLook in all nodes in all sub-class hierarchy and check that each root \n\tin the tree is within each of the given subjects.\n\t\n\tParameters\n\t----------\n\tgraph : rdflib.Graph\n\t\tgraph representation of the transitive hierarchy\n\tsubject : URIRef\n\t\tsubject URI for the root is located in\n\t\t<path>/"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\troot_tree, root_list = [],[]\n\tfor u,v in graph.triples((\n\t\t\t((u, graph.node[u], graph.node[v]))\n\t\t\tif graph.has_key(u) \n\t\t\tor graph.has_key(v)) \n\t\t\tfor node, (prop, parent) in graph.triples_properties(root))\n\t\t):\n\t\t\troot_list.append((u, node, graph.node[v]))\n\t\ndef find_roots_simple_match(graph, root_list, isRootOnly=False):\n\t\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\t# Get all roots of the sub-class hierarchy\n\troots = []\n\t\n\t# Loop through each node and add it for every node\n\tfor node_id, sub_class in graph.nodes.items():\n\t\tif node_id in roots:\n\t\t\tcontinue\n\t\t\n\t\t# Skip nodes with no common ancestor in base graph\n\t\tif graph.node[graph._node[node_id]['commonAncestor']]['value']:\n\t\t\tcontinue\n\n\t\tif graph.node[sub_class]['value']:\n\t\t\tcontinue\n\t\t\n\t\t# Add to all roots\n\t\troots.append(graph.node"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\n\ndef find_roots(subtype = None, node=None, tree=None, max_depth = 3):\n\t\n\t# Look for roots\n\tif tree is None:\n\t\ttree = rdflib.Graph()\n\t\ttree.subject_predicate(RDFS.subClassOf, SKOS)\n\t\ttree.add((node, SKOS), 2)\n\t\n\tif subtype is None:\n\t\tsubtype = SKOS\n\t\tfor node, subattrs, parent in tree.subnodes.iteritems():\n\t\t\ttree_root = find_roots(subtype = subtype, node"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\tdef find_roots(graph, rdflibClass):\n\t\tparent = find_root(graph, rdflibClass, SKOS.broader)\n\t\tif'skos.' in parent.properties:\n\t\t\tsubClass = find_subClass(graph, skosClass = SKOS.broader)\n\t\t\tparent.properties.append('skos.' + subClass)\n\t\t#else:\n\t\t#\tprint('WARNING: The subClass Of SKOS is neither \"skos.\" nor SKOS.__class__ of SKOS.')\n\tfind_roots(graph, rdflib."}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef get_parents(graph, rdflib):\n\t\n\tif ( rdflib.__class__ == rdflib.skos.subClassOf and\n\t\trdflib.datatype.SKOS not in [ rdflib.datatype.subClassOf, rdflib.datatype.broader ]\n\t\t\tor rdflib.datatype.bidir ):\n\t\t\n\t\treturn set([ x[1].get_parent() for x in graph.edges if len(x)==2 ] )\n\telse:\n\t\treturn set()\n\ndef get_node_groups(group_dict):\n\tfor node in group_dict"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\n\ndef sort_roots(root_iter, topdown=False):\n\t'''\n\tSort roots using a depth first search.\n\t\n\t@type root_iter: C{(tuple, object)}\n\t@param root_iter: An iterator object representing the root.\n\t\n\t@raises IndexError: If a root with a root_iter wasn't found.\n\t@param topdown: If the iterator is of the same root as the root of\n\tthe graph, the root object itself is returned as top.\n\t\n\t@rtype: C{(list, tuple)}\n\t@return: A"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\n\tif DEBUG: print(\"graph:find_roots\", graph)\n\n\tdef find_root(root, node):\n\t\tif root!= None:\n\t\t\treturn node\n\t\telse:\n\t\t\treturn node[0]\n\t\n\tdef get_root_graph(graph, root):\n\t\tgraph.postorder_of_input()\n\t\tgraph.parent(root)\n\t\twhile 1:\n\t\t\tchildren = graph.children(root)\n\t\t\troot = find_root(children[0], children[1])\n\t\t\tif DEBUG: print(\"graph:get_root_graph\", graph,"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ts = s.encode('utf-8')\n\t\tw = yaml.dump(s, default_flow_style=False)\n\t\treturn w\n\"\"\"\n\n\ndef get_all_lines_from_file(filename):\n\twith open(filename) as f:\n\t\tlines = f.readlines()\n\treturn lines# -*- coding: utf-8 -*-\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n\n        # Changing field 'ArticleGroup.language'"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\n\tdef __init__(self, yaml_path=\"config.yaml\"):\n\t\tself.yaml_path = yaml_path\n\t\twith open(self.yaml_path, \"r\", encoding=\"utf-8\") as f:\n\t\t\tself.yaml_dict = yaml.load(f)\n\t\t\tself.loaded_dict = dict(self.yaml_dict)\n\t\t\t\n\n\tdef dump(self):\n\t\twith open(self.yaml_path, \"w\", encoding=\"utf-8\") as f:\n\t\t\tyaml.dump(self.loaded_dict, f, default_flow"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ndef dump(object, f):\n\t\"\"\"\n\tDump object and write it to a file\n\t\"\"\"\n\t\nclass Serializer(yaml.SafeDumper):\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself._dict = vars()\n\t\tself._str = lambda x: str(x) + str(type(x)).replace('_','')\n\n\tdef default_flow_style(self):\n\t\t\"\"\"\n\t\tReturns the default flow-style\n\t\t\"\"\"\n\t\treturn 'default'\n\n\tdef get_flow_style"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\tdef dump_data_dict(self, dumper, stream):\n\t\t\"\"\"Export data dict to a py3-unicode or py2-string\"\"\"\n\t\tdumper.dump_dict(self, stream)\n\n\nclass Dumper(object):\n\t\"\"\"\n\tDumps data in py3-unicode or py2-string\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.dumper = self.Dumper()\n\n\tdef DumpDataDict(self, dumper, stream):\n\t\t\"\"\"Export data dict to a py3-unicode or py2-string\"\"\"\n\n\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\tf = file(file, mode='w')\t# Write to file\n\t\tyaml.dump(data, f)\n\t\tf.close()\n\t\t\n\"\"\"\n\tConvert data to yaml format\n\t\"\"\"\n\tdata = yaml.dump()\n\tf = file(file, mode='a+')\t# Write to file\n\tcfg = yaml.load(data)\n\t\n\tf.close()\n\tf = open(file, 'r')\t# Open file\n\tdata = yaml.load(f)\n\tf.close()\t\t\n\t\n\treturn data\n\t\ndef get_data"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\nimport traceback\nimport os\nimport json\nimport logging\nfrom types import ModuleType\n\nfrom.util import parse\nfrom.util.common import ensure_dir\nfrom.util.cli import setup_app\n\nlogger = logging.getLogger('py2app')\n\n\nclass CliError(Exception):\n\t\"\"\"Base class for exceptions from Cli and Ce\"\"\"\n\tpass\n\nclass CliErrorList(CliError):\n\t\"\"\"Object which allows errors to propagate to Cli classes\n\n\tThis should be subclassed by the derived class\n\t\"\"\"\n\n\tdef __str__(self"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t \n\timport sys\n\timport re\n\timport os\n\t\n\tfrom copy import deepcopy\n\tfrom pprint import pprint\n\n\timport argparse\n\n\tclass ParserException(Exception):\n\t\tdef __init__(self, **kwargs):\n\t\t\tsuper(ParserException, self).__init__(kwargs['msg'])\n\n\tparser = argparse.ArgumentParser(usage=parser.format_help())\n\tparser.add_argument(\n\t\t\"log_file\",\n\t\t\"file containing py2to json file\",\n\t\thelp=\"log output to file\"\n\t)\n\tparser.add_argument(\n\t\t\"target"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\tout = StringIO()\n\t\tyaml.safe_dump(self.data, out, default_flow_style=False, allow_unicode=True)\n\t\treturn out.getvalue()\n\t\t\n\tdef pprint(self):\n\t\traise NotImplementedError\n\"\"\"\n\n\tdef read_text(self):\n\t\t\"\"\"\n\t\tRead a text file given the file path and read it, write it to\n\t\ta py2-unicode\n\t\t\"\"\"\n\t\t# read the document\n\t\ttext = self.data\n\t\tin_utf8 = False\n\n\t\tfor line in text.splitlines():\n\t\t\tin_utf8"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ndef showDiff(oldVersion, newVersion):\n\t\n\t# Print as diff\n\tdiff = difflib.unified_diff(oldVersion.lower(), newVersion.lower(), fromfile='config.py', n=2)\n\tprint 'Diff ', diff[0]\n\t\n\t# Check if old version\n\tif type(oldVersion)!= str:\n\t\ttype(\"Old version must be a string\")\n\t\n\t# Check if new version\n\tif type(newVersion)!= str:\n\t\ttype(\"New version must be a string\")\n\t\t\n\t# Print as verbose\n\tprint'' +"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\nclass XMCLIObject:\n\t\n\tdef __init__(self):\n\t\t\n\t\tself.file = open('../data/xmcli.yml', 'w')\n\t\t\n\t\treturn\n\n\tdef get_xmcli(self, section):\n\t\t\n\t\tif section =='main':\n\t\t\tdata = {}\n\t\t\tfor sectiondict in self.sections:\n\t\t\t\tfor section in sectiondict.keys():\n\t\t\t\t\tif section not in data:\n\t\t\t\t\t\tdata[section] = []\n\t\t\t\t\tdata[section].append(sectiondict)\n\n\t\t\t#return data\n\t\t\n\t\tdata = {}\n\t\tfor sectiondict in self."}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\t# Get the location of this logger. Only for debugging: \n\t\t_logger = logging.getLogger(__name__)\n\n\t# Configure logging parameters --\n\thandler = logging.StreamHandler()\n\thandler.setLevel(logging.DEBUG)\n\thandler.setFormatter(logging.Formatter(\"%(process)d: %(asctime)s  %(threadName)s  %(levelname)s - %(message)s\"))\n\t_logger.addHandler(handler)\n\t\n\t# Use the same logger as this class\n\t# (which will log to a file and will not be displayed on the screen)\n\t_logger."}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\n\tdef __init__(self,fileroot=None,parentname='Logger'):\n\t\t\"\"\"\n\t\tInitialize\n\t\t\"\"\"\n\t\tsuper(Logger,self).__init__(parentname,parentname)\n\t\tself.fileroot=fileroot\t\t#Set the current path\n\t\tself.logger_enabled = False\n\t\t\n\tdef enable(self):\n\t\t\"\"\"\n\t\tEnable the application logger.\n\t\t\"\"\"\n\t\tself.logger_enabled = True\n\n\tdef debug(self):\n\t\t\"\"\"\n\t\tLog debug\n\t\t\"\"\"\n\t\tself.fileroot=self.logger_enabled\n\t\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nfrom sqlalchemy import create_engine, MetaData, Table, Column, select, and_, is_\nfrom datetime import datetime, timedelta\nfrom operator import itemgetter\nfrom django.core.management.base import BaseCommand\n\n\nclass Command(BaseCommand):\n\t\"\"\"\n\t\tUsed to extract an app's data from the database.\n\t\t\"\"\"\n\t\n\t\n\t# The file path where the current project or user settings will be stored\n\tPROJECT_SETTINGS_FILENAME=None\n\n\tdef handle(self, *args, **options):\n\t\t\"\"\"\n\t\tOpens all the possible paths in settings and extracts all \n\t\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass LoggerApp (QtCore.QMainWindow,QtGui.QTextStream):\n\n\t\"\"\"\n\tA logger subclass.\n\t\n\tUsed to communicate directly with Logo to the logging module\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tQtCore.QMainWindow.__init__(self)\n\t\t\"\"\"\n\t\t# setup some things\n\t\tself.log_handler = logging.StreamHandler()\n\t\t# set the parent to the handler\n\t\tQtCore.QTextStream.__init__(self)\n\t\t# set the parent to the output stream\n\t\tself.output_stream"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n#\n# logging.basicConfig(level=logging.INFO)\n# logging.getLogger('boto').setLevel(logging.INFO)\n#\n\n\ndef log(msg, *args):\n\t\"\"\"\n\tDecorator to use for debugging.\n\t\"\"\"\n\t\n\tdef wrapped(func):\n\t\t\"\"\"\n\t\tUsed for Python 2.7 debug log.\n\t\tWill be used for Python 3.\n\t\t\"\"\"\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tlogging.info(msg % func.__name__)\n\t\t\treturn func(self, *args, **kwargs)\n\n\t\treturn"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass Logger:\n\tdef __init__(self,name=\"pyinotify\"):\n\t\tself._log = None\n\t\tself._level = logging.INFO\n\t\tself.name = name\n\t\n\tdef error(self,format,**kwargs):\n\t\t\"\"\"\n\t\tLog error message. Format dict with error message as key/value\n\t\t\"\"\"\n\t\terror = dict(format)\n\t\terror['error']='pyinotify error message error:%s'%error['error']\n\t\tself._log = logging.getLogger(self._name)\n\t\treturn self._log.error(error)\n\t\n\tdef"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\n\nclass ErrorLog:\n\tdef __init__(self, log_name, err_cnt):\n\t\tself.log_name = log_name\n\t\tself.err_cnt = err_cnt\n\nclass LoggerAdapter:\n\t\"\"\"\n\tThis class handles logging and logging of output to a stream. This is the main\n\tapp's logger with a simple log handler. It's also responsible for writing commands\n\tinto a file object (asynchronously).\n\t\"\"\"\n\tdef log_debug(self, msg, *args):\n\t\t\"\"\"Logs a message with text message *msg"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ndef debug(msg):\n\t\"\"\"\n\tDebug method.\n\tWill print message to stdout\n\tReturns:\n\t\tmsg - str, str\n\t\"\"\"\n\tprint str(msg)+\"::\"+str(datetime.datetime.now())\n\ndef warn(msg):\n\t\"\"\"\n\tWarn method.\n\tWill print message to stdout\n\tReturns:\n\t\tmsg - str, str\n\t\"\"\"\n\tprint str(msg)+\"::\"+str(datetime.datetime.now())\n\ndef error(msg):\n\t\"\"\"\n\tError method.\n\tWill print message to stdout\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\tglobal config\n\t\tconfig = None\n\t\tif DEBUG:\n\t\t\tprint \"\"\n\t\t\tprint \"=========================\\n=========================\\n\"\n\t\t\tprint \"=========================\\n\"\n\t\t\tconfig_file = os.path.expanduser('.config')\n\t\t\tlogging.basicConfig(filename=config_file,level=logging.INFO)\n            debug_info('Configuration read at'+ config_file +':'+ config)\n\t\t\twith open('.config', 'r') as config_lines:\n\t\t\t\tlogging.info(\"Read Configuration \" + config_lines.read())\n\t\tif DEBUG:\n\t\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nimport logging\nlogging.getLogger(\"py.core\").addHandler(logging.NullHandler())\nimport shutil\n\nimport logging\nfrom logging.handlers import RotateFileHandler\nfrom struct import unpack, pack\n\nimport os,sys\nimport traceback\n\nimport pysqldb\n\ntry:\n\tos.stat(os.environ.get('PYTHONUSER',os.getlogin())+'/scripts/mysqld.py')\nexcept:\n\tpass\n\ntry:\n\tos.unlink(os.environ['MYSQL_USER'])\nexcept:\n\tpass\n\nclass LogHandler(logging"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\tsize = 10**20\n\t\n\tshape = (13, size)\n\t\n\tdtype = np.dtype([\n\t\t('dim1', np.complex64, 1),\n\t\t('dim2', np.complex128, 1),\n\t\t('dim3', np.longlong, 1),\n\t\t('dim4', np.ulonglong, 1),\n\t\t('dim5', np.ulonglong, 1),\n\t\t('dim6', np.ulonglong, 1)\n\t\t])\n\t\n\tdef __call__(self, x):\n\t\tnp.testing.assert_array_almost_equal(x[:,"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n#\tcheck types:\n#\tarr_arr = np.asarray(shape)\n#\tarr_dtype = np.find_common_type([np.double, np.float32, np.int32, np.int64, np.iinfo(np.int32).max], axis=1)\n#\tarr_arr_np = arr_arr.astype(dtype)\n#\tarr_arr_np.shape = shape\n#\tarr_arr_np.dtype = dtype\n#\treturn arr_arr_np\n\n#\tdef create_from_"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n# Define the array\ndef arange(shape, dtype=np.int32, order=0):\n\treturn np.arange(shape, dtype=dtype, order=order)\n\ndef triu(shape, dtype=np.float32, order=0):\n\treturn np.tril(np.arange(shape, dtype=dtype, order=order), 0)\n\ndef triu2(shape, dtype=np.float32, order=0):\n\treturn np.tril(np.arange(shape, dtype=dtype, order=order), 0)\n# -*-"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\n\tarr = np.zeros(shape, dtype=dtype)\n\n\n\t#print arr.shape, arr.dtype\n\n\treturn arr\n\n\t\t\n\n\ndef get_data_array_size(shape):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\n    Parameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\n\tprint shape, dtype\n\t#print np.shape(shape)\n\n\treturn len(shape)\n\n\ndef get_data_array"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\t\n\n# Functions\n# ------------------------------------------------------------------------------\n\n\ndef numpy_to_string_type(str_type = ''):\n\t\"\"\"\n\tFunction to convert a numpy string type to string \n\t\"\"\"\n\t\n\tif str_type == '':\n\t\treturn ''\n\t\n\tarr = str_type.split(',')\n\tif len(arr) > 1:\n\t\tif arr[1] == '\"':\n\t\t\ttemp = arr[0].strip('\"')\n\t\t\treturn temp\n\t\tnum = 0\n\n\t\tif arr[0] == \"'/\":\n\t\t\ttemp = arr[1].strip('\"')\n\t\t\tif temp =="}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n    def __getitem__(self, key):\n        \"\"\"\n\t\tAccess this array.\n\t\t\"\"\"\n        return self[key]\n        \n    def __setitem__(self, key, value):\n        \"\"\"\n\t\tAccess this array.\n\t\t\"\"\"\n\t\tself[key] = value\n    def __getslice__(self, start, end):\n        \"\"\"\n\t\tReturn a sliced array.\n\t\t\"\"\"\n        return self[start:end]\n\n    def __setslice__(self, start, stop, stride):\n        \"\"\"\n\t\tSet a sliced array.\n\t\t\"\"\"\n        #print start\n        #print stop, stride\n        #print"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\n\n#----\n# Functions of common shapes\n\ndef arrayDimArray(shape=None, dtype=np.int32, name=None):\n\n\t\"\"\"\n\tUsed to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : type \n\t\tdtype of the array\n\tname : str\n\t    optional name of the array, if you don't set this parameter\n\n\tReturns\n\t-------\n\tdata : `numpy.dtype`\n\t    data-type of the array\n\t\"\"\"\n\t_dtype = []\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n#\tprint \"Creating array %s\" % (shape,)\n\t\n#\t\tarr = np.array(shape, dtype=dtype)\n\t\n#\t\tnp.savez('arr',arr) #to save array\n#\t\treturn arr\n\t\n\t\n\tdef __mul__(self, other):\n\t\t\"\"\"\n\t\tFunction to multiply an array with another array\n\n\t\tParameters\n\t\t----------\n\t\tother : list/array-like\n\t\t\n\t\t\tThe array to be multiplied by\n\t\t\n\t\tReturns\n\t\t-------\n\t\tarr :\n\t\t\n\t\t\tThe sub-array with the input values\n\n\t\tNotes\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\ndef create_ndarray(shape, dtype):\n    numpy.array(shape, dtype)\n\n\ndef create_array(shape, dtype):\n    return np.array(shape, dtype)\n\n\n# -*- coding: utf-8 -*-\n\n'''\nCreated on 2/05/2016\n\n@author: Jonathan Rei, Radu Shafael\n\n'''\n\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom copy import deepcopy\n\ndef normalize_data(array_data, window='hanning', cval=0.0"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n#\tparameters\n\tmin = 10e3 # min acceptable number of digits\n\tmax = 10e8 # max acceptable number of digits\n\n\n\t#the variables will be created at the same locations for each value and\n\t#it is initialized just like a numpy object\n\tvalue = np.zeros([shape[1],shape[0],1])\n\t#the following are the parameters for the creation\n\t\n\t#the following variables will only be given as input arguments \n\tmin_var = np.array([0,min])\n\tmax_var = np.array([max,max])"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef sigma(x, sigmaX):\n\t\"\"\"\n\t\tSigma function :\n\t\t\n\t\tReturns the function that calculates the Gaussian centered around 0.2.\n\t\tTheta function :\n\t\t\n\t\tReturns the function that calculates the Gaussian centered around 0.2\n\t\t\"\"\"\n\t\ndef gaussian_covariance(x, sigmaX = 0):\n\t\"\"\"\n\t\tGaussian covariances function :\n\t\t\n\t\tIn this version, sigmaX=x is the value of mu and sigmaX=z \n\t\tthus sigmaX=s and sigmaX=u are the covariances of the data to be used to calculate the likelihood"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n#\tplt.figure()\n#\tplt.subplot(111)\n#\tplt.plot(-np.mean(y), [-2.3409, -2.4311])\n#\n#\tplt.subplot(211)\t\n#\tplt.plot(-np.mean(x), [-2.4226, -2.5151])\n#\n#\tplt.subplot(212)\n#\tplt.plot(-np.mean(y), [-2.2525, -2.4612])\n#\n#\tplt.tight_layout()\n#\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\tc=random_state(np.random.randint(4,8)).astype(float)/np.sqrt(1./9.) #(np.sqrt(6.))/np.sqrt(5.)/np.sqrt(8.),)\n\t\t# Compute the distribution of the CMC to the location of the closest sigma\n\t\tx=c *np.exp(-2*1./(1. - np.exp(-1./(1. - np.exp(-1./(1. - np.exp(-1./(1. + \\\n\t                                            0.2) - np.exp"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\n    \n    \n\n# Create an empty figure to hold all of the data to be plotted on\nfig = plt.figure(figsize=[6.5, 15])\nfig.suptitle(u\"Gaussian Normal distribution\")\nplt.plot(x,np.exp(x),label='p(x)')\nplt.xlabel(r'$\\sigma$ ($\\sigma_1$')\nplt.ylabel(r'$\\"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\tf = open('gauss_centered.dat','a')\n\t\tx = np.arange(0,float(size)/xDim,xDim)\n\t\tx += np.random.standard_normal(0.,stddev=1).reshape((xDim,xDim))\n\t\tx[size/2:size/2+size/2] = x[size/2:size/2+size/2+size/2] + 0.001\n\t\tX = x*(xdim-2.)\n\t\tf.write('F:%.1f'%f.write(X) +"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\tdef __init__(self, sigma = 0.1):\n\t\t\tself.sigma = float(sigma)\n\t\tdef f(self):\n\t\t\t\treturn 0.1\n\tdef F(self):\n\t\tself.sigma = 0.1.\n\t\treturn 0.1*self.sigma\nclass Gaussian(object):\n\t\tdef __init__(self, sigma = 0.1, sigma2 = 10**2):\n\t\t\tself.sigma = float(sigma)\n\t\t\tself.sigma2 = float(sigma2)\n\t\tdef F(self):\n\t\t\t\treturn 0.1*self."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\n\tdef __init__(self, sigma = 0.6):\n\t\tself.sigma = sigma\n\t\tself._mean = np.zeros(self.sigma)\t\t\t\n\t\n\tdef _get_mean(self):\n\t\t\"\"\"Compute the mean of the gaussian.\"\"\"\n\t\tif self._mean.size == 0:\n\t\t\treturn np.zeros(self.sigma)\n\t\tif self._mean.size == 1:\n\t\t\tret = np.mean(self.var)\n\t\telif self._mean.size == 2:\n\t\t\tret = np.mean(np.diag(self.var))"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n#\t# gaussian with sig2 = 1\n#\t# this is useful to do sigma = 0\n#\tdef gaussian_pdf(self, sigma):\n#\t\tf1 = self.sig2 * np.exp(-sigma * self.sig2 * (sigma - 1.) * self.sig2 * (self.sig2 - 1.)) # 1\n#\t\tf2 = f1 + self.sig2 * np.exp(1 - sigma)\n#\t\tp = np.random.random()\n#\t\tprob = np.exp(-p * self.sig"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\tdef __init__(self, sigma=-0.1, offset=-np.inf, **kwargs)):\n\t\t\t\"\"\"\n\t\t\t\tConstruct a Gaussian centered around 0.2 with the specified offset and offset.\n\n\t\t@param sigma: The standard deviation of Gaussian.\n\t\t@type sigma: float\n\t\t\t@param offset: An offset to the sigma.\n\t\t@type offset: float | int\n\t\t\t@param offset: The offset to the sigma.\n\t\t@type mu: float\n\t\t\t@param mu: The mean.\n\t\t@type delta: float\n\t\t\t@param delta: The absolute difference between the"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\n\tdef gaussian_sample(self,n,sigma,n_samples,t=0):\n\t\t\"\"\"\n\t\tGenerates samples from a gaussian distribution of the data.\n\n\t\tParameters:\n\n\t\t\t\tn \t-- dimension of the array that contains samples\n\t\t\t\tsigma \t-- sigma of the data to be sampled\n\t\t\t\tn_samples -- number of samples to be sampled\n\t\t\t\tt   -- number of iterations to perform\n\t\tReturns:\n\n\t\t\t\tlist -- list of samples. Each element is a float.\n\t\t\"\"\"\n\t\t#print \"n     = \",n,\"    sigma = \",sigma,\"    n"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tlog = logging.getLogger(__name__ + \".parsing.csv_file_parser\")\n\t\tlog.setLevel(logging.WARN)\n\t\tcfg = logging.getLogger(\"parse\")\n\t\tcfg.setLevel(logging.DEBUG)\n\t\t\n\t\tparser = cfg.initLogger(\"/tmp/read_configuration\")\n\t\tparser.setLevel(logging.DEBUG)\n\t\tlogging.config.fileConfig(parser.name)\n\t\tlogs = []\n\t\ttry:\n\t\t\twith open(config, 'r') as f:\n\t\t\t\tfor l in f:\n\t\t\t\t\tlogs.append({\"log\":l, \"parse"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nif args!= None:\n\tif not os.path.exists(args.config) == False:\n\t\traise Exception(\"Arguments not found\")\n\t\t\n\tconfig = None\n\tlog = None\n\ttry:\n\t\tconfig = yaml.load_all(args.config)\n\texcept Exception as e:\n\t\traise Exception(\"Configuration YAML not valid: %s\" % str(e))\n\t\t\n\tfor config_filename in config:\n\t\tlogging.config.fileConfig(config_filename, disable_existing_loggers=True)\n\t\tlog.configure(config)\n\t\tlog.do"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\ndef check_configuration(filename):\n\t\"\"\"Parses the specified configuration file, checking consistency between\n\ta sequence of parsed configurations and outputting the results to a File\n\t\"\"\"\n\t_assert_enabled(filename, \"configuration files are not enabled\", True)\n\n\tconfig = defaultdict(lambda: [])\n\n\tfor line in file(filename):\n\t\ttry:\n\t\t\t# Parse this line.\n\t\t\tparsedConfiguration = json.loads(line)\n\t\texcept ValueError as error:\n\t\t\tlog_error(line, \"Invalid configuration file: \" + str(error))\n\t\t\tcontinue\n\n\t\t\t#"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\ndef isConfigurationValidFilename(cfgPath):\n\t\"\"\"\n\tGiven a config file, return True if the configuration matches the supplied configuration\n\t\"\"\"\n\t\n\tif cfgPath not in CFG_FILES:\n\t\tlog.log(\"No such files:\", cfgPath)\n\t\treturn False\n\tif not os.path.isfile(cfgPath):\n\t\tlog.log(\"Configuration file does not exist:\", cfgPath)\n\t\treturn False\n\t\n\tlog.log(\"Found the configuration file '{}' with the following fields: {}\".format( cfgPath, CFG_FILES[cfgPath].keys() ) )\n\tif"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\ndef validateFilename(filename):\n\t\"\"\"\n\tGiven a config filename, returns the file-object corresponding to the configuration file.\n\n\tThe file-object is either:\n\t - a list of config files\n\t - None\n\t\"\"\"\n\t\t\n\t\n\tif not filename:\n\t\traise Exception(\"Please provide a config filename to parse.\")\n\t\t\n\tlines = filename.split(\"\\n\")\n\t\n\tconfig = dict()\n\tcfgLine = ''\n\tfor line in lines:\n\t\tsplitLine = line.split()\n\t\tif len(splitLine)!= 4:\n\t\t\terror(\"Invalid file config line:"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nlogging.basicConfig(\n\tlevel=logging.DEBUG,\n\tformat='%(asctime)s %(levelname)s:%(name)s: %(message)s',\n\tdatefmt='%Y-%m-%d %H:%M:%S'\n)\n\t\ndef main():\n\t\"\"\" \n\tParse command line arguments and invoke the appropriate application.\n\t\"\"\"\n\n\tparser = argparse.ArgumentParser(description='Parse configuration files. This is like the normal parser in command line mode.')\n\t\n\tparser.add_argument('-u',\n\t\t'--url',\n\t\thelp='Download configuration from the given URL (default"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\ndef validate_config(filename, parse_errors):\n\t\"\"\"\n\tValidate a configuration at a particular log file.\n\t\"\"\"\n\tfilename_parser = logging.getLogger( __name__ )\n\tfilename_parser.setLevel(logging.WARNING )\n\ttry:\n\t\tcfg = parse_configs( filename )\n\texcept ValueError:\n\t\tlogger.critical( \"Error while parsing %s\" % filename )\n\t\treturn None\n\tfor handler in cfg:\n\t\tlogger.info( \"Parsing %s with errors %s\" % (filename, handler) )\n\t\thandler_factory"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tlogging.debug(\"Reading configuration files...\")\n\t\twith open(CONFIG_FOLDER) as f:\n\t\t\tconfigs = json.load(f)\n\n\t\tfilenames = configs[\"filenames\"]\n\t\tfor filename in filenames:\n\t\t\t# Check to see if the filename in configs is in a sequence of configuration filenames\n\t\t\tsequence = filename.split(\".\")\n\t\t\tif len(sequence) < 2:\n\t\t\t\tlogging.fatal(\"Invalid filename: %s\", filename)\n\t\t\t\tcontinue\n\n\t\t\t# Attempt to work out the sequence of configuration files\n\t\t\tconfiguration_file = os.path.join(configs[\"working_dir"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\t# TODO: implement test case\n\t\ttry:\n\t\t\tself.create_config()\n\t\t\tlog = logging.getLogger(__name__)\n\t\t\thandler = logging.handlers.RotatingFileHandler(config_path, maxBytes=65536, backupCount=5)\n\t\t\thandler.setLevel(logging.DEBUG)\n\t\t\thandler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s]: %(message)s'))\n\t\t\tlog.addHandler(handler)\n\t\t\tlog.info('Initialized configuration successfully')\n\t\t\treturn handler, log\n\t\texcept Exception as error:\n\t\t\tlog.error('"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\n\n\tlogging.getLogger().setLevel(logging.INFO)\n\tconfig_lines = open(config_fn_str, \"rU\").readlines()\n\n\tresult = {}\n\tfor key, value in config_lines:\n\n\t\t# Validate each sequence of configs in the file.\n\t\t# For each element in the sequence, the corresponding entry in the config file will\n\t\t# have an entry for validation.\n\t\tvalid = False\n\t\tinvalid = False\n\t\tfor config_line in value.split('\"'):\n\t\t\tif (not valid) or (config_line.startswith(\"{\") and config_line"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef string_byte(obj, encoding='utf-8'):\n\t# convert obj to string\n\tencoding = encoding.lower()\n\tobj = obj.decode(encoding, 'ignore')\n\treturn obj\n\ndef string_int(obj):\n\t# convert obj to int\n\tencoding = 'UTF-8'\n\tobj = str(obj).decode(encoding, 'ignore')\n\treturn obj\n\ndef string_double(obj):\n\t# convert obj to double\n\tencoding = 'UTF-8'\n\tobj = float(obj)\n\treturn obj\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n### In[22]:\n\n\ndef readIn(inObj):\n\t\"\"\"\n    Read an object by byte stream in the current directory\n\t(in a StringIO object)\n\t\"\"\"\n### In[23]:\n\t\n\treturn numpy.loadtxt(inObj, dtype=[('x',numpy.uint8), ('y',numpy.int64)])\n### In[24]:\n\t\n\ndef gen2array(x):\n\t\"\"\"\n    Gena ndarray object from x.\n\t\"\"\"\n\tf = open('gen2array.txt','w')\n\tprint >> f, x.tobytes(),\n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\t\n\tobjval = 'utf8' if hasattr(obj, 'decode') else obj.encode\n\t\n\t\n\treturn objval\n\t\n\nfrom cStringIO import StringIO\n\nfrom wyrd.core.logger import logger\nlogger.info(\"Trying a UTF-8 encoded string\")\n\ndef is_type_utf8(string):\n\tif not isinstance(string, str):\n\t\treturn False\n\tif not is_str:\n\t\treturn False\n\t\t\n\t# Check for Unicode characters.\n\tif unicodedata.category(string) == \"NFC\":\n\t\treturn False\n\t\t\n\t# Decode.\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef c2str(c2str_string):\n    \"\"\"\n        This function converts a string given a C2 string object to a UTF-8 string\n        suitable for use in py2.\n    \"\"\"\n       \n        if c2str_string[0] in string_to_unicode(c2str_string):\n                print('This function passed %s to python because it contains \\\n                  non-English characters' % c2str_string)\n                return c2str_string \n        else:\n                print('String argument (%s) passed to python with \\\n                  non-English characters (%s) passed to python,"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\tobj = str()\n\t\ttry :\n\t\t\tobj = str(obj)\n\t\t\tobj = str(\"'\")\n\t\texcept :\n\t\t\tpass\n\t\treturn obj\n\t\t\n\tdef __format_result_to_readable(self, result) :\n\t\tif numpy.platform[:10] == \"win\" and sys.platform[:5] == \"cygwin\":\n\t\t\treturn result\n\t\treturn \"[{0}] {1}\".format(result, self.__format_value(result))\n\n \n\t# -*- coding: utf-8 -*-\n# \u00a9 2016 Jordi Esteve.\n# License"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\nif __name__ == '__main__':\n\t\"\"\"\n\tThis example requires some libraries that can be imported and used by the program.\n\t\"\"\"\n\timport re, struct\n\timport sys\n\tif sys.version_info[0] > 2:\n\t\tprint('PyMOL - Python 2.6 or later is required.')\n\t\tsys.exit()\n\n\tobjname \t= sys.argv[1]\n    rstring \t= sys.argv[2]\n    if len(sys.argv) > 3:\t# or objname == 'name':\n\t\tif r"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\tif obj == None and type(obj) == type(\"\"):\n\t\traise TypeError('Must enter null obj as a string!')\n\tif hasattr(obj, 'decode'):\n\t\treturn obj \n\n\tif type(obj) == type(None):\n\t\n\t\treturn None\n\n\tif type(obj) == type([]):\n\t\n\t\treturn obj \n\ndef __read(objFile):\n\t\n\tif objFile == 'root':\n\t\treturn 0\n\n\tif type(obj) == type(''):\n\t\n\t\treturn base64.decodestring(obj)\n\telse:\n\t\traise Exception(\"Don"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\treturn obj\n\ndef load_obj(path):\n\t\"\"\"\n\tThis function returns a python dictionary object corresponding to ``obj`` \n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\n\n\tif 'encoding' in path:\n\t\treturn path['encoding']\n\n\tif 'filename' in path:\n\t\tfor chunk in path['filename'].decode('utf-8').split('\\n'):\n\t\t\tyield chunk\n\n\nclass DicObj :\n\tdef __init__(self, obj, parent):\n\t\t#print \"DEBUG: load the \", obj, \" object"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef byte2str(b, verbose=False):\n\t\"\"\"\n\tIt returns the str object corresponding to ``obj`` in case it is a string using ASCII\n\t\"\"\"\n\t\ndef byte2obj(b, encoding='utf-8'):\n\t\"\"\"\n\tIt returns the object corresponding to ``obj`` in case it is a string using UTF-8,\n\twhen encoding is set to 'utf-8' and returns the object.\n\t\"\"\"\n\t\nif __name__==\"__main__\":\n\t\n\tif len(sys.argv) < 2:\n\t\tprint \"Usage: python convert"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\ndef decode_list(obj):\n\t\"\"\"\n\tThis function returns the first encoding in the list of ``obj`` in case it is a ``list`` type, returns ``None`` if not exists\n\t\"\"\"\n\t\ndef make_unicode(strings):\n\t\"\"\"\n\tThis function creates the unicode object given ``strings`` in case it is a ``list`` type, \n\tconverts them into unicode using ASCII encoding. \n\t\"\"\"\n\t\n## Start program...\n\t\n\tobj = ''\n\t\n\tfor s in strings:\n\t\tif is_py2:\n\t\t\tobj += s\n\t\telse:"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tticks = int(ticks)\n\texcept TypeError:\n\t\traise TypeError(\"ticks must be an integer\")\n\t\n\telif tz is None:\n\t\ttz = mktz(ticks)\n\t\n\treturn cls(ticks, tz)\n\ndef from_timedelta(days):\n\t\"\"\"\n\tCreate a timedelta from a timedelta (in hours).\n\t\n\t:param days: a timedelta\n\t:type days: timedelta\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if days is out of bounds\n\t\n\t:return: Time with days converted to given time\n\t\n\t\"\"\"\n\ttry"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n##\tTICK IS A NUM, BUT I HAVE TO BE USED IN THE DASI TO MATCH RADIO...\n##\tif tz:\n##\t\t\treturn Time(timezone=(tz if tz is not None else -127),\n##\t\t\t   *ticks)\n\t\n\t#:\n\t#  Use only the first tick within the time interval\n\t#  of the time series, if there is one\n\n\n\tif not ticks:\n\t\treturn Time(hour=ticks[3], minute=ticks[4],\n\t\t\t\t\tsecond=ticks[5] * 1E9 * 1E9"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tif tz is None:\n\t\tts = Time.now() + 0.001\n\t\ttz = pytz.timezone('Europe/Kiev')\n\t\n\treturn cls.from_td(ticks, ts, tz)\n\n\n@register(str)\nclass TimeDelta(TimeDelta):\n\t\n\tdef __init__(self, **kwargs):\n\t\tfrom dateutil.relativedelta import relativedelta\n\t\t\n\t\tself.offset = kwargs.get('offset', timedelta(0)), kwargs.get('factor')\n\t\t\n\t\tself.unit = 'd'\n\t\t\n\tdef __repr__(self):\n\t\tunit = self.unit"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\t# convert to seconds \n\ttick = ticks // 3600\n\ttmin = tick % 3600\n\ttsec  = tick / 60\n\n\t# convert minutes\n\tts  = ts - tmin\n\tts = ts // 60 % 60\n\t\n\tif tz is None:\n\t\ttz = make_aware(tsec * 86400)\n\n\tif ts < tick:\n\t\traise ValueError(\"Ticks out of bounds: \" + str(ticks))\n\n\tz = zonecode(tz, ts)\n\n\treturn Time(ts, z)\n\n\n# -----------------------------------------------------------------------\n\nclass UTCTime(object"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\t# ensure the ticks are in the format of seconds\n\tif tz is None:\n\t\ttz = datetime.time(0, 0)\n\t\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be nonnegative integer, got %s\" % ticks)\n\t\n\thours = ticks * 3600 * 24\n\tseconds = (\n\t\tseconds + 12 * 3600 * (ticks + 1) - 1\n\t) / 2\n\t\n\treturn Time(hours, seconds)\n\n\nclass Time(object):\n\tdef __init__(self, hours, seconds):\n\t\tself.seconds = seconds\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\t\n\tif isinstance(ticks, basestring):\n\t\tticks = Time(ticks)\n\t\t\n\tif tz is not None and tz.utcoffset(ticks) is None:\n\t\traise ValueError(\"Invalid value for tz.utcoffset.\")\n\t\t\n\treturn Time([\n\t\tdatetime(*([x.utctimetuple() for x in ticks.utctimetuple()]))\n\t\t\n\t])\n\t\n\n# from_seconds_long(from, lons)\ndef from_seconds_long(secs, from_=datetime(1970, 1, 1, 0, 0), lons=0):\n\t\"\"\"\n\tCreate a timestamp with given seconds of"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\treturn Time(ticks)\n\texcept ValueError:\n\t\traise ValueError(\"ticks invalid\")\n#!/usr/bin/env python2\n\n\"\"\"\n  Copyright (c) Katharine Penner 2015 - 2013-2013 Ivan Robertson\n\n  This code is released under the MIT license. Please see the file LICENSE.\n  The full license is in the distribution file LICENCE that it's available. You can\nget at http://opensource.org/licenses/MIT\n\n  If you make any attempt to fix this error, contact\n    Martin Martin. This can be made public"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\tif not tz:\n\t\t\ttz = datetime.timezone.utc\n\t\tticks -= 86400\n\t\ttime = datetime.datetime.now(tz=tz)\n\t\treturn Time(time.year, time.month, time.day, 0, ticks)\n\t\n\tdef to_seconds(self, tz=None):\n\t\"\"\"\n\tConvert seconds to seconds since midnight.\n\t\"\"\"\n\t\tif tz:\n\t\t\ttz = tz.astimezone(self)\n\t\tdelta = datetime.timedelta(0,0,0,pytz.timezone(tz))\n\t\tseconds = self - delta\n\t\tseconds"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tassert isinstance(ticks, int), 'ticks should be an integer'\n\tassert tz is None or tz == _DefaultTimezone(tz), 'tz must be a tzinfo'\n\tassert ticks > 0\n\tassert ticks <= 6144000\n\t\n\tif isinstance(ticks, int):\n\t\tticks = convert_ticks(ticks, _Minute)\n\telse:\n\t\tassert isinstance(ticks, (int, Decimal)), 'ticks must be an integer or Decimal'\n\t\n\tif ticks!= 1:\n\t\tif not isinstance(ticks, str):\n\t\t\tticks  = str(ticks)\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t\n\tif tz == 'UTC':\n\t\ttz = datetime.now().tzinfo\n\n\treturn Time(ticks, tz)\n\n\nclass TimeRange(timedelta):\n\t\n\t'''\n\tRepresents the range of time series. These may point to time series in 1-indexed order.\n\t'''\n\t\n\tdef __init__(self, start, end, step):\n\t\ttimedelta.__init__(self)\n\t\tself.start = start\n\t\tself.end = end\n\t\tself.step = step\n\t\t\n\tdef utcoffset(self, dt):\n\t\treturn self.start - self.step\n\t\n\tdef dst"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\thandler_info = {}\n\t\n\tif protocol_version: # validate protocol version\n\t\tif protocol_version not in cls.SUPPORTED_PANDA_VERSION_SEQUENCES:\n\t\t\traise TypeError(\n\t\t\t\t\"Unsupported version of Bolt protocol \" + str(protocol_version) +\n\t\t\t\t\". \"\n\t\t\t\t\"Supported versions are: \" + str(range_to_tuple(cls.SUPPORTED_PANDA_VERSION_SEQUENCES.keys()))\n\t\t\t)\n\t\thandler_info[\"Protocol\"] = protocol_version\n\t\n\t# validate protocol information for supported protocols\n\tsupported_protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.DEFAULT_VERSION\n\n        protocols = {}\n\t\tif protocol_version is not None and hasattr(cls, 'SUPPORTED_PROTOCOLS'):\n\t\t\tprotocols = cls.SUPPORTED_PROTOCOLS\n\t\treturn protocols\n\n\tdef get_handler(self, version):\n\t\tprotocols = self.get_protocols()\n\t\tif version not in protocols:\n\t\t\traise ValueError('invalid protocol version: \"%s\"' %\n\t\t\tutil.encode_version_tuple(version))\n\t\treturn protocols[version]\n\n\tdef set_protocol(self, protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\t#if protocol_version is None:\n\t\t\t\n\t\tif protocol_version is None:\n\t\t\t\n\t\t\t\tprotocol_version = cls.PROTOCOL_VERSION\n\t\n\t\tmajor, minor = protocol_version\n\n\t\tif major < 2 or minor < 2:\n\t\t\t\traise TypeError(\"Protocol version in [1, 2]) is not \" \\\n\t\t\t\t                    \"supported\"\n\t\t\t\n\t\t# Get the protocol specification for this version\n\t\t\n\t\tprotocol_info = cls.PROTO_MAP[protocol_version]\n\t\t\n\t\thandler = protocol_info.handler\n\t\t\t\n\t\n\t\tif handler and hasattr(handler, \"version\") and \\\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tif not issubclass(protocol_version, (tuple, list)):\n\t\traise TypeError(\"Protocol version must be a tuple for BoltProtocolHandler\")\n\t\n\t# this dictionary will hold the bolt versions\n\thandler_dict = {}\n\t\n\tfor version in protocol_version:\n\t\thandler_class = cls.__new__(cls, version)\n\t\thandler_dict[version] = handler_class\n\t\n\thandler_list = []\n\tfor handler in cls.__mro__:\n\t\tif issubclass(handler, protocol_handlers):\n\t\t\thandler_list.append(handler)\n\t\telse:\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\treturndict={}\n\t\n\tif protocol_version and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version parameter must be in tuple\")\n\n\tif protocol_version is None or protocol_version < (3,):\n\t\traise TypeError(\"Version tuple needs to be of length 3\")\n\n\tfor handler_cls, cls_name in protocol_handlers.iteritems():\n\t\tif handler_cls is None:\n\t\t\tcontinue\n\t\tif not issubclass(handler_cls, cls):\n\t\t\traise TypeError(\n\t\t\t\t\"\"\"The handler class passed must be an %r\"\"\" % \n\t\t\t\t[cls.__"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tcls._supported_version_codes = [] # a list of allowed supported versions for the current version range\n\t\n\tif protocol_version is None:\n\t\tversion_number = get_protocol_version(\"1\")\n\t\tif not version_number:\n\t\t\traise TypeError(\"Missing protocol version: none\")\n\t\t\n\t\tclientport = None\n\t\ttry:\n\t\t\tclientport = int(version_number)\n\t\texcept ValueError:\n\t\t\t# this doesn't seem to be a problem -- we want to try to\n\t\t\t# find the current version (3 is most likely)\n\t\t\tfrom tango_client"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is not None:\n\t\t\treturn cls._protocol_handlers[protocol_version]\n\t\telse:\n\t\t\t# Try all possible ones with the exact highest\n\t\t\t# priority. If this fails, fall back to defaults for\n\t\t\t# most platforms. If no specific version is set, fall back to the\n\t\t\t# default based on the most supported protocols.\n\t\t\tfor _p in protocols:\n\t\t\t\ttry:\n\t\t\t\t\tif (_p[0].lower() == _p[1].lower() and \n\t\t\t\tany((1, 2, 5) <= _p[2] and _"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t\n\tdef handler_factory(self, protocol_version, connection_class,\n\t\t\t\t\t\t\t\t        protocol, connection_parameters):\n\t\tif protocol_version is None:\n\t\t\tif len(protocol_version) == 2:\n\t\t\t\tprotocol_version  = protocol_version[0]\n\t\t\telif len(\n                    protocol_version) == 1:\n\t\t\t\tprotocol_version = protocol_version[0]\n\t\t\telif len(\n                    protocol_version) == 2:\n\t\t\t\tprotocol_version = protocol_version.split(\".\")\n\t\t\t\tif int(protocol_version[-1]) < 0:\n\t\t\t\t\traise"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.VERSION\n\t\n\t\t# We sort the versions according to protocol version\n\t\t# if version is None then the first handler will be called\n\t\t# first.\n\t\treverse_protocols = sorted(cls.handler_info.keys(),\n\t\t\t                              key=lambda s: s[3], reverse=True)\n\t\ttry:\n\t\t\treverse_protocols.reverse()\n\t\t\tprotocol_version = reverse_protocols[-1][3]\n\t\texcept IndexError:\n\t\t\tpass # Invalid protocol version\n\t\t\n\t\tprotocol = cls(protocol_version)\n\t\t\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\tif protocol_version is None:\n\t\t\tprotocol_version = cls.get_protocol_version()\n\t\t\tif protocol_version < (1, 5):\n\t\t\t\traise TypeError, (\n\t\t\t\t\t'cannot determine a supported protocol version '\n\t\t\t\t\t'for this version; got version %r'\n\t\t\t\t\t% (protocol_version,)\n\t\t\t\t\t)\n\t\treturn dict((protocol_version.__class__.__name__,\n\t\t\t\tcls) for cls in (\n\t\t\t\tprotocol_version,\n\t\t\t\tcls) if not issubclass(protocol_version, cls))\n\n\nclass BoltStatus(object):\n\t\"\"\"\n\t"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tfor v in values:\n\t\t\ttry:\n\t\t\t\tb = book()\n\t\t\t\tb.name=name = v\n\t\t\t\tif name is None:\n\t\t\t\t\traise ValueError(\"Bookmarks must have a name.\")\n\t\t\t\telse:\n\t\t\t\t\tb.id = id = b.name\n\t\t\t\tb.title=title = v # title is sometimes only lowercase\n\t\t\t\tb.author=\"University of Brain\"\n\t\t\t\tb.authors=v # authors are sometimes only uppercase\n\t\t\t\tb.isbn=int(v)\n\t\t\t\tif b.isbn >= 4000:\n\t\t\t\t\tb.cover = \"/"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tif not values:\n\t\treturn None\n\treturn cls([Bookmark.from_raw_values(x) for x in values])\n\nclass Bookmark(object):\n\tfrom_raw_values = lambda value: from_raw_values(value)\n\tfrom_raw_values_from = lambda value: from_raw_values_from(value)\n\t\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself._bookmarks = [Bookmark(x) for x in self.from_raw_values()]\n\t\n\tdef __iter__(self"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tfor value in values:\n\t\traw_item = string_to_item(value)\n\t\tif raw_item.startswith('@'):\n\t\t\traw_item = raw_item[1:]\n\t\telse:\n\t\t\traw_item = None\n\t\t\traw_item = raw_item.strip()\n\t\t\n\t\tif raw_item and not raw_item.startswith('#'):\n\t\t\traise InvalidDataFormatError(message='Bookmark {!r} is not formatted as a \\'{!r}\\''.format(value, raw_item))\n\n\t\ttry:\n\t\t\tbook_data = Bookmark(**"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\tbmarks = Bookmarks()\n\t\tfor bookmark in values:\n\t\t\tbmarks.add(None, bookmark)\n\t\treturn bmarks\n\t\n\tdef to_raw_values(self):\n\t\treturn ''.join(self.values)\n\t\n\tdef add_to_raw_values(self, bmarks):\n\t\"\"\"\n\tAdd the bookmark string values.\n\t\n\t:param bmarks: Bookmarks object to add\n\t:type bmarks: Bookmarks\n\t\"\"\"\n\t\tif self.value!= bmarks.value:\n\t\t\tbmarks.append(None, bmarks.value)"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tdef _get(self, key, default=None):\n\t\tif not self.is_loaded:\n\t\t\tself.load_from_db()\n\t\t\n\t\treturn self.get_by_key(key) if key in self._bookmarks else default\n\t\n\tdef _set_value(self, key, value):\n\t\tself.set_value(key, value)\n\t\t\n\tdef _get_value(self, key, default=None):\n\t\tif not self.is_loaded:\n\t\t\tself.load_from_db()\n\t\t\n\t\treturn self.get_by_key("}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\nclass Bookmarks(object):\n\t\"\"\"\n\tA Python 2-level dict-like object which allows you to construct bookmarks\n\tthat can be easily passed to :meth:`dict.Bookmarks.raw` or\n\t:meth:`Bookmarks.from_raw`.\n\t\n\t:param bookmark_values: a string of JSON encoding valid bookmarks\n\t:type bookmark_values: str or unicode\n\t\"\"\"\n\t\n\t\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreates a Bookmarks object.\n\t\t\n\t\t:raises ValueError: if the number of entries in the dict\n\t\t\talready exists.\n\t\t\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tentries = []\n\n\tfor v in values:\n\t\tentries.append('\\t\\t<bookmark id=\"{}\" title=\"{}\" bookmark=\"{}\" id=\"{}\"/>\\n'.format(v, v, v, v))\n\n\n\treturn Bookmarks(entries)\n\n\nimport sys\nimport re\nimport os\nimport codecs\nfrom calibre.constants import iswindows, u''\nfrom calibre.utils.icu import sort_key\nfrom calibre import prints, safe_unicode, filesystem_encoding, uencode\nfrom calibre.ebooks.oeb.base import OPFMetadata, OPF"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\n\tfields = dict(zip(cls.ID_FIELDS, values))\n\n\ttry:\n\t\tmodel = cls.from_raw_values(values)\n\texcept ValueError as e:\n\t\traise ValueError(unicode(e))\n\n\treturn modelfrom django import template\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.db import connection\nfrom django.http import Http404\nfrom django.template import TemplateSyntaxError\nfrom django.utils.html import include, conditional_escape\n\nregister = template.Library()\n\ndef _escape_param(value):"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t\n\tdef process(self, data):\n\t\t# Ensure that all required fields are present\n\t\tfor bookmark in values:\n\t\t\tif bookmark not in self.bookmarks:\n\t\t\t\ttry:\n\t\t\t\t\tself.bookmarks[bookmark] = self.objects.create(\n\t\t\t\t\t\ttitle=bookmark,\n\t\t\t\t\t\tauthors=self.categories,\n\t\t\t\t\t)\n\t\t\t\t\tprint(\"Bookmark saved as %s\" % bookmark)\n\t\t\t\texcept:\n\t\t\t\t\tprint(\"Bookmark invalid\")\n\t\n\tdef create(self):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object.\n\t\t\n\t\t:returns: A Bookmarks object\n\t\t:"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\tvalues = [x.strip() for x in values.split(',')]\n\t\tif len(values)!= len(values):\n\t\t\traise ValueError('\"%s\" must have the same length as bookmarks'% values)\n\n\t\tvalues = {k:Bookmark.from_raw_value(v) for k, v in enumerate(values)}\n\t\treturn cls(values=values)\n\n\t@classmethod\n\tdef from_raw_values_list(cls, values):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\t\n\t\tYou should not need to use this"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tbuf = io.BytesIO()\n\ttry:\n\t\tseq = yield from io.BufferedReader(buf, bufsize=bufsize)\n\t\tif not isinstance(seq, Iterator):\n\t\t\tseq = (seq,)\n\texcept TypeError:\n\t\tseq = None\n\ttry:\n\t\tseq = next(iter(seq))\n\texcept StopIteration:\n\t\tseq = None\n\treturn seq, iter(seq)\n\t\ndef _is_mutable_type(type):\n\t\"\"\"\n\tReturn True if *type* is mutable for read only (and not for write). \n\tType is a"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n#         if isinstance(seq, (list, tuple)):\n#             t = _get_seq(seq, bufsize)\n#             try:\n#                 seq = seq[0]\n#             except IndexError:\n#                 # this might throw a RuntimeError if more than one sequence\n#                 # appears in the sequence\n#                 pass\n#         else:\n#             seq = Sequence(seq, bufsize)\n\n\ndef _get_str(seq):\n\tif not seq:\n\t\treturn \"\"\n\tif not isinstance(seq, str):\n\t    seq = str(seq)\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = _DEFAULT_BUFSIZE\n\t\tbuflen = bufsize\n\t\tif isinstance(seq, str):\n\t\t\tseq = seq.split(b'\\x00', 1)[0]\n\t\telif isinstance(seq, bytes):\n\t\t\tseq = seq.decode('utf-8')\n\t\telse:\n\t\t\traise TypeError('seq argument must be a str or bytes sequence.')\n\t\ttry:\n\t\t\tlen(seq)\n\t\texcept TypeError as exc:\n\t\t\traise ValueError('seq should have a str, a bytes sequence, got {!r}'.format(exc))\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\treturn (_get_sequence(seq, bufsize=bufsize) \\\n\t\t\tor (_get_list(seq), bufsize))\n\n\ndef _get_type_with_sequence(type, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is a sequence type).\n\t\"\"\"\n\t\treturn (_get_sequence(seq, bufsize=bufsize) \\\n\t\t\tor (_get_list(seq), bufsize))\n\n\ndef _get_sequence(seq, bufsize=None):\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tres = []\n\t\tpos = 0\n\t\tfor record in seq.seq:\n\t\t\tpos += 1\n\t\t\ttype = type_from_record(record)\n\t\t\tseq = record[pos:pos + len(seq), pos:]\n\t\t\tif type & IN_SEQUENCE:\n\t\t\t\tseq_type = type & IN_SEQUENCE\n\t\t\telse:\n\t\t\t\tseq_type = type & ~IN_SEQUENCE\n\t\t\tres.append((seq, type & ~seq))\n\t\treturn res\n\ndef get_sequence(seq, type=0):\n\t\"\"\"Return a list of sequence types.\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = 8192\n\t\tassert type(seq) is type(type(str_seq)), \"need seq or type\"\n\t\tassert bufsize >= 0, 'buffer size too big'\n\t\tyield (seq, type(seq))\n\ndef _get_seq_without_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = 8192"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif type(seq) in (list, tuple):\n\t\t\tseq = tuple(seq)\n\t\tseq = _n_common_type(seq)\n\tfor seq_type in _type_map:\n\t\tif seq_type in seq.dtype.names:\n\t\t\treturn(seq_type, seq.names.index(seq_type))\n\t\telse:\n\t\t\traise TypeError(\"Expected sequence data-type\", seq_type)\n\n\ndef _type_conv_type(types, num_types):\n\t\"\"\"\n\tConvert array type to array type for use by ``numpy``."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\n\t# Determine the type\n\t# Note: In Python 2.x, the seq is not an even iterable.\n\tif bufsize:\n\t\tbufsize = int(bufsize)\n\telif seq:\n\t\tbufsize = 2\n\t\tif isinstance(seq[-1], int):\n\t\t\tbufsize += 1\n\n\t# Determine the type again\n\ttype = type(seq)\n\n\n\t# Return the data\n\treturn (None, type)\n\n\ndef _get_seq_for_type(seq, bufsize=None):\n\t\"\"\"\n\tGet a (sequence, type) pair.\n\tSequence"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\tif bufsize is None:\n\t\t\treturn seq, ()\n\t\treturn _get_seq_with_type(seq, bufsize)\n\ndef _get_seq_no_type(seq):\n\t\"\"\"\n\tReturn an empty sequence.\n\t\"\"\"\n\t\treturn _get_seq(seq, ())\n\ndef _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type, but *bufsize* ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tassert seq is not None, \"need a sequence or a non-sequence\"\n\t\n\treturn None, dict(*((a[0],a[1]) for a in seq if a), bufsize)\n\ndef _sequence_with_type(sequence, bufsize=None):\n\tif sequence not in sequence_types:\n\t\treturn seq, None\n\treturn sequence_types[sequence], sequence_types[sequence][0]\n\ndef _sequence(seq, bufsize=None):\n\ttry:\n\t\tseq = tuple(seq)\n\texcept:\n\t\tpass\n\telse:\n\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\t# make sure our input is an instance of LHistogram\n\tif isinstance(other, _Histogram):\n\t\treturn self._scale(other)\n\telif isinstance(other, numbers.Number) or \\\n\t\t   isinstance(other, numbers.Float) or \\\n\t\t   isinstance(other, numbers.Integral):\n\t\trescale = self._scale(other)\n\t\tif not any( (isinstance(x, numbers.Integral) and isinstance(y, numbers.Integral) for y in other) or \\\n\t\t   (isinstance(y, numbers.Integral) and isinstance(x, numbers."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tif other is not None:\n\t\tself.update(other)\n\t\n\tif recompute:\n\t\tif not self._recompute:\n\t\t\tself.compute()\n\nclass Line:\n\tdef __init__(self, x=1, y=0, w=None, h=0, fill=False, fillvec=None, stroke=0, strokevec=None, strokewidth=None):\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.w = w\n\t\tself.h = h\n\t\tself.fill = fill\n\t\tself."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tself.__calc(other=other, recompute=recompute)\n\treturn self\n\t\ndef count(self, n=None):\n\t\"\"\"\n\tCompute the histogram length (sum of all sub-arrays of n)\n\tof *self*.\n\t\n\tThe *n* parameter defines the total number of values in *self*. \n\t\n\tIf *n* is given, *count* uses that many instead of 1.\n\t\"\"\"\n\t\n\t\n\treturn sum(self.data.n(axis=0) for axis in range(n))\n\ndef where(self, condition,"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t return self.compute(other=other, recompute=recompute)\n\t\n\tdef set(self, x, y):\n\t\"\"\"\n\tSet the range of *x* and *y* of the histogram in *x* and *y*.\n\t\n\tThis function returns a new copy of the histogram. If *x* and *y* are\n\tboth given, the range returned can be modified or inserted to\n\tthe end of the existing range; they are then sorted in ascending order.\n\t\n\tUsage example::\n\n\t\tdef add(arr, pos, y):\n\t\t\ta = arr + 1"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tif other is None:\n\t\tother = self._other\n\t\n\ttry:\n\t\treturn self._to_int(self._other * other, copy=False)\n\texcept ValueError:\n\t\traise lena.LenaValueError('the scale of %s must equal its '\n\t\t\t\t\t'integral value' % self.__class__.__name__.lower())\n\t\n\tif self._to_integral:\n\t\treturn self._to_integral\n\tif self._other:\n\t\tself.scale = other\n\t\treturn self\n\t\n\tif recompute:\n\t\tself._to_integral = None\n\t\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tif isinstance(other, (int,long)):\n\t\tself.scale = (lambda x: x / float(other or 0.0))\n\t\tself.rescale = lambda x, new: (x / float(other) * float(new))\n\t\tif other:\n\t\t\t#rescale() raises ValueError if scale and scale changed\n\t\t\t#if recompute:\n\t\t\t\t#self.scale(other, recompute=True)\n\t\t\tif type(self) in types.StringTypes:\n\t\t\t\tself.scale = recompute=False\n\t\t\telif type(other) in types."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\t\n\tif self.__size == other.__size:\n\t\t# scale is already computed, do nothing\n\t\treturn\n\t\n\tif 0!= other:\n\t\tif recompute:\n\t\t\tself._scale += other\n\t\telse:\n\t\t\tself._scale += other\n\t\t\treturn\n\t\n\tif self.__size == other.__size:\n\t\t# nothing to do\n\t\treturn\n\n\tif other == 0:\n\t\treturn\n\t\n\ttry:\n\t\tself._scale = self._scale / (self.__size - other)\n\texcept ZeroDivisionError:\n\t\tself._scale = -self._scale\n\t\t\n\t\n\treturn"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tfrom.. import _lena\n\t\tself.scale(self.x + other * (len(self.histogram)))\n\t\n\tif recompute:\n\t\treturn 0\n\t\n\tscale = self._getscale()\n\tif other:\n\t\tx, y = other\n\t\tself.scale(x + y)\n\t\n\treturn scale\n\nclass ExactHistogram(BaseHistogram):\n\t\"\"\"\n\tRepresents a histogram of exponentially distributed\n\tvalues.\n\t\n\tHistograms with a value of zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\tif other is not None:\n\t\t\tif other < 0:\n\t\t\t\traise lena.LenaValueError( \"bad value '%f'\" % other )\n\t\t\tif other!= 0:\n\t\t\t\t# check for new dimensions\n\t\t\t\tif self.dimensions[ self.dimensions.index( other ) ] <> 1:\n\t\t\t\t\traise lena.LenaValueError( \"dimensions for scale \"\n\t\t\t\t\t\t\"cannot change, but %d!= 1\" % ( self.dimensions.index( other ) ) )\n\t\t\t\telse:\n\t\t\t\t\traise lena.LenaValueError( \"new scales were set to 0.\" )"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n# TODO: add check for different cases\n# TODO: verify whether we don't have a proper version of one\n# TODO: implement recompute\n# TODO: implement scale (and scale)\n\n# TODO: do it with a proper version of one, but also do the following:\n#def scale_for_uniform(uniform, scale=1.0):\n#\t\"\"\"\n#\tCompute or set a scale for the specified uniform value.\n\n#\t:param uniform: (double or sequence) the uniform value to scale\n#\t:return: the value of the given scale\n#"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\ndef findParentCoords(self):\n\t\"\"\"\n\tGet a list of parent coords - ordered from coordinate to parent\n\tcoordinate (top of list), given the current graph as ``self``.\n\n\tRaises :exc:`~.LenaValueError`\n\tIf *None* is returned.\n\n\t\"\"\"\n\tif not self.parent.parent is None:\n\t\treturn self.parent.parent.coords\n\treturn [self.parent.coords[0]]\n\ndef getChildren(self):\n\t\"\"\"\n\tGet a list of children - ordered from coordinate to parent\n\tcoordinate (rightmost last"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif (other is None) or (isinstance(other, (int,long)) and not self == other):\n\t\tif self!= other:\n\t\t\traise lena.LengthError(\"not a valid coordinate for scaling\")\n\t\tother = self.get_coordinate()\n\t\ttry:\n\t\t\treturn self._scale_x(other)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\telse:\n\t\t\treturn Graph.unit_scale(self, other)\n\t\t\n\tif other!= None:\n\t\trescale, rescale_map, remaining = self._rescale_coords(other)\n\t\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tself._scale = other\n\n\n\tdef _getWidth(self):\n\t\treturn self._width\n\n\n\tdef _getHeight(self):\n\t\treturn self._height\n\n\n\tdef _getColor(self):\n\t\treturn self._color\n\n\n\tdef _setWidth(self, width):\n\t\tself._width = width\n\n\twidth = property(lambda self: self._getWidth(), _setWidth)\n\theight = property(lambda self: self._getHeight(), _setHeight)\n\tcolor = property(lambda self: self._getColor(), _setColor)\n\tfillColor ="}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif not other:\n\t\treturn self._scale()\n\tif self._scale() == other._scale() or\\\n\t\t\t\tself._scale() < other._scale():\n\t\treturn self\n\t\n\treturn self.__class__(x = self._scale(), y = other._scale())\n\nclass Rectangle(object):\n\tdef __init__(self, x, y, width, height):\n\t\t\"\"\"\n\t\t:param x: x-coordinate, in degrees\n\t\t:param y: y-coordinate, in degrees\n\t\t:param width: maximum width of the rectangle (default 1)\n\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self\n\t\tif not isinstance(other, Graph):\n\t\t\tother = Graph.add(other, self)\n\t\tif other.x == 0:\n\t\t\treturn self\n\t\tif other.y == 0:\n\t\t\treturn self\n\t\tif other.z == 0:\n\t\t\treturn other.scale(self.x)\n\t\treturn None\n\n\tdef add(self, other):\n\t\"\"\"\n\tAdd a graph item and its children of other.\n\t\n\tIf *other* is set to ``None``, add all other graphs.\n\t\n\tA value of"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\tif self is None:\n\t\t\treturn self\n\t\treturn self.Scale(other)\n\t\n\tdef ToDict(self):\n\t\t\"\"\"\n\t\tGiven the graph to a dictionary, return a key-value list\n\t\tof the form (*format|, *coordinates*) for possible formats,\n\t\tor (*coordinates*, *coordinates**) for all coordinates.\n\t\t\n\t\tOther keyword arguments may be passed to dictionary format\n\t\tand they should be None (default: to return the default list):\n\t\t\n\t\t.. code-block:: python\n\n\t\t\tMyGraph.ToDict() # prints graph as a dictionary\n\t\n\tParameters\n\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\ttry:\n\t\t\ty = getattr(other, 'y', None)\n\t\texcept AttributeError:\n\t\t\treturn float(other)\n\t\ttry:\n\t\t\tz = getattr(other, 'z', None)\n\t\texcept AttributeError:\n\t\t\treturn int(other)\n\t\telif self is None:\n\t\t\tself.y = y\n\t\telif other is None:\n\t\t\tself.z = z\n\t\telse:\n\t\t\tself.y = self.z = _scale(self.y, other, z)\n\n\tdef getHeight(self):\n\t\t\"\"\"Return the height of the graph.\"\"\""}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\t\t\n\t\tif not self:\n\t\t\t\n\t\t\tself.x = None\n\t\t\tself.y = 0\n\t\t\n\t\tif other is not None and not isinstance(other, str):\n\t\t\t\n\t\t\tother, self.z = other, None\n\t\t\n\t\treturn self._scale(other)\n\t\n\tdef scale_graph(self, other=None):\n\t\t\"\"\"\n\t\tGet or set the scale of the graph.\n\t\t\n\t\tIf *other* is ``None``, return the scale of this graph.\n\t\t\n\t\tIf a numeric *other* is provided, rescale to that value.\n\t\tIf the graph has a known or zero scale"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif other is None:\n\t\tother = []\n\t\n\t\n\tself.get_x_and_y_xscale(other)\n\t\n\tif self.has_x_and_y_xscale:\n\t\trescale_by_xscale(self.xscale)\n\tif self.has_x_and_y_xscale and hasattr(self.xscale,'scale'):\n\t\trescale_by_xscale(self.xscale, self.yscale)\n\tif hasattr(self.yscale,'scale'):\n\t\trescale_by_y"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif isinstance(other, Graph):\n\t\tif self._xout is not None or other._xout is not None:\n\t\t\treturn self._scale(self.xout, other.xout)\n\t\t\t\n\t\tif other._yout is not None or other._yout is not None:\n\t\t\treturn self._scale(self.yout, other.yout)\n\t\t\t\n\t\treturn float(self.xout)\n\t\t\t\n\telse:\n\t\tu = self.copy()\n\t\tu.xout = u.xout - other.xout\n\t\tu.yout"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tif not hasattr(hist, \"get\"):\n\t\treturn hist\n\t\n\tif make_value is None:\n\t\tset_ = hist[get_coordinate].get(\"set\")\n\t\tif not len(set_):\n\t\t\tmake_value = hist[get_coordinate].get(\"set_0\")\n\t\t\traise Exception(\"Error in histogram \" + str(histogram.__dict__)) # no context is provided\n\t\t\n\t\tmake_value = set_[0]\n\telse:\n\t\tmake_value = make_value * 3 # not an int, tuple, or list.\n\tnew_value ="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tif len(hist) == 0:\n\t\treturn None\n\t\t\n\tif make_value[0] == \"left\":\n\t\tval = None\t\t\n\telif make_value[0] == \"right\":\n\t\tval = make_value[1]\n\telif make_value[0] == \"middle\":\n\t\tval = make_value[2]\n\telse:\n\t\tval = make_value[1]\n\t\t\n\ty_axis, value_axis = hist[0]\n\tx_axis = hist[1]\n\t\n\tif make_value[2] == True"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tfrom graphene.util import make_graph_of_changes, graph_to_graph\n\t\n\t# get all the fields\n\tfields = graph_to_graph(hist, make_value=make_value,\n\t\tget_coordinate={\n\t\t\t\"histogram\": \"y\",\n\t\t\t\"x\": \"x_mean\",\n\t\t\t\"y\": \"x_mean_error\"\n\t\t},\n\t\tfield_names={\n\t\t\t\"x\": \"x\",\n\t\t\t\"y\": \"y_mean\"\n\t\t},\n\t\tscale=[\n\t\t\t(1, 0, 0, 0"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\ndef hist_to_histogram(hist, make_value=None, get_coordinate=\"left\", max_bins=10, \n\tfield_names=None, scale=False, hist_width=5, field_names_default=[\"x\", \"y\", \"err\"]):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`~graphviz.hist2d` histogram.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\tWhen made output the histogram will be created and the\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tif make_value:\n\t\treturn make_histogram(hist, make_value, get_coordinate)\n\t\n\tif getattr(hist, 'dtype', None) == np.float:\n\t\tif not get_coordinate is None:\n\t\t\treturn make_histogram(hist.astype(np.float), make_value,\n\t\t\t\tscale*100.0)\n\t\telse:\n\t\t\treturn make_histogram(hist.astype(str), make_value,\n\t\t\t\tget_coordinate)\n\t\n\tif getattr(hist, 'dtype', None) == np.int64:\n\t\tif not get"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\tif history == None:\n\t\t\t hist = BinHistograms(hist, make_value, get_coordinate, field_names)\n\t\t\n\t\tif make_value is None:\n\t\t\t hist.scale=hist.var\n\t\telif make_value == \"left\":\n\t\t\t hist.scale=min([hist.var, hist[1].var])\n\t\t\telif make_value == \"right\":\n\t\t\t hist.scale=max([hist.var, hist[0].var])\n\t\t\n\t\tif hist.var==0:\n\t\t\t\thist.scale=hist.var\n\t\telif hist.var>=1:\n\t\t\t\thist"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\taxis = get_coordinate[0] if get_coordinate is not None else \"x\"\n\thist = pyl.utils.normalize(hist)\n\thist = pyl.utils.apply_to_shape(hist, axis=axis)\n\t\n\t# get y-bins\n\ty_bins = hist.flatten()\n\ty_bins.sort()\n\ty_bins = [\n\t\tpyl.utils.convert_value(x, h) for x, h in zip(x_bins, h_bins)\n\t]\n\t\n\t# create Graph class\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tbin_bounds = np.array(hist.bounds)\n\tn = num2num(bin_bounds)\n\t\n\tdata = np.linspace(min(bin_bounds), max(bin_bounds), n)\n\terrors = np.empty((n**2,), dtype=np.bool)\n\tif get_coordinate:\n\t\tfor y in range(n-1) :\n\t\t\tfor x in range(n) :\n\t\t\t\tfor x, y, color in zip(data if make_value else np.linspace(x, y, n) if make_value else bin"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\t# Get the values of each field.\n\tget_value = lambda bin_: bin_.value.astype(\"uint2\")\n\tfield_names = dict(name=\"x\", n_bins=9)\n\tvalue_list = list(hist)\n\t\n\tif make_value is None:\n\t\tvalue_list = []\n\t\n\t# Initialize the histogram context.\n\t# Note that the histogram context may not be set directly in\n\t# particular, use.set_context(value_list).\n\thistogram_context = create_histogram_context(field_names, value_list"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t\n\t\n\tif get_coordinate == \"left\" or get_coordinate == \"right\":\n\t\taxis = \"x\"\n\telse: \n\t\taxis = get_coordinate\n\t\n\thists = []\n\tif make_value!= \"middle\":\n\t\t\n\t\tvalue, bin_size = int(\n\t\t\thist.histogram_to_graph(\n\t\t\t\thistogram_type=(axis, make_value),\n\t\t\t\tcolumn_names=((axis, make_value), hist_dim=hist_dim)\n\t\t\t)\n\t\t)\n\t\t\n\t\tfor x in xrange(value):\n\t\t\thist_row = hist.histogram"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\tdef _verify(iface, candidate, tentative=False, vtype=None, **kwargs):\n\t\t\tverify(iface, candidate, **kwargs) \n\t\t\t\n\t\t\tif vargs.name:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"You can only provide an option with the same name\")\n\n\t\t\tif vargs.required:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"You are required to use the method `required` as the default\")\n\n\n\t#~~ the method interface\n\t\n\tdef _implement(iface, candidate):\n\t\tif vtype and vtype!= iface.get_attribute"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif vtype is None:\n\t\timport zope\n\t\ttry:\n\t\t\tfrom zope.interface import implementedBy\n\t\texcept ImportError:    # pragma: no cover\n\t\t\t# Some providers don't provide an implementer to provide\n\t\t\t# some attributes\n\t\t\tdef comparable(self):\n\t\t\t\treturn True\n\t\telse:\n\t\t\tself._verify(iface.providedBy, candidate, tentative=False)\n\t\t\n\t\n\telse:\n\t\ttry:\n\t\t\tfrom zope.interface import implementedBy\n\t\texcept ImportError:\n\t\t\t# Some providers don't provide an implementer to provide\n\t\t\t# some"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ttry:\n\t\t\tif iface.__class__ == (str, unicode):\n\t\t\t\treturn True\n\t\texcept TypeError:\n\t\t\traise ValueError(\"The passed-in value passed to validate() \"\n\t\t\t\t\"must return a string or Unicode string\")\n\t\t\n\t\tif iface.providedBy is not None and not iface.providedBy(iface.providedBy):\n\t\t\treturn False\n\t\t\n\t\tif type(candidate) not in (type, type(None)):\n\t\t\treturn False\n\t\t\n\t\tif tentative is not None:\n\t\t\tif type(tentative)!= type(None):\n\t\t\t\treturn False\n\t\t\n\t\treturn True"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\tif vistrails.core.version >= (8, 9):\n\t\t\tif iface.providedBy!= candidate.providedBy:\n\t\t\t\traise ValueError(\"Only a valid candidate provided by the \"\n\t\t\t\t\"interface %s was specified.\" % iface.providedBy)\n\t\tassert _must_pass_init_failure or _must_be_instantiated, \\\n\t\t\t\"Only valid %s methods or attributes will be passed to \" \\\n\t\t\t\"initialise or __init__ failure.\" % candidate\n\t\tassert not issubclass(candidate.__class__, Vistrails.core.module.moduleBase),"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\t# We can't have both:\n\t\tif vtype == iface.providedBy:\n\t\t\traise Invalid(iface, \"Trying to verify the interface \"\n\t\t\t\t\t \"provided by '%s' in '%s' method:\"%\n\t\t\t\t\t(iface.providedBy.getFirst(), iface.providedBy,))\n\t\t\t\n\t\t# Just raise the first one.\n\t\tif vtype is not None and \\\n\t\t\thasattr(iface, vtype):\n\t\t\traise Invalid(iface, \"The interface '%s' provides \"\n\t\t\t\t\t\t\t\t\t\t\t\t\"a valid implementation of '%s' class, \"\n\t\t\t\t\t\t\t\t\t\t\t\t\"but you specified '%s'."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif iface.providedBy is None or iface.providedBy is zope.interface.Interface:\n\t\traise zope.interface.Invalid('This method is an interface.')\n\n\tif not candidate:\n\t\tif vtype is None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'A function must provide a value for an attribute '\n\t\t\t\t'true or None.')\n\t\traise zope.interface.Invalid(\n\t\t\t'It is neither a function nor an interface. Instead, you '\n\t\t\t'must provide either a value or None type.')\n\n\t# Find all methods that satisfy the interface given.\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\traise Exception(\"Please avoid reimplementing this method\")\n\texcept TypeError:\n\t\tpass\n\t\n\ttry:\n\t\traise Exception(\"Please avoid reimplementing this method\")\n\texcept TypeError:\n\t\tpass\n\t\n\ttry:\n\t\traise Exception(\"Please avoid reimplementing this method\")\n\texcept TypeError:\n\t\tpass\n\texcept AttributeError:\n\t\tpass\n\t\n\tif vtype is None or vtype not in _TYPED_CONVERSION_MAP.keys():\n\t\t# TODO: Handle deprecation of attribute 'type' in the module\n\t\tcontinue\n\t\n\tfunc, args ="}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\n\t# This is the common case that a method to be tested is a single type\n\t# property. A class may implement a particular implementation (and\n\t# hence any implemented methods that are not supported). If a single\n\t# type property is given, its declaration should be checked against the\n\t# given type with the same names.\n\tdef interface_check(self, iface):\n\t\t\"\"\"\n\t\tIf a class is specified, then the method must make sure that the interface\n\t\tis implemented in ``iface.providedBy``.\n\t\t\n\t\tReturn the new class ``cls``.\n\t\t\n\t\t.. versionchanged::"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ndef provider(adapter, interface, method):\n\t\"\"\"\n\tCheck that a module and class pair is identified by the adapter.\n\t\n\tThis method is used to check that the module/class pair has a\n\tdescriptor. Returns True or False if the module/class pair\n\tcould be found, using a boolean True/False argument when needed.\n\t\n\tSee: `PyZOPE.interfaces.zope.interface.Adapter.iface`.\"\"\"\n\t\n\tmodule = adapter.module\n\tclassName = adapter.className\n\n\tif adapter.providedBy(module):\n\t\treturn True\n\t\n\tif"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n_okay = False # Flag used to know if some of the previous conditions\n\t\nclass NotInRange(object):\n\t\"\"\"\n\tIndicates that the condition indicated in *iface* is not valid (including\n\tall exceptions).\n\t\n\tNote that the constraint (see zope.interface.interface.providedBy) might\n\tbe incorrect, if the condition is present in *iface*, or if the code is not\n\tincorrect.\n\t\nThis also can be used to tell that the condition may not satisfy any of the\n\tprevious methods or attributes of `iface`.\n\t\n\t.. versionadded:: 6"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ttry:\n\t\t\tmethod = plugin.implementations_manager.Method(iface.providedBy(), candidates=[candidate], method_kwargs=dict(boundmethods=iface.boundmethods))\n\t\texcept ValueError, msg:\n\t\t\tlog.debug('Method with interface does not implement the requested method: %s', msg)\n\t\t\tif nottentative or method is not None:\n\t\t\t\traise\n\t\t\traise\n\t\tassert methods == True\n\n\t\tif not method.implementedBy() or method.implementedBy() == 'zope.interface.implements':\n\t\t\traise RuntimeError('%s does not implement the requested method'"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t        if not isinstance(candidate, (list, tuple)):\n\t    candidate = [candidate]\n        for method, attribute in candidate.items():\n\t    if attribute.is_object():\n\t        raise ValueError(\"Only one of the previous conditions are accepted\")\n\t    value = getattr(iface.providedBy, method)\n\t    if not candidate.callable(value) and not candidate.is_abstract(method):\n\t        raise ValueError(\"Not all the %s attributes in %s are callable, or are abstract\" %\n\t\t\t\t\t\t      (method, candidate))\n            return value\n\t    error_msg = 'Interface %s cannot contain object attributes' %"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\tif candidate in iface.providedBy or \\\n\t\t   candidate in iface.providedBy | \\\n\t\t   (implementer is not IContain, tentative is not IContain) \\\n\t   or \\\n\t\t   (iface.provides <zope.interface.providedBy or \\\n\t\t\t   iface.provides <zope.interface.providedBy):\n\t\t    if tentative:\n\t\t      # If the method has no explicit method to provide\n\t\t    # a method to provide, then that method is assumed to\n\t\t    # be for a particular entity as possible before execution\n\t\t      # and the method's signature won't be"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif isinstance(candidate, interfaces.IInterfaceAlias):\n\t\ttry:\n\t\t\treturn tentative(iface)\n\t\texcept KeyError:\n\t\t\tif tentative:\n\t\t\t\traise Invalid(iface,\"not implemented due to a tentative variant.\")\n\t\t\telse: \n\t\t\t\traise Invalid(iface,\"no implemented\")\n\t\t\t\t\t\ndef interface(iface):\n\t\"\"\"Returns ``True`` if interface identified by *iface* is compatible\n\twith zope.interface.provider.  This is used to decide whether\n\tthis interface should be installed via the *interface-alias* interface\n\n\t:"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ndef verifyObject(iface, candidate, tentative=False):\n\tif candidate is None: return tentative\n\telif isinstance(candidate, zope.interface.ICutable):\n\t\tif candidate.providedBy not in iface.providedBy:\n\t\t\traise zope.interface.Invalid('Missing required iface')\n\t\treturn verifyObject(iface.providedBy, candidate, tentative)\n\telif isinstance(candidate, zope.interface.Object):\n\t\tif candidate is not None and not isinstance(candidate, zope.interface.Interface):\n\t\t\traise zope.interface.Invalid('Expected given interface"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tfor method in (\n\t\t\"verifyObject\", \"get\", \"set\", \"remove\",\n\t\t\"getProperties\", \"getPropertiesWithDefault\", \"createProxy\",\n\t\t\"getPropertiesWithDefaultAttributes\", \"getMethods\",\n\t\t\"getObjects\", \"setMethods\", \"getObjectsWithDefaults\",\n\t\t\t\"getMethodSets\", \"getMethodSetsWithDefaults\",\n\t\t):\n\t\t\t\n\t\t\ttry:\n\t\t\t\tmethod(*candidate)\n\t\t\texcept Invalid, e:\n\t\t\t\tif getattr(e, \"providedBy\", None) == candidate, \\\n\t#\t\t\"The interface '%s'"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ndef verifyObjectClasses(iface, candidate, tentative):\n\t\"\"\"\n\tVerify that any of the classes required by *candidate* has been\n\tcreated *tentative*.\n\t\n\tThis involves:\n\t- Making sure the candidate claims that it has an actual class\n\t   from its :class:`zope.interface.Class`. This involves:\n\t\t - Making sure the candidate claims are in the proper format\n\t\t  and that it has a *interface*.\n\t\t - Making sure there is at least one **implementation*.\n\t\t - If the candidate implements *interface*, that's **implemented** by\n\t   "}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\t# TODO: Check if this can be implemented with each interface (or when\n\t# this is called from a method decorated with a zope.interface.Method).\n\tif (iface.providedBy and isinstance( tentative, basestring) and \n        callable(tentative) and tentative in iface.providedBy):\n\t    # It is a real tidy hack so that we do not have to get the \n\t\t    interface from it (and in certain cases if it does, for\n\t   \t# example, if it is a class but does not implement the interface or\n\t     its descendants)\n\t    # FIXME:"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t\n\tif not candidate:\n\t\traise BadTypeError(\"Invalid object for interface:\\n\"\n\t\t\t\t\t\t  \"\\n\".join(traceback.format_exc().splitlines()))\n\t\n\tprovider = iface.providedBy(candidate, tentative=tentative)\n\tmethods = []\n\t# If a single-argument callable was provided, then we will be invoked\n\t# on the interface once or inside the method. The only other way how\n\t# the callable is invoked is is to avoid doing extra calls to\n\t# methods of the same name, and because otherwise we wouldn't be\n\t# able to"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\ttry:\n\t\t\tv = self.getProperty(iface, tentative=tentative)\n\t\texcept errors.AttributeError:\n\t\t\tassert False, \\\n\t\t\t\t'Interface %r does not have expected value.' % iface\n\t\telse:\n\t\t\tassert_(isinstance(v, zope.interface.Class))\n\t\t\tif v is NotImplemented:\n\t\t\t\traise ValueError, \\\n\t\t\t\t\t'delcinates incompatible interface %r' % iface\n\t\t\t\n\t\t\tassert_(isinstance(v, zope.interface.Interface))\n\t\t\tif v.providedBy:\n\t\t\t\tcls = v.__class__\n\t\t\t\tval ="}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tfrom gi.repository import Gtk\n\ttry:\n\t\tself.iface = iface\n\t\tself.candidate = candidate\n\texcept Exception, e:\n\t\tprint(\"Error in class: \" + e, file=sys.stderr)\n\t\treturn\n\treturn self.interface == candidate\n#!/usr/bin/env python\n\nimport re\nfrom xml.dom import minidom\n\nfrom.. import __appname__\nfrom.. import helpers\n\n__author__ = \"Kewit Praeder\"\n__copyright__ = \"Copyright 2013 (c) Kewit Praeder\"\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t        if not isinstance(candidate, tentative):\n\t\traise Exception('The class is not in a trusted interface')\n\n        def verifyClass(iface, candidate):\n                assert(iface is not None)\n                if candidate.__class__ == cls:\n                        for member in candidate.__members__:\n                                if isinstance(member, tentative):\n                                        addr = member.getAddress()\n                                        assert addr is not None\n\t\t\treturn True\n\t\treturn verifyClass\n\n                \nexcept:\n    pass\n\nclass Stub:\n    class __Stub:\n        def getAddress(self):\n                return \"\"\n    class __Stub:\n        def getInfo"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tif isinstance(candidate, tentative):\n\t\tif type(iface) == type(\"Twisted\"):\n\t\t\t# If an installed version of Twisted is detected from the\n\t\t\t# environment, but has not been built into the system,\n\t\t\t# this will likely result from failure in the default\n\t\t\t# installation of Twisted from the command line.\n\t\t\treturn True\n\n\t# Check for a version-string that cannot be instantiated\n\ttry:\n\t\tversion = versioneer.get_version(iface)\n\texcept:\n\t\treturn False\n\n\tinstalled_version = None\n\tfor line in"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tresult = verifyClass(iface, candidate, tentative=tentative)\n\treturn result == True\t\n\ndef verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tresult = verifyMethod(iface, candidate)\n\t\treturn result == False\n\t\ndef verifyMethod(iface, method):\n\t\"\"\"\n\tVerify that *iface* or *method* might correctly provide *iface*.\n\t\"\"\"\n\t\tresult = verifyClass(iface, method)\n\t\treturn result == True\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tresult = cq(eapi, \"eth %s peer%s/%s\" % (iface, candidate, tentative))\n\t\tif tentative: result = eapi.get(eap.EAP_ETH_NAME, result)\n\t\tassert result == \"OK\"\n\ndef verifyAddress(iface, candidate, tentative=False):\n\tiface = iface.lower()\n\tcandidate = candidate.lower()\n\tret = eapi.get(eap.EAP_TABLE + '/' + candidate +'/static/ip', tentative, 'GET',"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\ttry:\n\t\t\traise\t\t# python 2\n\t\texcept NameError:\n\t\t\tpass\n\t\tif tentative==True:\n\t\t\tiface = iface.lower()\n\t\t\tif iface == 'NFS3':\n\t\t\t\treturn\n\t\t\t\telse:\n\t\t\t\t\treturn\n\t\t\t\tiface = 'NFS'\n\t\t\telse:\n\t\t\t\tiface = iface.upper()\n\t\treturn iface\n\ndef checkConfigMounts(iface):\n\t\"\"\"\n\tVerify that the *iface* can mount the mountpoint.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn getMountpoint(iface)\n\t\texcept ( OSError,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\t# Determine if the given candidate is contained in the list\n\t\tfor k in candidate:\n\t\t\tif tentative and iface in k:\n\t\t\t\treturn True\n\t\treturn False\n\n\tif iface in candidate:\n\t\t# If we know it is not contained within the list\n\t\treturn candidate[iface] in candidate\n\n\treturn True# -*- coding: utf-8 -*-\nfrom django.contrib.sitemaps import GenericSitemap\nfrom django.conf import settings\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.core.urlresolvers import reverse\nfrom django.conf.urls.defaults import patterns,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\tif candidate.__class__.__name__ == 'NetworkInterface':\n\t\t\treturn \n\t\tclass_info = candidate.__class__.info()\n\t\tif not class_info.has_key('Network'):\n\t\t\tif candidate.__class__.__name__ == 'ClassInfo':\n\t\t\t\tfor class in candidate.__class__.__bases__:\n\t\t\t\t\tif not hasattr(class,\"is_subclass\"):\n\t\t\t\t\t\tcandidate.is_subclass=True\n\t\t\telse:\n\t\t\t\tfor class in class_info.get('Class',''):\n\t\t\t\t\tif not hasattr(class,\"is_subclass\"):\n\t\t\t\t\t\tcandidate.is_sub"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\n\t# If we're supposed to have a suitable object (this is the main\n\t# thing), we'll just go ahead and fail!\n\tif tentative:\n\t try:\n\t\t\n\t\t\tiface = __findInterface(str(iface))\n\t\t\tif not iface:\n\t\t\t\traise Exception\n\t\t\t\n\t\t\t# Some platforms may give us a nice error message.\n\t\t\tif \"The requested class doesn't exist\" in str(exc): #pragma: no cover\n\t\t\t\tif (iface in sys.exc_value or\n\t\t\t\t\t\tstr(exc) == str(iface) or\n\t\t\t\t\t\tiface in sys.all"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\ttry:\n\t\t\tassert isinstance(iface, ifacestring)\n\t\texcept AssertionError:\n\t\t\tprint \"failed to verify class \"+str(iface)\n\t\telse:\n\t\t\tassert isinstance(iface, type)\n\t\t\tassert isinstance(iface, type) and hasattr(iface, \"class_name\")\n\t\t\tassert issubclass(iface, type) and iface.__class__==\"VncApi\"\n\t\t\t\n\t\t\t\n\t\t\ttry:\n\t\t\t\tassert any(issubclass(iface, vnc_api) for vnc_api in candidate)\n\t\t\texcept AssertionError as e:\n\t\t\t\te.args=[result.result for result in"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n    if explicit_mc is None:\n        print \"WARNING: Explicitly declared metaclass with no explicit mc. Try calling identify_metaclass():\"\n        return\n\n\nif __name__ == \"__main__\":\n    from sys import exit\n    sys.exit(get_class_info(find_metaclass(classname='Haskell')))\n\n##\n# Copyright (c) 2015, Tuininga Systems Incorporated\n#\n# Permission is hereby granted, free of charge, to any person obtaining \n# a copy of this software and associated documentation files (the \"Software\"), \n# to deal in the Software"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tif explicit_mc is None:\n\t\texplicit_mc = {}\n\t\t\n\tif bases:\n\t\tmc = explicit_mc.setdefault('__metaclass__', {})\n\t\tfor base in bases:\n\t\t\ttry:\n\t\t\t\tmc = explicit_mc[base]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t# Now if we did something very wrong with any metaclass\n\t\ttry:\n\t\t\tfor k, v in explicit_mc.items():\n\t\t\t\tif k not in mc:\n\t\t\t\t\traise ValueError(\"unknown metaclass\", k)\n\t\t\t\ttry:\n\t\t\t\t\tmc[k].__bases__ += [explicit_mc"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\ttry:\n\t\t\tfor metaclass in bases:\n\t\t\t\ttry:\n\t\t\t\t\tif issubclass(metaclass, type) and (not explicit_mc or metaclass.im_class is MCSimClass):\n\t\t\t\t\t\treturn metaclass\n\t\t\t\texcept TypeError:\n\t\t\t\t\tpass\n\t\t\treturn type.__new__(metaclass, None, (object,), {})\n\t\texcept:\n\t\t\tif explicit_mc:\n\t\t\t\ttry:\n\t\t\t\tif issubclass(metaclass, type) and (not explicit_mc or metaclass.im_class is MCSimClass):\n\t\t\t\t\treturn metaclass\n\t\t\t\texcept TypeError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif issubclass("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t# This is a metaclass class for each base class.\n\t# You don't want to use this class as a base class but instead pass it along explicitly.\n\tdef do_copy(cls):\n\t\treturn cls.__name__+\"(\"+str(bases) + \")\"\n\n\t# This code is part of a generator. It defines __repr__ and __str__ because\n\t# in Python 2 these will return either a string, a tuple tuple (if __str__ was passed in\n\t# instead of a list the second argument will be the args, not the kwargs dictionary).\n\tdef"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\n\tif len(bases)==1:\n\t\tbases=[])\n\tif not explicit_mc:\n\t\texplicit_mc=getBasicMeta(bases[0])\n\treturn __metaclass__(explicit_mc=explicit_mc)\n\ndef getBasicMeta(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine basic meta objects from 1+ bases and optional implicit __new__\n\t\"\"\"\n\t\n\n\tif not explicit_mc:\n\t\texplicit_mc=getBasicMeta(bases[0])\n\tbases=[e for e in bases if explicit_mc==e]\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t\n\t# First set up bases\n\tif not sys.builtin_module_names:\n\t\tbase_metaclass = abc.ABCMeta\n\telse:\n\t\tif not \"classmethod\" in sys.builtin_module_names:\n\t\t\treturn abc.ABCMeta( bases, explicit_mc )\n\t\telif explicit_mc is not None:\n\t\t\treturn None\n\t\telse:\n\t\t\tif explicit_mc.__module__!= '__main__':\n\t\t\t\timport sys\n\t\t\t\tcls = sys.modules[explicit_mc.__module__]\n\t\t\t\tbase_metaclass = cls\n\t\tif len(sys.builtin_module_"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t\n\tif explicit_mc is None:\n\t    explicit_mc = bases\n\t    if bases == ['Base'] and explicit_mc[0] == '_bases':\n\t        assert 'Base' not in explicit_mc[0]\n\t    elif bases == ['Base'] and explicit_mc[1] == '_bases':\n\t        assert 'Base' in explicit_mc[1]\n\t    assert explicit_mc[2] == _mro\n\t        assert 'Base' in explicit_mc[2]\n\t    assert explicit_mc[5] == _mro\n\t        assert explicit_mc[6] == _mro\n\t        assert"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n  \tif explicit_mc and isinstance(explicit_mc, class_types):\n  \t\treturn explicit_mc.__class__\n  \n  \t# Add the name of the singleton metaclass\n  \tif cls_type_names is None:\n\t\tcls_type_names = []\n\tfor base in bases:\n\t\t\n\t\tdef _addNonMetaMetaName(cls, baseslist, cls_name):\n\t\t\t\"\"\"\n\t\t\t\tAdd non-metaclass metaclass for class, which will\n\t\t\t\tnot inherit from the Base classes, and should be in a\n\t\t\t\tspecial class of \"str\", but they derive from\n\t\t\t\t\""}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n#\t# Set up global variables with their prefixes:\n\t#    _class_to_prefix = {}\n#\t_class_to_suffix = {}\n#\t_super_to_prefix = {}\n#\tdef _get_all_bases_and_subclasses(cls):\n#\t\tbases = []\n#\t\tfor b in cls.__subclasses__():\n#\t\t\tbases.append(b.get_prefix())\n#\t\tmro = bases[:]\n#\t\tif cls in _class_to_prefix:\n#\t\t\tfor k in _class_"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = []\n\t\tall_attrs = []\n\t\tfor base in bases:\n\t\t\tif isinstance(base, type) \\\n\t\tand not inspect.isabstractdescriptor(base):\n\t\t\t\tall_attrs.append(name2method(base))\n\t\t\t\t\n\t\t\t\t\n\t\tdef method(self):\n\t\t\t'''Returns first base method'''\n\t\t\tfor attr in self.attrs:\n\t\t\t\tif not isinstance(attr, types.MethodType):\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\treturn getattr(self, attr)\n\t\t\n\t\tif '__metaclass__' in all_"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\treturn self[key] if key in self else default\n\n\tdef popitem(self):\n\t\tk,d = next(iter(self.choices()))\n\t\treturn self.choices()[k] if k in self else d\n\n\tdef clear(self):\n\t\tself.choices()\n\n\tdef copy(self):\n\t\treturn self.__class__((self,) + self.choices())\n\n\n\tdef copy_with_self(self, cls):\n\t\tif cls.choices()!= self.choices():\n\t\t\treturn self.copy()\n\t\treturn cls(**self)\n\n\n\tdef __eq"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\td={}\n\t\ttry:\n\t\t\tv=self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\tif default is self.__marker: return d\n\t\t\telse: raise\n\t\td[k]=v\n\t\treturn d\n\n\n\tdef __setitem__(self, key, value):\n\t\tif key not in self:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\t\t\n\t\tself.__setitem__(key, value)\n\n\tdef __iter__(self):\n\t\tfor k in self._keys:\n\t\t\td=self.pop(k,default=__marker)\n\t\t\tyield d\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\td = self[key]\n\t\texcept KeyError:\n\t\t\tif default == __marker:\n\t\t\t\traise KeyError(key)\n\t\t\telse:\n\t\t\t\traise KeyError(key)\n\t\treturn d\t\n\tdef popitem(self):\n\t\tk, _ = self.popitem()\n\t\treturn default\n\tdef __reduce__(self):\n\t\timport pickle\n\t\treturn (self.__class__, (self[k],))\n\tdef __repr__(self):\n\t\treturn '%s(%r)' % (type(self).__name__, dict(self.items()))\n\tdef"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\ttry:\n\t\t\treturn default[self.index(key)]\n\t\texcept KeyError:\n\t\t\treturn default[self.index(key)]\n\t\t\nclass Map(dict):\n\tdef __init__(self, *args,**kw):\n\t\tdict.__init__(self)\n\t\tself.__data = []\n\t\tfor arg in args:\n\t\t\ttry:\n\t\t\t\tself[arg] = kw[arg]\n\t\t\texcept KeyError:\n\t\t\t\tself[arg] = kw[arg]\n\t\n\tdef __nonzero__(self):\n\t\treturn"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\tresult = self[key]\n\t\texcept KeyError:\n\t\t\tresult = default\n\t\treturn result\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), return and remove specified key and return the corresponding value.\n\t\tIf key is not found, raise KeyError if given, otherwise KeyError is raised.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdel self[self.__hdr[-1]]\n\t\texcept KeyError:\n\t\t\tif not self.key:\n\t\t\t\traise\n\t\treturn self.__hdr[-1]\n\n\tdef clear(self):\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__dict__.pop(key, default)\n\t\texcept KeyError:\n\t\t\traise\n\tdef setdefault(self, key, d=None):\n\t\t\"\"\"D.setdefault(k,d) -> v, d.update only if key already exists.\"\"\"\n\t\ttry:\n\t\t\treturn self.__dict__.setdefault(key, d)\n\t\texcept KeyError:\n\t\t\tself.__dict__[key] = d\n\t\t\treturn d\n\tdef update(self, *args, **kwargs):\n\t\tpass\n\tdef setdefault(self, key, d=None):\n\t\t\"\"\""}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\n\t\treturn self._find_pop(key, _marker=_marker.find, default=__marker)[key]\n\t\t\n\tdef setdefault(self, key, d=None):\n\t\tif key not in self:\n\t\t\td = self[key]\n\t\treturn d\n\t\n\tdef update(self, *args, **kwds):\n\t\t\"\"\"\n\t\tDeprecated.\"\"\"\n\t\tpass\n\t\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), return and remove and return kth item.\n\t\tk may be an int, in which case the result will be empty"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\tval = object.__getitem__(self, key)\n\t\tval = val.pop(key, self.default)\n\t\treturn val\n\n\t# Support for slice assignment.\t\n\tdef __setitem__(self, key, value):\n\t\tif key == \"__delitem__\":\n\t\t\tdel self.__dict__[key]\n\t\t\treturn\n\t\tobject.__setitem__(self, key, value)\n\n\tdef __delitem__(self, key):\n\t\tif key is not self:\n\t\t\tdel object.__getitem__(self, key)\n\n\tdef __iter__(self):\n\t\tkeys = list(object.__iter"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\treturn self.popitem(key, d)\n\t\t#self.popitem(key)\n\tdef popitem(self, key, *args):\n\t\ttry:\n\t\t\treturn dict.pop(self, key, *args)\n\t\texcept KeyError:\n\t\t\traise KeyError, key\n\ndef sorteddict(d, default=__marker):\n\t\"\"\"\n\tReturns an iterator over the sorted dict d in key-value pairs.\n\tFor example, in the following example, given the key 'A':\n\t>>> sorteddict({ 'A': 1 }, key=lambda x:x )\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\treturn self._get(self._delitem, key, default)\n\ndef setdefault(self, key, default=__marker):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> v\n\td.setdefault(k,d) -> v\n\t\"\"\"\n\t\treturn self._setdefault(self._setdefault, key, default)\n\ndef copy(self):\n\t\"\"\"\n\tD.copy() -> D\n\tReturn a shallow copy of d.\n\tDeep copies are the same as D1.copy().\n\t\"\"\"\n\t\treturn D(self)\n\ndef isEmpty("}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\twhile (len(self._queue) > 0):\n\t\t\tkey, value = self._queue.pop()\n\t\t\tif (len(value) > 0) and (key not in self._ignorelist):\n\t\t\t\treturn (key, value[0])\n\t\t\tself._queue.append((key, value))\n\t\t\tkey = None\n\t\treturn None\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturn a list of all the keys in the queue.\n\t\t(Note: We call this method to get all the keys in self._queue)\n\t\t\"\"\"\n\t\treturn self._queue."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tif self.data==None : return None\n\t\tnewkey=self.popitem()\n\t\tdel self.data[newkey]\n\t\tif newkey in self.data : return self.data[newkey]\n\t\treturn None\n\nclass DataStream(object):\n\tdef __init__(self, data, **kwargs):\n\t\tself.data = data\n\t\tif \"items\" in kwargs:\n\t\t\tself.items = kwargs[\"items\"]\n\t\telif kwargs.has_key(\"items\") :\n\t\t\titems = kwargs[\"items\"]; del kwargs[\"items\"]\n\t\telse :"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\t# if not self.parent.winfo_ismapped() and not self.parent.winfo_ismapped():\n\t\t\t# return (None, None)\n\t\t\n\t\t# pop item from both list items\n\t\tfor key, val in self.item.cget(last=True, vars='dict'):\n\t\t\ttry:\n\t\t\t\tself.item.delete(last=False, var=key, value=val)\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\n\t\t# remove last\n\t\tfor key in self.item.winfo_children():\n\t\t\ttry:\n\t\t\t\tself.item.delete(last="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\treturn None, None\n\n\tdef has_key(self, key):\n\t    return len(self._cache) == self.max_age() == key\n\t\n\n\tdef iteritems(self):\n\t    \"\"\"\n\t    Remove and return a `dict` from the cache with non-cached\n\t    attributes from the cache. Replaces existing entries\n\t    of both `self` and `cache` if cache has no such entry.\n        \"\"\"\n\t        # Get all of our keys\n\t        return dict.fromkeys(self._cache, self)\n\n\t    def __getitem__(self, key):\n\t        # Get all of our keys\n\t       "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tkey, value = self.items.pop(0)\n\t\tself.items.insert(0, (_(key, value), key))\n\t\treturn (key, value)\n\n# ********************************************************************\n# ** Functions\n# ********************************************************************\n\ndef _print_lst(x):\n    \"\"\"Prints `x` as a list element.\n\n\tThe list will be sorted by the number of elements of len(x)-1 by default.\n\n\tA list value of `len(x)` is a list of tuples of the length of the\n\t`len(x)` if the list is"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\nclass ConfigElementBase(metaclass=ABCMeta):\n    \"\"\"\n    Interface for an object that represents a `ConfigParser` (or `ConfigParser`).\n    \"\"\"\n    @abc.abstractmethod\n    def findvalue(self, section, option, value, default=None):\n        \"\"\"\n        Return the first found value for a section,\n        where `option` is either the option of an element,\n        or `default`.\n        \"\"\"\n        raise NotImplementedError\n\nclass ConfigParserInterpolationDict(ConfigElementBase):\n    \"\"\"\n    Used to interpolate a configuration file using a dict-like object.\n\n    Note that this module does *not* conform to the specification,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\nclass Stack:\n\t\"\"\"\n\tA collection of all sub-stacks; if a stack is already in the stack list,\n\tadd the topmost item in the stack list immediately\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tPop the stack from the stack list. If there's no stack, use the parent\n\t\tstack.\n\t\t\"\"\"\n\t\tself.stack = StackList()\n\t\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tRemove and return the most recently used item from the stack.\n\t\t\"\"\"\n\t\tif len(self.stack) > 0:"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\tkeys, values = self.pop()\n\t\tkey = keys.pop(0)\n\t\tself.keys = keys\n\t\tself.values = values\n\t\treturn key\n\n#!/usr/bin/env python\n#\n# Copyright (C) 2006 Stefan Weiss (schweiss@stanford.edu)\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2, or (at your option)\n# any later version.\n#\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\tkey, value = self.key, self.value\n\t\nclass ConfigBoolean(ConfigSequence):\n\tdef __init__(self, default=0, default_value=''):\n\t\tConfigSequence.__init__(self, [], default, default_value = default_value)\n\n#################################################\n#\n##  Menu item\n##\n#################################################\n\nclass Menu(Screen):\n\tdef __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tself.setTitle(_(\"Silent Dialogs\"))\n\t\tself['menu'] = MenuList([], enableWrapA"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\ttry:\n\t\t\titem = self._lst.pop(popitemvalue())\n\t\texcept IndexError:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn item\n\t\n\tdef insert(self, *keypairs):\n\t\t\"\"\"\n\t\tThis will add all items matching the given keys to the list of :attr:`self._lst` \n\t\tthose sorted, so that they are sorted before insertion.\n\t\t\"\"\"\n\t\tif self._lst.count(keypairs[0]) >= self.maxlen:\n\t\t\treturn\n\t\titem = self.popitem(*keypairs)\n\t\tif item is None:\n\t\t\t"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n#\ttry:\n#\t\t\tname = self.key[self.position()]\n#\t\t\tkey = self.key[:self.position()]\n#\t\texcept KeyError:\n#\t\t\tkey = self.key\n#\t\t\tif name not in self.keys():\n#\t\t\t\tself.key = key\n#\t\t\tself.position()\n#\t\t\treturn (key, value)\n#\t\treturn (None, None)\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tPop() is just a convenience wrapper for the list.\n\t\t\"\"\"\n\t\ttry:"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tkey = self.lastkey\n\t\tvalue = self.lastvalue\n\t\tif key in self:\n\t\t\tself.pop(key) # Move to the end of the list\n\t\t\tself.lastkey = key# Remove one\n\t\t\t\n\t\treturn key,value\n\n\n\n\n\ndef popitem_set(queue):\n\t\"Remove the first `key,value` pair from a queue.\"\n\tkey,value = queue.pop(0)\n\tqueue.append((key,value))\n\n\n\n\n# If no `key` is found, return `default`\ndef iskey(key):\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tfor key, item in self.data.items():\n\t\t\tif key in self.keys():\n\t\t\t\tvalue = self.data[key]\n\t\t\t\tif value >= item['value']:\n\t\t\t\t\tself.delete(key)\n\t\t\t\t\tself.append(key, value)\n\t\t\t\t\treturn (key, value)\n\t\treturn None\n\n\tdef replace(self, key, value):\n\t\t\"\"\"\n\tReplace the existing value in the Key/Value dictionary.\n\t\"\"\"\n\t\tif not self.has_key(key):\n\t\t\treturn\n\t\tfor i in self.data.items():\n\t\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tif not self.items:\n\t\t\treturn ()\n\t\tpos = 0\n\t\twhile pos < len(self.items):\n\t\t\tkey, value = self.items.pop(pos)\n\t\t\tself.items.insert(pos, (key, value))\n\t\t\tpos += 1\n\t\treturn (None, None)\n\n\tdef pop(self, key, defaultValue=None):\n\t\t\"\"\"\n\t\tRemove and return the `(key, default)` pair with the specified parameter `key`.\n\n\t\t:Parameters:\n\t\t\tkey (str or unicode key, defaults to `None`)\n\t\t\tdefaultValue (Optional"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tpopitem = self.pop()\n\t\tself.update(popitem)\n\t\treturn popitem\n\n\nclass TextItem:\n\t\"\"\"\n\tObject for storing text in files.\n\t\"\"\"\n\t\n\n\tdef __init__(self, text):\n\t\t\"\"\"\n\t\tSet `text` in files.\n\t\t\"\"\"\n\n\t\tself.text = text\n\t\tself.is_last_line = False\n\n\t\tself.reset()\n\n\t\n\tdef reset(self):\n\t\t\"\"\"\n\t\tInitialize content of file.\n\t\t\"\"\"\n\n\t\tself.lines = []\n\n\tdef append(self,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\treturn self._max\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\t\tReturn whether a key exists or not and is in this tree.\n\t\t\"\"\"\n\t\tif self.__root.get(key) == NODE_MISSING_VALUE:\n\t\t\treturn True\n\t\treturn False\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemove and optionally del the same item.\n\t\t\"\"\"\n\t\tif self._max == NoDE_MAX_VALUE or self._data.has_key(key):\n\t\t\tself._min = nodeint(self._data[key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tresult = None\n\t\tfor key, value in self.cache:\n\t\t\tif value > self.cache_threshold:\n\t\t\t\tresult = (key, self.cache_threshold)\n\t\t\telse:\n\t\t\t\tbreak\n\t\treturn result\n\t\t\n\tdef set(self, k, v):\n\t\tself.cache[k] = v\t\n\t\t\n\tdef get(self, k, f=lambda x: x, g=lambda x: x):\n\t\treturn self.cache.get((k, f(x), g(x)), (None, f))\n\t\t\n\tdef clear(self):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\n#        self._key = k\n\t\tif self._key is not None: # if key is not yet saved, pop the stored value\n\t\t\tself._value = None\n\t\tif self._value is not None:\n\t\t\tself._value = self._value.pop(-1)\n\t\t\tif self._value is not None and hasattr(self._value, 'pop'):\n                self._value = self._value.pop()\n\t\t\t\nclass QueueKey (object):\n\t\"\"\"\n\tBase class defining a key structure. The key value must be `None` because\n\tthe queue has no key value"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\treturn self.tk.splitlist(self.tk.call(self._w, 'pop'))[-1].split('=', 1)\n\n#---------------------------------------------------------------------------\n# Variable functions\n#---------------------------------------------------------------------------\n\ndef variable(VariableName, *args):\n\t\"\"\"\n\tCreate a variable.\n\n\t@param VariableName: Name of variable.\n\t@type VariableName: C{str}\n\t@param args: List of parameters to add to variable (the string is one item sequence).\n\n\t@return: Newly created variable.\n\t\"\"\"\n\t\tWidget.variable(self, VariableName, *"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t\tself.dict = {}\n\t\tfor key in self.list:\n\t\t\tif self.dict.has_key(key):\n\t\t\t\ttry:\n\t\t\t\t\tdel self.dict[key]\n\t\t\t\texcept:\n\t\t\t\t\tprint '*** Unkown Item(%s) ***' % key\n\t\t\t\t#if self.sortedkey.get(key) is not None:\n\t\tself.dict[self.sortedkey[key]] = val\n\t\t\t\n\tdef setItems(self, items):\n\t\timport copy\n\t\tself.items = self.items.copy()\n\t\tsortedlist = []\n\t\t"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tkey = self.__getitem__(1)\n\t\treturn self.popitem_dict[key]\n\t\n\n#####\n# Private functions\n#####\t\nclass _ListView(object):\n\tdef __init__(self):\n\t\tself.list = []\n\tdef add(self, *items):\n\t\tself.list.append(ItemsListView(items, self._items))\n\tdef remove(self, *items):\n\t\tself.list.remove(ItemsListView(items, self._items))\n\tdef list_with_id(self, id):\n\t\tif not id:\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\treturn self.queue.get(block=False)\n\n\tdef keys(self):\n\t\treturn (k for k, v in self.queue.iteritems()\n\t\t\t\tif v[1]!= None and str(v[1]) not in ('?', '?', '+', '-', '?', '.', '//', ':', '|', '>'))\n\n\tdef values(self):\n\t\treturn (v for v in self.queue.iteritems()\n\t\t\t\tif v[1]!= None and int(v[1]) > 0)\n\t\t# return [v for v in self.queue.items() if v[1"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tif self.top.curitem(self):\n\t\t\tpop_top = self.top\n\t\t\tpop_key = self.top\n\t\t\tif pop_top is None:\n\t\t\t\treturn pop_key, pop_top\n\t\t\telif pop_key in self.top.pop:\n\t\t\t\tpush_top = self.top\n\t\t\t\tdel pop_top[self.top.pop[pop_key]]\n\t\t\telse:\n\t\t\t\tpush_top = None\n\t\treturn key, self.top.topitem()\n\tdef decrement_num(self):\n\t\"\"\"\n\tDecrease"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tresult = []\n\t\tcount = self.count\n\t\twhile count <= self.maxcount - 1:\n\t\t\tif len(result) == len(self):\n\t\t\t\tdel self[0]\n#\t\t\t\t\tassert self[-1][-1].name == \"value\", len(self) - len(self) - 1\n\t\t\telse:\n\t\t\t\tpass\n\t\t\tcount = count - 1\n\t\twhile len(result):\n\t\t\tif isinstance(result[0], Key) and not result[0].name:\n\t\t\t\tresult = (result[0], result[1])\n\t\t\t\tbreak\n\t\t\ttry"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t    \tkey = self.key_function()\n\t\tvalue = self.value\n\t\twhile self.value in self.lastkey:\n\t\t\tself.key_function()\n\t\t\tyield (self.key_function(),)\n\n\t\tself.lastkey[self.value] = key, value\n\n\tdef key_function(self):\n\"\"\"\tPop a key, value pair from cache.\n\t\"\"\"\t\n\t\tif self.key_count == 0:\n\t\t\treturn None\n\n\t\tif self.lastkey.has_key(self.value):\n\t\t\treturn self.lastkey[self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tkeys = self.keys()\n\t\tkeys.reverse()\n\t\tlast = len(keys) - 1\n\t\treturn keys[last]\n\ndef poplist(self):\n\t\"\"\"\n\tRemove, return the last \"num\" keys, in alphabetical order.\n\t\"\"\"\n\t\tself.sort()\n\t\treturn self.items()\n\ndef pop(self, num=None):\n\t\"\"\"\n\tRemove and return (key, value) most recently used.\n\t@type num: integer\n\t@param num: maximum number of values to return, if None then\n\t\tdefault"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\n# ------------------------------------------------------------------------------\n#\n# The \"main\" program, which is a place to access the GUI.  It contains\n# the main window that will contain the program's GUI.\n#\nclass MainWindow:\n\tdef __init__(self):\n\t\tpass\n\n\t# ------------------------------------------------------------------------------\n\t# Main routine to show the game\n\t# ------------------------------------------------------------------------------\t\n\n\tdef main(self):\n\t\ttop = Toplevel(self.winfo_toplevel())\n\t\t\n\t\ttop.protocol(\"WM_DELETE_WINDOW\", self.quit)\n\t\ttop.resizable(width=FALSE, height=FALSE"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\t# Remove the item stored in the dict\n\t\tfor key, value in self.dict.items():\n\t\t\tself.dict.pop(key)\n\t\treturn (self.key, self.value)\n\t\t\n\tdef __iter__(self):\n\t\treturn iter([]) \n\n\nclass TreeTreeIterator:\n\t\"\"\"\n\tTreeTreeIterator implements a tree-based iterator over items whose keys are names and \n\tvalues are pointers to a parent node in the tree. You can iterate over a given tree-\n\tthe root node by calling either `iter` or `len`. \n\tIf the root node"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tself._uncachelist.popleft()\n\t\tvalue = self._uncachelist.pop()\n\t\tself._poplist.append(value)\n\t\treturn value\n\t\t\n\tdef pop(self, item):\n\t\tself._poplist.remove(item)\n\t\tif self._poplist:\n\t\t\tself._poplist.pop()\n\n\t@property\n\tdef _active(self):\n\t\treturn self._poplist\n\n\t@property\n\tdef _uncache(self):\n\t\treturn list(self._active)\n\t\t#return list(self._poplist)\n\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t\tkey = self.__keys.pop(None)\n\t\tif key is None:\n\t\t\treturn None\n\n\t\tdel self.__keys[key]\n\t\treturn key\n\n\nclass ConfigNothing(ConfigElement):\n\t\"\"\"\n\tA no-op ConfigElement was added for a given section\n\t\nThis is a replacement for `ConfigParser.ConfigParser.read_str()`.\n\t\n\t.. warning:: Configuration of ConfigParser is deprecated.  Please use `ConfigParser` instead.\n\n\t:param section: The section name to look into, if any, before looking for a given section name.\n\t:param fallback: When none"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tif self.counter < len(self.items):\n\t\t\treturn self.items[self.counter]\n\t\telse:\n\t\t\treturn (None, None)\n\n\tdef showData(self, key):\n\t\t(x, y) = self.getData(key)\n\t\treturn (x, y)\n\nclass Image(object):\n\n\t\"\"\"\n\tBase class for images.\n\t\"\"\"\n\n\tdef __init__(self, filename):\n\t\tassert os.path.isfile(filename)\n\t\tself.curFile = open(filename,'rb')\n\t\tself.filename = filename"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tkey = random.randrange(0, len(self._items))\n\t\tval = self._items[key]\n\t\tif key <= self._items.index(key):\n\t\t\treturn val[0]\n\t\treturn None\n\n\nclass Knowledge():\n\tfields = {\n\t\t'id': Field('ID'),\n\t\t'subject': Field('Submitted Topic', default=None),\n\t\t'is_archived': Field('Is Archived', default=False,\n\t\t\t\thelp_text=u'A boolean that indicates all archived messages are published.'\n\t\t),\n\t\t'm"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\n\tif len(self) > 0:\n\t\t\tres = self.pop(0)\n\t\t\tdelself.key, self.value\n\t\t\treturn self.set_item(res)\n\nclass Dict(UserDict):\n\t\"\"\"\n\tDict wrapper to parse, convert, and flatten a dictionary\n\t\"\"\"\n\tdef __init__(self, *keys, **kws):\n\t\tUserDict.__init__(self, *keys, **kws)\n\n\n\tdef _clean(self, entry):\n\t\tif isinstance(entry, OrderedDict):\n\t\t\tfor key in entry:\n\t\t\t\tself._clean(key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tif self[\"items\"].count() > 0:\n\t\t\treturn self[\"items\"].popleft()\n\t\treturn (None, None)\n\n\tdef update(self):\n\t\tself[\"items\"].reverse()\n\t\tself.refreshAll()\n\t\tself.selectionChanged()\n\t\treturn self\n\n\tdef getCurrent(self):\n\t\treturn self[\"items\"].getCurrent()\n\n\tdef setCurrent(self, index, item):\n\t\tself[\"items\"].setIndex(index)\n\t\tself.refreshAll()\n\t\titem.setText(self.getCurrent())\n\n\tdef removeCurrent(self):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tif self.has_key(KEY_POPUP):\n\t\t\tk = self.pop(KEY_POPUP)\n\t\t\tif k == KEY_LEFT:\n\t\t\t\tvalue = 0\n\t\t\t\tfor _ in xrange(0, self.key_length + 1):\n\t\t\t\t\titem = self.queue[KEY_LEFT]\n\t\t\t\t\tvalue = item[0] + item[1]\n\t\t\t\t\tk = item\n\t\t\telif k == KEY_RIGHT:\n\t\t\t\tvalue = 0\n\t\t\t\tfor _ in xrange(0, self.key_length + 1):\n\t\t\t\t\titem = self.queue[KEY_"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\n    def setkey(self, key):\n\t\tself.key = key\n\n    def setvalue(self, value):\n\t\tself.value = value\n\n    def add(self, result):\n        \"\"\"Add a result to the dictionary `self.keys`.\"\"\"\n\tself.list.insert(END, result)\n\n    def pop(self):\n        \"\"\"Remove an (key,value) pair from the dictionary `self.keys`.\"\"\"\n\tval = self.list[0]\n\tif self.key == self.list[0]:\n\t\tdel self.list[0]\n\n\telse:\n\t\tself"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn (self.key, self.value)\n\n\tdef __nonzero__(self):\n\t\treturn bool(self.key)\n\ndef popitems(self):\n\t\"\"\"\n\tRemove and return a generator of items from the list.\n\t\"\"\"\n\t\tfor key, value in self.items():\n\t\t\tyield (key, value)\n\nclass _HashMap(dict):\n\t\"\"\"\n\tDupe a map of hashes to a hashable hash.\n\t\"\"\"\n\n\tdef __init__(self, *arg, **kw):\n\t\tsuper(_HashMap, self).__init__(*"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\treturn self._keys.pop(self._key, None)\n\tdel self._keys[:]\n\traise StopIteration\n\nclass DuplicatesDefaultKeyError(ValueError):\n\t\"\"\"\n\tDuplicate keys must come first, as long as they are the same, as\n\tkey is not a dict value.\n\t\"\"\"\n\tpass\n\nclass DictDefaultKeyError(ValueError):\n\t\"\"\"\n\tIf an `iterable` is passed, it must be a dictionary instead of\n\tdictionary.\n\t\"\"\"\n\tpass\n\nclass OrderedDefaultKeyError(KeyError):\n\t\"\"\"\n\tMissing `order` option"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\tvalue = self.pop()\n\t\tself._items.remove(value)\n\t\treturn value\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove from the top of the list.\n\t\t\"\"\"\n\t\tif self._items:\n\t\t\tvalue = popitem(self._items.pop())\n\t\telse:\n\t\t\tvalue = self.popitem()\n\t\treturn value\n\tdef update(self, value):\n\t\t\"\"\"\n\t\tUpdate this list with a new `value`.\n\t\t\"\"\"\n\t\tself._items.insert(0, (value,))\n\t\t# This is a copy of"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t\nclass Random:\n\t\"\"\"\n\tWrapper for a random number generator.\n\t\"\"\"\n\tdef __init__(self, minval, maxval):\n\t\tself.minval = minval\n\t\tself.maxval = maxval\n\t\n\tdef random(self):\n\t\treturn self.minval + self.maxval\n\nclass PaddedRandom(Random):\n\t\"\"\"\n\tPadded random number generator.\n\t\"\"\"\n\tdef __init__(self, width=2):\n\t\tsuper(PaddedRandom, self).__init__(width, randint(0, 2**width))\n\t\n\tdef pad("}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\nin_regex: Pattern = Pattern()\n\ndef parse_string(text:str) -> Pattern:\n\n\tif text == \"\":\n\t\treturn in_regex\n\n\tmatch = re.compile(in_regex)\n\treturn match\n\n\ndef parse_file(filename:str, filename_type:str) -> Pattern:\n\t \"\"\"\n\t\tParse an input file in the format described by the file contents.\n\t\t\n\t\tThe format (like this) can be derived on the content type given in a\n\t\t:class:`~kombu.serialization.Encoding` using the :func:`parse_file`"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\tdef construct_regex(self, pattern: Pattern) -> Pattern:\n\t\t\"\"\"\n\t\tConstruct a regular expression pattern.\n\t\t\n\t\tArguments:\n\n\t\t* ``pattern`` -- ``Pattern`` to be evaluated.\n\t\t\n\t\tExample::\n\t\t\t\n\t\t\t>>> from pyomo.units import Ram\n\t\t\t>>> r = Ram(r\"/Users/bartl/temp/myrodcm.wav\")\n\t\t\t>>> r.create_regex(r\".+$(.wav)\")\n\t\t\t>>> # Match if there is an extension to /audio\n\t\t\t>>> r = Ram(r\"/Users/b"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\tdef parse_filter_name(self) -> Pattern:\n\t\t\"\"\"\n\t\tParses the filter name (without trailing punctuation, like ``$`` is being used in filter arguments).\n\t\t\n\t\t:return: a :class:`re.Match`\n\t\t\"\"\"\n\t\t\n\tdef _create_string_from_filter(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate a new regular expression from a filter string.\n\t\t\n\t\t.. _filter: https://en.wikipedia.org/wiki/Parameter_regex#Parameter_regex\n\t\t\n\t\t:return: a new :class:`re.Match`\n\t\t\"\"\""}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\ndef create_param(self) -> Pattern:\n\t\"\"\"\n\tCreate the regular expression for a single parameter name. Used to validate the parameter name against a parameter\n\tname.\n\t\n\tReturns the regular expression (:class:`re.Regex`).\n\t\"\"\"\n\t\ndef validate_name(self, name: str, param_name: str) -> Optional[Pattern]:\n\t\"\"\"\n\tValidate the name against the given name.\n\t\n\tReturn the parameter name name corresponding to the parameter name.\n\t\"\"\"\n\t\n##\t\n\tdef validate_input(self, param_name: str, param"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\tdef _build_regex(self, input_str: str) -> Pattern:\n\t\t\"\"\"\n\t\tCreate a regular expression.\n\t\t\n\t\tParses and builds the regular expression using it the result of the\n\t\texpression match function that was generated. The default implementation\n\t\tis '^' which matches any whitespace character.\n\n\t\t:param input_str: Any string that should be processed by the\n\t\t\tregex method used to build the regular expression.\n\t\t\n\t\t:rtype: function\n\t\t\"\"\"\n\t\tcompiled_regex = re.compile(input_str)\n\t\t\n\t\treturn compiled_regex\n\t\n\tdef _convert"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n#### G. R. (201510) Brute force fielding. ##\n\t\n\tdef __init__(self, param_type_name: str, param_size: int) -> None:\n\t\t\"\"\"\n\t\tInitialize the parameter with the specified type.\n\t\t\n\t\t.. note:: If the type is a string, then the following strings are handled:\n\t\t\t - Any string can be converted to numbers.\n\t\t\t- Any integer can be converted to strings and all other string types\n\t\t\tare ignored.\n\t\t\n\t\tThe parameter value can be specified as a Python expression or a string.\n\t\t\n\t\tParameters\n\t\t-----------"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t  \n\n\tdef _create_sub_regular(self) -> Pattern:\n\t\"\"\"\n\t\tCreate the sub-style parameter regular expression.\n\t\t\n\t\tReturns the sub-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\t\n\t  \n\n\tdef _create_pattern(self) -> Pattern:\n\tr'(\\w+)' # Returns the regular expression.\n\t'''\n\t\treturn self.params1.pattern\n\n\tdef _create_pattern_2(self) -> Pattern:\n\tr'\\(\\w+)' # Returns the regular expression.\n\t'''\n\t\n\n\tdef _create_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\tdef create_match(self, value: AnyType) -> Pattern:\n\t\tval = super().create_match(value)\n\t\tif isinstance(val, Pattern):\n\t\t\treturn val\n\t\t\n\t\ttry:\n\t\t\tif re.search(expr_regex, val, flags=re.IGNORECASE|re.MULTILINE):\n\t\t\t\t# TODO: Support this with regex pattern\n\t\t\t\treturn val\n\t\texcept AttributeError:\n\t\t\tpass\n\n\t\tif isinstance(val, str):\n\t\t\tmatch = re.match(regex_regex, val, flags=re.IGNORECASE|re.MULTILINE)\n\t\t\treturn"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\tdef _create_in_path(self) -> Pattern:\n\t\tpattern = re.compile(ur\"(?P<pattern>.+?)\\s*((?P<value>.+)\\s*)\")\n\t\treturn pattern\n\n\tdef _get_regex(self, name: str) -> Pattern:\n\t\tpattern = re.compile(ur'\"{0}\"(\\[(.*?)|'.format(name)) + u\"\\s*(.*?))$\".format(name)\n\t\treturn pattern\n\n\tdef _make_in_str(self) -> str:\n\t\tstr_to_match = ''\n\t\tfor"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t\n\tdef _create_re_pattern(self) -> Pattern:\n\t\t\"\"\"\n\t\tReturns the regular expression for the pattern.\n\t\t\n\t\tReturns a new regular expression consisting of alphanumeric characters as\n\t\tthe first character and a `$` in it (which is an underscore).\n\n\t\t`$` means a non-underscore character.\n\n\t\tExample: `$` will match a string as `$` and the result of `$`\n\t\twill be ``$foo``.\n\t\t\n\t\tExample: `1$` will match \"Foobar\" (with an underscore).\n\t\t\n\t\t`$foo` will match \""}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\t@classmethod\n\tdef init(cls, param: _ConvertingParams):\n\t\t\"\"\"\n\t\tConstructor.\n\t\t\n\t\tThe parameter style converter is initialized from the constructor\n\t\tparameters. The converter is returned if there are no\n\t\texisting converters. The converter instance is returned if\n\t\tthere is no existing converter.\n\t\t\"\"\"\n\t\t\n\t\tassert cls is not None, \"The Converting class\"\n\t\t\n\t\t# Create converter objects\n\t\tconverter = cls(_convert_param=param.converter)\n\n\t\t\n\t\t\n\tdef _set_validator(self, param: _ConvertingParams) -> None:\n\t\t\"\"\""}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn converter\n\t\n\t@_create_converter.setter\n\tdef _create_converter(self, converter):\n\t\tself._converting._converter = converter\n\t\n\t@_create_converter.deleter\n\tdef _create_converter(self):\n\t\tdel self._converting._converter\n\t\n\tdef __init__(self, *args, **kw):\n\t\t# Setup style conversion\n\t\tconverter = self._create_converter()\n\t\treturn converter(*args, **kw)\n\t\t\n\tdef toParam(self, value):\n\t\t\"\"\"\n\t\tConvert a parameter based on the parameter names in the\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, converters=None):\n\t\tif converters is None:\n\t\t\tconverters = [_converting._ Convertable(_converter)\n\t\t\t\t\t\t\tfor _converter in (self.DEFAULT_CONVERTERS)]\n\t\t\n\t\tsuper(_Converting, self).__init__(converters)\n\n\tdef handle_argument(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tThe entry point of the command line, in case that is not available after arguments\n\t\tare specified.\n\t\t\n\t\tNote: You cannot use this method if you are building your own converter.\n\t\t\n\t\tArguments:\n\t\t\t- ``self"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, param_name: str, param_type: types.TupleType):\n\t\tassert param_name.endswith('name')\n\t\tassert param_name.endswith('_type')\n\t\t\n\t\tself._param_type = param_type\n\t\t\n\t\tself._type_to_converter = _converter(name=param_name)\n\t\t\n\t\tself._converter = self._create_converter()\n\n\tdef to_param(self) -> _param.Param:\t\t\n\t\t\"\"\"\n\t\tReturn a new Parameter object with the given param_name.\n\t\t\n\t\tThis returns a :class:`"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting._Converter()\n\t\t\n\ufefffrom PIL import Image\nfrom scipy.misc import imread\nfrom scipy.misc import imresize\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors, kneighbors_graph\nfrom sklearn.externals import joblib\n\ndef get"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _Converting._create_converter(self)\n\t\n\tdef _get_converter(self) -> _Converting:\n\t\"\"\"\n\tUse this method to convert parameter style to a converter.\n\t\n\tReturns the converter (:class:`._converting._Converter`)\n\t\"\"\"\n\t\treturn _converting._get_converter(self)\n\n\tconverter = property(_get_converter, _set_converter)\n\t\n\t@staticmethod\n\tdef get_parameters(converter: _Converting) -> _ConvertingParams:\n\t\"\"\"\n\tGet the parameter style params for the given class.\n\t\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn _converting.to_converter(self)\n\n\t@property\n\tdef default(self) -> _converting.Converter:\n\t\treturn self._converter_defaults[self.default_converter]\n\n\t@property\n\tdef param(self):\n\t\treturn self._parameter_format[self.param_format]\n\n\t@parameterizers()\n\tdef default_converter(self):\n\t\t\"\"\"Return the default converter used in this parameter style.\"\"\"\n\t\treturn self._default_converter\n\n\t@property\n\tdef converter(self) -> _converting.Converter:\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\treturn self._converter\n\t\t\n\tdef _create_handler_converter(self, type: _Converting._Type):\n\t\t\"\"\"\n\t\tCreate the handler converter.\n\t\t\n\t\tReturns the handler converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\tif type == Converting._Type.JSON:\n\t\t\treturn self._converter.load_json(self.data_path)\n\t\t#return NotImplemented\t\n\t\n\t\n\tdef _get_conversion_name(self) -> str:\n\t\t\"\"\"\n\t\tReturn the name of the param converter.\n\t\t\n\t\tReturns:\n\t\t\tThe convertion name."}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\nimport copy, warnings\nfrom.parameter_style import ParameterStyle\nfrom.util import get_converter\nfrom.util import make_paramstyle\n\nclass Unary(ParameterStyle):\n\t\"\"\"\n\tUnary (e.g., a Python builtin parameter).\n\t\n\tThe result of a named parameter is a list of parameters, which is the length\n\tof the input.\n\t\n\t:type value: list of numbers\n\t:param value: The value of this parameter.\n\t\n\t\"\"\"\n\t__slots__ = ()\n\tdef __init__(self, value=(), *args, **kw):"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\t\n\tdef _create_config(self, name: str, val: str) -> ConfigTypeConverterParams:\n\t\twith self.session() as session:\n\t\t\tconverter = _converting.Converter()\n\t\t\tconverter.name = name\n\t\t\tconverter.val = val\n\t\t\tsession.add(converter)\n\t\t\treturn converter\n\t\n\t\n\tdef get_config(self, name: str) -> ConfigTypeConverterParams:\n\t\t\"\"\"\n\t\tGets a conversion instance for the given name.\n\t\t\"\"\"\n\t\treturn self._create_config(name, ConfigTypeConverterParams)\n\t\n\tdef set_"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\t\n\tnow = dt.datetime.now().date()\n\t\n\tday_no_fraction = now\n\t\n\tif not dt_str.startswith(\"-\"):\n\t\thour_part, minute_part, second_part, tz_part = dt_str[0:6].partition(\":\")\n\t\tif not hour_part:\n\t\t\thour, minute, second, tz_part = dt_str[6].partition(\":\")\n\t\n\telif dt_str[-1] == \"-\":\n\t\thour, minute = dt_str[:-1].split(\":\")[0:4]\n\t\tif not minute:\n\t\t\tminute ="}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n__all__ = ['isoparse', 'parsedate', 'parsetz', 'tzutc', 'validate_timezone','make_timezone_safe', \n        'make_timezone_safe_naive']\n\nimport time\nimport re\n\nclass isoparse(object):\n\t\"\"\"\n\tParse a :class:`datetime.datetime` into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T``"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t     \n\ttz = None\n\t\n#\tfrom datetime import datetime, timedelta, tzinfo, tzinfoutil\n#\tdt = dt_util.tz.tzutc(dt_str, tz=tz)\n\t\n\t#\ttz = dt.dstoffset(dt_str)\n\t\n\t#\treturn datetime.combine(dt,tz),utcoffset(dt)\n\t\t\t\n\t\t\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t    \n\t\n\ttry:\n\t\tinput_string = dt_str.read()\n\texcept EOFError as err:\n\t\traise ParseError(err, input_string)\n\texcept Exception as err:\n\t\traise ParseError(err, input_string)\n\t\n\tif not isinstance(input_string, io.IOBase):\n\t\ttry:\n\t\t\tinput_string = io.BytesIO(input_string.read())\n\t\texcept IOError as err:\n\t\t\traise ParseError(err, input_string)\n\t\n\t# parse the date-time string, creating a set() that will handle the \n\t# format of"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\t\nclass datetimeutil(object):\n\t\"\"\"\n\tA generic base class which represents the base class for all :class:`datetime`.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreate a :class:`datetimeutil` instance.\n\t\t\n\t\tMost attributes are:\n\t\t\t- ``utc`` : a tzinfo object\n\t\t\t- ``utctimetuple`` : a timetuple (or `None`)\n\t\t\t- ``isoformat`` : a human-readable `datetime`_ string\n\t\t\t- ``iso8601`` : a UTC-compliant ISO-8601 datetime string.\n\t\t\t\n\t\tDefault attributes:"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\ndef normalize_date(dt):\n\t\"\"\"\n\tAccepts a standard ISO-8601 date string as an input by this parser\n\te.g. ``2010-08-08`` to be ignored.\n\t\n\tSupported formats are ``[yyyy-mm-ddThh:mm:ssZ]``.\n\t\n\tThe standard has a standard set of ISO-8601 formats,\n\te.g. ``2011-06-04`` to be preserved.\n\t\n\t:param dt:\n\t    A string/stream containing only an ISO-8601 date string\n\t\"\"\"\n\t\n\tif isinstance(dt_str,"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n\t\tif isinstance(dt_str, unicode):\n\t\t\tstr_t = dt_str.decode('ascii')\n\t\telif isinstance(dt_str, basestring):\n\t\t\tstr_t = dt_str\n\t\t\t\n\t\telse: raise TypeError('Type %r not recognized' % dt_str.__class__)\n\t\t\n\t\tdt = datetime(*(strptime(d, str_t)[:3] for d in dt_str.split(';')))\n\t\treturn cls(dt)\n\t\n\tdef __init__(self, dt_str):\n\t\t\"\"\"\n\t\t@param dt_str:\n\t    An ISO-8601 ``"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t    dt_str = iter_stag(dt_str).next.lower()\n\t    if dt_str!= 'T':\t# pragma: no cover\n\t\t\traise exceptions.ValueError(\"invalid ISO date format\")\n\t    if dt_str.isnumeric():\n\t        raise exceptions.ValueError(\"invalid ISO 8601 date format\")\n\t    for i in range(dt_str.intstrlen):\n\t        yield dt_str.get()\n\telse:\n\t\tyield ()\n\t\n\n@contextmanager\ndef timezone(tz):\n\t\"\"\"\n\tTries to obtain a timezone for a datetime object\n"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t\n@deprecated(\"The :func:`isoparse` interface is deprecated in favor of \"\n                \"ISO-8601 as it is now deprecated now\")\ndef isoformat(dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 date string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not*"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t    \n\t# parse the RFC 1902-2003-08 and RFC 1902-2001-08 format strings\n\t# that we return the current time.\n\t\n\tif isinstance(dt_str, (basestring, TextType)):\n\t\tstring_type = str\n\telif not isinstance(dt_str, UnicodeType):\n\t\traise TypeError(\"'%s' is not a valid RFC 1902-2003-08 string.\" % (dt_str,))\n\t\n\tl = len(dt_str)\n\tdt_str = dt_str[0:l]\n\t# if the leading timezone part is missing"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n        match = fuzzy_parser(timestr)\n        if match is not None:\n            self.fuzzy_time = datetime.datetime.strptime(\n                match.group(1), DEFAULT_DATETIME_FORMAT)[:18]\n            return (self.fuzzy_time, self.fuzzy_time.isoformat())\n        else:\n\t\t\n        match = fuzzy_parser('0')\n        if match is not None:\n            self.fuzzy_time = datetime.datetime(\n                int((match.group(1)+1), 16),\n                int((match.group(2)+1), 16), 18)\n            return (self.fuzzy_"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\n\ndef parse_time(t):\n\t\"\"\"Parse a parsed date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param str t:\n\t    A date/time string in the specified format, with the same\n\t    number of seconds as the date/time.\n\t\t\n\t:rtype: :class:`datetime.datetime`\n\t    \n\t\"\"\"\n\t    \n\t\t\n\t# If the given time was passed as a `datetime.datetime` object, it might\n\t# be timezone aware. Doubling of this does not work for timezones due to\n\t# misconfiguration, which"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    if isinstance(timezones, datetime.tzinfo):\n\t    return (timezones.tzname(tzinfo=timezones))\n\t    if ignoretz:\n\t    return (timezones.tzoffset(tz=timezones)\n\t    if isinstance(timezones, str):\n\t    ret = tuple(e.replace(u' ', u'') for e in self.utc.utctimetuple()[:7])\n\t    else:\n\t    ret = tuple(e for e in self.utc.utctimetuple()[:7] if e not in self.utc.tzinfo.tzname())\n\t    raise ValueError, \"%s does not match the time zone\""}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t    \n\tif (default is None)\t:\n\t\tdefault = self.default\n\t\t# Default value for the `fuzzy_with_tokens` option is ignored,\n\t\t# in that case you can use ``fuzzy_with_timestamps=False``.\n\t\t# Default value `False`.\n\t\tfuzzy = parser.fuzzy_with_timestamps\n\t\tif (fuzzy):\n\t\t\treturn (None, default)\n\ttry:\n\t\treturn (parse(timestr, default,\n\t\t\tparse_timespec=True,\n\t\t\tignoretz=ignoretz,\n\t\t\ttzinfos=tzinfos\n\t\t\t"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t     \n\t    \n    def _parse(self, tstr, ignoretz, default,\n\t\"\"\"\n\tParse a :class:`datetime.datetime` object into a :class:`datetime.date`\n\tobject. For Python 3.4 and newer, `tstr` is guaranteed to be a\n\tdatetime.date object.\n\tIf not provided, the ``tzinfo`` is set to the current\n\ttime zone.\n\t\"\"\"\n\t     \n\t    \n    def parsedate(self, ymd, format=\"%Y-%m-%d\"):\n\t\t\"\"\"\n\t\tParses a date/time string. This is used internally"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t        if timestr is not None:\n\t        if isinstance(timestr, str):\n\t        self.update(timestr, default)\n\t        else:\n\t        timestr = parse(timestr, default=default,\n\t\t\t        ignoretz=ignoretz, tzinfos=tzinfos)\n\t        self.update(timestr, default=default,\n\t\t\t\t     ignoretz=ignoretz, tzinfos=tzinfos)\n\t        return timestr\n\t\n#    def extract(self, tokens): \n#        \"\"\"\n\tExtract and transform a text string to date/time strings.\n\tGiven a sequence"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tif not timestr:\n\t\t\treturn default\n\t\telif len(timestr.split()) > 1:\n\t\t\traise ValueError(\"Invalid string format: %s\" % timestr)\n\t\telif not default:\n\t\t\tif len(timestr.split()) == 3:\n\t\t\t\traise ValueError(\"Invalid datetime: %s\" % timestr)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Invalid string format: %s\" % timestr)\n\t\telif default[0] == \"\":\n\t\t\traise ValueError(\"Invalid datetime: Unknown format: %s\" % default)\n\t\telif default"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\n\t\n\ttzoffset = kwargs.get('tzoffset', 'UTC')\n\t\n\tif not timestr or not timestr.startswith(' '):\n\t\traise ParserError(\"Invalid timestr\")\n\t\t\n\tnow = time.mktime(time.localtime(timestr))\n\ttoday = time.strftime(\"%Y-%m-%d\", now)\n\ttm = time.strftime(\"%m/%d/%Y\", now)\n\tdd = time.strftime(\"%d/%b/%Y\", now)\n\t\n\tif stripseconds:\n\t\tfor k in kwargs:\n\t\t\tif k.startswith('+') or len(k)=="}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t\tif not isinstance(timestr, unicode):\n\t\t\traise TypeError(\"expected non-unicode string; got %r\" % type(timestr))\n\t\n\t\tutc = gettz(\"UTC\")\n\t\tparsed = timestr.split(\"%f %t\" % (utc.tm_year, utc.tm_mon).lstrip(\"0\"))\n\t\tif len(parsed) <= 2:\n\t\t\traise OverflowError(\"Expected a pair of valid date/time formats, \" \\\n\t\t\t\t\t\t\t+ \", but found %r\" % parsed)\n\t\ttry:\n\t\t\toffset_string = \"\"\n\t\t\t# Skip \"0\""}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t        # handle parsing to allow for ambiguous time zones\n\t        tokens = parse_fuzzy(timestr, ignoretz=ignoretz, tzinfos=tzinfos,\n\t\t                                          tzinfo_options=tzinfo_options,\n\t\t                                          fuzzy_if_none=fuzzy_if_none,\n\t\t                                          fuzzy_without=fuzzy_without)\n\t\t    # make sure that we've got one token for each time zone\n\t\t        if not tokens:\n\t\t            tokens = iter(default)\n\t\t        for tok in tokens:\n\t\t                try:\n\t\t\t                    try:\n\t\t\t\t                    val = tokens.next()\n\t\t\t                except StopIteration:\n\t\t\t\t"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\tif dt is None:\n\t\treturn None\n\t\n\ttz1 = None\n\ttz2 = None\n\tfor item in dt.timetuple():\n\t\ttz1 = item[2]\n\t\ttz2 = item[1]\n\treturn tz1 if tmtz is None else tmtz2 if tz1 < tmtz < 0 else None\n\t\nclass FixedOffsetTimezone(object):\n\tdef __init__(self, offsettime=None):\n\t\tself._offsettime = None\n\t\tself._offset = None\n\t\tif offsettime is None:\n\t\t\tself._offsettime = Timezone"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tassert dt in (self.ST, self.DAM)\n\t\tself.tz = dt\n\telse:\n\t\tassert dt in datetime(self.ST, self.DAM)\n\treturn self\n\nclass _UnambiguationState(object):\n\t\"\"\"\n\tA container for the values of a unambiguous date. It will have its\n\tvalues set to None, for instance.\n\t\"\"\"\n\t\n\tdef __init__(self, values, **kw):\n\t\t\"\"\"\n\t\t:param values:\n\t\t\tValue to assign to the unambiguous date.\n\t\t:param kw:"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\n\timport time\n\tdt = dt.astimezone(self.orig).replace(tzinfo=self.orig.tzinfo)\n\tdt = dt.replace(tzinfo=self.orig.tzinfo)\n\treturn dt\n\n\nclass LASTEST_YEAR(datetime.tzinfo):\n\t\"\"\"\n\tThe LASTEST_YEAR class stores the most recent year and time in \n\tthis module in CET.\n\t\n\t:param dt:\n\t    The timezone - naive datetime\n\t\"\"\"\n\tdef fromutc(self, dt):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime in a given timezone"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\n\t# This method is a no-op to the _update_date_of_iso_time_on_iso_time()\n\t# because we have to set the timezone to None and make the\n\t# assumption that the time of the first occurrence was the same.  We must\n\t# return today from this logic because it must never be set to today.\n\t# However, on the lower boundary of the datetime, this cannot be \n\t# combined with the other case.\n\t#\n\treturn dt.replace(\n\t\ttzinfo=None,\n\t\ttzinfoclass=None,"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\treturn datetime.time(dt.year, dt.month, dt.day)\n\n\tdef fromutcseconds(self, secs):\n\t\"\"\"\n\tGiven a number of numbers, calculates a time from the given number of\n\tseconds.\n\t\n\t:param secs:\n\t    A number of seconds from the given one: the number is the number\n\t    which will be the same across the timezone.\n\t\"\"\"\n\t\treturn time.mktime(secs.timetuple())\n\n\tdef set_timezone(self, tz):\n\t\tif isinstance(tz, datetime.tzinfo):\n\t\t\ttz = gettz(tz"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n__all__ = [\"normalize\"]\n\n\ndef normalize(dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a new timezone, calculates\n\ta timezone-aware float time, keeping the datetime behind\n\t\"\"\"\n\tif not isinstance(dt,datetime):\n\t\tdt = datetime.datetime.fromutc(dt.astimezone(tzlocal()))\n\treturn dtfrom pyjamas import DOM\nimport re\nfrom pyjamas.ui import *\n\nfrom pyjamas.popup import Popup\nfrom pyjamas.ui import *\nfrom pyjamas.ui."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n#import numpy as np \n\n#def is_ambiguous(dt):\n\"\"\"\n\tReturns true if the given timezone-aware datetime has an\n\tambiguous datetime type. Note that this is because no ambiguous timezone\n\tis defined that we would expect a timezone to be aware.\n\t\"\"\"\n\t# if dt.tzinfo is None:\n\t# \treturn False\n\t# if dt.tzinfo._utcoffset is not None:\n\t# \treturn dt.tzinfo._utcoffset\n\t# return (dt - dt.utcoffset()).seconds < 60\n\n#def is_weekday(dt):\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t\n\t\n\t# We'll just convert it into a timezone-aware datetime.\n\t\n\tdt = dt.replace(tzinfo=self._tzinfo)\n\t\t\n\t# This is different from now. It only uses an ambiguous\n\t# datetime, which means we're not on the back end of the world.\n\t\n\treturn dt\n  \n \ndef timeFromLocalTZ(obj):\n\t\"\"\"\n\tGiven a timezone-aware datetime, returns the local time as a\n\ttimestamp.\n\t\n\tThis is useful for dates as it does not have timezone limitations.\n\tAs long as the local timezone is a \"time zone\" we"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n#\tif dt.tzinfo is  None:\n#\t\t# assume that tzinfo is UTC, since we won't be running in\n#\t\t# non-UTC timezones.\n#\t\t# make sure it is UTC (see below)\n#\t\tdt = dt.replace(tzinfo=None)\n#\t\tif dt.tzinfo is None:\n#\t\t\tdt.tzinfo = pytz.utc\n\n\tret = datetime.datetime(*dt.timetuple()[:6])\n\n\tif (ret.tzinfo is None and ret.tzinfo == pytz.utc):\n\t\tret = returntz"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\tif dt.tzinfo!= self.tzinfo:\n\t\t\tself._tzinfo = dt.tzinfo\n\t\n\tdef dst(self, dt):\n\t\t\"\"\"\n\t\tSimilar to:\n\t\t:meth:`fromutc`, called on :meth:`fromutc.dst` to determine\n        whether an ambiguous datetime is ambiguous.\n\t\t\"\"\"\n\t\tif dt.tzinfo!= self.tzinfo:\n\t\t\tself._tzinfo = dt.tzinfo\n\t\n\tdef utcoffset(self, dt):\n\t\t\"\"\"\n\t\tSimilar to:\n\t\t:meth:`fromutc`, called on :meth:`fromutc.utcoffset` to"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    \n\tdef default_timezone(dt, dt_tzinfo):\n\t    return dt_tzinfo.tzname(dt)\n\t\nclass Locale(object):\n    \"\"\"\n\tThe locale of a ``datetime`` object (see :py:class:`datetime.tzinfo`)\n\tused for parsing a :py:class:`datetime.datetime` string.\n\t\n\t.. doctest::\n\t\t\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> dt = datetime.datetime(2011, 3, 15, 12, 30, 00)\n\t    >>> print(tzoffset(dt, tzoffset.FixedOffset(None, -15)))\n\t    E"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t     \n\t# If the timezone is naive, return an integer\n\tif dt is None: dt = timezone.utc\n\t\n\t# If the timezone is present, convert timezone aware datetime\n\ttry: dt = dt.astimezone(tzinfo)\n\texcept ValueError, error:\n\t\tdt = timezone.utc.localize(dt, is_dst=False)\n\t\n\t# If the timezone is not, try explicit time zone\n\ttry:\n\t\ttz = dt.astimezone(timezone)\n\texcept ValueError:\n\t\ttz = timezone.utc.localize(dt, is_dst=False)\n\t\t\n\t\t#"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    if dt is not None:\n\t\t return dt\n\t\t\n\t\ttz_obj = tzinfo()\n\t\tdt = tz_obj & default_tzinfo(dt, tz_obj.tzinfo)\n\t\tdt = tz_obj | default_tzinfo(dt, tz_obj.tzinfo)\n\t\ttz_obj = tz_obj & default_tzinfo(dt, tz_obj.tzinfo, 0)\n\t\tdt = tz_obj & default_tzinfo(dt, tz_obj.tzinfo, 23)\n\t\tdt = tz_obj & default_tzinfo(dt, tz_obj.tzinfo, 59)\n\t\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    return dt\n\n\nclass FixedOffsetTimezoneBase(BaseOffsetTimezone):\n    \"\"\"\n\tThis is the base class to implement :py:class:`FixedOffsetTimezone`.\n\t\n\tIt adds the ``timezone`` and ``default_tzinfo`` keyword arguments.\n\t\n\tAlso the ``tzinfo`` argument may be used to provide any\n    timezone data used when parsing a time zone based on its ``DT`` string.\n\t\n\tIf the string ``tzinfo`` is ``None``, then *all* tzinfo data are used.\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign\n\tto"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\t\n\t## The following makes it possible to create a timezone object from a\n\t##  timezone string to a :py:class:`TimeDelta` instance (like :py:class:`datetime.time`)\n\t## and then use it instead of the default factory: \n\ttimezone_func = time.localtime\n\t\n\t\n\t\n\ttzinfo = timezone_func(localtime(dt).utctimetuple())\n\t\n\treturn tzinfo\n\n\n###########################################################\n##\n## Base Time and datetime classes\n#\n###########################################################\nclass base_time_class(list):\n\t\"\"\"\n\tBase class for time and datetime classes.\n\t\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    assert(dt.tzinfo is not None)\n\t    if tzinfo is None:\n\t       tzinfo = default_tzinfo(dt, tzinfo)\n\t    return tzinfo.replace(tzinfo=dt)\n\t    \n        return dt\n    \n    \n    def _fix_tz_format_to_aware(s, dt):\n\t\"\"\"\n\tReturns a date formatted according to naive datetime objects\n\t\"\"\"         \n\tassert(type(s) == str)\n\tassert(dt.tzinfo is None)\n\tdt = dt.replace(tzinfo=get_default_tz())\n\treturn dt        \n    \n    def _fix_tzinfo_"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t     return timezone.now().replace(tzinfo=dt, tzinfo=tzinfo).replace(tzinfo=None)\n        \n    class UTCZone(tzinfo.FixedOffset, tzoffset.FixedOffset):\n\t\"\"\"\n\tReturns a fixed offset from UTC.\n\t\n\t.. doctest::\n\t\n\t    >>> from datetime import datetime\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> dt = datetime(2014, 1, 12, 30, 00)\n\t    >>> print(timezone.utc_offset(dt, tzinfo=tzoffset.FixedOffset(23)))\n\t    2014-01-01 12:30:00\n\n\t    :param dt:\n\t    The"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n    def default_time_zone_offset(dt, tz = 'UTC', ambiguous=True, ambiguous_explicit=False):\n\t\"\"\"\n\tSets the ``tzoffset`` parameter on naive datetimes only\n\t\n\tIf ``ambiguous`` or ``ambiguous_explicit`` is passed to ``_tzinfo_utcoffset``\n\tor ``_tzinfo_dst`` instead of ``_tzinfo_dst_old``.\n\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t    >>> from dateutil.tz"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t    \n\tif is_naive(dt) and default_tzinfo!= tzinfo:\n\t\tdt = default(dt, tzinfo)\n\t\t\t\n#      \n\t# \tprint(dt)\n\t# \tprint(dt.tzinfo)\n\t# \tprint(dt.isoformat())\n\t\n\t# \ttzoffset(dt, tzinfo)\n\tif is_naive(dt) and default_tzinfo!= tzinfo:\n\t\tdt = default(dt, tzinfo)\n\t\t\t\t\t\n\t# \tprint dt\n\t# \tprint('UTC tzinfo set to %s' % tzinfoinfo(dt))"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\treturn dflt_tz\n\n\nclass get_current_timezone():\n\t\"\"\"\n\tReturns ``get_current_timezone()`` (a naive :py:class:`datetime.tzinfo`)\n\t\"\"\"\n\treturn get_current_timezone_utc\n\t\n\nclass get_timezone(object):\n\t\"\"\"\n\tProvides classes which can be set up on :py:class:`set_timezone, get_current_timezone`, etc.\n\t\"\"\"\n\tdef __init__(self, value, timezone):\n\t\tself.value = get_timezone_internal(value)\n\t\tself.timezone = make"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\tif before is not None and not after is not None:\n\t\t\tself.split_line(before, after)\n\n\t\tif before is None:\n\t\t\t# This is a plain line before the begining of the file. If\n\t\t\t# we got a file then we have a cut and leave the line alone.\n\t\t\tself.split_line(self.get_cut_at(self.get_line_index(0))[0], self.get_end_line())\n\n\tdef get_cut_at(self, start: int, end: int) -> None:\n\t\t\"\"\""}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\nclass CutToSlice(CutToSlice):\n\t\n\t\"\"\"\n\tRestrict CutterSlice object to a subset of the previous portion of the\n\tfilename. \n\t\n\tArgs:\n\t    before: File to be considered due to cut.\n\t    after: File to be considered due to cut.\n\t\"\"\"\n\nclass SliceToCut(SliceToCut):\n\t\n\t\"\"\"\n\tRestrict CutterSlice object to a subset of the previous portion of the\n\tfilename.\n\t\n\tArgs:\n\t    before: File to be considered due to cut.\n\t    after: File to be considered due to cut.\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\nwith open(self.filename, \"rb\") as f:\n\tlength = self.chunksize\n\t\n\t# Prepare cutting buffer, one chunk per position\n\tstart = 0\n\tend = before\n\tchunk_size = length / (length + 1)\n\t\n\tlines = f.readlines(chunk_size)\n\t\n\tfor line in lines:\n\t\tstart = start * chunk_size\n\t\tend = start * chunk_size\n\t\tself.lines[start:end] = line\n\t\n\t# Split and process each line\n\tf.seek(end - start, 1)\n\tstart"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\ndef delimit_slice_point(points: Iterable[bytes]):\n\t\"\"\"\tThis function replaces the start and end points with\n\tany bytes in the file.\n\t\"\"\"\n\t\ndef remove_from_lines(lines: Iterable[bytes], pos: int, delimiter: bytes) -> bytes:\n\t\"\"\"\tReplaces the given string with any remaining positions.\n\tArgs:\n\t    lines: The lines to remove from the slice.\n\t    pos: The position to replace lines at.\n\t    delimiter: The original delimiter.\n\tReturns:\n\t    The remainder of the slices.\n\t\n\t\"\"\"\n\t\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.__cut_chars = before.replace(before[0], before[1])\n\t\tself.__cut_string = after.replace(after[0], after[1])\n\n\tdef cut(self) -> bytes:\n\t\t\"\"\"\n\t\tCut the file.\n\n\t\t\"\"\"\n\t\tself.__cut_chars = self.__cut_string.replace(self.__cut_chars, \"\")\n\t\tself.__cut_chars = self.__cut_chars.replace(\"\\\\\", \"\\\\:\")\n\n\t\treturn self.__cut_chars\n\n\tdef cut_at(self, before: bytes, after: bytes)"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t     \n\twith self.get_open_mode_lock():\n    \tself._start_pos = 0\n\tself._slice_buffer = bytearray(0)\n\tself._chunk_size = 0\n\tself._chunk_offset = 0\n\tself._next_chunk = []  # type: bytes\n\tself._chunk_offsets = []  # type: list[int]\n\tself._slice_size = 0\n\tself._chunk_pos = 0\n\n\tfor chunk in before:\n\t    self._chunk_offsets.append(chunk - self._start_pos)\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t      \n\tself.clear()\n\t\n\t\n\tfor i in range(before, after + 1):\n\t    self.save()\n\ndef cut_and_cut(infile, cutfunc, after=len(infile)):\n\n    \"\"\"\n    Input:\n\t    infile: File input.\n\t    cutfunc: Function that cuts file before given delimiter.\n\t    after: File after given delimiter.\n\t            \n\tReturn:\n\t\t    File input.\n\t\"\"\"\n    pos = 0\n\twith open(infile, \"r\") as infile:\n\t    for line in infile:\n\t \t\tfound_pos = not found_pos\n\t \t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\nclass Range(object):\n\t\"\"\"\n    A basic representation of a range.\n\n\tInternally, this class wraps around a string.  This is used as a\n    separator (string that can be closed or moved at both ends).  The\n    string representation of an object will be represented in the\n    range itself.\n\t\"\"\"\n\tdef __init__(self, length: int=None, line: int=0):\n\t\t\"\"\"\n\t\tArgs:\n\t    length: Length of the range.\n\t    line: Line of the range.\n\t    \"\"\"\n\t\tself.num_lines: int=0\n\t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.cut_chars = self.cut_chars.replace(before, after)\n\t\t\n#\n# STEP 1: parse the chunk by chunk boundary.  This follows\n# what would be done internally by the chunker and the chunker will create\n# chunks for each chunk in a new chunker.  (The first chunk should\n# be of the form \" \" * (3-3), but that can be different at the same\n# time). \n\nclass Chunker(object):\n\t\"\"\"\n\tWrapper around chunked() so that there are no extra overhead\n\tfrom seek"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\tself.before = before.rstrip(self.after)\n\t\tself.after = after.rstrip(self.before)\n\n\tdef set_no_limit(self) -> None:\n\tself.no_limit = False\n\n\tdef set_no_header(self) -> None:\n\t\tself.no_header = False\n\n\tdef remove_all_chars(self):\n\tself.after = self.before.split(self.after)\n\n\tdef set_no_line_terminator(self) -> None:\n\t\tself.no_line_terminator = False\n\n"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\treturn request.has_request_type(requestType) and request.identifier() == requestType and request.is_legacy_msg()\n\t\"\"\"\n\tTry the message to identify if this is a legacy message. (Does not work correctly if more than one message is required.)\n\t\"\"\"\n\t\n# If this is a divisp message, return an error, otherwise return the \"UnknownMessage\", else return the Diaspora message.\n# If this is a divisp message, return an informative error message, otherwise return the Diaspora username.\ndef check_in_"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t\n\tif request._user_agent is None:\n\t   print \"No user agent found. Doing nothing to check.\"\n\t   return\n\t\n\tmatch = request._matches()\n\t\n\tif match.user_agent:\n\t   print(\"Request contains user agent {}\".format(match.user_agent))\n\t   print(\"You have the following information:\\n\\n{}\".format(match.pretty()))\n\t   print(\"\")\n    \t\n\t\n\tif not match.is_valid():\n\t   print(\"Invalid user agent. Ignoring this message.\")\n\t   return\n\t\n\tmessage = match.as_string()\n\t\n\tif _is_legacy_payload(message"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\ndef encode_request(message: Message, hash_fun: Callable):\n\t\"\"\"\n\tEncode the request to a message type in the Diaspora message, using the message's key, the corresponding encoded hash function. This function converts a request to the correct message type.\n\t\n\tIf the message's hash function was given an existing hash_fun, then that hash is considered as a public message.\n\t\n\tWill raise error if the hash didn't have anything.\n\t\"\"\"\n\t\ndef parse_request(request: RequestType):\n\t\"\"\"\n\tParses a request into a message type in the D"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\n\tif request.delivery not in MessageDelivery.SUPPORTED:\n\t\ttry:\n\t\t\tmessage = DataService.getMessage(request)\n\t\t\tif message is None:\n\t\t\t\traise InvalidDataException(\"No message for: %s\" % request)\n\t\texcept DiasporaMessagesException as e:\n\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n\t\t\n\t\treturn message[\"event\"] == \"public\" and message[\"subject\"]\n\telse:\n\t\ttry:\n\t\t\treturn DataService.isPublicHosted(request.delivery)\n\t\texcept Exception as e:\n\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t return request.type in [MessageType.REGISTERED, MessageType.CONFIRMED]\n\n\traise Exception(\"{} isn't a public message\".format(request))\n\ndef get_request(request: RequestType) -> Request:\n\t\"\"\"\n\tReturn a Diaspora request based on the specified Request type.  Can return a Diaspora event or a legacy payload\n\t\n\t@param request: The Diaspora request to get.\n\t\"\"\"\t\n\tif request.type == MessageType.REGISTERED:\n\t\treturn register_request(request)\n\t\t\n\telif request.type"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t# get_credentials (session parameters from session)\n\tcredentials = get_credentials(request)\n\t\n\t# check that username, password is a valid Diaspora request. If it isn't, a 404 is displayed\n\tif credentials is None and request.get_form(\"loginPassword\").is_valid():\n\t\treturn None\n\t\n\t# get_request (session variables from session)\n\trequest.add_data(identifier=u\"session\",\n\t   identifier_password=credentials.get_hash(),\n\t   issuer=identifier=request.get_form(\"issuer\").get_text"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\ndef is_new_block:\n\t\"\"\"\n\tCheck if this is a new block.\n\t\n\tReturn True if this is a new block.\n\t\"\"\"\n\t\ndef is_message_sent(message: MessageType, block_index: int = 0) -> bool:\n\t\"\"\"\n\tCheck if this message needs to be sent in a new block.\n\t\n\tIf this is a message sent a message is sent and a new block is added to this block.\n\tThis might be because the message is not confirmed, and it could require\n\tmore interactions when sending a message to this block"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n#\ttry:\n\t\tservice = SERVICE_REGISTER(request)\n\texcept Exception as e:\n\t\tlogging.error(\"Unable to detect the request type %s : %s\", service.name, e.args)\n\t\treturn\n\t\n\tservice = ServiceRegistry.add_handler(service, \"request\", identify_request)\n\t\n\treturn service\n\ndef find_service(request: RequestType, service_name: str) -> ServiceRegistry:\n\t\"\"\"\n\tHelper function to return the appropriate Diaspora request.\n\t\n\tReturns an empty ServiceRegistry instance if the request is not an"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t\n\tdef process_request(data: str) -> bool:\n\t\tif data == \"ok\":\n\t\t\treturn process_response(request.payload)\t\t\n\t\telif data == \"fail\":\n\t\t\treturn process_response(\"Unable to identify request.\")\n\t\telif data == None:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn process(data)\n\t\n\treturn process_request(data_for_check=True)\n\ndef process_response(payload: str) -> str:\n\tif type(payload) is dict:\n\t\tif data_from_json is not None:\n\t\t"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\tassert'status' in request\n\t\treturn request['status'] >= 400\n\telif'status' in request:\n\t\treturn request['status'] >= 400\n\ndef prepare_message(request: RequestType):\n\t\"\"\"\n\tPrepare the message to send through the network.\n\t\"\"\"\n\t\tif request['status'] >= 403:\n\t\t\traise InvalidAPI(500, 'Forbidden', 401)\n\t\treturn request\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2011 Pexego Sistemas Inform\u00e1ticos All Rights Reserved\n"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n#\t\n#\t\n#\t\tif (\n#\t\t\trequest.identity == 0 and\n#\t\t\trequest.origin_id == MATCH_ORIGIN_HANDLE and\n#\t\t\trequest.origin == \"\" and\n#\t\t\trequest.method == \"POST\"\n#\t\t) or\n#\t\t\t\t\n\t\treturn True\n\t\n\telif ((request.code is not None and request.code!= request.get(\"code\"))\n\t\tor (request.origin is not None and request.origin!= request.get(\"origin\"))):\n\t\t#print \"Failed to identify request with code:"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\ttry :\n\t\t\treturn request.api_request\n\t\texcept ValueError:\n\t\t\treturn False\n\ndef identify_response(response: ResponseType) -> bool:\n\t\"\"\"\n\tTry to identify whether this is a Matrix response\n\t\"\"\"\n\t\ttry :\n\t\t\treturn response.api_response\n\t\texcept ValueError:\n\t\t\treturn False\n\ndef get_user_info(\n\tself,\n\tapi_request: RequestType,\n\tapi_response: ResponseType,\n\tapi_error: ErrorType\n) -> bool:\n\t\"\"\"\n\tReturn user details of this matrix"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\ttry:\n\t\t\t_ = request.uri\n\t\texcept AttributeError:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n#Copyright (c) 2012, Daniel Glietzyk\n#\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the \"Software\"), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and/"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\tif request.user and not (request.user.is_superuser or request.user.is_managed):\n\t\t\treturn False\n\t\telse:\n\t\t\tif not request.user.is_superuser:\n\t\t\t\treturn False\n\t\t\treturn True\n\n\treturn False\n\ndef get_request_for_user(request: RequestType) -> Optional[RequestType]:\n\treturn get_request(request=request, return_users=True)\n\ndef get_request(request: RequestType=None, return_users: bool=False) -> Optional[RequestType]:\n\t\"\"\"\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n\tdata: dict[str, Any] = {}\n\tif request.user == UserProfile.get_current_time():\n\t\t# If this user has no matrix, assume it's a Matrix request\n\t\treturn False\n\t\n\ttry:\n\t\tdata['user'] = request.user\n\texcept AttributeError:\n\t\treturn False\n\t\n\t# Now, let's ensure that a user can handle this user via 'update' or'request'\n\tif not request.client.is_authenticated:\n\t\tdata['client'] = request.client\n\t\n\tif data['client'] == request.client:\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\tif not isinstance(request, RequestType):\n\t\t\treturn False\n\t\tif request.is_matrix:\n\t\t\treturn True\n\treturn False\n\n\ndef add_matrix_request(x: Union[List[Any], List[Any]],\n\t\t\t\t\t\tmatrix: List[Any],\n\t\t\t\t\t\tinclude_matrix_source: bool,\n\t\t\t\t\t\texclude_matrix_source: bool,\n\t\t\t\t\t\tinclude_matrix_token: bool,\n\t\t\t\t\t\tis_array: bool,\n\t\t\t\t\t\tis_map: bool,\n\t\t\t\t\t\tis_array_item: bool,\n\t\t\t\t\t\tis_map_item: bool,\n\t\t\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn False\n\n\tdef identify_response(request: ResponseType, request_id: int) -> bool:\n\t\"\"\"\n\tTry to identify whether this is a Matrix response\n\t\"\"\"\n\t\treturn False\n\n\nclass MatrixRequest(ObjectType):\n\t\"\"\"\n\tA subclass of object that implements both the Matrix and other request types for performing matrix operations\n\t\"\"\"\n\n\tdef __init__(self, connection: ConnectionType, connection_id: int, max_rows: int, table: TableData, max_columns: int, parent: Optional[\"Object\"] = None) -> None:\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn True\n\ndef handle_request(request: RequestType, user_data: Dict[str, Any]) -> None:\n\t\"\"\"\n\tTry to handle requests that don't match our data\n\t\"\"\"\n\t\ndef build_matrix_request(request: MatrixRequestType, data: MatrixData) -> MatrixRequest:\n\t\"\"\"\n\tCreate a new MatrixRequest object that has no request as our data\n\t\"\"\"\n\t\ndef handle_matrix_request(data: MatrixResponse) -> MatrixRequest:\n\t\"\"\"\n\tMake a new matrix request from our data\n\t\"\"\"\n\t\ndef"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\n# We want to look as a different client than the server is in here!!!\n# We must handle this case if this client is a \"non-zero\" server, for a very large\n# time, this will work for the purposes of testing\n\tif request.client.address.startswith(\"http://\"):\n\t  return True\n\telif request.client.address.startswith(\"https://\"):\n\t  return True\n\telif request.client.address.startswith(\"www.\"):\n\t  return True\n\telse:\n\t  return False\n\ndef get_matrix_id():\n\treturn get_unique_id(config"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t\treturn True\n\ndef is_valid_request(request: RequestType, data: dict) -> bool:\n\treturn data['is_valid']\n\n\ndef is_batch_request(request: RequestType) -> bool:\n\treturn request.request['batch_id'] == request.request['request_id']\n\ndef is_bulk_batch_request(request: RequestType) -> bool:\n\treturn request.request['batch_id'] == request.request['request_id'] and not is_valid_request(request)\n\ndef get_matrix_by_"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.strftime('%H:%M:%S')\n\ndef format_date(dt):\n\t\"\"\"\n\tFormat a datetime in a human readable way.\n\t\"\"\"\n\t\treturn dt.strftime('%Y-%m-%d')\n\n\ndef main():\n\t\"\"\"Main.\"\"\"\n\n\t# Create main objects.\n\twith open(\"users.csv\") as users_csv, open(\"users_users2.csv\") as users2_csv:\n\t\tfor line in users_csv.readlines():\n\n\t\t\t# Get datetime.\n\t\t\tdt = line.split(\"\\t\")[0].strip(\""}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.strftime(\"%Y%m%d\")\n\tdef format_dd(dd):\n\t\t\"\"\"\n\t\tFormat a timedelta in seconds which gives the number of days\n\t\tsince the epoch.\n\t\t\"\"\"\n\t\treturn int((dd.days * 24 * 3600) // 60)\n\tdef format_ddm(ddm):\n\t\t\"\"\"\n\t\tFormat a timedelta in minutes which gives the number of days\n\t\tsince the moment.\n\t\t\"\"\"\n\t\treturn int((ddm.days * 60) // 60)\n\tdef format_hms(hour):\n\t\tt = datetime"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.strftime('%s %H:%M:%S,%f %Y')\n\ndef get_dt_formatted(dt):\n\t\"\"\"\n\tGet the given date and return a formatted date format with 'long'\n\tformats. The date is formatted as a decimal format, e.g. '2010-05-05' is the\n\tfollowing format: 'Y-mm-dd'\n\t\"\"\"\n\t\n\tfullDateTime = dt.strftime('%Y-%m-%d')\n\tfullDateTime = fullDateTime[:30] + 'Z' + fullDateTime[30:]\n\t\n\tif"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn time.strftime('%H:%M:%S', time.strptime(dt, '%Y-%m-%d %H:%M:%S'))\n#\tend if\n\n\n#------------------------------------------------------------------------------------#\n# Functions\n#------------------------------------------------------------------------------------#\n\ndef is_valid_d(d):\n\t\"\"\"\n\tDetermine if the given datetime object is of a valid format.\n\t\"\"\"\n\t\tif not is_day_string(d): # invalid format\n\t\t\treturn False\n\t\t# convert to a date\n\t\tif not d.endswith('+00:00'): # invalid format\n\t\t\td = d +"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\tstring = ''.join((str(i) for i in dt.to_pydatetime()))\n\t\treturn string\n\n\ndef to_dt_string(dt):\n\t\"\"\"\n \tConvert a datetime in the way that D* nodes expect. E.g. '11.22.33.44', '11:22:33.44'\n \t\"\"\"\n\t\tif dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:\n\t\t\treturn \"Invalid datetime: %s\" % str(dt)\n\t\treturn dt.astimezone(dt - dt.utcoffset(dt))\n\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n#------------------------\n# Function: load_from_file(file: str, user_folder: str, project_folder: str) -> bool\n#------------------------\n\n\ndef load_from_file(file: str, project_folder: str, directory_name: str, project: str, file_ext: str):\n#    \"\"\"\n\tFunction to load a file specified by file and return whether it is OK (True/False).\n\n\tParameter\n\t----------\n\tfile: str\n\tproject_"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\tif not allow_negative:\n\t\t\traise ValidationError( \"Invalid date format specified. Date format is 'yyyy-mm-dd'\" )\n\t\tif isinstance(dt, (datetime.datetime,datetime.date)):\n\t\t\treturn dt.strftime( '%H%M%S')\n\t\tif not isinstance(dt, datetime.datetime):\n\t\t\traise ValidationError(\"Invalid date format specified. Valid date/time formats are 'yyyy-mm-dd' (HH:MM:SS) or 'YYYY-MM-DD'\")\n\tformat_dt = None\n\tfor d in dt:\n\t\tif isinstance("}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\n\td = dt.days\n\tedate = d+relativedelta(months=-1)\n\tdt_str = \" - \".join([dt.strftime(\"%d\")[:10], d*edate])\n\n\treturn dt_str\n\n\n#  Copyright (C) 2016 - 2018 Ivan Krajets <me@it-ikrajets.eu>\n#\n#  This program is free software; you can redistribute it and/or modify it under\n#  the terms of the GNU General Public License as published by the Free Software\n#  Foundation; either version 2 of the"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\tm = d2datetime.date2dt(dt, dt)\n\t\treturn m.strftime('%Y-%m-%d %H:%M:%S')\n\ndef write_output(text, force=False):\n\t\"\"\"\n\tWrite a given message to the file in the given mode.\n\t\"\"\"\n\t\tprint text.decode()\n\tif force:\n\t\tprint \"Would you like to write to all or just write changes?\"\n\ndef save_file(fileName, fileName, force = False):\n\t\"\"\"\n\tSave the file in the given directory.\n\tCan"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t\t# Convert string to python datetime\n\t\t#if dt.strip():\n\t\tif re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', dt):\n\t\t\tdt = (dt.strip() + 'Z') % 1\n\treturn dt\n\n\ndef timestamp_to_datetime(timestamp):\n\t''' Return a local datetime object for a given timestamp. '''\n\tdt = timestamp_to_dict(timestamp)\n\treturn dt['date']\n\n\ndef timestamp_to_dict(timestamp):\n\t''' Return a dictionary based on the timestamp returned by"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\t# Ensure we have a list of tags\n\tif isinstance(text, str):\n\t\ttext = [text]\n\t\n\t# Ensure that all tags are strings.\n\tif not all(isinstance(t, str) for t in text):\n\t\traise ValueError('Invalid arguments to find_tags: \"%s\"' % text)\n\t\n\t# Search for all substrings enclosed by the tag. If we get here then\n\t# any whitespace and any punctuation characters are not valid so we \n\t# can't use _strip().\n\ttext = [t for t in text if re.match('(?<!"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\tif \"re\" in text or repl:\n\t\t# if the replacer fails (e.g., if the input is the same\n\t\t# as in regular expression), or it fails (e.g., if the input is to\n\t\t# be replaced by re.match (not equal to the input).)\n\t\twith open(text, 'r', encoding='utf-8') as f:\n\t\t\ttext, regex = f.read(), re.match(replacer, text)\n\t\ttext = text.lower()\n\t\t\n\t\tif text.lower().find(' ')!= -"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\treplacer = replacer or _re_punctuation\n\t\n\t\n\treplacements: Set[str] = set()\n\tdef find_tag(text):\n\t\tif text.startswith(\"?\") or text.startswith(\"!\"):\n\t\t\ttext = text[1:-1]\n\t\t\tif _in_tag_block(text):\n\t\t\t\ttags = []\n\t\t\t\tfor block in _find_block(text):\n\t\t\t\t\ttags.extend(replacer(_find_block(block)))\n\t\t\t\treturn tags, text\n\t\t\ttext = text[1:]\n\t\t\tif _in_tag_"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\n\t# if there's no tag or no replacer, then return None\n\tif not replacer:\n\t\treturn set()\n\treturn set(text.split())\n\t\ndef find_code_blocks(code: str, replacer: callable = None) -> Set[str]:\n\t\"\"\"\n\tFind the code blocks enclosed by a code, such as \\n\\n and \\r.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the code word with the original block\n\tcontaining the replacer using the replacer keyword.\n\t\n\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\tfound_tags = set()\n\ttext_count = len(text)\n\t\n\t# find longest matches first\n\ti = 0\n\twhile i < text_count:\n\t\tif replacer and text[i] in found_tags or text[i] in found_tags:\n\t\t\tbreak\n\t\ti += 1\n\t\n\tfor tag_str in text:\n\t\tif replacer:\n\t\t\tif tag_str in found_tags and tag_str in found_tags:\n\t\t\t\tbreak\n\t\t\tfound_tags.add(tag_str)\n\treturn found_tags"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\ttext, tags = re.split(text, \"\")\n\t\n\tfor tag in tags:\n\t\tif re.match(\"^\" + tag + \"$\", text, re.IGNORECASE):\n\t\t\tbreak\n\t\t\t\n\treturn set(tags)\n\n\ndef regex_match(pattern: str, text: str) -> bool:\n    \"\"\"\n    Returns if matched regex.\n\tRegex may be a regular word or word template.\n\t\n\tIt is up to you to parse tags with special syntax.\n\t\n\tUsage: parse_tags(pattern, text) -> bool\n\t\"\"\"\n    text = text.strip()  #"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\tregex: Str\n\ttags = []\n\t# TODO: this could probably use a more specific filter to test for a regexp.\n\tkeywords = {}\n\t\n\ttry:\n\t\tkeywords, regex = re.compile(regex)\n\texcept re.error:\n\t\tpass\n\t\t\t\t\t\t \n\t\t\n\tmatch = regex.search(text)\n\tif match is not None and match.group() in keywords:\n\t\treturn keywords, match.group(0)\n\t\t\n\tif len(text) == 0:\n\t\treturn words, text\n\tif match.group()[0].isspace():\n\t\treturn"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n        if replacer is None:\n            replacer = lambda text, repl: repl.sub('', text)\n        \n        rstripped = re.sub(r'\\[\\[.*?\\]\\]', repl, text)\n        return set(rstripped.split(' '))\n\n\"\"\"\n\tA global variable to help keep track of current state of all tags in a list.\n\t\n\tThe list of tags can be changed from one another but this can not be saved.\n\t\n\tThe class variable __init__.py contains:\n\t- a list containing all tags from one another containing: [TAG]\n\t- a single tag is"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t\n\t# TODO: this is not really required\n\t# I will add the ability to add tags to a file called \"text\" into the file \"output\",\n\t# in some cases where a new tag is inserted, but it's possible a different way to do so\n\t# this doesn't work. \n\tif (\n        isinstance(replacer, Callable) is True\n    ) or isinstance(replacer, Iterable) is False:\n\treturn set(list(text)) - set(replacer)\n\t# I will not use the function `find_last_word_index`,"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t return set(\n\t\t\tline.strip() if replacer is None else replacer(line).strip()\n            for line in text.split('\\n')\n\t\t\tif '\\n' in line)\n\nclass TagTextView(View):\n\t\"\"\"\n\n\tA View that displays text using a set of tags.\n\t\n\tFor example:\n\t\t\n\t\t@app.route('/')\n\t\tdef index():\n\t\t\treturn Response('Hello world in a cat.')\n\n\t\t@app.route('/tags')\n\t\tdef tags():\n\t\t\ttags = [line.strip() for line in next(sys.stdin)."}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t    \t# Process references to text, including sublinks\n\t    \t# If the page has non-ascii whitespace, there's no way\n\t    \t# to properly fix Unicode issues.\n\t\tif 'wiki.wikimedia.org' in text:\n\t\t\treturn\n\t\t\n\t\t# Look up external links, looking for possible encoding issues from\n\t\t# the text.\n\t\ttry:\n\t\t\n\t\tfor link in text:\n\t\t\tif link in ('&', '\\b', u'\\b'):\n\t\t\t\tlink = r'\\b[%s\\d]' % link\n\t\t\t\tlink = link.encode('utf8')\n\t\t\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\tfor item in text:\n\t\t\tif not ':' in item:\n\t\t\t\tcontinue\n\t\t\tname, attrs = item.split(':', 1)\n\t\t\tlink = self._linkify(attrs['id'])\n\t\t\tlinks.append((name, link))\n\n\tdef process_url_attributes(url):\n\t\t\"\"\"\n\t\tProcess links in the web page, adding a text link around links to the <a>\n\t\telement and adding its \"href\" URL.\n\t\t\"\"\"\n\n\t\tfor item in url:\n\t\t\tif not ':' in item:\n\t\t\t\tcontinue\n\t\t\tname, attrs = item.split(':',"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\tif not isinstance(text, (list, tuple)):\n\t\t\ttext = [text]\n\t\tfor link in Link.objects.filter(id=text[0]):\n\t\t\tfor attr_name, attr_val in link.attr_value_links.items():\n\t\t\t\tlink.attr_value.add(attr_val)\n\t\t\t#if attr_val.rel_url in text[1:]:\n\t\t\t#\tfor attr in text[1:]:\n\t\t\t#\t\tlink.attr_value.remove(attr)\n\ndef create_or_update_link_from_text("}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\tif sys.version_info < (3, 0):\n\t\t\ttext = text.decode(\"utf-8\")\n        for match in re.finditer(r'#.*<a href=([\"\\']).*?>', text):\n            external_href = match.group(1) + match.group(2)\n\t\t\tsource = match.group(3)\n\t\t\ttitle = match.group(4)\n\t\t\tis_page = match.group(15)\n\n\t\t\tsource = unescape_html(source)\n\t\t\turl = unescape_html(url)\n\t\t\tif is_page:\n\t\t\t\tsource = \""}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\tfor field, text_link in links.items():\n\t\t\tif hasattr(self, 'links_%s' % field): \n\t\t\t\tself.links_%s(self, field, text_link)\n\t\t\telse:\n\t\t\t\tsetattr(self, field,text_link)\n\t\t\t\t\n\tdef process_tags(self):\n\t\"\"\"\n\tProcess all special cases: links with text like `#` will work and\n\tthen tags with `#` will be parsed as tags.\n\t\"\"\"\n\t\ndef parseSmallestAttributes(text):\n\tdef get_name(x):\n\t\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\n\tglobal text_links\n\n\tlines = text.splitlines(True)\n\tfor link in lines:\n\t\tif link:\n\t\t\tlink = link.strip()\n\t\t\tif is_regex(link):\n\t\t\t\ttext_links.append(link)\n\t\t\telif len(link) > 1 and len(link) < 5:\n\t\t\t\tlink = link.strip()\n\n\tfile_name = sys.argv[0].split('/')[-1]\n\tif file_name:\n\t\tfile_name = file_name.split('.txt')[0] + '.txt'\n\t\tlinks_name"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\ttext = parse_html(text, 'text')\n\t\ttry:\n\t\t\tfor link_type, href, title in text:\n\t\t\t\tlink_text = linkify_link_text(href, title)\n\n\t\t\t\ttry:\n\t\t\t\t\tparsed = process_link(link_text)\n\n\t\t\t\texcept:\n\t\t\t\t\tprint \"Error processing link: \" + link_text\n\n\n\t\t\t\ttry:\n\t\t\t\t\tparsed = process_inline_links(parsed)\n\n\t\t\t\texcept:\n\t\t\t\t\tprint \"Error processing inline link: \" + parsed\n\n\t\t\treturn True\n\t\texcept:\n\t\t\tprint \"Error processing link:"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\ndef process_linked_links(lLinkList):\n\tfirst = False\n\tfor lLink in lLinkList:\n\t\tlLinks = ''\n\t\tfor lLink in lLinkList:\n\t\t\tif lLink.text is not None:\n\t\t\t\tif first:\n\t\t\t\t\tfirst = False\n\t\t\t\tlLinks += '<a href=\\''+lLink.href + '\\\">'\n\t\t\t\tlLinks +='<a href=\\''+lLink.href + '\\\">'\n\t\t\telse:\n\t\t\t\tlLinks += '\"/>\\''\n\t\t\t\tlLinks +='<a href=\\''+lLink"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n#\tlinks = text.xpath('//table[@id=\"linksTable\"]//tr[@id=\"linkList\"]//td[1]/a')[0::]+[a.text for a in text.xpath('//td[1]/a')]\n\t\n\t\n\t#\tnow add the original text\n\ttext = '<div class=\"wiki-link\">'\n\tfor link in links:\n\t\tlink.replace(text)\n\t\t\n\treturn text\n\n\n\"\"\" Create a wikitext item with no link (if it hasn't added, but not yet added)\n\"\"\"\n#\tprint \"create wikitext item\"\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\tfor line in text.replace('\\n','').split('\\n'):\n\t\t\tif line and \" \" in line:  # remove whitespace, comments and comments line\n\t\t\t\ttry:\n\t\t\t\t\tlink = Link(line.strip())\n\t\t\t\texcept Exception, e:\n\t\t\t\t\tprint(e)\n\t\t\t\t\tlink = None\n\n\t\t\tif link:\n\t\t\t\tLink.insert(link, link.get_text())\n\t\t\t\tlinkify_text = re.sub(r'<span[^>]*', '', line).strip()\n\t\t\t\ttry:\n\t\t\t\t\tif linkify_text:\n\t\t\t\t\t\tlinkify = Linkify(linkify_"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t return urllib3.utils.get_auth(url).headers['ACCEPT']\n\t\ndef parse_url(url: str) -> Optional[str]:\n\t\"\"\"\n\tParse the URL into a :py:class:`urllib3.response.HTTPResponse`:\n\tif not redirected, or content type is not provided we assume a remote url.\n\t\"\"\"\n\tif url.startswith(WEB_URL):\n\t\treturn _parse_web_url(url)\n\telse:\n\t\treturn _parse_raw_url(url)\n\ndef _parse_raw_url(url: str)"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\tresponse = requests.get(str(url))\n\texcept requests.exceptions.ConnectionError as e:\n\t\tlogging.error(\"Failure downloading the file.\")\n\t\tif e.args[0]!= 404:\n\t\t\traise\n\tif response.status_code == 201:\n\t\tresponse_content_type = response.headers.get('Content-Type', None)\n\t\tif response_content_type is None:\n\t\t\tresponse_content = response.content\n\t\t\tif 'image/png' in response.headers['content-type']:\n\t\t\t\tresponse.headers['content-type'] = '"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\t# Get all the lines of the file\n\tstream = urlopen(url)\t\n\tcontent_type, url = stream.read().strip().split(\"charset=\", 1)\n\n\t# Find the encoding in case no encoding is set\n\tif content_type == \"binary\":\n\t\tif url.split(\"charset=\")[-1] == \"latin-1\":\n\t\t\treturn \"binary encoding\"\n\telse:\n\t\tif (url.split(\"charset=\")[-1] == \"euc-jp\" and\n\t\t\tnot url.split(\"CHARSET=\")[1]\n\t\t):\n\t\t\treturn \""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\treturn _get_response(urllib2.urlopen(url))\n\n\nclass URLHandler:\n\tdef __init__(self) -> None:\n\t\tself.url: str = None\n\n\tdef open(self) -> None:\n\t\twith open(self.url, 'r') as f:\n\t\t\tself.parse(f)\n\n\tdef parse(  # pragma: NO COVER\n\t\tself, f: str, recursive: bool, **kwargs: Any\n) -> None:\n\t\tif isinstance(f, str):\n\t\t\tself.url = requests.get(f, verify"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\turl = url or self._url\n\t\tlogger.info(\"Fetch content type from %s\", self.url)\n\t\trequest = urllib2.Request(url, headers={'User-Agent': self.user_agent},\n                                 timeout=30)\n\t\tresponse = urllib2.urlopen(request)\n\t\tresult = json.load(response)\n\t\tlogger.info(\"Successfully fetched content type from %s\", url)\n\t\tif result['data']['contentType'] in ['application/json']:\n\t\t\treturn result['data']['contentType']\n\n\n\n\tdef find_by_id(self, id"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\n\n\tparsed_type = urllib.parse.urlsplit(url)[0]\n\turl_parts = parsed_type.rsplit('@', 1)\n\tparts = urlparse.urlparse(url_parts)\n\n\tresult_types = set()\n\twhile parts.path:\n\t\tresult_types.add('application/json')\n\t\tbody, file_ = urllib.parse.urlsplit(parts.path)\n\t\tresult_types.add('text/javascript')\n\t\tresult_types.add('application/x-javascript')\n\n\treturn result_types.pop()\n\n\n\nclass BlitzGateway"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\theaders = None\n\t\theaders = fetch_basic_metadata(url)\n\n\t\tif headers:\n\t\t\theaders = extract_url(\n\t\t\t\turl,\n\t\t\t\theaders=headers,\n\t\t\t\tuse_proxy=True,\n\t\t\t\theaders=headers,\n\t\t\t\tproxies=None,\n\t\t\t\ttimeout=float(headers.get('timeout')),\n\t\t\t)\n\n\t\treturn headers\n\n\n\n\tdef update_config(self, config: dict) -> None:\n\t\"\"\"\n\tFill in config with the following items:\n\n\t - Cached URL as a string, or 'data:'. See https://github.com/"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\tresponse = get_result(\n\t\t\turlparse.urlparse(url).query,\n\t\t\ttimeout=5\n\t)\n\tif response is not None:\n\t\treturn response.content_type\n\ndef get_content_disposition_query_params(headers: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:\n\t\"\"\"\n\tRetrieve query params to determine and extract response data.\n\t\"\"\"\n\t\tif headers is not None:\n\t\t\treturn headers\n\n\t\tresponse = get_result(\n\t\t\turlparse.urlparse(url).query,\n\t\t\ttimeout=2\n\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\treq_headers = dict(\n\t\t\tapi_token=url, api_user=url, api_passwd=url, headers=req_headers, url_username=url, url_password=url,\n\t\t\tverify_ssl=True, verify_tls=True\n\t\t)\n\t\treq = requests.get(url, verify=verify_ssl, verify_ssl=verify_ssl, headers=req_headers)\n\n\t\tif req.status_code == requests.codes.ok:\n\t\t\treturn req.json()['type']\n\texcept requests.exceptions.ConnectionError as e:\n\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\tif url.startswith((\"https:\", \"ftp:\", \"ftps\")):\n\t\t\treturn fetch_type(url)\n\t\treturn None\n\ndef fetch_content_length(url: str) -> Optional[str]:\n\t\"\"\"\n\tFetch the HEAD of the remote url to determine the length of the content\n\tof the remote url.\n\t\"\"\"\n\t\tif url.startswith((\"https:\", \"ftp:\", \"ftps\")):\n\t\t\treturn fetch_length(url)\n\t\treturn None\n\ndef fetch_url(url: str, method: str, headers: List[str]=None) ->"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tword, count = tokenize.tokenize(tag)\n\t\tassert 0 <= count < len(word)\n\t\tassert tag[count] == \"w\"\n\tt, rtags = TweetTags(tokenized=True)\n\tassert \"w\" in rtags, rtags\n\t# Tokenizes as a \"w\" tag instead of a w tag, so a tag can be either \"a\" or \"b\", depending on the tag we're checking\n\tassert \"a\" in rtags, rtags\n\tassert \"b\" in rtags, rtags\n\tassert tag in r"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\ttry:\n\t\t\ttag(tag)\n\t\texcept TypeError as e:\n\t\t\tprint('Invalid type in tag: {}'.format(e))\n\n\tdef test_case(case: str) -> bool:\n\t\"\"\"\n\tTest case like \"a=b\" on a comment.\n\n\tNote: this test will fail if you pass a list of non-keyword parameters.\n\t\"\"\"\n\t\ttest_case_with_tag_case.init_from_case(case)\n\t\ttest_case_with_tag_case.case_tag(tag)\n\n\tdef test_case_"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tassert tag.lower() == \"tag\"\n\n\tdef test_tag_ignorecase(tag: str) -> bool:\n\t\t\"\"\"\n\t\tTest if tag contains the case of a special case (whitespace)\n\t\t\"\"\"\n\t\twords = (\"this is a tag\", \"this is a tag \", \"other is a tag \")\n\t\tfor word in words:\n\t\t\tassert tag in word, \"word=%s!= word=%s\" % (word, tag) \n\n\tdef test_is_numeric(self) -> bool:\n\t\t\"\"\"\n\t\tTest if a number (float) is accepted"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tname_test_data = [\"foo\", \"bar\", \"'foo'\", \"\\n    bar\", \"'\"]\n\t\twords = list(filterchars(tag, name_test_data))\n\t\tassert words[0] == \"foo\"\n\t\tassert words[1] == \"'bar\"'\n\n\tdef test_tag_to_name(pattern: str) -> str:\n\t\t\"\"\"\n\t\tMake sure that a word tag with a pattern has a string name.\n\t\t\"\"\"\n\t\tword_tags = list(filterchars(b'foo', pattern.split()))\n\t\treturn word_tags["}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tif tag is None:\n\t\t\tfrappe.msgprint(_(\"Word is mandatory\"), raise_exception=True)\n\t\t\treturn False\n\t\tif \"v\" in tag:\n\t\t\tcontinue\n\t\tif \"s\" in tag:\n\t\t\tcontinue\n\t\tif \"w\" in tag:\n\t\t\tcontinue\n\n\n\t\tif frappe.db.exists('Document', tag):\n\t\t\t\treturn True\n\n\n\t\tfrappe.msgprint(_(\n\t\t\t\"Tag {0} cannot be parsed as \"\n\t\t\t\"existing document: {1}\")\n\t\t\t.format(tag, frappe.bold(frappe.bold(__file__)."}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\treturn tag == tag_to_bool(tag)\n\nclass TestWord:\n\t\"\"\"\n\tTest the Word class.\n\t\"\"\"\n\t\nclass TestWordError(WordError):\n\t\"\"\"\n\tTest Word class Exception.\n\t\"\"\"\n\t\nclass TestWordWholeWords:\n\t\"\"\"\n\tTest the Word object for 'word' objects wrapping the whole\n\tdictionary and checking that nothing needs to be added to this\n\tword_list. Only used for testing the 'type' attribute\n\t\"\"\"\n\t# The basic Whole word classes should be tested with an empty\n\t"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tif tag == \"NNNN\":\n\t\t\treturn True\n\n\t\ttest_tags_list = []\n\t\tfor tag in tagsets.get_all_tags(tag):\n\t\t\tif test_tags_list.count(tag) == 1:\n\t\t\t\tprint(\"Should be one.\")\n\t\t\telse:\n\t\t\t\t# The word is either accepted or accepted as a generic word.\n\t\t\t\tif tag_words.is_acceptable(tag):\n\t\t\t\t\ttest_tags_list.remove(tag)\n\t\t\tif test_tags_list!= tagset.get_all_active(tag):\n\t\t\t\t"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\ttoken = tag\n\t\tif is_tag_or_tag(token):\n\t\t\treturn True\n\t\treturn False\n\n# -*- coding: utf-8 -*-\n#\n# Gramps - a GTK+/GNOME based genealogy program\n#\n# Copyright (C) 2011-2014  Donald N. Allingham\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tassert str(tag).lower() == tag.lower(), error_msg[0](tag) + \": Must be a valid string\"\n\n\ndef test_sentence_tags(tag: str) -> bool:\n\t\"\"\"\n\tTest that the proper sentence tags can be set as tokens.\n\n\tShould be able to take a sequence of valid sentences with each tag\n\tor to pass as a keyword argument.\n\t\"\"\"\n\t\tassert str(tag).upper() in sentence_tags.keys(), error_msg[0](tag) + \": Must be a valid string\"\n\n#--------------------------------------------\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t\tif isinstance(tag, basestring):\n\t\t\ttag = tag.lower()\n\t\treturn tag.startswith(\"tag:\")\n\n\nasync def test_list_tags(tag: str) -> bool:\n\t\"\"\"\n\tTest a word whether it will list the words that it contained in.\n\t\"\"\"\n\t\tif isinstance(tag, basestring):\n\t\t\ttag = tag.lower()\n\t\treturn tag in _list_tags_re\n\n\nasync def test_update_tags(tag: str) -> list:\n\t\"\"\"\n\tTest a word whether it will update the words that it contained in."}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\ttag = XMLNode()\n\ttag.name = node.tag.name\n\treturn tag\n\nclass TreeNode(object):\n\tdef __init__(self, name=None):\n\t\tself.children = []\n\t\tself.element = None\n\t\tself.name = name\n\t\tself.childrenNS = {}\n\t\tself.attributes = []\n\t\n\tdef __repr__(self):\n\t\treturn self.name + \"(%r]\" % (self.children[0].name,repr(self.children[1]))\n\t\n\tdef __getitem__(self, name):\n\t\treturn self"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, tree):\n\t\tself.tag = root_tag(self, tree.tag)\n\t\tself.root = None\n\t\tself.children = []\n\t\treturn\n\ndef set_root(root):\n\t\"\"\"\n\tSet the root tag of the document.\n\t\n\tThe root tag of the document is the 'root' tags of the document.\n\tIt supports Python nodes.\n\t\n\t@param root: root node of the document\n\t@type root: node\n\t\"\"\"\n\tassert root.tag == 'root', 'root tag is not root'"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\nclass _XMLParser:\n\tdef __init__(self):\n\t\tself._treecache = {}\n\t\n\t\n\tdef set_tree(self, node):\n\t\t\"\"\"\n\t\tSets a new node for a DOM node.\n\t\t\n\t\t@parameter node the node to be parsed by the parser\n\t\t\"\"\"\n\t\n\t\n\tdef newtag(self, tag):\n\t\t\"\"\"\n\t\tCreate an XML Tag instance for the parsed tag.\n\t\t\n\t\t@parameter tag the tag to create\n\t\t\"\"\"\n\t\n\t\n\tdef parse(self, xml):\n\t\t\"\"\"\n\t\tParse the given XML and yield it to the current node"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n#\t\tif isinstance(node, list):\n\t\t#\t\treturn {k:xml_children_as_dict(node[1]) for k in node}\n\t\telse:\n\t\t\treturn {node}\n\n\t# if node is a dictionary\n\tif isinstance(node, dict):\n\t\treturn tree2dict(node)\n\telse:\n\t\tif hasattr(node, '__unicode__'):\n\t\t\tnode = node.encode('utf-8')\n\t\treturn tree2dict(node)\n\n\n#\t@return\n#\tdef __str__(self):\n#\t\tprint"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t# Get all the children of the node\n\tchildren = node.getchildren()\n\n\t# Split the children into lines\n\tsplit_output = string.join(children[:-1], '')\n\t\n\treturn {\n\t\t'meta':''.join(split_output.split(' '))\n\t}\n\n\n\n# Create new node\nif __name__==\"__main__\":\n\timport xml.etree.ElementTree as ET\n\n\t# Open the root XML file of the file\n\txmlDocFile = xml.dom.minidom.parse(xmlFilePath).toprettyxml(indent='    ')\n\n\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\ndef xml_children_as_dict_recursive(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\treturn {\n\t\t\n\t\n\t\t\n\t\t\n\t}\n\t\n\n\n\n\n\n\n__author__ = 'Ling Weinstein'\n\nimport collections\n\n\nclass AWL(object):\n    def __init__(self, name, nameType):\n        self._name = name\n        self._nameType = nameType\n        self.__name"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\tdef _remove_tags(child):\n\t\t\ttag = child.tag \n\t\t\tif node.tag.startswith(tag):\n\t\t\t\tchild.clear()\n\t\t\t\tchild.tail = \"\"\n\t\t\t\treturn\n\t\t\treturn\n\t\t\n\t\tfor child in node.childNodes:\n\t\t\t_remove_tags(child)\n\t\treturn root.toxml(depth=None, remove_blank_text=True)\n\ndef xml_children_at(node, level = 0):\n\t\"\"\"\n\tReturn all the children of node <xml> at level <level>.\n\t\n\tThis is only a straight up for simple"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\ndef xml_element_to_dict(element, **kwargs):\n\t\"\"\"\n\tTurn the ElementTree element into dict, keyed by element name. Also takes all the attributes of the element,\n\tnot keys and default value. If a dict is given, keys and its values are merged in.\n\tReturns the key/value pair.\n\t\n\tEach element value can be anything but an Attribute or a subclass\n\tof Attributes\n\t\"\"\"\n\t\t\n\t\tif element.tag == 'attrs':\n\t\t\tchildren_xml = []\n\t\t\tfor attr in element.attrib.keys():\n\t\t\t\tif not isinstance(attr"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tnodes = list(chain(node.children, node.tag_name.split(\".\")))\n\treturn {str(node.tag_name): node for node in nodes}\n\n\nclass Node(dict):\n\t\"\"\"\n\tA node and children that maps to Node objects\n\t\"\"\"\n\tdef __init__(self, id, name, children=None):\n\t\tdict.__init__(self, id)\n\t\tself[name] = children if children is not None else node\n\t\tself[name][\"children\"] = children if children is not None else node.children  # TODO: Make"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\tif node.tag!= \"children\":\n\t\treturn {}\n\t\n\t\n\tret = { }\n\tfor child in node.getchildren():\n\t\tret[child.tag] = {}\n\t\tfor childNode in childNode:\n\t\t\tif not isinstance(child, xml_child_type):\n\t\t\t\tnodeTags = [ childNode.tag ]\n\t\t\telse:\n\t\t\t\tnodeTags = [ childNode ]\n\t\t\tret[child.tag][\"%s_%s\" % ( childNode.tag, childNode.text) ] = xml_child_type( childNode )\n\t\n\treturn ret\n\t\n\ndef xml_child_"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\t# Check we sent the same message twice:\n\tif (not isinstance(sender_handle, str) or sender_handle == sender_handle):\n\t\t# Send the same message twice, check sender and handle\n\t\tassert isinstance(target_handle, str) or target_handle == target_handle\n\t\tassert isinstance(target_handle, str) or target_handle == target_handle\n\t\tsender_handle_new = sender_handle\n\t\tentity_handle_new = entity_handle\n\t\tmessage_content = ''\n\t\tfor s in iter(generator.__iter__()):\n\t\t\t("}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tsender_handle = sender_handle.lower()\n\tentity_handle = entity_handle.lower()\n\t# We must match both by now.\n\tif sender_handle and entity_handles:\n\t\tif sender_handle == entity_handle:\n\t\t\tpass\n\t\telse:\n\t\t\t# Note:\n\t\t\t# We have both the same object and sender\n\t\t\t# Therefore we must match both objects\n\t\t\t#\n\t\t\tif sender_handle!= entity_handle or sender_handle!= \"new\":\n\t\t\t\t# We must have a new sender here\n\t\t\t\tassert sender_handle!= entity_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\tif sender_handle!= None and entity_handle!= None:\n\t\t\traise Exception(\"Cannot send to both sender and their objects in the same sender.\")\n\t\t\n\tdef on_send_person_received_notification(sender, entity_handle, person_handle, properties):\n\t\t\"\"\"\n\t\tCheck that we received a notification for the person.\n\t\t\n\t\t@param sender: A person handle\n\t\t@param entity_handle: The entity handle\n\t\t@param person_handle: The Person handle\n\t\t@param properties: Person properties\n\t\t@return: Return True if the notification has been sent\n\t\t\"\"\"\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\tassert sender_handle == sender_handle and entity_handle == entity_handle,'sender and entity handle mismatch'\n\t\tresp = {}\n\t\tif len(receiver_headers)!= len(sender_handlers) or len(entity_handlers)!= len(receiver_headers) or len(receiver_handlers)!= len(sender_handlers):\n\t\t\treturn resp\n\t\telse:\n\t\t\treturn make_response(app_common.Status.SUCCESS, 'all done in %2.2f seconds' % sender_handle)\n\n\tif request.method == 'POST':\n\t\ttry:\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef check_reply_to_sender(sender, entity):\n\t\n\tif sender!= entity.to.username:\n\t\treturn\n\t\n\tif entity.to.username in ('alice', 'alice@example.com', 'alice+alice@example.com') and \\\n\t\t\t(sender.lower()!= entity.to.id or \\\n\t\t\t\tentity.sender.lower()!= entity.to.username.lower()):\n\t\treturn\n\t\n\treturn\n\ndef check_sender_name"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tsender_name = sender_handle.split('@')[0]\n\t#if sender_name == sender:\n\t\n\t#\tmessage_format = 'Sender {0} and entity {1} received, received {2}'\n\t#\treturn (check_sender_and_entity_name(sender_name, entity_name), sender_name,\n\t#\t\t\tmessage_format.format(sender, entity_name))\n\t#else:\n\t\n\t\n\treturn (False, '')\n\t\n\n@receiver(signals.pre_save, sender=User)\ndef save_user("}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\t# if sender_handle and entity_handle are different, then check the sender has been re-set (and now it's same) so that we're not trying to set the sender as sender_handle\n\tif sender_handle.upper()=='sender_id_set_and_entity_handle':\n\t\treturn\n\n\t# if we don't have the sender_id_set and entity_handle is identical the receiver matches itself, then it's not expected to change the sender\n\tif sender_handle==entity_handle:\n\t\treturn\n\n\n\t# if no other author means that we didn"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n#\t\tmessage = f\"<presence-type>Test {entity_handle} {recipient_handle} {sender_handle} {record} {contact_no} {record.source_contact_name} {contact_no} {record.originating_entity_name} {recipient_handle} {record.destinat_date} {record.body} {record.source_contact_name} {recipient_handle} {record.originating_entity_name} <{sender_handle}> {entity_handle}.</p>\"\n\t\n#\t\tassert self._checker."}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t\n\tif sender_handle!= entity_handle and entity_handle!= sender_handle and sender_handle!= entity_handle:\n\t\tmsg.info(\"sender %s and %s match. They have to match.\" % (sender_handle, entity_handle))\n\n\ndef encode_message(from_user, to_user, msg):\n\t\"\"\"\n\tCreate a message from a message body and check to see if it's right.\n\t\"\"\"\n\n\t\n\t# The message in the body of this format\n\t# This is the msg string in the payload. For example:\n\t# msg"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\tassert sender_handle!= None\n\t\tassert entity_handle!= None\n\t\tassert sender_handle == entity_handle\n\ndef check_subject_message_matches(subject_message, check):\n\t\"\"\"\n\tEnsure that subject message matches.\n\t\n\tBasically we've already verified the subject message is who they say when receiving the payload. However, the subject message\n\tmay have already been validated by the sender when it receives the payload.\n\t\n\tIn order to verify this the sender and entity might be logged by other clients.\n\t\"\"\"\n\t\tsubject_message = subject_message.lower()\n\t"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif document_path:\n\t\tdoc = self.get(url + '/<name>', schema=Document)\n\t\tfor d in doc.findall('.//document'):\n\t\t\tif document_path and d.attrib.get('file'):\n\t\t\t\tdoc_path = doc_path + '/<name>' + d.attrib['file']\n\t\t\t\tbreak \n\t\telse:\n\t\t\tdoc = None\n\t\t\twarn('Document path not configured on WellKnown Documents.')\n\t\t\treturn None\n\t\n\t\n\tdocuments = self.get(url, schema=Documents)\n\t\n\t# if document_path is supplied, then"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif url is None:\n\t\traise ValueError(\"url cannot be None\")\n\tassert isinstance(document_path, str), \"document_path is str type\"\n\t\t\n\turl = url.rstrip('/')\n\tdocument_path = pathjoin(path, document_path.decode('utf-8'))\n\treturn wellformed_document(url, document_path)\n\n\ndef _get_file_meta(filename):\n\treturn {\n\t\t'token': token.encode('utf-8'),\n\t\t'file': open(filename+'.meta', 'r').read()\n\t}"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif not document_path:\n\t\tdocument_path = get_doc_path(url)\n\t\n\tinfo = DocumentInfo(document_path)\n\n\tfor line in open(info.path, \"rb\").readlines():\n\t\t# ignore whitespace and quotes\n\t\tline = line.strip()\n\t\t\n\t\tif line is None or line[0] == \"#\":\n\t\t\tcontinue# skip blank lines\n\t\t\n\t\tif line.startswith(\"Document:\") or line.startswith(\"Document:\"):\n\t\t\tn = None\n\t\t\tname = line.split(\":\")[1]\n\t\t\t\n\t\t\tif name == document_name:\n\t\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\tpath = document_path or os.path.join(settings.WALKON_API_DATA_PATH, \n\t\t\t\t\t\t\t\tdocument_path)\n\t\tif os.path.abspath(path).startswith(settings.WALKON_API_DATA_PATH):\n\t\t\treturn parse.parse_nodeinfo_path(url, document_path)\n\t\t\n\t\t# No nodeinfo\n\t\tdoc = {\n\t\t\t\"name\": settings.ELEVANTE_ROOT, \n\t\t\t\"url\": url,\n\t\t\t\"document_path\": path,\n\t\t\t\"is_leaf\" : False"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif not document_path:\n\t\tdocument_path = url\n\n\n\trequest = Request(url, request_method='GET')\n\t\n\t# get the response with the query\n\tresp = request.urlopen(document_path)\n\tnodes = resp.read().decode('utf-8').split(None, 3)\n\n\tif nodeinfo is None or len(nodes) == 0:\n\t\traise NodeInfoError(msg=\"Could not read nodeinfo content\", url=url)\n\n\tif len(nodes) == 1:\n\t\treturn nodes[0]\n\n\treturn Node(urls"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\ttry:\n\t\t\tdocument_path = document_path or _document_path_default\n\t\t\tdocument_format = 'xml'\n\t\texcept AttributeError:\n\t\t\tdocument_format = 'xml'\n\n\t\t# Use a different scheme for the wnd\n\telse:\n\t\tdocument_format = 'wnd'\n\n\t\t# Use a different scheme for the wnd\n\tif document_path is None:\n\t\tdocument_path = 'well-known/{0}'.format(document_format)\n\n    if document_path.lower().endswith('document_format'):\n\t\tdocument_format = document"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\n\ttry:\n\t\tdoc_root, node_info_path = get_well_known_path(url,document_path)\n\t\tif not doc_root:\n\t\t\traise DocumentNotFoundError(\"Could not find well known document: {0}\".format(node_info_path))\n\t\tdoc_url = doc_root.rstrip(\"/\") + \"/\" + node_info_path\n\t\t\n\t\tdoc = DocNodeInfo.from_document(doc_url)\n\t\treturn doc\n\texcept DocumentNotFoundError:\n\t\treturn\n\ndef get_node_info(url, document_name=None,"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tdom = xml.dom.minidom.parse(url)\n\texcept:\n\t\traise Exception(\"Error parsing %s\" % url)\n\n\twell_known_document = dict()\n\twell_known_document['URI'] = dom.getAttribute('URI')\n\twell_known_document['Date'] = dom.getAttribute('Date')\n\t\n\twell_known_document['title'] = dom.getAttribute('title')\n\twell_known_document['description'] = dom.getAttribute('description')\n\twell_known_document['abstracts'] = dom.getElementsByTagName"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif document_path:\n\t\tif document_path.find('.')!= -1:\n\t\t\tparsed_url = urlparse.urlparse(url)\n\t\t\turl = parsed_url\n\t\t\tdocument_path = document_path[:document_path.find('.')]\n\t\t\n\t\tdocument_path += '/'+url\n\t\t\n\t\tif not os.path.exists(document_path):\n            # If no document path provided, the current directory contains a valid\n\t\t\t# document_path, so we add the path to the list and return the url.\n\t\t\tdocument_path += '/'\n\t\t\n\t\tif not os.path."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\tif document_path == None:\n\t\tdocument_path = os.path.join(os.getcwd(), document_name)\n\tif os.path.exists(document_path):\n\t\tos.remove(document_path)\n\t\n\tclient = SkyServer(url)\n\t\n\tdocument = client.get_document(document_path)\n\t\n\tnodeinfo = dict()\n\n\t# get all relevant nodes from the document\n\tdom = parse(document.to_html())\n\t\n\tfor node in dom.documentElement.childNodes:\n\t\tif node.nodeType!= node.TEXT_NODE"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\tsigned_content = _generate_signed_message_from_doc(\n\t\tdoc, signature, 'x-x509',\n\t\tprivate_key=private_key, private_key_passphrase='XSS'\n\t)\n\t\n\tassert signed_content.signature == signature\n\t\n\t# get the message\n\txml = signed_content.doc\n\tdoc_signature = xml.xpath('.//{http://www.w3.org/2000/09/xmldsig/}signInfo[@signName=\"%s\"]/signInfo[@signature=\"%s\"]/content'% (signed_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\tprint \"Verifying\", signature\n\t\n\tif not signature == 'error':\n\t\tprint \"Unexpected signature for message %s%s%s\" % (signature, public_key.get_subject(), signature);\n\t\t#print \"For \" + signature + \" see above: \" + signature + \" is signed\", signature\n\t\tcontinue\n\telif signature == 'true':\n\t\tprint \"Signs properly and signed:\", signature\n\telse:\n\t\traise TypeError, \"Received invalid message signature for message %s%s\" % (signature, public_key.get_subject());\n\ndef verify"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\t# Create a hash that identifies the signed message\n\tmsghash = hashlib.sha256()\n\tmsghash.update(doc.encode('utf-8'))\n\tmsghash.update(signer.encode('utf-8'))\n\t\n\treq = Request(public_key, msghash)\n\tres = urlopen(req)\n\tsigned = res.read().split(' ')\n\tkey_sha1, signed_msgHash = signed[:8], signed[8:]\n\t\n\t# Calculate the relative signature to each line in the request,\n\t# taking note of the relative precision\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\tassert private_key == doc['keys']['crypto'])\n\t\tassert public_key == doc['keys']['crypto']['pubkey']\n\t\tassert verify_rsa(public_key, doc['keys']['crypto']['pubkey'], signature, doc['keys']['crypto']['bits'], signature, signature)\n\t\tassert verify_message(private_key, doc, signature)\n\t\tassert signature == doc['keys']['crypto']['signature']\n\t\tassert doc['keys']['crypto']['status'] == 'approved'\n\nclass Wallet_Wallet_Transaction_XML(Transaction_XML, Wallet_Transactions):\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\tassertion_id = doc.get(\"assertionid\")\n\tsignature_id = doc.get(\"signature\")\n\tif signature_id is None:\n\t\t\n\t\treturn\n\t\n\tresult = \"\"\n\tmessage_signature = doc.find(\"message_signature\")\n\tif message_signature is not None:\n\t\tresult = signature + \"\\n\".join(message_signature.text.split(\"\\n\")[6:])+\"\\n\"\n\tprint \"\\tVerifying signed XML for: \"+signedby_name(signedby_name(signature))+\" (\"+signedby_name(documentation_type"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\tassert isinstance(signature, bytes)\n\t\tsigned_signatures = [Signature(signature=signature)]\n\t\tfor signature_element in signed_signatures:\n\t\t\tassert signature_element.type!= Signature.TUPLE, signature_element\n\t\t\tsignature_element_type = Signature()\n\t\t\tsignature_element_type.type = Signature.TUPLE\n\t\t\tsignature_element_type.code = 0x0d\n\t\t\tsigned_signatures.append(signature_element_type)\n\t\tassert len(signed_signatures) == len(doc.signatures)\n\n\t\tdef verify_signature(signature, signature_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\tsig2 = Signature(public_key, signature)\n\t\tsig2.verify()\n\t\tself.assertIsNotNone(sig2.signature_sha256_384)\n\t\tself.assertIsNot(sig2.signature_sha256_384, signature)\n\t\tassert sig2.signature_sha256_384 and signature!= sig.signature_sha256_384_sha256_384\n\n\tdef verify_signature_different(self, public_key, doc, signature):\n\t\t\"\"\"\n\t\tVerify that the signed verification of this element is\n\t\tdifferent than the unverified one.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\tresult_set = set([])\n\t\t\n\t\tsigned_xml_body = doc.get(\"signed\").find(\"signed-by-x-web-appsigners\").text\n\n\t\tif len(signed_xml_body) == 0:\n\t\t\t# No signed message\n\t\t\treturn\n\n\t\ttry:\n\t\t\txappsigner = xml_to_text(signed_xml_body)\n\t\t\tresult_set = set([\n\t\t\t\txappsigner[\"x-app-sig\"], xappsigner[\"signature-date\"],\n\t\t\t\txmpp.Presence.DISCARD\n\t\t\t])\n\t\texcept"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t\n\tclaimed_author = re.search(r\"https://(www\\.gravatar\\.com/avatar/)\\?\", signature)\n\tif claimed_author:\n\t\t\n\t\t\n\t\tquery = \"\"\"\n\t\tSELECT claimed_author.email, claimed_author.given_name, claimed_author.family_name\n\t\tFROM `tabClaim` claimed_author\n\t\tWHERE claimed_author.type='user'\n\t\tAND claimed_author.claimed_date <= %s\n\t\tAND claimed_author.claim_date <= %s\"\"\" % (\n\t\t\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\tself.assertEqual(signature, self.signature_expected)\n\t\n\tdef verify_relayable_signature_two(self, public_key, doc, signature1, signature2):\n\t\t\"\"\"\n\t\tVerify that the signed XML elements to have confidence that the claimed\n\t\tauthor did actually generate this message, but with the two signature\n\t\tvalues specified. There is no check for an unacceptable result in the\n\t\tsignatures above.\n\t\t\"\"\"\n\t\tself.assertEqual(signature, self.signature_expected)\n\t\tself.assertEqual(doc.getElementsByTagName('signed')[0].attributes['version'],"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\tresult: Dict[str, Any] = {}\n\t\t\n\t\t# get data\n\t\tdocs = json.loads(document)\n\t\tresult.update({\n\t\t\t\"features\"\t\t: []\n\t\t})\n\t\t\n\t\ttotal_count = docs.get(\"count\")\n\n\t\tfeatures = 0\n\t\tif \"features\" in docs and docs[\"features\"] > 0:\n\t\t\t# count\n\t\t\tfeatures += docs[\"features\"]\n\t\t\ttotal_count += docs[\"features\"]\n\t\telse:\n\t\t\t# all\n\t\t\ttotal_count = total_count[1:]\n\t\t\n\t\t# build query\n\t\tquery:"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\tdocuments = document.split(\"\\n\")\n\t# If the new format for the XRD document doesn't exist, try reading it from JSON.\n\ttry:\n\t\tdocument = yaml.safe_load(documents[0])\n\texcept Exception as e:\n\t\tdie(\"The data file is corrupted.\")\n\t\t\n\tdoc_url = \"https://diaspora.github.io/\" + document + \"/\"\n\t\n\ttry:\n\t\tdata_obj = requests.get(doc_url)\n\texcept Exception as e:\n\t\tdie(\"The data file is incorrect.\")\n\t\n\tdoc"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\tdocument = json.loads(document)\n\t\treturn {\n\t\t\t'diaspora_webfinger': document,\n\t\t}\n\"\"\"Test the P2Crypto backend.\"\"\"\n\nimport os\nimport subprocess as sp\nimport unittest\nimport sys\nimport tempfile\nimport shutil\nimport os\nimport mock\nfrom random import Random\n\nfrom p2crypto import P2KDF\nfrom p2crypto import test, hashutil, cryptofactory\nfrom p2crypto.hashes import SHA256\nfrom p2crypto import messages\nfrom p2crypto.keys\n\nfrom Crypto.Hash import SHA"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\t# Note: Do not use this function with a standard function!\n\t# TODO: Get rid of this.\n\t# TODO: Return the raw json-data\n\n\t# TODO: Check for json errors if file doesn't exist.\n\t# TODO: Check for multiple json-lines\n\n\tif document == \"\":\n\t\treturn {}\n\tdata = json.loads(document)\n\tif data is None:\n\t\treturn {}\n\n\tjson_data = data\n\t# The json-object doesn't contain all the fields\n\tif \"diaspora_webfinger\" in json"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\td = Document()\n\td.parse(document.encode('utf-8'))\n\n\td.setParentNode(None)\n\td.setDocumentId(document.encode('utf-8'))\n\treturn d\n\n\ndef convert_name(document: str) -> str:\n\t\"\"\"\n\tConvert names in the given document to a common name.\n\t\"\"\"\n\ttry:\n\t\td = Document().parse_document(document.encode('utf-8'))\n\t\treturn translate_name(d).get('name')\n\texcept DocumentFormat.NotFound: return None"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\tdina = Document.objects.get(text='http://localhost:9200')\n\t\n\t\n\tif \"Diaspora\" not in dina.domain_domains:\n\t\tprint 'Did not find \"Diaspora\" domain in Diaspora webfinger.'\n\t\treturn None\n\telse:\n\t\t# parse xRD\n\t\txld = Diaspora.parse_xrd(dina)\n\t\t\n\t\t# check if Diaspora is open on x.ld\n\t\t# get all domain names from x.ld, for example the first name is foo"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\tglobal diaspora_client\n\n\tfrom diaspora_federation.config.settings import FEDERATION_APP_ID, FEDERATION_CLIENT_SECRET\n\tfrom diaspora_federation.config.settings import GITHUB_OAUTH, GITHUB_APPLICATION_ID, GITHUB_SECRET\n\n\ttry:\n\t\toauth_token = str(doc[\"OAUTH_TOKEN\"])\n\texcept KeyError:\n\t\toauth_token = doc['OAUTH_TOKEN']\n\t\n\ttry:\n\t\tgithub_access_token = str(doc[\"GH_ACCESS_TOKEN"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\tdiaspora = {}\n\t\n\tf = open(DOCUMENT_FILE, 'r')\n\troot = f.readlines()\n\tfor x in root:\n\t\tx = x.strip()\n\t\tkey = x[:x.find('#')]\n\t\tv = x[x.find(':')+1:]\n\t\tif (key, v) not in diaspora:\n\t\t\tdiaspora[key] = [{str(v):{'type':'diaspora_webfinger', 'value':x[:x.find(':')[0]+1]} }]\n\t\telse:"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\tparsed_document = json.loads(document)\n\toriginal_type = str(parsed_document.keys()[0])\n\t\n\t# We store the \"data\" for later use by Webfinger, so that we can load\n\t# the data later in the function, and then in the original function,\n\t# we must convert the JSON document from strings to dictionaries\n\toriginal_data = {}\n\t\t\n\tjsonDict = original_type(parsed_document)\n\tfor item in jsonDict:\n\t\tif \"data\" in item:\n\t\t\t# If \"data\" has already been processed"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\t\n\t# Replace with XRD, using XRD\n\td = document.split(\";\")\n\td = filter_domain(d)\n\td = filter_domain(d)\n\t\n\t# Replace with all drama_diaspora.md pages\n\tif len(d) > 2:\n\t\n\t# Do the reverse if possible\n\tif d[1] == \"raspberrypi-reproduce-v2\":\n\t\td[1] = \"raspberrypi-reproduce-v2\"\n\t\n\td[1] = d[2]\n\t"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\t\t\tdoc = WebPage.objects.get(handle=handle)\n\t\t\tlogger.debug('Retrieving Web Page for: {}'.format(document.name))\n\t\t\treturn doc.get_webfinger_document()\n\t\texcept WebPage.DoesNotExist:\n\t\t\traise HTTPNotFound()\n\t\texcept WebPage.MultipleObjectsReturned:\n\t\t\traise HTTPMultipleObjectsReturned()\n\ndef try_download_webfinger_document(handle: str) -> None:\n\t\"\"\"\n\tDownload a file from WebPage.\n\t\"\"\"\n\t\ttry:\n\t\t\tdoc = WebPage"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n#\tprint('try_retrieve_webfinger_document: handle is : {0}'.format(handle))\n#\tprint('try_retrieve_webfinger_document: URL is {0}'.format(url))\n\t\n#\turl_text = None\n#\t\n#\ttry:\n#\t\t\n\t\t#\tresponse = requests.get('{}/webfingerDocuments'.format(url))\n\t\t#\turl_text = response.text \t\t\n#\texcept Exception as e:\n#\t\t\n\t\t#raise Exception('could not get webfinger documents')\n\t\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\n\t\t\timport requests\n\n\t\t\tresponse: HttpResponse = requests.get(handle)\n\t\texcept Exception as ex:\n\t\t\tmsg = 'Failed to readWebfinger document from WebFingerClient: {0}'.format(ex)\n\t\t\tlogging.exception(msg)\n\n\t\t\treturn None, None, 'Failed to get WebFinger document on WebFingerClient, retry in {}'.format(_retry_delay)\n\n\t\traise webfinger.WebFingerApiException(200, msg, handle, _use_existing_webfinger_client=True)\n\n\tdef _download_document"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\twebfinger_document, status = validate_webfinger_document(handle)\n\n\t\tif not webfinger_document:\n\t\t\treturn None\n\n\t\tif status!= 200:\n\t\t\t# status 404, status 404\n\t\t\treturn None\n\n\t\treturn webfinger_document.content or None\n\n\ndef get_webfinger_from_handle(handle, strict: bool) -> Optional[webfinger.Webfinger]:\n\t\"\"\"\n\tParses a webfinger document and extracts a webfinger URL\n\t\"\"\"\n\t\ttry:\n\t\t\twebfinger_id ="}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn frappe.get_cached_value(\"WebFinger Settings\", \"document\", handle)\n\t\texcept Exception as e:\n\t\t\tif not frappe.safe_eval(handler_setting_name):\n\t\t\t\tif frappe.has_permission(\"System Manager\", \"write\") and frappe.config.user!= \"Guest\":\n\t\t\t\t\t# The user has been forced to change the email to something that is not allowed to read from WebFoot.\n\t\t\t\t\tfrappe.log(recipients=recipients, message=e, debug=True)\n\t\t\t\t\traise\n\n\t\t\telse:\n\t\t\t\t# The user has"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\n\treturn find_resource_by_id('WebFinger', handle, lambda r: r.get_absolute_uri())\n\n\ndef get_latest_document_from_url(url: str) -> Optional[Optional[str]]:\n\t\"\"\"\n\tRetrieve and return the latest document from a URL. Does not raise if there's no document in the catalog.\n\t\"\"\"\n\t\n\n\turl_parsed = urlparse(url)\n\tdocument = url_parsed.path\n\tdoc_type = url_parsed.query.split('/', 1)[1.rstrip('&')][-1]\n\t\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tresponse = urllib2.urlopen(handle)\n\texcept urllib2.URLError,\\\n\t\te:\n\t\tfrom twisted.web.server import Site\n\t\tSite().addPortalObject('session', Site(handle, weblog.RootLogger))\n\t\tresponse = e\n\t\treturn None\n\t\n\texcept urllib2.HTTPError,e:\n\t\t\"\"\"\n\t\tHTTP error message.\n\t\t\"\"\"\n\t\treturn str(e)\n\t\n\telse: #there is some error, try and use the original one, otherwise fallback a standard response\n\t\tresponse = str(DEFAULT_WEBPAGE)"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\n\tif not os.path.isfile('webfinger.json'):\n\t\ttry:\n\t\t\thandle = json.loads(urllib2.urlopen(\"https://docs.openoffice.org/Office/2.0/\" + handle).read())\n\t\t\treturn handle\n\t\texcept UnicodeDecodeError:\n\t\t\treturn None\n\treturn None\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n#\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\ndef get_webfinger_document(handle: str, max_id: int) -> Optional[WebfingerDocument]:\n\t\"\"\"\n\tLoad the contents of the webfinger document from the server as the client and process the request\n\tinto a message object.\n\t\"\"\"\n\t   \n\tif handle.lower().startswith('webfinger'):\n\t\t# Try to retrieve the document from the webfinger server\n\t\tw = WebfingerDocument.objects.get(url=handle, max_id=max_id)\n\t\treturn w\n\telif handle.lower().startswith('"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t\twebfinger_url = f'{handle}{urllib.quote(url.format(handle=handle, querytype='webfinger', params={'action':'referrer'}))}\n\n\t\treturn None if webfinger_url is None else webfinger_url\n#!/usr/bin/env python\n\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n#\n#"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\thttp = urlopen('http://www.diacritic.org/file/get_xml_info.xml')\n\trequest_text = None\n\theaders = {'content-type': 'text/xml'}\n\ttry:\n\t\tline = http.read()\n\texcept HTTPError:\n\t\tpass\n\telse:\n\t\trequest_text = line.replace('<B1>', '')\n\ttry:\n\t\tline = http.read('http://www.diacritic.org/file/get_xml_info_detail.xml')\n\texcept HTTPError:\n\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n#\tLoad and parse the Diaspora WebFinger\n\tdana_webfinger = DiasporaWebfinger(handle)\n\treturn dana_webfinger\n\n\n#-----------------------------------------\n\n#\tExtracting Bed\n\t_extract_bed_from_record(line_1_1,line_2_1,line_3_1,line_4_1,str(int(line_1_1),\"\"));\n\n\ndef extract_bed_from_record(line_1_1,line_2_1,line_3"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\tpage = None\n\twebpage = None\n\t\n\tcur = None\n\t\n\t# Parse HTTP page\n\tcur.setLink('http://www.diaspora.org/'+handle.server_address)\n\tpage = cur.get()\n\t\n\t# Parse HTML page\n\tpage = parse_html(page)\n\t\n\t# Close HTTP connection...\n\tcur.close()\n\t\n\t# Split the page for each result\n\tfor page in page.split('<p'):\n\t\tif cur.isBlocked():\n\t\t\tif cur.isBlocked():\n\t\t\t\tpage = split_html_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Fetch a Diaspora webfinger document\n\tdata = urlopen(handle).read()\n\t\n\t# Parse the DOF file, returning a list of Diaspora documents\n\ttry:\n\t\tdata = dia_api_documents.parse_file_doc(data)\n\t\n\t\n\t\n\texcept requests.exceptions.HTTPError:\n\t\t# Could not parse the document, try again later\n\t\tpass\n\t\n\t# Parse the DOF file\n\ttry:\n\t\tdata = json.loads(data)\n\t\n\texcept (ValueError, TypeError):\n\t\t# Could not parse the document,"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\thtml = handle.read()\n\tsoup = BeautifulSoup(html)\n\t\n\tif soup.find('div', {'class':'col-md-3'}).text[0] == 'House':\n\t\tdata_to_return = fetch_housing_data_to_return()\n\t\treturn parse_data(data_to_return)\n\n\treturn None\n\ndef extract_housing_data_to_return():\n\t\"\"\"\n\tExtract the data to the dict on each entry\n\t\n\t:returns: list of dictionaries\n\t\"\"\"\n\t\t\n\t\n\tres = {}\n\t\n\twith"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\treturn get_diaspora_webfinger(handle, 'diaspora')\n\n\ndef retrieve_and_parse_remote_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\treturn get_remote_diaspora_webfinger(handle, 'diaspora')\n\n\ndef get_diaspora_webfinger(handle, url):\n\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\ndef get_xml_node(xml_node):\n\t\"\"\"\n\tExtract xml elements from XML node.\n\t\n\t:arg xml_node: Remote xml node\n\t:returns: dict\n\t\"\"\"\n\t\ndef extract_tag_nodes(xml_node):\n\t\"\"\"\n\tExtract tag nodes from XML node.\n\t\n\t:arg xml_node: Remote xml node\n\t:returns: list\n\t\"\"\"\n\t\ndef get_detached_nodes(node):\n\t\"\"\"\n\tExtract detached from XML node.\n\t\n\t:arg node: Remote node to parse\n\t:returns: list"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\n\tresult = retrieve_doc(handle)\n\tresult['name'] = result.pop('id', None)\n\treturn result\n\n\ndef download_json(url):\n\t\"\"\"\n\tDownload a JSON representation of a url.\n\t\n\t:param url: URL to download\n\t\"\"\"\n\n\th = requests.get(url)\n\tfile = StringIO(h.content)\n\n\t# Open file for writing and closing\n\tf = open(filename=file.name, 'w')\n\tf.write(h.content)\n\tf.close()\n\t\t\n\treturn file."}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\t# Get the document type of the handle that was sent\n\tresponse = handle.read()\n\t\n\t# Skip the headers the response is not coming from\n\trequest = HS_GET_HEADERS.copy()\n\n\t# Get all the parameters that is required\n\trequest.update(dict(\n\t\t# Do not include the 'Content-type' header from the response\n\t\theaders={\n\t\t\t'Content-type': response.get_content_type(),\n\t\t\t'User-Agent': response['User-Agent'],\n\t\t}\n\t))\n\t\n\tresponse = handle.request(request)"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\turl = 'http://www.diaspora.com/download/download.php?d={}'.format(handle.url)\n\tbody = retrieve_raw_diaspora_webfinger(url, parse_data=True)\n\t\n\tfor each_dict in body.keys():\n\t\teach_tuple = urllib.parse.urlparse(each_dict['url'])\n\t\t\n\t\treturn_data = dict(zip(each_tuple.netloc,each_tuple.path.split('/')))\n\t\treturn_data = {\n\t\t\t\"date\" : str(datetime.datetime."}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\tresult = servermanager.rpc(\n\t\t\tservermanager.RPC_GET_DIASPARACLEADAPORAHOST_META + \"/\" + str(host)\n\t\t).find(\"data\")\n\t\tif result:\n\t\t\treturn result[0][\"diaspora\":]\n\telif host in diasporas:\n\t\tresponse = servermanager.rpc(\n\t\t\tservermanager.RPC_GET_DIASPARACLEADAPORAHOST_META + \"/\" + str(host)\n\t\t).find(\"data\", {\"xmlns:xmlns\": TYPES_URN})\n\t\tif response"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tdoc = None\n\t\n\t\n\tif host is None:\n\t\treturn\n\n\thost = host.lower()\n\thost = host.replace(\"host\", \"\")\n\t\n\tdoc = libxml2.parsedoc(host)\n\n\ttry:\n\t\tdoc.parse()\n\n\t\thost_meta = xml2dict(doc, xpathFirst=True)\n\t\thost_meta['name'] = host_meta['name__s']\n\t\tdel host_meta['name']\n\t\t\n\t\tdoc = libxml2.parsedoc(host_meta['name'])\n\t\tdoc.parse()\n\t\n\t\tdiaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\ndef retrieve_remote_meta (host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\ndef retrieve_host_meta (hostname):\n\t\"\"\"\n\tRetrieve metadata for <hostname>. If <hostname> is a local name this first\n\tavailable host name is taken from the document, e.g.\n\t/host-name/host-name.txt.\n\t\"\"\"\n\t\ndef construct_url_base (baseuri, hostname, hostname"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tdef _retrieve_drdbits_remote_meta(drdbits):\n\t\t\"\"\"\n\tRetrieve a remote Diaspora data-repository remote-meta document.\n\t\n\t:arg drdbits: Remote-database to fetch. It must contain a \n\t`DRDBIT`_ instance to get to the server.\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\t# FIXME: This function is copied along with libdnf (in order to\n\t# return a different Dilvik-Dnfe database). However, it may\n\t# return a `"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\tdoc = libxml2.parseDoc(xml)\n\t\treturn libxml2.parseDoc(doc).host_meta\n\n\tdef retrieve_host_meta_from_xml(node):\n\t\tmeta = retrieve_host_meta(xml_to_unicode(node))\n\t\treturn libxml2.parseDoc(meta)\n\n\tdef retrieve_diaspora_info_from_xml(node):\n\t\tinfo = retrieve_diaspora_info(xml_to_unicode(node))\n\t\treturn libxml2.parseDoc(info)\n\n\tdef retrieve_host_host("}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\txml = get_host_meta(host)\n\t\treturn XRD()\n\t# endretrieve_diaspora_host_meta\n\n\ndef retrieve_domain_meta(domain):\n\t\"\"\"\n\tRetrieve a remote domain meta document.\n\t\n\t:arg domain: Domain to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\txml = get_domain_meta(domain)\n\t\treturn XRD()\n# endretrieve_domain_meta\n\n\ndef retrieve_disk_info(disk):\n\t\"\"\"\n\tRetrieve a remote disk-info document.\n\t\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\td_hosts_meta = fetch_meta(host)\n\t\n\tif \"hosts\" in d_hosts_meta:\n\t\treturn RRD(d_hosts_meta[\"hosts\"])\n\t\n\tfor d in host.get_hosts():\n\t\tif \"id\" in d:\n\t\t\treturn DiasporaHost(d[\"id\"])\n\t\t\n\tlog.warning(\"Host missing\")\n\treturn None\n\ndef retrieve_diaspora_hosts():\n\t\"\"\"\n\tRetrieve all diaspora host ids.\n\t\"\"\"\n\treturn [r.id for r in get_"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\tmeta = get_host_meta().get_host_meta()\n\treturn meta.get_diaspora_host_meta(host)\n\nclass MetadataViewHandler(BaseHandler):\n\t\"\"\"\n\tHandle HTTP GET request and the'metadata' collection.\n\t\n\t:arg httpmethod: HTTP method to use, 'GET', or 'POST'\n\t:arg location: Location where retrieved meta was found\n\t:arg path: Relative location (absolute URI) should be accessed\n\t:arg query: Query string\n\t:arg method: Method to use when fetching the metadata for"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tif host.get('type') =='remote':\n\t\treturn _get_diaspora_from_remote(host)\n\telif host.get('type') == 'direct':\n\t\treturn _get_diaspora_from_direct(host)\n\telse:\n\t\traise Exception\n\n\ndef retrieve_diaspora_metadata(metadata):\n\t\"\"\"\n\tRetrieve a remote Diaspora metadata document.\n\t\n\t:arg metadata: Metadata document\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\t\n\tif metadata.get('type')"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\n\ttry:\n\t\turl = host.get_absolute_url()\n\t\t\t\n\t\tresult = url_for('get_host_meta', host_meta_id=host.id,\n\t\t\tservice='diaspora', host_meta_id=host.id)\n \n\t\t\t\t\n\t\tprint result    \n\t\treturn result\nfrom __future__ import unicode_literals\nfrom django.db import models\nfrom taggit_label_parser.models import TaggableManager\nfrom taggit_label_parser.managers import TaggableManager as TaggableManagerManager\n\nfrom django.db."}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\treturn send_request(url, data, method=method, *args, **kwargs)\n\t\t\ndef send_multipart(url, data, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a multipart request. The ``multipart_data`` data will be fed into the body of a multipart request.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tif not isinstance(data, bytes):\n\t\tdata = str(data)\n\n\treturn requests.put(url, headers={\n\t\t\t\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1393.132 Safari/537.36',\n\t\t\t\t'Accept': 'text/html,application/xhtml+xml,application/xml,application/xml;q=0.9,*/*;q=0.8',\n\t\t\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n#        params = {\n#           'method': method,\n#            'params': urlencode({'data': data})\n#        }\n\t\n#        try:\n#            response = requests.post(url, data=params, timeout=timeout, **kwargs)\n#        except requests.RequestException, e:\n#            code = e.response.status_code\n#            body = e.response.text\n\t\n#        #print response.headers['content-type']\n#\t#print json.dumps(response.json(), indent=2, sort_keys=True, sort"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n#\tSet to False to disable SSL validation for SSL v1 or greater for sslv2\n\tclass sslValidationDisabledException(requests.exceptions.SSLError):\n\t\tpass\n\n\tclass request_factory(object):\n\t\t\"\"\"\n\t\tDefines a factory for creating a request object and attaches\n\t\tcredentials and headers:\n\t\t\n\t\t* ``headers`` - Request header strings for the request.\n\t\t* ``credentials`` - Requests authentication credentials.\n\t\t* ``text`` - HTTP message text to send.\n\t\t* ``params`` - Dict containing any additional parameters provided to the request.\n\t\t* ``files"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\ttry:\n\t\t\turl = url.scheme + \"://\" + url.netloc + url.path\n\t\t\treq = None\n\t\t\tif args:\n\t\t\t\treq = requests.post(url, data=data, timeout=timeout, **kwargs)\n\t\t\tif'status_code' in req:\n\t\t\t\tif req['status_code'] is not None and req['status_code']!= 200 or req.get( 'content-type', None ):\n\t\t\t\t\traise AssertionError(\"Could not send document when status_code is %d\" % req['status_code'])\n\t\t\t\treturn (req.status_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\theaders = _headers(data)\n\t\n\t# Call the superclass after sending data to the server\n\tsuper(APIError, cls).__init__(\"api_error\", url, headers, timeout, *args, **kwargs)\n\treturn _api_error(url, data, headers)\n\ndef _get_post_parameters(url, params, timeout=10, method=\"get\", *args, **kwargs):\n\t\n\t# Call the superclass after sending data to the server\n\tresult = None\n\ttry:\n\t\tresult = requests.post(url, params, timeout=timeout, **"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\tlogger.debug(\"URL: %s\", url, data)\n\t\tlogger.debug(\"TIMEOUT: %s\", str(timeout))\n\t\treturn frappe.sendmail(\n\t\t\temail_to=data[\"from\"],\n\t\t\tsender=data[\"from\"],\n\t\t\trecipient=data[\"to\"],\n\t\t\tsubject=data[\"subject\"],\n\t\t\tmessage=data[\"body\"],\n\t\t\treply_to=data[\"reply_to\"],\n\t\t\tcc=data.get(\"cc\"),\n\t\t\tsend_via=method,\n\t\t\treference_dt=data.get(\"dt\"),\n\t\t\tsend_cc=data"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t\n\tfor param in data:\n\t\ttry:\n\t\t\tresponse = requests.post(url, data[param], timeout=timeout)\n\t\t\tresponse.raise_for_status()\n\t\t\tif response.status_code!= requests.codes.ok:\n\t\t\t\tstatus = response.status_code\n\t\t\t\tlogging.info(status)\n\t\t\t\tcontinue\n\t\t\treturn True\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tlogging.info(e)\n\n\tlogging.info(\"Document %s sent to %s:%s.\" % (url, method, urllib.parse.urlparse(url).netloc"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t    \n\n\ttry:\n\t\tdata = json.dumps(data) # convert the dict to string form of strings\n\t\tr = requests.post(url, data=data, timeout=timeout, headers=kwargs) # request data\n\t\t# if request is not POST, we raise an exception\n\t\t# if request is POST, request needs more info\n\t\tif r.status_code == requests.codes.ok:\n\t\t\treturn r.json()\n\t\traise exceptions.HttpError()\n\texcept:\n\t\traise exceptions.HttpError(response_code=\"400\", response_header=\"Malformed JSON"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\tlogger.debug(\"send %r\", url)\n\t\n\tif method == \"post\":\n\t\theaders = parse_headers_headers(data)\n\t\t\n\t\treq = requests.post(url, headers=headers, data=data, verify=False)\n\t\tlogger.debug(\"returning response %s\", req.status_code)\n\t\tresp = make_response(req.status_code)\n\t\t\n\t\tif req.status_code == 200:\n\t\t\treturn resp.json()\n\t\telse:\n\t\t\tlogger.warning(\"status code %d of %s is %s\", req.status"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tret = dic.setdefault(key, {})\n\t\tfor k in keys:\n\t\t\tret.update(dic[k].setdefault(key, {}))\n\t\treturn ret\n\treturn dict_insert(dic, value, key, *keys)\n\t\ndef update_nested_key(dic, key, *args, **kwargs):\n\treturn dict_update(dic, key, *args, **kwargs)\n\ndef append_nested_key(dic1, key, key2, *args, **kwargs):\n\treturn dict_append(dic1, key, key2, *args"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n##        print \"key: \"+str(key)+\" keys:\"\n##\n##        for i in keys: \n##            print dic[i]\n##\n##            dic_append = dic.setdefault(i, [])\n\t\t\n#            dic_append.append(key)\n\t\t\n#           dic_append.extend(dic[key])\n\n\n##        print dic_append, dic\n\t    \n\n##        print dic_append\n##        for k in keys: \n##            print dic[k]\n\n\n\ndef get_values(key):\n\t\"\"\"\n\treturns the"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tval = str(val)\n\t\tif not (not keys): return\n\t\tif not key in dic:\n\t\t\tdic[key] = []\n\t\tdic[key].append(val)\n\t\n\tdef dicts_delitem(dic, key, *keys):\n\t\t\"\"\"\n\tdel a nested key into a dictionary\n\t\n\tcan use python's dict.iterkeys() to check if it's an empty dict, or delete that key\n\t\"\"\"\n\t\tkeys = keys[:]\n\t\tif not keys:\n\t\t\treturn\n\t\tif not isinstance(keys[0], dict):\n\t\t\tkeys = keys[(0"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tkeys = keys[:-1] + \\\n\t\t\t(key, keys[-1].split('.', 1))\n\t\tif key in dic:\n\t\t\tdic.insert(dic, val)\n\t\telse:\n\t\t\tdic[key] = val\n\t\n\ndef dict_merge(dic1, dic2):\n\t\"\"\"\n\tmerge two dictionaries recursively\n\t\n\t:param dic1: a dictionary representing an unmerged dictionary or list\n\t:param dic2: a dictionary representing an merged dictionary\n\t:return: new dictionary with all keys merged with the unmerged\n\t\"\"\"\n\t\treturn dict(dic1."}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n#\tif isinstance(keys, tuple):\n#\t\t\tkeys = args[len(keys):]\n#\t\t\tkeys.append(keys)\n\n\t#\telif isinstance(keys, list):\n#\t\t\tkeys = list(keys)\n\n\t#\telif isinstance(keys, type):\n#\t\t\tkeys = [key]\n\t#\telse:\t\t\n\n\t#\traise TypeError(\"{0} not a tuple\".format(keys.__class__.__name__))\t\t\n\n\t#\n\n\t# get the dictionary data of a nested key from the given list\n\t# using the dict method for"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tif key in dic:\n\t\t\tdic[key].append(val)\n\t\telse:\n\t\t\tdel dic[key]\n\treturn dic\n\n#\n# utility functions:\n#\n\nfor attr in dir(dict):\n\tsetattr(dict, attr, dict.__getattribute__(dict,attr))\n\n#\n# Utility functions:\n#\n\ndef list_to_string(val):\n\t\"\"\"\n\tTake a single string from the dictionary\n\t\n\tThis function will convert a single string to a string\n\t\n\t:param val: the input string\n\t:type val: string\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\ndef dict_get(dic, key, *keys):\n  \"\"\"\n\treturn nested key in the dictionary\n\t\t\n\tThis is a convenient function because we want to return a list of nested\n\tvalue dictionaries.\n\t\n\texample:\n\t  dict_get({}, 'val', {'a', 'b'}).get(['a'])\n\n\t:param dic: a dictionary object to return\n\t:param key: key to find nested value, or 'key' to find nested value for\n\t\"\"\"\n\ndef dict_split(dic):\n  \"\"\"\n\treturn list of nested dictionaries split by key\n\t\n\tExample:\n\t "}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\t# if we get a list or dictionary, try to get the key and insert\n\t\ttry:\n\t\t\tlst = [key]\n\t\t\tlst.append(val)\n\t\t\tlst.extend(keys)\n\t\texcept ValueError:\n\t\t\traise KeyError(\"key cannot be passed as parameter\")\n\t\telse:\n\t\t\tfor val2 in lst:\n\t \t\t# this is tricky, if the value is already passed in the\n\t \t\t# dict then insert it.  get the value and insert it as a \n\t \t\t# key in the chain of key that we must insert\n\t\t\n\t\t# update the dictionary..."}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n#--- BEGIN OF LOOPS ---\n#   dict_insert(dict,'subkey','subkey', '2' if key=='key' else key)\n#   dict_insert(dict,'subkey','subkey','subkey', 3 if key=='key' else key)\n#   dict_insert(dict,'subkey', key, 'key', 2 if key=='key' else key)\n#   dict_insert(dict, 'value', 'key1.key2', 'key1.value2()')\n\t\n#--- END OF LOOP"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\tdic = dic if is_dict_key(val) else {}\n\t\tfor key in keys[:-1]:\n\t\t\tdic_add(dic, val, key)\n\t\tdic[key] = val\n\treturn dic\n\t\ndef dict_clear(dic):\n\t\"\"\"\n\tclears the nested key contents from a dictionary\n\t\n\texample:\n\t  dict_clear({}, 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to delete a nested key\n\t:returns: True if the nested key was to be deleted, else False\n\t\"\"\"\n\t\tdic = dic"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t \n\tif len(settings_dirs)==0:\n\t\traise ValueError(\"Missing settings file specified in settings_dirs\")\n\telse:\n\t\treturn ItypeList()\n\ndef _dict_list_to_python(settings_dict):\n\t\"\"\"\n\tConvert a dict containing the parameters in a list to a python dictionary\n\t\"\"\"\n\t\n\t\n\t\"\"\"Check if a string is passed, split it into lists to convert\n\n\tif isinstance(settings_dict, (type, types.DictionaryType)):\n\t\tsettings_dict = [settings_dict]\n\n\tfor line in settings_dict.split"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\t# this will be converted to real files, it's not quite a real file system\n        # but may change in the future.\n\t\tfiles_dir = os.path.join(os.path.join(os.path.dirname(os.path.dirname(__file__)),\"..\"),'data',spec_option)\n\t\tfiles_dir = join(files_dir, files_dir)\n\t\tlist_of_files = []\n\t\tfor file in listdir(files_dir):\n\t\t\t\tif file in [u\"../data\",u\"../index\",u\"../metadata\"]:\n\t\t\t\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n#\tSettings file locations for initype file\n#\tsettings_files = glob.glob(os.path.join(settings_dirs, \"*.ini\"))\n#\tfor settings_file in settings_files:\n#\t\t\tif os.path.exists(settings_file):\n\t\t#\t\t\tname = os.path.basename(settings_file)\n\t\t#\t\t\tfilename_split = name.split(\".\")\n\t\t#\t\t\tfor name_split in filename_split[:-1]:\n\t\t\t#\t\t\t\tfullname_split = name_split + \".ini\"\n\t\t\t#"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\n\tif(settings_dir is None): settings_dir = os.path.dirname(os.path.abspath(__file__))\n\n\tout_dir = settings_dir\n\tout_dir = config.get('in_dir',out_dir)\n\n\t# If the user specifies a directory with no explicit settings dir, do so\n\tif(settings_dir is None): settings_dir = out_dir\n\tis_in = False\n\n\t# List of file names and directory names containing all of them\n\t# If a file is a list like [...], then it is assumed all"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\ndef add_file_to_list_of_type(settings_dirs, out_dir, out_dir_in, out_dir_out):\n\t\"\"\"\n\tCreate a file in the current directory.\n\t\n\tparams:\n\t\tsettings_dirs [str]\t: Path to directory with the settings\n\t\"\"\"\n\t\n\tglobal list_of_file_names\n\tlist_of_file_names = []\n\t\n\tfor spec_name, spec_dir_name in settings_dirs.iteritems():\n\t\tfile_name, ext = os.path.splitext(spec_dir_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\tconfig_options = {\n\t\t\"spec\"\t\t\t\t\t: spec_option\n\t}\n\treturn create_complex_type_with_configs(config_options, settings_dirs, spec = True, spec_option = spec_option)\n\ndef create_complex_type_with_configs(config_options, settings_dirs, spec = True, spec_option = None):\n\t\"\"\"\n\tCreate a new IniType complex type\n\tAnd return a list of complex type names\n\t\"\"\"\n\tdef add_config_to_settings(settings_filename, settings_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\treturn IterationContext(settings_dirs)\n\n\tdef __init__(self, settings_dirs):\n\t\tassert spec_option in ['settings_dir','settings_files'], 'Specified: settings_dir, settings_files'\n\t\tself.settings_dirs = settings_dirs\n\t\tself.settings_files = []\n\t\tself.directory_names = []\n\n\tdef find_settings_directory(self, typename, config_file_name):\n\t\t\"\"\"\n\t\tGiven the type of configuration file, return the path of the specified directory.\n\t\t\"\"\"\n\t\treturn find_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n   \tfilename = spec_option.filename\n   \t\t\n   \tfile_content = create_input_file(filename, \n                                     settings_dirs, \n                                     spec_option.filename)\n    \treturn [str(file_content)]\ndef create_json_type(json_type_json, json_type_extension, \\\n    json_type_separator = ',', strip_comments = True, \n                                    omit_comments = True, omit_semicolon = True):\n\t\"\"\"\n\tCreate an IniType complex type\n\tThis is the method for creating the json type for 'complex',\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\n\tsettings_dir = dir(settings_dirs)\n\n\tfile_path = settings_dir.abspath() \n\n\tif not os.path.exists(file_path) == False:\n\t\tprint \"Error creating directory {0}. Exiting.\".format(file_path)\n\t\tsys.exit(1)\n\n\tfile_path += \".ini\"\n\n\tfiles = get_file_paths(file_path, settings_dir)\n\n\treturn files\n\t\t\n\ndef gen_file_paths(datadir, dir):\n\t\"\"\"\n\tList files\n\t\"\"\"\n\tout_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\tdef create(file_paths):\n\t\t\"\"\"\n\t\tCreate a new IniType complex object\n\t\t\n\t\t@param file_paths: a list of file paths\n\t\t\"\"\"\n\t\ttmp_file_paths = [(file_paths[2][0], file_paths[2][1])]\n\t\tini_type = type(file_paths[0])\n\t\tfor file_path in file_paths[1:]:\n\t\t\ttmp_file_paths.append((file_path[0], file_path[1]))\n\t\treturn ini_type(tmp_file_paths)\t\t"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\treturn get_config_manager('ansible')\n#   Copyright 2012 VMware, Inc.\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#   not use this file except in compliance with the License. You may obtain\n#   a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#   WARRANTIES"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\treturn cls.config_manager\n\nclass AnsibleConfigManager(BaseConfigManager):\n\t\"\"\"\n\tAnsible configuration manager.\n\t\"\"\"\n\n\tdef __init__(self, config_manager=AnsibleConfigManager()):\n\t\t\"\"\"\n\t\tConstructor.\n\n\t\tArguments:\n\t\t\tconfig_manager (BaseConfigManager): Configuration manager.\n\t\t\"\"\"\n\n\t\tsuper().__init__()\n\t\tself.config_manager = config_manager\n\n\tdef _construct(self, config_manager=None):\n\t\t\"\"\"\n\t\tConstruct a config manager with given parameters.\n\t\t\"\"\"\n\t\tconfig_manager = super()._"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\tsuper(AnsibleConfigManager, self).__init__(\n\t\t\tname=\"Ansible config manager\",\n\t\t\tdescription=\"A simple, secure and modular and secure implementation of Ansible. Config files are \" \\\n\t\t\t\t\"available via Ansible's config flow.\",\n\t\t\tversion=\"0.13\",\n\t\t\tauthor=\"Sean Rabbeck\",\n\t\t\tauthor_email=\"seanrbbecker@ansible.com\",\n\t\t\tversion_specific_option=dict(\n\t\t\t\tdescription=\"The CLI interface provided by Ansible.\",\n\t\t\t\tsupported_platforms=[\"any\"],\n\t\t\t\tversion='"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t return cls(\n\t\t\thost,\n\t\t\tuser = pwd.getpwuid(os.getuid())[0],\n\t\t\tgid=os.getgid(),\n\t\t\tshell = shell,\n\t\t\tuse_sudo = use_sudo \n\t\t)\n\n\ndef get_inventory(host=None, inventory=None, cache=False):\n\t\"\"\" Gets the Inventory object instance based on a host.\n\t\t\"\"\"\n\t from cloudbot import __version__\n\t return inventory.get_inventory(host=host, inventory=inventory, cache=cache)\n\n\n\ndef get_host_environment(config=None,"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\tconf = cls.config_manager.get(cls.name, {})\n\t\tconf.update(cls.config_manager.get_sections())\n\treturn conf\n# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Course',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n               "}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\tconfig_manager = object()\n\t\tif getattr(cls, '_ansibleConfigManager', None):\n\t\t\tconfig_manager = cls._ansibleConfigManager\n\treturn config_manager\n\n\ndef open_config_filename(filename, default_value=None):\n\t\"\"\"\n\tOpens a configuration file.\n\t\"\"\"\n\t\ttry:\n\t\t\tf = open(filename, \"r\")\n\t\texcept IOError as e:\n\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\traise\n\t\t\treturn default_value\n\t\tf.close()\n\t\treturn f.name\n\n\ndef parse_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t if not hasattr(cls, 'ansible_config_manager'):\n\t\t\tclass MockConfigManager(object):\n\t\t\t\tdef __init__(self):\n\t\t\t\t\tself._manager = AnsibleConnection(\n\t\t\t\t\t\thost='localhost',\n\t\t\t\t\t\tport=None\n\t\t\t\t\t)\n\t\t\t\t\tself._manager.use_syslog = False\n\t\t\t\t\tself._manager.no_log = True\n\t\t\t\t\tself._manager.remote_user = 'joe'\n\t\t\t\t\tself._manager.remote_pass = 'wat'\n\t\t\t\t\tself._manager.timeout = None\n\t\t\t\t\tself._manager.retry_interval = None\n\t\t\t\t\tself._manager."}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\n\n\treturn getattr(config, \"Ansible\").get_config_manager()\n  \nimport math\n\ndef makeQuadratic(lat1,lat2,lon1,lon2):\n    return math.sqrt(1 + math.pow(10,lat1)|(10 + math.pow(10,lat2)|(10 - math.pow(11,lon1)|(10 - math.pow(12,lon2)|(10 - math.pow(13,lon1))*8)))\n\ndef makePolarCelestial(lat1,lat2,lon1,lon"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\treturn AnsibleConfigManager()\n\ndef ansible_config_mgr(conf, force, **kwargs):\n\t\"\"\"\n\tGets a config manager, and updates it for a set of options.\n\t\"\"\"\n\t\t# we need a single variable with a name\n\t\toptions = kwargs.get('variables', {})\n\t\tconfig_mgr = AnsibleConfigManager(conf, **kwargs)\n\t\tconfig_mgr.create(*options.keys())\n\t\n\t\t# and finally, set the new defaults\n\t\tfor k, v in kwargs.iteritems():\n\t\t\tsetattr(config_mgr, k, v)\n\t\n\t"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t\treturn get_config_manager(cls._core)\n\n\n\ndef get_config_manager(cls):\n\t\"\"\"\n\tGets the config manager for this Ansible Ansible config manager.\n\t\"\"\"\n\t\treturn getattr(cls, '_ansible_config_manager')\n\n\ndef _get_config_manager(config_module_obj):\n\t\"\"\"\n\tGets the ansible config manager for this Ansible config manager.\n\t\"\"\"\n\t\tconfig_module_data = getattr(config_module_obj, '__main__')[0]\n\n\t\t# Do not copy the module's dependencies to config manager.yaml"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\tw_manager = None\n\tfor d, g in _WORKSPACE_MANAGER_MAP.iteritems():\n\t\t\n\t\ttry:\n\t\t\tif g.working_directory:\n\t\t\t\tw_manager = g.working_directory\n\t\texcept Exception, e:\n\t\t\tprint \"***** Unable to handle workspace manager:\", d\n\t\t\tpass\n\t\n\treturn w_manager\n\t\nclass WorkspaceManager(object):\n\t\"\"\"\n\tUses the workspace manager to get info for a given workspace.\n\t@param workspace_manager Name of workspace manager to get info\n\t\"\"\"\n\t\n\tdef __init__(self,"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls._workspace_manager\n\t@workspace_manager.setter\n\tdef workspace_manager(cls,workspace_manager):\n\t\tif not isinstance(workspace_manager,WorkspaceManager):\n\t\t\traise TypeError(\"Workspace manager must be instance of WorkspaceManager\")\n\t\tif not workspace_manager.name in cls._workspace_manager.list_names():\n\t\t\traise AttributeError(\"Workspace manager has no '{0}' class\".format(workspace_manager.name))\n\t\tif workspace_manager.instance is not None:\n\t\t\traise AttributeError(\"The workspace manager has already be created\")\n\t\telse:\n\t\t"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n##\t@brief Mock the C++ C++ method interface.\n##\tdef createWorkspace(self, name, **kwargs):\n##\n##\t\t\t\t\t\t\tpass\n\t  \n##\t##################################################################\n\n##\t@apiVersion 1.1\n\tasync def getWorkspaces(self, query_string, **kwargs):\n\t\t\"\"\"\n\t\tRequest a workspace by parsing the query string and returning them.\n\t\t\"\"\"\n\t\t# Construct the query string from the JSON representation\n\t\tquery_parsed = json.loads(query_string)\n\n\t\t# Build the query\n        query = query_parsed['name"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls._cls.workspace_manager\n\n\t@api_method\n\tdef all(self):\n\t\t\"\"\"\n\t\tGets all the available windows, as well as all of its children.\n\t\t\"\"\"\n\t\treturn self._cls.all\n\n\t@api_method\n\tdef get(self, window):\n\t\t\"\"\"\n\t\tGets the data for the given window.\n\t\t\"\"\"\n\t\tdata = self._cls.get_by_workspace_id_and_user()\n\t\tself._populate_data(data)\n\t\treturn data\n\t\n\tdef create(self, windows, title="}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\tdef process_data(self, data):\n\t\t\"\"\"\n\t\tProcess data which is a dict and get the values\n\t\tfrom the workspace manager and add it to the dict\n\t\t\"\"\"\n\t\tself.workspace_manager[ws_name]['workspace']['dict'] = data\n\n\nclass Circle(BaseShape):\n\tclass Meta:\n\t\tchoices = {'circle': ''}\n\n\tdef process_data(self, data):\n\t\t\"\"\"\n\t\tConvert a dictionary into a Shape (that is, a dict).\n\t\t\"\"\"\n\t\tself.circle = data\n\nfrom core.himesis"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn get(context.GLOBAL.workspace_manager)\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2013 Vincent Jacques <vincent@vincent-jacques.net>\n# All Rights Reserved.\n\n\"\"\"\nHelper functions for reproducibility with Pyramid.\n\"\"\"\n\nimport math\nimport itertools\nimport json\nimport logging\nimport requests\nimport string\nimport binascii\nfrom pyramid.response import Response\nfrom pyramid.compat import text_type\nfrom pyramid.paster import get_appsettings\nfrom pyramid"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn getattr(cls, \"get_%s_workspace_manager\" % cls.NAME, None)\n\t\texcept AttributeError:\n\t\t\traise AccessFailure(\"No workspace manager named '%s'\" % cls.NAME)\n\n\t@classmethod\n\tdef load_workspace(cls, workspace_manager_name):\n\t\t\"\"\"\n\t\tLoads a workspace manager.\n\t\t\"\"\"\n\t\t# TODO: we need work with a factory that just loads a workspace from a name.\n\t\t# TODO: refactor later when we have a better way to do it\n\t\tworkspace_manager = registry.get(workspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls()\n\n\nclass WorkspaceManager(GObject.Object, Workspace):\n\t\"\"\"\n    This class creates and processes the workflow as a container.\n    \"\"\"\n    signal_count = 0\n\n\tdef __init__(self):\n\t\tself.workspace = None\n\t\tGObject.Object.__init__(self)\n\n\tdef launch(self, workflow=None):\n\t\t\"\"\"\n\t\tSets the WorkspaceManager.\n\t\t\"\"\"\n\t\tgobject.idle_add(lambda: self.emit(GLib.PRIORITY_DEFAULT, 1))\n\n\t\tif workflow is None:\n\t\t\tworkflow ="}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\treturn cls._workspaceManager\n\t@workspace_manager.setter\n\tdef workspace_manager(cls, value):\n\t\t\"\"\"\n\t\tSets the workspace manager.\n\t\t\"\"\"\n\t\tif value <> None:\n\t\t\tsetattr(\n\t\t\t\tcls, \"workspaceManager\",\n\t\t\t\tvalue.__class__(os.path.expanduser(value))\n\t\t\t)\n\tworkspaceManager = classmethod(workspace_manager)\n\n# Copyright 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t\n\tdef __init__(self, host, port):\n\t\t\"\"\"\n\t\tInitializes the workspace manager.\n\t\t\n\t\t:param host: the host name\n\t\t:param port: the port name\n\t\t\"\"\"\n\t\t\n\t\tself.host = host\n\t\tself.port = port\n\t\tself.conn = None\n\t\tself.conn_lock = threading.Lock()\n\t\tself.active_connections = { }\n\t\tself.active_workspace_info = {}\n\t\t\n\tdef __del__(self):\n\t\t'''\n\t\tDeletes the workspace manager.\n\t\t@return:\n\t\t'''\n\t\tif self."}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn cls()._plugins_manager\n\n\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tfrom app import app\nfrom flask_restful import Resource\n\nfrom apps.app.views.user_roles import RoleListResource\nfrom apps.app.views.user_auths import UserAuthListResource\nfrom apps.app.views.user_roles_permissions import UserAuthPermissionsListResource\nfrom apps.app.views.user_roles_permissions_permissions import \\\n    UserAuthPermissionsPermissionsPermissionsResource\nfrom apps.auth.oauth2connect import OAuth2connect\n\nfrom app import"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\treturn App.plugins_manager\n\n# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n#"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn PluginManager()\n\nclass PluginManager(object):\n\t\"\"\"\n\tRetrieves/sets the plugin manager.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitializes the manager object.\n\t\t\"\"\"\n\t\tself.plugins = None\n\n\tdef plugin_start(self, plugin_name):\n\t\t\"\"\"\n\t\tStarts a plugin for debugging purposes.\n\t\t\"\"\"\n\t\t\n\t\tif not plugin_name:\n\t\t\traise ValueError(\"'plugin_name' is not valid.\")\n\t\t\n\t\tif not self.plugins.get(plugin_name):\n\t\t\tself.plugins[plugin_name"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\timport os\n\t\timport logging\n\t\tlogger = logging.getLogger(\"Thug\").getBaseLogger()\n\t\treturn LoggerManager(logger)\n\t\n\tdef loadDefaultPlugins(self):\n\t\timport os\n\t\tself.plugins = os.walk('/opt/Thug/Projects/Thug/', True)\n\t\tplugins = self.plugins\n\t\tplugins_dir = plugins[0][1]\n\t\tfor plugin in plugins[1:]:\n\t\t\tif os.path.isdir(os.path.join(plugins_dir, os.path.basename(plugin))):\n\t\t\t\tself.plugins.remove("}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\tif not cls.get_plugin_manager():\n\t\t\tassert cls._plugins\n\t\treturn cls._plugins\n\n\t@classmethod\n\tdef get_plugin_manager(cls):\n\n\t\treturn cls._plugins[0]()\n\n\tdef run(self):\n\n\t\tself.log_level = getattr(logging, 'CRITICAL', None)\n\n\t\tlogger = logging.getLogger(str(self.__class__.__name__))\n\n\t\tlogger.indent()\n\t\tformatter = logging.Formatter('%(asctime)s - %(module)s - %(message)s')\n\t\tlogger.log(fmt=formatter, level=self"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn plugin.get_plugin_manager()\n\t\t\n\n@plugins_manager.plugin('http')\ndef http_plugins_manager(cls):\n\t\"\"\"\n\tReturns a URLMap that can be passed to plugins.\n\t\"\"\" \n\t\n\t\n\turlmap = plugin.get_http_plugins_manager()\n\turlmap.set_header('User-Agent', '%s %s' % (plugin.getInfo('version', 'unknown'), 'Python/2.6'))\n\t\n\treturn urlmap\n\t\n\ndef get_plugins_manager():\n\treturn plugin.get_plugins_manager("}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn cls.plugin_manager\n\tdef __getattr__(self, name):\n\t\tfor pl in self.plugin_manager:\n\t\t\tif pl.__dict__.has_key(name):\n\t\t\t\treturn pl.__dict__[name]\n\t\traise AttributeError, name\t\n\t\n\t@classmethod\n\tdef add_plugin(cls, module):\n\t\tname = module.__name__\n\t\tplugin_mgr = module.plugin_manager\n\t\t#\n\t\t# Add the plugin\n\t\t#\n\t\ttry:\n\t\t\tpl = plugin_mgr[name]\n\t\texcept KeyError:\n\t\t\tpl = plugin_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\treturn cls.__plugin_manager\n\t@classmethod\n\tdef get_all_plugin(cls):\n\t\treturn cls.__all_plugin\n\t@classmethod\n\tdef get_plugin(cls, plugin_id):\n\t\t# TODO: This should be cached from another function\n\t\treturn cls.__plugins.get(id=plugin_id)\n\t@classmethod\n\tdef get_plugin_for_list(cls, items):\n\t\t# TODO: This should be cached from another function\n\t\treturn cls.__plugins.get()\n\n#########################################\n# Base class is derived from all classes."}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\n\n\tdef get_plugin_name(cls, plugin_dir):\n\t\t\"\"\"\n\t\tGets the name of the plugin file where the given plugin path corresponds to.\n\n\t\t:param plugin_dir: The path where the plugin file name is.\n\t\t:type plugin_dir: str\n\t\t:return: The full path of the plugin file.\n\t\t:rtype: str\n\t\t\"\"\"\n\n\t\tplugin_dir = os.path.abspath(plugin_dir)\n\t\t\n\t\treturn os.path.join(plugin_dir, \"plugins\")\n\n\n\nclass PluginManager(object): \n\t\"\"\"\n\t"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t\tcls._plugin_manager = {}\n\t\tcls.initialized = True\n\n\tdef init_plugin(cls, obj):\n\t\t\"\"\"\n\t\tInitializes/initializes the plugin object with given parameters.\n\t\t\"\"\"\n\t\tcls._plugin_manager[cls](obj)\n\n\tdef add_plugin(self, obj, plugin_obj):\n\t\t\"\"\"\n\t\tAdds the plugin object to the current plugin manager.\n\t\t\"\"\"\n\t\tself._plugin_manager.append({cls: plugin_obj})\n\n\tdef get_settings(self):\n\t\t\"\"\"\n\t\tGets the application settings.\n\t\t\"\"\""}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\t# validation for mandatory data\n\t\tif spec_content is None:\n\t\t\tspec_content = cls._validate_spec_content(yaml_content)\n\t\tif spec_content.get(\"field\", \"no\")!= \"no\":\n\t\t\traise IRValidatorException(\"non valid field: no\")\n\n\t\tdef get_keys(key):\n\t\t\treturn set(_json_to_utf8(item) for item in spec_content.get(key).keys())\n\n\n\t\tfor key in get_keys(spec_content[\"field\"]):\n\t\t\tspec_content[key] = spec_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\ttry :     spec_content = cls.validate_from_content_if_content_is_string(spec_content)\n\texcept IRValidationError :\n\t\traise IRValidationErrorException(\"mandatory data in spec file must be a string\")\n\t\n\treturn spec_content\n\ndef validate_from_content_if_not_provided(spec_content) -> None:\n\t\"\"\"\n\tvalidates that spec (YAML) content has no required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidationErrorException: when mandatory data in spec file must be"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t# Validate the spec\n\tif not spec_content:\n\t\tspec_content = cls._read_specs()\n\tvalidate_spec(spec_content)\n\n\tspec_content = cls._validate_yaml(spec_content)\n\tvalidate_spec(spec_content)\n\n\t# Validate the fields\n\n\tif not specs.validate_fields(spec_content):\n\t\traise IRValidatorException(\"Required fields for spec file\")\n\t\t\n\t# Validate the values\n\tvalid_values = validator.validate_values(spec_content[spec_content.keys()[0]])\n\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\tfrom re import match\n\t\tfrom..extensions.utils import is_extension\n\t\tif spec_content:\n\t\t\t_valid_to_validate(spec_content)\n\t\n\tdef validate_from_config(self, spec_config):\n\t\t'''Check if spec config contains the required required\n\t\tproperties'''\n\t\ttry:\n\t\t\tfor field, data in ((spec_config.get(field, None), {})\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor field in self._required_field_names):\n\t\t\t\tif not data.get('required'):\n\t\t\t\t\traise IRValidatorException(\"required schema entry missing for '{"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\t# if input spec is supplied, pass required data here. If any not, raise exception\n\n\t# we'll validate it\n\tvalidate_from_content = kwargs.pop('validate_from_content', True)\n\t\n\t# otherwise error, error out\n\tif isinstance(spec_content, str):\n\t\t\n\t\t# validate spec file content (and create if needed\n\t\tif validate_from_content:\n\t\t\tspec_content   = _get_file_content(spec_content)\n\n\t\treturn spec_content\n\t\n\t# parse spec dictionary\n\tspec_dict = dict()\n\t\n\twith"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\n\tdata_file_parser = cls.file_parsers.get_parser(\"textspec\").parse_file(spec_content)\n\n\n\n\t# validate data has fields\n\tif \"parameters\" in spec_content:\n\t\tparameters = utils.load_json_from_text_file(spec_content[SPEC], \"parameters_list\", \"parameters\")\n\t\tfor parameter in parameters:\n\t\t\tif \"required\" not in parameter:\n\t\t\t\traise ImportError(\"Required field missing in parameters.json.\")\n\t\tif \"default\" in spec_content:\n\t\t\tdata_file_parser.parse_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\nclass SpecParseException(Exception):\n\tdef __init__(self, message, spec_file, **kwargs):\n\t\t\"\"\"\n\t\tValidates that spec yaml specification is a valid IR Spec file\n\t\t\n\t\t:param message: message to raise Exception\n\t\t:param spec_file: specfile for which valid spec is valid\n\t\t:param kwargs: passed to __init__\n\t\t:raise SpecParseException: when error or if spec has a syntax error\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.valid_data = spec_file.read()\n\t\t\tself._yaml_data = spec_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\tif not spec_content:\n\t\tif cls.validator_error:\n\t\t\traise cls.validator_error\n\n\treturn SchemaValidator().validate(spec_content)\n\n\ndef validate_from_doc(doc_doc, default_validator=None, error_format=\"missing\"):\n\t\"\"\"\n\tvalidates that doc_doc is a dictionary with all required fields\n\t\n\t:param default_validator: optional default validator value - optional\n\t:param error_format: Error format for validation (default'missing')\n\t\n\t:return: The schema validated by this YAML validator\n\t\"\"\"\n\t\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\t\n\tif not spec_content:\n\t\traise IRFactoryException('Spec content is missing')\n\n\tdef validate(spec_content):\n\t\tvalidate_value(spec_content,'spec_content', str)\n\t\t\n\t\tif len(spec_content) > 0:\n\t\t\tvalidate_item_list(spec_content,'spec_content')\n\n\n\t\t\t\n\t\t\n\t\treturn spec_content\n\t\t\t\t\n\t\t\nimport json\nimport urllib\nfrom bs4 import BeautifulSoup\nfrom.errorhandler import ErrorHandler\nfrom.compat import get_soup_text\nfrom.utils import unescape_entities\nfrom.helpers"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\tif 'path' not in spec_content:\n\t\tmsg = 'Spec file content missing'\n\t\traise IRValidatorException(msg)\n\t\n\tspec_file = open(spec_content, 'r')\n\t#if not spec_file:\n\t#\traise IRValidatorException(\"Spec file content missing\")\n\t\n\tspec_content = spec_file.read()\n\t\n\tspec_file.close()\n\t\n\treturn spec_content\t\n\t\n@task\n@with_settings(WATERBUTLER_RESOURCE=False)\ndef apply_ir_update(self, metadata"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tfrom.file_manager import FileReader\n\tfrom.file_manager import read_yaml\n\n\tif yaml_file is None:\n\t\tyaml_file = read_yaml(YAML_PATH)\n\t\n\n\tif isinstance(yaml_file, FileReader):\n\t\treturn read_yaml(yaml_file, cls)\n\n\telif isinstance(yaml_file, basestring):\n\t\ttry:\n\t\t\t# First validate that files are defined as YAML files.\n\t\t\tfrom slycoup.lib.utils.yaml_files import yaml_get_object\n\t\t\tyaml_obj = yaml_get"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tif yaml_file and not os.path.exists(yaml_file):\n\t\traise IRValidationErrorException('Required data missing: %s' % yaml_file)\n\n\t# Validate file\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidationErrorException('Required file not found: %s' % os.path.abspath(yaml_file))\n\n\treturn utils.to_strdict(yaml_file)\n\n\"\"\"This module contains the PC class that manages the camera and the\ncamera_list.\n\n@author: Sam Di P"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\twith open(yaml_file) as f:\n\t\t\tdata = yaml.load(f)\n\t\tvalidation = cls(**data)\n\t\treturn validation\n\t\n\texcept IOError:\n\t\tpass\n\t\n\texcept IRValidationError:\n\t\traise IRValidationError(\"not valid yaml file\")import pyro4py\nimport pyroqt\nimport Rgba\nimport vtk\n\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\n\nfrom OpenGL.GLUT import *\n\nfrom lib.opengl.visuals.surface_shader import *\n\nfrom. import"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tif yaml_file is not None:\n\t\tparser=XMLSchemaFile.load(yaml_file)\n\t\n\tif not parser:\n\t\traise IRValidatorException(\"Unable to load data from file\", \"Not enough YAML data present\")\n\n\tfrom lxml import etree\n\tdef _validate_field_has_all(cls, yaml_file):\n\t\ttry:\n\t\t\tif not etree.iselement(yaml_file):\n\t\t\t\traise IRValidatorException(\"Invalid file %s\", yaml_file)\n\t\texcept AttributeError:\n\t\t\traise IRValidatorException(\"Invalid YAML file %s\","}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tif not yaml_file:\n\t\traise IRValidatorException(\"must provide both yaml_file and the IRValidator.\")\n\t\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tconfig = yaml.safe_load(f)\n\texcept IOError as e:\n\t\traise IRValidationErrorException(\"Missing required configuration file {}: {}\".format(\n\t\t\tos.path.abspath(yaml_file), e\n\t\t))\n\t\n\traise IRValidationErrorException(\n\t\t\"Missing required configuration file {}\".format(yaml_file))\n\t\n\treturn config\t\n\nif"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\tparser = IRArgumentParser(description=\"Validates YAML format\")\n\t\n\tparser.add_argument('--schema', dest='schema', required=True,\n\t\thelp=\"Path to the specified schema\")\n\t\n\tif yaml_file is None:\n\t\tfor line in file(cls.SCHEMA_FILENAME):\n\t\t\tfields = line.strip().split(',')\n\t\t\tif len(fields) == 2:\n\t\t\t\tif field[0] =='schema':\n\t\t\t\t\tschema_file = fields[1]\n\t\t\t\t\tbreak\n\t\t\tif field[0] =='schema-only':\n\t\t\t\tschema_file ="}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\tif not isinstance(yaml_file, str):\n\t\t\traise IRValidatorException(\"Expected a YAML file.\")\n\t\t\n\t\tif yaml_file:\n\t\t\twith open(yaml_file, \"rb\") as stream:\n\t\t\t\tdata = yaml.load(stream)\n\t\t\t\tif isinstance(data, dict):\n\t\t\t\t\treturn data\n\t\t\t\telse:\n\t\t\t\t\traise IRValidatorException(\"Failed to load the file. File: {0}\".format(yaml_file))\n\t\t\t\n\n\t@classmethod\n\tdef get_default_attributes(cls):\n\t\treturn {}\n\n\tdef get_default_value(self"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\n\tdef validate_data(self):\n\t\t\"\"\"\n\t\tPerforms a validation against data found on yaml file\n\t\t\n\t\t:return: Dictionary with data with all required fields\n\t\t\n\t\t:raise IRValidatorException: when yaml file is missing from yaml file\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith open(self.yaml_file) as in_data:\n\t\t\t\tif yaml.safe_load(in_data):\n\t\t\t\t\traise IRValidationError(\"Cannot continue parsing the \"\n\t\t\t\t\t\t\t\t\t     \"YAML file: %s\" % yaml_file)\n\t\texcept IOError as e:\n\t\t\traise IR"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n#\t\t# if this file has data from YAML and not the file it knows how to proceed with it!\n\t#\t\tif yaml_file is None:\n#\t\t\traise IRValidatorException,\\\n#\t\t\t\t\"Invalid YAML file for IrisValidator - check the Iris data folder\"\n#\t\telse:\n#\t\t\ttry:\n#\t\t\t\t# reading data\n#\t\t\t\tr.open(yaml_file)\n#\t\t\t\t# checking the file structure is OK\n#\t\t\t\t# in reality we try to make these simple checks... \n#\t\t\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\t# First validate the file\n\ttry:\n\t\tfp = open(yaml_file)\n\t\tvalidated_data = fp.read()\n\t\tif yaml_file is None:\n\t\t\tfp.close()\n\t\t\traise IRValidatorException(\"File '\" + yaml_file + \"' not found!\")\n\texcept Exception as ex:\n\t\tmsg = str(ex)\n\t\traise IRValidatorException(\"{0}: {1}\".format(cls.__name__, msg))\n\t\n\t# If the file is empty, we need to validate it\n\tif not validated_data:\n\t\treturn None\n\n"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\tparser = _IncludeParser()\n\tinclude_macro_in_parser = parser.parse_include_file_macro_in_parser(False)\n\tinclude_macros = include_macro_in_parser.macros['include_macros']\n\tinclude_files = _parse_include_file_list(include_macros)\n\tself._include_group_map = dict(include_files)\n# Copyright (C) 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\ndef parse_include_groups(parser):\n\t\"\"\"\n\tExtracts groups of directive options from the parser,\n\treturns each element of <option> with name:value pairs corresponding\n\tto those groups.\n\t\n\t@param parser: the parser object\n\t@type parser: L{IParser}\n\t@param parser_dict: the dictionary of data into which include\n\t@type parser_dict: L{dict}\n\t@return: a list of dict. Dictionary for each group of\n\t#\t<option> tuples.\n\t\n\tNote that the list is passed as C{"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tinclude_group_map = parser_dict[\"include_group\"]\n\t\tif include_group_map:\n\t\t\tself._inc_include_groups.add(str(include_group_map))\n\nclass ConfigurationSection(object):\n\t\"\"\"\n\tA class to read the configuration section.\n\t\"\"\"\n\tdef __init__(self):\n\t \n\t@classmethod\n\tdef init_from_file(cls, filename):\n\t \n\n\t    data = {}\n\t#Read data from a file.\n\tdef read(self, section, options):\n\n\t    for (key, value)"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n# Make sure at least one line of the section appears on the command line\n# with the 'include \"foo\"' (if found).\n# E.g. if 'include FOO = bar;'.startswith('include \"foo\"') or\n# 'include \"foo\" = bar, 'include \"bar\"' is not followed by\n# '> -L\" -L\" -L\"\"'.\n# Hence they remain separate words:\n#   include \"foo\\nbar\\nbaz\";\n\twords = parser_dict[self.name]\n\tfor match in words.finditer(self.section"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\t\n\tinclude_list = set(parser_dict.get(\"include\", []))\n\tcompiler_list = [\"msvc\"]\n\t\n\t# This will be an empty parser_dict for the include list that we\n\t# will load.\n\tparser_dict[\"include\"] = include_list - compiler_list\n\n\tlexer = _get_lexer_from_parser_dict(parser_dict)\n\t\n\t# Look for compiler option in include_flags\n\tcompiler_type = _get_compiler_type(compiler_list)\n\tcompiler_flag = _get_compiler_flag(compiler_list"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\t# add the includes at the start:\n\t\tsection_dict = dict(parser_dict)\n\t\tsection_dict.setdefault('include_dirs', ['.'], prepend=False)\n\t\tfor src_name in self._process_files(parser_dict['include_dirs']):\n\t\t\tself.directives.update(section_dict['include'])\n\t\t\tsection_dict.setdefault('include_'+src_name, '').append(self.directives)\n\n\tdef _add_directives_to_section(self, section_dict):\n\t\tfor src_name in self._process_files(section_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\tfor file in parser_dict['include']:\n\t\t\t# if we've already seen one or more of the include files,\n\t\t\t# then nothing was commented out\n\t\t\tif file not in parser_dict['include']:\n\t\t\t\tcontinue\n\n\t\t\t# remove existing includes\n\t\t\tparser_dict['include'].remove(file)\n\n# -*- coding: utf-8 -*-\n# Generated by Django 1.8.5 on 2016-02-06 00:10\nfrom __future__ import unicode_literals\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db."}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\ndef _add_global(parser_dict):\n    \"\"\" \n\tAdds the global dictionary to a parser_dict.\n\t\"\"\"\n\tparser_dict.update(global_dict)\n\n\ndef _include_include(parser_dict):\n\t\"\"\" \n\tLoads the include directive from the specs from global options.\n\t\"\"\"\n\tparser_dict.update(_get_args_with_global_options(parser_dict))\n\n\n@deprecated('2.0')\ndef resolve_config_files_paths(args):\n\t\"\"\"\n\tConverts config_files.yaml to a dictionary"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\nparser_dict.setdefault('include', []).extend(INCLUDE_GROUPS)\nif __name__ == '__main__':\n    \n    opts, args = parser.parse_args()\n    \n    _include_directories(opts.include,\n                    args.directories,\n                    (opts.ignore_patterns |\n                     [r\".*_include\\.(\\s*)\"],\n                     opts.namespace,\n                    opts.name)\n    _include_namespaces(opts.include,\n                    opts.namespaces,\n                    (opts.ignore_patterns |\n                     [r\".*\\.(cpp|cc)?\\.py$\",\n                      r\".*\\.cc$\"],\n                     args.namespace"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\n\tparser_dict.update(self._groups)\n#!/usr/bin/env python\n#########################################################################\n#\n# Copyright (C) 2014 University of Nicira, Inc. All rights reserved.\n#\n# This file is part of the test suite of the HUB project.\n#\n# HUB is free software: you can redistribute it and/or modify it under\n# the terms of the GNU General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option)\n# any later version.\n#\n# HUB"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n\t\n\tglobal options\n\t\n\tconfig_dir = options.directory\n\n\tif (not os.path.exists(config_dir)):\n\t\tlog.error('config directory not found:'+ config_dir)\n\t\tsys.exit(1)\n\t\n\tconfig_dir = config_dir[len(config_dir)-1:]\n\t\n\tif options.host == 'localhost' and not options.port:\n\t\tlog.error('server must begin with localhost')\n\t\tsys.exit(1)\n\tif options.type!='source':\n\t\tlog.error('input type not supported')"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\ndef get_sources_for(spec, dirs_to_scan, paths_to_scan = None, env=None):\n\t\"\"\"\n\tA generator for all sources for the given specs. The source\n\tpaths may alternatively be paths. The default is to scan to find\n\tsource files under the top of the current directory. The given\n\tpaths will be resolved relative to the project root.\n\t\n\tIf env is provided, they are assumed to be relative to the\n\tproject root (i.e. it appears where to run a Python code).\n\t\"\"\"\n\t\n\t\n\tif paths"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tconfig = self.get_config()\n\t\tsources = []\n\t\tfor spec in config.get(\"required_modules\"):\n\t\t\tif spec.startswith(\"http\"):\n\t\t\t\tsources += self._resolve_http_params(spec[6:])\n\t\t\telse:\n\t\t\t\tsources.append(spec)\n\t\treturn sources\n\t\t\t\n\tdef _resolve_http_params(self, spec):\n\t\tif spec.startswith(\"http\"):\n\t\t\treturn {k.strip(): v.strip() for k, v in spec[6:].items()}\n\t\tif spec.startswith(\"https\"):\n\t\t\treturn {"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\t# If our spec defines a list, add a list entry\n\t\tif self.spec_list:\n\t\t\tself.spec_list = tuple(self.spec_list)\n\n\t# Return a spec object with additional defaults.  Note that we\n\t# don't currently support default value values, but rather an overly\n\t# restrictive approach to using the default values in _spec and in\n\t# effect no changes to or from.spec() are returned.\n\treturn self._spec\n\n\ndef get_default_value(spec, spec_dirs):\n\t\"\"\"\n\tGiven an"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tfor arg in self.spec.args:\n\t\t\tif arg.default:\n\t\t\t\tif self.type!= 'path':\n\t\t\t\t\tself.spec[arg.name] = get_env_arg(arg.env, arg.default)\n\t\t\t\telse:\n\t\t\t\t\tself.spec[arg.name] = [self.type] # TODO make this mutable!\n\n\tdef expand_args(self):\n\t\tfor arg in self.spec.args:\n\t\t\tif arg.default:\n\t\t\t\tif self.type!= 'path':\n\t\t\t\t\tself.spec[arg.name] = get_"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\treturn {}\n\nclass GetTargetFromSpec(Task.Task):\n\t\"\"\"\n\tResolves target of a project template: a path or url.\n\t\"\"\"\n\tcolor = 'BLUE'\n\trun_str = '${RUN}'\n\n\tcolor = 'GREEN'\n\trun_str = '${RUN-DIR}/bin/${RUN}'\t\n\n\tcolor = 'YELLOW'\n\trun_str = '${RUN-DIR}/bin/${RUN-NUM}'\n\n\tcolor = 'RED'\n\trun_str = '${RUN-DIR}/bin/${RUN-RUN}'"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tif self.args[0]!= None and self.args[0]!=\"include\":\n\t\t\traise RuntimeError\n\t\telif self.args[0]!= None and \"include\" in self.args:\n\t\t\traise RuntimeError\n\n\n\t\tsources = {}\n\t\tglobal_sources = {}\n\t\tfor source in self.args[1:]:\n\t\t\t# Resolve sources that will be created relative to\n\t\t\t# the spec\n\t\t\tglobal_sources[source] = os.path.join('..', source)\n\n\t\t\t# Resolve sources that are already in the same build directory\n\t\t\tsources[source] = os"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\n#\t\tif self.spec.get('type')!= 'builtin':\n\t\t\tself.log('unknown type: %s'%self.spec['type'], 1)\n\n\t\tif self.spec.get('type') == 'function':\n\t\t\t# The 'function' command expects to be called\n\t\t\tif not self.env['PLATFORM'] == 'win32':\n\t\t\t\treturn self.find_spec('function')\n\t\t\treturn self.find_spec('function')\n\n\t\t# Other sources require 'name', 'type', or 'visibility'. We shouldn't\n\t\t# really do anything special because"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tif 'args' in self.spec:\n\t\t\tfor key in self.spec['args']:\n\t\t\t\tif key.find(':')!= -1:\n\t\t\t\t\ttmpval = self.get_key_value(key.split(':')[1])\n\t\t\t\t\tself.set(key.split(':')[0], tmpval)\n\t\telse:\n\t\t\tassert not self.is_arg_spec\n\t\t\t\treturn self.spec\n\t\t\n\t\tif 'pre' in self.spec and self.spec['pre']:\n\t\t\tassert self.key == 'pre'\n\t\t\tdef process_pre(self, val):\n\t\t\t\tif"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t\tself._get_arg_values('include', self._spec_args)\n\ndef get_spec_arguments(self, key):\n\t\"\"\"\n\tResolve arguments' values from spec.\n\t\"\"\"\n\t\tself._get_arg_values('include', self._spec_args)\n\n\tdef _resolve_arg(key, key_val):\n\t\targs = key_val.split('.')\n\t\tif len(args) > 0:\n\t\t\tkey = args[0]\n\t\t\ttry:\n\t\t\t\targs[1:], value = key.split('=', 1)\n\t\t\t\tvalue = value"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\timport pymel.api as pm\n\t\treturn dict(\n\t\tcommand=pm.command(self),\n\t\tdeprecated=True,\n\t\treturnValues={\n\t\t\t\"options\": self.options,\n\t\t\t\"name\": \"deprecated\",\n\t\t\t\"help\": \"The name of this method.\",\n\t\t\t\"arguments\": {},\n\t\t\t\"version\": \"3.9\",\n\t\t\t\"returnType\": \"CommandResult\",\n\t\t\t\"returnTypeDescription\": \"Display the type which the method is deprecated.\"\n\t\t}\n\t)\n\ndef get_command_type(self, name, required, is"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\nclass NotDeprecatedError(Exception):\n\tpass\n\nclass NotDeprecated(Exception):\n\tpass\n\nclass WarningCountTooLight(Exception):\n\tpass\n\nclass ExceptionWithFormat(Exception):\n\tpass\n\nclass Warning(object):\n\t\"\"\"\n\tRepresents a message that is thrown from the parser system.\n\t\"\"\"\n\t\n\t\nclass RecognitionException(Exception):\n\tdef __init__(self, msg, error):\n\t\tmessage = msg\t\t# string\n\t\terrorCode = error\t# integer\n\t\textraInfo = error # string\n\t\tsuper(Recognition"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\tdeprecated_args = dict()\n\t\tfor key, value_dict in self.deprecated_items.items():\n\t\t\t# We don't want user input to override defaults in the configuration file, since a normal argument is ignored\n\t\t\tif key == \"description\":\n\t\t\t\tdeprecated_args[key] = self.default_description()\n\t\t\t# The config file will be overridden with the overridden_config option, in order to allow config_dir and configuration settings from the module. They are also overridden by a configuration file and should remain the same across runs.\n\t\t\t# Set it to None here so that the override"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\twarnings = {'format' : \"deprecated option\",\n\t\t\t\t\t'level' : \"WARNING\",\n\t\t\t\t\t'section' : \"DEPRECATED\"}\n\t\treturn {'warning' : ('warning', self.warning),\n\t\t\t\t'true' : ('success', self.success),\n\t\t\t\t'autosummary' : ('autoname','autoname_method'),\n\t\t\t\t'deprecated' : dict(deprecated_option.items())}\n\n# this section is to be renamed to `add_warning_info` and to\n# `add_deprecated_info`\nclass DeprecatedSettingValueBase(Settings):\n\t\"\"\""}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn dict(self.get_config().items('deprecated'))\n# -*- coding: utf-8 -*-\n\"\"\"Tests that the `coffee` tool works with C/C++ files.\"\"\"\n__docformat__ ='restructuredtext'\n\nimport os\nimport sys\nimport time\nimport unittest\n\n__all__ = ('TestCoffee', )\n\nDATA_DIR = os.path.dirname(os.path.realpath(__file__))\nPROJECT_DIR = os.path.abspath(os.path.join(DATA_DIR, '../'))\n\nsys.path.insert(0,"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn {\n\t\t\t'use_deprecated': True,\n\t\t\t'warnings': self.warnings,\n\t\t\t'error_message': self.error_message,\n\t\t\t'warnings_on_deprecated': self.warnings_on_deprecated,\n\t\t\t'format_string': self.format\n\t\t}\n\ndef _get_args_from_flags(flags):\n\treturn list(get_debug_arguments_from_flags(flags))\n\ndef _get_debug_arguments_from_flags(flags):\n\treturn [x.string.decode(\"utf-8\") for x in"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\ndef help_parser_add(args):\n\t\"\"\"\n\tAdd help message to a parser.\n\t\"\"\"\n\t\nclass ConfigList(Action):\n\t\"\"\"\n\tContext for parsing multiple config files.\n\t\"\"\"\n\t\n\tdef __call__(self, parser, env, values, option_string=None):\n\t\t# Handle non-optionals, and options that aren't actually\n\t\t# used. This prevents us from doing extra parsing from \n\t\t# parsing from a file that doesn't actually handle options.\n\t\toption_groups = []\n\t\tunwrapped = [value for value in env"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn dict(\n\t\t\t--conf=self.conf,\n\t\t\t--level=self.get_level(),\n\t\t\t--use-debug-info=self.use_debug_info,\n\t\t\t--disable-deprecated-options=self.disable_deprecated_options,\n\t\t)\n\n@cli.Command('version')\n@click.option('--release', '-r', type=int, help='The version of the release')\n@click.option('--version', '-v', type=str, metavar='VERSION',\n              help=\"A version specifier\")\n@click.option('--short', '-"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\n#\tReturning list of deprecated arguments from the module.\n#\tDEPRECATED_ARGS = DEPRECATED_OPTIONS[:]\n#\n#\tThe key is deprecated because it depends on the argument names:\n#\n#\t\t * args: Argument names.\n#\t\t * deprecated_arg: Deprecated argument name.\n#\t\t\t * description: Deprecated argument description.\n#\t\t\t\t\tWill be removed from future releases.\n#\t\t\t * required: Deprecated argument that isn't required.\n#\t\t\t * deprecated_arg_value: Deprecated argument value.\n#\t\t\t * description: Deprecated argument description.\n#\n\n\nDEPRECATED_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t\treturn {\n\t\t\t'verbose_name': self.verbose_name,\n\t\t\t'verbose_name_plural': self.verbose_name_plural,\n\t\t\t'max_number_of_rows': self.max_number_of_rows,\n\t\t\t'min_number_of_rows': self.min_number_of_rows\n\t\t}\n#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Mangodb - The Mangodb Client\n# Copyright (C) 2012-2017, Sebastian Heijingen,"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\ndef do_deprecation_warning(cli_args, answer_file_args):\n\t\"\"\"\n\tChecks the deprecated arguments and updates the documentation accordingly.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\n\t\n\treturn\n#!/usr/bin/env python2\n\nimport sys\n#\n#\n#  Copyright (c) 2001 - 2016 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\toptions, args = cli_utils.parse_argv()\n\t# If args is wrong type type it's ok\n\t#if isinstance(args, dict):\n\t#\t# Run through all the args and pass all data to CLI\n\t#\toutput = [parser.format_help() for parser in args.iteritems()]\n\t#\tfor k, v in options.items():\n\t#\t\toutput.append(\"\\t{0} {1} {2}\".format(k, v, k))\n\t#\tPrint(utils"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tif \"version\" not in self.module._info:\n\t\t# module not found\n\t\tself.module.fail_json(msg=\"CLI Error: The 'cli\" command requires the running ansible module to be installed\", version=self.module._info[\"version\"])\n\n# pylint: disable=too-few-public-methods,too-many-instance-attributes\nclass OpenShiftCLIError(Exception):\n\tpass\n\nCLI_PKG_CONFIG_FILE = '/etc/origin/{0}/cli.ini'.format(os.environ.get('C', 'ANSIBLE_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n    def get_user_dir(self):\n\t   os.makedirs(\"./tests/\")\n\n    def get_option(self, cli_args, option_name):\n        cli_args = cli_args[option_name]\n        \n\n    def display_help_text_for_options_in_help(self, argv):\n        print \"\\nCommands:\\n\\t%s\\t\\t\\t\" % str(cli_args)\n\n\n    def validate_arguments_for_given_options(self, cli_args, answer_file_args):\n\t\"\"\"Validates and prints the valid arguments for given"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tdeprecation_arguments = {}\n\t\n\tfor key in cli_args.keys():\n\t\t\n\t\targument_type = type(cli_args[key])\n\t\t\n\t\tif argument_type in [\"str\", \"unicode\"]:\n\t\t\t \n\t\tif argument_type is list:\n\t\t\t\n\t\t\t\tif(len(cli_args[key]) > 0):\n\t\t\t\t\tdeprecation_arguments[key] = cli_args[key][0]\n\t\t\t\telse:\n\t\t\t\t\tdeprecation_arguments[key] = cli_args[key][0]\n\t\t\telse:\n\t\t\t\n\t\t\t\t# validate if argument is present\n\t\t\t\tif \""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\ndef run_cli(cli_args, answer_file_args, default):\n\t\"\"\"\n\tExecutes a set of commands and checks for validity.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args: the dict of arguments from files\n\t:param default: the cli default\n\t:return: None\n\t\"\"\"\n\t\ndef checkArgs(args, question):\n\t\"\"\"\n\tChecks the mandatory arguments.\n\t\n\t:param args: the dict, with the value of the mandatory argument, or\n\t:param question: the question"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\n\tdef _print_deprecation(self, cli_args, answer_file_args, args_help_str):\n\t\t\n\t\t\"\"\"\n\t\tPrints usage message when argument deprecation is turned on\n\t\t\"\"\"\n\n\t\tparser = argparse.ArgumentParser(self.description, conflict_action=\"error\")\n\t\tparser.add_argument(\"-v\", dest=\"version\", action=\"version\", version=\"%(prog)s version 0.0.1\")\n\t\tparser.add_argument(\"--no-color\", dest=\"color\", action=\"store_true\",\n\t\t\tdefault=False)\n\t\tparser.add_argument(\"-v"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\t#check the answer file dict that is loaded\n\t#to ensure that the type of answer from files is correct\n\tif answer_file_args is None:\n\t\terror_response(\"Error\", cli_args)\n\t\treturn\n\t\n\t#get the list of args from the command line\n\targs = self._get_args(cli_args)\n\t\n\t#check that the given files are readable\n\t\n\t#check if the current type of arg has the wrong type\n\targ_type = check_type_from_file(args.file_type, args.validation_type)\n\t\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n#\t@staticmethod\n#\tclass ConfigFileParser(ConfigParserPlugin):\n#\t\t\"\"\"\n\t#\t\tReads config file and creates a tree\n#\t\t\"\"\"\n#\n#\t\t#\n\t #\t\t@abstractmethod\n#\t\t#\n#\t\t#\t\t@return: A ConfigFileParser object\n#\t\tdef __init__(self, configfilename, argparser, args):\n#\t\t\"\"\"\n\t\tThis method loads the config file and parses the options into a\n\t\tConfigParserPlugin object.\n\n\t:param configfilename: The path of the"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tfor key in sorted(answer_file_args.keys()):\n\t\tprint(key, answer_file_args[key])\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom __future__ import print_function\n\nimport unittest\nimport random  # to generate random numbers (random string & int for python)\nfrom collections import Sequence, defaultdict\n\n\n\ndef uniform(value0, value1):\n    return '%s/%s' % (value0, value1)\n\n\n\nclass BasicRandomWalk(unittest.TestCase):\n\n    def setUp(self):"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n#\treturn command_parser_option_specs[command_name]\\\n#\t\t\t\t(*command_parser_option_specs[command_name][1:])\n\t\n\tdef parse_command_options(self, parser):\n\t\t\n\t\tif not parser.has_option('--quiet'):\n\t\t\tparser.set_defaults(quiet=True)\n\t\t\n\t\t# Add the options used in the arguments and the command_parser_options\n\t\tfor option_name, parser_options in parser.option_list:\n\t\t\t# Remove any option name that can't be used in command options\n\t\t\t# (if they exist"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\tparser = self.__parser.add_parser(command_name)\n\t\n\tparser.add_argument(\"--quiet\", action=\"store_true\")\n\t\n\tparser.add_argument(\"--verbose\", action=\"store_true\")\n\t\n\tparser.add_argument(\"--ignore_test\", dest=\"ignore_test\", action=\"store_false\",\n\t\t\t\t    help=\"Skip all tests with given parameters\")\n\t\n\tparser.add_argument(\"--log\", type=str, default=\"info\",\n\t\t\t\t    help=\"log messages to the file at stdout rather than stdout\")\n\t\n\tparser.add_argument(\"--no-ignore-"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\tparser = self.add_parser(command_name)\n\t\tparser.add_argument(\"--domname\", action=\"store_true\", default=False,\n\t\t\thelp=\"Find the domby domain name by using libvirt version.\")\n\t\tparser.add_argument(\"--domuuid\", action=\"store_true\", default=False,\n\t\t\thelp=\"Find the domby uuid by using libvirt version.\")\n\t\tparser.add_argument(\"--vmid\", action=\"store_true\", default=False,\n\t\t\thelp=\"Find the vmid by using libvirt version.\")\n\t\tparser.add_argument(\"--vcpu\","}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\tparser = self.command_parsers[command_name]\n\t\t\n\t\t# get the options from the commandline string\n\t\toptions = parser.parse_args([command_name])\n\t\t\n\t\t# set the command options to the correct values\n\t\tfor attr_name, option_spec in self.command_attributes.iteritems():\n\t\t\tif attr_name == 'config_file':\n\t\t\t\toption_spec.update({\n\t\t\t\t\t\t\t\t\t\t    'config_file': self.command_directory + option_spec['config_file']\n\t\t\t\t\t})\n\t\t\telif attr_name == 'default_console_output':\n\t\t\t\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\toptions = []\n\tif self.help_option_spec:\n\t\toptions.append('help'+ command_name)\n\n\treturn options\n\n\n\n\n#-------------------------------------------------------------------------\n#\n# Support functions\n#\n#-------------------------------------------------------------------------\n\ndef get_options_to_parser(argv=None, parser=None):\n\n\tparser_cmd = None\n\t\n\tif parser!= None:\n\t\tparser_cmd = parser.parser.option_parser(argv)\n\t\t\n\tif argv:\n\t\tfor i,arg in enumerate(argv[1:]) :\n\t\t\tif i == 0 :\n\t\t\t\tparser_cmd.error"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\tparser = self.get_parser(command_name)\n\t\tgroup = parser.add_mutually_exclusive_group()\n\t\tgroup.add_argument(\"-U\", \"--unmodified\", dest='unmodified', action='store_true', help=\"Do not execute unmodified command\")\n\t\tgroup.add_argument(\"-T\", \"--timeout\", metavar=\"N\", dest='timeout', action='store', choices=(['10ns','50ns'],['1sec','2sec','3sec','4sec','5sec','6sec','7sec','8sec','9sec','10sec','11sec"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\toption_list=[]\n        self.debug(\"Creating an optParserOptionParser for the module %s\", command_name)\n\t\n\n        # parse command_name as a tuple of (opt_name, arg, type)\n        option_list.append((\"option\",command_name))\n\toption_list.append((\"help\", self.helpText))\n        self.opt_parser_parser_module = CParser(option_list, self.helpText)\n\n\tself.debug(\"Done creating an optParserOptionParser\")\n\t\n\t\treturn(option_list)\n\n\tdef debug(self, msg"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t\n\tusage = \"\"\"\\\nusage: %prog [options] <name_to_option>\" <command_name>... \\\n\"\"\"\n\tlogger.info(usage)\n\tcmd = \"%s %s\" % (self._get_parser_option_spec(), command_name)\n\treturn cmd\n\n\ndef is_active(cmd):\n\t\"\"\"\n\tChecks if the active command has been run\n\t\n\t:param cmd: the command name (main, virsh, ospd, etc...)\n\t\n\t:return: True if the command executed succesfully\n\telse False.\n\t\"\"\""}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\tif self._options_obj is None:\n\t\t\tself._options_obj = super(CLIGroup, self).__get_parser_option_specs('default', command_name)\n\t\treturn self._options_obj.option_specs.keys()\n\t\n\tdef get_opt_value(self, option):\n\t\t\"\"\"\n\t\tGets the value of a named option\n\t\t\n\t\t:param option: the option value\n\t\t:type option: string\n\t\t:return: the loaded value or None\n\t\t\"\"\"\n\t\tvalue = None\n\t\tfor opt in self.cli_opts:\n\t\t\tif hasattr"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\treturn super(TestCmdLine, self).get_parser_option_specs(command_name)\n\nclass TestCmdLineTest(CmdLineTestBase):\n\tdef parse_args(self, args):\n\t\tparser = optparse.OptionParser()\n\t\tparser.add_option('-f', '--filename', dest='filename', default='log', help='filename for the commands expected')\n\t\tparser.add_option('-x', '--xml', dest='xml_file', default='xml', help='xml file for commands expected')\n\t\tparser.add_option('-v', '--verbosity', dest"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\n\tout = self._run_command(['info', 'get', '-w', command_name, 'details', argument_name])\n\tdetails = out.split()\n\tdetails.insert(0, 'command name: %s' % command_name)\n\tdetails.append('arg name: %s' % argument_name)\n\treturn details\n\t\ndef get_option_spec_str(option_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name and its contents.\n\t\"\"\"\n\t\n\tspec = self._run_command(['"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\ttry:\n\t\t\treturn self._option_spec\n\t\texcept AttributeError:\n\t\t\traise InvalidAttributeError(\n\t\t\t\t\"get_option_spec for '%s' does not have '_option_spec'\" %\n\t\t\t\tcommand_name)\n\tdef get_description_info(self, command_name, argument_name, argument_default=''):\n\t\t\"\"\"\n\t\tDescribe the specified option for the specified option name, or\n\t\treturn a string describing the option.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._description_info[command_name][argument_name]\n\t\texcept:\n\t\t\traise"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\tif self._options is not None:\n\t\t\treturn self._options.get(command_name, None)\n\t\telif self._options is None:\n\t\t\tmodule_name = command_name.split('/')[0].replace('_', '-')\n\t\t\tspec = self._options.get(module_name, None)\n\t\t\tif spec is None:\n\t\t\t\traise ConfigurationError(\"Specified command %s (%s) for option name %s is not defined in the configuration file.\" % (command_name, command_name, argument_name))\n\t\t\telse:\n\t\t\t\treturn spec\n\t\tself._options["}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn self._command_spec.get(command_name, {}).get(argument_name, {})\t\n\t\t\t\n\tdef get_option_spec_from_tree(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name, including the options listed in the config file.\n\t\"\"\"\n\t\treturn self._command_spec.get(command_name, {}).get(argument_name, {})\t\n\t\t\n\nclass NoConfigSectionError(Exception):\n\tdef __init__(self, args):\n\t\tself.config_parser = argparse.ArgumentParser"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\nclass CommandArgumentSpec(ParameterSpec):\n\tparser_signature = 'COMMAND1:FOO'\n\trunner_cls = CommandArgumentRunner\n\t\n\targument_spec = [\n\t\tParameterSpec(\n\t\t\t('--foo', dest='foo', action='store_true'),\n\t\t\t'--bar', action='store_true',\n\t\t\thelp='run FOO with this option'),\n\t\tParameterSpec(\n\t\t\t('--bar', action='store_true', dest='bar',\n\t\t\t  help='run BAR with this option'),\n\t\t),\n\t\tParameterSpec(\n\t\t\t('--bazazu', dest='bazazu"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn [ x[0] for x in self._config.options if x[1].get(command_name) == argument_name ]\n\t\n\tdef set_option_spec(self, command_name, argument_name, value):\n\t\t\"\"\"\n\tsets the option spec string of the specified option name.\n\t\"\"\"\n\t\tself._config.options[command_name][argument_name] = value\n\t\t\n\tdef get_option_spec(self, command_name):\n\t\t\"\"\"\n\tgets the option spec string for specified option name.\n\t\"\"\"\n\t\treturn [x["}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn {\n\t\t\t'option': '-p',\n\t\t\t'type': type(command_name)\n\t\t}[command_name]\n\n\tdef get_option_type(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tIs the specified option type 'option' (option or option list command).\n\t\t\"\"\"\n\t\treturn self.get_option_spec(command_name, argument_name).type\n\n\nclass Option(object):\n\t\"\"\"\n\tA class to represent a single option parameter in the output.\n\t\"\"\"\n\n\tdef __init__(self, options):"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\treturn 'option --%s=name --%s' % (command_name, argument_name)\n\t\n\tdef format_option(self, option_spec):\n\t\t\"\"\"\n\t\tFormats a list of arguments to be passed to the\n\t\tcli.\n\t\t\"\"\"\n\t\targs_str = command_name_and_argument_spec(command_name=self.command_name, argument_spec=option_spec)\n#\t\tself.exit_status = 0\n\t\treturn '%s' % ((' '.join('{!s}'.format(x) for x in args_str)))"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\toption_spec = {\n\t\t\t\"--quiet\"        : [\"-x\"],\n\t\t\t\"--yes-reply\"       : [\"-x\"],\n\t\t\t\"--file\"            : [\n\t\t\t\t[\"\", \"--file\"],\n\t\t\t\t[\"\", \"--files\"],\n\t\t\t\t[\"\", \"--files\"],\n\t\t\t],\n\t\t}\n\t\twith self.options(argument_name):\n\t\t\tfor spec in self.options_raw[command_name]:\n\t\t\t\tself._process_value(spec, command_name)\n\n\tdef get_option_help(self, command_name):\n\t\treturn \"Shows the"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t\n\tassert isinstance(command_name, basestring)\n\t\n\toption = None\n\tfor option_name, option in self.config.options.items():\n\t\tif command_name.startswith(option_name):\n\t\t\toption = option_name\n\treturn option\n\t\n\nclass CommandsError(EnvironmentError):\n\tpass\n\nclass UnknownExtraConf(EnvironmentError):\n\tpass\n\nclass ConfigError(EnvironmentError):\n\tpass\n\nclass DuplicateConfigurationError(EnvironmentError):\n\tpass\n\nclass NoSectionError(EnvironmentError):\n\tpass\n\nclass NoOptionError(Environment"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg[0] is not \"-s1\":\n\t\t\t\tif len(arg) < 2:\n\t\t\t\t\t# a negative number indicates no additional argument.\n\t\t\t\t\tcontinue\n\t\t\t\tif arg[1] == \"-\":\n\t\t\t\t\tcontinue\n\t\t\t\tif arg[2] == \"-\":\n\t\t\t\t\targs[-1] = arg[:-1]\n\t\t\t\telif arg[2] == \"--\" and arg[-1][-1] == \"-\":\n\t\t\t\t\targs[-2] = arg[:-1]\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\targs.append(arg)\n\t\"\"\"\n\tdef get("}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n#\tif self.args.get('silent', 0):\n#\t\tsilent_args, silenced_args = [self.args['silent_args'], \"s\"]\n\t#\telse:\n#\t\tsilent_args = [self.args['silent_args']]\n#\t\tsilent_args = silenced_args.keys()\n#\t\tsilent_args.sort()\n       \n\t\tsilent = getValue(self.args.get('silent', 0), False)\n\t\t\t\n        if not silent:\n\t\t\tsilent_args, silenced_args = get"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\targnames = []\n\t\tfor i in range(len(args)):\n\t\t\targnames.append(self._get_arg_names(args[i]))\n\t\treturn argnames\n\t\n\tdef _get_arg_names(self, arg):\n\t\treturn [self._convert_arg_name(arg[i], self._get_arg_value(i)) for i in range(len(arg))]\n\n\tdef _get_arg_value(self, value):\n\t\treturn int(value)\n\n\tdef _get_arg_names(self, arg):\n\t\treturn ["}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\tif self.args and args:\n\t\t\tdel args[:]\n            for arg in self.args:\n\t\t\t\targ_list = arg.split()\n\t\t\t\tif len(arg_list)==1:\n\t\t\t\t\tdel args[:]\n\t\t\t\t\tbreak\n\t\treturn args\n\tdef get(self, key):\n\t\tif self.args and key in self.args:\n\t\t\treturn self.args[key]\n\t\telse:\n\t\t\treturn None\n\n\tdef __setattr__(self, key, value):\n\t\tif key in self.__dict__:\n\t\t\tself.set(key,value)\n\t\telse:"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\treturn args.slienced if args else args.names\n\n\tdef find_output_file(self, filename):\n\t\treturn glob.glob(filename)\n\n\tdef check_for_modified_file(self, filename):\n\t\tfor pattern in self._modified_patterns:\n\t\t\tif self.check_pattern(pattern):\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef check_pattern(self, pattern):\n\t\tmatch = re.search(pattern, self._file_name)\n\t\tif not match:\n\t\t\tmsg = \"Silly: {0} not found on"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\targ_dict = {}\n\t\tfor argument in args:\n\t\t\tif argument[0].__name__.startswith('dnd'):\n\t\t\t\tret = argument[1:]\n\t\t\t\targ_dict[ret] = int(ret)\n\t\t\t\tdel arg_dict[ret]\n\t\t\telse:\n\t\t\t\treturn arg, False\n\t\t\n\t\treturn list(arg_dict.values())\n\t\t\t\n\tdef is_silent(self, arg):\n\t\t\"\"\"\n\t\tcheck if argument is in the silenced form\n\t\t\"\"\"\n\t\tif arg!= None:\n\t\t\treturn (self.is_silent(arg) and"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\t\n\treturn [ arg.get('name') for arg in args ]\n\nclass ArgList(object):\n\t\"\"\"\n\tGet a list of argvs and return either a single list or a list.\n\t\"\"\"\n\t\n\tdef __init__(self, argv):\n\t\t\"\"\"\n\t\t:param argv: List of argvs separated by commas.\n\t\t:return: arglist\n\t\t\"\"\"\n\t\t\n\t\tself.argv = argv.split(',')\n\t\n\tdef get(self, name):\n\t\t\"\"\"return argv slighed by name and index\"\"\"\n\t\t\n\t\tif name in self.argv:\n\t\t\treturn"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\t\n # FIXME: remove this and use a generic decorator like it\n\tdef log(msg):\n\t\traise Exception, msg\n\n return log\n\n\n################################################\n### \n###### Helpers     \n################################################\ndef strip_list(lst):\n\t\"\"\"\n\tRemoves empty list lines (excluding empty line separators, to keep\n\tall lines alone, etc.).\n\t:param lst: list\n\t:return: stripped list\n\t\"\"\"\n\t\n# \ndef sanitize(lst, case = True, line_terminator = '*'):\n\t\"\"\"\n\tSanitize list to be compatible with a single line"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\n\treturn dict(((k,self.read_value(k))) for k in args))\n\t\nclass SliencedArgumentSet(dict):\n\t\"\"\"\n\tSet of `SliencedArg` objects\n\t\n\tYou can retrieve `SliencedArg`s for example:\n\t\n\t- `slienced[A] = A + 0`.\n\t\n\t:param argnames: Slienced argument names.\n\t\n\t.. note::\n\t\t`get()` on the argument will return a mutable list.\n\t\tUse :meth:`get_silent_args()` to return a mutable list"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\tif args == []:\n\t\t\treturn []\n\t\telse:\n\t\t\tif isinstance(args, basestring):\n\t\t\t\tif not isinstance(args, list) or type(args)!= list:\n\t\t\t\t\t\targs = [args]\n\t\t\ttry:\n\t\t\t\tif len(self._get_debug_info()):\n\t\t\t\t\tprint \"*** get_silent_args: args: \", args\n\t\t\t\t\tprint \"    \", self._get_debug_info()[0].args\n\t\t\texcept IndexError, e:\n\t\t\t\traise SilencedArgError, e\n\n\tdef _get_debug_info(self):\n\t"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tif args in ['-q', '-qqq', '-qqqq']:\n\t\t\treturn []\n\t\tif args in ['-l', '-lq', '-lqq', '-lqq']:\n\t\t\treturn ['-lqq', '-lqq']\n\t\treturn []\n\n\n\tdef run_program(self, program, main_class, args):\n\t\t\"\"\"\n\t\tRun the given program.\n\n\t\tArguments are as returned by parse_args function.\n\n\t\tArguments to pass to main_class.run_program() are passed in\n\t\ton the command line.\n\n\t\tThe program needs to be defined in self."}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\trequired_arguments = [self.name, self.path]\n\t\n\tfor required_argument in required_arguments:\n\t\tif not self.required.has_key(required_argument) or self.required[required_argument]!= \"\" or self.required[required_argument] is False:\n\t\t\t\traise Exception(\"Missing required argument %s\" % required_argument)\n\t\t\n\treturn required_arguments\n\t\nimport itertools\n\nclass Conjunction(object):\n    \"\"\" \n    A class representing a set of Conjunction objects, consisting of \n    one or more Conjunctions, which is a collection of triples"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\ttry:\n\t\t\tassert_list(args, [self.parser, self.file])\n\t\texcept AssertionError as e:\n\t\t\tlogger.error(\"A malformed input is required for {}.\".format(e))\n\n\tdef _add_args_to_parser(self, parser):\n\t\t\n\t\targs = self.parser.parse_args(args)\n\t\t\n\t\t# validate input\n\t\tif self.parser.error:\n\t\t\traise ArgumentError(self.parser)\n\t\t\n\t\t# validate input\n\t\tif args.read_only:\n\t\t\traise ArgumentWarning(args.read_only)\n\t\t\n\t\tif"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in self._required_args:\n\t\t\tif not callable(arg) and not self._is_callable(arg):\n\t\t\t\traise argparse.ArgumentTypeError(\"Argument '{0}' type is invalid\".format(arg))\n\n\t@property\n\tdef name(self):\n\t\treturn self.argument_value.name\n\n\t@staticmethod\n\tdef is_callable(function):\n\t\treturn function and not hasattr(function, 'argspec')\n\n\t@property\n\tdef required(self):\n\t\tif not self.argument_value:\n\t\t\traise argparse.ArgumentTypeError(\"'require' argument must"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n    def parse_required_args(self, args, missing=False):\n        \"\"\"\n\t\tParse command line arguments that are not required by the user.\n\t\t\"\"\"\n        if args.force:\n            log.debug('Skipping command line arguments, but --force was specified. Skipping.')\n\t\treturn True\n        if args.help is False:\n            log.debug('Skipping command line arguments, but --help was specified. Skipping.')\n        if args.force or not self.validate_command(args):\n            log.debug('Skipping command line arguments, but required and invalid option: \"%s\"', args)\n           "}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n#\targs = list(args)\n\n\tmissing = lambda x,y: x if isinstance(x, (list, tuple)) else y\n\trequired = lambda x,y: isinstance(x, (list, tuple)) and isinstance(y, (list, tuple))\n\n\tfor v in args:\n\t\t#print v\n\t\tif v not in missing():\n\t\t\treturn False\n\treturn True\n\n\nclass _MethodDispatcher(abc.ABC):\n\n\t@abc.abstractmethod\n\tdef __init__(self, parent):\n\n\t\t#self._parent = parent\n\n\t\t#self._methods"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\n\t\n\tif not len(args):\n\t\tself.print_error(\"Missing required arguments.\")\n\t\n\t\n\tself.version = str(get_version(args[0]))\n\t\n\timport re\n\n\tfor arg in args[1:]:\n\t\tif not re.search(\"^\" + re.escape(arg) + \"$\", arg):\n\t\t\tself.print_error(\"Invalid argument:\", arg)\n\t\telse:\n\t\t\tself.print_error(\"The argument \\\"\" + arg + \"\\\" is not a valid parameter.\")\n\t\t\n\ttry_setup_dir(args[0])\n\"\"\"\nThis file is"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not isinstance(arg, basestring):\n\t\t\t\traise TypeError('Argument to validate_requires_args must be str')\n\t\n\tdef validate_python(self, python):\n\t\tif not isinstance(python, bool):\n\t\t\traise TypeError('Argument to validate_python must be a boolean')\n\t\t\n\t\ttry:\n\t\t\tif not isinstance(python, str) or python.lower()!= '-1':\n\t\t\t\traise TypeError('Argument to validate_python must be a string, integer, float')\n\t\t\n\t\texcept AttributeError:\n\t\t\tpass\n\n# Copyright (c) 2015, F"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tfor key in args:\n\t\t\tif not hasattr(self, key):\n\t\t\t\traise ArgumentError(\n\t\t\t\t\t\"'{0}' should be provided in list of {1} arguments\".format(\n\t\t\t\t\t\tkey, type(args))\n\t\t\t\t)\n\n\tdef check_required_args(self, args):\n\t\t\"\"\"\n\t\tCheck if the provided argument list of args in the package provided by the user is present.\n\n\t\tNote that if at least one required argument was provided in the set of arguments. In this case a required argument\n\t\tis required for all other arguments.\n\n\t\tArgs:\n\t\t\targs"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t\tif isinstance(args, basestring):\n\t\t\tif not self.validate_args(args):\n\t\t\t\traise ArgumentError('One or more required positional arguments must be provided')\n\t\tif not isinstance(args, list):\n\t\t\traise ArgumentError('One or more required positional arguments must be provided')\n\t\t\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if the arguments are allowed or not.\n\t\t\"\"\"\n\t\tif (args is not None and len(args) > 0) or ('.' in args):\n\t\t\traise ArgumentError(\"Missing required arguments\")\n\nclass ArgumentError("}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\treturn self.get_options_value(\"args_are_required_for_command_name\", command_name,\n\toptions_spec, self.allow_only_arguments(), [])\n\t\n\tdef _get_argument_value_args(self, command_name, options_spec,\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\n\tcommand_options = []\n\t\n\t\n\t # if there is a mandatory argument or it's not present\n\t\tif options_spec['required_when'] == options_spec['absent']:\n\t\t\t    return self._get_args_from_list(command_name, options_spec, \\\n\t\t\t\t\targs, command_name.lower())\n\t\t\n\t # if a specified argument is required\n\t\tif options_spec['required_when'] is not None:\n\t\t    # if the specified method exists and has a required_when\n\t\t    # attribute then check if it has a required_then attribute\n\t\t    # otherwise, throw an error\n\t\t   "}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\targ_parser = OptionGroup(argument_groups = [\n\t\tOption(\"--check-required-when\",   action=\"store_true\",\n\t    default = False,\n\t    help=\"check request to succeeding with `--required-when`.\")),\n\t\tOption(\"--wait\",                action=\"store_true\",\n\t    default = False,\n\t    help=\"wait for condition to succeeding\")),\n\t\tOption(\"--not-all\",             action=\"store_true\",\n\t    default = False,\n\t    help=\"check that `--not-all` is set to `True`.\")),\n\t\tOption(\"--all\",                   action=\"store"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\nif not hasattr(command_spec, 'is_required'):\n\traise ValueError(\"CommandSpec can not be used as a'required' argument.\")\n\nclass CommandCommandSpec(CmdSpec):\n\t\"\"\"\n\t`CommandCommandSpec([arguments]], {\"command_name\":\"command\", \"argument_names\":args})`\n\t\n\tThe `command_name` can be specified in the configuration, and it is\n\tthe command_spec or the command_argument attributes.\n\t\n\t``arguments`` can be a dictionary or an ``argparse`` object.\n\t\n\tNote: The ``argument_names`` and ``"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\tall_inputs = []\n\t\tfor command_option in command_name.split('_')[1:]:\n\t\t\tall_inputs.append(command_option)\n\t\treturn all_inputs\t\t\n\t\n\tdef requires_required_condition(self, args):\n\t\tfor i, argument in enumerate(args):\n\t\t\tif i in self.options.required_when:\n\t\t\t\treturn True\n\t\treturn False\n\n# Copyright (c) 2015-2016 Cisco Systems, Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\toutput = []\n\t\tfor value in options_spec.values():\n\t\t\tcommand_name_and_arguments = command_name +'' + str(value)\n\t\t\tif not bool(command_name_and_arguments):\n\t\t\t\tcontinue\n\t\t\toutput.append(command_name_and_arguments)\n\t\treturn output\n\t\n\tdef _write_argument(self, commands, arg, options_spec,\n\t\"    {args}\"):\n\t\t\"\"\"\n\t\tWrite ``arg`` to the CLI. Intended to be called multiple times (for example for\n\t\twhen ``sys.argv"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\targs = []\n\t\tfor arg in args_from_command(command_name, options_spec):\n\t\t\tif not arg:\n\t\t\t\tcontinue\n\t\t\tif arg[0] == '!':\n\t\t\t\tcontinue\n\t\t\tif arg[0]!= '!':\n\t\t\t\traise ArgumentError(\"Argument names in %s must \"\n\t\t\t\t\t\"be in command name, '%s'\"\n\t\t\t\t\t% (command_name, arg))\n\t\t\trequired_when = self.allowed_when\n\t\trequired_if = []\n\t\tfor i in range(len(args_from_command(command_name, options_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\ndef _get_conditionally_required_options(\n\tcommand_name, command_path, args, options\n\t):\n\t\"\"\"\n\tList (list of) argument names which match ``required_when`` condition.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t@return: tuple of ``required_when``, ``required_when`` condition\n\n\t\"\"\"\n\trequired_when = _get_arg_and_value(\n\t\tcommand_name,"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t    \n\tif self._options is None:\n\t\tself._options = Options()\n\t\tself._options.add_option(\n\t        '--no-defaults',\n\t        dest='no_defaults',\n\t        action='append',\n\t        type=_BoolAction,\n\t        default=False\n\t        )\n\tif not hasattr(self._options, '__call__'):\n\t\t# self._options.__call__([], options_spec, args)\n\t\traise DistutilsOptionError(\n\t\t        '`{0}\\' method must be called with either `all` '\n\t\t        'arguments as provided'.format(command_name"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\tredef_re = '\\s*(?:\\b\\w{3}\\s*(?:\\b\\w{3}\\s*)*)\\s*\\n'\n\t\tredef_re += '#define %(method_name)s_required_when\\n'%(options_spec)\n\t\tredef_re+= '#define %(method_name)s_required %(type)s, %(required)s\\n'%(\n\t\t        options_spec)\n\t\tredef_re += '#'\n\telif command_name == 'force_redefine_argument':\n\t\n\t\tredef_re = '\\s*(?:\\"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tif not len(args) > MAX_MSG_LENGTH:\n\t\t\tmessages.log.error(\"Command line arguments are too long: {0!r} > {1!r}\".format(MAX_MSG_LENGTH, args))\n\t\telse:\n\t\t\ndef validate_options(options):\n\t\"\"\"\n\tValidate the command line options.\n\t\n\t:param options: The received command line options.\n\t\"\"\"\n\t\ndef format_message(message, result_log):\n\t\"\"\"\n\tFormat a message\n\t\n\t:param message: Message to be formatted.\n\t:param result_"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n#\tvalidate_length_args should be called after parse of arguments\n\tdef validate_length(self, args, value):\n\t\tmaxLength = 0\n\t\tfor arg in args[0:3]:\n\t\t\tif len(arg) > maxLength:\n\t\t\t\tmaxLength = len(arg)\n\t\tif len(args) > 1:\n\t\t\traise ValidationError(\"The provided command argument length must be less than that specified.\")\n\t\tif len(args) < 4:\n\t\t\traise ValidationError(\"the command is required for input arguments.  Please make sure that \"\n\t\t\t\t\"the len argument is"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif len(args) > 255:\n\t\treturn args[:255]\n\telse:\n\t\treturn args # this will be the same as for the arg_parser\n\ndef main(arg_parser=None):\n\t# this is the function in charge of passing the arg_parser as a\n\t# reference\n\treturn arg_parser.main(arg_parser)\n\t\n\t\nif __name__ == \"__main__\":\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"-c\", help=\"Path to CSV file or directory\", type=str)\n\t\tparser.add"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif (len(args) >= 1):\n\t\t\tprint \"Argument list contains length >= 1\"\n\t\t\tprint args\n\t\t\traise ArgumentTypeError(\"Argument list contains 1 element.\")\n\n\tlength = len(args[0])\n\tif length < 1: length = 0\n\tif length > 1000: length = 1000\n\t\n\tmessage = \"Must have length <= 1000 characters.\"\n\tif message not in message_errors:\n\t\tprint message\n\t\traise ArgumentTypeError(message)\n\t\t\nfrom.argparser import Parser\n\ndef build_argparse(parser, argparsers):\n\tparser.add_"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ndef validate_number_args(self, args):\n\t\"\"\"\n\tCheck if value of arguments is not shorter than number specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ndef validate_decimal_args(self, args):\n\t\"\"\"\n\tGet the list of arguments and validate the number args.\n\t\n\t:param args: The received arguments.\n\t:type args: list\n\t:return: A list of arguments, and the number args.\n\t\"\"\"\n\t\ndef validate_str_args(self, args):\n\t\"\"\"\n\tValidate the string args"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) >= CONF.oslo_validate_length_args:\n\t\t\traise TypeError(_('Ensure length of arguments is less than or equal to '\n\t\t\t                '{0} arguments.').format(_(args)))\n\n\tdef _validate_non_negative_integer(self, value):\n\t\tif not 0 <= value <= CONF.oslo_validate_max_integer:\n\t\t\traise TypeError(_('Ensure that maximum value of arguments is an '\n\t\t\t                'integer.'))\n\n\tdef _normalize_args(self, args, enforce_minimum=False):\n\t\t\"\"\"\n\t\tNormalize value of arguments passed"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif len(args) > self.min_args:\n\t\t\traise ValidationError(\"Invalid \" + self.min_args + \" argument.\")\n\n\nclass MinLengthValidator(Validator):\n\t\"\"\"\n\tThe valid value for the min_length attribute in min_length_validators\n\targument based on the validator's type's default values\n\t\n\t:param kwargs: The keyword arguments.\n\t\"\"\"\n\tdef __call__( self, value, **kwargs ):\n\t\tif len(self.min_length) > len( value ):\n\t\t\tvalue = value[:self.min_length]\n\t\treturn"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass ReqTypeEnum(Enum):\n\t\"\"\"\n\tThe response type.\n\t\"\"\"\n\tGET =\t\t\t1\n\tPOST =\t\t\t2\n\tPUT =\t\t\t3\n\tDELETE =\t\t4\n\tUPDATE =\t\t5\n\tDELETE_ALL =\t\t6\n\n\tclass GET_NOT_ENOUGH_BODY(Enum):\n\t\t\"\"\"\n\t\tReturns HTTP status code NOT found from request.\n\t\t\"\"\"\n\t\tBAD_METHOD =\t\t10\n\t\tFORBIDDEN =\t\t11\n\t\tNO_MEDIA =\t\t12\n\n\t\tclass POST"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\t# check if length provided\n\ttry:\n\t\tvalue = int(args.get(\"length\"))\n\t\tif value < 0:\n\t\t\traise ValueError(\"Length must be less than 0.\")\n\t\t\n\texcept ValueError as e:\n\t\traise ValueError(\"Length must be a positive integer or greater than 0. %s\" % e)\n\t\n\t# check that length is > 0\n\ttry:\n\t\tif not (0 <= int(args.get(\"length\")) <= 1):\n\t\t\traise ValueError(\"Arguments must be non-negative integer.\")\n\texcept ValueError as e:\n\t\traise ValueError(\"Arguments"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tmax_length = len(args)\n\tif args and len(args) > 0 and args[0] in \"01234567890\":\n\t\tif max_length > len(args):\n\t\t\traise ValueError(\"Length of arguments can not exceed length, it should be at least %(max_length)s characters and it is %(len)s characters. Think of argument(s) is %(len_args)s.\")\\\n\t\t\t\t% locals())\n\nfrom django.contrib import admin\nfrom datetime import datetime\nfrom django.db import models\nfrom django.core.urlresolvers import reverse\n"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ttry:\n\t\t\tif args[0] in self.list_available_choices:\n\t\t\t\treturn True\n\t\texcept ValueError:\n\t\t\treturn False\n\tdef _validate_choice_value(self, args, key):\n\t\tchoices = self.choices\n\t\ttry:\n\t\t\tchoices.index(key)\n\t\texcept ValueError:\n\t\t\tpass\n\t\telse:\n\t\t\treturn args\n\tdef _validate_choices_key(self, args):\n\t\tkey = args[0]\n\t\tif key in self.list_choices:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg not in (None, \"\", \"0..1\") and self.args_to_check:\n\t\t\t\treturn arg\n\n\n_valid_choices = [\n\t(\"1\", _(\"one\")),\n\t(\"0\", _(\"zero\")),\n\t(\"1_or_zero\", _(\"yes\")),\n\t(\"1_or_n_or_zero\", _(\"no\")),\n]\n\nclass Validchoices(object, WIKip):\n\t\"\"\"Interface class for valid choices which can be validated as input.\"\"\"\n\n\tVALID_OPTIONS = list(\n\t\twato"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t        if isinstance(args, types.ListType):\n\t\tchoices = [choices]\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\t\tfor arg in args:\n\t\t\tif not isinstance(arg, (list, tuple)):\n\t\t\t\targ = [arg]\n\t\t\tif not isinstance(arg, types.StringTypes):\n\t\t\t\traise CardinalityError('expected a list or tuple')\n\t\t\tif arg not in choices:\n\t\t\t\traise CardinalityError('%s not in choice list' % type(arg))\n\t\t\t\n\n\nclass Cardinality:\n\tdef __init"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ttry:\n\t\t\tif len(args) < 1:\n\t\t\t\traise ValueError(\"Invalid argument when reading value.\")\n\t\texcept (ValueError, TypeError):\n\t\t\traise AttributeError(\"Received invalid value for \" + self.__class__.__name__ + \" instance.\")\n\n\tdef get_choice_default(self):\n\t\t\"\"\"\n\t\tThe default value of choice argument.\n\t\n\t:return: Choice instance of the type as a string and None as specified by its value.\n\t\"\"\"\n\t\treturn self._choice_config.value\n\n\tdef get_choice_choices(self):\n\t\t\"\"\"\n\t\tA convenience"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tfor arg in args:\n\t\t\tif arg.__class__ == Choice and arg.name() in self.__class__.choice_choices:\n\t\t\t\treturn False\t# if not, try to parse it correctly\n\t\t\t\n\t\t\telif (arg.name() == \"\" and args.__dict__[arg.name()][\"type\"] == \"text\" \n\t\t\t\tand self.__class__.choice_choices[arg.name()][\"type\"] == \"combo\" \n\t\t\t\tand arg.name() in self.__class__.choice_choices):\n\t\t\t\treturn False\n\t\n\treturn True\t# return False if not allowed\n\n\nclass Choice:"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tif len(args) > 1:\n\t\tif type(_) is not list:\n\t\t\targs = [args]\n\t\tif set(args)!= set(_):\n\t\t\tfrappe.throw(_('Wrong choices in the arguments.'))\n\n\nclass FormValidator(object):\n\tdef __init__(self):\n\t\tsuper(FormValidator, self).__init__()\n\t\t\n\t\tself.form_list = [ ]\n\t\t\n\tdef validate_fields(self, fields):\n\t\tfields = list(fields)\n\t\tvalidate_valid_fields(fields)\n\t\t\n\tdef validate_for_children(self, field"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif self.value not in choices:\n\t\t\traise ArgumentError('Valid choices are: %s; got %s: %s.%s.' % \n\t\t\t\t(choices.keys(), choices.values(),\n\t\t\t\tself.key, self.value, self.key))\n\ndef format_argument(val):\n\t\"\"\"\n\tReturn a text-formatted argument value.\n\t\"\"\"\n\t\tformat = '%(key)s %(value)s'\n\t\tif val:\n\t\t\tpos_key ='= %(pos)s' % {'key': val, \n\t\t\t\t'pos': get_pos_"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif not self.is_available(args['choices']):\n\t\t\treturn False\n\t\n\tif not self.is_available(args['choices'].strip()):\n\t\t\treturn False\n\t\n\treturn True\n\t\t\ndef read_options(options):\n\t\"\"\" Read values from the database, from the provided dict.\n\t\n\t:param options: A dictionary of the values to read.\n\t\"\"\"\n\tvalues = read_yaml(options['file_path'])\n\t\n\treturn values\n\n\ndef select_options(options, index):\n\t\"\"\" Select values from the specified options list. \"\"\"\n\t\n\tfor option in"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif not isinstance(args, list):\n\t\t\traise AssertionError('Value of choice arguments must be a list.')\n\t\tfor i in range(1, len(args)):\n\t\t\tif args[i] not in self.choices:\n\t\t\t\traise AssertionError('Value of choice value is only one of the available choices.')\n\t\n\tdef validate_input_type(self, input_type, params):\n\t\t\"\"\"\tValidates the type and its initial value.\n\t\t\n\t\t:param input_type: The type to validate. Choices are %(choices)s which must be an integer, but %(choices_min)"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tdef validate_choice(choice):\n\t\t\tif argument_has_length(choice, \"choices\") \\\n\t\t\t\tand not argument_has_length(choice, \"mandatory\"): # if the mandatory argument is\n\t\t\t\traise InvalidArgumentException(\"%s doesn't match any %schoices.\" % (choice, \", \".join(sorted(choices))))\n\t\t\tif argument_has_length(choice, \"required\"):\n\t\t\t\traise InvalidArgumentException(\"%s doesn't match any required %schoices.\" % (choice, \", \".join(sorted(choices))))\n\t\t\treturn choice\n\n\t\tdef validate_single_choice("}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\tif args is None:\n\t\traise ConfigurationError(\"Value of minimum value must be provided\")\n\t\n\t# We need to look at minimum and maximum values.  Use min\n\t# and max because the max check doesn't do it until min exceeds\n\t# max.\n\tassert(min(args) <= max(args))\n\t\n\treturn args\n\n\nclass Sender(object, metaclass=abc.ABCMeta):\n\t\"\"\"\n\tA subclass of :py:class:`Sender`, which provides methods and additional behaviour in the\n\t:py:class:`NotificationManager`.\n\n\tIf you have to"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\n\trequired_data = ['start_time', 'end_time', 'number_of_points','minimum_point','maximum_point']\n\t\n\tfor key in required_data:\n\t\tif args.get(key):\n\t\t\tif(args.get(key) >= args.get(required_data[key])):\n\t\t\t\treturn \"Minimum than maximum values given.\"\n\t# end for\n\t\n\treturn False\t\t\n\nclass DataBase(object):\n\t\"\"\"\n\tA DataBase is the base data class for all other data classes.\n\t\"\"\"\n\t\n\t\n\t__metaclass__"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tmin_val_min = args[0]\n\tmin_val_max = args[1]\n\tmax_val_min = args[2]\n\tmax_val_max = args[3]\n\n\tif min_val_min <= max_val_min:\n\t\traise InvalidMinOrMax(\n\t\t\t\"The value of the %s function can't be between minimum \"\n\t\t\t\"%s and maximum %s.\".format(\n\t\t\t\tself.__class__.__name__,\n\t\t\t\tmin_val_min,\n\t\t\t\tmax_val_min\n\t\t\t)\n\t\t)\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n            if not (0 <= arg <= 100):\n\t\t\t\tmessage = _(\"Range %(min_value)s %(max_value)s\") % {\n\t\t\t\t\t\"min_value\": arg,\n\t\t\t\t\t\"max_value\": 100\n\t\t\t\t}\n\t\t\t\traise TypeError(message)\n\n\tdef __call__(self, value):\n\t\tif isinstance(value, int) or isinstance(value, float):\n\t\t\treturn self.max_value_validation(value)\n\t\ttry:\n\t\t\treturn self.min_value_validation(value)\n\t\texcept ValueError as e:\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ndef get_min_max_value(args):\n\t\"\"\"\n\tCall get_min_max_value() using the given args.\n\t\n\t:param args: The arguments passed in to the function.\n\t\n\t:return: The value for the min(max_value) args.\n\t\"\"\"\n\t\ndef validate_max_value(args):\n\t\"\"\"\n\tCheck if value of arguments is between maximum and minimum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ndef get_valid_max_value(args):\n\t\"\"\"\n\tCall get_valid"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif not args.min_max:\n\t\t\treturn\n\t\tif not isinstance(args.min_max, (int, float)):\n\t\t\tmsg = _(\"Argmin/max must be an integer >= %(min_max)g, \"\n\t\t\t\t\"i.e., must a float < %(min_max)g or float > %(min_max)g\")\n\t\t\traise forms.ValidationError(msg % {'min_max': args.min_max})\n\n\tdef validate_max_args(self, args):\n\t\tmax_arg = args.max_arg\n\t\tif isinstance(max"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\ndef validate_min_max_args_no_zero(self, args):\n\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum.\n\t\"\"\"\n\n\t\nclass Arg(object):\n\t\"\"\"\n\tThis is what gets called when the command is used.\n\t\"\"\"\n\n\tdef __init__(self, type):\n\t\t\"\"\"\n\t\tThis is called for every command the type is.\n\t\t\n\t\tArgs: \n\t\ttype: the type of command.\n\t\t\"\"\"\n\t\t\n\t\tself.type = type\n\t\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the name of"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t\t\n\tif len(args) < 4:\n\t\tlogger.warning(\"Invalid number of arguments\")\n\t\traise Exception(\"Invalid number of arguments\")\n\t\n\tmin_val, max_val = args[0:4]\n\tif isinstance(min_val, basestring):\n\t\ttry:\n\t\t\tint(min_val)\n\t\t\treturn True\n\t\texcept ValueError:\n\t\t\tlogger.warning(\"min_val must be of type string\")\n\t\t\tlogger.warning(\"This is currently a deprecated code, please run the update function.\")\n\t\t\treturn False\n\telse:\n\t\tmin_val = float"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\nclass ValidatingArg(object):\n\t\"\"\"\n\tA validation of argument type.\n\n\t:param name: Function name to be stored.\n\t:param description: Description for this argument.\n\t:param type: Type of argument (one of ``argument``, ``integer``,\n\t``string``, or ``float``).\n\t\"\"\"\n\t\n\tdef __init__(self, name, description, type=None):\n\t\t\"\"\"\n\t\tCreate a new instance of ``ValidationArg``.\n\t\t\n\t\t:param name: ``Name`` of the validator instance.\n\t\t\n\t\t\t* ``validate_length`` - The length"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif sum(map(lambda x: x.lower(), args)) not in _MIN_MAX_OPTIONS:\n\t\t\t# no minimum or maximum\n\t\t\treturn\n\t\telse:\n\t\t\treturn\n\n\tdef validate_min_max_args_are_a_string(self, args):\n\t\"\"\"\n\tCheck if value of arguments is a list of valid integers.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tif not isinstance(args, (list, tuple)):\n\t\t\t# only one arg must be a tuple\n\t\t\treturn\n\t\telse:\n\t\t\t# validate that each"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('command', metavar='command', help = '''\n  Run the given command\n  \n  The command name may be defined with the following format:\n    command: name=value\n\n- command: --longarg=value  Specifies an integer argument to the command that\n  has the form -m1 -Nc > Nc(1) and specifies an argument to the command that\n  returns the last valid value.  If the args contain two arguments, it\n  is interpreted as as many times as many args that will occur"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\n\tprint '-----------------------------'\n\t\n\tif type_name!= '':\n\t\tprint 'type_name == %s' % type_name\n\n\tfor spec in option_spec:\n\t\targ_type_name = spec.get('type_name')\n\t\tdefault_arg_type = [default_arg_type_name]\n\t\t\n\t\tif type_name!= '':\t\n\t\t\targ = 'type = %s' % arg_type_name\n\t\t\t\n\t\t\tif arg_type_name!= '':\n\t\t\t\tcmd.append('type = %s' % arg_type_name)\n\t\t\n\t\tif"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\nclass Type(object):\n\tdef __init__(self, subcommand, help):\n\t\tself._subcommand      = subcommand\n\t\tself._subcommand_help = help    \n\n\t\tself._subcommand_help['description']     = self._subcommand_help['help_text']\n\t\t\n\t\tself._subcommand[2:] = self.parse(self._subcommand_help['long_description'])\n\t\t\n\t\tself._subcommand_help['long_description'] = \\\n\t\t\tself._subcommand[2:] if self._subcommand_help['long_description'] is not None"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\treturn self.argument_types[type_name].complex_arg_type\n\t#return complex_argument_type\n\t#self.argument_types[type_name].complex_arg_type = ComplexArgType\n\t#self.argument_types[type_name].complex_arg_type = ComplexArgType\n\t#print(\"create_complex_argumet_type: type_name: {}\".format(type_name))\n\ndef parse_all_definitions(definitions, type_name=None, option_name=None):\n\t\"\"\"\n\tParse a list"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\n\tif is_number(type_name):\n\t\t# is a type that's numeric\n\t\tif type_name == 'complex':\n\t\t\tif not is_number(self.get_argumets(type_name)):\n\t\t\t\traise ArgumentTypeError(self.get_argumets(type_name), '%s')    \n\t\telse:\n\t\t\t# is a type that's string\n\t\t\tif not is_string(self.get_argumets(type_name)):\n\t\t\t\traise ArgumentTypeError(self.get_argumets(type_name), '%s')\n\t\n\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\n\t\n\n\nclass CppType(object):\n\t\"\"\"\n\tClass containing cpp types\n\t\"\"\"\n\tdef __init__(self, name, alias, description, options):\n\t\tself.name = name\n\t\tself.cname = alias\n\t\tself.description = description\n\t\tself.options = options\n\t\n\t\t# options that are not relevant\n\t\tself.non_opt_args = options.__dict__\n\t\t# self.options is a map of (option (default), default_value)\n\t\t\n\t\t# options that are options declared through the\n\t\t# option definition string\n\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\n\t#if isinstance(type_name, tuple):\n\t#\ttype_name, arg_type = type_name\n\t#\tif arg_type == 'int':\n\t#\t\treturn 'int', '-1'\n\treturn None, None\n\n\ndef create_complex_parameter(self, parameter_name, option_name,\n\tspec_option, sub_option):\t\n\t\"\"\"\n\tBuild a complex parameter name.\n\t\n\t:param parameter_name: the parameter name\n\t:param spec_option: parameter's specifications\n\t:param sub_option:"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\treturn type_name\n\t\n\tdef get_default_value(self, subcommand, spec_option, option_name):\n\t\t# TODO(shickey): if no default value, use the user as default.\n\t\t# In the future, support to support options like --arg\n\t\tif not self._parser_by_name.has_key(subcommand):\n\t\t\traise ValueError(_(u\"Unknown command '%(subcommand)s'\") % \\\n\t\t\t\t{'subcommand': subcommand })\n\t\t\n\t\tdef is_boolean(data):\n\t\t\ttry:\n\t\t\t\tboolean_value = data."}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\t\n\t# construct the command class\n\tclass cls(type):\n\t\t\n\t\tdef __init__(self, options):\n\t\t\t\n\t\t\n\t\t'''\n\t\t\tConstruct a class of the complex type to be used as an instance of this class\n\t\t\t'''\t\t\n\t\t\t\n\t\t\n\t\t# do not add any option in this method.\n\t\tdef __argumet_init__(self):\n\t\t\t\n\t\tdef __argumet_init_complex__(self, subcommand, type_name, option_name,\n\t\t\t\t\t\t\tspec_option, subparser = None):\n\t\t\t\n\t\t'''\n\t\t\tInitialize the complex type to be used as an instance of"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\nclass CustomArgumetType(complex):\n\t\"\"\"\n\tWrap an argumet type in one argument\n\t\"\"\"\n\nclass CustomArgumet(object):\n\t\"\"\"\n\tAdd custom argumet\n\t\"\"\"\n\tdef __init__(self, type):\n\t\t\"\"\"\n\t\t:param type: the complex type to add\n\t\t\"\"\"\n\t\tself.type = type\n\nclass ArgumentWithType(complex):\n\t\"\"\"\n\tWrap a real-valued argumet\n\t\"\"\"\n\tdef __init__(self, inner):\n\t\t\"\"\"\n\t\t:param inner:"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tcustom_args=[]\n\t\n\tif args and len(args)==0:\n\t\tansible.log.debug('No default arguments should be specified when'\n\t\t\t'calling the script.')\n\t\treturn (None, None)\n\t\n\tif args:\n\t\tif isinstance(args[0], dict):\n\t\t\tfor item in args[0]:\n\t\t\t\tif isinstance(args[1], dict) and 'control_args' in args[1]:\n\t\t\t\t\tcustom_args.append(item)\n\t\t\tcustom_args=[]\n\t\t\tfor item in args[0]:\n\t\t\t\tif isinstance(item,"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\tcontrol_args = {}\n\t\ndef main():\n\targ_spec = sys.argv[1:][0]\n\tif arg_spec == 'control':\n\t\targs = list(sys.argv[2])\n\telse:\n\t\targs = parse_args(arg_spec, add_help=False)\n\n\tif not args or not args.control_dict:\n\t\tsys.exit('{} requires at least one control argument'.format(sys.argv[0]))\n\tif len(args.control_dict) == 0:\n\t\tsys.exit('{} does not allow"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\t# if args is not a list, let's assume it's \"flat\" for any\n\tif 'control_args' not in args:\n\t\targs = self.get_args(args)\n\t\t\n\t\t# parse input from cli args\n\t\tif isinstance(args, dict):\n\t\t\targs = CliArgs()\n\t\t\n\t\t# parse input from cli input\n\t\telif isinstance(args, list):\n\t\t\t# construct a dict from the input\n\t\t\tcli_input = dict(list(args))\n\t\telif isinstance(args, AnsibleCliArgs):\n\t\t\t# generate dict from the input\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\t# Make sure the input arguments are consistent\n   \tif (self.spec.get('action'))!='merge':\n\t\tself.fail_json(msg=\"Need 'action' param\")\n\n    def add_arguments(self, args_add):\n\t\"\"\"\n\tAdd one or more arguments to the command line.\n\t\n\tArgs:\n\t   args_add: dict, mapping input arguments to the parsed arguments.\n\t\"\"\"\n\targs_add.update(self.args.items())\n\n\t# Make sure the args_add are not in the spec\n   \tassert self.args.keys()"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\nclass CliRunner(object):\n\tdef __init__(self, host, args):\t\t\t\n\t\tpass\n\t\t\n\tdef get_inventory_from_spec(self, params):\t\n\t\treturn self.ansible.get_inventory_from_param(params)\n\t\t\n\tdef get_nested_from_cli_vars(self, cli_vars):\t\n\t\treturn self.ansible.get_nested_from_param(cli_vars)\n\t\t\n\tdef load_to_args(self, args):\n\t\t\n\t\tparams = get_params(args)\n\t\t\n\t\tself.ansible = AnsiblePlay"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n#\n\t\ndef _print_json_data(raw_data, indent):\n\tjson.dump(raw_data,stream=sys.stdout,indent=indent,indent_dict=True)\n\t\ndef _print_raw_data(raw_data, indent):\n\tprint repr(raw_data)\n\ndef get_spec_yml(spec_path, config, yaml_file):\n\t\"\"\"\n\tGet the config YAML from the spec path and the config.yml.\n\n\t:param spec_path: path of the spec yaml file\n\t:param config: config"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tif args and isinstance(args[-1], dict):\n\t\treturn self._get_nested_yaml_dict(args[-1])\n\telif args is not None:\n\t\t# Allow a list of args in args, to allow nested\n\t\t# argument specification to not break the yaml parsing.\n\t\tif len(args) == 0 or isinstance(args[0], list):\n\t\t\treturn self._get_nested_yaml_dict(args[0])\n\t\telse:\n\t\t\treturn self._get_nested_yaml_dict(args)\n\treturn {}\t\n\t\t\ndef get_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n#\t\treturn self.set_var(args[0], args[1], args[2])\n\t\t\n\t\n#\t\treturn self.add_custom(args[0], args[1], args[2])\n\t\t\n\t\n#\t\treturn self.add_ctrl(args[0], args[1])\n\t\n#=======================================================================================\n# The following lines are only called by the ansible playbooks\n#=======================================================================================\n\nclass Host(object):\n\t\"\"\"\n\tRepresents a host in the docker network.\n\t\"\"\"\n\t\n\tdef __init__(self, data, name):\n\t\t\"\"\"\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\targuments = self.split_command_line(args, '--', self.control_args)\n\tcontrol_args = self.split_command_line(arguments, '--', self.control_args.split(','))\n\tnested_args = self.split_command_line(arguments, '--', self.config.get('spec_yml'))\n\tcustom_args = self.split_command_line(arguments, '--', self.control_args.split(','))\n\t\n\tif self.config.has('ansible_connection'):\n\t\tansible_connection = self.config['ansible"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t\n\t# pylint: disable=too-many-arguments\n\tdef _merge_dict(d, new_d):\n\t\t# type: (dict) -> Dict[str, Any]\n\t\t\n\t\t# for nested args\n\t\tif 'custom' in d:\t\n\t\t\tfor key, _ in d['custom'].iteritems():\n\t\t\t\tfor new_key, _ in d[key].iteritems():\n\t\t\t\t\tif new_key not in new_d:\n\t\t\t\t\t\tnew_d[new_key] = {}\n\t\t\t\t\tnew_d[new_key].update(d[key][new_key"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n#\t\tif vars_dict.has_key('extra_vars'):\n#\t\t\tfor k in vars_dict['extra_vars']:\n#\t\t\t\tif type(vars_dict['extra_vars'][k]) is dict:\n#\t\t\t\t\textra_vars.add_dict(vars_dict['extra_vars'][k])\n#\t\t\telse:\n#\t\t\t\textra_vars.update(vars_dict['extra_vars'])\n\t\t\n\t#\t\tif vars_dict.has_key('args'):\n\t#\t\t\textra_vars.update(eval(vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\textra_vars_list = []\n\n\tfor k,v in list(vars_dict.items()):\n\t\textra_vars_list.append(v+\"_extra_\" + k)\n\t\n\tnew_vars_list = []\n\tfor k,v in extra_vars_list:\n\t\tnew_vars_list.append(v)\n\t\t\n\t#print sorted(new_vars_list, key = lambda k: len(v))\n\t\t\t\t\n\treturn vars_dict, new_vars_list\n\n\ndef merge_extra_vars_with_path(paths, extra_vars_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\n\t# if vars is not a dict, add it to the extra vars list\n\tif not vars_dict == {}:\n\t\tfor key in vars_dict:\n\t\t\tif not key in extra_vars:\n\t\t\t\textra_vars.append(key)\n\treturn vars_dict\n\n\ndef merge_envvars(envvars_to_merge):\n\t\"\"\"\n\tMerge additional environment variables into ``envvars`` dict.\n\tIt is used to merge in environment variables specified in :\n\n\t* ``vars_to_merge`` -> list of environment variables to merge\n\t* ``extras``"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\tif extra_vars:\n\t\t\tfor v in extra_vars:\n\t\t\t\tvars_dict.pop(v,None)\n\t\tnew_vars = vars_dict.copy()\n\t\tfor k in vars_dict:\n\t\t\tif k not in new_vars:\n\t\t\t\tnew_vars[k] = vars_dict.pop(k)\n\t\treturn new_vars\n\ndef read(filename, default=-1):\n\t\"\"\"\n\tRead a python script into a string.\n\t\n\tThe string defaults to the variable specified by the ``default`` \n\tparameter.\n\t\n\t:param filename"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\ndef merge_extra_vars_2(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` in a single level\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\tif extra_vars is None:\n\t\textra_vars = extra_vars[:]\n\tfor var in vars_dict:\n\t\tif var in extra_vars:\n\t\t\tvars_dict[var] = extra_vars.pop(var)\n\t\telse:\n\t\t\tvars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\tif extra_vars is None:\n\t\textra_vars = []\n\t\t\n\tfor k,v in vars_dict.items():\n\t\t# Make sure we have the same number of vars in all subvars.\n\t\tif len(extra_vars) > len(k):\n\t\t\textra_vars = extra_vars[:len(k)] + extra_vars[len(k):]\n\t\n\tfor i,v in enumerate(extra_vars):\n\t\t# Handle the type check above\n\t\tif v:\n\t\t\tsetattr(i, 'variable_manager', v)\n\ndef merge_extra"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\t#print 'Merging variables with'+ str(extra_vars)\n\t\t#print('vars_dict:'+ str( vars_dict ) )\n\t\t#print 'extra_vars:'+ str( sorted( extra_vars ) )\n\t\t\n\t\t# Merge the items from dicts to vars_dict\n\t\tfor item_dict in vars_dict.items():\n\t\t\tif type(item_dict[1])!= type( vars_dict[item_dict[0]] ):\n\t\t\t\tprint 'Error:'+ str( item_dict ) + ', expected one of the following types:\\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\t# Remove any extra vars and add default values\n\tfor k in extra_vars:\n\t\tif k not in vars_dict:\n\t\t\tvars_dict[k] = vars_dict[k]\n\t\n\t# Merge ``vars_dict`` and ``extra_vars`` variables\n\tfor k, v in vars_dict.items():\n\t\tvars_dict[k] = merge_extra_vars(v, extra_vars)\n\t\n\t# Merge all additional options for ``vars``\n\tfor k, v in extra_vars.items():\n\t\tvars_dict[k] ="}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\t\n\textra_vars = extra_vars if extra_vars else sys.argv[1:]\n\t\n\t\n\tmerge_dict = {}\n\tfor k in extra_vars:\n\t\tif k in vars_dict:\n\t\t\tmerge_dict[vars_dict[k]].update(vars_dict[k])\n\t\telse:\n\t\t\tdel vars_dict[k]\n\t\t\n\tvars_dict = merge_dict\n\t\t\n\treturn vars_dict\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n    \n    \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n# TODO: Remove these when they are supported.\n#if not kwargs.get(\"pre_exec\"):\n#    raise Exception(\"kwargs['pre_exec'] should be overridden.\")\n\n# -*- coding: utf-8 -*-\n# Copyright 2015-2016 Jacob Kaplan-Moss\n# Copyright 2015-2016 The pygit2py authors\n#\n# This file is part of pygit2py.\n#\n# pygit2py is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by\n#"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t \n\tp = ansible_playbook_cli(ir_workspace, \"start\",\n\t\t\t'--verbose=%s' % verbose,\n\t\t\t'--playbookpath=%s' % playbook_path)\n\t\n###\n# A plugin implementation for the current plugin\n###\n\t\n\ndef ansible_playbook_cli(ir_workspace, command, verbose=None,\n\t\t# extra_vars is ignored since this code does not work\n\t\t# in most cases!\n\t\t# 'extra_vars' becomes: 'args=dict(plugin.args)',\n\t# and 'args' becomes: 'args=dict(plugin"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\nclass AnsibleModule(object):\n\t\"\"\"\n\tA base class for all methods the playbook parser and the\n\tCLI. Also a helper for the ansible_playbook CLI.\n\t\"\"\"\n\t\nclass AnsibleParser(object):\n\t\"\"\"\n\tA parser which takes a playbook file and returns a AnsibleModule object.\n\t\"\"\"\n\t\t\n\t\nclass Plugin(AnsibleModule):\n\t\"\"\"\n\tImplements parsing plugins: the infrared plugin is a dictionary\n\tthat is populated with the parameters passed in.\n\t\"\"\"\n\t\t\n\t\ndef load_plugin(name, plugin_paths, plugins, modules, module_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\ndef add_to_module(module_type, ansible_args, module, extra_vars):\n\t\"\"\" Add Ansible module to Ansible's working directory.\n\t\n\t:param module_type: The type of module you want to add\n\t:param ansible_args: a dict of arguments to the ansible-playbook function\n\t\twhich should be passed to the ansible-module function\n\t:param module: A module to add\n\t:param extra_vars: a dict of extra-vars that are passed to the ansible-module function\n\t\"\"\"\n\t\n\tansible_module_directory = module.get_"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\tif extra_vars:\n\t\t\treturn ast.literal_eval(extra_vars)\n\t\t\n\t\tif playbook_path:\n\t\t\treturn IrWorkflow.invoke_plugin(ir_workspace, ir_plugin, verbose=verbose,\n\t\t\t\tplaybook_path=playbook_path, full_path=playbook_path)\n\t\t\n\t\treturn None\n\treturn AnsibleWorkflow()\n\ndef isolate_or_not_ansible(ir_workspace, ir_plugin, playbook_path, inventory, isolate_script, target_owner=None):\n\t\"\"\"\n\tReturns whether or not to insure that the"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t    def run(*args, **kwargs):\n                # Don't call ansible-playbook if not being in CLI.\n                # See issue #1280 for more details\n\n\t\tansible_plugin = ir_workspace.get_module(ir_plugin)\n\n\t\targs.extend(playbook_path)\n\n\t\tif verbose is None: verbose = self._DEFAULT_VERBOSE\n\n\t\tif verbose: ansible_plugin.verbosity = verbosity\n\n\trun(*args, **kwargs)\n\n\tif verbose is not None:\n\t\tif logger.isEnabledFor(logging.DEBUG):\n\t\t\tlogger.debug(\"ansible.playbook: %"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\n\tif verbose == 'debug':\n\t\tif verbose_level >= 2:\n\t\t\tprint '>>>'+ os.path.basename(playbook_path)\t\n\t\tprint 'Opening in infrared wise tool'\n\telse:\n\t\tif verbose_level >= 2:\n\t\t\tprint '>>>'+ os.path.basename(playbook_path)\t\n\t\tprint 'Opening in current plugin'\n\tif verbose_level >= 3:\n\t\tprint '>>> Runner:'+ str(ir_worker)\n\tif verbose_level >= 2:\n\t\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\nclass InfraredPlugin(object):\n\tdef __init__(self, module, plugin_opts, workdir):\n\t\t\"\"\"\n\t\tThe base class for all Ansible plugins.\n\t\n\t:param module: The module which provides the working directory\n\t:param plugin_opts: A dict of plugin options. If a section exists for a\n    \tspecific plugin with the section containing the plugin configuration, the named\n\tplugin is appended. If a section contains a single subsection, the named\n\tplugin is appended in. Otherwise, it is appended to the default config\n\tsection.\n\t:param work"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\nclass InfraredPlugin(Infrared):\n\t\n\t\"\"\"\n\tThe 'ansible_playbook' CLI interface.\n\t\n\tThe main Infrared plugin interface calls a module within the current\n\tworkspace, then calls the actual functionality of the module via the\n\tmain() function.\n\t\n\t:param ir_workspace: The Infrared Workspace object representing the active\n\tworkspace\n\t:param ir_plugin: The InfraredPlugin object of the current plugin\n\t:param playbook_path: The playbook to invoke\n\t:param verbose: The Ansible verbosity level\n\t:param extra_vars"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\nclass InfraredPlugin(object):\n\n  def __init__(self, name):\n    if name not in sys.modules:\n      sys.modules[name] = self.Module(name)\n    self.name = name\n\n  def build(self):\n    if self.name not in sys.modules:\n      sys.modules[self.name] = get_plugin_by_name(self.name)\n\n  def __getattr__(self, name):\n    return getattr(self.build(), name)\n\n  def run(self):\n    ir_plugin = InfraredPlugin(\"ansible-playbook\")\n    result ="}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t  # set up extra vars here so we can see from the playbook itself\n\tansible_vars = dict(\n\tansible_host = host_or_ip, \n\tansible_host_vars = _extract_ansible_variables_from_ansible_playbook(ir_workspace,\n\t                                                         host_or_ip, verbose=verbose,\n\t                                                         extra_vars=extra_vars)\n\t)\n\t\n\t# create an Outdated Playbook instance and the Infrared plugin.\n\tout = out_plugin(ir_workspace, module, playbook_path)\n\tout.create()"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\t# Do the 'playbook load' plugin\n\t# In this module we actually want to know what to run in the CLI\n\tansible_args = _get_ansible_args(cli_args, cli_workspace)\n\t\n\tlogger.debug('Loading playbook from %s' % vars_dict)\n\t\n\t# Run playbooks\n\tplays = _run_playbook(ansible_args, vars_dict, ir_workspace, ir_plugin)\n\t\n\tlogger.debug('Playbook results: %s' % (plays,))\n\t\n\treturn plays\n\ndef _run_playbook(ansible_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\t# make sure these are imported in order to avoid circular dependencies and\n\t# to ensure that the plugin is loaded properly\n\ttry:\n\t    from ansible.playbook.block import Block\n\t    from ansible.playbook.roletask import RoleTask\n\t\t\n\t\tvars_dict.update(vars_dict)\n\texcept ImportError:\n\t    raise AnsibleError('The Python MX-PM or MX-CI modules require `the json_inventory` python library.')\n\t\t\t\n\ttry:\n\t    from ansible.plugins import load_plugins as pl_loader\n\t    from ansible.utils.vars import combine_vars as _combine_vars\n\t   "}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tvars = _get_vars(vars_dict)\n\tansible_cli = execute_ansible_cli(vars_dict, cli_args)\n\n\tif isinstance(ansible_cli, Ansible):        \n\t\tansible_results = ansible_cli.run()\n\n\telif isinstance(ansible_cli, Fail):\n\t\tansible_cli.fail()\n\n\telif isinstance(ansible_cli, AnsibleExitJson):\n\t\tansible_results = ansible_cli.run()\n\n\telse:\n\tansible_cli.exit()\n\t\n\n\ndef execute_ansible_playbook(vars_dict, cli"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\n# =================================================================================\n# MODULE CONSTANTS \n# =================================================================================\nDEFAULT_TASK_ID = 'playbook'\nDEFAULT_EVENT = 'default'\nDEFAULT_NAME = '/localhost/webapps/ansible/playbook.yml'\nDEFAULT_PORT = 8001\n\n# =================================================================================\n# INITIALIZATIONS \n# =================================================================================\ndef _init_vars(cli_args):\n    \n   # get default task id\n   if isinstance(cli_args, dict):\n      default_task_id = cli_args.get('DefaultTaskId', DEFAULT_TASK_ID)\n      \n      # the default"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\ndef generate_data(cfg, ansible_args, var_dict):\n\t\"\"\"\n\tGenerates playbook by config, and runs ansible cli using vars dict\n\t\"\"\"\n\tlogger.info(\"Generating playbook and execution data\")\n\t\n\t# Parse CLI args \n\t# TODO: Identify this code\n\tcmd_args = shlex.split(cli_args)\n\tif len(cmd_args) > 0:\n\t\tif cmd_args[0] == \"list\":\n\t\t\tansible_args = parse_cli_args(ansible_args, var_dict)\n\t\t\tansible_args"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\n\tir_args[0] = 'ansible_playbook'\t\n\tif vars_dict.get('options'):\n\t\tir_args = [ \"add_key\", \"name=%s\" % (vars_dict['options'].get('ansible_playbook_name', '')) ] * len(ir_args)\n\n\tvariables = get_variables(ir_workspace)\n\tif not variables:\n\t\treturn {\"msg\": \"No vars found for current playbook\"}\n\tvariables['_inputenv_vars'] = {'ansible_playbook': variables,'ansible_playbook_path': variables.get('_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tif cli_args is None:\n\t\tcli_args = args\n\tassert cli_args is not None\n\tansible_args = {\n\t\t'user': ir_workspace.user.get(ir_workspace.name, 'admin')\n\t}\n\n\t# add host specific overrides for host roles\n\t# hosts may be specified in the form of host_name, where:\n\t#   - host_name = <hostname>\n\t#   - [port] = <port>\n\t#   - [path] = <path>\n\t#   - [command line"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t\n\tif cli_args is None:\n\t\tcli_args = []\n\t\n\tprint('Loading playbook from {0}...'\n     .format(ir_workspace.get_name()))\n\ttry:\n\t\tif not os.path.exists(ir_workspace.get_path('play')) and not os.path.exists(ir_workspace.get_path('play')):\n\t\t\tprint('Could not find the playbook!')\n\t\t\tsys.exit(2)\n\t\t\n\texcept InfraGraphException as error:\n\t\tif cli_args is None:\n\t\t\tcli_args = []\n\t\t\n\t\tprint"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n# TODO: Should probably not get this from the master\n\t# TODO: Add an issue here...\n\t# FIXME: This will fail or break if the master is using 'ansible run'\n\t#\n\t# TODO: Use set the run/results plugin\n\n\tcommand_params = []\n\tcli_options, cli_args = _get_cli_and_options(ir_args, cli_args)\n\n\t# print \"Running ansible cli: \" + cli_options\n\t\n\tif not os.path.exists(os.path.dirname(cli_args.name)):\n\t\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\t# Build dict of variables and their values to pass to commands\n\t\tvars_to_vars = {}\n\t\tvars_values_to_pass = []\n\t\t\n\t\tfor key, var in vars_dict.items():\n\t\t\t# Check if the var is a string\n\t\t\tif isinstance(var, str):\n\t\t\t\ttry:\n\t\t\t\t\tv = vars_dict[key]\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tv = dict()\n\t\t\t\t\tv[\"name\"] = var\n\t\t\t\t\tv[\"value\"] = \"failed to find variable in config file\"\n\t\t\t\t\tv[\"detail\"] = str(e)\n\t\t\t\t"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tif not parser_name in self._cli_args_mapping:\n\t\tparser_name_list = ', '.join('\"{}\"'.format(s) for s in parser_name)\n\t\tparser_name_list_list = '[\"{}\"]'.format(args.subcommand if self.options.subcommand else '')\n\t\traise CLIError(\"Unrecognised argument '{0}'\".format(parser_name_list_list_list))\n\t\n\tfor key, value in values_dict.iteritems():\n\t\tif not isinstance(value, collections.Iterable):\n\t\t\tif not (isinstance"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\tvalues_dict = values_dict or {}\n\t\tif isinstance(values_dict[parser_name], dict):\n\t\t\tvalues_dict = dict((key, _convert_argument_value(parser_name, value)) for key, value in values_dict.iteritems())\n\t\t\n\t\tif not isinstance(values_dict, dict):\n\t\t\traise InvalidType(\"value: %s, expecting a dict, received %s\" %(parser_name, type(values_dict)))\n\t\t\n\t\tfor k, v in values_dict.iteritems():\n\t\t\tif isinstance(v, (str, unicode)):\n\t\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\ttry:\n\t\tvalues_dict = values_dict.copy()\n\t\tvalues_dict[CLIP_CMD] = clIP_cmd_mapping[parser_name]\n\t\t\t\n\texcept AttributeError as e:\n\t\tlogger.exception(\"Error in conversion arguments: %s.  Error was\", e)\n\t\tvalues_dict = values_dict.copy()\n \n\tif clIP_cmd_mapping[CLIP_CMD] == \"enable\":\n\t\t\n\t\tif values_dict.get(\"value\") == \"true\":\n\t\t\tvalues_dict[\"value\"] = True\n\t\telif values"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tparser_cmd = values_dict.get('parser_cmd')\n\tif parser_cmd is None:\n\t\traise RuntimeError(\"Invalid command '\" + parser_name + \"'\")\n\tif not parser_obj.is_string(parser_cmd):\n\t\traise RuntimeError(\"Invalid command '\" + parser_name + \"'\")\n\t\n\ttry:\n        if values_dict.get('user_cmd') is not None:\n\t\t\tparser_name = values_dict.get('user_cmd')\n\t\tif parser_name.lower() <= 'true' or (parser_name.lower()"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\ndef _get_command_args(commands, option=None):\n\t\"\"\"\n\tLook up and parse a list of dictionaries passed as input to the command-line\n\twith the following restrictions:\n\t\tIf option is non-empty, they must be an argument to the corresponding command-line.\n\t\tBe warned this will happen even for options which have a valid argument.\n\t\tIf option is specified then it can be treated the same as a command-line argument to\n\t\tparse as specified, e.g. --debug=true --debug=true\n\t\tIt is also guaranteed that all arguments"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n#\tThis is the list. It is currently used in the command\n\t#\tcommand but there's no need for a more explicit type,\n\t#\tit is not used here so we can use this for other code,\n\t#\tor a very simple script. It's used in the help output of\n\t#\tit.\n\tparser_info_list = []\n\tfor name, val in values_dict.items():\n#\t\tparser_info_list.append('%s: %s' % (parser_name, name))\n#\t\t# parser_info"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\t_all_options = {\"virsh_bin\"\t\t: \"bash\",\n\t\t\t\t \"ospd_bin\"\t\t: \"p4_sanitizer\",\n\t\t\t\t \"virsh_program\"\t\t: \"qemu-script\",\n\t\t\t\t \"virsh_user\"\t\t: \"qemu\"\n\t\t\t\t }\n\t\n\t_commands = _all_options.get(parser_name, set())\n\t\n\tfor command in self._commands:\n\t\tvalues_dict.setdefault(command, []).extend(getattr(self, command))\n\t\n\tfor option in _all_options:\n\t\tif ("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tfor i in range(len(values_dict)):\n\t\tcommand = values_dict.get(i, 'default').lower()\n\t\tif str(command) == 'true':\n\t\t\tvalues_dict[i] = \"true\"\n\t\t\tcommand = None\n\t\tif str(command) in ('false', '3', 'true'):\n\t\t\tvalues_dict[i] = \"false\"\n\t\t\tcommand = None\n\t\telif command is None:\n\t\t\tvalues_dict[i] = None\n\t\t\tcommand = command\n\t\telif isinstance(values_dict[i], str):\n\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\n\tlog.warning('Got a non-cli argument passed...')\n\tfor kwarg, value in values_dict.iteritems():\n\t\tsetattr(self, parser_name + '_' + kwarg, value)\n\t\n\tvalues = []\n\tfor key, value in values_dict.iteritems():\n\t\t# convert boolean to 1 for True\n\t\tif value in [1, \"true\"]:\n\t\t\tvalues.append(1)\n\t\t# convert int to 1 for False\n\t\telif value in [0, \"false\"]:\n\t\t\tvalues.append(0)\n\t\telse:\n\t\t\traise ValueError(\"'%s'"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n#\tcheck_arg(values_dict)\n        \n        value_set = values_dict[\"__value_set\"]\n\n        new_values_dict = { }\n        for name, value in values_dict.items():\n            \n            # Check that value actually exists\n            # FIXME:\n            # print \"Checking {}={} {}\".format(name, type(values_dict[\"value\"]), value, types_dict.keys())    \n            \n            \n            \n            # Check if value is a string\n            # TODO: Do we need to detect str type?\n            \n            if str(value) in type(values_dict[\"value\"]):\n                \n                # Check type is acceptable"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\ndef plugin_get_plugin_flat_dict(plugin_dir):\n\t\"\"\"\n\tReturns a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t :return: A flat dict corresponding to a plugin_dir \n\t\"\"\"\n\t\ndef plugin_has_plugin_dir_in_same_directory(plugin_dir, check_only=False):\n\t\"\"\"\n\tDoes a file in which a plugin directory is a top level directory?\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: True"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\tdict = {}\n\tplugin_dir = os.path.realpath(plugin_dir)\n\tpath = str(plugin_lib.index_for_spec(plugin_dir, [\"path\"]))\n\tif path in _PLUGIN_NAMESPACES:\n\t\tdict['plugin_map'] = _PLUGIN_NAMESPACES[path]\n\n\tfor p in _PLUGINS[path].__dict__:\n\t\tif type(p) == dict:\n\t\t\tdict[p] = get_plugin_spec_flatten_dict(p)\n\t\telse:\n\t\t\tdict[p] = {}"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\n\t# get the plugin spec\n\tplugin_source = os.path.join(plugin_dir, \"plugins\", \"plugins_source\")\n\tplugin_root, __, __ = os.path.split(__file__)\n\n\t# walk the plugin root\n\tfor root, dirnames, filenames in os.walk(plugin_root):\n\n\t\t# get the filename\n\tplugin_source_basename = os.path.basename(root)\t# extract from the plugin's name\n\t\tif plugin_source_basename.lower().endswith(\".js\"):\n\t\t\tplugin_source = \"plugins/\"+base"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n#\t\t# TODO\tGet all of the plugin's plugins\n#\t\tplugin_path =\"../plugin_configs/plugins/\"+os.path.basename(plugin_dir)+\"/\"\n#\t\tplugin_spec_flat_dict = get_plugin_spec(plugin_dir)\n\t\ndef make_plugin_paths_flat(plugin_path):\n\t\"\"\"\n\tMakes plugin path flat\n\t:param plugin_dict: A flat dictionary containing the plugin's entries\n\t\"\"\"\n\n\t\n#\t\tPlugin Path\n#\t\tif os.path.isdir(plugin_path):\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\timport json\n\n\tret, data = '', ''\n\tfor item in os.listdir(plugin_dir):\n\t\tif item.endswith('.json'):\n\t\t\ttry:\n\t\t\t\tret = json.loads(open(os.path.join(plugin_dir, item)).read())\n\t\t\t\tdata = json.loads(ret)\n\t\t\t\tbreak\n\t\t\texcept:\n\t\t\t\tpass\n\t\telse:\n\t\t\tret = plugin_dir + '/' + item\n\n\treturn ret, data\n\n\n\ndef _get_base_plugin_name(plugin_name, plugin_dir):\n\treturn plugin_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\n\tplugin_path = os.path.join(plugin_dir, plugin_filename)\n\tif not os.path.isfile(plugin_path):\n\t\traise Exception(\"Plugin \" + plugin_path + \" does not exist\")\n\twith open(plugin_path) as file:\n\t\tfor line in file.readlines():\n\t\t\tparts = line.strip().split(\":\")\n\t\t\tif parts[0].find(\"plugin\") == 0:\n\t\t\t\tplugin_spec = get_plugin_spec(parts[1].split(\":\")[1:])\n\t\t\t\tif not \"type\" in plugin_spec"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\td = {}\n\t\n\tname = os.path.basename(plugin_dir)\n\t# remove leading slash\n\tsplit_dir = name.split('//')\n\tplugin_dir = '/'.join(split_dir[0:plugin_dir.rfind(\"\\\\\") + 1:])\n\t\n\t# add namespace if that's the root\n\tpath = os.path.join(os.path.realpath(plugin_dir), '..', 'templates', 'plugins')\n\t\n\t# use plugin spec to get full plugin\n\treturn {\n\t\t'name' : name.lower(),\n\t\t'description':"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tyaml_dict = plugin_dir+\"/plugins/\"+plugin_name+\"/plugin_configuration.yaml\"\n\tif os.path.isfile(yaml_dict):\n\t  # This is a file\n\t  cfg = yaml.load(open(yaml_dict))\n\t  return cfg\n\t\n\treturn dict({'file': '', 'plugins': [], 'plugin_path': plugin_dir+\"/plugins/default\"})\n\t\n\ndef create_plugin_configurations(json_dict):\n\t\"\"\"\n\tCreates plugin configurations from JSON\n\t\"\"\"\n\t\n\tdict_copy = json_dict.copy()\n\t\n\tfor d in dict_copy"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tmodule_spec = get_plugin_spec(plugin_dir)\n\t\n\tplugin_name = plugin_dir.split(os.sep)[-1] + '.json'\n\tfor module in plugin_name:\n\t\tmodule_spec[module].update(**get_plugin_spec_flatten_dict(os.path.join(plugin_dir,module)))\n\t\n\ttaskdef = get_taskdef_from_module(module_spec,task_name=task_name)\n\t\n\t\n\ttry:\n\t\ttaskdef.update(**get_plugin_spec_flatten_dict(module"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t\n\tif plugin_dir.endswith(\".\"): plugin_dir = plugin_dir[:-1]\n\t\n\tflat = {}\n\tflatten_dict = {}\n\tplugins_dir = plugin_dir\n\tfor plugin in os.listdir(plugins_dir):\n\t\tif not plugin.endswith(\".py\"): continue\n\t\tif plugin.endswith(\".so\"):\n\t\t\tif plugin_dir.endswith(plugin):\n\t\t\t\tf = open(os.path.join(plugins_dir, plugin), 'rt')\n\n\t\t\t\ttry:\n\t\t\t\t\tinfo = f.readline().split(',')\n\t\t\t\t\tif len(info) >"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tprint(\"Would you like to configure the Config Path with the environment: \", self.env_vars)\n\t\tif self.env_dict and self._initenv == \"default\":\n\t\t\ttry:\n\t\t\t\tdel(os.environ[self.env_vars])\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\ndef set_config(self):\n\tprint(\"setting config to {}\".format(self.env_vars))\n\tif not self.inienv:\n\t\tself.inienv = os.environ.get(\"WALKEDIASECLUSTERENV\", \"WALK"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tself._environ[\"AFFEMPLATE\"] = os.environ.get('AFFEMPLATE') or \"\"\n\t\n\tdef env_prefix(self):\n\t\t\"\"\"\n\t\tGet the prefix of the config environment with no embedded space.\n\t\t\"\"\"\n\t\treturn os.path.abspath(os.path.expanduser(DEFAULT_CONFIG_DIR) + os.sep)\n\t\n\tdef env_var(self):\n\t\t\"\"\"\n\t\tGet the environment variable in the config environment with no embedded space.\n\t\t\"\"\"\n\t\treturn os.environ.get(\"AFFEMPLATE\") or ''\n\t\n\tdef"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tconfig = self.environ.copy()\n\t\tif 'DJANGO_SETTINGS_MODULE' in config:\n\t\t\tdel config['DJANGO_SETTINGS_MODULE']\n\t\tself.environ = config\n\nsetup(\n  name='tumblr',\n  version='0.1.2',\n  description='Tumblr App',\n  long_description=long_description,\n  author='Tumblr',\n  author_email='trumbl@gmail.com',\n  url='https://github.com/trumblr/tumblr',\n  packages=['tumblr'],\n "}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tself.environ.update(self.config)\n\n\tdef exec_command(self, command, timeout = 30):\n\t\t\"\"\"\n\t\tExecute the specified command and return the results, defaulting to the first found.\n\t\t\"\"\"\n\t\treturn self.exec_command_line(command, timeout)\n\n\tdef exec_command_line(self, command, timeout = 30):\n\t\t\"\"\"\n\t\tExecute the specified command and return the results.\n\t\t\"\"\"\n\t\t# Try to be a bit of a sophisticated way to get the command line.\n\t\ttry:\n\t\t\treturn exec"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tenv_path = os.environ.get(\"XDG_CONFIG_HOME\", \"/usr/share/config\")\n\t\tif env_path == \"/\" or os.path.exists(env_path):\n\t\t\tos.environ[\"XDG_CONFIG_HOME\"] = env_path\n\n\tdef _write_env_setting(self, x):\n\t\tos.environ[\"XDG_CONFIG_HOME\"] = self.default_user_dir\n\t\tos.environ[\"XDG_CONFIG_HOME\"] = self.default_conf_dir\n\t\tos.environ[\"XDG_CONFIG_"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tself.override_configfile()\n\t\tif self.get('autoconfdir'):\n\t\t\tself.write('/etc/cobbler/autoconf')\n\t\t\timport os\n\t\t\tif self.get('autoconfdir') in os.listdir(\"/etc\"):\n\t\t\t\tself.write('root=\"/etc/cobbler/autoconf\" enable')\n\n\tdef do_write(self, s):\n\t\tif not self.isset():\n\t\t\t# we are a hack, just not setting the file name to None\n\t\t\tself.write(\"[!] Nothing to set!\")\n\t\t\treturn\n\t\tif \"co"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif envvar is not None:\n\t\t\treturn\n\n\t\t# this will not ever be possible but it seems to work without the config file\n\t\tself.env.INJECT_CONFIG = 0\n\t\tif env.INJECT_CONFIG:\n\t\t\treturn\n\t\t\n\t\t# get environment values from gstreamer, if set\n\t\tgst.require_version('Gst', '1.0')\n\t\tif not gst.get_global_signal_list():\n\t\t\tgst.require_version('Gst', '2.0')\n\t\t\tgst.require_version('Gst', '3"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\tif self.config_path is None:\n\t\t\tself.config_path = os.environ\n\tdef check_config_path(self):\n\t\treturn os.path.exists(self.config_path) if self.config_path else None\ndef read_config_data(config_path = None):\n\tif config_path is not None:\n\t\tself.config_path = os.path.expanduser(self.config_path)\n\t\twith open(config_path, 'r') as f:\n\t\t\tself.config = ConfigObj(f.read())\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\ndef get_config_value(config, path):\n\ttry:\n\t\treturn config.get(path, \"\")\n\texcept:\n\t\tpass\n\t\n\t\t\t\nclass ConfigFile(object):\n\t# This class is intended to be inherited from L{ConfigStorage}. \n\t# It can not be used directly. But it's used by L{write_config()}.\n\tdef __init__(self, path):\n\t\tself._path = path\n\t\t\n\t# The config file stores a global value each time a path is modified.\n\t# In the context of the instance, these value of the"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t\ndef is_file_present(path):\n\t\"\"\"\n\tCheck if the path at the user's current dir is either a file or binary file.\n\t\"\"\"\n\tif not os.path.isfile(path):\n\t\treturn False\n\t\n\tif os.path.isdir(path):\n\t\treturn True\n\t\t\n\treturn False\n\t\nif __name__ == '__main__':\n\t# set default variables\n\tconfig['site_var'] = 'PQeML.Default.QQLabitQueryWebServer.js'\n\tconfig['site_section'] = 'PQeML"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tparser = BaseArgumentParser(description=\"Extends the project's CLI and other options such as --config, --config-file etc as command-line arguments. You can use add_cli_options to provide command line options.\")\n\t\n#\t\tparser.add_argument(\"-v\", \"--verbose\",\n#\t\t\t\taction=\"store_true\", default=False,\n#\t\t\t\thelp=\"verbose mode (do not run with verbose mode)\")\n    #!/usr/bin/env python\n\nfrom __future__ import division, print_function\nfrom math import pi, exp\n\nfrom numpy import float"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tparser_opt = subparsers.add_parser('extend-cli', help = 'Extends a cli from an argument.')\n\t\tparser_opt.add_argument('-v', '--versions', action ='store_true',\n\t\t\thelp = 'List upversions for all installed versions.', default = False)\n\t\t\n\t\t\n\tdef do_exit(self, exit_code):\n\t\tsuper(CliModule, self).do_exit(exit_code)\n\t\tlogger.info('Exiting cli module.')\n\t\t\n\tdef parse_args(self):\n\t\tsubparser = subparsers.add_parser('version"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t    \n\tparent_parser = root_subparsers.add_parser('install',\n\t\tadd_help = \"Install an existing virtual environment on the build.\")\n  \n\tinstall_parser = install_parser_class()\n  \n\tinstall_parser.set_defaults(func=self.cli)\n  \n\tinstall_parser.add_argument(\n\t\t'-c', '--channel', nargs=2, choices=CHANNEL_CHOICES, \n\t\thelp='channels to install (default %s)' % ','.join(CHANNEL_CHOICES))\n  \n\tinstall_parser._add_argument(\n\t  \t'-s"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tsuper(SpecCli, self).extend_cli(root_subparsers)\n\t\tfor subsub in root_subparsers:\n\t\t\tsubsub.add_argument('--version', action='version', version=SpecVersion.version, \n\t\t\t\t\thelp='Display version information and exit')\n\t\t\t\t\t\ndef handle_params(args):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--debug', action='store_true', help='Enable debug output')\n\tparser.add_argument('--debug-verbose', action='store_true', help='Enable debug verbosity output')\n\t\n\t\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tdef do_get(self):\n\t\tself.sub_dict['action']['set'].append(cli_arg.submenu)\n\t\tprint \"Updating...\"\n\t\tself.sub_dict['menu'] = self.add_submenu(dict(self.sub_dict['action']))\n\t\treturn True\n\t\t\n\nif __name__ == '__main__':\n\tparser = argparse.ArgumentParser(add_help=False, description='Add options')\n\tparser.add_argument('-l', help=\"the list of commands, eg. help add\")\n\tparser.add_argument('-F"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n#\t\tif self._cli_options.has_option(\"-x\", \"--file_name\"):\n#\t\t\tself._cli_options.remove_option(\"-x\")\n#\t\t\tif hasattr(self._cli_options, \"-o\"):\n#\t\t\t\tself._cli_options.remove_option(\"-o\")\n#\t\t\tif hasattr(self._cli_options, \"-p\"):\n#\t\t\t\tself._cli_options.remove_option(\"-p\")\n\t\n\n\t#\t\tif self._args.do_print:\n#\t\t\troot_subparsers.add_"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\t# The main namespace\n\targs_subparser = subparser.add_subparsers(title='Args Commands')\n\targs_subparser.required = True\n\t\n\targs_subparser.add_argument('command', type=str,required=True)\n\targs_subparser.set_defaults(\n\t\tcommand=argparser.do_nothing, \n\t\tmode=argparser.do_nothing, \n\t\tprefix=argparser.do_nothing,\n\t)\n\t\n\targs_subparser.add_parser(cmd_name='append', help='Append a new entry"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\tfor arg in root_subparsers.__bases__:\n\t\targ_parser = arg.__self__\n\t\targ_parser.__init__.argtypes = [str, str, str]\n\t\targ_parser.__init__.return_value = None\n\t\targ_parser.parse_args(self)\n\t\n\tif self._add_name:\t\t\n\t\tfor arg in self._name:\n\t\t\targ_parser = arg.__self__\n\t\t\targ_parser.__init__.argtypes = [str, str, str]\n\t\t\targ_parser.__init__.return_value = None\n\t\t\targ_parser"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\tfor sub in root_subparsers:\n\t\t\tself.parse_cli_options(sub, self.command_line_spec(sub))\n\t\t\t\t\n\tdef parse_cli(self, subparser):\n\t\t\"\"\"\n\t\tParse the command line and determine if any arguments are required for this command.\n\t\t\n\t\tThe arguments are ordered based on their command name. The command name is then set to\n\t\tthe command the subparser parses as.\n\n\t\t:param cli: the subparser object to parse.\n\t\t\"\"\"\n\t\tif not subparser.actions:\n\t\t\tsubparser.actions = list()\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t\n\t# Parse arguments\n\tparser = argparse.ArgumentParser(add_help=False, description='Add options to the root subparser.')\t\n\t\n\tparser.set_defaults(usage='./manage.py config')\n\n\tparser.add_argument('--version', action='version', version='%(prog)s {0}'.format(__version__))\n\tparser.add_argument('--host', action='store', type=str, help='Name of Raspberry Pi IP')\n\tparser.add_argument('--port-file',  action='store', type=str, help='Name of the R"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\tfor pathvar in path:\n\t\t\tpath = os.path.normpath(os.path.normpath(path).replace(root, '/', 1))\n\t\treturn os.path.normpath(path)\n\ndef strip_ext(path, extension):\n\t\"\"\"\n\tRemove extension from path, throw exception on failure.\n\t\"\"\"\n\t\tpart = os.path.normpath(os.path.normpath(path).replace(extension, '/', 1))\n\t\treturn os.path.normpath(part)\n\n\n# A file system-specific version string.\n\nversion = __import__('pyh"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\ttry:\n\t\t\tos.remove(path)\n\t\texcept OSError as e:\n\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\traise\n\t\telif os.path.isdir(path):\n\t\t\traise exception.ObjectCollision('Directory path does not exist: %s' % path)\n\t\telse:\n\t\t\traise exception.ObjectCollision('Can\\'t find directory: %s' % path)\n\n\ndef read_file(path):\n\t\"\"\"\n\t\tRead (open) file, and all comments/comments read in the file.\n\t\t\"\"\"\n\t\twith open(path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\tos.remove(root)\n#!/usr/bin/env python3\n\n\"\"\"\nThis module contains all the basic base classes for all command line stuff\n\"\"\"\n\nimport argparse\nimport sys\n\n\nclass CommandLine(argparse.Action):\n    \"\"\"\n    A context manager which runs this command as a child environment\n\n    Usage: python -m pulp.plugins.dask.db\n\"\"\"\n    def __init__(self, name, description, action, dest, metavar=None,\n                 default=None, type=None, choices=None, required=False,\n                 help=None):\n        \"\"\"\n       "}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\t\n\tif path[0] == '/':\n\t\treturn path[1:]\n\n\tif len(path) == 1:\n\t\treturn path[0]\n\n\treturn path\n\n\ndef write_path_contents(fd, path, contents):\n\treturn fd.write(contents)\n\n\ndef walk(path, followlinks=True):\n\tif path == '.':\n\t\treturn walk_all_parents(path)\n\telse:\n\t\tfor path_sub in walk_all_children(path):\n\t\t\tyield path_sub\n\n\ntree_iter = lambda x: (x, x"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\ttry:\n\t\tstat = os.lstat(path)\n\texcept:\n\t\treturn path\n\t\t\t\n\treturn os.fsencode(stat.st_mode & 07777) - stat.st_ino\n\ndef split_path(start_path, end_path, chars='*?'):\n\t\"\"\"\n\tSplit path on chars. If chars is None, splitting on spaces is done\n\t.\"\"\"\n\treturn \"\".join([\".\"+\"x\" for x in xrange(len(chars))])\n\ndef path_is_empty(path):\n\t\"\"\"\n\tReturn True if path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\treturn remove_root(path)\n\telif not is_path(path):\n\t\treturn None\n\n\tif not is_path(root) and not root == '':\n\t\traise InvalidPathException(\n\t\t\t\"Cannot remove root: '%s'\\n\"\n\t\t\t\"%s!= '%s'\" % (root, path, root)\n\t\t)\n\telse:\n\t\treturn remove_root(path)\n\ndef set_root(path, root=None):\n\t\"\"\"\n\tSet root to path, if root exists and that is valid.\n\t\"\"\"\n\t\tif is_path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\tpath_hash = root[-4:]\n\t\treturn path[8:-1]\n\telse:\n\t\traise\n\n\ndef get_root_from_path(root):\n\t\"\"\"\n\tDevide path string into directory.\n\t\"\"\"\n\n\t\tpath = root[:-4]\n\t\troot = path.dirname()\n\t\treturn path_hash + '/' + root[0:-4]\n\telse:\n\t\traise\n\n\n@task\ndef clean_repo_path(repo):\n\t\"\"\"\n\tRemove trailing '/' from path, throws exception on failure.\n\t\"\"\"\n\t\tpath_"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\tif is_file(path):\n\t\t\tpath = os.path.normpath(path)\n\t\tif not is_executable(path):\n\t\t\tif not check_call([\"rm\", \"-rf\", path]):\n\t\t\t\traise OSError(\"{} doesn't exist\".format(os.path.normpath(path)))\n\n\t\treturn path\n\n\tdef remove_leading_zeros(data):\n\t\t'''\n\t\tRemove leading zeros from dict (used by strip_root)\n\t\t@param data: dict data to remove leading zeros from\n\t\t@type data: list\n        '''\n\t\ttotal = len(data)\n\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\ttry:\n\t\tos.remove(path)\n\texcept Exception as e:\n\t\tif e.output:\n\t\t\traise \n\telse:\n\t\traise KeyError('path not in list')\n\n\ndef is_root(path):\n\t\"\"\"\n\tCheck if path is a valid root folder.\n\t\"\"\"\n\ttry:\n\t\troot = subprocess.check_output([\"find\", \"-type\", \"file\", path, \"/\"], universal_newlines=True).strip().split()[-1]\n\t\treturn \".\" in path\n\texcept:\n\t\treturn False\n\treturn True\n\ndef"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\traise NotImplementedError\n\ndef path_join_with_slash(path, sep):\n\t\"\"\"\n\tJoin path with given root path separator, then apply separator\n\tseparator. If root is a folder directory, return new path\n\tand replace path separator with root path separator\n\t\"\"\"\n\t\t# TODO\n# \tassert isinstance(path,str), \"Path is an abstract file path.\"\n\t\tsep_path_parts = []\n\t\tassert sep.startswith(\"/\")\n\t\tassert sep.endswith(\"/\")\n\n\t\tnew_path = os.path.join(path, sep_path_parts[0"}]}
