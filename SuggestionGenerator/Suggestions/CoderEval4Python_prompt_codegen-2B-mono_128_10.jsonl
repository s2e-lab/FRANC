{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\ttry:\n\t\treturn TimeUTC.utc_from_datetime(parse(str(nanoseconds)), tz)\n\texcept ValueError:\n\t\treturn None\n\n\ndef hydrate_localtime(nanoseconds, tz=None):\n\t\"\"\"\n\tHydrator for `LocalTime`.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\ttry:\n\t\treturn TimeUTC.utc_from_datetime(parse(str(nanoseconds), tzinfo=py"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn _fife.hydrate_time(nanoseconds, tz)\n\ndef hydrate_geojson(geojson, tz=None):\n\t\"\"\"\n\tHydrator for `GeoJson`.\n\t:param tz:\n\t:param geojson:\n\t:return: GeoJson\n\t\"\"\"\n\treturn _fife.hydrate_geojson(geojson, tz)\n\ndef hydrate_color(rgb, tz=None):\n\t\"\"\"\n\tHydrator for `ColorRGBA` and `"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\timport tzlocal\n\tfrom types import BooleanType\n\t\n\tif _types.issubclass(type(tz), tzlocal.tzlocal) or tz is None or isinstance(tz, BooleanType):\n\t\ttz = tzlocal.get_localzone()\n\t\treturn datetime.time(\n\t\t\tyear=2000, month=1, day=1, hour=8, minute=0, second=2, microsecond=0, tzinfo=tz)\n\telif tz is None:\n\t\ttz = _types.UTC\n\telif not"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif not isinstance(nanoseconds, (int, float)):\n\t\traise TypeError(\"`nanoseconds` is not type `int` or `float`\")\n\telif not isinstance(tz, tzinfo) and tz!= None:\n\t\traise TypeError(\"`tz` is not type `NoneType` or `tzinfo`\")\n\t\n\ts_datetime = nanoseconds // 1000000000.0\n\tnanoseconds = nanoseconds % 1000000000.0\n\tt = datetime(1970, 1, 1,"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(timestamp=(nanoseconds / 1000) * 1000, tzinfo=tz)\n\n\ndef hydrate_time_delta(value, tz):\n\t\"\"\"\n\tHydrator for `TimeDelta` values.\n\n\t:param value:\n\t:param tz:\n\t:return: TimeDelta\n\t\"\"\"\n\n\tif not isinstance(value, TimeDelta):\n\t\treturn value\n\telse:\n\t\treturn value.reindex(timestamp=(value.timestamp % 60)).astype(np.float64) / 60\n\n\ndef dehydrate"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\treturn datetime.utcfromtimestamp(timestamp_datetime(seconds=nanoseconds / 1e9).timestamp())\n\telse:\n\t\treturn datetime.fromtimestamp(timestamp_datetime(seconds=nanoseconds / 1e9).timestamp(), tz)\n\n__all__ = (\n\t'tz_offsets_from_string',\n\t'hydrate_time'\n)\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tsecs = nanoseconds / 10000000.0\n\tt = time.fromtimestamp(secs).replace(tzinfo=tz)\n\n\treturn t\n\n\ndef hydrate_float_time(secs, tz=None):\n\tt = time.fromtimestamp(int(secs)).replace(tzinfo=tz)\n\t\n\treturn t\n\n\ndef hydrate_timestamp(nanoseconds, tz=None):\n\treturn hydrate_time(nanoseconds, tz)\n\n\ndef hydrate_timestamp_milli"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(nanoseconds, tz)\n\ndef hydrate_interval(start, end, weeks):\n\t\"\"\"\n\tHydrates an interval.\n\t\n\t:param start:\n\t:param end:\n\t:param weeks:\n\t:return: Interval\n\t\"\"\"\n\treturn Interval(start, end, weeks)\n\n\n\nclass Envelope(object):\n\t\"\"\"\n\tAn envelope for sending data over the WebSocket protocol.\n\n\t:param subject: String subject for the message; defaults to the empty string.\n\t:param body:"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(nanoseconds, tz=tz)\n\nclass Json(object):\n\t\"\"\"`Json` data structure.\"\"\"\n\tdef __init__(self, d):\n\t\tself.original = d\n\n\tdef __getattr__(self, name):\n\t\tif name in self.__slots__:\n\t\t\t# return value if name exists\n\t\t\treturn getattr(self, name)\n\t\treturn self[name]\n\n\tdef __getitem__(self, item):\n\t\t\"\"\"Allow indexing\"\"\"\n\t\tif isinstance(item, basest"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, Time):\n\t\t_dummy = datetime.utcfromtimestamp(nanoseconds // 1e9)\n\t\treturn _dummy.replace(tzinfo=tz)\n\telif isinstance(nanoseconds, tuple):\n\t\tfrom. import date\n\t\treturn date.from_timestamp(nanoseconds[0], tz)\n\telse:\n\t\treturn None\n\n\ndef hydrate_date(date_string, tz=None):\n\t\"\"\"\n\tHydrates `date_string` into a"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn datetime.timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n\t\n\t\t\n@register.filter\ndef get_field_type(model, field_name):\n\t\"\"\"\n\tThis is so that admin will be able to use this same code to extract \n\tfield type and to extract any field which is required to be a valid choice.\n\t\"\"\"\n\treturn model._meta.get_field(field_name).get_internal_type()\n\n\n@register.simple_tag\ndef get_list_"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn {\"days\": value.days, \"seconds\": value.seconds}\n\n\n__all__ = (\n\t\"json_encode\",\n\t\"json_decode\",\n\t\"format_iso8601_datetime\",\n\t\"dehydrate_datetime\",\n\t\"dehydrate_date\",\n\t\"dehydrate_time\",\n\t\"dehydrate_datetime_ticks\",\n\t\"dehydrate_iso_8601_datetime\",\n\t\"dehydrate_timedelta\",\n)\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn ('{} days, {} hours, {} minutes and {} seconds').format(\n\t\tvalue.days, value.seconds // 3600, (value.seconds // 60) % 60, value.seconds % 60)\n\n\n__all__ = ['enhydrate_datetime', 'dehydrate_datetime', 'enhydrate_date', 'dehydrate_date', 'hydration_method']\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.total_seconds()\n\n\n\"\"\"\nDehydrators\n\"\"\"\n_DEHYDRATORS = {\n\tdatetime.now: dehydrate_date,\n\tdatetime.utcnow: dehydrate_date,\n\tdate: dehydrate_date,\n\ttime: dehydrate_time,\n\tdatetime.timedelta: dehydrate_timedelta,\n}\n\n\ndef dehydrate(value):\n\t\"\"\"\n\tDehydrator decorator.\n\t\n\tExample:\n\t\tdehydrate = fields.Dehydrator(...)\n\t\t@"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.days * 86400 + value.seconds * 100 + value.microseconds // 100\n\ndef hydrate_timedeltahours(value):\n\"\"\"\n\tHydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: float\n\t:return:\n\t\"\"\"\n\tvalue = value * 1000000  # microseconds\n\treturn datetime.timedelta(days=value // 86400, seconds=value // 100, microseconds=value - value // 100)\n\ndef dehydrate_date(value):\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn ':'.join([\n\t\tstr(int(value.total_seconds()/60/60*4)),\n\t\tstr(int(value.total_seconds()/60/24*4)),\n\t\tstr(int(value.total_seconds()/60*4)),\n\t\tstr(int(value.total_seconds())),\n\t\tstr(int(value.total_seconds()*1000)),\n\t])\n\n@decorators.public_method\ndef dehydrate_bool(value):\n\t\"\"\"\n\tDehydrator for `bool` values.\n\t\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn timedelta2str(value)\n\n\ndef dehydrate_boolean(value):\n\"\"\"Dehydrator for `boolean` values\n\n:param value:\n:type value: boolean\n:return:\n\"\"\"\n\treturn value\n\n\nclass Stringer(dict):\n\tdef __init__(self, **kwargs):\n\t\tsuper(Stringer, self).__init__(**kwargs)\n\t\n\tdef __getattr__(self, key):\n\t\tv = self[key]\n\t\tif v == \"<empty>\":\n\t\t\treturn \"\"\n\t\tif"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif value == timedelta():\n\t\treturn None\n\treturn '%s min' % min(round(value.total_seconds() / 60), 6)\n\"\"\"\n    class MimeTypes:\n\"\"\"\n\"\"\"\n\t@classmethod\n\tdef mime_types(self):\n\"\"\"\n\"\"\"\n\n\"\"\"\n\tdef mime_suffix(self, ext):\n\"\"\"\n\"\"\"\n\n\"\"\"\n\tclass UploadSuffixes:\n}\n\ndef upload_url(pk, endpoint, **kwargs):\n\"\"\"\n\"\"\"\n\tpath = reverse('upfiles"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn \"\" if value in (None, datetime.timedelta(0), datetime.timedelta()) else str(value)\n\n\ndef dehydrate_url(value):\n\t\"\"\"\n\tDehydrator for `url` values. This will call urlparse.urljoin if urlparse doesn't work for you,\n\tor just return the value if you are happy with the result, but plan for it to fail anyways.\n\n\t:param value:\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tfrom urllib.parse import urlparse\n\t\twith contextlib."}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif value:\n\t\treturn \"%09d\" % ((value.microseconds + (value.seconds + value.days * 24 * 3600)\n\t\t                 * 10 ** 6) // 10 ** 6)\n\telse:\n\t\treturn None  # empty value to pass validation\n\n\ndef dict_to_object(value):\n\"\"\"\n\tConvert a dictionary to an object.\n\t\n\t:param value:\n\t:type value: dict\n\t:return:\n\t\"\"\"\n\treturn {\"uuid\": value[\"uuid\"],\n\t        \"code\": value[\"code\"],\n"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tdt = value.isoformat()\n\treturn dt.split('+')[0] if '+' in dt else dt\n\n\t\nclass Serialize:\n\t\"\"\"\n\tA base Serializer used to provide common functions all serializers should\n\tinherit from.\n\n\t:param value: The value to serialize\n\t:type value: :class:`any`, :class:`dict`, :class:`list`\n\t:param is_formatted: Whether or not the value is already formatted such as\n\t  those added to the session. Used if serial"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.strftime(\"%H:%M:%S\")\n\ndef hydrate_date(value):\n\t\"\"\"\n\tHydrates a string into a Python `datetime` value.\n\t\n\t:param value:\n\t:type value: str\n\t:return:\n\t\"\"\"\n\treturn datetime.strptime(datetimeFormat(value), datetimeFormat())\n\ndef hydrate_datetime(value):\n\t\"\"\"\n\tHydrates a string into a Python `datetime` value.\n\t\n\t:param value:\n\t:type value"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn time.mktime(value.timetuple())\n\n\nclass TimeType(IntType):\n\t\"\"\"\n\tEnumeration for `time` values.\n\t\n\t:param time_format: default time format: `%Y-%m-%dT%H:%M:%SZ`\n\t:param default_time: time object representing default value (default: `time.time()`)\n\t\"\"\"\n\t\n\t_name = 'time'\n\t\n\tdef __init__(self, *"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"name\": value.name,\n\t\t\"time\": value.time,\n\t\t\"ticks\": value.ticks,\n\t\t\"delta\": value.delta,\n\t\t\"hour\": 0,\n\t\t\"min\": 0,\n\t\t\"second\": 0,\n\t\t\"millisecond\": 0,\n\t\t\"day\": 0\n\t}\n\n\ndef dehydrate_duration(value):\n\t\"\"\"\n\tDehydrator for `duration` values.\n\t\n\t:param value:\n\t:type value: Duration\n\t:return:\n\t\"\"\""}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.__str__()\ndehydrate_time = s(dehydrate_time)\n\nclass Datetime(Mixed):\"\"\"\n\tSubclass of :py:class:`py4web.orms.py4web.field.Mixed` representing a Datetime.\n\t\n\t:param date_format:\n\t:type date_format: str\n\t:param date_format:\n\t:keyword default_timezone:\n\t:keyword timezone_field:\n\t:keyword default_timezone:\n\t\"\"\"\n\"\"\"\n\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t'type': value.__class__.__name__,\n\t\t'value': value.isoformat()\n\t}\n\n\n# Decorators\ndef time_filter(column, table, view):\n\t\"\"\"\n\tA filter that accepts time values with datetime.time objects.\n\t\n\t:param column: The column being applied as part of the filter - the type of `column` should accept a **time** parameter.\n\t:type column: str\n\t:param table: The table to which the column points (in case `column` is a foreign key"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value is not None:\n\t\treturn value.strftime(\"%H:%M\")\n\telse:\n\t\treturn u''\n\t\n\t\ndef _prepare_filters(filters, orderby):\n\tfilters, orderby = filters and list(filters) or [], orderby and \\\n\t\t\t\t\t\t(u\",\".join(orderby) or u'')\n\treturn u\",\".join(\n\t\t[(u\"{}__lte\".format(col), lte(getattr(field_name, \"name\", ''), val))\n\t\t\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn '' if value is None else value\nhydrators[TimeType] = hydrator_time\n\n#---------------------------------------------------------\nclass hydrator_timedelta(object):\n\t\"\"\"\n\tDehydrator for `pyramid_tweaker.types.timedelta`.\n\n\t:param value:\n\t:type value: TimeDelta\n\t:return:\n\t\"\"\"\n\tdef __init__(self, value):\n\t\tif not isinstance(value, TimeDelta):\n\t\t\traise TypeError(\"Cannot Hydrate {} of type %s. Must be a TimeDelta object.\" %"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.strftime(\"%H:%M:%S\")\nfrom functools import wraps\n\nimport logging\n\nfrom peewee import SqliteDatabase\n\nfrom data_models.data_model import db\n\nfrom data_models.data_model import User\nfrom data_models.data_model import Course\nfrom data_models.data_model import Task\nfrom data_models.data_model import Status\n\n\nclass UserDatabase(SqliteDatabase):\n\n\tCREATE_USER = \"\"\"create table if not exists `user` (\n\t\t\t\t\t`user_id"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value is not None:\n\t\treturn value.isoformat()\n\telse:\n\t\treturn None\n\nField.dehydrated_type = dehydrate_time\n\n\nclass DateTimeField(fields.DateTimeField):\n\t\"\"\"\n\tSubfield for `datetime` values.\n\t\n\t:param default_tzinfo:\n\t:param converter_module:\n\t:param converter_name:\n\t:type default_tzinfo: datetime.tzinfo\n\t:type converter_module: str\n\t:type converter_module: str\n\t:type converter_"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn dict(\n\t\ttype = u'Polygon',\n\t\tcoordinates = _encode_polygons(\n\t\t\tvalue.coords, value.srid))\n\ndef dehydrate_user(user):\n\treturn dict(\n\t\tis_authenticated = user.is_authenticated,\n\t\temail = user.email,\n\t\tusername = user.username,\n\t\tprofile_url = user.get_absolute_url())\n\ndef dehydrate_user_create(user_create):\n\treturn dict(\n\t\temail = user_create.email,\n\t\tusername"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"x\" : value.x,\n\t\t\"y\" : value.y\n\t}\n\ndef dehydrate_point_with_z(value):\n\t\"\"\"\n\tDehydrator for Point with z.\n\t\n\t:param value:\n\t:type value: PointWithZ\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"x\" : value.x,\n\t\t\"y\" : value.y,\n\t\t\"z\" : value.z\n\t}\n\ndef dehydrate_enum(value):\n\t\"\"\"\n\tDe"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"latitude\": float(value.coords[\"lat\"]),\n\t\t\"longitude\": float(value.coords[\"lon\"]),\n\t\t\"radius\": None\n\t}\n\n\ndef dehydrate_way(value):\n\"\"\"\n\tDehydrator for Way data.\n\t\n\t:param value:\n\t:type value: Way\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"node_ids\": list(value.nodes),\n\t\t\"radius\": float(value.tags[M_WMS_WAYRADIUS][VALUE"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, tuple):\n\t\treturn None\n\treturn \"SRID=3857;POINT(%s %s)\" % (value[0], value[1])\n\nclass PointType(GisDataType):\n\t\"\"\"\n\tThis class holds a point value.\n\t\"\"\"\n\tDEFAULT_WIDTH = 8\n\t_ATTRIBUTES = {\n\t\t'width': {'pget': GIS_WIDTH, 'pset': GIS_WIDTH, 'py_type': int, 'dehydrator': dehyd"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn json.dumps(value)\n\n\n@jsonld_functions.register_function(point='topojson.points',\n\t\tfunction_types=[SPATIAL_DATATYPES],\n\t\tversion='function.Topology.point',\n\t\tdocumentation='Compute the extent of a Point')\ndef extract_from_point(value): \"\"\"\n\tExtracts a Geospatial Object.\n\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value\n\n\n@jsonld_functions.register_function"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn [value.X, value.Y]\n\n\n# ----------------------------------------------------------------------\ndef dehydrate_line(value):\n\t\"\"\"\n\tDehydrator for Line-based object\n\t\n\t:param value:\n\t:type value: LineString\n\t:return:\n\t\"\"\"\n\treturn [dhydrate_point(value.start), dhydrate_point(value.end)]\n\n\ndef dehydrate_pointm(value):\n\t\"\"\"\n\tDehydrator for Point-based object\n\t\n\t:param value:\n\t:type value: Point\n\t:return"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn 'POINT(%s %s)' % (value.x, value.y)\n\t\n\t\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn \"%s,%s\" % (value.x, value.y)\n\n\"\"\"\n\tDehydration of any GeoJSON-like dictionary that may be passed in via a\n\t``feature``. ``value`` must be a dict or GeoJSON-like structure (which\n\tcurrently means it must conform to the JSON Geomeral JSON schema).\n\t\n\t:param value:\n\t:type value: Dict[str, Any]\n\t:return:\n\t:rtype: str\n\t\"\"\"\n\tif isinstance(value, dict) and \"type\" in value"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif hasattr(value, \"bounds\"):\n\t\treturn {\"value\": (value.x, value.y)}\n\telse:\n\t\treturn {}\n\n\n@parser.django_to_json.register(LineStringFieldSQLExt)\ndef dehydrate_line(value):\n\"\"\"\n\tDehydrator for LineString data.\n\t\n\t:param value:\n\t:type value: LineString\n\t:return:\n\t\"\"\"\n\tif hasattr(value, \"bounds\"):\n\t\treturn {\"value\": (value.point_list[0"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"lon\": val(value.Longitude),\n\t\t\"lat\": val(value.Latitude)\n\t}\n\n\ndef dehydrate_line(value):\n\t\"\"\"\n\tDehydrator for Line data.\n\t\n\t:param value:\n\t:type value: Line\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"paths\": None,\n\t\t\"sourceId\": val(value.SourceID),\n\t\t\"sourceName\": val(value.SourceName),\n\t\t\"referenceSystem\":\n\t\t\tval(value.ReferenceSystem),\n"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tpass\n\treturn [] # todo: implement\n\ndef length(self):\n\"\"\"\n\tReturn the length of the record. \n\tUsed by :meth:`Writer.dump` and :meth:`Reader.load`.\n\t\n\t:return: the record length\n\t\"\"\"\n\tpass\n\treturn 0 # todo: implement\n\ndef update(self, new_values):\n\"\"\"\nAdd/update new record values.\n\n:param new_values: new record values\n\n:type new_values: list\n\"\"\"\n\ndef update_record(self"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tpass\n\treturn []\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\ttry:\n\t\treturn self.dbc[ZK_ROOT].keys()\n\texcept:\n\t\tlog.error(\"Error accessing node\")\n\t\treturn None\n\n\ndef key_to_id(key):\n\t\"\"\"\n\tReturn the id corresponding to a key in the ZK.\n\t\n\t:param key: the key name\n\t:return: the id of the node\n\t\"\"\"\n\ttry:\n\t\tif (key!= None):\n\t\t\treturn int(key)\n\t\treturn None\n\texcept:\n\t\tlog.error(\"Error converting key to id\")\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tpass\n\ndef new(key, ttl=0):\n\t\"\"\"\n\tCreate a new record.\n\t\n\t:param key: (``string``) domain name of the record\n\t:param ttl: (``integer``) time-to-live for the record: 0 if record does not expire\n\t:returns: (``dns.rrset.RRset``) new record\n\t\"\"\"\n\tpass\n\ndef packet(id=None):\n\t\"\"\"\n\tReturn a packet that can be encoded in DNS format.\n\t\n\t:param id: the"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._dict.keys()\n\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__keys\n\n\t\nclass Record:\n\t\"\"\"\n\tA class used to represent records that has at least: the record\n\tidentifier, and a dictionary of keys and values.\n\t\n\t:member values: a dictionary\n\t:attribute keys: a list of keys\n\t:method save: used to save values for the current record\n\t:method get: used to get value for a particular key\n\t\"\"\"\n\n\tdef __init__(self, record_identifier):\n\t\t\"\"\"\n\t\tInitializes a record using the provided record_id, and an empty\n\t\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tpass\n\t\ndef length(self):\n\"\"\"\n\tReturn the length (number of records) of the record.\n\t\n\t:return: number of records\n\t\"\"\"\n\tpass\n\t\ndef namespace_url(self):\n\"\"\"\n\tReturn the namespace URL of the record.\n\t\n\t:return: URL of this record resource\n\t\"\"\"\n\tpass\n\t\ndef resource_type(self):\n\"\"\"\n\tReturn the type (class name) of the record.\n\t\n\t:return: type of this resource\n\t\"\"\"\n\tpass\n\t\ndef ur"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t# # print('keys is {}'.format(os.path.basename(self._fpath)))\n\treturn list(self._tbl.keys())\n\n\n@ensure('record')\ndef values(self):\n\"\"\"\n\tReturn the values of the record.\n\t\n\t:return: list of value entries\n\t\"\"\"   \n\t# # print('values is {}'.format(os.path.basename(self._fpath)))\n\treturn list(self._tbl.values())\n\n\n@ensure('record')\ndef iteritems(self):\n\"\"\"\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tpass\n\ndef key(self, strg):\n\"\"\"\nGet key value.\n\n:param strg: key name\n\n:return:\n\tValue of key.\n\tNone if key not exists.\n\t\"\"\"\n\tpass\n\ndef key_exists(self, strg):\n\"\"\"\nTell if key exists.\n\n:param strg: key name\n\n:return: boolean\n\tTrue if key exists\n\tFalse if key not exists\n\t\"\"\"\n\tpass\n\ndef last_key(self):\n\"\"\"\nGet last_key of a record.\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._db[\"collections\"].keys()\n\"\"\"\nGet the names of all keys in the record.\n\n:return: List of key names\n\t\"\"\"\nkeys = get_keys\n\n# Create\ndef create(id, txt, col=None):\n\t\"\"\"\n\tCreate a key in the record.\n\t\n\t:param id: String identifier\n\t:param txt: The value of the new key\n\t:param col: Optional, the name of the collection\n\t:return: None\n\t\"\"\"\n\tif id is None:\n\t\tid = \""}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError('protocol_version must be a tuple')\n\tif protocol_version is None:\n\t\tprotocol_versions = cls.supported_protocol_versions()\n\telse:\n\t\tprotocol_versions = { protocol_version } if protocol_version in cls.known_versions( ) else None\n\t\tif protocol_versions is None:\n\t\t\traise BoltProtocolError('Unsupported protocol version: %r' % protocol_version)\n\tif protocol_versions:"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\td = {}\n\tif protocol_version:\n\t\t# This will throw a TypeError if protocols_version is not\n\t\t# a tuple\n\t\td = {protocol_version: _find_protocol_handler_class(protocol_version)}\n\telse:\n\t\tfor protocol_version in cls.valid_versions():\n\t\t\ttry:\n\t\t\t\td[protocol_version] = _find_protocol_handler_class(protocol_version)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\treturn d\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif type(protocol_version)!= tuple:\n\t\traise TypeError(\"protocol_version must be a tuple\")\n\n\tif not protocol_version:\n\t\treturn dict(cls._handlers.items())\n\n\tsupported = dict()\n\tfor k, v in cls._handlers.items():\n\t\tif protocol_version > k:\n\t\t\tsupported[k] = v\n\t\telif protocol_version < k:\n\t\t\tbreak\n\t\telse:\n\t\t\tsupported[k] = v\n\treturn supported\n\n\ndef negotiate_protocol_version(client_description):"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tsupported = [3, 4]\n\t\n\tif protocol_version is None:\n\t\treturn {\n\t\t\tprotocol_version: get_handler_class('HEAD_FIRST')\n\t\t\tfor protocol_version in supported\n\t\t}\n\telif _is_proto_version(protocol_version, supported):\n\t\treturn get_handler_class('HEAD_FIRST'),\n\telse:\n\t\traise TypeError(\"Expected str, got %r\" % protocol_version)\n\n\ndef _is_proto_version(protocol_version, supported):\n\tif"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls.server_info.protocol_handlers(protocol_version)\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = cls._handlers\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError('protocol_version must be tuple')\n\t\thandlers = handlers.get(protocol_version[:2])\n\t\tif handlers is None:\n\t\t\traise ValueError(\n\t\t\t\t'unknown protocol protocol_version: {}'\n\t\t\t\t.format(protocol_version)\n\t\t\t)\n\t\tif len(protocol_version) > 2:\n\t\t\thandlers = handlers.get(protocol_version[2"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls._protocol_handlers\n\tif protocol_version not in cls._protocol_handlers:\n\t\traise TypeError(\n\t\t\t\"protocol must be a tuple of the form (major, minor): \" +\n\t\t\trepr(protocol_version)\n\t\t)\n\tsupported_handlers = {\n\t\tprotocol_version: cls._protocol_handlers[protocol_version],\n\t}\n\tif len(cls._protocol_handlers) > 1:\n\t\tsupported_handlers"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tretval = dict()\n\t\tfor version, version_class in cls._protocol_versions.items():\n\t\t\tif version_class.supports_standalone_messages():\n\t\t\t\t# if it's standalone, there's at least one here\n\t\t\t\tretval[version] = version_class\n\t\t\telif version_class.supports_changelog:\n\t\t\t\t# if it's not standalone, but it supports the \"changelog\" feature,\n\t\t\t\t# which one can just check by looking at the supported_api_versions class attribute"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = cls._protocol_handlers\n\treturn handlers.get(protocol_version, {}) if protocol_version else handlers\n\n\ndef register_handler(handler, protocol_version=None):\n\t\"\"\"\n\tRegister a new handler for a specific or all protocol versions.\n\tA TypeError exception will be raised if the protocol version is not\n\ta valid protocol version identifier (e.g. not a tuple).\n\n\t:param protocol_version: tuple identifyig a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:param"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls._protocol_handlers\n\telif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol_version must be a tuple\")\n\n\tresult = {}\n\tfor version, handlers in cls._protocol_handlers.items():\n\t\tif version > protocol_version:\n\t\t\tbreak\n\t\tresult[version] = handlers\n\treturn result\n\n###############################################################################\n## Connection Management\n###############################################################################\n\n@classmethod\ndef _get_ssl_context(ssl_"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef uowfn(fn):\n\t\tdef wrap(self, *args, **kwargs):\n\t\t\tmetadata = kwargs.pop('metadata', None)\n\t\t\ttimeout_ = kwargs.pop('timeout', None)\n\t\t\tmetadata = dict(metadata or {}, **metadata or {})\n\t\t\tif metadata and timeout_ is not None:\n\t\t\t\traise TypeError(\"timeout can only be specified when metadata is None\")\n\t\t\tif timeout_ is None:\n\t\t\t\ttimeout_ = self.db.network.config.default_timeout\n\t\t\twith using(self):\n\t\t\t\tmetadata.update"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tfrom decorator import FunctionMaker\n\tfrom functools import partial\n\tfrom types import BuiltinFunctionType\n\ttry:\n\t\tfrom inspect import getannotations\n\texcept ImportError:\n\t\tdef getannotations(func):\n\t\t\treturn {}\n\t\n\tdef _set_tx_meta_data(metadata=None, timeout=None):\n\t\tif metadata is None:\n\t\t\tmetadata = {}\n\t\ttx_metadata = {}\n\t\ttx_metadata.update(metadata)\n\t\tif timeout is not None:\n\t\t\tif not isinstance(timeout, (int, float)):"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrapped(*args_, **kwargs_):\n\t\ttry:\n\t\t\treturn do(*args_, **kwargs_)\n\t\texcept TimeoutError as ex:\n\t\t\tif timeout is None:\n\t\t\t\traise\n\t\t\telapsed = time.time() - start_time\n\t\t\tif elapsed + timeout > ex.seconds:\n\t\t\t\traise\n\t\tfinally:\n\t\t\tend_time = time.time()\t\t\n\treturn wrapped\n\n\ndef do(*args_, **kwargs_):\n\t\"\"\"\n\tUsed as an decorator for returning an entity::\n\n\t\tfrom neo4j"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\treturn functools.partial(neo4j._execute_transaction, *args, **kwargs)\n\n\ndef check_access_or_raise(permission):\n\tdef wrap(function):\n\t\t@wraps(function)\n\t\tdef check(*args, **params):\n\t\t\tcurrent_app.logger.debug('\\n\\n----\\n%s', inspect.currentframe().f_back.f_code.co_name)\n\t\t\tinstance = args[0]\n\t\t\t_set_auth_userid(instance, kwargs)\n\t\t\tauth"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef retry(func):\n\t\t@wraps(func)\n\t\tdef retry_wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t    return func(*args, **kwargs)\n\t\t\texcept (TimeoutError, ServiceUnavailableError, TransientError, CypherError) as error:\n\t\t\t    self.log.error(\"Transaction timed out. {}\".format(error))\n\t\t\t    if self.is_in_transaction():\n\t\t\t        raise error\n\t\t\t    else:\n\t\t\t        raise TransactionError() from error\n\t\treturn retry_wrapper\n\treturn"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\treturn _TransactionDecorator(metadata, timeout=timeout)\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tpass\n\t\n\t@classmethod\n\tdef unit_of_work_for(klass):\n\t\tdef unit_of_work_by_class(klass):\n\t\t\tdef wrapper(func):\n\t\t\t\t@functools.wraps(func)\n\t\t\t\tdef wrapped(*args, **kwargs):\n\t\t\t\t\twith klass.provider.new_transaction() as tx:\n\t\t\t\t\t\treturn func(tx, *args, **kwargs)\n\t\t\t\treturn wrapped\n\t\t\treturn wrapper\n\t\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorator_function(func):\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tconnection = args[0]\n\t\t\tmetadata = _get_tx_meta_data(metadata)\n\t\t\tresult = connection.run_unit_of_work(func, metadata, timeout)\n\t\t\treturn result\n\t\treturn decorated_function\n\treturn decorator_function\n\n\ndef _get_tx_meta_data(metadata):\n\tif metadata is None:\n\t\tmetadata = {}\n\treturn {\n\t\t\"metadata\": metadata,\n\t\t\"duration\": None,\n\t\t\""}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef _unpack_metadata(metadata=None):\n\t\tif metadata is None:\n\t\t\treturn\n\t\telif callable(metadata):\n\t\t\tmetadata = metadata()\n\t\treturn OrderedDict(sorted(metadata.items(), key=itemgetter(0)))\n\t\n\tdef outer(ret):\n\t\tdef outer(f):\n\t\t\t@functools.wraps(f)\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t\twith handle_transaction(ret, timeout, metadata) as future:\n\t\t\t\t\treturn f(*args, **kwargs)\n\t\t\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef inner(func):\n\t\tdef execute(*params, **kwparams):\n\t\t\twith get_driver().session() as session:\n\t\t\t\tresult = session.run(func, *params, **kwparams)\n\t\t\t\tsession.metadata = metadata\n\t\t\t\trecord = result.single()\n\t\t\t\tif result.forward() and not record:\n\t\t\t\t\traise RuntimeError(\"Failed to execute transaction\")\n\t\t\t\ttry:\n\t\t\t\t\treturn record[\"result\"]\n\t\t\t\texcept KeyError:\n\t\t\t\t\traise ValueError(\"Transaction execution completed but result was missing\")\n\t\texecute.__doc__ = func.__doc__\n"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry:\n\t\tif self._hashtable == None:\n\t\t\tself.__init__(capacity)\n\t\t\t\n\t\thash = hash(key) % self._capacity\n\t\tfor index in range( self._capacity):\n\t\t\tif self._data[index] == ( None):\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tel = self._data[index]\n\t\t\t\tif key == el[0]:\n\t\t\t\t\treturn index\n\t\t\t\t\n\t\treturn None\n\texcept:\n\t\treturn None\n\t\n\t\n\tdef contains(self, key):\n`\t\n\tReturn true if the item is"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif key in self.__keys:\n                i = self.__keys.index(key)\n\t\treturn self.__len + i\n\n\traise KeyError(f\"{key} not found.\")\n\n\t\n\n\n\t\t\n\n\n\ndef insert(self, item):\n        index = self.index(item)\n        self.buckets[index] = item\n        self.count += 1\n\"\"\"\n\tInserts the item at the appropriate place in the ordered dictionary according\n\tto the order of the keys.\n\tRaises an KeyError if the key is already present.\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._index[key]\n\t\n\t\nclass MinHeap:\n\t\"\"\"Heaps have a parent node and an array with values.\n\n\tWhen added to the heap, it is either inserted in the root or the leaf\n\tif it is the parent node.  The parent node is used to maintain the\n\tmin-heap property by comparing the value of the parent node to the node's\n\tvalue.  The index of the parent node will be stored in self._parent_indices.\n\t\n\tWhen the parent node is removed, it is re-inserted at the"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tindex = self._locate(key)\n\tif index!= Key.BAD_INDEX_SIZE:\n\t\treturn index\n\"\"\"\nclass Lru(object):\n\tdef __init__(self):\n\t\tself.size = 0\n\t\tself.cache = dict()\n\t# \u6cbf\u7528\u5b57\u5178\u8bb0\u5f55\u5143\u7d20\n\"\"\"\n\n\nif __name__ == '__main__':\n\tlru = LRU()\n\tlru['abc'] = 1\n\tlru['123'] = 2\n\tprint(l"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif key in self:\n\t\treturn self.values().index(key)\n\telse:\n\t\tself.append(key)\n\t\treturn len(self) -1\n\ndef keys(self):\n\"\"\"\n\tReturn a list of keys in the map, in iterable form.\n\t\n\t:return: keys\n\t:rtype: list\n\t\"\"\"\n\treturn self.get()\n\ndef items(self):\n\"\"\"\n\tReturn a iterator for the map.\n\t\n\t:return: iterator\n\t:rtype: iterator\n\t\"\"\"\n\treturn self."}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry :\n\t\treturn self.hash_map[key]\n\texcept :\n\t\treturn -1\n\t\ndef get_data_from_file(file_path):\n\t\"\"\"\n\tReturn all data from a File.\n\t\n\t:param file_path: A File Path\n\t:return: list of dictionary\n\t:rtype: list\n\t\"\"\"\n\tdata = []\n\tfor line in open(file_path, 'r') :\n\t\ttemp = {}\n\t\tline_split = line.split(\".\")\n\t\ttemp[\"name\"] = line_split["}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.indexing_function(lambda row : row[key])\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\thash = self.hash(key)\n\treturn hash % self.count\n\"\"\"\nFind the size:\n\n\"\"\"\nclass HashMap:\n\tdef __init__(self, arr, length, count):\n\t\tself.arr = arr\n\t\tself.count = count\n\t\tself.length = length\n\"\"\"\n\n\"\"\"\nUse Splay for hashing:\n\"\"\"\n\"\"\"\n\tdef hash(self, key):\n\n\t\thash = 0\n\t\tfor char in key:\n\t\t\tchar = ord(char)\n\t\t\thash = (self.length * hash + char) % self."}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t__idx = getattr(self, idx)\n\treturn __idx(key)\n\n\ndef _get_size_of_type(a_type):\n\t\"\"\"\n\tDeterministically get the size (in bytes) of a type\n\n\tIt looks for all known types defined in the platform and use the\n\treturn values to get the size of the corresponding C type (from ctypes)\n\t\"\"\"\n\tfrom itertools import chain\n\t\n\tif a_type in {float, _float_}:\n\t\treturn ctypes.sizeof(a_type"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.table[self.hash(key)]\n\n\nclass HashTable(Base):\n    \"\"\"\n    A hash table class that stores key/value pairs by using different hash \n    functions to map keys to values in buckets.\n\t\n\t:param size: the size of the hash table.\n\t:type size: int\n\t:param load_factor: The load factor of the hash table, must be between 0 and 1.\"\"\"\n    \n    def __init__(self, size=11, load_factor=0.75): \n        \"\"\"\n        A function"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys:\n\t\tkeys = sorted(self._data.keys())\n\treturn tuple([self[key] for key in keys])\ndef keys(self):\n\t\"\"\"\n\tReturn a list of all keys in the record\n\t\n\t:return: list of keys\n\t:rtype: list\n\t\"\"\"\n\treturn list(self._data.keys())\n\n\nif __name__ == \"__main__\":\n\timport pprint\n\tr = Record()\n\tr.name = \"Foo\"\n\tprint(type(r.name))\n\tprint("}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn [self._records[key] for key in self._iterkeyset(keys)]\n\n\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n# vim: set ts=4 sw=4 sts=4 tw=0:\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\treturn self.filter(keys)\n\t\n\t\n\tdef filter(self, keys):\n\t\"\"\"\n\tReturn the keys included in the record, optionally filtering\n\tby specific keys to be included or excluded.\n\t\n\t:param keys: indexes or keys of the items to include or exclude;\n\tif none are provided, all items will be included or excluded;\n\t    if they are strings, they will be converted to lowercase\n\t    by the library; if they are not strings,\n\t    they can be used as an OR list of filter operations\n\t:return"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn _get_record_values(self.record, keys)\n\t\n\t\n\"\"\"\nDict Wrapper for the record - a dictionary-like object that\nretrieves the record values by index or key\n\"\"\"\nclass record:\n\tdef __init__(self, record):\n\t\tself.record = record\n\t\t\n\tdef get(self, default, *keys):\n\t\t\"\"\"\n\t\tReturn the record values, optionally filtering to\n\t\tinclude only certain values by index or key.\n\t\t\n\t\t:param default: default value; ignored if values are not\n\t\t:param"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tfor key, value in self.items():\n\t\tif key in keys or not keys:\n\t\t\tyield value\n\nif __name__ == '__main__':\n\t\n\trecs = [(0, 2, \"Hello\"),\n\t        (1, 2, \"World\")]\n\t\n\trec = Record(recs)\n\t\n\tprint rec\n\t\n\trec2 = record(keys=\"World\", items=0)\n\tprint rec\n\t\n\trkeys = record.keys(5)\n\tprint rkeys\n\t\n\t# rkeys = record"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\t\t\n\t\tfor v in self.keyValuePairs:\n\t\t\tfor index in range(1, v.count()):\n\t\t\t\tvName = v.item(0, index).text()\n\t\t\t\t\n\t\t\t\tif vName in keys:\n\t\t\t\t\tyield self.value(vName)\n\t\n\t\n\tdef keyValuePairs(self):\n\t\t\"\"\"\n\t\tReturn a list of key-value tuples representing the\n\t\trecords current record data.\n\t\t\"\"\"\n\t\n\t\t\n\t\treturn []\n\t\n\t\n\tdef keys(self):\n\t\t\"\"\"\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys and self._values:\n\t\treturn reduce(lambda a, e: a + e,\n\t\t\t\t\t   [_v.values(key) for key, _v in list(self.items()) if key in keys])\n\telse:\n\t\treturn self._values\n\n\nclass ObjectWrapper(object):\n\t\"\"\"Class that provides uniform methods for objects in a list.\"\"\"\n\n\tdef __init__(self, obj_list):\n\t\t\"\"\"Initializing the object wrapper.\n\n\t\t:param obj_list: a sequence of objects to wrap\n\t\t\"\"\"\n\t\tself.obj_list"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t_filter = (lambda *x: True,) +  _filterize(keys)\n\t\n\treturn self.record[_filter]\n\t\n\t\ndef filterByValue(self, value, type, *key):\n\t\"\"\"\n\tReturn a :class:`.Record` with only the records\n\tcontaining the item as a specific value.\n\t\n\t:param value: value to match\n\t:param type: item type\n\t:param key: key[s] that match the item value\n\t:return: filtered records\n\t\"\"\"\n\t_filter = (_filterize("}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tpass  # class\n\"\"\"\n\tdef view(self, *keys):\n\t\tview = None\n\t\tif len(keys) == 1 and isinstance(keys[0], str):\n\t\t\tview = self.index(keys[0])\n\t\telif len(keys) > 1:\n\t\t\tview = self.index(*keys)\n\t\treturn view\n\"\"\"\ndef __new_doc(cls, name, value):\n\t\"\"\"\n\tDummy definition for __new_doc to prevent a PyMethodDef to be\n\tregistered on the module.\n\t\"\"\"\n\treturn None"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\tif not keys:\n\t\treturn self.get_values()\n\telse:\n\t\treturn [self.get(key) for key in self.keys() if key in keys]\n\t\n\n# \tdef values(self, *keys):\n# \"\"\"\n# Return the values of the record, optionally filtering to\n# include only certain values by index or key.\n#\n# :param keys: indexes or keys of the items to include; if none\n#             are provided, all values will be included\n# :return: list of values\n# `\n"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn [self.get(key) for key in self._keys if key in keys]\n\n\"\"\"\n============================\nCompound data type definitions\n============================\n\nThe following data type definitions are provided:\n\n:class:`.Mapping`\n\tA record type, mapping keys to values. If a record has these\n\telements, the record\u2019s data may be retrieved either as a\n\tdictionary or by key name in place of the elements. If the\n\tvalue fields of any of the record elements is a :class:`.Record`\n\tinstance, the mapping of a"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t# make sure we have a sequence to operate on\n\tif not isinstance(keys, collections.Sequence):\n\t\tkeys = [keys]\n\treturn self._data(*keys)\n# /data\n\nclass RecordList(list):\n\"\"\"\n\tList subclass with support for field name indexing and field name\n\tand record name string comparisons.\n\n.. attribute:: name\n\n\tRecord set name; ``None`` for the top-level Record instance\n\n.. attribute:: key_fn\n\n\tKey function: a function that extracts the \"key\" portion of a record;\n\tdefaults"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\td = {}\n\tfor i in range(len(self)):\n\t\tv = self.item(i).data\n\t\tif keys and i in keys:\n\t\t\td[self.fields[i].name] = v\n\t\telse:\n\t\t\td[self.fields[i].name] = v\n\treturn d\n\nclass RecordMap:\n\"\"\"\n\tProxy over a record-like object, forwarding each item by value;\n\tthis type supports a case-insensitive lookup and item retrieval\n\tvia a single key. In the case of the record, a ``key-ordering"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn {key:value for (key,value) in self}\n\ndef set_key(self, key, value):\n\"\"\"\n\tSet a key-value pair in this record.\n\t\n\t:param key: the field name\n\t:param value: the value\n\t\"\"\"\n\tself.data[key] = value\n\nclass Record(dict):\n\t\"\"\"\n\tThis class behaves like a :class:`dict`, and is constructed\n\tfrom a string of the form `<key> / <value> /...`.\n\t\n\tConstructing an actual record object"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn self._record_d['data'](*keys)\n\t\n\t\ndef _get_record(self, index) -> JSBinja:\n\t# the actual record can be found in '__data' as\n\t# the record is part of a compound array\n\tif '__data' not in self._record_d:\n\t    raise ValueError(f'{self.__class__.__name__} has no array type defined, and is not a compound array')\n\tassert self._record_d['element_count'] == len(self._record_"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif len(keys) == 0:\n\t\tkeys = self._fields.keys()\n\td = {}\n\tfor k in keys:\n\t\tif k not in self._fields:\n\t\t\td[k] = None\n\t\telif k not in _all_cdata and len(getattr(self, k, _nil)) == 0:\n\t\t\td[k] = None\n\t\telse:\n\t\t\ttry:\n\t\t\t\tv = getattr(self, k)\n\t\t\texcept TypeError:\n\t\t\t\tif k in _all_cdata and v is not _nil:\n\t\t\t\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif keys:\n\t\tf = lambda i: i in keys\n\telse:\n\t\tf = lambda i: True\n\treturn dict(filter(f, self))\n\ndef copy(self):\n\t\"\"\" Return a deepcopy of this record \"\"\"\n\treturn type(self)(**self.data().copy())\n\ndef as_tuple(self):\n\t\"\"\" Return a tuple of all the fields in this record, in the\n\tsame order as the columns are defined\"\"\"\n\treturn tuple(self.data().values())\n\ndef as_dict(self):\n\t\"\"\" Return"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif not keys:\n\t\treturn self._data\n\telse:\n\t\treturn {k: self._data.get(k) for k in keys}\n\nclass RecordTemplate(Record, tuple):\n\t\"\"\"\n\tSubclass of :class:`~Record` and parent class :class:`tuple`,\n\tdesigned for use in declaring a record template for an\n\tindex that has multiple fields and can hold one or more\n\trecords. Every :class:`~Record` that will be inserted into\n\tan index is cast into an instance of this class.\n\t\"\"\"\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\td = {}\n\tfor i, value in enumerate(self):\n\t\tif i in self._data_indexes:\n\t\t\td[i] = value\n\t\tif i in keys:\n\t\t\td[i] = value\n\t\t\tdel d[keys[keys.index(i)]]\n\n\t\tif i >= len(self._data_size):\n\t\t\traise IndexError('Invalid index')\n\n\treturn d\n\ndef dt(format='%H:%M:%S', offset=True):\n\t\"\"\" Returns a formatted time given elapsed time since the epoch in the"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif not keys:\n\t\treturn {}\n\n\tvalue = self._data\n\tfield = self._title\n\n\tfor idx in range(*keys):\n\t\tif idx >= len(value) or idx < -len(value):\n\t\t\traise IndexError(\"key %r out of range for Record of size %r\" % (idx, len(value)))\n\n\treturn dict((field, value[idx]) for idx in keys)\n\n\nclass RecordException(Exception):\n\n\tdef __init__(self, msg, record):\n\t\tself.record = record\n\t\tsuper"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack_from(\"<H\", self.data, self.buf_ofs)[0]\n\t\n\t\n\t\ndef pop_u8(self):\n\t\"\"\"Remove the last byte from data, and return it.\"\"\"\n\treturn struct.unpack_from(\"<B\", self.data, self.buf_ofs)[0]\n\t\n\t\n\t\ndef pop_s64(self):\n\t\"\"\"Remove the last 8 bytes from data, and return them as a big-endian\n\t64-bit signed integer.\"\"\"\n\treturn struct"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack('>H', self.f.read(2))\n\n\n# --------------------------------------------------------------------\n##  Utility functions\n\nR5 = [0, 5]\nR6 = [0, 6]\nR7 = [0, 7]\nI7 = [1, 7]\n\nI5 =  [1, 5]\nI6 =  [1, 6]\nI8 =  [1, 8]\nI16 = [1, 16]\nI32 = [1, 32]\n\n\ndef add(a, b):\n\t\"\"\"add,"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.data_item_struct.unpack_from(\"<H\", self.data_item_struct.buffer(self.data_item_struct.size) )[0]\n\nclass Data(object):\n\t\"\"\"\n\tHandles reading in a data array.  In the first version of the parser\n\tthe data will be stored in the file in one go. In subsequent versions\n\tmore than one data array can be in the file and the data will be split\n\tbetween the arrays. The next is to be implemented.\n\t\"\"\"\n\tdef __init__(self"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t__pop_u16_fn = __pjsua2_sctp.__dict__.get(\"_pjsua2_sctp_inst_func_pop_u16\")\n\treturn __pop_u16_fn(self._inst.inst_id)\n\n\nclass Ipv4AddrTransport(object):\n\t\n\tdef __init__(self, inst_id, max_data_rate, enable_delay_tx):\n\t\tthis = __pjsua2_sctp.new_Ipv4Addr"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack(self._endian + 'H', self._buffer.read(2))[0]\n\nclass FileWriter:\n\t\"\"\"\n\tWrapper class, which exposes the write() method for a file-like object.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself.fo = None\n\t\tself.nbytes = None\n\t\tself.write_pos = None\n\t\tself.seek = None\n\n\t\tself.fo = open(name, 'wb')\n\t\tself.nbytes ="}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self._read_mem_aligned(2, '<H')\n\n\t\n\t\n\t\n\n\t\n\t\n\t\n\n\n\n\n\n\n\"\"\"\n\nclass PCDUSBData(RawMemoryMap):\n\t\"\"\"\n\tUSB raw data map for the PC driver.\n\t\"\"\"\n\tdef __init__(self, config):\n\t\tsuper(PCDUSBData, self).__init__(config, 0x4000, 0x8000)\n\n\tdef pop_int(self):\n\t\t\"\"\"\n\t\tPop the last four bytes of data into an unsigned 32-bit integer.\n\t\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tfrom beb._bitarray import bitarray\n\treturn self.pop_bytes(2).get_int()\n\nclass pop_u32(pop_u16):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a\n\tbig-endian unsigned integer.\n\t\"\"\"\n\tfrom beb._bitarray import bitarray\n\tdef pop_bytes(self, amount):\n\t\t\"\"\"\n\t\tRemove the specified number of bytes of data from the end of a\n\t\tbytearray, returning the bytes as a contiguous bitarray.\n\t\t\"\"\"\n\t\treturn"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tpass\ndef pop_caddr(self):\n\"\"\"\n\tRemove the last word of data, and return a string.\n\tThe last word is usually what the C address of the referenced\n\tblock is.\n\t\"\"\"\n\tpass\ndef pop_cint32(self):\n\"\"\"\n\tThe last word of data, and return an integer in the range\n\tof C int, and a big-endian representation as well.\n\tThe last word is usually what the C address of the referenced\n\tblock is.\n\t\"\"\"\n\tpass\ndef pop_cint64(self"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tpass\n_u16 = None\ndef push_bytes1(data):\n\"\"\"\n\tPush a sequence of bytes at the current offset.\n\t\"\"\"\n\tpass\ndef u8_u16(data):\n\"\"\"\n\tPut the next 8-bytes of the given string as a big-endian\n\t16-bit unsigned integer. The original string is unchanged.\n\t\"\"\"\n\tpass\ndef u16_u16(data):\n\"\"\"\n\tPut the next 16-bytes of the given string as a big-endian\n\t16-bit unsigned integer. The original string is"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t(data,) = struct.unpack(\">H\",self._unsync_buffer[2:4])\n\tself._unsync_buffer = self._unsync_buffer[4:]\n\treturn data\npop_u16 = sark.safecall(\"pop_u16\",locals())\n\ndef pop_u32(self):\n\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\t(data,) = struct.unpack(\">I\",self._unsync_buffer["}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.sendall()\n\treturn Response(\n\t\t(b'', self.response_status,b'',[b'DISCARD'],\n\t\tb'discard %d' % (n),\n\t\tstr(qid),\n\t\tb'' if not dehydration_hooks else 'discard %s'\n\t\t% dehydration_dict_to_string(dehydration_hooks),\n\t\t\tb'' if not qid else 'qid %d' % (qid),\n\t\tb'' if not dehydration_hooks else 'hydrate %s'\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn Response._send_commands(0x4f, bytes([n, qid]),\n\"\"\"\n\tSsB    # SSBB\n\tSsB    QID         # SSBQ if more than one query is being sent,\n\tSqH   CNT          # CNT is command count\n\tSsB    # SSBB\n\tSsB    # SBBE\n\tSsB\n\"\"\")\n\ndef get(qid=0, n=0, mode='binary', handlers=None,\n\tdehydration_hooks=None,"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif not (n or qid) or (n and qid):\n\t\traise Exception(\"Specify the arguments, n or qid\")\n\t\n\tif not n and not qid:\n\t\tself._flush()\n\t\treturn self.app.discard()\n\t\n\thandlers = set()\n\t# append the arguments list for the DISCARD messages to the current queue\n\t# but return the current queue otherwise, this will cause the _flush() to \n\t# flush all DISCARD's but not the current one while the next DISCARD\n\t# in the"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t_msg(self, \"DISCARD %r, %r\" %(n, qid))\n\n\n@contextmanager\ndef query(self, *args, **kwargs):\n\t\"\"\"\n\tContext manager for queries, yields the new Response class on exit instead of closing the socket\n\n\t:type self: Response\n\t\n\t:param args: arguments passed directly to the constructor\n\t:param kwargs:\n\t    keyword arguments passed directly to the constructor\n\t\"\"\"\n\tres = self(*args,**kwargs)\n\ttry:\n\t    yield res\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append(self.packstream.createcommand(0x05, b\"DISCARD\", \n\t    b\";\", qid, n, 0x0a, self.packstream.getpackstreaminfo().streams,\n\t    hydration_hooks, dehydration_hooks, hdrs=handlers))\n\t\n\treturn Response(self)\n\t\ndef discardall(self, handler=None, *args, **kwargs):\n\t\"\"\"\n\t:type handler: function | object\n\t\n\t:keyword args: any arguments to `execute"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef do(self,\n\t\tqid=-1,\n\t\tqname='default',\n\t\tfields='default',\n\t\tflags='default',\n\t\topcode='default',\n\t\tpayload=None, \n\t\ttimeout=None,\n\t\tmax_bytes=None, \n\t\tbackend='default',\n\t\tbackend_kwargs=None,\n\t\tdehydration_hooks=None, \n\t\t**kwargs):\n\t\t\"\"\"\n\t\tOpens a new connection in the queue and send a DISCARD message.\n\t\n\t\t:param qid: query ID"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t# check that Q is well formed by sending a QUERY message with\n\t# the provided QID\n\tif qid < 0:\n\t\tqid = self._last_response.query_id\n\t\tif qid == None:\n\t\t\terr_code, _ = self.send(self._q)\n\t\t\tif err_code!= dns.rcode.NOERROR:\n\t\t\t\tprint('Error %d sending message for DISCARD' % err_code)\n\t\t\t\treturn None\n\t# make a query by sending a QUERY message with the provided id and\n\t# QID"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.msg.encode(\n\t    n, qid, dehydration_hooks=dehydration_hooks,\n\t    hydration_hooks=hydration_hooks, handlers=handlers\n\t)\n\n\tif self.verbose:\n        \tprint \"Sending DISCARD\"\n\n    \tresponse = self.server.sendall(self.dop)\n\n    \tresponse_bytes = self.server.recv(self.drec)\n\n    \tresponse = self.parser.decode(response_bytes)\n\n    \tif self.verbose"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tassert isinstance(dehydration_hooks, (dict, NoneType))\n\tassert isinstance(hydration_hooks, (dict, NoneType))\n\tif (hydration_hooks is None) or (dehydration_hooks is None):\n\t\thydration_hooks = { }\n\t\tdehydration_hooks = { }\n\tif (n is None) and (qid is None):\n\t\traise ValueError('either n (to discard) or qid (for specified query id) must be specified')\n\tresponse = packstream._r("}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tr = Response()\n\tif qid == -1:\n\t    qid = self.qid\n\tif n > 0:\n\t    r._add(self._send(\"DISCARD\", n, qid, cn=self.cid))\n\telse:\n\t    r._add(self._send(\"DISCARD\", qid, qid, cn=self.cid))\n\treturn r\n"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t_validate_begin(\n\t\tmode, bookmarks, metadata, timeout, db,\n\t\thydration_hooks, imp_user, **kwargs)\n\n\t# If the transaction was already started\n\tif packet.transaction_id in self._in_progress:\n\n\t\t# It means a failure was encountered during connection setup\n\t\tif self._fate!= \"started\": self._log(\n\t\t\t_logger.error,\n\t\t\t\"Failed to establish a new connection\\n\")\n\n\t\t# If a failure was encountered during transactions setup\n\t\tif self._fate!= \""}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn self\n\t\ndef end(self, bookmark=None, metadata=None, timeout=None, db=None,\n\timp_user=None, dehydration_hooks=None,\n\thydration_hooks=None, handlers=None,\n\tflags=0):\n\t\"\"\"\n\tAppends an END message to the output queue.\n\t\n\t:param bookmark: bookmark value after which this transaction should end\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t    Requires Bolt 4.0+.\n\t:param timeout: timeout for transaction execution (seconds)"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tmsg = self.append_payload(\n\t\tname=self.OUTPUT_QUEUE_NAME,\n\t\tcontent=__end_transaction(\n\t\t    mode=mode,\n\t\t    bookmarks=bookmarks,\n\t\t    metadata=metadata,\n\t\t    timeout=timeout,\n\t\t    db=db,\n\t\t    imp_user=imp_user,\n\t\t    dehydration_hooks=dehydration_hooks,\n\t\t    hydration_hooks=hydration_hooks,\n\t\t    handlers=handlers,\n\t\t),\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn self._response_impl(mode=mode, db=db, bookmarks=bookmarks,\n\t\tmetadata=metadata, timeout=timeout, body_stream=self._connection.output_streams.begin(self._idb_url[db], mode, bookmarks, metadata, timeout, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, handlers=handlers))\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tassert not self.error, \"error\"\n\tself.check_initialized()\n\tif not self.connection.is_open:\n\t\treturn\n\tself.mode = mode\n\tif mode is None:\n\t\tself.mode = \"READ\"\n\telif mode not in (\"READ\", \"WRITE\"):\n\t\traise PackStreamException(\"invalid read/write mode\")\n\tself.bookmarks = bookmarks\n\tif self.mode == \"READ\":\n\t\tif mode == \"READ\" and (bookmarks or metadata or timeout):\n\t\t\traise PackStreamException(\"'"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tpass\n\n\ndef commit():\n\t\"\"\"\n\tAppends a COMMIT message to the output queue.\n\t\n\t:return: None\n\t\"\"\"\n\tpass\n\n\ndef createDatabase():\n\t\"\"\"\n\tCommits and creates a database on disk.\n\t\n\t:return: database name\n\t:rtype: str\n\t\"\"\"\n\tpass\n\n\ndef dropDatabase():\n\t\"\"\"\n\tDrops the database with the given name on disk.\n\t\n\t:param name: Database name to drop\n\t:return: None\n\t\"\"\"\n\tpass\n\n\ndef execute"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\ttry:\n\t    if mode is None:\n\t        mode = BoltResponse.WRITE\n\t    self.output_mode = mode\n\t    \n\t    if bookmarks is None:\n\t        bookmarks = []\n\t    self.bookmarks = bookmarks\n\n\t    if metadata is None:\n\t        metadata = {}\n\t    self.metadata = metadata\n\t    \n\t    if timeout is None:\n\t        return BoltResponse(\"ERROR\")\n\t        \n\t    if imp_user is None:\n\t        imp_user = \"\"\n\t    self.imp"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not self._state == self._state_open:\n\t\traise IllegalState('Transaction not open')\n\t\n\tif self._tls and self._tls.localPort!= 0:\n\t\tlogging.warning('Appending a BEGIN message to an already opened transport or client session,'+\n\t\t\t            'this is not recommended')\n\t\n\treturn Response(self._writer.write(request={'method': 'begin',\n\t\t}), self)\n\n\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn None, None, None, None, None, None, None, None, None, None, \\\n\t   None, None, None, None, None, None, None, None, None, None, None, None, None\n\n\ndef check_fail_status(value=None, status=None):\n    \"\"\"\n    Checks the status from the Bolt error, prints it and returns a negative\n    error value.\n\n    :param value:\n    :param status:\n    :return: negative error code to use in the returned Response object\n    \"\"\"\n    if status:\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not isinstance(c, DatabaseConfiguration):\n\t\traise ProgrammingError(\"`c` must be an instance of DatabaseConfiguration\", 400)\n\n\tpackstream_version = Version.parse_package_version(c.version)\n\t\n\tif not packstream_version.minimum_supported_api_version_major_in_py():\n\t\traise ProgrammingError(\"BOLT 4.0+ is required for START statement\", 400)\n\n\tif not packstream_version < Version(\"4.5.0\"):\n\t\traise NotSupportedError(\n\t\t\t\"This library does not support Bolt"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn round(n + 0.5) if (n % 0.5) - (n % 1) >= 0.5 else n\n\n\ndef round_int(n, base=0):\n    '''\n    >>> round_int(12)\n    12\n    >>> round_int(100, 2)\n    100\n    >>> round_int(100, 3)\n    500\n    >>> round_int(999, 3)\n    999\n    >>> round_int(100, -3)\n    500\n    >>> round_int(100, -5)\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\trounded = n\n\tif floor(abs(n - rounded)) > 0.5:\n\t\trounded += 1\n\treturn rounded\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\timport math\n\n\treturn math.ceil(n)\n\nfrom datetime import datetime\n\n\ndef weekday(dt):\n\"\"\"\n\t>>> weekday(datetime(2016,1,25))\n\t    4\n\t>>> weekday(datetime(2016,1,27))\n\t    3\n\t>>> weekday(datetime(2016,1,28))\n\t    4\n\t\n\t:param dt:\n\t:return:\n\t\"\"\"\n\treturn dt.weekday()\n\nimport random\n\n\ndef shuffle(x):\n\t\"\"\"\n\t>>> random."}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n % 1 >= 0.5:\n\t\treturn n + 1\n\telse:\n\t\treturn n\n\ndef max_number(n):\n\t\"\"\"\n\tReturn maximum of args\n\t:param n:\n\t:return:\n\t\"\"\"\n\tresult = None\n\tfor item in n:\n\t\tif result is None or result < item:\n\t\t\tresult = item\n\treturn result\n\t\n\nif __name__ == \"__main__\":\n\timport doctest\n\tif doctest.testmod().failed == 0:\n\t\tprint(\"\\n*** ALL T"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\timport math\n\t\n\t#return round(n + 0.5) if math.floor(n)!= n else round(n)\n\treturn math.floor(n)"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tfraction, whole = n\n\tremainders = [rem for rem in xrange(1, 10) \\\n\t\t\t\t\tif 2 * remainder >= whole + rem]\n\td = {-1: -2, 1: 1, 2: 0}\n\tif not remainders:\n\t\treturn int(n)\n\treturn 10 + d[len(remainders)] + remainder\n\n\n\n# print(round_half_to_even(3.8))\n# if the fraction is even, just subtract 2 or add 2 if the remainder is 1.\n\n\"\"\""}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif type(n)!= int or n<0 or n>100:\n\t\traise ValueError(\"number must be an integer between 0 and 100\")\n\n\thalf = n//2\n\tif (n % 2) == 1:\n\t\treturn half + 1\n\n\treturn half\n\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod(verbose=True)"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\"\"\"\n\t:type n: float\n\t:rtype: float\n\t\"\"\"\n\tround(n,2)\n\n\ndef sum_digits(n):\n\t\"\"\"\n\t:type n: int\n\t:rtype: int\n\t>>> sum_digits(233)\n\t233\n\t>>> sum_digits(2147483589)\n\t1847\n\t>>> sum_digits(19)\n\t6\n\t>>> sum_digits(13)\n\t1\n\t>>> sum_digits(1)\n\t0\n\t>>> sum_"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\timport math\n\treturn math.floor(n + 0.5)\n\nif __name__  == \"__main__\":\n    import doctest\n    doctest.testmod(verbose=True)\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn round(n + (0.5 if n % 2 else -0.5))\n\n\ndef round_half_to_nearest_even(n):\n\n\twhile n > 0:\n\t\tif (\n\t\t\tn % 2 == 1\n\t\t):\n\t\t\tn -= 1\n\t\telse:\n\t\t\tif (n + 0.5) % 2 == 0:\n\t\t\t\treturn round(n + 0.5)\n\t\t\telse:\n\t\t\t\treturn round(n)\n\treturn 1\n\ndef round_half_to_nearest_even_iter(n):\n\t\n\t"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint = type(name, (Point,), { '_fields': fields,'srid': srid_map[name] })\n\treturn Point\n\nPoint._fields = (\n\t('x', 'float32'),\n\t('y', 'float32'),\n)\n\ndef point_field_type(fields, field_names):\n\tif ('x' not in field_names) or ('y' not in field_names):\n\t\traise ValueError('missing required x and y values')\n\n\tif len(fields)!= len(field_names):\n\t\traise Value"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# Define class name and mapping\n\tkwargs = srid_map.mak_srid_mapping(**fields)\n\t\n\t# Create new class\n\tattrs = {'__module__' : name}\n\tattrs.update(kwargs)\n\tname += 'Point'\n\tcls = type(name, (GeoPoint,), attrs)\n\n\ndef linestring_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically create a LineString subclass.\n\t\"\"\"\n\t# Define class name"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# set the name\n\tname = str(name)\n\t# define the base\n\tbase = Point\n\t# add the fields\n\tif 'x' in fields:\n\t\tbase = Point_x\n\tif 'y' in fields:\n\t\tbase = Point_y \n\t# create the class\n\tclass Point_(base):  \n\t\t# the fields dictionary\n\t\tfields = fields\n\t\tsrid_map = srid_map\n\t\t# the srid key in a map\n\t\tsrid_map_key = srid_map.keys()[0]\n"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tfields_name = []\n\tfields_type = []\n\tfields_srid = []\n\tfor field in fields:\n\t\tfields_name.append(field['name'])\n\t\tfields_type.append(field['type'])\n\t\tfields_srid.append(srid_map[field['srid']])\n\n\tpoint_name = name\n\tpoint_type = 'Point'\n\tpoint_fields = ', '.join(map('%s %s' % (tp, nn) \\\n\tfor nn, tp in zip(fields_"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# For consistent naming. \n\tfield_count = str(len(fields)) # for use in __init__\n\tfield_keys = \", \".join([i[0] for i in fields])\n\n\t# Create the class.\n\tt = \"Point\"\n\targs = \"\"\n\n\tt += \"(\" + field_keys + \")\"\n\targs += field_count + \",\"\n\n\tt += \"srid\" + field_count + \"=0\"\n\targs += \"0,\"\n\n\t# Create constructor.\n\t# We need to generate the coordinates by hand,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# create a new type\n\tfields.insert(0, 'id')\n\tPoint = namedtuple('Point', fields, defaults='x y'.split())\n\tclass Point(cls.Geo):\n\t\t__doc__ = \"\"\"\n\t\tA feature that is a named point.\n\t\t\"\"\"\n\t\tid = db.Column(db.Integer, primary_key=True)\n\t\ttype = db.Column(db.String(16))\n\t\tx = db.Column(db.Float)\n\t\ty = db.Column(db.Float)\n\t\t# fields = s"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoints = [\"geom\", tuple(\"x y\")]\n\tpoint = type(name, (GeoPoint,) + tuple(points), {})\n\n\tif len(fields) == 2:\n\t\tdef get_x_y(self):\n\t\t\treturn self.x, self.y\n\n\t\tpoint.x = property(get_x_y)\n\n\t\tdef set_x_y(self1, x, y):\n\t\t\tself1.x, self1.y = x, y\n\n\tpoint.set_srid(fields[2])\n\tfor attr, f"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tname = str(name)\n\tfields = fields or (None, )\n\tsrid_map = srid_map or default_srid_map\n\t\n\t\n\tclass _point(object):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t__slots__ = ('srid', ) + fields\n\t\t\n\t\tPoint = GEOSGeometryBase\n\t\n\t\n\t_geo_fields = set(name for name, field in fields) & set(BaseField.__dict__.keys())\n\t_field_map = dict((key, key) for key in fields"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tsrid_keys = dict([(k, v) for k, v in srid_map.items() if k is not None])\n\tPoint = old_point_type(name, fields, srid_keys)\n\tPoint._meta.abstract = False\n\treturn Point\n\ndef point_subclass(name, fields, srid_map):\n\t\"\"\"\n\tDynamically create a subclass of Point that supports SRID\n\t\"\"\"\n\tPoint = new_point_type(name, fields, srid_map)\n\tPoint._meta.abstract ="}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom django.contrib.gis.db.backends.util import gis_version\n\tfrom django.contrib.gis.db.backends.pythonic.models import GEOSGeometry\n\tGEOSALLOWED_GEOMETRY_TYPES = getattr(backend, 'GEOSALLOWED_GEOMETRY_TYPES', (GEOSGeometry,))\n\n\tclass Point(GEOSALLOWED_GEOMETRY_TYPES):\n\t\tdef __init__("}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef fset(self, func):\n\t\twarnings.warn(\"%s is deprecated. %s\" % (func.__name__, message),\n\t\t\tcategory=DeprecationWarning,\n\t\t\tstacklevel=2\n\t\t)\n\t\treturn func\n\treturn property(fset)\n\nclass classproperty(object):\n\t\"\"\" A property that is evaluated at class instantiation time.\n\t\n\tExample:\n\t\n\t    class Foo:\n\t    \t@property\n\t    \tdef foo(self):\n\t    \t\treturn 'foo'\n\t    \n\t    "}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func):\n\t    @functools.wraps(func)\n\t    def wrapper(*args, **kwargs):\n\t        warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t        return func(*args, **kwargs)\n\t    return wrapper\n\treturn decorator\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t    def new_func(*args,**kwargs):\n\t        warnings.warn(\"%s has been deprecated in favour of %s\"%(\n\t            message, func.__name__),\n\t        PendingDeprecationWarning, 2)\n\t        return func(*args,**kwargs)\n\t    return new_func\n\n\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef f_deprecated(f):\n\t\tdef new_f(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn f(*args, **kwargs)\n\t\t\n\t\tnew_f.__name__ = f.__name__\n\t\tnew_f.__doc__ = f.__doc__\n\t\tnew_f.__dict__.update(f.__dict__)\n\t\treturn new_f\n\t\n\treturn f_deprecated"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef fset(self, message):\n\t\tself.message = message\n\treturn property(fget=getattr, fset=fset, doc=deprecated.__doc__)\n\n_deprecatedRegistry = {}\n\n\ndef _register(func, message):\n\t\"\"\"Register a callable into the deprecated registry. func must be a\n\tfunction that accepts 1 argument, a message.\n\t\"\"\"\n\tfname = func.func_name\n\tdeprecatedRegistry = _deprecatedRegistry\n\tif fname not in deprecatedRegistry:\n\t\tdeprecatedReg"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef _(wrapped):\n\t\tdef f(_,*__,**kw):\n\t\t\twarnings.warn('%s is deprecated' % message,\n\t\t\t    category=DeprecationWarning,\n\t\t\t    stacklevel=2)\n\t\t\treturn wrapped(*args,**kw)\n\t\tf.func_name = wrapped.__name__\n\t\tf.__doc__ = wrapped.__doc__\n\t\treturn f \n\treturn _\n\n\nclass _DeprecatedList(list):\n\t\"\"\"\n\tA class for tracking what list items are deprecated.\n\t\n\tThe"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef fset(self, func):\n\t\tdef _deprecated(*args, **kwargs):\n\t\t\twarnings.warn(\"'%s' is deprecated: %s\" %(func.__name__, message),\n\t\t\t\tcategory=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn _deprecated\n\tdef fget(self):\n\t\treturn None\n\treturn property(fget=fget, fset=fset, doc=\"\")\n\ndef deprecatedModuleAttribute(mod_name, attr_name, message"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef decorate(funcOrClass):\n\t\tif isinstance(funcOrClass, type):\n\t\t\tcls = funcOrClass\n\t\t\tpatch_for_class(cls, globals(), message)\n\t\t\treturn cls\n\t\telse:\n\t\t\tdef _(*args, **kw):\n\t\t\t\tprint(\"%s() is deprecated, use %s() instead\" %(funcOrClass.__name__, message))\n\t\t\t\treturn funcOrClass(*args, **kw)\n\t\t\treturn _\n\treturn decorate\n\n@deprecated(\"'foo' has been deprecated in favour"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func1):\n\t\tdef func2(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func1(*args, **kwargs)\n\t\t\n\t\t# Update the docstring\n\t\tfunc2.__doc__ = func1.__doc__\n\t\treturn func2\n\treturn decorator\n\n\n\n\ndef run(n_splits=5, save=False, seed=0, verbose=True, random_state=0, \n\t\tearly_stopping=False,"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef f2(*args, **kw):\n\t\twarnings.warn(\"%s has been deprecated in favour of %s\"%(message, name),\n\t\t\t\t\t  category=DeprecationWarning, stacklevel=2)\n\t\treturn func(*args, **kw)\n\ttry:\n\t\tf2.__name__ = func.__name__\n\texcept TypeError:\n\t\t# work around Python < 2.6\n\t\tf2.__name__ = func.func_name\n\tf2.__doc__ = func.__doc__\n\tf2.__dict__"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\toptions = (\n\t\tf'--rcfile={R_OPENBPM}.rc --r-base {R_OPENBPM} --no-write-back cache '\n\t\t'--verbose -V 2 --logto console --logfile log.R')\n\n\tif (\n\t\t'PYPRO_R_EXE' in os.environ\n\t\tand R_OPENBPM in os.environ['PYPRO_R_EXE']\n\t\tand 'PYPRO_EXCLUDE_ONLY_PACKAGES' not in os."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = _inline_r_setup_before(code)\n\tcode += '\\t# R options:\\n'\n\tcode += '\\trc()\\n'\n\tcode += '\\tsetwd(\"$R_HOME/R\")\\n'\n\tcode += '\\tR.get(environment())# load R via the shell\\n'\n\tcode += '\\ttmpdir()# and temporary directory\\n'\n\treturn code\n\n\n@register_inline\ndef _inline_r_cleanup(code: str) -> str:\n\treturn \"\\"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode_template = \"\"\"\nR -f --vanilla -e \"options('-no-save-dashboard')\" -i r_code\n%s\n\"\"\"\n\treturn code_template % code\n\n\ndef _inline_r_code(code: str, code_template: str):\n\tcommand = _inline_r_setup(code_template % (code,))\n\treturn command"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr(\"\"\"\n# Add an R package directory to the list of package directories\nsetClass(\"ROSENV_PREFIX\", rqts::SysPath)\n# rsource in R\n# FIXME: This should be changed for other systems\n# (ROSENV_VERSION is not defined for Windows)\nsetClass(\"ROSENV_RFILESYSTEM\", \"Windows\")\nsource(\"http://www.jb.com/resources/download.shtml#R\");\nsource(\"http://www.rforge.vc.uk/tutorials/install_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tret = \"\"\"\n        # Inline R environment:\n        setenv.source(\"$R_INCLUDE_PATH\")\n        setenv.source(\"$R_LIBS\")\n        setenv.source(\"$R_LIBS_EXTERNAL\")\n        \"\"\"\n\tif 'rstudio' in sys.executable:\n\t\tret += \"\\n        setenv.source('https://cran.rstudio.com/bin/activate')\"\n\treturn ret\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn code.format(\n\t\tcwd = THIS_DIR.as_posix(),\n\t\tpython = SRC_DIR.as_posix()\n\t)\n\ndef _cmd_r_init(code: str, init: str, cwd: str, *args: str):\n\treturn \"cd '{cwd}'; R --slave --vanilla --slave init={init}\" \\\n\t\t.format(cwd = cwd, init = init, args = args)\n\ndef _cmd_r_setup(code: str, args: str, cwd:"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tenv_var_path = '/R-base/r/etc/R'\n\tr_lib = 'r-base'\n\treturn \"R_home=\" + env_var_path + \"\\n\" + \\\n\t\t\"R_sitepackages=\" + env_var_path + \"/../lib\\n\" + \\\n\t\t\"RSLANGPATH=\" + env_var_path + \"/../lib/rslanguages\\n\" + \\\n\t\t\"R_LIBSILENT=1\\n\" + \\\n\t\t\"R_SYMLINK_PATH=\\\"${"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tenv = code.find('R.init')[0]\n\tsetup = code.find('R.setup')[0]\n\n\tcode.replace(\n\t\t(env, '-S', 'R_HIDE_SETVARS'),\n\t\t(setup, '-S', '-S', 'R.init -no-savefile'),\n\t)\n\n\tcode.replace(\n\t\t(env, '-p', 'R.param(c(\"CACHEFN\", NULL))', '-p', '\"-p\",\"CACHEFN\", NULL\"'),\n\t\t("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"{_set_env_cmd(\"RIBOSOME_START_ADDRESS\", code)}\n    ribosome::core::coreservice::set-option ribosome::ribosome::end {code}\n        ribosome::core::coreservice::set ribosome::ribosome::start {code}\"\"\"\n\n\ndef _inline_r_setval(\n    key: str,\n    val,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n) -> str:"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = f\"\"\"\n{code}\n\n# Set r.device._reset() = True to disable automatic cleaning\n# r(\"Device::device._reset(r(\"config\", \"device\")))\n\n# Disable the default \"R will not execute that code!\" message\nr(\"setDevice.config.device.allow-not-running-message-via-r\")\n\n# Use `device(\"foo\")` instead of `device(r(\"device\", \"foo\"))`\nr(\"setDevice.device.allow-other-input\")\n\n# Disable use of \"r-console: \""}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tif len(args) <= 0:\n\t\tprint \"ERROR: No arguments provided.\"\n\t\tdie()\n\t\t\n\tif color:\n\t\tfor arg in args:\n\t\t\tos.system('color')\n\t\t\tprint\n\t\t\tprint arg,\n\telse:\n\t\tfor arg in args:\n\t\t\tprint\n\t\t\tprint arg,\n\n\tprint\n\n\"\"\"\n\tPartitions command based on given arguments.\n\t\n\tname: Name of the command.\n\targs: Command-line arguments to run the command with.\n\"\"\"\ndef partition(name, args):\n\t# TODO:"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\timport os, subprocess, sys, platform\n\tcolor = True\n\tt_c = 15\n\n\tdef _cmd():\n\t\treturn \" \".join(sys.argv[1:])\n\tif color and sys.stdout.isatty():\n\t\tos.environ[\"TERM\"] = \"linux\"\n\telse:\n\t\tcolor = False\n\t\tdef _cmd():\n\t\t\treturn \" \".join(sys.argv[1:1]).encode(sys.stdin.encoding or \"utf8\")\n\n\tr = int(os.environ.get"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor = False\n\ttarget_concurrency = 10\n\t\n\t\n\tif p_p.pty in (p_p_int.B_TTY_NO, p_p_int.B_NO):\n\t\tcolor = True\n\t\t\n\tif target_concurrency > p_p.concurrency:\n\t\ttarget_concurrency = p_p.concurrency\n\t\n\t\n\tdef _xargs_concurrency(\n\t\tp_process, p_processes, p_name=\"Process\"\n\t\t):\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tif"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\targs = sys.argv[1:]\n\t# Try to be clever about a few special ones:\n\tif args[0] == 'color':\n\t\targs.pop(0)\n\t\tcolormode = args.pop(0)\n\t\targs.pop(0)\n\t\targs.pop(0)\n\t\tsys.exit('%s; xargs -P %d -F - color:%s' % (\n\t\t\tsys.argv[0],\n\t\t\tos.getpgrp(),\n\t\t\t'%s && set -e' % colormode,\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t__doc__ += color.doc(target_concurrency=\"<concurrent>\")\n\tcmd = ''\n\tconcurrency = []\n\targs = sys.argv[1:] or ['help']\n\tif args[0] == '--help':\n\t\tprint(__doc__)\n\t\treturn 0\n\n\tfor i, arg in enumerate(args):\n\t\tif i == 0 and arg == '--help':\n\t\t\targ = 'help'\n\t\tif i == 0 and arg == \"x\":\n\t\t\targ = \"exact\"\n\t\telif i == 0 and"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tdef run(argv):\n\t\tfrom posix import pipe\n\t\tfrom collections import deque, namedtuple\n\t\timport math\n\t\tprint('Argument vector %r' % argv)\n\t\targv = namedtuple('Argv', argv)(*argv)\n\t\tsep = '\\n'.join(\"=\".join((\"/\".join(x.split(\"//\")), str(int(x.count(\"=\") / 2)), str(argv), str(x.count(\"=\") % 2))) for x in argv.target.split(\"//"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\n\t# if the target_concurrency is more than the number of partitions\n\t# we need, we must use the pty\n\tif (target_concurrency > len(partitioned)):\n\t\tx_args = \"xargs\"\n\t\tp = subprocess.Popen([\"color\", \"??\"], stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n\t\tpartitioned = p.communicate(input)\n\t\tpartitioned = subprocess.Popen([\"sh\", \"-c\", (f\"\"\"p=1"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t# import\n\timport sys\n\timport os\n\timport signal\n\n\t# signal handling\n\tdef on_sigend(num, stack): exit(0)\n\tsignal.signal(signal.SIGQUIT, on_sigend)\n\n\n\t# default variables\n\tcolors = False\n\tcolor_arg = \"\"\n\n\t# check args\n\tif len(sys.argv) > 1 and sys.argv[1] == \"-color\" or len(sys.argv) > 2:\n\t\tcolors = True\n\t\tcolor_arg"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcmds = xargs.argparse(args, docstring=color.__doc__, target_concurrency=target_concurrency)\n\treturn cmds\n\nif __name__ == \"__main__\":\n\tif len(sys.argv) <= 1:\n\t\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\t\t\t\t\txargs.py: Simplified implementation for xargs.\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tcolor: Make a pty if on a platform that supports it\n\t\t\t\t\t\t\ttarget_concurrency: Target number of partitions to run concurrently\n\t\t\t\t\t\t\tinput: Command that would be run"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\timport sys, subprocess, errno, signal, fcntl\n\tcol = color = target_concurrency = None\n\tif sys.argv[-6]:\n\t\ttry:\n\t\t\tcol = sys.argv[-5]\n\t\t\ttarget_concurrency = int(sys.argv[-6])\n\t\texcept ValueError:\n\t\t\tif '-c' in sys.argv:\n\t\t\t\ttarget_concurrency = int(sys.argv[-1])\n\t\tif col in ['b', 'g', 'r', 'c', 'yc',"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\ta = range(len(seq))\n\trandom.shuffle(a)\n\treturn a #a\n\t\ndef _random(seq: Sequence[str], max_num: Optional[int] = None) -> list[str]:\n\"\"\"\n\tRandomly shuffle\n\t\"\"\"\n\t#print(max_num)\n\tr = range(len(seq))\n\tif max_num and max_num > len(seq):\n\t\t#print(f'max = {max_num}, num= {t}')\n\t\tr = _shuffled(seq)\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(seq) + random.shuffle(seq)\n\n\n\n\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(seq)\n\n\ndef _order(seq: Sequence[str]) -> list[str]:\n\treturn seq\n\n\ndef _cycle(seq: Sequence[str]) -> list[str]:\n\t\"\"\"Cycle order\"\"\"\n\treturn seq\n\n\ndef _random(seq: Sequence[str]) -> list[str]:\n\t\"\"\"Randomize order\"\"\"\n\treturn seq\n\n\nORDER_FUNCS = {}\n\nORDER_FUNCS[\"random\"] = _random\nORDER_FUNCS[\"order\"] = _order\nORDER_FUNCS[\"shuffle\"] = _"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\trand = random.Random()\n\trand.seed()\n\tret = [seq[i] for i in rand.sample(range(len(seq)), len(seq))]\n\treturn ret\n\"\"\"\n\tA sample of a function will always be the same,\n\texcept the sample being ordered randomly.\n\t\"\"\"\ndef _sample_ordered(seq):\n\"\"\"\n\tUnordered;\n\t\"\"\"\n\treturn seq\n\"\"\"\n\tA random sample of k elements in a list,\n\twith the order preserved.\n\t\"\"\"\ndef _random_sample(seq, k):"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tif len(seq) < 2:\n\t\treturn seq\n\tseq2 = list(seq)\n\trandom.shuffle(seq2)\n\tassert seq2!= seq\n\treturn seq2\n\n\n@dataclass\nclass DatasetMetrics:\n\tdataset_name: str\n\tdataset_samples: int\n\tdataset_size_ratio: int = None\n\tdataset_num_files: int = None\n\tfraction_train_samples: float = 1\n\tfraction_val_samples: float ="}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\trng = np.random.default_rng()\n\tlength = len(seq)\n\tpos1 = rng.integers(length, size=len(seq))\n\tpos2 = rng.integers(length, size=len(seq))\n\tc2 = permutation(seq)\n\tc1 = seq\n\tc = np.concatenate([c1, c2], axis=0)\n\tperm = np.array([pos1, pos2])\n\tc = c[perm, ]\n\treturn c\n\n\ndef _shuff"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tseq[:] = sorted(seq, key=_index)\n\tindexed = [(i, d) for (i, d) in enumerate(seq)]\n\trandom.shuffle(indexed)\n\tshuffled = []\n\tfor i, d in indexed:\n\t\tshuffled.append(d)\n\treturn shuffled\n\n\ndef _shuffle(batch: Sequence[Sequence[str]]) -> list[Sequence[Sequence[str]]]:\n\t\"\"\"\tOrdering sequences by length and mixing up positions.\"\"\"\n\tshuffled = []\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t\n\tseq = list(seq)\n\tn = len(seq)\n\tseq.sort()\n\tn = len(seq)\n\t\n\tfor i in range(0, n):\n\t\trand = i\n\t\tfor j in range(i + 1, n):\n\t\t\tr = rand\n\t\t\ts = seq[rand]\n\t\t\tseq[rand] = seq[r + 1]\n\t\t\tseq[r + 1] = s\n\t\t\n\t\tr = rand\n\t\ts = seq[rand + 1]\n\t\tseq[rand + 1] = seq[rand]\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\trandom.shuffle(seq)\n\treturn seq\n#\n\n\nclass Config:\n\tdef __init__(self, _path) -> None:\n\t\tself.load(_path)\n\t#\n\n\tdef load(self, path: str) -> None:\n\t\twith open(CONFIG_FILE, 'rt') as f:\n\t\t\treader = csv.reader(\n\t\t\t\tcsv.reader(\n\t\t\t\t\tcsv.reader(\n\t\t\t\t\t\tf, delimiter=',', quotechar='\"'\n\t\t\t\t\t), delimiter=',', quotechar='\"'\n\t\t\t\t),\n\t\t\t\tdelimiter"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tresult = list(seq)\n\tshuffle(result)\n\treturn result\n\n@_wraps(random_string, str)\ndef get_random_string(length: int, type: str = \"letters\", ascii_only: bool = False) -> str:\n\tresult = str()\n\tcharset = _alphabet + '_abcdefghijklmnopqrstuvwxyz0123456789'\n\tif type == 'lowercase':\n\t\tcharset = charset.lower()\n\telif type == 'upp"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tpairs = s.split('.', maxsplit=1)\n\tif 'rc' not in pairs[0].lower():\n\t\treturn int(pairs[0]), 0, 0\n\telse:\n\t\tif 'b' in pairs[0]:\n\t\t\tis_beta = True\n\t\t\tpairs[0] = _remove_b(pairs[0])\n\t\telse:\n\t\t\tis_beta = False\n\t\tpairs = pairs[1].split('.', maxsplit=1)\n\twhile len(pairs) == 1:\n\t\tpairs = pairs["}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tver = s.split('.')\n\ttry:\n\t\tl = int(ver[0])\n\t\th = int(ver[1])\n\t\tb = int(ver[2])\n\texcept:\n\t\traise ValueError\n\treturn l, h, b\n\n\ndef test() -> None:\n\t\"\"\"\n\ttesting\n\t\"\"\"\n\tif __name__ == '__main__':\n\t\tversion1 = '01.01.00'\n\t\tversion2 = '10.0.10'\n\t\tversion_str = f'v{version1:.0<"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn tuple(map(int, s.split(\".\")))\n\"\"\"\n\tpoor man's semantic_versioning\n\"\"\"\nfrom semantic_version import Version\n\ndef semantic_version(s: str) -> Version:\n\t_ = Version.coerce (parse_version(s))\n\treturn _\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif not isinstance(s, str):\n\t\traise TypeError\n\n\t# Strip whitespaces, convert to int\n\tres = tuple(map(int, filter(None, s.split('.'))))\n\tif len(res)!= 3:\n\t\traise Exception(\n\t\t\t\"Failed in parse_version({})!\".format(s)\n\t\t)\n\n\treturn res\n\ndef is_lesser_than(v1: tuple[int,...], v2: tuple[int,...]) -> bool:\n\t\"\"\"\n\tCompares both version\n\t\"\"\"\n\tif"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif isinstance(s, str):\n\t\ts = s.strip()\n\t\tif len(s) == 0:\n\t\t\traise StopIteration\n\tsplit = re.split('(\\d+\\.\\d+)', s, 1)\n\ttry:\n\t\tv = tuple(int(i) for i in split[0].split('.'))\n\texcept ValueError:\n\t\traise ValueError(f'Unknown: {s}')\n\treturn v\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn tuple(map(int, s.split(\".\")))\n\n\nfrom._version import __version__\nfrom.errors import *\n\nif sys.version_info < (3, 3, 2):\n\n\tdef fsenc(path: str) -> str:\n\t\t\"\"\"Returns the normalized, NFC normalized string value for `path`. This\n\t\tmethod handles Unicode encoding strings.\n\t\t\"\"\"\n\t\t# This is required because Python 3 3.2 allows Unicode file paths. By\n\t    # ensuring the strings are in the same Unicode format, we can preserve\n\t    # cross-platform behavior"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tpvs = [int(p) if str(p).isnumeric() else float(p) for p in s.split(\".\")]\n\treturn tuple(map(int, pvs))\n\n\ndef check_numeric(s: str) -> bool:\n\t\"\"\"\n\tpoor man's version checking\n\t\"\"\"\n\tprint(s, \" \", type(s))\n\tif str(type(s)).find(\"'\") > 0 or str(type(s)).find(\n\t\t' ') > 0:  # for now, assume that non-numeric values are"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif \",\" in s:\n\t\tv = [int(p) for p in s.split(\",\")]\n\t\treturn (*v, 0)\n        \n\tif \".\" in s:\n\t\treturn (int(s.split(\".\")[0]), *range(1, 3), 0)\n\n\treturn (*sys.version_info[:3], 0)\n\n\t\nif __name__ == \"__main__\":\n\tn = 4\n\t\n\tprint(\"{}\".format(n))\n\tprint((parse_version(n))))\n\n\tn = \"0."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tcomponents = s.split(\".\")\n\tn = 0\n\tparts = []\n\twhile n < len(components):\n\t\tparts.append(int(components[n]))\n\t\tn += 1\n\tif len(parts) == 1:\n\t\treturn parts[0], 1\n\telif len(parts) == 2:\n\t\tx, y = parts\n\t\tif x < y:\n\t\t\treturn *parts, 1\n\t\telif x > y:\n\t\t\treturn *parts, -1\n\t\telif x == y:\n\t\t\treturn x, 0\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif not s:\n\t\traise SyntaxError('version value does not contain a number')\n\tresult = tuple(map(int, s.split('.')))\n\treturn result\t"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# convert to tuple if not, since it's easy to forget.\n\tif isinstance(cmd, str):\n\t\tcmd = tuple(cmd)\n\t# make sure that cmd is a tuple\n\tif not isinstance(cmd, tuple):\n\t\traise TypeError(f\"command should be a tuple, not {type(cmd)}\")\n\t# ensure that we can preserve the first element:\n\t# the shebang. This is the \"magic\" line we just\n\t# added to enable the shebang support for everything\n\tshebang = (\n\t\t\"\\xef\\x"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tcmd = list(cmd)\n\targ_index = 0\n\twhile arg_index < len(cmd) and cmd[arg_index][0] in os.pathsep:\n\t\tcmd[arg_index] = (cmd[arg_index][0] + os.pathsep + cmd[arg_index][1:])\n\t\targ_index += 1\n\t\n\tcmd = tuple(cmd)\n\tnormalized_cmd = cmd[:]\n\t\n\t# For windows, add a shebang to the first command\n\t# Make sure your shebang matches the `"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tcmd = list(cmd)\n\tif os.name == \"nt\":\n\t\t# remove shebangs\n\t\tif cmd[0].startswith('#!'):\n\t\t\tcmd[0] = cmd[0][2:]\n\n\t\t# make deep-path shebangs work\n\t\tfor index, item in enumerate(cmd):\n\t\t\tif item.startswith('/') and not item.startswith('\\\\\\\\/'):\n\t\t\t\tnew_cmd = []\n\t\t\t\tfor sub_item in cmd[index:]:\n\t\t\t\t\t# skip files\n\t\t\t\t\tif os.path.isf"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif isinstance(cmd, list):\n\t\tcmd = tuple(cmd)\n\tif not os_path_isfile(cmd[0]):\n\t\tcmd = shlex.split(cmd[0]) + cmd[1:]\n\treturn cmd\n\ndef parse_shebang(path: str) -> str:\n\t\"\"\"\n\tParses the first #! command if it exists\n\t\"\"\"\n\tif path.endswith(\".exe\") or path.endswith(\".out\"):\n\t\tpath = path[:-4]\n\ttry:\n\t\twith Path"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\t# fix shebangs. Windows does not understand them\n\tif _is_using_shebang and (len(cmd) == 1 and not isinstance(cmd[0], str)):\n\t\tcmd = list(cmd)\n\t\tcmd.append('env')\n\t\t\n\t# the following is for windows only\n\tif is_windows() and not _is_using_shebang and (\n\t\t'env' in cmd and 'PYTHONUNBUFFERED' not in cmd and\n\t\tnot any('-u' in c and 'python' in c for c"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\tret = cmd\n\t\n\tif type(cmd) == type(\"hello world\"):\n\t\tret = shlex.split(cmd)\n\t\n\t# Make sure it's a tuple\n\tif type(ret) == type(()):\n\t\tif len(ret)!= 2:\n\t\t\tret = cmd + (\"\", )  # Make 2 args so Windows can run the command\n\t\n\treturn ret\n\n\ndef check_output_command_exists(cmd: str, command: bool = False) -> None or bool:\n\t\"\"\"\n\tChecks for an executable"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tcmd, shebang = cmd\n\n\t# fix for #8557, shebangs have no quotes. See:\n\t# https://unix.stackexchange.com/q/1636\n\t# https://docs.python.org/3/reference/lexical_analysis.html#shebang-strings\n\t# this was actually the case at the time I discovered this issue\n\tif is_on_windows():\n\t\tif '\"' in shebang and \"'\" not in cmd[0]:\n\t\t\t# we have a shebang which starts with a quote, as seen in"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tshebang = None\n\tassert len(cmd) == 2\n\tif cmd[0] == \"exec\":\n\t\tshebang = \"\"\n\telif cmd[1] is not None and isinstance(cmd[1], bytes):\n\t\tshebang = cmd[1]\n\telif get_shims().get(cmd[0], None) is not None:\n\t\tshebang = \"\\n\".join([get_shims().get(cmd[0], ''), cmd[1]]) if type(cmd[1]) == str else cmd[1]\n\telif platform"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# Remove empty first argument which is the script\n\tif cmd[0] == '':\n\t\tdel cmd[0]\n\t# Strip leading and trailing whitespace\n\twhile len(cmd) > 0 and cmd[0] == '':\n\t\tcmd.pop(0)\n\twhile len(cmd) > 0 and cmd[-1] == '':\n\t\tcmd.pop()\n\t# Convert 'PATH' prefix to '{PATH}'\n\ttmp = cmd[0]\n\tif prefix == '':\n\t\tprefix = os.environ.get('PATH')\n\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t_cmd = list()\n\t\n\t# https://stackoverflow.com/a/14222086\n\tfor i in range(0, 1 + len(cmd), 2):\n\t\t_cmd.append(''.join((\n\t\t\tsubprocess.list2cmdline([\n\t\t\t\t\n\t\t\t\t] + list(cmd[i : i + 2]) + (\n\t\t\t\t\t[' '] if pycompat.PY3 else\n\t\t\t\t\t[''+ os.linesep +'']\t\t\n\t\t\t\t)),\n\t\t\t'\\n'\n\t\t)))\n\treturn _cmd\n\n\ndef is"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorating_function(user_function,\n\t\t\tcache=cache, key=key, lock=lock):\n\t\tmessage = \"Notice: Using a memory cache with the caching decorator.\\n\\n\"\n\t\tmessage += \"The caching decorator doesn't yet support multi-processing or multi-GPU.\\n\\n\" \n\t\tmessage += \"You're welcome to fix this.\"\n\t\twarnings.warn('Using a memory cache with the caching decorator has not been tested\\n' + message)\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif lock is not None:\n\t\t\tlock.acquire()\n\t\ttry:\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\tfinally:\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\treturn wrapper\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tclass CacheItem(object):\n\t\t\tdef __init__(self, value, cache):\n\t\t\t\tself.value = value\n\t\t\t\tself.cache = cache\n\t\t\tdef __hash__(self):\n\t\t\t\treturn hashkey(self.value)\n\t\t\tdef __eq__(self, other):\n\t\t\t\treturn other and self.value == other.value\n\t\t\tdef __repr__(self):\n\t\t\t\treturn repr(self.value)\n\t\t\tdef __cmp__(self, other):\n\t\t\t\tif other == None:\n\t\t\t\t\treturn -1\n\t\t\t\treturn"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\t# \u52a0\u5165\u4fdd\u5b58\u7f13\u5b58\n\t\tcache[key] = {}\n\t\t\n\t\tif lock:\n\t\t\tdef memoizing_lock_decorator(func):\n\t# \u6240\u6709\u5176\u4ed6\u7684\u88c5\u9970\u5668\u90fd\u662f\u7c7b\u4f3c\u4e8e\u8fd9\u4e2a\u7684\u7c7b\n\t# \u5efa\u7acb\u4e86\u4e00\u4e2a\u7a0b\u5e8f\u5316\u7684\u52a0\u4e86\u9501\u7684\u88c5\u9970\ufffd"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorating_function(user_function, cache=cache, key=key, lock=lock):\n\t\twrapped_function = cache_wrapped(user_function, cache, key=key, lock=lock)\n\t\treturn update_wrapper(wrapped_function, user_function)\n\treturn decorating_function\n\n#@cached(cache='cache', key=lambda x: x)\n@timed_cache\ndef slow_add(a, b):\n\tif not hasattr(slow_add, \"_slow_cache\"):\n\t\tslow_add._"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\td = {}\n\t\th = hashkey\n\t\tcached_callable = decorator_call\n\t\tif lock not in (None, False):\n\t\t\tdef locked(arg,k,d = d):\n\t\t\t\tif not cache.acquire(k):\n\t\t\t\t\treturn False\n\t\t\t\ttry:\n\t\t\t\t\tif not d[k][0]:\n\t\t\t\t\t\td[k][0] = func(arg)\n\t\t\t\t\t\td[k][1] = k\n\t\t\t\t\t\treturn True\n\t\t\t\t\tif d[k][0] == func(arg):\n\t\t\t\t\t\td[k"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tcache_dict = cache() if callable(cache) else cache\n\t\tget = cache_dict.get\n\t\tput = cache_dict.put\n\t\thashed_key = haskey if callable(hashed_key) else lambda x: hashkey(x)\n\t\tdef memoized_func(*args, **kwargs):\n\t\t\tkey = hashed_key(args) + str(''.join(f'{kw:s}-{kwv:s}' for kw, kwv in sorted(kwargs.items())))"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrap(func):\n\t\tif cache is None:\n\t\t\tif lock==None:\n\t\t\t\tlock=threading.RLock()\n\t\t\tcache=WeakKeyDictionary()\n\t\t\tlock=RLock()\n\t\t\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key](func, *args, **kwargs)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key](func, *args, **"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decorated_function(*args, **kwds):\n\t\t\ttry:\n\t\t\t\tcache_key = key(*args, **kwds)\n\t\t\texcept TypeError:\n\t\t\t\t# We got an uncachable argument; forget about caching pre-2007.\n\t\t\t\treturn func(*args, **kwds)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tif cache_key in cache:\n\t\t\t\t\treturn cache[cache_key]\n\t\t\t\t# print 'calling decorated func:'\n\t\t\t\t# print func\n\t\t\t\tresult = func(*args, **"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tif cache is None:\n\t\t\tif lock is None:\n\t\t\t\tlock=_unspecified\n\t\t\tcache = LRU(capacity, lock)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tif key is None:\n\t\t\t\tkey = arg2key(args, kwds)\n\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tvalue = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = cache[key] = user_function(*args, **kwds)\n\n\t\t\treturn value\n\n\t\tdef invalidate():\n\t\t\t"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef memoized(func):\n\t\tcached_attr = '__cached_' + func.__name__ if not typed else '__cached_{}_{}'.format(func.__name__, re.sub(r'[\\s.]+', '_', str.lower(str(func))))\n\t\ttime_attr = func.__name__ + '.times_called'\n\t\tcache = {} if maxsize == 0 else OrderedDict()\n\n\t\tif getattr(func, time_attr, 0) < ttl:\n\t\t\t# No point in caching if"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef ttl_cache_decorator(func):\n\t\tmru = lru_cache(maxsize=maxsize, ttl=ttl, timer=timer, func_ref=func, typed=typed)\n\t\t# mru.__name__ = func.__name__\n\t\t# mru.__qualname__ = func.__qualname__\n\t\t# mru.__module__ = func.__module__\n\t\tmru = functools.update_wrapper(mru, func)\n\t\treturn mru\n\treturn ttl_cache_dec"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tif not isinstance(maxsize, int):\n\t\traise TypeError(\"'maxsize' must be 'int' instead of '%s'\" %\n\t\t\t\t\t\t(type(maxsize).__name__))\n\n\tif maxsize < 0:\n\t\traise ValueError(\"'maxsize' must be zero or greater\")\n\n\tif ttl <= 0:\n\t\traise ValueError(\"'ttl' must be greater than zero\")\n\n\tdef decorator(func):\n\t\tkey = \"%s_%s_%s_%s\" % (\n\t\t\tfunc.__module__, func."}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t_NO_DEFAULT = object()\n\n\tdef decorating_function(user_function):\n\n\t\tcache = dict()\n\t\tsentinel = object()\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn f'{len(cache)}/{maxsize} Bytes'\n\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\t\tnonlocal cache_hits, cache_misses, cache_invalids, misses_left\n\t\t\tcache_hits = cache_invalids = cache_misses"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tif not hasattr(typed, 'cache'):\n\t\ttyped.cache = {}\n\n\tdef _lru_cache_aux(kwd):\n\t\tkwd = tokey(kwd)\n\n\t\ttry:\n\t\t\ts = typed.cache.get(kwd)\n\t\t\tif maxsize and len(typed.cache) >= maxsize:\n\t\t\t\t# cache reached maximum capacity; remove least recently used\n\t\t\t\tif s:\n\t\t\t\t\tif isinstance(s[0], str):\n\t\t\t\t\t\tdel typed.cache[kwd]\n\t\t\t\t\telse:\n\t\t\t\t\t\tdel typed.cache"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef lru_ttl_cache(func):\n\t\tLRU_klass = ttl_cache_node if typed else ttl_cache_non_typed # use dict to enable TTLs of non-hashable types.\n\t\tcache = LRU_klass(maxsize, ttl, timer)\n\n\t\t@wraps(func)\n\t\tdef cached_func(*args, **kwargs):\n\n\t\t\t# If none of the arguments are hashable, don't bother checking or saving anything in the cache.\n\t\t\tif any(isinstance(arg, Hashable) for arg"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\n\t\tif inspect.iscoroutinefunction(function):\n\n\t\t\t@functools.wraps(function)\n\t\t\tasync def async_cache_decorator(*args, **kwargs):\n\t\t\t\treturn await cache(function, *args, **kwargs)\n\n\t\t\treturn async_cache_decorator\n\n\t\t@functools.wraps(function)\n\t\tdef cache_decorator(*args, **kwargs):\n\t\t\treturn cache(function, *args"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = FastDict if typed else SmartDict\n\t\tttl = maxsize if callable(maxsize) else (lambda n: maxsize)\n\t\tget = cache.get\n\t\tif callable(ttl):\n\t\t\tp = '%s(%s ms per elem)'\n\t\telse:\n\t\t\tp = '%s per elem'\n\t\tp = callable(ttl) and p or '.%d'\n\t\tdef wrapper(data=None, *args, **kwargs):\n\t\t\tkey = repr(data)"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = TTLCache(maxsize,ttl,timer,typed)\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args,**kwargs):\n\t\t\timport time\n\t\t\tkey = args if kwargs=={} else (args,kwargs)\n\t\t\tcached,info=cache.get(*key,key)\n\t\t\tif cached:\n\t\t\t\tttu=True\n\t\t\t\tcache.cache_update(info,ttu)\n\t\t\telse:\n\t\t\t\tttu=False\n\t\t\treturn func(*args,**"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = {} if typed else WeakKeyDictionary()\n\t\tTTL = TTLCache(maxsize, ttl, timer, lambda k: cache.pop(k))\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkw = cache.get(args)\n\t\t\tif kw is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tcache[args] = result\n\t\t\t\texcept ValueError:\n\t\t\t\t\tmsg = 'Impossible to cache %s."}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize is None:\n\t\treturn tlru_cache(None)\n\n\tdef real_cache(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = pickle.dumps((args, kwargs))\n\t\t\twith cache_lock:\n\t\t\t\tmru_pos = MRU_pos\n\t\t\t\tif mru_pos < maxsize and (cache.get(key)!= None or key in LRU_cache):\n\t\t\t\t\tcache[key] = cache[mru_pos]\n\t\t\t\t\tcache"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = {} if maxsize is None else OrderedDict()\n\n\t\thits = misses = 0\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tnonlocal hits, misses\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tresult = cache.get(key, None)\n\t\t\tif result is not None:\n\t\t\t\thits += 1\n\t\t\telse:\n\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tmisses += 1\n\n\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize <= 0:\n\t    return counter\n\tcache = collections.OrderedDict()\n\n\tdef decorating_function(user_function, tuple=tuple, sorted=sorted, len=len, KeyError=KeyError):\n\t\tif sys.version_info[:2] >= (3, 4):\n\t\t    key_func = lambda x: x[0]\n\t\telif sys.version_info[:2] < (2, 5):\n\t\t    from backports.functools_lru_cache import lru_cache\n\t\t    "}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tcache = dict()\n\t\tuse_counts = dict()\n\n\t\tdef lru_cache_info():\n\t\t\t\"\"\"\n\t\t\tthis is called in ``__wrapped__`` to make info on the wrapper\n\t\t\t\"\"\"\n\t\t\tdef id_generator():\n\t\t\t\tfor i in range(ids):\n\t\t\t\t\tyield None, None\n\n\t\t\tif '__wrapped__' in cache:\n\t\t\t\tyield cache['__wrapped__'], cache['_cache'], cache['_use_counts']\n\n\t\t\tyield func.__module__, func,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef outer_func(func):\n\t\tsentinel = object()\n\t\tcache = {}\n\t\tmru_order = False\n\t\tif issubclass(type(maxsize), int):\n\t\t\t# We want lookup to give an error \n\t\t\t# under 2.7; there is no Ordered Dict \n\t\t\tfrom collections import OrderedDict\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tif not mru_order:\n\t\t\t\t\tcache.update(OrderedDict((k, cache[k]) for k in cache.keys()[maxsize:]))\n\t\t\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef mrgu(user_function):\n\t\tcache = dict()\n\t\thits, misses = 0, 0\n\t\tget_mru = lambda: cache.get(hashkey, None)\n\t\tput_mru = lambda key, value: cache.__setitem__(hashkey, value)\n\t\tkey_ref = tuple(str(key) for key in sorted(cache.keys()))\n\t\tcache_len = len(cache)\n\t\t_hashes = map(hash, key_ref)\n\t\t_hash = lambda key: _hashes[cache_len"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\n\t    if maxsize is None:\n\t    \n\t        _maxsize = 2048\n\t    \n\t    else:\n\t        _maxsize = maxsize\n\t    \n\t    if not __debug__:\n\t        # When unpickling these functions, the containing class will be\n\t        # uninitialized because it did not have a reference to the\n\t        # logging module before the interpreter pickled it. Save a reference\n\t        # to it here so that it will be correctly restored.\n\t        _log"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize is None:\n\t\treturn _mru_cache_default\n\telse:\n\t\tdef opt_lru_cache_wrapper(\n\t\t\toriginal_function,\n\t\t\tmax_size=128,\n\t\t\tmax_unused_size=128,\n\t\t\tttl_size=128,\n\t\t\tttl_num=2,\n\t\t\tis_static=False,\n\t\t):\n\t\t\tif is_static:\n\t\t\t\treturn _StaticMRUCache(\n\t\t\t\t\toriginal_function,\n\t\t\t\t\tmaxsize=maxsize,\n\t\t\t\t\tmax_unused_size=max_un"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef mru_cache_wrapper(func):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\tcache_get_count, cache_put_count, cache_incr_count = 0, 0, 0\n\n\t\tdef mru_cache_redis(value, key):\n\t\t\t# get from cache\n\t\t\tcached_value = None\n\t\t\tif cache_get_count > maxsize - 2:\n\t\t\t\t# redis.delete(cache_key[i])\n\t\t\t\treturn []\n\t\t\tif func.__name__ in cache.history and len"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized_cache(func):\n\t\tsentinel = object()  # marker for cache_get\n\t\tcache = {}  # mapping of arguments to results\n\t\tuse_count = Counter()  # times each argument was used\n\n\t\tdef cache_get(arg):\n\t\t\t\"\"\"\n\t\t\tGet cache entry or `sentinel` if not found. Doesn't\n\t\t\tcheck whether entry is a cache hit or a miss. \"Use\"\n\t\t\tis any time someone calls cache_get even if a previously\n\t\t\tcached value was retrieved. A miss is different from a\n\t\t\tcache hit in two ways"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize is None: maxsize = 128\n\t\n\tdef decorator(function):\n\t\tkeyfunc = operator.attrgetter('key')\n\t\tcache = {}\n\t\tacache = cache.__setitem__ = _fast_cache_wrapper_setdefault\n\t\tucache = cache.__delitem__ = _fast_cache_wrapper_delitem\n\t\tucache = cache.pop = _fast_cache_wrapper_pop\n\t\tucache = cache.popitem = _fast_cache_wrapper_popitem\n\t\tauc = cache.update = _fast_cache"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif wraps:\n\t\tfunc.__wrapped__ = wrapped_function\n\t\tfunc.__name__ = wrapped_function.__name__\n\t\tfunc.__doc__ = wrapped_function.__doc__\n\tif maxsize is None:\n\t\treturn lru_cache()(wrapped_function)\n\tparameters = inspect.signature(wrapped_function).parameters.keys()\n\tcache_get, cache_set, cache_del = (\n\t\tfunctools.lru_cache(maxsize=maxsize, typed=typed)(\n\t\t\tdecor"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tkeyfunc = keys_name(maxsize, typed, func)\n\t\twrapper = lrucachefunc(func, maxsize, keyfunc)\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorator\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif isinstance(maxsize, int):\n\t\tif maxsize < 0:\n\t\t\tmaxsize = 128\n\telif isinstance(maxsize, float) and maxsize < 1:\n\t\tmaxsize = 128\n\telif not isinstance(maxsize, int):\n\t\traise ValueError('Expected maxsize: integer or float but got: %s' %\n\t\t\t\t type(maxsize))\n\n\tif isinstance(typed, basestring):\n\t\traise ValueError(\"Decorator cannot have a typed argument (you should use keywords instead).\")\n\n\tif not call"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif isinstance(maxsize, int):\n\t\told_max = maxsize\n\t\tmaxsize = None\n\telse:\n\t\told_max = None\n\tdef memoize(func, maxsize=maxsize, *args, **kw):\n\t\tkey = (func, maxsize, args[1:])\n\t\tcache = {}\n\t\tsentinel = object()\n\t\troot = Node(key, None, root, sentinel)\n\t\ttail = root\n\t\tcounts = Counter()\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize is None:\n\t\treturn lru_cache(2**30)\n\n\tdef decorating_function(func):\n\t\tcache = {}\n\t\tcall_counter = Counter()\n\n\t\t@functools.wraps(func)\n\t\tdef tool(*func_args, **func_kwargs):\n\t\t\tif func_args or func_kwargs:\n\t\t\t\tkey = cache, call_counter, func_args, func_kwargs\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = func(*func_args, **func_kwargs)\n\t\t\t\tcall"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = TTLCache(maxsize, ttl,\n\t\t\t\t\t\t True if typed else False)\n\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# First we check our cache\n\t\t\tcache_result = cache.get(key, cache.is_alive)\n\t\t\tif cache_result is not False:\n\t\t\t\t# The key is in the cache\n\t\t\t\treturn cache_result\n\n\t\t\t# Add key to cache\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache.add(key, result,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif isinstance(maxsize, int):\n\t\tdef decorating_function(user_function):\n\t\t\tcache = dict()\n\t\t\thits = collections.deque(maxlen=maxsize)\n\n\t\t\t@functools.wraps(user_function)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tkey = make_key(args, kwargs)\n\t\t\t\tresult = None\n\t\t\t\tif key in cache:\n\t\t\t\t\thits.remove(key)\n\t\t\t\t\thits.append(key)\n\t\t\t\telse:\n\t\t\t\t\tresult = user_function(*args, **kw"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tif maxsize is None: name = user_function.__name__\n\t\telif maxsize == True: name = '_lru_' + get_cache_name(func=user_function, maxsize=2000)\n\t\telse: name = '_lru_' + get_cache_name(func=user_function)\n\t\tif name not in globals():\n\t\t\tglobals()[name], __dict__[name] = LRUCache(maxsize=,cache_name=name, typed="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function, maxsize=maxsize, typed=False):\n\t\tstats = {}\n\t\tHITS, MISSES = 0, 1\n\t\tif maxsize is None:\n\t\t\tufunc = user_function\n\t\telse:\n\t\t\tcache = collections.OrderedDict()\n\t\t\tstats['hits'] = misses = 0\n\t\t\tfull = object()\n\t\t\tdef wrapper(key):\n\t\t\t\tnonlocal stats, misses\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = cache[key] = user_function("}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tlfu_cache_wrapper = LFUCache(maxsize, typed)\n\n\tdef decorator(user_function):\n\t\tdef wrapped(*args, **kwds):\n\t\t\treturn lfu_cache_wrapper(user_function, *args, **kwds)\n\n\t\treturn wrapped\n\n\treturn decorator\n\n\nclass LFUCache(object):\n\t\"\"\"\n\tSimple LRU Cache implementation\n\t\"\"\"\n\n\tdef __init__(self, maxsize, typed=False):\n\t\t\"\"\"\n\t\t`maxsize`\n\t\t\tThe maximum size of the cache. When the cache\n\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\n\n\n\t\tcache = collections.OrderedDict()\n\n\t\tdef wrapper(*args):\n\t\t\tcache_key = args\n\n\t\t\tif kwargs:\n\t\t\t\tcache_key += (kwargs,)\n\n\t\t\tif cache_key in cache:\n\t\t\t\treturn cache[cache_key]\n\t\t\telse:\n\t\t\t\tresult = user_function(*args)\n\t\t\t\tcache[cache_key] = result \n\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\t\n\t\t\t\t\tcache.popitem(False)\n\n\n\t\t\t\treturn result\n\n\t\treturn wrapper\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timed, typefunc=typefunc)\n\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tparameter = str(parameternumber)\n\t\t\tif parameter in cache:\n\t\t\t\t# This key does exist in the cache. If we want to get the list of \n\t\t\t# items we need to call the cache, the cache must have the items\n\t\t\t\tlistOfKeys = listOfKeysOfList(cache[param"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize is None:\n\t\treturn _lfu_cache\n\telse:\n\t\tdef wrapper(func):\n\t\t\t_lfu_cache = _make_lfu_cache(func, maxsize, typed,\n\t\t\t\t_fill_cache_by_name_for_cache)\n\t\t\treturn _lfu_cache\n\t\treturn wrapper\n\ndef lfu_cache_json(maxsize=128, typed=False):\n\t\"\"\"Same as lfu_cache, but when the key-value pair stored is a JSON object / dict.\n\tIf JSON object and string data are"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized_lru(maxsize=128, typed=False):\n\n\t\tmemo = {}\n\n\t\tdef decorating_function(f, maxsize=maxsize, typed=typed, *args, **kw):\n\t\t\tkey = key_func(f, *args, **kw) if typed else f\n\t\t\t# get stored value\n\t\t\tval = memo.get(key, None)\n\t\t\t# if val exist\n\t\t\tif val is not None:\n\t\t\t\t# if hit, return val\n\t\t\t\tif len(memo) <= maxsize:\n\t\t\t\t\treturn val\n\t\t\t\t#"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = {}\n\t\thits = misses = 0\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += (kwds,)\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\t\thits += 1\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\t\tmisses += 1\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tcache.pop(_lowest_key(cache))\n\t\t\treturn result"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized_LFUCache(fn):\n\t\tcache = LFUCache(maxsize)\n\t\tdef lfu_cache_call(*args, **kwds):\n\t\t\tkwds = decode_bytes(args, kwds) if kwds else {}\n\n\t\t\tkey = encode_bytes(args, kwds)\n\t\t\tif key not in cache.keys:\n\t\t\t\tcache[key] = fn(*args, **kwds)\n\n\t\t\treturn cache[key]\n\t\tlfu_cache_call.uncache = lambda: cache.clear()\n\t\treturn lfu"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif isinstance(maxsize, int):\n\t\t# @lfu_cache(128)\n\t\tdef wrapper(func):\n\t\t\t@functools.lru_cache(maxsize=maxsize, typed=typed)\n\t\t\tdef wrapped(*args, **kwargs):\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapped\n\telif callable(maxsize):\n\t\t# @lfu_cache(128.50)\n\t\tdef wrapper(func):\n\t\t\t@functools.lru_cache(maxsize=maxsize)\n\t\t\tdef wrapped"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\n\t\tcache = dict()\n\t\tcall_counter = Counter()\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tresult = cache.get(key, None)\n\t\t\tif result is not None:\n\t\t\t\tcall_counter[key] -= 1\n\t\t\t\tif call_counter[key] <= 0:\n\t\t\t\t\tcache.pop(key)\n\t\t\t\t\tcall_counter.pop(key, None)\n\t\t\tif len(cache) > maxsize:\n\t\t\t\tcache."}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = collections.OrderedDict()\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# Use inspect.getcallargs to parse arguments passed to the function\n\t\t\t# It'll be a big time saver\n\t\t\tcall_args = inspect.getcallargs(func, *args, **kwargs)\n\t\t\tk, v = tuple(call_args.items()), (func, call_args, args, kwargs)\n\n\t\t\tf, args, kw, caller = v\n\t\t\ttry:\n\t\t\t\tvalue, hits = cache["}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t# remove\n\tdel self.queue[0]\n\"\"\"\n\tRemove item `x` from `self`.\n\t\"\"\"\n\t# Remove\n\tself.queue.remove(x)\n\treturn x\n\t\ndef delete(self):\n\"\"\"\n\tDelete the queue.\n\t\"\"\"\n\tself.queue.clear()\n\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self._key_to_item is None:\n\t\traise Exception(\"Dict is empty\")\n\treturn self.pop(self._key_to_item[0])\n\n\n\n\tdef pop(self, key,default = None):\n\t\"\"\"\n\tIf `key` is in the Dict return its value if `key` is found, else return `<default>.\n\t\"\"\"\n\tif self._key_to_item is None:\n\t\traise Exception(\"Dict is empty\")\n\tif key not in self._key_to_item:\n\t\treturn default\n\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self.peek():\n\t\tkey = self.peekItem()\n\t\tself.remove()\n\t\treturn key, self.__dict__[key]\n\"\"\"\n\n\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif random_state_property_object_.is_set:\n\t\traise DsaNotTracked()\n\t(key, value), self.data = self.data, {}\n\treturn key, value\n\ndef remove(self, key):\n\t\"\"\"\n\tRemove and return the `key` itself.\n\t\"\"\"\n\tif random_state_property_object_.is_set:\n\t\traise DsaNotTracked()\n\tself.data.pop(key)\n\ndef moves(self):\n\treturn self.data\n\ndef __reduce__(self):\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif len(self._data) == 0:\n\t\traise Exception(\"empty\")\n\tlastKey = self._data_list[-1][0]\n\treturn (lastKey, self._data.pop(lastKey))\n\n\nclass Stack:\n\t\n\tdef __init__(self):\n\t\tself._data = DoublyLinkedList()\n\t\n\tdef size(self):\n\t\treturn self._data.size()\n\t\n\tdef is_empty(self):\n\t\treturn self._data.empty()\n\t\t\n\tdef head(self):\n\t\tif not"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif not size:\n\t\traise KeyError\n\treturn self._heap.heappop()\n\n\t\n# @Time    :    2021/5/1\n# @Author  :   Zhang Hui\n# @File    :   merge_sort.py\n# @Software:   PyCharm\nif __name__ == \"__main__\":\n\ta = [10, 5, 20, 22, 22, 5, 6, 7, 8, 23]\n\tprint(a)\n\tmerge_sort(a)\n\tprint(a)\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tdef __delitem__(self, key):\n\"\"\"\n\tRemove entry *key* from the `(key, value)` mappings.\n\n\t:raises: KeyError: if `key` in not `key` in the map\n\t\"\"\"\ndef keys(self):\n\"\"\"\n\tReturn an iterable of all keys in the cache\n \"\"\"\ndef make_key(key, version=None, prefix=None):\n\"\"\"\n\tCreate a cache key from to a key `key`\n\n\tIf `version` is not provided, it defaults to all the instance\n\targuments"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\treturn self.__heapQueue.pop()\n\texcept IndexError:\n\t\traise HANDLEBUG()\n\ndef heappush(heap, data):\n\t\"\"\"\n\tPush `data` to heap.\n\n\tIn case the data is bigger than the last item,\n\tthe size of the heap is updated using `len(heap)`.\n\n\tIn case the size of the heap (last item) is larger than the\n\theap size, the last item is replaced.\n\n\tWhen heap is full, the item pushed the `len(heap)+"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tpass\n\ndef peekitem(self):\n\t\"\"\"\n\tReturn (key, value) items without removing/discarding them.\n\t\"\"\"\n\tpass\n\ndef remove(self,key):\n\t\"\"\"\n\tRemove an item using `key` value.\n\t\"\"\"\n\tself.pending_removals = []\n\tif self.get(key) is not None:\n\t\tself.data.remove(key)\n\ndef pop(self,key):\n\t\"\"\"\n\tRemove an item using `key` value.\n\t\"\"\"\n\ndef setdefault(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self.len == 0:\n\t\traise KeyError()\n\tkey, val = list(self.items())[0]\n\tdel self[key]\n\treturn key, val\n\ndef iteritems(self, iterative=False, reverse=False):\n\t\"\"\"\n\tIterate through all `(key, value)` pairs ordered according to `key`.\n\t\n\t\tkeys_list = list(itervalues(my_dict.items()))\n\t\"\"\"\n\tif reverse:\n\t\tfor key in reversed(sorted(self.keys())):\n\t\t\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn D.setdefault(key, default)\n\t\ndef iteritems(obj, (*chr, **xpla):\n\t\"\"\"\n\titeritems(S[,chr], xpla, **xpla) -> an iterator over the (key, value) items of S, in\n\tthe order of the keys by default; (key, value) items can be retrieved by\n\titeritems(d, (key, default) **kwargs) or items((key, value) **kwargs)\n\t\"\"\"\n\treturn obj.iteritems(*chr, **"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tresult = D.__dict__.get(key)\n\tif result == None:\n\t\tresult = D.__dict__[key] = default\n\treturn result\n\ndef setdefaultn(n, default=None):\n\t\"\"\"\n\tn is the default value, default is returned if absent\n\t\"\"\"\n\treturn D.get(n, D.setdefault(n, default))\n\n\nclass NoDotLookup:\n\t\"\"\"\n\tRaises an exception - we can't do this:\n\t\tf = open(foo.txt, 'w')\n\t\tf"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tpass\ndef setentry(self, i, key, value):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tpass\ndef setdefaultkv(self, k, v):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tpass\ndef setdefaultkv(self, k, v):\n\"\"\"\n\tD."}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs): pass\n\tdef __len__(self, *args, **kwargs): return 0\n\tdef __setitem__(self, *args, **kwargs): return\n\tdef __delitem__(self, *args, **kwargs): return\n\n\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tpass\ndef setlistdefault(self, key, default_list = None):\n\"\"\"\n\tD.setlistdefault(k,[v]) -> D.getlist(k) or D.setlist(k, [v])\n\t\"\"\"\n\tpass\ndef setrecursionlimit(self, num):\n\"\"\"\n\tRaise the maximum string recursion depth.\n\tThis does not prevent recursion from occurring, it merely sets the maximum limit.\n\t\"\"\"\n\tpass\ndef u(self, name):\n\"\"\"\n\tUppercase the string `name`.\n\t\"\"\""}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t_checkForRecursion(self, stack)\n\treturn dictlib.setdefault(self, key, default)\n\n# for compat:\n\n\t_get = get\n\t_keys = keys\n\t_pop = pop\n\t_copy = copy\n\n_maketrans = _trans_table\n_get_get = lambda m: m.__getitem__\n_has_key = lambda d, k: k in d\n_contains = lambda e, k: k in e\n_len = len\n\ndef getkey(obj, key):\n\treturn get"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn _Method(2, key=key, kwarg_count=2)\n\n\ndef update(self, *args, **kwargs):\n\treturn self.__class__(self * (args or kwargs.items()))\n\n\ndef values(self):\n\treturn (x for x in self.items())\n\n\ndef zip(self):\n\treturn ((k, v) for k, v in enumerate(\n\t\ttuple(zip(*(tuple(item) for item in iteritems(self)))),\n\t\tstart=self.get(\"get\", None"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef setdefault(self, *args, **kwargs):\n\t\"\"\"\n\t\t.setdefault(key[, default]) -> value, also set if key not in D\n\t\t\"\"\"\n\tdef items(self):\n\t\"\"\"\n\t\t.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\t\"\"\"\n\tdef keys(self):\n\t\"\"\"\n\t\t.keys() -> a new view of D's keys\n\t\t\"\"\"\n\tdef values(self):\n\t\"\"\"\n\t\t.values() -> a new view of D's values\n\t\t\"\"\"\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tpass\ndef setattr(object, name, value, *args, **kwargs):\n\t\"\"\"\n\tSed.setattr(s,n,v)\n_setattr(object,name,value)\t# equivalent to the second invocation\n_setattr(object,name,value,flags)\t# flags argument (see below)\n\t\"\"\"\n\tpass\ndef setattr_global(name, value=None, *args, **kwargs):\n\t\"\"\"\n\tAtomically set the global attribute or one of its indexed element(s) equal a new value.\n\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef append(self, item):\n\"\"\"\n\tL.append(item) -- append item to the end of the list\n\t\"\"\"\n\tdef clear(self):\n\"\"\"\n\tL.clear() -- clear all items\n\t\"\"\"\n\tdef copy(self):\n\"\"\"\n\tL.copy() -- return a shallow copy of the list\n\t\"\"\"\n\tdef count(self, item):\n\"\"\"\n\tL.count(item) -- return number of occurrences of item\n\t\"\"\"\n\tdef extend(self, iterable):\n\"\"\"\n\tL.extend(iterable)"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tpass\ndef has_value(self, value):\n\"\"\"\n\tD.has_value(value) -> bool\n\t\"\"\"\n\tpass\ndef items(self, *args, **kw):\n\"\"\"\n\tD.items([filt]) -> an iterator over the (key,value) items of D;\n\nif filt is not None, it must be a callable accepting a single argument and\nreturning either True or False when invoked with each key's value.\n\"\"\"\n\tpass\ndef iteritems(self, *args, **kw):\n\"\"\"\n\tD.iter"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tpass\ndef getattr(self, name, *args, **kwargs):\n\"\"\"\n\tD.getattr(self, name) -> value/items/any(), equivalent to the Python method getattr(self, name)\n\"\"\"\n\tpass\ndef getcwd(self):\n\"\"\"\n\tCurrent the working directory\n\"\"\"\n\tpass\ndef getcwdb(self):\n\"\"\"\n\tD.getcwdb() -> a string\n\n\n\n\"\"\"\n\tpass\ndef getdoc(self):\n\"\"\"\n\tD.getdoc() -> documentation string for this object\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self.data[key]\n\texcept KeyError:\n\t\treturn default\n\n\n\n\"\"\"\n\t>>> a = MappingProxyType({1:2,3:4})\n\t>>> a[1]\n\t2\n\"\"\"\n\tdef __getitem__(self, key):\n\t\tif (key in object):\n\t\t\treturn object.__getattr__(self, key)\n\t\telse:\n\t\t\treturn self.get(key, None)\n\n\n\t\"\"\"\n\t>>> sorted(a.keys())\n\t[1, 3]\n\"\"\"\n\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self.data[key]\n\texcept LookupError:\n\t\treturn default\n\t\ndef setdefault(self, key, default=None):\n\"\"\"\tSets D[key] to value if key is in D, else sets D[key]=default.  Returns\ndefault if key not in D.\n\t\"\"\"\n\tif key in self.data:\n\t\treturn self.data[key]\n\telse:\n\t\tself.data[key] = default\n\n# https://docs.python.org/2/library/threading.html#locked"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn property(operator.attrgetter('_m_' + key), doc=_doc_meth(operator.attrgetter('_m_' + key), \"Get Member '{}' as {!s}\"\\\n.format(key, klass)))\n\n#end\n\nclass Module(object):\n\t_fields_ = [\n\t(\"__loader__\",                 PY_LOADERS),\n\t(\"__name__\",                   None),\n\t(\"__package__\",                None),\n\t(\"(Frozen)__spec__\",           {}),\n\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\ndef getattr_recursive(obj, attr):\n\t\"\"\"\n\tRecurses through an attribute chain to get the ultimate value.\n\t\"\"\"\n\tfor item in attr.split(\".\"):\n\t\tobj = getattr(obj, item)\n\treturn obj\n\ndef convert(obj, str_to_bytes):\n\t\"\"\"\n\tConvert all string values to bytes in list object.\n\t\"\"\"\n\tif isinstance(obj, list):\n\t\tfor i in range"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tpass\ndef get(self, key, default=None):\n\t\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\n\tpass\ndef has_key(self, key):\n\t\"\"\"D.has_key(k) -> True if D has a key k, else False.\"\"\"\n\tpass\ndef items(self):\n\t\"\"\"D.items() -> a set-like object providing a view on D's items.\"\"\"\n\tpass\ndef iterkeys(self):\n\t\"\"\"D.iter"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tpass\n\ndef getattr(obj, name, *args):\n\t\"\"\"\n\tReturn the value of the named attribute on the given object;\n\tif such attribute doesn't exist, or isn't readable through\n\tthe normal system attribute lookup logic, an exception is raised.\n\targs is a list of arguments which are passed directly to\n\t__getattr__().\n\t\"\"\"\n\tpass\n\ndef getattribute(obj, name):\n\ttry:\n\t\treturn getattr(obj, name)\n\texcept BaseException:\n\t\tpass\n\ttry:\n\t\treturn getattr("}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn None\ndef keys(self):\n\"\"\"\n\tD.keys() -> a set-like object providing a view on D's keys\n\"\"\"\n\treturn []\ndef items():\n\"\"\"\n\tD.items() -> an iterator over the (key, value) items of D\n\"\"\""}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tpass\ndef has_key(self, key):\n\"\"\"\nReturn True if key is in the dictionary.\n\"\"\"\n\tpass\ndef items(self):\n\"\"\"\nod.items() -> an object providing a view on od's items\n\nthe order of items in the output of od.items() is not defined,\nso not for regular use: since CPython does not implement it,\ncheck od.items() for the order.\n\"\"\"\n\tpass\ndef iteritems(self):\n\"\"\"\nod.iteritems -> an object providing an iterator over the\n(key, value) items"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif (isinstance(cache, dict)):\n\t\tif (lock is None):\n\t\t\tlock = RLock()\n\t\tdef wrapper(self, *a, **kw):\n\t\t\tkey = keyfunc(*a, **kw)\n\t\t\twith lock:\n\t\t\t\t#print 'cachedmethod lookup key (%s) in cache (size = %s)' % (saferepr(key), len(cache))\n\t\t\t\tif key in cache:\n\t\t\t\t\treturn cache[key]\n\t\t\t\tmethod = func\n\t\t\t\tif hasattr(method, 'im_self') and (method.im_self is"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tcache = {} if not cache else cache\n\n\tdef memoized_method(method):\n\t\t\n\t\tdef newmethod(*args, **kwargs):\n\t\t\tkey = key_maker(args, kwargs)\n\t\t\tnew_key = key\n\t\t\twith lock\n\t\t\tcached_value = cache.get(key, None)\n\t\t\tif cached_value:\n\t\t\t\t#return cached_value\n\t\t\t\treturn cached_value\n\t\t\telse:\n\t\t\t\t#return method(*args, **kwargs)\n\t\t\t\tcache[key] = result = method(*args, **kwargs)\n\t\t\t\treturn"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorator(method):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = method(self, *args, **kwargs)\n\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\n\t\t\tdata = cache[key] = cache.get(key, None)\n\t\t\tif data is None:\n\t\t\t\treturn None\n\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\n\t\t\treturn data\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n# def format_objectpaths_by_category(objs, filter"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif not cache or not isinstance(cache, dict):\n\t\traise ValueError(\"Unhashable cache!\")\n\n\tdef func(self, *args, **kwargs):\n\t\tqueuekey = (key(self, *args, **kwargs))\n\n\t\t# If we have a memoized cache entry, return it and exit early\n\t\tif queuekey in cache:\n\t\t\treturn cache[queuekey]\n\t\t\n\t\t# If the desired cache does not exists, create it and return None\n\t\tif cache.get(key) is None:\n\t\t\tcache[key] = cachedmethod."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef __init__(self, method, cache=cache, key=key, lock=lock):\n\t\tself.method = method\n\t\tself.cache = cache\n\t\tself.key = key\n\t\tself.lock = lock or (lambda x: True)\n\n\tdef __get__(self, obj=None, type=None):\n\t\treturn self.method.__get__(obj, type)()\n\n\tdef __call__(self, *args, **kwargs):\n\t\tcached = self.key(self.method, *args, **kwargs)\n\t\twith"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tgetter, result_key = create_cache_getter(func, key, cache, lock)\n\t\t@wraps(func)\n\t\tdef memoizer(self, *args, **kwargs):\n\t\t\treturn getter(self, *args, **kwargs)\n\t\tmemoizer.__name__ = func.__name__\n\t\tmemoizer.__doc__ = func.__doc__\n\t\tmemoizer.cache_info = func.cache_info\n\t\tmemoizer.cache_clear = func.cache_clear"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorating_func(func):\n\t\tfunc.cache = cache\n\t\tfunc.key = key\n\t\tfunc.lock = lock or threading.Lock()\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tfunc.lock.acquire()\n\t\t\ttry:\n\t\t\t\tkey = func.key(*args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tres = func.cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tres = func.cache[key] = func(*args, **kwargs)\n\t\t\tfinally"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorating_function(self, *args, **keywds):\n\t\tckey = key(self, *args, **keywds)\n\t\twith locking.RLock() if lock else contextlib.suppress():\n\t\t\tresult = cache.get(ckey, NotExisting)\n\t\t\tif result is not NotExisting:\n\t\t\t\twith logging.debug(\"Cached method '%s'(%s)\", func_name, ckey) as debug:\n\t\t\t\t\tdebug('Cached Method called', ckey)\n\t\t\t\tresult = result\n\t\t\telse:\n\t\t\t\twith logging"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif not args or kw and not callable(cache):\n\t\traise TypeError(\"it should be a callable\")\n\n\tdef memoizer(func):\n\t\tcfunc = cache()\n\t\tif type(cfunc).__name__ == \"boundmethod\":\n\t\t\tcfunc = cfunc.im_func\n\t\tif not hasattr(func, \"_callcount\"):\n\t\t\tfunc._callcount = 0\n\n\t\ttry:\n\t\t\tvars(cfunc)\n\t\texcept AttributeError:\n\t\t\tcache__vars = {}\n\t\telse:\n\t\t\tcache__vars ="}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.__cache__ = cache\n\t\tfunc.__cachekey__ = key\n\t\tfunc.__cachelock__ = lock\n\t\treturn func\n\treturn decorator\n"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# if there is an exception, and e.stype and e.stype.__str__\n\tif is_py3k:\n\t\ttb_lines = traceback.format_exception(type(e), e, getattr(e, '__traceback__', None))\n\telse:\n\t\ttb_lines = traceback.format_exception(str(type(e)), str(e), getattr(e, '__traceback__', None))\n\t# try to fix path to get a relative path\n\tif hasattr(e, 'path"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn cls(e, 'ext', max_level, max_path_level)"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\texcl_str = '{}: '.format(cls.__name__)+str(e)\n\tif max_level == 0:\n\t\treturn excl_str\n\treturn cls.extostr(e, max_level - 1, max_path_level) + excl_str\n\n\n@lru_cache(maxsize=128)  # Use it as cache\ndef find_package_location_in_library_path(package_obj):\n\t\"\"\"\n\tFind the actual location of the module from a list of packages.\n\t:param package_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts = StringIO()\n\tprint >>s, '%s:'% e.__class__.__name__\n\tprint >>s,' Object is %s' % safe_repr(e)\n\tprint >>s,' Traceback (most recent call last):'\n\ttraceback.print_tb(e.__traceback__ if hasattr(e, '__traceback__') else sys.exc_info()[2], limit=max_path_level, file=s)\n\tprint >>s,' Exception: %s'"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ttext = '\\n{}\\n{}: {}\\n'.format(cls.name2(),e,str(e))\n\ttext += cls.get_trace(e, max_level, max_path_level)\n\treturn text\n\n\nclass _DoublyLinkedTuple(tuple):\n\t\"\"\"\n\tA one-element tuple that can be converted easily into an array.\n\tIt is automatically doubly linked.\n\t\"\"\"\n\tdef __new__(cls, e1, e2=None):\n\t\tif not len(e1):"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tsio  = io.StringIO()\n\ttype_ = type(e)\n\tif type_ not in cls.re_map:\n\t\tcls.re_map[str(type_)] = (None, 1)\n\tcls.re_map[str(type_)] = (cls.re_map[str(type_)][0], max_path_level)\n\tif (type_ == str) and (cls.re_map[str(type_)][0]: # string error message\n\t\tcls.re_map["}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\texc_details = cls.__name__, e.__class__, e, e.args[0] if e.args else None\n\te_k = '%s.%s-%s:'% exc_details\n\ts = e_k\n\tseen_levels = 0\n\tdetails = None\n\n\ttb = getattr(e, \"__traceback__\", None)\n\twhile tb is not None and seen_levels < max_path_level:\n\t\tif details is not None:\n\t\t\ts += \"\\n\" + details\n\t\tt"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts = traceback.format_exc()\n\tif isinstance(e, SystemExit):\n\t\treturn s\n\tse = SafeRepr()\n\t#\n\tif isinstance(e, (TypeError, KeyError)):\n\t\treturn s + \"\\n#ERROR, type(v)={} and v={}\".format(type(e).__name__, se(e))\n\telif isinstance(e, SyntaxError):\n\t\timport inspect\n\t\tfn = inspect.getfile(cls)\n\t\tif not fn or len(fn) == 0:"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\texclist = ('__init__', )\n\ttrace_lines = traceback.format_exception(type(e), e, exc_info()[2])\n\ttrace = ''.join(trace_lines)\n\n\tif issubclass(type(e), HTTPException):\n\t\tmessage = trace.split('\\n', 1)[1]\n\t\tif str(e).__contains__('\\nException in HTTP request'):\n\t\t\tif not message:\n\t\t\t\tmessage = 'Error in HTTP request'\n\telif issubclass(type(e), ConnectionRef"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tt = Traceback.extract(limit=max_level, max_path_level=max_path_level, exception=e)\n\tstack_str = \"Traceback (most recent call last):\\n\" + \"\\n\".join(\"%s\\n%s\" % t for t in t.flatten())\n\ts = \"%s: %s\" % (cls.__name__, e)\n\treturn stack_str.rstrip() + \"\\n\" + s + \"\\n\"\n\t\n"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\tfile = None\n\t\tif not exists(file_name) and overwrite is False:\n\t\t\tfile = open(file_name, \"w+\")\n\t\t\tfile.close()\n\t\tif file:\n\t\t\tfile = open(file_name, \"ab\")\n\t\t\tfile.write(text_buffer)\n\t\t\tfile.close()\n\texcept:\n\t\tprint(\"Exception appending to file\")\n\t\treturn(-1)\n\treturn(file_write_size(file_name, encoding))\n\ndef read_to_file(\n\tfile_name, encoding"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttextBuffer = text_buffer.encode(encoding).decode('utf-8')\n\tfile = FileIO(file_name, 'ab' if overwrite else 'a+b')\n\tfile.writeBytes(textBuffer)\n\tfile.close()\n\treturn len(textBuffer)\n\ndef append_text_to_file_safe(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tTries to safely write to the file. It is safe if nothing is written, it\n\tstill returns the number of bytes written.\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tlog(2,\"Appending to '{}'\".format(file_name))\n\tif not overwrite and os.path.exists(file_name):\n\t\tlog(3,\"File '{}' already exists. Ignoring.\".format(file_name))\n\telif not os.path.exists(os.path.dirname(file_name)):\n\t\ttry:\n\t\t\tos.makedirs(os.path.dirname(file_name))\n\t\texcept:\n\t\t\traise Exception(\"Failed to create directory '{}'.\".format(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\treturn __append_text_to_file(file_name, text_buffer, encoding, overwrite=overwrite)\n\n\n# ----------------------------------------------------------------------\n# \n# A Python Script Wrapper (POW)\n#\n# ----------------------------------------------------------------------\n\ndef save_python_script_to_file(Python_Version, source, filename, encoding='latin_1', include_hidden=False, overwrite=False):\n\t\"\"\"\n\tSave a Python script into a text file that can be run on a PC.\n\t:param Python_Version: The version of Python to use, e.g."}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\twith io.open(file_name, 'a+' if overwrite else 'w', encoding=encoding) as file:\n\t\tif not file.writable():\n\t\t\treturn 0\n\t\tnum_bytes = file.write(text_buffer)\n\treturn num_bytes\n\n\n\ndef append_text_to_file_and_read_all_it(file_name, text_to_write, read_bytes, encoding, overwrite=False):\n\t\"\"\"\n\tWrite the specified text into the file,\n\tand then read the specified number of bytes from the file.\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\treturn 0\n\n\ndef append_text_to_file_with_timestamp(file_name, text_buffer, overwrite=False):\n\t\"\"\"\n\tA variant of append_text_to_file that will create a new file if one does not exist.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\timport struct\n\timport datetime\n\tfrom suitcase.binaryarchive import BinaryFile\n\n\tif isinstance(file_name, BinaryFile) == True: raise Exception('Cannot append to binary file!')\n\n\t# Check arguments:\n\tif len(file_name) == 0: raise Exception('Filename required!')\n\tif len(text_buffer) == 0: raise Exception('Body required!')\n\n\tif overwrite:\n\t\tif not BinaryArchive.does_file_exist(file_name): raise Exception('File does not exist')\n\telse:\n\t\tif BinaryArch"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tglobal file_data_buf\n\tglobal file_binary_buf\n\tglobal file_lines_number\n\tglobal file_write_pos\n\tglobal file_max_lines_number\n\tglobal file_overwrite_flag\n\tif encoding is None:\n\t\tencoding = Config.get_app_data_filename_encoding(file_name)\n\ttext_encoded = ''.join(chr(v) for v in text_buffer if v)\n\ttext_encoded_len = len(text_encoded)\n\tif not text_encoded_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif encoding == MFLProperty.default_font_encoding:\n\t\ttry:\n\t\t\twith open(file_name, \"wb\" if overwrite else \"x\", 0, encoding=encoding) as file_handle:\n\t\t\t\tfile_handle.write(text_buffer)\n\t\texcept Exception as e:\n\t\t\traise Exception(\"Failed to write data to file '%s': %s\"%(file_name, str(e)))\n\t\telse:\n\t\t\treturn os.path.getsize(file_name)\n\telse:\n\t\tif not file_exists(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tlogger.trace(\"Writing text to: %s\", file_name)\n\tif not overwrite and os.path.isfile(file_name):\n\t\terr_str = \"Cannot append contents to existing (overwrite=False) file : %s\" % file_name\n\t\tlogger.error(err_str)\n\t\traise IOError(err_str)\n\twith open(file_name, 'a', encoding=encoding, errors=\"replace\") as text_file:\n\t\ttext_file.write(text_buffer)\n\n\ndef clear_line(print"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, \"r\", encoding=encoding) as fp:\n\t\t\treturn fp.read()\n\texcept:\n\t\treturn None\n\ndef file_to_lines(file_name, encoding=\"UTF-8\"):\n\t\"\"\"\n\tReturns a list of lines.\n\t:author: Paradoxis <info@paradoxis.nl>\n\t:param file_name:\n\t:param encoding:\n\t:return:\n\t\"\"\"\n\treturn file_to_textbuffer(file_name, encoding).split(\"\\"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\ttext = f.read()\n\texcept ValueError as e:\n\t\tprint(e)\n\t\treturn None\n\treturn text\n\n\ndef write_textbuffer(text, file_name, mode='w'):\n\t\"\"\"\n\tSave the provided contents to a file.\n\tThis function can be used to save small text snippets to file and read them into memory without affecting files on\n\tdisks.\n\tCAUTION : This function also takes care of properly closing/closing"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\tprint(\"File %s not found\" % file_name)\n\t\treturn None\n\n\t# Get file content\n\ttext = None\n\twith open(file_name, 'rb') as fp:\n\t\ttry:\n\t\t\ttext = fp.read().decode(encoding)\n\t\texcept UnicodeDecodeError:\n\t\t\traise Exception(\"Unable to decode file %s!\\n\\nIs the encoding correct?\\n\" % file_name)\n\t\texcept Exception:\n\t\t\traise Exception(\"Exception while"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\ttxtBuff = open(file_name, 'r', encoding=encoding)\n\t\treturn txtBuff.read()\n\texcept:\n\t\t_logger.error('[file_to_textbuffer_error]')\n\t\treturn ''\n\n\n# -----------------------------------------------------------------------------------\n# -----------------------------------------------------------------------------------\n\n\ndef get_list_ids(model_name, ids_path):\n\"\"\"\n\tReturn ids of a recordset.\n\t:param model_name: Model name (ex: BaseModel, WizardTemplateLine).\n\t:type model_name: str"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t\n\ttry:\n\t\tmy_file = open(file_name, \"r\")\n\texcept IOError:\n\t\tprint(\"Unable to delete file : \",error)\n\t\treturn None\n\t\n\t# Create/open a text buffer for the file :\n\tinput_buffer = io.TextIOWrapper(my_file.buffer, encoding=encoding, line_buffering=True)\n\treturn input_buffer\n\n\n\ndef display_file_textbuffer(text_buffer, encoding, max_line):\n\"\"\"\n\tDisplay the content of a text buffer,"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith codecs.open(file_name, encoding = encoding) as f:\n\t\ttext = f.read()\n\treturn text\n\ndef find_line(text, search_str):\n\t\"\"\"\n\t\tFuse : find_line(\"The quick brown fox\", 'quick') and return : 0 and 11.\n\t\"\"\"\n\tstart_index = 0\n\tend_line = -1\n\tmatch = True\n\twhile match :\n\t\tend_line = text.find(search_str, start_index)\n\t\tif end_line == -1 :\n\t\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\treturn open(file_name, 'r', encoding=encoding)\n\texcept Exception as e:\n\t\traise IOError(e)\n\n\ndef file_to_list(file_name, encoding):\n\t\"\"\"\n\tLoad a file into a list of elements (for JSON/CSV)\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A list of CSV data\n\t:rtype str\n\t\"\"\"\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tbuffer = None\n\ttry:\n\t\twith open(file_name, \"r\") as input_file:\n\t\t\tbuffer = (input_file.read()).encode(encoding)\n\t\treturn buffer\n\texcept OSError:\n\t\tprint(\"Erreur dans lecture du fichier : %s\" % file_name)\n\treturn buffer\n\n\ndef json_file_to_textbuffer(file_name):\n\t\"\"\"\n\tLoad a json file toward a text buffer (UTF-8), using the specify encoding while reading.\n\t:param file_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'rt', encoding=encoding) as f:\n\t\t\ttxt = f.read()\n\t\t\tf.close()\n\t\t\treturn txt\n\texcept Exception:\n\t\treturn None\n\n\ndef main()->str:\n\t\"\"\"\n\tMain function, entry point of the script.\n\t:return:\n\t\"\"\"\n\tif len(sys.argv) < 2:\n\t\tprint('Passez un fichier \u00e0 traiter.')\n\t\tsys.exit()\n\n\n\tfile_name = sys.argv[1"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith codecs.open(file_name, \"r\", encoding, errors=\"strict\") as fp:\n\t\t\treturn fp.read()\n\texcept OSError:\n\t\treturn None\n\ndef path_to_textbuffer(path):\n\"\"\"\tLoad a file toward a text buffer (UTF-8), using the specify path while reading.\n\t:param path: Path.\n\t:type path: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\treturn"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\traise IOError(\"Invalid file name\")\n\ndef get_file_path(file_name):\n\t\"\"\"\n\tGet file path.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: file name with full path.\n\t:rtype str\n\t\"\"\"\n\treturn join(get_desktop_path(), file_name)\n\ndef get_text_from_file(file_name):\n\t\"\"\"\n\tGet"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not fsnd_config_mgmt_util.is_valid_file_name(file_name):\n\t\treturn False\n\t\n\treturn True\n\n\ndef create_file(file_name):\n\t\"\"\"\n\tCreate file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t\"\"\"\n\tfile_name = fsnd_config_mgmt_data.FILE_EXCEL\n\tfile_path = fsnd_path_mgmt_data.FSND_CONFIG_MANAGEMENT_SYS_CONFIG"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tpath_for_check = os.path.join(curr, file_name)\n\tif os.path.isfile(path_for_check): \n\t\treturn True\n\telse:\n\t\treturn False\n\ndef check_if_user_owns_file(user, file_name):\n\t\"\"\"\n\tCheck if user has the permission to access the file.\n\t:param user: User ID.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: If the user has permission to access the file,"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tprint(\"[*] Checking file: %s\" % file_name)\n\treturn os.path.isfile(file_name)\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\ndef readlines(file_name):\n\"\"\"\n\tRead the whole file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return file content in return values.\n\t:rtype str/int, str/int, str/int,...\n\t\"\"\"\n\twith open(file_name, 'r+') as f:\n\t\treturn f.readlines()\n\ndef readline(file_name):\"\"\"\n\tRead one line.\n\t:"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telif file_name == '' or file_name is None:\n\t\treturn False\n\telse:\n\t\traise SystemExit('%s: ERROR: File do not exists...' % file_name)\n\ndef get_file_list(path, ext=None):\n\"\"\"\n \tGet the list of file under specific path.\n \t:param path: Current directory.\n \t:type path: str\n \t:return: File list\n \t:rtype: list\n\"\"\"\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif type(file_name)!= str:\n\t\traise TypeError(\"file_name should be str type\")\n\n\tis_valid = \"\"\n\tfile_name = file_name.strip()\n\n\t\t\n\ttry:\n\t\tif int(file_name):\n\t\t\tis_valid += \".0\"\n\t\t\tfile_name\n\texcept ValueError:\n\t\tif file_name[-4:] == '.pdf' or file_name[-4:] == '.doc' or file_name[-4:] == '.docx':\n\t\t\tis_valid += '.1'\n\t\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif file_name == \"\":\n\t\tprint(\"File name is invalid.\")\n\t\treturn False\n\telif not os.path.isfile(file_name):\n\t\tprint(\"File does not exist.\")\n\t\treturn False\n\treturn True\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path', '-p', help = 'The absolute directory path of the file to be written to')\n    parser.add_argument('--file_name', '-n', help = 'The file"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tos.path.exists(file_name)\n\t\tis_file_exist_flag = True\n\texcept FileNotFoundError:\n\t\tis_file_exist_flag = False\n\t\treturn False\n\treturn is_file_exist_flag\n\n\ndef get_file_name(path, file_name_list, number_list, suffix_name):\n\t\"\"\"\n\tGet and return all file name.\n\t:param path: Path of data files.\n\t:type path: str\n\t:param file_name_list: List to store file"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\ndef is_directory_exist(directory_name):\n\"\"\"\n\tCheck if directory exist.\n\t:param directory_name: Directory name\n\t:type directory_name: str\n\t:return: return true (exist)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isdir(directory_name)\ndef get_file_name(file_dir):\n\"\"\"\n\tThis function check the directory exists, if not, raise an error.\n\t:param: file_dir: The file directory."}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.level_dict = dict()\n\n\tlevel_list = set()\n\tfor log_str in logging.root.handlers:\n\t\tif not isinstance(log_str, logging.handlers.TimedRotat\n\t\t\tileHandler):\n\t\t\tcontinue\n\t\tlevel = getLevelName(log_str.level)\n\t\tif not level in cls.level_dict:\n\t\t\tcls.level_dict[level] = set()\n\t\tcls.level_dict[level].add(log_str.__class__)\n\n\t# Remove empty log dict"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlogging.shutdown()\n\tlogging._acquireLock()\n\ttry:\n\t\tlevel = logging.root.level\n\t\tlogging.root.handlers = []\n\t\tlogging._root = logging.root\n\t\tlogging._root_client = logging._root\n\n\t\tif sys.stderr in logging.root.handlers:\n\t\t\t# Don't want to break any existing logging configuration\n\t\t\tlogging.root.removeHandler(logging.root.handlers[1])\n\n\t\tif level > logging.DEBUG:\n\t\t\treturn\n\t\tif level <= logging."}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# disable existing loggers and set as new one\n\tcls._logger = logging.getLogger(cls.__name__)\n\tcls.__logger    = cls._logger\n\n\n##############\n# Logging #\n##############\n\ndef _get_logger(name: str = None, path: str = None, level: int = None):\n\tlogger = logging.getLogger(name)\n\t\n\t# set handler \n\tif level is None:\n\t\twhile logger.level == logging.NOTSET:\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlogger = logging.getLogger(None)\n\tif logger.hasHandlers():\n\t\tfor handler in logger.handlers:\n\t\t\tlogger.removeHandler(handler)\n\tcls.__reset_logging()\n\n\tformatter = logging.Formatter('%(name)s - %(levelname)s - %(asctime)s : %(message)s', '%Y-%m-%d %H:%M:%S')\n\tfile = handlers.TimedRotatingFileHandler(cls.__logs_path,"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\tif cls.log_handler_info:\n\t\tcls.log_handler_info.stream.close()\n\t\tcls.log_handler_info.removeHandler(cls.log_handler_info)\n\t\tcls.log_handler_info = None\n\t\t\n\tif cls.log_handler_err:\n\t\tcls.log_handler_err.stream.close()\n\t\tcls.log_handler_err.removeHandler(cls.log_handler_err)\n\t\tcls.log_handler_err = None\n\n\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\timport logging\n\tlogging.disable(logging.CRITICAL)\n\n_reset_logging()\n\n\"\"\"\n#==============================================================================\n# TODO: add logger to this\n#==============================================================================\nTODO-B\nMake sure the log file is reset for the script to get started from scratch.\n\nTODO-C\nAllow for easy changes in the script name.\n\nTODO-D \nLet the user select the logger type.\n\nTODO-E\nLet the user select a logging level.\n\"\"\"\n\nimport logging,sys\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t\n\t_LOGGER.info(\"[LOG RESET]\")"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# Clear console handler\n\tfor handler in cls._root_logger.handlers:\n\t\tcls._root_logger.removeHandler(handler)\n\n\t# Clear handlers config\n\tfor handler in cls._handlers:\n\t\tcls._handlers[handler].reset()\n\n\t# Clear handlers\n\tfor handler in cls._handlers.values():\n\t\thandler.clear()\n\n\t# Configure\n\tcls._handlers = dict()\n\tcls._log_level = \"INFO\"\n\n\tcls.clear()  # Clear root logger\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tfor handler_class in cls._LOGGING_HANDLERS:\n\t\thandler = getattr(cls.LOGGER, handler_class)\n\t\t# remove handler.\n\t\tif handler in cls._LOGGING_HANDLERS:\n\t\t\thandler.close()\n\t\t\tfor handler_class_name in cls.ALLOWED_LOGGING_HANDLERS:\n\t\t\t\tif handler_class_name in cls._LOGGING_HANDLERS:\n\t\t\t\t\thandler = getattr(cls.LOGGER, handler_class_name)"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._logger = logging.getLogger(cls._default_logger_name())\n@classmethod\ndef _reset_loggers(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tfrom multiprocessing import shared_memory as SHM\n\timport os\n\tif SHM.address is not None:\n\t\tos.environ[\"NUMEXPR_MAX_THREADS\"] = '1'\n@classmethod\ndef _reset_tables():\n\"\"\"\n\tReset\n\t\"\"\"\n\timport numexpr as ne\n@classmethod"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n\n\t\nclass PyRefleakTests(_PYRefleakBase, unittest.TestCase):\n\t\"\"\"\n\tTest the refleak detector in Py\n\t\"\"\"\n\t_getTargetClass = staticmethod(_getTargetClass)\n\t\n\tdef _cleanPathCreated(cls, path, result):\n\t\tos.remove(path)\n\t\t\n\t# test the whole object is collected and sent to gc\n\tdef test_refleak(self):\n\t\timport gc\n\t\tgc.collect()\n\t\tbefore ="}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tdef _getTargetInterface(self):\n\t\"\"\"\n\tReturn the interface of the Target class.\n\t\"\"\"\n\treturn _Object\n\n\n_DUMP_ALL = False\ndef pydoc(object, title=None):\n    \"\"\"Helper to generate a document listing like that returned by inspect.getdoc().\"\"\"\n    from pyannotate.checker import AnnotationChecker\n    return AnnotationChecker().pydoc(object, title=title, allchecks=_DUMP_ALL)\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplemented\n\n\ndef _getTargetID(self):\n\t\"\"\" Returns the C-name of the object \"\"\"\n\treturn '%s(%s)' % (self.__class__.__name__,\n\t'' if 'ID' in self.__class__.__dict__ else 'ID')\n\n\nif hasattr(object, '__metaclass__'):\n\t_NSPersistentStore = object.__metaclass__('_NSPersistentStore', (NSObject,), {'description' : \"\",\n\t'_objectMap"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\timport zope.interface\n\n\ttype = zope.interface.direct.registered[1] # return the concrete class\n\tif type is None:\n\t\ttype = Interface\n\n\tif type.tzoffset is not None:\n\t\tfrom dtaidistance.tz import FixedOffsetTimezone\n\t\ttype = type.copy(tz=FixedOffsetTimezone(type.tzoffset))\n\n\treturn type\n\nFrozenPy = _getTargetClass()\n\n\n'''\n(..)  *classifier* :\n\nThe type of the classifier in most cases will be simply *instance*"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn _import.getImplementation,_import.registerImplementation\n\ndef getImplementationId(obj):\n    \"\"\"\n    Returns an ID for the given Python object.\n\n    @type obj: any :term:`mapping` or :term:`sequence`\n    @param obj: The Python object about which a stable ID should be assigned.\n    @rtype: string\n    @return: A string ID for the given object.\n    \"\"\"\n\n    if type(obj) == DottedName:\n        return obj.__str__()\n\ndef idToClass("}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\"This must be subclassed\")\n\n##########################################################################\n\nclass _FakeModule(object):\n\t@classmethod\n\tdef get(klass, module):\n\t\tif type(module) is klass: return module\n\t\treturn None\n\n_v = lambda s: s.strip() or None\n\ndef load_fallback(module):\n\t\"\"\"\n\tLoad the fallback for this engine, if any, and return its _Engine subclass.\n\t\"\"\"\n\n\tmodule = _v(module)\n\tif module.startswith('"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError()\n\n\nclass _FallbackStrategy:\n\t\"\"\"\n\tAn I{object} providing the fallback for a type.\n\t\"\"\"\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturns a textual representation of the type.\n\t\t\"\"\"\n\t\treturn \"<type '%s.%s'>\" % (self.module, self.getFallbackAlias())\n\n\nclass _FactoryStrategy:\n\t\"\"\"\n\tAn I{object} providing the factory for a type.\n\t\"\"\"\n\n\tfactory = None\n\n\tdef __init"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn target_cipd.LocalPantsDict\n\nclass CipdFileList(target_cipd.CipdDict):\n\tdef __init__(self, *args, **kwds):\n\t\tname = kwds.pop('name')\n\t\ttarget_cipd.CipdDict.__init__(self, args, kwds, name)\n\t\tif self.name:\n\t\t\tlog.info(\"Using CIPD-filelist at '%s'\" % (self.name,))\n\t\telse:\n\t\t\tlog"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn _SimpleITK.__init__\n\n\nclass SimpleITK(NodeGeneric):\n\t\"\"\"\n\tSimple ITK Node.\n\t\"\"\"\n\n\t# Properties.\n\tpropertyPicker = \"\"\"\n\t# Automatically generated by fslibskeleton@fsl.fm. Include extra properties.\n\t\"\"\"\n\n\n\t# Class generic properties.\n\tdef _setProperty(self, key, value):\n\t\ttry:\n\t\t\tnode = self._getNode().GetNodeRepresentation(0)\n\t\texcept TypeError:\n\t\t\traise TypeError(\"'{0}' Node"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tfrom z3core.abstract import Set\n\treturn Set\n\t\n\t\n\n# EOC z3core.py\n\n# EOC z3core\n'''\n(if True:\n            return m\n        elif False:\n            return n\n        else:\n            raise Exception\n    self.assertEqual(m, n) # if 1 2 else 3))\n    if False:\n            return m\n        elif not False:\n            return m\n        else:\n            raise Exception\n    self.assertEqual(k, m)\n    if True"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\torderings = copy(orderings)\n\torderings_len = len(orderings)\n\tif(orderings_len == 1):\n\t\treturn(orderings)\n\t\n\t# Sort the list of orderings based on their last characters,\n\t# meaning that we are trying to find where the last characters\n\t# appear next to last\n\torderings = sorted(orderings, key=lambda x: (len(x), x[-1]))\n\tlast = [-1] * orderings_len\n\t\n\tfor i in range(orderings_len):"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# For each ordering\n\t#\tFilter objects that are in more than one ordering\n\t#\tFilter each such object with _filterOrdered(key, ordering2)\n\t#\tMake the resulting sequence unique by removing duplicates.\n\t#\tRecursively call _legacyMergeOrderings for each key.\n\ts = lambda x : ','.join(sorted(s for s in x))\n\ti = itertools.izip_longest(*map(groupby, sorted(orderings, key=s), itertools.count())))\n\tu"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# Merge inplace to combine\n\ttmp = []\n\tfor o1 in orderings:\n\t\ttmp.extend(o1)\n\torderings = tmp\n\t\n\t# Do the merge\n\torderings = orderings.next()\n\twhile orderings!= None:\n\t\ttmp = []\n\t\tfor o2 in orderings:\n\t\t\tfor o1 in orderings:\n\t\t\t\tif o1.startswith(o2):\n\t\t\t\t\ttmp.append(o1 + o2[len(o1):])\n\t\t\t\t\tbreak\n\t\torderings = tmp\n\torder"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tresult = []\n\t\n\tif not orderings:\n\t\traise ValueError(\"_mergeOrderings: orderings must contain at least one element\")\n\t\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\t\n\torderings_merged = map(set, orderings)\n\tfor idx in range(len(orderings_merged) - 1):\n\t\tfirst = orderings_merged[idx]\n\t\t\n\t\tseen_set_add = True\n\t\tfor i in range(idx + 1, len("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif (len(orderings) < 2):\n\t\treturn orderings\n\n\torderings = deepcopy(orderings)\n\n\tif _isListLike(orderings[0]):\n\t\torderings[0].sort()\n\telse:\n\t\torderings[0] = sorted(orderings[0])\n\t\n\tfor i, ordering in enumerate(orderings[1:]):\n\t\tif (len(ordering) == 0) or (isinstance(ordering, basestring)): continue\n\t\tif (isinstance(ordering, list)) and all([isinstance(part"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\torderings = sortByLengths(orderings)\n\torderings.sort(key=len)\n\ti = 0\n\twhile i < len(orderings) - 1:\n\t\tif orderings[i + 1][len(orderings[i]):] == orderings[i]:\n\t\t\tdel orderings[i + 1]\n\t\telse: i += 1\n\treturn orderings\n\ndef _mergeOrderings2(orderings):\n\tdef _equal_sortKey(item):\n\t\treturn len(item), item\n\tif orderings == []"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\timport itertools\n\t\n\torderings = list(orderings)\n\t\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\t\n\torderings = itertools.chain(\n\t\torderings,\n\t\titertools.izip_longest(*orderings, fillvalue = tuple())\n\t)\n\t\n\t# This is a simple generator, which yields all combinations\n\t# of the given orderings. This makes it quite easy to manipulate them,\n\t# since each of the returned objects are immutable tuples.\n\torder"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tresult = []\n\tresultOrderings = []\n\t\n\tfor ordering in orderings:\n\t\tif not ordering:\n\t\t\tresultOrderings.append(ordering)\n\t\telse:\n\t\t\tmergingResult = True\n\t\t\tfor ordered in resultOrderings:\n\t\t\t\tif ordering[0] in ordered:\n\t\t\t\t\tordered += ordering[1:]\n\t\t\t\t\tmergingResult = False\n\t\t\tif mergingResult:\n\t\t\t\tresultOrderings.append(ordering)\n\t\n\tfor ordering in resultOrderings:\n\t\tresult += ordering\n\treturn result\n\ndef _mergeOrderings1_2"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\torderings = sorted(orderings, key=len, reverse=True)\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor o in ordering:\n\t\t\tif o in result:\n\t\t\t\tresult.append(o)\n\treturn result\n\n_defaultSortOrderings = lambda x,y,z: 1\n_sortOrderings = lambda orderings: sorted(orderings, key=len, reverse=True)\n_getMinimalOrderings = lambda orderings: [\n\tmax(OrderedSet(orderings), key=lambda x: _defaultSortOrderings"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif not orderings:\n\t\treturn orderings\n\tresult = []\n\tfor ordering in orderings:\n\t\tnewOrdering = _mergeOrderings(result +  [ordering])\n\t\tresult = newOrdering\n\treturn result\n\ndef mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple orderings to get the full set in sorted order\n\t\n\tIf either operation is entirely null the other operation is complete\n\t\"\"\"\n\treturn _legacy_mergeOrderings(orderings)\n\ndef _mergeOrderingTuples(orderings):\n"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t# pylint:disable=protected-access\n\treturn Declaration(getObjectProvides(object))\n\n\ndef classProvides(*interfaces):\n    \"\"\"Declare interfaces provided directly by a class\n    \n    The arguments after the class must be one or more interfaces or\n    interface specifications (`~zope.interface.interfaces.IDeclaration` objects).\n    \n    This function is called in a class definition (that is, it must be called\n    from within a class definition or in an object dispatch)::\n    \n      class MyClass:\n        implements"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tmodname = _object_frame(object)['__name__']\n\treal_name = _object_frame(object)['__real_name__'] # pylint:disable=invalid-name\n\treturn getObjectProvides(modname, real_name)\n\ndef classProvides(*interfaces): # pylint:disable=redefined-builtin\n\"\"\"\n\n\t@param interfaces: C{(I1, I2,...)}\n\n\n\tUnlike L{directlyProvidedBy}, the value returned from this\n\tfunction is an `~zope"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tklass = None\n\tdispatch = ('__provides__',)\n\tnamed = special_named_objs()\n\tmodule = getattr(object, '__module__', None)\n\n\tdef __init__(self, obj, name=''):\n\t\tself.__obj = obj\n\n\t@property\n\tdef __name__(self):\n\t\treturn repr(self.__obj)\n\n\tdef __call__(self):\n\t\traise TypeError(\"Must implement __provides__ as a class attribute\")\n\n\tdef keys(self):\n\t\ttry:\n\t\t\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif not providedBy(object):\n\t\treturn ()\n\tprovides = providedBy(object)\n\tspec = implementedBy(object)\n\tresult = set()\n\tfor base in spec.__bases__:\n\t\tif isinstance(base, SpecificationBase):\n\t\t\tresult.update(base._implied)\n\tresult.update(provides)\n\treturn tuple(result)\n\ndef classProvides(*interfaces):\n    \"\"\"Declare interfaces provided directly by a class\n\n      This function is called in a class definition.\n\n      The arguments are one or more interfaces"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tklass = providedBy(object)\n\treturn Declaration(klass)\n\ndef providedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\ttry:\n\t\tspec = providedBy.__func__(object) # pylint:disable=no-member\n\texcept TypeError:\n\t\tspec = providedBy.__get__(object, None) # pylint:"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn Declaration(ProvidedBy(object, object, _calledInside()))\n\ndef classImplements(*interfaces): # pylint:disable=redefined-builtin\n    \"\"\"\n    Declare class to implement the given interfaces.\n\n      classImplements(*interfaces)\n\n    is equivalent to:\n\n      directlyProvides(cls, *interfaces)\n\n    but may be more efficient because the class does not need to\n    properly manage its own __provides__.\n\n    Parameters:\n\n      interfaces\n             One or more interfaces or interface specifications\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tprovided = object[IBASE_ATTRIB]\n\tdel getattr(provided, IMPLID_ATTR, _marker)\n\tk = (provided, ) # Mark used to detect when we are out of bases\n\tfor b in bases:\n\t\tprovider = getattr(b, IBASE_ATTR, _marker)\n\t\tif provider is not _marker and provided is not provider:\n\t\t\tif k in _inherit_cache:\n\t\t\t\tdeferred = _inherit_cache[k]\n\t\t\t\tbreak\n\t\t\tdone_with_this_"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface import providedBy\n\treturn providedBy(object)\n\t\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn Declaration(ZopeSecurityPolicy('request')(ZopeSecurityPolicy('owner')(_directlyProvidedIndirection(object))).__provides__)\n\ndef classProvides(*interfaces): # pylint:disable=redefined-outer-name\n\t\"\"\"\n\tDeclare interfaces provided directly or indirectly by the class.\n\n\t.. seealso:: `zope.extras.declarative.providedBy`\n\n\t\"\"\"\n\tclassProvides = interfaces + (None,)\n\tclassProvides = ZopeSecurityPolicy('request')(classProvides)\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tklass = providedBy\n\treturn object.__provides__\nprovides = directlyProvidedBy # backwards compatability\ndirectlyProvidedByClass = directlyProvidedBy # backwards compatability\n\ndef alsoProvides(object, *interfaces):\n    \"\"\"Declare additional interfaces directly for an object, beyond what\n    is already declared by his class\n    \"\"\"\n    klass = providedBy(object)\n    interfaces = tuple(_normalizeargs(interfaces))\n    directlyProvides(object, *interfaces)\n    for interface in interfaces:\n        _"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tdef __init__(self, classes=[], base=None):\n\t\tself.classes = sorted(classes)\n\t\tself.base = base\n\n\tdef __repr__(self):\n\t\treturn \"<MinimalBases '%s' for '%s'>\" % (self.classes, self.base)\n\n\tdef __eq__(self, other):\n\t\treturn (\n\t\t\t(isinstance(other, MinimalBases) and\n\t\t\tother.base == self.base and\n\t\t\tother.classes == self.classes) or\n\t\t\tisinstance(self.base,"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclassesOrdered = classes\n\tif False in classes:\n\t\treturn classes\n\telse:\n\t\tif classesOrdered == []:\n\t\t\treturn classes\n\t\telse:\n\t\t\tordered = {}\n\t\t\twhile classesOrdered!= []:\n\t\t\t\ti = classesOrdered.pop(0)\n\t\t\t\tif i not in ordered:\n\t\t\t\t\tfor j in i:\n\t\t\t\t\t\tfor k in ordered:\n\t\t\t\t\t\t\tif j in ordered[k]:\n\t\t\t\t\t\t\t\tordered[k].append(j)\n\t\t\t\t\t\tif len(ordered[k]) == len(i):\n\t\t\t\t\t\t\tordered[k] = i \n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\tequivalent = []\n\tequivalent_classes = set()\n\t\n\tfor i in range(0,len(classes)):\n\t\t\n\t\tif'subclass_of' in classes[i]:\n\t\t\tequivalent =  set(classes[i]['subclass_of'])\n\t\t\tequivalent_classes = equivalent_classes.union(equivalent)\n\t\t\tequivalent = equivalent.union(classes[i]['equivalence'])\n\t\t\tequivalent_classes = equivalent_classes.union(equivalent)\n\t\t\t\n\t\telse:\n\t\t\tequivalent ="}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t\n\tclassesList = list(classes)\n\tlist2 = classesList.copy()\n\t\n\tfor i in range (0, len(classesList), 1):\n\t\tif i + 1 < len(classesList):\n\t\t\tcounter = 0\n\t\t\tfor j in range (1, len(classesList), 1):\n\t\t\t\tif (classesList[i] == classesList[j]!= None):\n\t\t\t\t\tif (counter < 1):\n\t\t\t\t\t\tclassesList[i] = classesList.pop(i)\n\t\t\t\t\t\tcounter += 1\n\t\t\n\treturn classesList\n\t\n\nclass"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tbases = collections.Counter(classes)\n\t# create a dictionary for all of the subclasses and assign the value 1\n\tvalues = {b: 1 for b in bases if bases[b]}\n\t# use dictionary comprehension to add all the subclasses to a list and get the minimum\n\t# repeat the process for all of the values and take the minimum\n\tmin_value = min((_ for _ in values.values()), default=1)\n\tfor b in bases:\n\t\t# check if any value is 1 just skip over any of the base classes that do not have an in"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif classes == None:\n\t\treturn []\n\tbases = [classes[0]]\n\tfor c in classes[1:]:\n\t\tk = -1\n\t\t#print classes\n\t\tfor i in range(len(bases)):\n\t\t\tif issubclass(c, bases[i]):\n\t\t\t\tk = i\n\t\t\t\tbreak\n\t\tbases.insert(k + 1, c)\n\treturn bases\n\ndef isTypeInBases(cls, bases):\n\t\"\"\"\n\tReturn True if cls can be obtained from one of the bases in bases\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn reduce_inplace(classes, lambda x: min(x), lambda x, y: x.__add__(y))\n\n\ndef get_class_list(objectClass, method):\n\t\"\"\"\n\tGet the class list of a method\n\t\"\"\"\n\tif method == \"getClassType\":\n\t\treturn Object.public._get_class_list(objectClass)\n\telse:\n\t\treturn Object.public._get_class_list(Object.getClass(objectClass).classLoader)\n\n\ndef get_class_list_names(objectClass, method):\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tminimal_classes = []\n\t# keep a pointer array for faster lookups\n\tpointer = []\n\tfor i in range(len(classes)):\n\t\tpointer.append(-1)\n\t# first, for each class, find the class it has no direct parent class\n\tfor i, className in enumerate(classes):\n\t\tif pointer[className] < 0:\n\t\t\tminimal_classes.append(className)\n\t\t\t# the class itself is a direct parent class, skip\n\t\t\tif className == classes[className]:\n\t\t\t\tcontinue\n\t\t\t# keep going"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif classes is None:\n\t\treturn classes\n\telse:\n\t\twhile True:\n\t\t\tnew_classes = []\n\t\t\tfor cls in classes:\n\t\t\t\tif cls not in new_classes:\n\t\t\t\t\tnew_classes.append(cls)\n\t\t\t\telse:\n\t\t\t\t\tidx = new_classes.index(cls)\n\t\t\t\t\tif idx == 0:\n\t\t\t\t\t\tnew_classes = classes\n\t\t\t\t\telse:\n\t\t\t\t\t\tnew_classes.pop(idx)\n\t\t\t\t\t\tif types.IsSubClass(cur1, cur2):\n\t\t\t\t\t\t\tnew_classes.pop(idx"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\toutcome = []\n\t\n\tdef addResult(cls):\n\t\tif not cls in outcome:\n\t\t\toutcome.append(cls) \n\t\n\n\tfor i in classes:\n\t\tfor j in classes:\n\t\t\tif i!= j:\n\t\t\t\tbase1 = set(i.mro())\n\t\t\t\tbase2 = set(j.mro())\n\t\t\t\t#print(base1, base2)\n\t\t\t\tif not set.intersection(base1, base2) in outcome:\n\t\t\t\t\taddResult(i)\n\n\treturn outcome\n\n\n\nif __name"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# allow for both method names+docstring and attribute names+docstring\n\titems = {}\n\tif all:\n\t\tnames = dir(self.iface)\n\telse:\n\t\tnames = self.iface.namesAndDescriptions()\n\n\tfor name in names:\n\t\t_, desc, _ = description(self.iface, name) or (None, None, None)\n\n\t\tif desc is None:\n\t\t\tcontinue\n\n\t\titems[name] = desc or ''\n\treturn items\n\t# # print '<InterfaceWrapper:%s>' % ("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif not all:\n\t\treturn self.__attrnames[:], self.__attrdescriptions[:]\n\telse:\n\t\ttry:\n\t\t\treturn self.__attrnameswithdescriptions[:]\n\t\texcept AttributeError:\n\t\t\tnames = self.__attrnames[:]\n\t\t\tdescriptions = self.__attrdescriptions[:]\n\t\t\tfor attr, info in self.methodImplementedIntrospection():\n\t\t\t\tnames.append(attr)\n\t\t\t\tdescriptions.append(info)\n\t\t\tself.__attrnameswithdescriptions = names"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self.__attr.keys()\n\telse:\n\t\tresult = filter(lambda x,m=self.__attr.keys(): x not in m,dir(self))\n\tresult.sort()\n\treturn result\n\n\nclass Element:\n\t\"\"\"\n\tThe foundation for Query validation.\n\tQueryValidation requires an element to exist on its own which in turn\n\trequires that its attributes are defined and that each attribute\n\trequired has a value. In addition, all the child elements returned\n\tby an interface must be instances of the defined element.\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tattrs = self.__attrs\n\tif all:\n\t\tresult = self.__attrs.copy()\n\telse:\n\t\tresult = {}\n\tfor key, value in self.__attrs.items():\n\t\tif isinstance(value, Method):\n\t\t\tif not value.__name__.startswith('_') and \\\n\t\t\t    hasattr(self, value.__name__):\n\t\t\t\tfunction = getattr(self, value.__name__)\n\t\t\t\tif isinstance(function, MethodType):\n\t\t\t\t\tfunction = function.im_func\n\t\t\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif not all:\n\t\treturn [x[0] for x in self.__namesAndDescriptions__]\n\n\tr = []\n\tfor item in self.__namesAndDescriptions__:\n\t\tr.append((item[0], item[1][0]))\n\tr.sort()\n\treturn r\n\ndef namesAndPorts(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn attribute names and port description defined by interface.\n\t\"\"\"\n\tif not all:\n\t\treturn [(x["}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\tresult_type = None\n\telse:\n\t\tresult_type = tuple\n\tresult = []\n\tfor desc in self.description():\n\t\t# FIXME: remove ignoreClass information\n\t\tif not desc.ignoreClass:\n\t\t\tresult.append((desc.key, desc.description % desc.args))\n\tif not all:\n\t\tif result:\n\t\t\treturn result_type(result)\n\t\telse:\n\t\t\treturn result_type([])\n\treturn result\n\n\t\ndef description(self): # pylint:disable=redefined-builtin"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self.__elementNamesAndDescriptions(all)\n\n    __name__ = 'ElementNamesAndDescriptions'\n\n\nclass Enumeration(Tuple):\n    \"\"\"\n    Inherits from Tuple except if an IInterface is passed into the\n    constructor it is uses that as the interface. Also, elements\n    in the Tuple become subclasses of Enumeration and so they can\n    also be iterable and have special functionality added.\n\n    @ivar name: The name of the Enumeration, not necessarily unique.\n    @ivar names: List"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\tresult = []\n\telse:\n\t\tresult = {}\n\tfor desc in self._v_attrs.values():\n\t\tif desc.__name__ not in result:\n\t\t\tresult[desc.__name__] = (desc.__doc__ or \"\")\n\t\telif all:\n\t\t\tresult[desc.__name__] += (desc.__doc__ or \"\")\n\t\telif desc.__doc__:\n\t\t\t_LOGGER.warning(\"%s.%s already in description and has docstring. It is also set to %r,"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\tnames = {}\n\t\tfor interface in self.__iro__:\n\t\t\ttry:\n\t\t\t\tnames.update(interface.namesAndDescriptions())\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\treturn names\n\telse:\n\t\tresult = {}\n\t\tif not self.__doc__:\n\t\t\ttry:\n\t\t\t\tdoc = pydoc.getdoc(self)\n\t\t\texcept: # may disappear in future Python versions\n\t\t\t\tdoc = None\n\t\telse:\n\t\t\tdoc = self.__doc__\n\t\tresult = {'__doc__': doc}"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self._namesAndDescriptions.copy()\n\n\t\tdesc = { '__doc__': 'MetaData for interfaces and implementations' }\n\t\tdescPy = desc.copy()\n\t\tif not desc:\n\t\t\tresult['__doc__'] += '\\n\\nNone'\n\t\telif not PY3 and isinstance(desc, unicode):\n\t\t\tdescPy['__doc__'] = desc\n\t\telse:\n\t\t\tdescPy = None\n\t\tself.__doc__ = desc\n\t\tresult['__doc__'] += '\\n%s"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tby_source = {}\n\tfor desc in self.itrdesc(all):\n\t\tname = desc.name\n\t\tsrc = desc.source\n\t\tif src not in by_source:\n\t\t\tby_source[src] = []\n\t\tby_source[src].append(name)\n\n\tfor src in by_source: # pylint:disable=redefined-loop-variable\n\t\tif src.interface.name == conf.host:\n\t\t\tconf.hostname = src.interface.name # this overrides some of the previous settings\n\t\t\tconf.host = src\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tret = []\n\tfor name, info in self._names.items():\n\t\tif info[0] is None or all:\n\t\t\tret.append(name)\n\treturn ret\n\n\t\nclass IInterface(Interface):\n\t\"\"\"\n\tInterface Class\n\tBase class for Interface classes.\n\tProvides very common behaviour and information.\n\t\"\"\"\n\n\tdef getName(self):\n\t\t\"\"\"\n\t\tGet the name of the interface\n\t\t\"\"\"\n\t\treturn self._name\n\n\tdef __init__(self, name, bases=()):\n\t\t\"\"\"\n\t\tCreate an Interface"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self.__attr.keys()\n\telse:\n\t\tresult = [attr for attr in self.__attr.keys()\n\t\t\t       if not attr.startswith('__')]\n\treturn result\n\n# ------------------------------------------------------------------------------\n\nclass IEnumerableMapping(Interface):\n    def __iter__():\n        \"\"\"Return iterable over the dictionary's items.\"\"\"\n\nclass IEnumerableMappingItems(IEnumerableMapping):\n    def __getitem__():\n        \"\"\"Get an item.\"\"\"\n\nclass IEnumerable"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tret = {}\n\tfor desc in providedBy(self.__class__):\n\t\tfor name in desc._getAttributeNames(all):\n\t\t\tret[name] = 1\n\treturn ret\n\ndef attributes(self): # pylint:disable=redefined-builtin\n\"\"\"\n\tFilter the attributes names defined by the interface\n\t\"\"\"\n\treturn list(self.names(all=1))\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self.interface.names\n\n\t\tinherit = None\n\t\tfor base in self.__bases__:\n\t\t\tif base.isequal(_marker):\n\t\t\t\tresult = inherit\n\n\t\t\tinherit = result\n\t\t\tfor k in base.names(True):\n\t\t\t\tif k not in result:\n\t\t\t\t\tresult[k] = None\n\t\treturn result\n\n\treturn self.interface.names\n\n\t#def names(self, all=False):\n\t#\t\"\"\"\n\t#\tReturn the name of a class, interface or constant defined by the interface"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self.__attrnames\n\t\tif result is None:\n\t\t\treal result = self.__iro__\n\t\t\t# Copy approach from our cache-busting re-builder,\n\t\t\t# and just slicing to be clear.\n\t\t\tresult = result[:]\n\t\t\tfor i in bcpython.iter():\n\t\t\t\tif i not in result:\n\t\t\t\t\tresult.append(i)\n\t\t\tself.__attrnames = result\n\t\treturn result\n\telse:\n\t\treturn self.__attrnames\n\t\nclass method(object): # pylint:disable="}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tresult = self.getBases()[0].names()\n\t\tfor c in self.bases:\n\t\t\tresult.extend(c.names())\n\t\treturn result\n\treturn self.names\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tout = self.__sorted_attributes - self.__query_attributes\n\t\tout.discard('query')\n\t\treturn out\n\telse:\n\t\treturn self.__query_attributes\n\n\t\nclass ElementBase(object):\n\t\"\"\"\n\tBase class for all elements in the specification. Elements are the basic building blocks of :doc:`pyatspi` interfaces.\n\t\"\"\"\n\t\n\tdef __init__(self, l, attributes = {}):\n\t\t\"\"\"\n\t\tConstruct new instance.\n\t\t\"\"\"\n\t\tself.l ="}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif not all:\n\t\treturn self.__attrmap.keys()\n\treturn _implements[self]\n\t\n\n\ndef providedBy(ob):\n    \"\"\"\n    Return the interfaces provided by value.\n\t\"\"\"\n    if isinstance(ob, SuperInterface):\n\tret = ob.getInterfaces()\n        for c in getmro(ob.__class__):\n            if c is not object:\n                for base in c.__bases__ :\n\t    \t    ret.extend(getattr(base, '__implements', ("}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\treturn self.__attrs.keys()\n\treturn self.__attrs.keys()[:]\n\n\nall_attrs = Attribute\n\n# Now implement attribute access directly using the attr() function\ntry:\n\t# The non-weak version of getattr\n\t__getattr__ = getattr\nexcept NameError:\n\t# Fall back to __getattribute__ instead\n\tdef __getattr__(self, name):\n\t\tif name == '_cls_inst':\n\t\t\traise AttributeError(name)\n\t\treturn get_attr"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif output is None:\n\t\toutput = []\n\t\n\tfor a in sequence:\n\t\tif isinstance(a, Decl):\n\t\t\t_append(output, (a.name, a))\n\t\telse:\n\t\t\tif isinstance(a, SpecificationBase):\n\t\t\t\tif not a.implements((\"declaration\",)):\n\t\t\t\t\traise TypeError(\n\t\t\t\t\t\t\"normalized arguments should not be specs: %r\"\n\t\t\t\t\t\t% (a,))\n\n\t\t\t_normalizeargs(a, output)\n\n\n#@-node:__init__\n#"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, types.TupleType):\n\t\tif (isinstance(output, types.TupleType)\n\t\t\tand len(output) == len(sequence)):\n\t\t\treturn output\n\n\t\tresult = list(sequence)\n\t\tfor i in range(len(result)):\n\t\t\tif not isinstance(result[i], tuple):\n\t\t\t\tresult[i] = (result[i],result[i])\n\telse:\n\t\tif (hasattr(output,'__len__') and output == sequence\n\t\t\tand not type(output) == types.StringType):"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\traise AssertionError()\n\treturn sequence\n\ndef _identitymethod(function, result):\n\t\"\"\"\n\tBind the function name and docstring for a function with no arguments.\n\t\n\tAlso change the function's name to a simple 'anonymous' function.\n\t\"\"\"\n\ttry:\n\t\tsetattr(result, '__name__', function.__name__)\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tresult.__doc__ = function.__doc__\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = (tuple, Declarions)\n\tif isinstance(sequence, output):\n\t\treturn sequence\n\tfor typ in output:\n\t\ttry:\n\t\t\tresult = typ(sequence)\n\t\t\tif result!= sequence:\n\t\t\t\treturn result\n\t\texcept TypeError:\n\t\t\tbreak\n\treturn sequence\n\ndef callspec(spec):\n\t\"\"\"\n\tCreate a call specification from a Declaration (used by\n\t:ref:`declaration factories<callable-declaration>`)\n\t\"\"\"\n\tdef decorator(decorated):"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif isinstance(sequence, Specification):\n\t\tsequence = sequence.interfaces\n\n\tif isinstance(sequence, Declaration):\n\t\tsequence = (sequence,)\n\n\tif hasattr(sequence, \"__iter__\") and output is not None: output +=(sequence,)\n\t\n\treturn output\n\n\n__all__ = [\n\t\"I\",\n\t\"Specification\",\n\t\"Interface\",\n\n\t\"ISimpleNamespacePackage\",\n\t\"IDelayedResolutionNamespacePackage\",\n\n\t\"namespaceKeywords\",\n\t\"packageKeywords\",\n\t\""}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\ttry:\n\n\t\tif output is None:\n\t\t\tfor value in sequence:\n\t\t\t\t_normalizeargs(value)\n\n\t\telse: # pragma: no cover\n\t\t\traise TypeError(\"'output' parameter must be None\")\n\texcept TypeError:\n\t\traise ValueError(\"Expected Sequence type, got %s.%s\" %\n\t\t\t(type(sequence), sequence) )\n\n\n\ndef spec(name, cls, *interfaces):\n\t\"\"\"\tReturn a factory function to build a single interface\n\tspec that will match a class.\"\"\"\n\treturn factory("}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = list()\n\tfor x in sequence:\n\t\tif isinstance(x, InterfaceClass):\n\t\t\tif not hasattr(x, 'inherit'):\n\t\t\t\tx = x._implicitImplements(IInterface)\n\t\t\toutput.append(x)\n\t\telif isinstance(x, Function):\n\t\t\tif getattr(x, 'abstract', False):\n\t\t\t\traise InvalidImplementation(\"Abstract methods cannot be overridden\", x)\n\t\t\tif getattr(x,'synchronous', None):\n\t\t\t\traise InvalidIm"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif type(sequence) in (types.StringType, types.InstanceType):\n\t\tt = type(sequence)\n\t\tx = {types.StringType: \"DottedName\",\n\t\ttypes.InstanceType: \"%s.%s\" % (sequence.module,sequence.__name__)}[t]\n\telse:\n\t\tx = [ _normalizeargs(s, output) for s in sequence ]\n\t\tif type(sequence) == type(Declaration): return x\n\tif output is None:\n\t\treturn x\n\t\n\tn = (x, output"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = []\n\t\t\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\toutput.append(arg)\n\t\telif isinstance(arg, list):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\t# we know that this is an InterfaceReference, or\n\t\t\t# Implements\n\t\t\tif InterfaceReference not in arg:\n\t\t\t\targ = DeclarationDecorator(arg)\n\t\t\toutput.append(arg)\n\treturn output\n\ndef _normalizeargs(sequence, output="}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\treturn (output or sequence)(_normalizeargs)\n\n\t#~ \"\"\"\n\t#~...\n\t#~ def _normalizeargs(args):\n\t\t#~ args2 = []\n\t\t#~ for arg in args:\n\t\t\t#~ arg = _normalizeargs(arg)\n\t\t\t#~ args2.append(arg)\n\n\t#~ return args2\n\t#~ \"\"\"\n\n\t#~ #def _normalizeargs(args):\n\t\t#~ args2 = []\n\t\t#~ for arg in args:\n\t\t\t#~ if isinstance("}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\tclass NoModule(Exception):\n\t\t\"Used to pass through.\"\n\t\tpass\n\n\tif os.environ.get(\"CPATH\", \"\"):\n\t\ttry:\n\t\t\tfrom os import readlink\n\t\texcept ImportError:\n\t\t\tfrom stat import *\n\n\t\tdef _is_syslibs_dir(path):\n\t\t\t''' Return true if path is the sys.prefix or/and/or/and/or the\n\t\t\tenvironment variable CPATH contains the given path.\n\t\t\t'''\n\t\t\tif not os.path.isabs(path):\n\t\t\t\tpath = os."}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\treturn ctypes\n\texcept ImportError:\n\t\treturn False\n\t\t\ndef getLibPaths():\n\t\"\"\"\n\tReturn paths where lib_swig.so can be found for swig to import C\n\tmodules.\n\t\n\tIt uses the Python-Unix-Standard find algorithm.\n\t\"\"\"\n\t\n\tl = []\n\timport os\n\tif sys.platform[:3] == \"win\":\n\t\t# Search in Windows registry for needed tools\n\t\timport _winreg as winreg\n\t\tregpath = \"Software\\\\"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\timport ctypes.util\n\t\treturn ctypes.util.find_library('crypto')\n\texcept ImportError:\n\t\treturn False\n\n\n#------------------------------------------------------------------------------\nclass py_c_crypto_engine(base_crypto_engine):\n\t\"\"\"\n\tA pycrypto \"fake\" crypto engine class.\n\n\tThis could be used as a foundation for another crypto engine.\n\t\"\"\"\n\n\t#==========================================================================\n\tdef __init__(self, msource=None, verbose=False):\n\t\t\"\"\"\n\t\tConstructor."}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tmod = _c_optimizations_available_mod()\n\ttry:\n\t\timport _C_optimizations\n\t\treturn mod\n\texcept ImportError:\n\t\treturn False\n\t\t\n_C_optimizations_available = _c_optimizations_available_init()\n\n\n# ------------------------------------------------------------------------\n#\n#   Debugging\n#\n# ------------------------------------------------------------------------\n\n#: A true or false value indicating if debugging should be turned on as\n#: well.\n_g_Debug_build = False\n\ndef _c_build_error():\n\t\"\"\"\n\tDisplay an error message if"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport _fife\n\t\treturn _fife.Optimization\n\texcept ImportError:\n\t\treturn None\n\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\ttry:\n\t\t\tif not ctypes.sizeof('P') == 8: # size_t is 4-byte on 32-bit systems, and 8-byte on 64-bit, with a difference in ctypes\n\t\t\t\traise SystemError('You appear to have an old Python installation, ctypes is broken')\n\t\t\tfrom ctypes import c_long\n\t\texcept:\n\t\t\traise SystemError('The size of a PYPY_LONG object must always be equal to sizeof(void*) * sizeof(long)' )\n\t\t\n\t\ttry:\n\t\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\timport warnings\n\twarnings.warn( \"Please use \" + __name__ + \".c\" )\n\treturn None\n\n\ndef _optimizer_c( c ):\n\t\"\"\"Return the object that contains the C optimization and error.\n\t\n\tThis requires that the optimizations and eroor be available\n\tin its parent package (i.e., that this module is in $PATH).\n\t\n\tThe error is an AssertionError.\n\t\n\tThis is to make sure that if the optimization and error are not\n\tavailable in the parent's module, one is not aware of"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tfrom. import _coptimizations\n\t\t\n\tif not _coptimizations.is_available():\n\t\traise ImportError(\n\t\t\t\"the C optimizations module is not available, \"\n\t\t\t\"and these optimizations are thus not \"\n\t\t\t\"available for this module %s\" % (self.__class__.__name__,))\n\n\treturn _coptimizations\n\ntry:\n\tcoptimizations_available = _c_optimizations_available()\nexcept ImportError:\n\t_coptimizations_available = None\n\t\n\n## \n## \tOptimization"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\t\treturn ctypes\n\texcept ImportError:\n\t\treturn None\n__c_optimizations_available = _c_optimizations_available()\t# The name changes so it can be checked later\n#------------------------------------------------------------\ndef _c_module_exists(name, mode, libname = None):\n\ttry:\n\t\t__import__(name, None, None, (name, mode))\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\tif libname is not None:\n\t\t\t_dllname = libname\n\t\telse"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t# We have to set the optimization to have \"no\" optimizations, as they\n\t# were already turned off by the above tests.\n\timport sys\n\n\tif \"Py_OptimizeFlag\" in cls._settings:\n\t\tpy_optimize_flag = sys._getframe(1).f_locals[\"m\"]\n\telse:\n\t\tpy_optimize_flag = sys.flags.optimize\n\t\t\n\tif py_optimize_flag > 2:\n\t\tprint \"Failed to load c optimizations.\"\n\t\treturn False\n\t\n\tcls._"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl.get(False):\n\t\treturn True\n\n\t# We haven't disabled it explicitly, so try to.\n\tif on_py2:\n\t\treturn PYPY2\n\n\t# It is a bit hacky, but on Python 3.5.0 on OS X 10.6 or some\n\t# other platforms, there are implementation details such that the\n\t# platform check reports `False` in all cases except for\n\t# OSX/10.6.  The implementation details may change in future\n\t# versions of Python.  This is very unlikely"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif nopython and os.environ.get('PURE_PYTHON', None) == '1':\n\t\treturn False\n\telse:\n\t\tfrom numba.pycc import CC\n\t\treturn not isinstance(CC.compiler, CC.PyPyCompiler)\n\n\ndef try_c_compile(sig, locals={}, flags=(), cache=None):\n\t\"\"\"\n\tAttempt to compile the function with C signatures.\n\tLocals must be 'pycc_kwargs'\n\t\"\"\"\n\tflags = list(flags)\n\t# Ensure"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif Py_ENABLE_SHARED:\n\t\treturn True\n\telif getattr(sys, 'pypy_version_info', None) is not None:\n\t\treturn False\n\telse:\n\t\t# PyPy's C implementation is only tested with PYPY/CPython,\n\t\t# so always enable it there.\n\t\treturn _use_c_impl\n\n\n# These types are new in Python 3\nclass SimpleNamespace:\n\t_types = total_ordering\n\tdef __init__(self, **kwargs):\n\t\tself.__dict__."}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn _has_c_impl() and not PYPY and not PURE_PYTHON and _use_c_impl()\n\n\n_use_c_impl_msg = \" Use the generated C extensions (%s)?\"\ndef _use_c_impl(msg=_use_c_impl_msg):\n\t\"\"\"\n\tReturn True if we should try to use the C implementations.\n\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_has_c_"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif PUYPY_AND_ABOVE_PYPY_419:\n\t\treturn False\n\t\n\tpure_python = _use_c_impl.get()  # pylint: disable=no-member\n\treturn (PURE_PYTHON and pure_python) or (not pure_python) or \\\n\t\t   IS_WINDOWS\n\n\n# These are of the exact types of objects Python will try to optimize over\n# (in CPython's C optimization, their types are mostly ambiguous).\nTYPE_COPY_BUILD = (bytes_, by"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tis_cpython = _is_cpython()\n\tuse_c_impl = use_c() or PURE_PYTHON\n\n\t# If we're not on CPython or otherwise not on Cython, don't try any\n\t# CPython optimizations. Though, for Jython and Python internal\n\t# implementations, we still want to check if we're on non-Cython\n\t# platforms\n\tif not use_c_impl:\n\t\treturn True\n\n\telif is_cpython or not has_c_implementation():\n\t\t# For now, we only"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl is True:\n\t\treturn _is_pypy or os.environ.get('PURE_PYTHON') is None\n\telif _use_c_impl is False:\n\t\treturn False\n\telse:\n\t\treturn _use_c_impl # pragma: no cover - this is only reachable on user-request\n\n\ndef detect_api_version(kwargs): # real signature unknown; restored from __doc__\n\t\"\"\"\n\tdetect_api_version(kwargs)\n\t\n\tReturns a tuple of"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn not pypy and _use_c_impl == \"always\"\n\n\nclass ObjectConstantRenamer(object):\n\t\"\"\"\n\tConvert (almost) arbitrary class constants to Tuples of (name, value) whenever\n\tpossible.\n\n\tThis helps with garbage-collecting constants.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t# Note: keeping constants non-hashed avoids some \"too many instance\n\t\t# attributes\" lint warnings.\n\t\tself._constant_cache = {}\n\t\tself._constants = []\n\t\tself._type ="}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif hasattr(sys, 'pypy_version_info'):\n\t\treturn True\n\tif 'PURE_PYTHON' in os.environ:\n\t\tuse_c = os.environ['PURE_PYTHON']!= '0'\n\telse:\n\t\tfrom..sixjmp import is_jitted\n\t\treturn not is_jitted()\n\treturn use_c\n\ndef _get_optimize_args(coptimize, native, inline):\n\t# First, handle the -O and -OO flags by setting the optimization"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif hasattr(sys, 'pypy_version_info'):\n\t\t# i.e. PyPy\n\t\treturn True\n\treturn False if PURE_PYTHON and os.getenv('PURE_PYTHON', False) else False\n\n\ndef get_cython_version():\n\n\treturn '.'.join(map(str, pyximport.install.PYX_TO_CYTHON.values()))\n\n\nclass _Module(ModuleType):\n\n\tdef __setattr__(self, name, value):\n\t\ttry"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn {opt: not re.match('(^|_)__[a-zA-Z0-9_]+', opt)\n\t\tfor opt in _OPTIMIZATIONS_REQUIRED}\n\n\ndef _get_optimizations_flags(base, optimizations, ignored):\n\treturn tuple(set(\n\t\titertools.chain(\n\t\t\t*((\n\t\t\t\t'-' + opt.replace('__', '-') if ignored[opt] else '-q' + opt,\n\t\t\t\t)\n\t\t\t\tfor opt in sorted(chain(base, optimizations)))\n\t\t)"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\timport os\n\tif os.environ.get('PYTHONARC'):\n\t\treturn False\n\tif sys.version_info.minor >= 6:\n\t\treturn False\n\textra = ('exceptions', 'long', 'longlong','memoryview', 'thread')\n\treturn {n for n in _c_optimizations if n not in extra}\n\ndef _c_extensions_required():\n\t\"\"\"\n\tList of C extensions required to compile with Python.\n\t\"\"\"\n\textra = _c_extensions_needed()\n\toptimizations ="}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn exclusions.only_if(\n\t\tlambda xtra: not xtra._c_optimizations_required() )\n\nrequired_bytecode_tags = exclusions.only_if(\n\tlambda xtra: all(hasattr(x,'requires_bytecode') for x in xtra.optional_items)\n)\n\ndont_run_on_python2 = not PY2\ndont_run_on_py3 = not PY3\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn exclusions.only_if(\n\t\tlambda config: not config.options.c_optimizations_required,\n\t\t\"requirements are ignored for C optimizations\"\n\t\t)\n\n\ndef sqlalchemy_12(config):\n\t\"\"\"\n\tDon't inherit the test of the version before 12 for the 12 onwards\n\ttest.\n\n\t.. versionadded:: 1.2.0\n\n\t\"\"\"\n\treturn exclusions.skip_if(\n\t\t\tlambda v: v not in (12, ),\n\t\t\t\"SQLAlchemy 1.2 or greater is required for SQLAlchemy 1."}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tglobal _c_optimizations_required\n\t_c_optimizations_required = not _c_optimizations_required\n\ndef ffiplatform_noop(s):\n\t\"\"\"Hack to prevent recompilation, just return a string.\"\"\"\n\treturn s\n\ndef _rawffi_module_getattr(set_):\n\tglobal ffi_warned\n\n\tname = _byte_string(set_)\n\tif \".\" not in name:\n\t\traise AttributeError(\"__class__\")\n\tklass = name.split('.')[0]\n\t"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn exclusions.open()\n\n\ndef _c_optimizations_available():\n\t\"\"\"\n\tChecks if all of the implementations are at least as good\n\tas or better than as advertised.\n\n\tThis check will let us know if we are under-optimized for\n\tsomething.\n\n\t\"\"\"\n\treturn exclusions.open()\n\n\ndef _have_required_dynamodb_features():\n\treturn exclusions.skip_if(\n\t\tlambda config: not config.db.has_table('dynamodb'),\n\t\t\"dynamodb tables disabled by"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn conf.env.CC_TGT_F == \"/usr/bin/ppc64-linux-opt\"\n\ndef _asm_if_needed(conf, src):\n\tif src.parent.get_tgt_name() == \"cc\":\n\t\tconf.check_asm_isdir(src, \"as\")\n\ndef configure(conf):\n\t\"\"\"\n\tDetect the flags in the machine,\n\tand sets the flags into conf.env.CFLAGS and conf.env.ASFLAGS.\n\t\"\"\"\n\tif (conf.env.DEST"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn set() ## This is the same as `_optimizations_required` above.  Its docstr\n                 ##  speeds up Sphinx\n                 \n# A string value that the user can use to disable optimizations; these optimizations\n# are turned off by default.\n_optimizations_disabled = \"NONE\"\n\n# A list of optimizations that the user requires.\n_optimizations_required = set([\"SIMD_FFT\", \"SIMD_SUB\", \"SIMD_AVX2\", \"SIMD_MMX\"])\n\n# A set of optimizations that the"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn []\n\ndef _c_optimizations_required():\n\t\"\"\"\n\tReturns the required optimizations for CFFI.\n\t\"\"\"\n\tif os.name == \"nt\":\n\t\t# These are the same optimizations on Windows, except we use\n\t\t# stdcall calling convention on Windows.\n\t\treturn [\"win32\"]\n\telse:\n\t\treturn CFFI_OPTIMIZATIONS\n\ndef _c_post_init_code():\n\treturn \"\"\"\n\t#if PY_MAJOR_VERSION >= 3\n\t\tpybind_call_function_retained"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn \"; \".join(\n\t\topt for opt in _c_optimizations_required if opt not in _c_optimizations_ignored\n\t)\n\n\n_simple_string_optimizations = frozenset(\n\t[\n\t\t\"fast\",\n\t\t\"for\",\n\t\t\"inline\",\n\t\t\"inline_everything\",\n\t\t\"inlineeverything\",\n\t\t\"inline_fast\",\n\t\t\"inlineeverything\",\n\t\t\"minimal\",\n\t]\n)\n\n_c_special_compile_time = {}\n\"\"\"\n\tDictionary used for special compile-time"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn PURE_PYTHON\n\t\n\nif _use_c_impl() is True:\n\tfrom gi.repository import GtkGL as gtk\n\n\tclass _Context: # pylint: disable=useless-object-inheritance\n\n\t\t\"\"\"\n\t\tWrapper around GI's context.\n\t\t\n\t\tThis is a bit naughty: we have chosen to nest the\n\t\t:class:`Gtk.Widget` abstraction so as to make it easier for the user\n\t\tto subclass :class:`Gtk.Widget`.\n\t\t\n\t\tThis type"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn not _use_c_impl\n\n\nclass CodeObjectArgReprPrinter(argparse.HelpFormatter, argparse.ArgumentDefaultsHelpFormatter):\n\t\"\"\"ArgumentParser for printing code objects.\"\"\"\n\n\tdef __init__(self,\n\t             prog='Python',\n\t             usage='%(prog)s %(usage)s',\n\t             description='python %(prog)s%(epilog)s\\n',\n\t             formatter_class=argparse.RawDescriptionHelpFormatter,\n\t             indent_"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn True if _use_c_impl() else False\n\n\ndef _wrap_functions(src_lines, c_src_dict):\n\tprint(\"*** WRAPPING FUNCTIONS\")\n\tfuncs = []\n\n\tfor func in src_lines:\n\t\tfunc_name = get_name_from_src(func)\n\t\tif func_name in c_src_dict:\n\t\t\tfuncs.append(func)\n\t\n\treturn funcs\n\n\ndef init():\n\treturn None\n\n\n\ndef print_output(func):\n\t\"\"\"\n\t\tC"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t# No Python? Then no C.\n\tif not _use_c_impl:\n\t\treturn False\n\tif _PURE_PYTHON:\n\t\treturn False\n\tif (sys.flags.b_platform == 'darwin' and (\"macos\" in platform.mac_ver()[0] or\n\t\t\t\t\t\t\t\t\t\t\t\t\"aix\" in platform.system().lower())):\n\t\tif sys.maxsize > 2**32: # 64bit Python?\n\t\t\treturn False\n\treturn True\n\n_BOOL_OPTIONS = ('c_strict_ali"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn not PURE_PYTHON\n\ndef _make_pure_param(name, default, doc, method, options):\n\t\"\"\"\n\tMake a new `Parameter` from the name, default, doc and options\n\tthis is used to implement ``makeParameters`` internally\n\t\"\"\"\n\tif PY_VERSION < (3, 7):\n\t\tcls = _Parameter2\n\telse:\n\t\tcls = _Parameter3\n\treturn cls(name, default, doc, method, options)\n\nmakeParameters = mtd(object,\n\t\"\"\"A lazy"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl(True)\n\n\ndef _c_optimizations_enabled():\n\t\"\"\"\n\tReturn a true value if C optimizations are enabled for the process.\n\t\n\tThis uses the ``USE_C_OPTIMIZATIONS`` macro as documented in `_enable_c_optimizations`.\n\tIf the value of ``USE_C_OPTIMIZATIONS`` is ``True``, we need to\n\tcheck the ``PURE_PYTHON`` variable as well\n\t\"\"\"\n\treturn _use_c_impl(False) or _"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl() is False:\n\t\treturn False\n\telif PURE_PYTHON:\n\t\treturn False\n\n\tif sys.platform == 'cli':\n\t\treturn True\n\t\n\timport pycparser\n\tversion_str = \".\".join(pycparser.__version__.split(\".\")[:2])\n\treturn version_str >= \"2.18\"\n\ndef _c_optimizations_present():\n\t\"Return a true value if C optimizations are present.\"\n\ttry:\n\t\timport warnings\n\t\twarnings."}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl or getconfig(PURE_PYTHON)\ndef _c_optimizations_supported():\n\t\"\"\"\n\tReturn a true value if the C optimizations are supported.\n\t\"\"\"\n\treturn (_use_c_impl or getconfig(PURE_PYTHON)) and \\\n\t\t   numpy.__version__ >= '1.10.0'\n\ndef _is_ok_for_c_declaration():\n\t\"\"\"\n\tCheck that the code is acceptable to use C.\n\n\t.. note:: The :func:`"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn use_c_impl.PURE_PYTHON or (use_c_impl.use_c_impl and use_c_impl.lto_flags)\n\n\ndef _c_unsupported_feature_message():\n\t\"\"\"\n\tReturn just a note about C optimizations.\n\t\"\"\"\n\tif _use_c_impl.PURE_PYTHON or (not _use_c_impl.use_c_impl and not _use_c_impl.lto_flags):\n\t\treturn \"\"\n\n\tc_unsupported_features = {"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif not _use_c_impl.get():\n\t\treturn False\n\treturn bool(_use_c_impl() & PURE_PYTHON) or _pure\n\n\ndef _should_compile_method(method):\n\tif not callable(method):\n\t\treturn False\n\tname = method.__name__\n\n\t# Skip internal methods.\n\t# if not name.startswith('_'):\n\tif not name.startswith('_'):\n\t\treturn True\n\treturn False\n"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tdef keys(self):\n\t\"\"\"\n\tReturn the keys of the histogram, as a tuple.\n\tBucket labels can be retrieved as:\n\n\t>>> h = Histogram(10)\n\t>>> keys = h.keys()\n\n\tThis is sorted by bucket_width, in increasing order (i.e.\n\t``keys[0]`` is the label for the first of the 10 buckets/bins that the\n\thistogram uses.)\n\t\"\"\"\n\tdef items(self):\n\t\"\"\"\n\tReturn the tuple of the histogram's items.\n\tThis is sorted"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tdef render(self, index):\n\t\"\"\"Render each histogram item\"\"\"\n\tpass\n\n\tdef set_bin(self, index=-1, **args):\n\t\"\"\"Set a new bin to the histogram\"\"\"\n\tpass\n\n\tdef set_range(self, first=-1, last=-1, step=-1):\n\t\"\"\"Set a new data range of the histogram\"\"\"\n\tpass\n\n\tdef set_selected_bins(self, selected=[], hist=False):\n\t\"\"\"\n\tSet the bins *selected* for plotting.\n\tYou may also access the currently"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.reset()\n\n\ndef print_histo(self, fname, **kwargs):\n\"\"\"\n\tReturns a formatted text with a histogram.\n\n\tSee *binlabels* argument to adjust the formatting and\n\tthe size of resulting columns.\n\n\tDefault values for width and height are *100* and *25* respectively.\n\t\n\t*fname: filename or StringIO object.\n\t\t\tfile in which the text should be written. See keyword *file*\n\t\t\tof *matplotlib.pyplot.savefig()*.\n\n\tOther keyword arguments are passed to *"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self.is_empty:return\n\n\tself._contexts[:] = self._new_contexts\n\n\tfor bc in self.bins.values():\n\t\tbc.reset()\n\ndef isempty(self):\n\t\"\"\"\n\tIndicate if the current context is empty.\n\t\"\"\"\n\treturn self.is_empty\n\ndef isfull(self):\n\t\"\"\"\n\tIndicate if the current context is full.\n\t\"\"\"\n\treturn len(self)-1 == self.max_slots\n\ndef _add_to(self, data, key_"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.context = {}\n\treturn {'value': 'histogram.reset()'}\n\n@public\ndef xlim(*args):\n\t\"\"\"\n\tSet xaxis limits.\n\n\tExample:\n\t\tf.xlim(10,.2)\n\t\n\tArguments (in pixels):\n\t\tleft -- the new minimum value of the xaxis\n\t\tright -- the new maximum value of the xaxis\n\t@return:\n\t\t{\"left\":<val>, \"right\":<val>}\n\t\n\t\"\"\"\n\t# return locals()\n\treturn {'value':"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tpass\n\ndef set_color(self, color):\n\t\"\"\"\n\tChanges the color of histogram bars.\n\t\n\tArguments\n\t---------\n\t- **color**, 3-sequence or RGBA\n\t\n\t.. note::\n\t\n\t\tThe arguments have to be either a :ref:`color <colors-html>` or\n\t\tcan be RGBA (with some exceptions). The exact list of accepted\n\t\tvalues and exceptions are described in the corresponding descriptions\n\t\tof the individual classes.\n\t\"\"\"\n\tpass\n\ndef _get_color_rgba"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif initial_value:\n\t\treturn self.histogram: dict = {k: v for k, v in self.bins.items()}\n\telse:\n\t\treturn make_bins(self, hist)\n\n\t\"\"\"\n\tCount the number of occurrences for each value and add to a dict.\n\t\t\n\tIf *hist* is not a dict, it is instead treated as an iterable object of bins.\n\tWhen added to a non-empty dict, the bins with the same key are aggregated,\n\tso that counting the number of occurrences at each bin is equivalent to"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tp = _default_context.__class__()\n\tp._bins = _default_bins\n\tp.reset()\n\treturn p\n\ndef save(path):\n\t\"\"\"Save Context.\n\n\tSaves current parameters in current context and returns\n\tthe path of the saved file.\n\t\n\tUseful for saving histogram context to restore later.\n\t\"\"\"\n\timport pickle\n\tif not os.path.isdir(path):\n\t\tos.mkdir(path)\n\twith open(path + '/histogram.pkl', 'wb')"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\treturn _HKIPcamera._Mat_histogram_resetImpl(self)\n\ndef empty(self):\n\t\"\"\"\n\tEmpties and destroys histogram (memory buffer).\n\t\"\"\"\n\treturn _HKIPcamera._Mat_emptyImpl(self)\n\ndef is_empty(self):\n\t\"\"\"\n\tReturns true if histogram is empty. It doesn't use the\n\thistogram at all.\n\t\"\"\"\n\treturn _HKIPcamera._Mat_is_emptyImpl(self)\n\ndef is_full(self):\n\t\"\"\"\n\tTests if"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\treturn _moduleconnectorwrapper.PyTango_Histogram_reset(self)\nPyTango_Histogram_swigregister = _moduleconnectorwrapper.PyTango_Histogram_swigregister\nPyTango_Histogram_swigregister(PyTango_Histogram)\n\nclass PyTango_FloatVector(object):\n    \"\"\"Proxy of C++ Tango::FloatVector class.\"\"\"\n\n    thisown = _swig_property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\tdef __init__(self, graph, separator=\",\", header=None):\n\t\tself.graph = graph\n\t\tself._is_data = lambda x: isinstance(x, dict)\n\t\t\n\t\tif header is None:\n\t\t\theader = \"x,y,z,t,r\\n\"\n\t\tself._write(header, separator)\n\t\n\tdef __iadd__(self, other_header):\n\t\tself._write(str(other_header), separator=';')\n\t\t# to support +=\n\t\treturn self\n\t\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport string\n\tif header is None:\n\t\theader = \"\"\n\t# TODO: add to sequence the separator\n\toutput = []\n\tkeys = self.header_keys()\n\tfor coord in self.points():\n\t\tn_values = len(coord)\n\t\tvalues = []\n\t\t# Add coordinate in new line\n\t\tfor coord_values in range(0,n_values):\n\t\t\tvalues_list = self.header_values(keys, coord_values)\n\t\t\tif coord_values == 0:\n\t\t\t\t# Write value in header\n\t\t\t\toutput.append("}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif isinstance(self.points, list):\n\t\treturn \",\".join([separator.join([str(p.get_coord()[i]) for i in range(len(p.get_coord()))]) for p in self.points])\n\telse:\n\t\tp = self.points.get_coord()\n\t\tif p:\n\t\t\treturn \",\".join([separator.join([str(p[i]) for i in range(len(p))]) for i in range(len(p))])\n\t\treturn \"\"\n\"\"\"\n\t.. deprecated:: 0"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t#from lena.output import ToCSV\n\n\t#if type(iterable)!= Iterable:\n\t#\tif type(iterable)!= type([]):\n\t#\t\traise TypeError(\"'%s' is not Iterable. \"\n\t#\t\t%str(type(iterable)))\n\t#converter = ToCSV(self, separator, header)\n\tconverter = to_csv\n\t\n\treturn converter(iterable, separator, header)\n\n\n#..............................................................................\ndef _get_type_name(obj,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\tif header is None:\n\t\tyield header\n\t\n\tfor point, value in zip(self.points, self.values):\n\t\tvalue_strings = [\"%d\" % elem for elem in value]\n\t\tyield separator.join([\"%f\" % point.x, \"%f\" % point.y, ] + value_strings)\n\n\n\nclass ToCSV(lena.output.OutputGenerator):\n\t\"\"\"\n\t.. seealso:: :class:`lena.output.OutputGenerator`, :ref:`csv`\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise RuntimeError(\"deprecated\")\n\"\"\"\t\n\n\n# Output\n\nclass BaseOutput(object):\n\t\"\"\"\n\tFor more information about supported formats, see the\n\t:attr:`~.output.FORMATS`.\n\t\"\"\"\n\t\n\t#: Format identifier.\n\t_FORMAT = \"\"\n\t\n\t#: Default *separator* separator.\n\t_DEFAULT_SEPARATOR = \"\"\n\t\n\t#: Default *header* header.\n\t_DEFAULT_HEADER = None\n\t\n\tdef __init__(self, fd"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn get_data(self).to_csv(separator, header)\n@property\ndef _as(self):\n\t\"\"\"\n\tThe underlying :class:`Graph` or :class:`GraphList`\n\tor similar type.\n\t\"\"\"\n\treturn self._graph\nif _PY3K:\n\t_as.func_name = \"__func__\"\n#\n#\ndef tostring(separator=\" \", output = sys.stdout):\n\t\"\"\"\n\tConvert graph's points to string.\n\t\n\t*separator* is the delim"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\targs = (header, )\n\theader = args[0]\n\treturn __converter(self, header, self.__iter__,\n\t\t\t   converter=lambda _graf, *_vargs, **_kwargs: (_graf._list, ))\n\ndef to_xml(separator=\", \"):\n\t\"\"\"\n\tConvert graph's points to XML.\n\t\n\t*separator* is separator to separate the part.\n\t\n\tTo convert :class:`Graph` to XML in a Lena sequence,\n\tuse :class:"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\timport warnings\n\tfrom. import lena_globals\n\timport lena.lib.utils\n\t\n\tif not isinstance(separator, str):\n\t\traise TypeError(\"'{}' is not a valid type for separator\".format(separator))\n\t\n\tif separator not in (\"\\t\", \",\", \";\", \":\", \"|\", \"?\", \" \"):\n\t\twarnings.warn(\"'{}' is not a valid separator\".format(separator))\n\t\n\tif header in (\"\", None):\n\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn_explicit(\"method is deprecated\",\n\t                       category=DeprecationWarning,\n\t                       filename=\"\",\n\t                       lineno=0)\n\ttables = to_points_tables(self, header=header)\n\tfor k in list(tables.keys()):\n\t\ttables[\"{0}/\".format(k)] = \"\\n\".join(\"{0}{1}{2}\".format(k,separator,row)\n\t\t                                    for row in tables[k])\n\treturn \"\\n\".join("}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tdef __init__(self, data, coord, error_names):\n\t\tsuper(ErrorIndices, self).__init__() \n\t\tself.coord = coord \n\t\tself.data = data \n\t\t#self.error_names = error_names\n\t\tself.inds = np.array(coord.get_inds())\n\t\tif not(all(el in data.indices for el in self.inds)):\n\t\t\tself.error_inds =  ind_filter(data.indices, self.inds) \n\t\t\t#print("}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tnpts = self.data[coord_name].shape[0]\n\tif npts > 1:\n\t    return np.array(range(npts))\n\telse:\n\t    return np.array(\n\t\tnp.arange(self._data[coord_name].shape[0])\n\t\t\t\t  + (\n\t\t  self._data[coord_name] > self._vmin) * (npts - 1)\n       )\n\n\t    coord_min = self.get_data_coordinate(coord_name).min\n\t    coord"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tself.err_obj = self.goto.error_obj  # alias\n\t\n\t\n\t# if coord_name in {'x', 'y', 'R', 'r'}:\n\t# return 1  # TODO: Make more sophisticated\n\tif coord_name in ['r']:\n\t\treturn 2  # TODO\n\tif coord_name in ['x','y']:\n\t\treturn 0\n\t\n\ndef _load_dat(self, dat):\n\"\"\"\n\tLoad a single data file.\n\t\"\"\"\n\t\n\t_dat = self."}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t# if the user passed in a single coordinate value, assume that the error is\n\t# always 1.0 in that element\n\treturn [True, Ellipsis]\n\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\"\"\"\n\tDefine the method for plotting all the histograms in parallel.\n\"\"\"\n\ndef _plot_all_hist_in_parallel():\n\"\"\"\n\tUsing parallel, plot all of the histograms.\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_index=[]\n\tfor i in range(len(coord_name)):\n\t\tif pd.isnull(coord_name[i]) or coord_name[i]=='nan':\n\t\t\terr_index.append(i)\n\t\telif '$' not in coord_name[i]:\n\t\t\ttry:\n\t\t\t\tpd.to_numeric(coord_name[i])\n\t\t\texcept:\n\t\t\t\terr_index.append(i)\n\t\t\telse:\n\t\t\t\tpass     \n\t\telse:\n\t\t\terr_index.append(i)\n\t\t\t\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif isinstance(coord_name, tuple):\n\t\tcoord_names = coord_name\n\telse:\n\t\tcoord_names = (coord_name, )\n\n\tindices = (\n\t\tidx\n\t\tfor cn in coord_names\n\t\tfor idx in self._data[self.NAME].get_indices(cn, wrap=True)\n\t\tif idx < self.data_num\n\n\t)\n\treturn indices\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_map = self._error_map\n\tif coord_name in err_map:\n\t\tidx_list = err_map[coord_name]\n\telse:\n\t\tidx_list = []\n\treturn idx_list\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_indices = []\nif self._fit_type == 'full':\n\t# In a full fit these are the indices for the first model parameter\n\tr_indices = range(0,int(self._npar[0]))\n\terr_indices.extend(r_indices)\n\nif self._fit_type =='reduced':\n\t# In a reduced fit, these are the indices corresponding to the first\n\t# model parameters from the first fit model in the composite model\n\tn_params = int(self['n_params'][0"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_indices = []\n\tj = -1\n\tfor key in (self.coord_names + self.coord_names_inverted + \n\t\tself.index_names):\t\t\t\t\t\t\t\n\t\ttry: \t\t\t\n\t\t\tj = self.fid[key][coord_name]\n\t\texcept TypeError:\n\t\t\tj = self.fid[key][coord_name][0]\n\t\tif j in self.p_cnt_err:\n\t\t\tif self.logical_coords:\n\t\t\t\tif coord_name in self.index_names:\n\t\t\t\t\terror"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t\n\terror_indices = []\n\tif self.error_type == 'rmsd':\n\t\terror_indices = self.rmsd_to_index_dict[coord_name][0]\n\telif self.error_type == 'rmsd_z':\n\t\terror_indices = self.rmsd_to_index_dict[coord_name][0]\n\t\t\n\treturn error_indices\n\t\n\t\n\nif __name__ == '__main__':\n\timport sys\n\timport argparse\n\t\n\tparser"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n\n\ndef _update_subindices(self, index, subindex):\n\"\"\"\n\tUpdate subindices of this graph (both source and destination).\n\tCalled when some of its nodes are removed.\n\t\n\t\"source\" and \"dest\" nodes of the function graph are never\n\tremoved at one point of time (since they are not really\n\taffected by these graph deletion).\n\tIndexing may also be changed due to removal of other nodes.\n\tReturn the new indices of any subindex.\n\t\n\tThe indices returned are used to find the indices\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext.error.extend(graph._errors)\n\treturn\n\n################################################################################\n\nclass Graph:\n\t\"\"\"\n\tStore graph and its context\n\t\"\"\"\n\tdef __init__(self, graph, ctx):\n\t\tself._graph = graph\n\t\tself._context = ctx\n\n\t@property\n\tdef graph(self):\n\t\t\"\"\"\n\t\tName of the graph.\n\t\t\"\"\"\n\t\treturn self._graph\n\n\t@property\n\tdef context(self):\n\t\t\"\"\"\n\t\tDictionary mapping property name to value.\n\t\t\"\"\"\n\t\treturn self._"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself.log(\"_update_context\")\n\t# if not a flow graph, check if context is empty\n\tif not self.is_flow:\n\t    if not context.error and not context.value:\n\t        return\n\t\t\n\t    if context.error:\n\t    # this will have been done already\n\t        return\n\t\t\n\t# error_E_low appears here\n\tif False:\n\t    context.error = {\"x_low\": {\"index\": 2}}\n\telse:\n            new_error = {\"x_low\": {\"index\": 2"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\tif self.mode == \"subset\":\n\t\tcx_dict, subcx_dict = self.cx._subset_to_dict()\n\t\tnx_graph = self.flow.to_networkx(\n\t\t\t**{\"nodes_dict\": self.nx_nodes, \"edges_dict\": self.nx_edges}\n\t\t)\n\t\tself.to_edgelist(nx_graph)\n\t\tself.to_subnet(nx_graph, cx_dict, subcx_dict)\n\t\tself"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif getattr(self, '_data_cache', None) is None:\n\t\tself._data_cache = self.to_dict()\n\n\tn_errors = len(self.error.x_high)\n\tif not n_errors:\n\t\t# this makes value.y always equal to 0.0\n\t\t_value_y_prop = 'value_y'\n\t\t_error_y_prop = 'error_y'\n\t\tif self.y_high.empty:\n\t\t\t_value_y = _error_y = ''\n\t\telse:\n\t\t\t_"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\terror_E_low = graph.prop(\"x\")\n\terror_x_low = graph.prop(\"y\")\n\terror_x_low = graph.prop(\"z\")\n\"\"\"\n\tReturns the *name* of the error component corresponding\n\tto the low error of the graph. If there is only one \"error\", \n\treturns the name of the error without 'error_'.\n\"\"\"\n\n\"\"\"\n\t*error_E_high* is appended with indices of errors.\n\tExample subcontext for a graph with"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif not isinstance(self.context, GraphContext): raise TypeError\n\t\t\n\tif isinstance(self.context, GraphSubContext):\n\t\tcontext += self.context\n\t\tself.context = self.context.context.copy()\n\telse:\n\t\terror = []\n\t\tfor i in range(self.npoints):\n\t\t\tind = i + 1 + self.first_index\n\t\t\tif hasattr(self, \"error_\" + str(ind + 1)):\n\t\t\t\terror += [self] + list(getattr(self, \"error_\" + str("}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tpass\n\t\n\"\"\"===============================================================================\nThe list of methods of this object does have special meaning.\n\n(i)   *x*, *y* and *z*:\n    the next two are readonly properties that represent the current\n    values of *x*, *y* and *z*. To change them use *set_xyz()* method instead.\n    x, y and z are readonly so that they can be used for error propagation.\n    For other attributes they are read/write, but they are protected from\n    inheriting from nodes, so if a node inherits"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t_update_ctx_subcontext(context, 'error')\n\nclass ToHTML(object):\n\t\"\"\"\n\tRender a graph as HTML.\n\t\n\tThe returned HTML is not a valid HTML rendering\n\tby humans. It uses the same HTML 4 CSS formatting\n\tbut should render as much of an HTML page as possible.\n\t\n\t\"\"\"\n\tdef __init__(self, graph_name, **kw):\n\t\t\"\"\"\n\t\t*graph_name*   name of the graph\n\t\t\n\t\tExtra kw arguments are passed directly to the constructor\n\t\tof :class:"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.label:\n\t\tcontext.append('# graph name = \"{}\"'.format(self.name))\n\t\tif self.label == '*':\n\t\t\tcontext.append(\"# * is a placeholder\")\n\telif self._subcontexts:\n\t\tcontext.append('# graph name = \"{}\"'.format(self.name))\n\t\tif self.label == '*' and self._label_placeholder:\n\t\t\tcontext.append(\"# * ({}) and following {} lines are ignored\".format(\n\t\t\t\t\t\t\t\tself._subcontexts,\n\t\t\t\t\t\t\t\tlen(self._label_"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tpass\ndef inv_gaussian(scale, mean, var):\n\"\"\"\n\tCompute variograms for given scale and variance.\n\t\n\t*scale* is the distance metric, as provided by\n\t:class:`.inv_gaussian_scale`.\n\t*mean* and *var* are the mean and variance of\n\tthe Gaussian, resp.\n\t\"\"\"\n\tpass\ndef inv_gaussian_scale(scale, mean, var):\n\"\"\"\n\tCompute Gaussian scale and variance for given\n\tdistance metric *scale*.\n\t\n\t*var* is"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tpass\n\ndef legendre():\n\t\"\"\"\n\tCalculate the Legendre polynomials.\n\t\"\"\"\n\tpass\n\ndef loadData():\n\t\"\"\"\n\tCompute the polynomial basis for a 1D :mod:`.barytree`\n\tdata model, using a 2D coordinate set in *data*.\n\t\"\"\"\n\tpass\n\ndef main():\n\t\"\"\"\n\tInitialization of code, usage and example of functions.\n\tRun the example:\n\n\t* from matplotlib import pylab, mlab\n\t* mlab.m"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tN = len(bins)\n\ttotal = 0.\n\tfor i in range(N-1):\n\t\tlow, high = edges[i], edges[i+1]\n\t\t# For a normal distribution with mu=2, we have\n\t\t# pdf(2)=0.25, cdf(2)=0.25.pdf(1)=cdf(1)=0.5.\n\t\t# We can only calculate integral if we ignore\n\t\t# the PDF of the lower part of the range (2,3)\n\t\t# thus the condition below the line of code below in"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn (bins[1:] - edges[:-1])\n\nclass histogram:\n\t\"\"\"\n\tClass representing the histogram for a single\n\tdimension of a 2D array.\n\t\n\t`n` is the number of bins and `edges` contains\n\tboth the edge values and the number of points inside\n\teach bin. `edges[n-1] > histogram.edges[n]` iff there\n\tare data points in the final bin.\n\t\n\tAttributes:\n\t\n\t`n`:\n\t\tThe number of bins\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tpass\n\n\ndef ln_scale(log_data):\n\"\"\"\n\tCompute the logarithmic scale.\n\t\n\tA logarithmic scale allows for easy visual interpretation \n\tand comparison of the data.\n\t\"\"\"\n\tpass\n\n\ndef log_scale(d):\n\"\"\"\n\tConvert any numeric variable to a logarithm\n\t\n\tThis is applied via :func:`magnitude`.\n\tThis function returns *None* for\n\t*vartype* `None`.\n\t\"\"\"\n\tpass\n\n\ndef log_to_"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tdef int_edges(vals, bins):\n\t\t\"\"\"Returns integration integral for given values.\"\"\"\n\n\tdef int_vals(vals, bins):\n\t\t\"\"\"Returns integration integral for given values.\"\"\"\n\n\treturn (int(int_vals(bins.bin(left_bound)).sum() - int_edges(vals, bins) - int(int_edges(vals, bins).sum()))/bins.bin_width)\n\n@jit\ndef integral_2d(data, bx, by):\n\"\"\"Calculates 2D integral of the histogram."}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tedges = numpy.array(edges)\n\tif(not numpy.isfinite(bins).all() or\n\t\tnot numpy.isfinite(edges).all() or\n\t\tnot numpy.isfinite(bins).all()):\n\t\treturn None\n\tn = len(edges)\n\tbins = numpy.array(bins)\n\tedges = numpy.array(edges)\n\tbins_idx = (edges[:-1] < bins)\n\treturn numpy.sum(numpy.diff("}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tret = 0.0\n\tfor i in range(len(bins)):\n\t\tret += (\n\t\t\t(edges[i+1] - edges[i]) *\n\t\t\t(interp(bins[i], edges, weights))\n\t\t)\n\treturn ret\n\n\n@vectorize(target=args.target)\ndef interp(x, bins, y):\n\"\"\"\n\tCompute the interpolation value.\n\t\n\t:param x: the x value\n\t:param bins: the bins values\n\t:param y: the values\n\t\"\"\"\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\ti = np.zeros(2)\n\ti[0] = edges[1] - edges[0]\n\ti[1] = np.trapz(bins[1:], x=bins[:-1])\n\treturn i\n\n\n\"\"\"\n**Histogram with fixed edges - (1)** *counts_edges*\n\nThe same histogram, with data from *counts*, which has some edges, can be \nshown as \n\n..math::\n\n\th_c = \\sum_{i} \\delta_{cg}\n\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\t# number of bins\n\tnb = len(bins)\n\tif nb>1:\n\t\tbimin = np.where(bins[1:]!=bins[:-1])[0][0] \n\telse:\n\t\tbimin = 0\n\tbimax = nb-1\n\t\n\t# indices of edges for integral\n\ti1 = np.where(edges>=bins[0])[0][0]\n\ti2 = np.where(edges>bins[bimin])[0]["}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn (not is_source_seq(seq)) and seq.tag == \"FillRequest\"\n\ndef is_fill_request_cx_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestCXSeq.\n\t\n\tTrue only if it is a FillRequestCXSeq, or a fill request body;\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn (not is_source_seq(seq)) and seq.tag == \"FillRequestCXSeq\"\n\ndef is_get_response("}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\treturn isinstance(seq, (FillRequest, FillRequestSeq)) or \\\n\t\t   seq.__class__ in fill_request_seq\n\ndef is_source_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a SourceRequestSeq.\n\t\n\tTrue only if it is an object of \n\t(or contains one of)\n\tthe SourceRequestSeq types.\n\t\"\"\"\n\t\n\treturn (isinstance(seq, SourceRequestSeq) or \n\t\t   seq.__class__ in source_request"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tisinstance(seq, FillRequestSeq) or \\\n\t    (hasattr(seq, \"source\") and \\\n\t\t(not isinstance(seq.source, SourceSeq)))\n\n\n\ndef is_file_request_id(id):\n\"\"\"\n\tTest whether *id* is an integer and in file-request-ids list.\n\t\n\tTrue only if id is a non-negative integer.\n\t\n\tNote that any file-request ID can be in the FileRequest list\n\teven when the list is empty.\n\t\"\"\"\n\treturn isinstance(id"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tfor p in seq:\n\t\tif isinstance(p, Tag):\n\t\t\ttag = p.tag\n\t\telse:\n\t\t\ttag = p\n\t\t\t\n\t\tif tag in (F_BEGIN,F_END,F_SOURCE):\n\t\t\treturn False\n\t\t\t\n\treturn True\n\ndef get_fill_request_tags(seq):\n\"\"\"\n\tReturn a copy of the fill_request element's tags\n\tfrom the end of the sequence *seq*.\n\nReturn tuple of (s_seq,c_seq)\n\twhere the following hold:\n\t- *"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n# from bbQuantumFlow import printError\nimport sys\nfrom quspin.basis import spin_basis_1d\n# from quspin.operators import hamiltonian # operator class\nfrom quspin.operators import hamiltonian, exp_op # Hamiltonian class\nfrom scipy.sparse.csgraph import minimum_spanning_tree as mst # for MST\nfrom scipy.sparse import csc_matrix\n\n# import matplotlib.pyplot as plt\nfrom itertools import tee, isl"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\ttry:\n\t\tif fillRequestElt(seq) is None:\n\t\t\treturn False\n\texcept AssertionError:\n\t\treturn False\n\treturn True\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t# if it's a source, that can't be a fill request seq\n\tif seq.type == \"FillRequest\": return False\n\t# must be an array\n\tif not is_array(seq): return False\n\t# must contain at least one element\n\tlength = len(seq)\n\tif length == 0: return False\n\t# can't contain a fill request\n\tfor elem in seq:\n\t\tif elem.type == \"FillRequest\": return False\n\treturn True\n\n#---------------------------------------------------------------------\ndef is_request_response_seq(seq, msg_type"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif seq_type(seq) == seq_type(FillRequest):\n\t\treturn True\n\tfor e in seq:\n\t\tif is_fill_request_seq(e):\n\t\t\treturn True\n\treturn False\n\ndef is_header_req_element(e):\n\"\"\"\n\tTest whether *e* can be converted to a HeaderReq element.\n\tReturns True if this is exactly equal to HeaderReq\n\tso that a header element will result in a sequence of one element.\n\t\n\tFalse otherwise.\n\t\"\"\"\n\th = HeaderReq(e."}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn (seq[0] == FillRequest and (\n\t\tnot hasattr(seq, \"_FillRequests_\") or\n\t\tseq not in _FillRequests_\n\t\t) and not is_source_seq(seq))\n\n\ndef is_cancel_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a CancelRequestSeq.\n\t\n\tTrue only if it is a CancelRequest element or contains\n\tat least one such.\n\t\"\"\"\n\treturn (seq[0] == CancelRequest or seq[0] == CancelRequest(\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\ttry:\n\t\treturn seq.tag == '{4CCB823A-0AB2-11D3-A715-00A0C904EE0B}' \\\n\t\t\tor seq.tag.startswith('{83A33D50-27C5-11CE-BFD4-00400513BB57}') \\\n\t\t\tor seq.tag.startswith('{83A33D51-27C5-11CE-BFD4-00400513BB57}') \\\n\t\t\tor seq.tag.startswith('{"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn str(obj.__class__).find('fill_request_el') > -1 \\\n\tor str(obj.__class__).find('request_el') > -1\n\n\ndef is_check_request(obj):\n\"\"\"\n\tObject contains executable methods 'check' and'request'.\n\t\"\"\"\n\treturn str(obj.__class__).find('check_request') > -1 \\\n\tor str(obj.__class__).find('request') > -1\n\n\ndef is_read_request(obj):\n\"\"\"\n\tObject contains executable methods"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\ttry:\n\t\treturn obj[u\"fill\"] and obj[u\"request\"]\n\texcept KeyError:\n\t\treturn False\n\ndef is_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill','request' and'set_request'.\n\t\"\"\"\n\ttry:\n\t\treturn obj[u\"request\"]\n\texcept KeyError:\n\t\treturn False\n\ndef is_fill_el(obj):\n\"\"\"\n\tObject contains executable method 'fill'.\n\t\"\"\"\n\ttry:\n\t\treturn obj[u\"fill\"]\n\texcept KeyError"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (obj['_class'] == 'java.lang.Object') and (obj['_methods']['fill']!= None) and (obj['_methods']['request']!= None)\n\ndef has_fill_request_el_in_body(obj, body):\n\"\"\"\n\tObject has non-null _fill and _request methods.\n\t\"\"\"\n\tif is_method_el(obj):\n\t\treturn (obj['_methods']['fill'] in body) and (obj['_methods']['request'] in body)\n\t"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn bool(\"fill\" in dir(obj) and \"request\" in dir(obj))\n\n\n\n# if not is_fill_request_el(obj):\n#         logging.debug(\"\\t\\t\\t\\t\\tobj=%r\", obj)\n#         return True\n# logging.debug(\"\\t\\t\\t\\t\\tFilling request %s\", obj.__name__)\n# fill_request_el(obj)\n# logging.debug(\"\\t\\t\\t\\tFilled request %s\", obj.__name__"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif type(obj)==list and len(obj)>0:\n\t\treturn False\n\tif type(obj)==dict:\n\t\t#print('dict')\n\t\t\n\t\t#print(obj.keys())\n\t\treq_names=['fill','request']\n\t\tfor x in req_names: \n\t\t\tif not x in obj.keys():\n\t\t\t\t#print(x,' is not in keys ', obj.keys()) \n\t\t\t\treturn False\n\t\t\treturn True\n\treturn False\n\ndef is_fill_request(obj):return is_fill_request_el(obj"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (hasattr(obj, 'fill') and hasattr(obj,'request'))\n\n_FILL_OP_TYPE = 'FILL'\n_REQUEST_OP_TYPE = 'REQUEST'\n\n#---[ FILL METHOD ]-------------------------------------------------------------\n\ndef fill(obj, op = _FILL_OP_TYPE):\n\t\"\"\"\n\tExecute the 'fill' method of the given 'operaion' node/element\n\t\"\"\"\n\n\tobj.logger.debug('FILL method called on %s' % obj.__class__.__name"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn is_method_el(obj, \"fill\", \"request\")\n\ndef is_set_color_request_el(obj):\n\"\"\"\n\tObject contains executable methods'setColor' and'requestColor'.\n\t\"\"\"\n\treturn is_method_el(obj, \"setColor\", \"requestColor\")\n\ndef is_color_response_el(obj):\n\"\"\"\n\tObject contains recursive property 'color' with type Color and\n\tmethod'response'.\n\t\"\"\"\n\treturn is_classroom_object(obj, \"Color\") and is_property_el("}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn is_method_name(obj, 'fill')\\\n\tif'method' in obj else is_method_name(obj,'request')\n\n\"\"\"\n\tMethod description.\n\t\"\"\"\nis_method_description_el = lambda obj: lambda x: re.search(r'description\\/\\*=\\s?(#?)(?P<description>.*)', x, re.S)\\\nlambda x: re.search(r'declare\\/\\*=(\\s*)(?P<declare>.*)', x, re.S)\\\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif \"fill\" in obj.__dir__():\n\t\treturn True\n\tif \"request\" in obj.__dir__():\n\t\treturn True\n\n\treturn False\n\n\ndef is_fill_response(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and'response'.\n\t\"\"\"\n\tif \"fill\" in obj.__dir__():\n\t\treturn True\n\tif \"response\" in obj.__dir__():\n\t\treturn True\n\n\treturn False\n\n\ndef get_method_name_by_signature(signature):\n\"\"\"\n\tExtracts method"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn is_callable(obj, 'fill') and is_callable(obj,'request')\n\n\ndef is_fill_request(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\treturn is_execute_request_el(obj) and is_fill_request_el(obj)\n\n\ndef is_execute_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\treturn is_callable(obj, 'fill') and is_callable(obj"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, \"execute\") and (re.match(COMBINS['run'], name))\n\ndef is_runall_el(obj):\"\"\"\n\tObject contains executable method 'runall'.\n\t\"\"\"\t\n\treturn hasattr(obj, \"execute\") and (re.match(COMBINS['runall'], name))\n\ndef is_stop_el(obj):\"\"\"\n\tObject contains executable method'stop'.\n\t\"\"\"\n\treturn hasattr(obj, \"execute\") and (re.match(COMBINS['stop'], name))"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, Run_el)\n\n@pytest.mark.slow\n@pytest.mark.parametrize('obj', objects_to_check[1])\n#@pytest.mark.parametrize('obj', objects_to_check[2:])\ndef test_run(obj):\n\n\t\"\"\"\n\tTest the run method.\n\t\"\"\"\n\tobj_.run_obj = obj_\n\n\t# First run without any arguments.\n\tobj_.clear_console()\n\tobj_.run()\n\tobj_.print_log(\"\\"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn(True) or ('object' in dir(obj) and 'run' in dir(obj) and ('self' in dir(obj) and 'obj' in dir(obj)))\n\ndef run_el(obj):\n\t\"\"\"\n\tExecute a self-contained executable file inside obj.object.run\n\t\"\"\"\n\tif is_run_el(obj):\n\t\ttry:\n\t\t\tobj.run()\n\t\texcept TypeError:\n\t\t\tpass\n\ndef is_load_file(obj):\n\"\"\"\n\tObject contains executable method 'load'.\n\t\"\"\"\n\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn is_run(obj)\n\n\ndef is_run_or_static(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\tAnd this method has no parameters (but return) or outputs.\n\t\"\"\"\n\treturn obj.run(None)\n        \ndef is_run_class(obj)->bool:\n\"\"\"\n\tObject contains executable method 'run'.\n\tBut this method has no parameters (but return) or outputs.\n\tAll data and methods are static\n\t(like normal Python functions).\n\t\"\"\"\n\treturn is_run_or_static"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, RunObject)\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn run.__globals__['run'] in [obj.__class__]\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj,'run') and callable(getattr(obj,'run'))\n\n\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn callable(obj.run)\n\n# \u4ee3\u7801\u6ce8\u91ca\ndef is_symbol_el(obj):\n\t\"\"\"\n\tObject contains symbol.\n\t\"\"\"\n\treturn isinstance(obj, _Symbol)\n\ndef is_str_el(obj):\n\t\"\"\"\n\tObject is string.\n\t\"\"\"\n\treturn isinstance(obj, str)\n\ndef is_dict_el(obj):\n\t\"\"\"\n\tObject is dictionary.\n\t\"\"\"\n\treturn isinstance(obj, _Dict) == None\n\ndef"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn True\n\telif sys.version_info[0] < 3:\n\t\treturn not inspect.isfunction(obj) and not inspect.ismethod(obj)\n\telse:\n\t\treturn callable(obj) and not inspect.isroutine(obj)"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj.run(args)"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn obj.methods.get('fill', '') and obj.methods.get('compute', '')\n\ndef fill_compute_size(obj):\n\"\"\"\n\tSize of the object's fields.\n\t\"\"\"\n\tsz = obj.methods.get('fill', '')\n\tif not sz:\n\t\treturn 0\n\tsize, _ = exec_size(sz, obj)\n\tif is_fill_compute_el(obj):\n\t\tsize += get_fill_compute_size(obj)\n\treturn size\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\t# if obj.method1 or obj.method2:\n\t# \treturn True\n\treturn False\n\ndef is_method(obj):\n\"\"\"\n\tObject contains natively executable method.\n\t\"\"\"\n\tif not is_fill_compute_el(obj):\n\t\treturn False\n\treturn obj.is_method()\n\ndef is_implements_method(obj, method):\n\"\"\"\n\tIf obj.method1 or obj.method"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif obj.obj_offset in [0x68, 0x69, 0x6a, 0x6c, 0x78, 0x79, 0x7a, 0x7c,\n\t\t\t\t\t\t0x80, 0x82, 0x83, 0x84, 0x88, 0x89, 0x8a, 0x8c,\n\t\t\t\t\t\t0x8e, 0x9c, 0xe1, 0xca, 0xcb, 0xcc, 0xce, 0x2a,\n\t\t\t\t\t\t0x40, 0x64, 0"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif not (hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))):\n\t\tlogger.error('%s object has not both \"fill\" and \"compute\" methods', type(obj).__name__)\n\t\treturn False\n\treturn True\n\ndef is_fill_compute_elm_eq(elm_list):\n\tif not any(['fill' in elm.tag and 'compute' in"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif (\n\t\t\tisinstance(obj, fill_exec__.Fill) or\n\t\t\tisinstance(obj, compute_exec__.Compute) or\n\t\t\t(\n\t\t\t\tisinstance(obj, object) and\n\t\t\t\thasattr(obj, 'fill') and\n\t\t\t\tgetattr(obj, 'fill', None) and\n\t\t\t\tcallable(getattr(obj, 'fill')) and\n\t\t\t\tgetattr(obj, 'compute', None) and\n\t\t\t\tgetattr(obj, 'compute', None) and\n\t\t\t\tcallable(getattr(obj, 'compute'))"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif obj:\n\t\tif obj['compute']['compute'] or obj['compute']['compute_2']:\n\t\t\treturn True\n\t\telse:  ## fill only\n\t\t\tif obj['fill']['fill'] and (\n\t\t\t\tobj['fill']['function'] or\n\t\t\t\t(obj['fill']['data'] and obj['fill']['data']['data']) or\n\t\t\t\t(obj['fill']['data'] and obj['fill']['data']['data'][0])\n\t\t\t):\n\t\t\t\treturn True\n\t\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tprint(\"is_fill_compute_el\")\n\tif is_setattr(obj, 'fill'):\n\t\treturn True\n\treturn is_setattr(obj, 'compute')\n\ndef get_attr_index(fname, attr, obj):\n\"\"\"\n\tTries to resolve attribute name attr from object obj to an index number.\n\tIf attr is not found returns default index=0.\n\t\"\"\"\n\tprint(fname)\n\tprint(attr)\n\tprint(obj)\n\tprint(dir(obj))\n\tobj ="}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif 'fill' in dir(obj):\n\t\treturn True\n\telse:\n\t\treturn False\n\nclass EulerAngleEncoder:\n\"\"\"\n\t\n\t\n\t\"\"\"\n\tdef __init__(self, parent, axes, decoder, parent_device_name, *, label=None,\n                 from_config:dict=None):\n\t\tif parent:\n\t\t\tself.is_bidi\t\t= True\n\t\t\tself.parent_device_name = parent_device_name\n\t\t\tself.device_name = \"\"\n\t\t\tself.parent = parent\n\t\t\tself"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tpass\n\ndef get_methods(obj):\n\"\"\"\n\tMethods are returned from the callable objects.\n\t\"\"\"\n\tpass\n\nclass getattr_.getattr__:\n\"\"\"\nAttributes are not bound. No descriptor attributes are in effect.\n\"\"\"\n\tpass\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nimport inspect\nimport re\n\n\n# https://pytrends.github.io/py_trends/trending_searches.html\nclass TrendingSearches:\n\tRE_SEARCH = re.comp"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn obj.get_fill_method() and (not obj.get_fill_method() == '')\n\n\ndef is_fill_compute_and_is_compute_method(obj):\n\tobj_fill = obj.get_fill_method()\n\tobj_compute = obj.get_compute_method()\n\treturn (obj_fill and obj_compute) and (not obj_fill == '')\n"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\treturn a_difference(d2, d1, level)\n\ndef union(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a combined version of dictionaries *d1* and *d2*.\n\t\n\tThe *d1* and *d1* are both deep copied.\n\tAfter combining, *d1* or some of its subdictionaries may be returned\n\tdirectly.\n\t\n\t *level* sets the maximum depth of recursion. For infinite\n\trecursion, set that to -1. For level 1,\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td2 = copy.deepcopy(d2)\n\td1 = copy.deepcopy(d1)\n\tif level > 0:\n\t\tfor k in d1:\n\t\t\tif k not in d2:\n\t\t\t\tcontinue\n\t\t\td1[k] = difference_one(d1[k], d2[k], level)\n\tfor k in d2:\n\t\tif k in d1:\n\t\t\tcontinue\n\t\td1[k] = d2[k]\n\treturn d1\n\n\ndef difference_one(a, b, level):\n\t\"\"\""}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td2 = dictionary()\n\td3 = d1.difference(d2, level=level)\n\treturn d3\n\ndef intersection(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a ``set`` of keys shared between *d1* and *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present in *d1* and *d2*, with different values,\n\tit is included into the intersection."}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1, d2, level = _normalize_levels(d1, d2, level)\n\t\n\t# Check if we have a flat structure\n\tif d1 is d2:\n\t\treturn d1\n\t\n\t# Determine the base structure if we don't have a dictionary\n\tif not hasattr(d1, 'has_key') or not hasattr(_init_dict(), '__contains__'):\n\t\tif not hasattr(d1, 'itervalues'):\n\t\t\td2 = _init_dict(d2)\n\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif any(not isinstance(x, dict) for x in (d1, d2)):\n\t\traise ValueError(\"Arguments must be dictionaries\")\n\treturn dict([(k, v1) if v1 is not None and v1!= v2 else (k, v2) for \n\t\t\t\t(k, v1), (k, v2) in zip(d1.items(), d2.items())])\n\ndef get(obj, key):\n\t\"\"\"\n\tUtility function from :func:`hasattr` that will raise :exc:`Key"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tres = {}\n\tif not d1:\n\t\treturn res\n\tif level > 0:\n\t\tdef _rec_diff(new_res, diff_d1, diff_d2, level):\n\t\t\tfor k in diff_d1:\n\t\t\t\tif k in diff_d2:\n\t\t\t\t\tif new_res.get(k) is None:\n\t\t\t\t\t\tnew_res[k] = []\n\t\t\t\t\tif isinstance(diff_d1[k], dict) and isinstance(diff_d2[k], dict):\n\t\t\t\t\t\tnew_res[k] = _rec"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tpass\ndef __reduce_ex__(self, proto):\n\tpass\n\ndef __setattr_knownkind_str(self, s):\n\tpass\n\ndef __str__(self):\n\treturn \"\"\ndef __lt__(a, b):\n\t\"Not strictly ordered.\"\n\tpass\ndef __repr__(self):\n\treturn \"\"\ndef __getnewargs__(self):\n\treturn ()\n\ndef __sizeof__(self):\n\treturn 0\n\ndef __eq__(self, other):\n\treturn False\n\ndef"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1, d2 = dict(d1), dict(d2)\n\treturn dict_difference(d1, d2, level=level)\n\t\n\t\ndef dictionary_difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with subtructed items from *d1* and *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1, \n\tif a key is present in *d1* but *d2*"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1, d2, result = _intermediate_dictionaries(d1, d2)\n\treturn dict([(key, d1[key]) for key in (d1.keys() - d2.keys())\n\t              if not (level == 0 and key in result)])\n\n\ndef _intermediate_dictionaries(d1, d2):\n\t\"\"\"\n\tRecursively build two intermediate dictionaries for the\n\tdifference computation that are easier to handle than returning\n\tdifferent result objects.\n\t\n\tA copy of *d1* is returned"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif hasattr(d1, 'has_key') and hasattr(d2, 'has_key') and (not d1.has_key(k)) \\\n\tand not d2.has_key(k):\n\t\treturn d1\n\tres = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tres[k] = d1[k]\n\tfor k in d1:\n\t\tif k in d2:\n\t\t\td1d, d2d = d1, d2\n\t\t\tif level==-1:"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._htype[coord] += weight\n\ndef scale(self, coord):\n\"\"\"\t\n\tScale histogram at *coord* by the given factor\n\n\tCoordinates outside the histogram edges are ignored::\n\nscale(h.scale(3), 2)               --> h.scale(3*2)\n\"\"\"\n\tself._hscale[coord] *= self._bins[coord].size\n\ndef __len__(self):\n\t\"\"\"\n\tReturns the total number of bins\n\n\tIt does not include the special edges.\n\t\"\"\"\n\treturn self._"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\ndef fillStyle(self, x=None, y=None):\n \"\"\"\n Fill in the style associated with the histogram bins at *x* and *y*.\n \n *x*, and *y* can be 1D or 2D ndarrays, in which case a 2D histogram is\n computed using scipy.ndimage.measurements.profile_line, which is much\n faster than scipy.ndimage.measurements.profile_line2d.\n \n If *x* and/or *y* are scalars, a 2"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\treturn _checkcall(_lib.SKHistFill(self._hnd, coord, weight) if _ffi.cast(\"size_t\", coord) <\n_ffi.cast(\"size_t\", self.data.size) else 0)\n\n@_check_refnum\ndef search(self, coord, weight=1):\n\"\"\"\n\tReturn pair of (weight, index) at the position *coord*\n\t\n\tSearch can be performed by a number of types::\n\n\t  - Find the first coordinate < *coord*.\n\t  - Find the last coordinate >= *"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\ndef get_axis(self):\n\"\"\"\n\tReturn the axis of this histogram data structure (axes are the vertical axis if this is a vertical histogram, the horizontal axis is the horizontal axis if this is a horizontal histogram, the color axis is the color axis if this is a color histogram).\n\t\n\tReturning an *Axis* would allow things like iterating over this histogram (or its parts) in column-major order\n\t\n\tIf no axis is returned this is not a histogram.\n\t\"\"\"\n\tpass\ndef get_counts(self"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._data[coord] += weight\n\ndef get(self, coord):\n\"\"\"\n\tGet the value at the *coord*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\treturn self._data[coord]\n"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\ndef full(self, coord, weight=1):\n\"\"\"\n\tFill the entire histogram with the given *weight*.\n\t\n\tThis is equivalent to calling :meth:`fill` with *coord* 0.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\ndef height(self, coord=-1):\n\"\"\"\n\tGet the maximum number of events of type *event_type* in each coordinate.\n\tThe default is to return all of them.\n\t\n\tParameters:\n\t\n\t\tcoord (int): optional"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif weight < 0:\n\t\traise InvalidEntry('weight has to be positive')\n\tif weight and coord < 0:\n\t\traise InvalidEntry('coord has to be postive')\n\twhile coord >= len(_v):\n\t\t_v.append(_MISSING)\n\t_v[coord] *= weight\n\n### plotting ###\n\ndef viewHistogram(points, title=None):\n\t\"\"\"Open an interactive plot window for a histogram.\"\"\"\n\timport pylab\n\tif title and title is not None:\n\t\tpylab.suptitle(title"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass\ndef fill_tail(self, n=1, weight=1):\n\"\"\"\n\tFill histogram by repeating the tail until it is full.\n\t\n\tn is the number of positions to repeat, default: 1\n\tweight is the corresponding weight, default: 1\n\t\n\tThe histogram is filled from the tail on.\n\t\"\"\"\n\tpass\ndef fill_tail_range(self, start=0, stop=0, n=1, weight=1):\n\"\"\"\n\tFill histogram by extending the range of the histogram until the\n\t*stop*"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tt = int(math.floor(weight+0.5))\n\tif t<0:\n\t\treturn\n\tif coord > self.max_bin or coord < 0:\n\t\treturn\n\twhile coord<self.left_edge:\n\t\tself.left_edge += 1\n\t\tif t>0:\n\t\t\tt-=1\n\t\t\tself.fill(self.left_edge-1,t)\n\t\tif t<0:\n\t\t\treturn\n\twhile coord>self.right_edge:\n\t\tself.right_edge += 1\n\t\tif t>0:"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tassert weights_same_precision(weight, self.weights.get_precedence())\n\tif weight == 0:\n\t\tdel self.counts[coord]\n\t\tif self.frozen:\n\t\t\tself.frozen = False\n\telse:\n\t\tself.counts[coord] = weight\n\treturn\n\n@deprecated_parameter('bump', 'n')\ndef normalize(self, n=1, copy=True):\n\"\"\"\n\tNormalize the histogram.\n\t\n\t*n* counts that are already in the histogram are preserved"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = []\n\tre_keys = _compile_pattern(RE_KEY)\n\tre_labels = _compile_pattern(RE_LABELS)\n\tfor key, value in labels.items():\n\t\tif re_keys.match(key):\n\t\t\terrors.append((key, \"got bytes-like object\"))\n\t\telse:\n\t\t\tif re_labels.match(key):\n\t\t\t\terrors.append((key, f\"'{key}' does not match the regex '{RE_KEY_LABELS}'\"))\n\t\t\telif not re_lab"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif isinstance(labels, str):\n\t\ttry:\n\t\t\treturn re.match(labels, \"valid\")\n\t\texcept re.error:\n\t\t\tpass\n\tif isinstance(labels, dict):\n\t\tlabels_error = []\n\t\tfor key, val in labels.items():\n\t\t\t_validate_labels(key)\n\t\t\t_validate_labels(val)\n\t\t\tlabels_error.append({key: val})\n\t\treturn labels_error\n\telif isinstance(labels, list):\n\t\tlabels_error = []\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor key, pattern in RE_VALID_LABEL_KEYS.items():\n\t\tif key in labels:\n\t\t\tmatch_obj = pattern.match(labels[key])\n\t\t\tif not match_obj:\n\t\t\t\tdel labels[key]\n\t\t\t\tmatch_obj = pattern.match(labels.get(key))\n\t\t\t\tif not match_obj:\n\t\t\t\t\terror = (key, f\"ValidationError - regexp matching with label key '{key}'\\n\"\n\t\t\t\t                    \"was not successful, it does not match the pattern \\n{pattern}\")\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tregexes = {\n\t    # key or label\n\t    \"key\": \"[a-zA-Z][_a-zA-Z0-9]*\", \n\t    # label value\n\t    \"label\": \"[\\$][^A-Za-z0-9]+\",\n\t    # \"boolean\": \"True|False\",\n\t}\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append({\"key\": key, \"value\": \"expected string"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = {}\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\tre.compile(key)\n\t\texcept Exception:\n\t\t\terrors[key] = {'key': key, 'value': 'Expected string or bytes-like object'}\n\t\tif isinstance(value, (str, bytes)): continue\n\t\terrors[key] = {'key': key, 'value': 'Expected string or bytes-like object'}\n\t\tfor candidate in value:\n\t\t\tif not isinstance(candidate, (str, bytes)):\n\t\t\t\terrors["}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tglobal labels_regexp_expressions\n\t\n\ttry:\n\t\tif not isinstance(labels, dict):\n\t\t\traise Exception()\n\t\tif not labels:\n\t\t\traise Exception()\n\t\tfor label in labels:\n\t\t\tlabels_regexp_expressions[label] = labels_regexp_expressions.get(label,\n\t\t\t                                                                r'' if label == '__unmatched__' else None)\n\t\t\tif labels_regexp_expressions[label] is None:\n\t\t\t\traise Exception(f'No such matching regexp for label"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor key, value in labels.items():\n\t\tif isinstance(value, str) and (value == \"\" or value.isspace()):\n\t\t\tif not (re.match(labels[''], \"\") and re.match(labels[\"$\"], \"\")):\n\t\t\t\traise ValidationError((key, \"Label '{}' does not match regex\".format(key)))\n\n\t\t# check whether the label is a regex\n\t\telif isinstance(value, str) and re.match(value, \"\"):\n\t\t\t# if it is a regex, but does not match,"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif not isinstance(labels, dict):\n\t\traise ValidationError('Labels should be of type dict, not {}'.format(type(labels)))\n\t\n\tfor name, regex_expr in LABEL_REGEXES.items():\n\t\tif name not in labels:\n\t\t\tcontinue\n\t\t\n\t\tvalue = labels[name]\n\t\tif isinstance(labels, str):\n\t\t\tlabels[name] = text_type(labels)\n\t\t\n\t\t# NOTE: The regexes below will need to be adapted if any other validator types\n\t\t#"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# Check for empty label input\n\tif not labels:\n\t\treturn\n\n\t# Check whether the regular expression is given for the keys/values\n\tif not set(labels) <= {'$regex', '$options'}:\n\t\treturn\n\n\t# Create regular expression matcher for $regex labels\n\tregexes = {}\n\tfor pattern, value in keys_labels_iter(labels, \"$regex\"):\n\t\tregexes[pattern] = value\n\n\tfor key in labels:\n\t\tif key not in regexes:\n\t\t\tcontinue\n\n\t\tfor"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tregexes = {\n\t    \"valid\": \"[a-zA-Z0-9_-]+\",\n\t    \"invalid\": \"[^a-zA-Z0-9_-]+\",\n\t    \"$$\": \"\".join([\"[^\", \"a-zA-Z0-9_-]+\", \"[$]\"]),\n\t    True: \"[^TtYy][^a-zA-Z0-9_-\\s]*\"\n\t}\n\terrors = []\n\tfor key, label in labels.items():\n\t\tif type"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn (\n\t\tre.compile(\"--.*_(?P<id>\\w+)\"),\n\t\tre.compile(\"--.*\")\n\t)\n\ndef read_input_data():\n\t\"\"\"\n\t\tRead data in from the input file, validate it and return it as a\n\t\tdictionary.\n\t\t\n\t\tReturns:\n\t\tDictinary: the parsed structured data from the input file.\n\t\"\"\"\n\tif not os.path.exists(INPUT_DATA_FILE):\n\t\traise IOError(\"Input DATA file {} does not exist.\".format("}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(\n\t\tfr\"^[\\w_.-]+$\", re.ASCII\n)\n\n\ndef resource_get_by_name(\n\t\tname: str,\n\t\tclient: kopf.client.Client,\n\t\tnaming_function=Resource.from_url_field\n) -> Resource:\n\t\"\"\"\n\tGet a resource by its name.\n\n\tArgs:\n\t    name (str): The name of the resource to retrieve.\n\t    client (kopf.client.Client): The client.\n\t    naming_function ("}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif hasattr(\n\t\t_get_resource_name_regex, \"regex\"\n\t):\t# if there is already a regex, return it\n\t\treturn getattr(_get_resource_name_regex, \"regex\")\n\t# else compile a regex, with one re group representing the name\n\t# and no regex group representing the namespace\n\tregex = re.compile(\n\t\tr\"\"\"(?P<name>(?P<re_name>[^'\"]*)\n\t\t(@(?P<re_ns>(?:.*/)+)?)?\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t# The regex should accept both non-empty resource names\n\t# and all other valid parameters.\n\tpattern = re.compile(\"^[a-z,A-Z0-9_]+$\")\n\treturn pattern\n\t    \t\t\t\n\t \t   \t\ndef _validate_resource_name(value:str) -> bool:\n\t\"\"\"\n\tValidates the input string as a resource name\n\n\tParameters:\n\t    value (str): the resource name to be validated\n\t        \n\tReturns:\n\t    (bool): True if the input name conforms to"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif not resources_resource_name_regex_cache:\n\t\tresources_regex_string = \\\n\t\t\tresources_resource_name_regex_str\n\t\tresource_name_regex = resources_regex_string\n\t\tresources_resource_name_regex_cache.update({\n\t\t\t'pattern':re.compile(resource_name_regex, re.IGNORECASE)\n\t\t})\n\n\t\treturn resources_resource_name_regex_cache['pattern']\n\telse:\n\t\treturn resources_resource_name_regex_cache['pattern']"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(\n        r\"\"\"\n        (?:\\w{2,}|/[^/]*/)*\n        $\n        \"\"\",\n        re.IGNORECASE,\n    )\n\n\ndef _get_name_regex():\n\t\"\"\"\n\tGet the regex to build the regular expression used to validate the resource name.\n\t\n\tRaises:\n\t    None\n\tReturns:\n\t    (re.Pattern): the compiled regular expression, to validate the\n\t    resource name.\n\t\"\"\"\n\tmatch = _get_resource_name"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn resource_name_regex.compile(r'^[\\w.-]+$')\n\ndef _get_resource_name_allowlist():\n\t\"\"\"\n\tReturns the allowlist that is used when the user wants to\n\tdefine custom resource names.\n\t\"\"\"\n\treturn ['KRAKE.CONFIG', 'KRAKE.INTERNAL', 'KRAKE.PROC', 'KRAKE.PROC.PROC', 'KRAKE.SELFDESTRUCT']\n\ndef _get_resource_names_regex():\n\t\"\"\""}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\tresource_pattern = re.compile(\n\t\tr\"\"\"\n\t\t(?P<res_id>\n\t \t((?P<semi_res_id>{})?)\t\t\t# resource id or id:name syntax\n\t\t)?(?P<non_semi_part>{})\t\t\t# name part (or just the 'name' part)\n\t\t\\[(?:(?P<kraken__data_source_id>[\\w\\-_]*)(::|::)(?P<kraken__data_source_version>[\\w-"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif _resource_name_regex is None:\n\t\tres_names_regex = r\"^[a-z][a-z_0-9$&*]*$\"\n\t\t_resource_name_regex = re.compile(res_names_regex,\n\t\t\t\t\t\t\t\t\t\t  re.IGNORECASE)\n\treturn _resource_name_regex\n\n\ndef _resource_type():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe type of the resources.\n\n\tReturns:\n\t    (re.Pattern"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t\n\t_resource_name_re = None\n\t\n\tif not _resource_name_re:\n\t\t# name not specified in.pypi_resource/package_name.json\n\t\t_resource_name_re = re.compile(r\"^[A-Za-z_]\\w*$\") # pylint:disable=C0103\n\telif _resource_name_re.pattern!= r\"^[A-Za-z_]\\w*$\":\n\t\t# The regular expression was passed into file.\n\t\t# p"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match('\\w+', value):\n\t\traise ValidationError('%s not match')\n\tif len(value) <2:\n\t\traise ValidationError('%s too short')\n\telse:\n\t\tprint(f'{value=}')\n\n\ndef validate_password(password):\n\"\"\"\n\tValidates the password. It will check for minimum length and minimum number of digits.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: in case the given password is incomplete or"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.compile(r\"[-_a-zA-Z0-9]{1}\")\n\t\tif not re.match(p, value):\n\t\t\traise ValidationError()\n\texcept:\n\t\traise ValidationError()\n\n\n\nclass UserService(BaseService):\n\t\"\"\"\n\tUserService represents the user service.\n\t\n\tAttributes:\n\t    users: collection of objects of type User\n\t\"\"\"\t\n\tdef __init__(self, name=\"User\", collection=\"\"):\n\t\tself.__users = {}\n\t\tBaseService"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tregexp = re.compile(regexp_map[field])\n\tresult = regexp.match(value)\n\t\n\tif result:\n\t\treturn True\n\telse:\n\t\traise ValidationError('{field} must be of type {type}'.format(field=field, type=regexp.pattern))\n\ndef get_data(data, regexp_map):\n\t\"\"\"\n\tHelper method to construct the corresponding data structure.\n\t\n\tArgs:\n\t    data: a list to retrieve from the database\n\t    \n\t    \n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tre_str = data['re_str']\n\t\n\tif not re_compiled[data['re_mode']].match(value):\n\t\traise ValidationError(\"%r does not match regular expression %r\" \\\n\t\t\t\t\t% (value, re_str))\n\nclass ModelBase(BaseModel):\n    \"\"\"\n    Base class for all the models\n    \"\"\"\n\n    def _get_model_fields(self):\n        \"\"\"\n        Override from BaseModel\n        Get a dictionary with all the model fields as keys and it's\n        type as value\n        \"\"\"\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif pattern and not re.match(pattern, value):\n\t\traise Exception('The given value does not match with the pattern.')\n\t\n########################################################################################################################\n\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t\n\tif regex.search(validate_pattern, str(value)):\n\t\tpass\n\telse:\n\t\traise ValidationError(\n\t\t\tf\"Value {value} is not conform to the pattern {validate_pattern}\"\n\t\t)\n\n\n@validation_schema(\n    {\"type\": \"dict\"}\n)\ndef validate_dict_key(value):\n\"\"\"\n\tValidate the given dictionary key.\n\t\n\tArgs:\n\t    value: the dictionary key to be validated\n\t\n\tRaises:\n\t    ValidationError: if the \""}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tm = VALID_VALUE.match(value)\n\tif not m:\n\t\traise ValidationError(\"Invalid value\")\n\treturn\n\ndef validate_value_from_list(value):\n\t\"\"\"\n\tValidate the given value against the given list of possible values\n\t\n\tArgs:\n\t    value_from_list: the value to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the given list of possible values.\n\t\"\"\"\n\tif not value in VALUE_LIST:\n\t\traise ValidationError"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tregex = re.compile(\"^[a-z0-9_]+$\")\t# regex for alphabets only\n\tif not (pattern := regex.search(value)):\n\t\traise ValidationError(f\"Not valid value {value!r} \" )\n\n\n@dataclass\nclass BaseModel:\n\t\"\"\"\n\tSuper class for all of your database models, used to make sure that all\n\tof your database models conform to the naming convention. It also gives a\n\tconsistency wrapper, which helps in creating, validating and saving to\n\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not isinstance(value, basestring):\n\t\traise ValidationError(\"'value' argument must be a string\")\n\telif not re.match(r\"^(?:0x)?[0-9a-f]\\{8,\\}(\\|[^\\|]{4,})\\??$\", value):\n\t\traise ValidationError(\"'value' argument must be a string like: 0xdeadbeef|ABCDEFGH|123\")\n\nclass ListType(Type):\n\t@classmethod\n\tdef validate_value(cls, value"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tvalidate_name(value)\n\tvalidate_int(value)\n\ndef validate_name(name):\n\"\"\"\n\tValidate the given name against the corresponding regular expression.\n\t\n\tArgs:\n\t    name: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given name is not conform to the regular expression.\n\t\"\"\"\n\tvalidate_email(name)\n\tvalidate_re(name)\n\tvalidate_name_no_extra_characters(name)\n\tvalidate_name_only_alph"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tpattern = re.compile(r\"\\$[0-9a-fA-F]+\")\n\tif not pattern.fullmatch(key): #validate if key is a string\n\t\traise ValidationError('Key is not in a string format')\n\t\n\t\ndef add_key(db, node_name, key):\n\"\"\"\n\tAdd a key to the database. Raises an error if the key already exists.\n\n\tArgs:\n\t    db: the database object\n\t    node_name: the name of the node (string)\n\t    key:"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tKey(key)\n\texcept KeyError as e:\n\t\traise e\n\ndef validate_time(time):\n\"\"\"\n\tValidate the given time using the regular expression.\n\t\n\tArgs:\n\t    time: time to validate\n\t\n\tRaises:\n\t    ValidationError: if the given time is not conform to the regular expression.\n\t\"\"\"\n\tvalidate_time_string(time)\ndef validate_key_exists(key):\n\"\"\"\n\tValidate the given key exists\n\t\n\tArgs:\n\t    key"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t\n\t_key_regex = \"^[_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFF"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tpattern = re.compile('^[0-9a-zA-Z_$]+$')\n\tif pattern.match(key):\n\t\treturn\n\telse:\n\t\traise ValidationError('Invalid characters in key')\n\t\t\n\t\t\n\t\t\n\t\n\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not isinstance(key, str):\n\t\traise ValidationError('\"key\" must be a string')\n\tif not re_validates(key, r'^[$]{32,}$'):\n\t\traise ValidationError('\"key\" is not a valid secret key')\n\t\n\treturn key\n\t\n\t# ################################\n\t# Define additional functions if required\n\t# ####################################\n\t\n\t\t\n\ndef create_table(secret_key=\"p4ssw0rd\", app_name=None, region=None):"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tdef _validate(regex, key):\n\t\tif not re.search(regex, key):\n\t\t\traise ValidationError('The given key is not a string: %s' % type(key))\n\t_validate(REGEX_KEY, key)\n\treturn\n\ndef validate_secret(secret):\n\"\"\"\n\tValidate the given secret against the corresponding regular expression.\n\t\n\tArgs:\n\t    secret: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given secret is not conform to the regular expression.\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tpattern = r\"^(?!\\\").(\\w+)(?!\\\").*$\"\n\tif not re.search(pattern, key):\n\t\traise ValidationError()\n\n\nclass Create(Resource):\n\t\"\"\"\n\tA class for handling the creating tasks of the user profile with the help of the\n\tvalidate_method in profile_app.schema\n\t\"\"\"\n\t@jwt_required\n\tdef post(self, user_id):\n\t\t\"\"\"\n\t\tFunction for handling the creation of user by the user with id given in the\n\t\tpost request.\n\t\t\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.search(R\"^[\\u4e00-\\u9fa5~\\-_]+$\", key):\n\t\traise ValidationError(\"Key {} is not valid\".format(key))\n\ndef validate_numeric_between(key, lower, upper):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not in the given range.\n\t\"\"\"\n\tif not key.isd"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif key is None:\n\t    key = \"\"\n\n\n\n\tvalid_key_pattern = re.compile('^[A-Za-z][A-Za-z0-9 ]*$')\n\n\tif not isinstance(key, str):\n\t    raise ValidationError (\"key\", 'invalid literal for str with base 0')\n\n\tif not valid_key_pattern.match(key):\n\t\traise ValidationError (\"key\", 'invalid characters in key')\n    \n\n    __slots__ = ['_path', '_type', '_value"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(RE_VALIDATE_KEY, key):\n\t\traise ValidationError('%s is not a valid key' % key)\n\ndef normalize_config(config):\n\t\"\"\"\n\tGenerate the normalised config dict from the given config dict and\n\tvalidate its format against the given validator.\n\t\n\tThe normalised config dict has the following structure:\n\t\n\t    {\\'validate:\\': {\n\t       \\'key-1\\': {\n\t           \\'pattern\\': \\'{prefix}.key.key.$\\',\n"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = copy.deepcopy(observer_schema_def)\n\t# iterate through the fields in the nested schema\n\tif isinstance(manifest_dict, list):\n\t\tif first_level:\n\t\t\tfor key in manifest_dict:\n\t\t\t\tif key in observer_schema:\n\t\t\t\t\tobserver_schema[key]['from_file'] = True\n\t\tfor value in iter(manifest_dict):\n\t\t\tif (isinstance(value, dict) or isinstance(value, list)) and len(manifest_dict) > 1"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\toutput = None\n\toutput = extend_subfields(manifest_dict, first_level)\n\tfor key, val in output.items():\n\t\tif first_level and isinstance(val, dict):\n\t\t\tname = validate_and_append_subfield(output, key, FIRST_LEVEL_VALIDATION,\n\t\t\t                                   MANIFEST_KEY[MANIFEST_FILE])\n\t\t\tif name:\n\t\t\t\tbreak\n\t\telif not first_level and not isinstance(val, list):\n\t\t\tname = validate_and_append_subfield(output"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t# Check if manifest dict was partially generated\n\tif first_level:\n\t\tidentifier = manifest_dict['identifier']\n\t\tif not ('value' in manifest_dict and 'namespace' in manifest_dict):\n\t\t\tlogging.warning(f'Unregistered identifier on manifest file, unable to generate observer_schema for '\n\t\t\t                 f'{identifier}')\n\t\telse:\n\t\t\t# Create corresponding k8s resource using manifest_dict\n\t\t\tresource_to_create_from = manifest_dict['namespace'].capitalize() + manifest_dict['"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tresource_observer_schema = copy.deepcopy(manifest_dict)\n\t\n\t# If the value does not contain list, create one.\n\tfor key, value in resource_observer_schema.items():\n\t\tif not isinstance(value, list):\n\t\t\tresource_observer_schema[key] = list()\n\t\t\t\n\t# Replace `None` with default values\n\t[\n\t\tresource_observer_schema[key][i] \\\n\t\t\tif resource_observer_schema[key][i] is not None \\\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t# Replace non-list and non-dict values for this part of the observer_schema\n\tif manifest_dict.get(\"type\"):\n\t\tmanifest_dict.update({k: v for k, v in manifest_dict.get(\"type\").items()\n\t\t                      if isinstance(v, (dict, list))})\n\t\tif manifest_dict and isinstance(manifest_dict[\"type\"], list):\n\t\t\t# Remove from manifest_dict for this part of the observer_schema\n\t\t\tmanifest_dict[\"type\"] = manifest_dict[\"type\"]["}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdef recursive(target_dict, schema_identifier, parent_identifier, new_identifier):\n\t\tnew_id = {schema_identifier: {new_identifier: deepcopy(target_dict[parent_identifier])}} # dict(dict(dict...))\n\t\tto_check = []\n\t\tif first_level:\n\t\t\tif schema_identifier in manifest_dict:\n\t\t\t\tfor manifest_key in manifest_dict[schema_identifier].keys():\n\t\t\t\t\tkey = manifest_key\n\t\t\t\t\tif manifest_key == new_identifier:\n\t\t\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = copy.deepcopy(manifest_dict)\n\tif isinstance(observer_schema, dict):\n\t\tdef get_recursive_attr(obj, attrpath, default=None):\n\t\t\tfor key in attrpath.split('.'):\n\t\t\t\ttry:\n\t\t\t\t\tobj = obj[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\treturn default\n\t\t\treturn obj\n\t\tid_value = manifest_dict.get('metadata', {}).get('labels', {}).get('id')\n\t\tfor k, v in observer_schema."}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not first_level:\n\t\tif \"kind\" in manifest_dict:\n\t\t\tmanifest_dict[\"kind\"] = manifest_dict[\"kind\"] + \"-default\"\n\n\t\t# Add default namespace for single-namespace resources\n\t\tif manifest_dict.get(\"namespace\") == \"default\" and manifest_dict.get(\"namespace\") not in manifest_dict \\\n\t\t\t\tand manifest_dict.get(\"namespace_patterns\"):\n\t\t\tmanifest_dict[\"namespace\"] = manifest_dict[\"namespace_patterns\"].split()[0]\n\n\t\t# Create"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict) or isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(manifest_dict[key], first_level)\n\t\telif first_level:\n\t\t\tobserver_schema[key] = value\n\treturn observer_schema\n\n\ndef generate_default_observer_schema_list(manifest_list, first_level=False):\n\t\"\"\""}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not first_level:\n\t\tk8s_resource_type = manifest_dict['kind']\n\t\tk8s_resource_name = manifest_dict['metadata']['name']\n\t\tk8s_resource_version = manifest_dict['metadata']['resourceVersion']\n\t\tk8s_resource_namespace = manifest_dict['metadata']['namespace']\n\telse:\n\t\tk8s_resource_type = manifest_dict['type']\n\t\tk8s_resource_name = manifest_dict['metadata']['name']\n\t\tk"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tassert len(last_applied_manifest)>=len(observer_schema), \\\n\t\"Length of last_applied_manifest=%d and observer_schema=%d \" \\\n\t% (len(last_applied_manifest), len(observer_schema))\n\t\n\tfor k, v in observer_schema.items():\n\t\tif k in last_applied_manifest and \\\n\t\tlast_applied_manifest[k] is None:\n\t\t\tlast_applied_manifest[k] = v"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor resp in response:\n\t\t# handle all the different observed fields in a single place\n\n\t\tresp_observer_schema = obs_schema_type(resp[\"observer_schema\"])\n\t\tfor field_name in last_applied_manifest[\"observer_schema\"]:\n\t\t\tif resp_observer_schema[field_name] == k8s_pb2.Unset:\n\t\t\t\tlast_applied_manifest[\"observer_schema\"][field_name] = resp[\n\t\t\t\t\t\"observer_schema\"][field_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef is_observer_present(val) -> bool:\n\t\tif val is None:\n\t\t\treturn False\n\t\tif val!= \"\":\n\t\t\treturn True\n\t\treturn False\n\n\tfor field, schema in zip(last_applied_manifest, observer_schema):\n\t\tif is_observer_present(field):\n\t\t\tcontinue\n\n\t\tif field == '':\n\t\t\tif is_observer_defined(schema):\n\t\t\t\traise ValueError('\"current_manifest\" is not empty but schema is defined')\n\t\t\tfield = schema\n\t\telif '__add"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field_name in observer_schema[\"fields\"]:\n\t\tif observed_field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observed_field_name] = {}\n\n\t\tif \"value\" in observer_schema[\"fields\"][observed_field_name]:\n\t\t\tif \"value\" in response[2][\"lastAppliedManifest\"][observed_field_name]:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest["}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# if the Kubernetes manifest doesn't contain the whole list of observables,\n\t# just return it\n\tif len(list(filter(lambda x: x['status'] == 'present', observer_schema))\n\t\t)!= len(response):\n\t\treturn last_applied_manifest\n\t# else update our list for each observable to the partie that we just get\n\t# from the response of the Kubernetes API\n\tfor observable in observer_schema:\n\t\tlast_applied_manifest[observable['name']]['status']"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = list()\n\tfor _, obs in observer_schema.items():\n\t\tlast_applied_manifest_field = dict(\n\t\t\ttype=\"null\",\n\t\t) if obs.last_applied_manifest in (\"null\", None) else obs.last_applied_manifest\n\t\tif last_applied_manifest_field not in last_applied_manifest:\n\t\t\tlast_applied_manifest.append(last_applied_manifest_field)\n\t\n\tfor obj in response:\n\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field, value in _update_from_resp(\n\t\t\tobserver_schema,\n\t\t\tresponse if type(response)==dict else None,\n\t\t\t'last_applied_manifest'\n\t\t).items():\n\t\tlast_applied_manifest[field] = value\n\t\n\tif 'apiVersion' not in last_applied_manifest:\n\t\tlast_applied_manifest['apiVersion'] = DEFAULT_KUBERNETES_OPEN_API_VERSION\n\t\n\tif 'kind' not in last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# Iterate over the observed fields\n\tfor field_name, field_value in observer_schema.items():\n\t\t# Check if the field exists\n\t\tif field_name not in last_applied_manifest:\n\t\t\t# Not seen before, assign the new value\n\t\t\tlast_applied_manifest[field_name] = field_value\n\n\t\telse:\n\t\t\t# Seen before and has the same value, update\n\t\t\tfield_type = field_value['type']\n\t\t\tlast_applied_manifest_value = last_applied_manifest[field_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# The function is recursive. It is invoked recursively to find non None\n\t# values for each observed fields, and updates the inner list with them\n\tfor item in observer_schema:\n\t\tif item.get('lastAppliedManifest'):\n\t\t\t# Update inner\n\t\t\tself._update_last_applied_manifest(last_applied_manifest, item)\n\t\telse:\n\t\t\tif hasattr(self, f'update_{item[\"name\"]}_observer_schema') and\\\n\t\t\t\t\tgetattr(self, f'update_{item"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobj, k8s_client = last_applied_manifest, k8s_client\n\tif not all(key in ob for key, ob in zip(obj.keys(), observer_schema)):\n\t\tprint('Error: missing fields in observer_schema when updating last_applied_manifest')\n\t\tprint('Observed values: ', end='')\n\t\tprint(json.dumps(observer_schema, indent=2))\n\t\treturn\n\tif not all(key in rew for key, rew in zip(obj.keys(), response"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tfor key in observer_schema['fields'].keys():\n\t\t\tif key not in response['items'][0]:\n\t\t\t\t# In the case of not present keys, use the value as is: None\n\t\t\t\t# To add new keys, we can use add_if_not_exist\n\t\t\t\tcontinue\n\t\t\tlast_applied_manifest[key] = response['items'][0][key]\n\texcept KeyError:\n\t\t# When the observed field is not present, raise exception (the value is\n\t\t# likely an empty dict), or the next function will catch"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor k, v in json.loads(response['obj']['metadata']['raw'][0]['data'].lower()).items():\n\t\tif k not in last_applied_manifest:\n\t\t\tlast_applied_manifest[k] = v\n\t\telse:\n\t\t\tif not isinstance(last_applied_manifest[k], list):\n\t\t\t\tlast_applied_manifest[k] = [v]\n\t\t\telse:\n\t\t\t\tlast_applied_manifact_set = set(last_applied_manifact[k]."}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tfor field in sorted(SCHEMA_OF_LAST_APPLIED_MANIFEST, key=lambda x: x['name']):\n\n\t\t\tif field['field_name'] in OBJECT_TYPE_MAP:\n\t\t\t\tif field['type'] in ('integer', 'long'):\n\t\t\t\t\t# no need to try and convert an int(0) into a long as none of\n\t\t\t\t\t# the integers will ever be greater than 2 ** 63 - 1\n\t\t\t\t\tif field['property_name'] in ('min','max'):\n\t\t\t\t\t\tif response[SCHEMA_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field, observed_field_value in list(response.items()):\n\t\tif not isinstance(observed_field_value, list):\n\t\t\tobserved_field_value = [observed_field_value]\n\t\tfor f in observer_schema:\n\t\t\tif observed_field == f.NAME:\n\t\t\t\tlast_applied_manifest[f.NAME] = observed_value\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise KeyError(\"Unable to find \" + observed_field + \" in \" + response)\n\n\treturn []\n\n\ndef update_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tkubernetes_api = KubernetesApi(\n\t\tbase_url=response['url'],\n\t\tcertificate_authority_data=observer_schema['ca'],\n\t\tbasic_auth_username=observer_schema['username'],\n\t\tbasic_auth_password=observer_schema['password'])\n\t\n\ttry:\n\t\tfor field in observer_schema['spec'].keys():\n\t\t\tif field in response:\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\telse:\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key, value in observer_schema.items():\n\t\tif key not in response:\n\t\t\traise KeyError(f\"Field was not present and was expected\")\n\n\t\tif key == \"resourceVersion\":\n\t\t\t# resourceVersion is currently not supported.\n\t\t\tpass\n\t\telif key == \"manifest\":\n\t\t\tlast_applied_manifest[key] = value[\"observer\"]\n\t\telif key == \"conditions\":\n\t\t\t_update_applied_condition_from_resp(\n\t\t\t\tlast_applied_manifest[key],\n\t\t\t\tjson.loads("}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef field_from_list_or_string(resp_to_update, key, val):\n\t\t\"\"\"This function is used to ensure that each value is either a list or a string\"\"\"\n\t\tif key in resp_to_update:\n\t\t\tif isinstance(resp_to_update[key], str):\n\t\t\t\treturn resp_to_update[key]\n\t\t\treturn [field_from_list_or_string(resp_to_update[key], j, v) for j,v in enumerate(resp_to_update[key])]\n\t\treturn val\n\n\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = observer_schema.get('last_applied_manifest')\n\tif last_applied_manifest is None:\n\t\treturn\n\tif last_applied_manifest not in response:\n\t\traise KeyError(f\"`{last_applied_manifest}` is not present in the Kubernetes response\")\n\tlast_applied_manifest.update(response['last_applied_manifest'])\n\tupdate_last_applied_manifest_dict_from_resp(\n\t\tobj="}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observed_fields():\n\t\ttry:\n\t\t\tlast_applied_manifest[field] = observer_schema[field]\n\t\texcept KeyError:\n\t\t\tlast_applied_manifest[field] = None\n\n\treturn\n\n\ndef update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial manifest list from\n\ta partial Kubernetes response\n\t\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserve_name = obs_name = self.obj_name\n\tif self.obj_name!= \"\":\n\t\tobserve_name += \"-\" + self.obj_name\n\tfor field_name in observed_fields:\n\t\tif field_name in response:\n\t\t\tlast_applied_manifest[observe_name][field_name] = response[\"status\"][self.obj_name][field_name]\n\t\telse:\n\t\t\tself.logger.warning(f\"Updating last_applied_manifest for {response['message']"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor custom_schema, resource_type in (\n\t\t(\n\t\t\tapp.klass.get_service_spec,'service'\n\t\t), (app.klass.get_ingress_spec, 'route'\n\t\t), (app.klass.get_deployment_spec, 'deployment'\n\t\t), (app.klass.get_configmap_spec, 'configMap'\n\t\t), (app.klass.get_secret_spec,'secret'\n\t\t)\n\t):\n\t\tif custom_schema:\n\t\t\tmodel = custom"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tmapping = {}\n\tmanifest = app.model.resources.get(\"spec\").manifest\n\t\n\t# Add the default observer schema for each Kubernetes resource\n\tfor (domain, kind, _) in list(kubernetes.load_resources(kubernetes.APIS_KUBERNETES_API_VERSION)):\n\t\tif app.model.resources.get(\n\t\t\t\"/\".join(\n\t\t\t\t[\n\t\t\t\t\tapp.model.resources.get(\"/\").api,\n\t\t\t\t\tapp.model.resources.get(\"/\").version,"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t_log.info('Generating default observer schema for {}'.format(app.spec.manifest))\n\n\tif not app._config.observations.default_observer_schema or app.spec.resource =='replicationcontroller':\n\t\treturn\n\n\tspec = kubernetes.load_spec(app.spec.manifest)\n\tmetadata = spec['metadata']['name'] + '-default-schema'\n\tapp.observer.resource_version = '0'\n\n\t# Create the schema object\n\tschema = schemas.Schema("}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tfor app_api, app_spec, app_ref in k8s.parse_api_versions_and_standards(app.spec.manifest):\n\n\t\tif app_api in app.spec.observers:\n\t\t\tschema = None\n\t\telse:\n\t\t\tapp_api = k8s.get_latest_tagged_kubernetes_api_version_for_all_resources(app_api)\n\t\t\tschema = k8s.get_resource_api_schema(app_api)\n\n\n\t\t# Add observaton resource"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif not (app.spec_version >= kubernetes.spec.v1beta1.Version(\"v1beta1\")):\n\t\treturn\n\n\tif app.spec.kube_api_version == kubernetes.spec.v1:\n\t\tversion = 'v1beta1'\n\telif app.spec.kube_api_version == kubernetes.spec.v1beta2:\n\t\tversion = 'v1beta2'\n\n\tif app.kubernetes_scaling_config.group_resource_type not in config.GROUP"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\ttry:\n\t\tpod_manifest_ref = app.metadata.template.spec.manifest.ref\n\texcept (AttributeError, KeyError):\n\t\treturn\n\n\ttry:\n\t\ttarget = (\n\t\t\tkubernetesxapi.KubernetesXAPI(\n\t\t\t\turl=app.metadata.template.spec.kubernetes_api_definition.url, name=app.metadata.name)\n\t\t\t.manifest(\n\t\t\t\tpod_manifest_ref.split('/')[-1])\n\t\t\t.get_manifest()\n\t\t)\n\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tconfig = app.config\n\n\tlog.debug('Generating the default observer schema')\n\tfor spec in app.deployment_spec['spec']['template']['spec']['containers']:\n\t\tif spec['name'] == 'ambassador':\n\t\t\tname = app.name\n\t\t\tspec['volumes'] = generate_observer_default_volume_spec(app)\n\t\t\tspec['taints'] = generate_default_kubernetes_taint_spec(\n\t\t\t\tapp, prefix='kubernetes.ambassador.io/')\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tschema_builder = KubernetesSchemaBuilder(app.spec.manifest, namespace=app.namespace,\n\t\tmetadata=app.spec.metadata, pod_template_manifest=app.template)\n\tschema_builder.build_observer_schema(app.observer_schema)\n\ndef generate_default_consumer_schema(app):\n\"\"\"\n\tGenerate the default operator and subscriber schema for the application for all\n\tmanifest-defined resources.\n\t\n\tArgs:\n\t    app (krake.data.kuber"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor name, resource_info in app.spec['manifest'].items():\n\t\tif 'observer' in resource_info:\n\t\t\tcontinue\n\t\tresource = app.client.resources.get(None, name)\n\t\ttry:\n\t\t\tobserver_schema = resource.get(params={'pretty': True, 'allow_watch_bookmarks': False, 'labelSelector': 'app=%s' % name},\n\t\t\t\t\t\t  namespace='default',\n\t\t\t\t\t\t  timeout=5).annotations\n\t\texcept NotFound:\n\t\t\tcontinue\n\t\tobserver = ObserverSche"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tlog.debug('Generating default observer schema for {app}'.format(app=app.instance_id))\n\n\tif app.spec.manifest.observer.pattern!= '':\n\t\t# Don't try to generate observer schema if an observer has been\n\t\t# specified at the command line\n\t\tlog.debug('No default observer schema requested; doing nothing')\n\t\treturn\n\n\tschema_string = render_observer_schema(app)\n\tif (schema_string):\n\t\ttry:\n\t\t\tschema = schema_tools.load_schema(schema"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\tparams = SQLParams.from_style(SQLParams.in_style, params)\n\treturn SQLQuery(sql, (str(i) for i in params if i), \"in_style\")\n\n\ndef format_in_style(sql, params):\n\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~col"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif not sql:\n\t\treturn sql, params\n\n\tif not params:\n\t\treturn sql, params\n\n\ts0 = sql\n\n\tparams = params.copy()\n\tparams.update(SQLParams.from_sql(sql).params)\n\t\n\tsql = s0\n\tconverter = sql_encode_params\n\tif params[SQLParams.out_style]:\n\t\tconverter = sql_decode_params\n\t\n\tif any(SQLParams.in_style <= s <= SQLParams.out_style\n\t\tfor i,"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn (\n\t\tsql_query,\n\t\tdict(\n\t\t\t(\n\t\t\t\t(\n\t\t\t\t\tk,\n\t\t\t\t\tSQLParam.convert(\n\t\t\t\t\t\tt,\n\t\t\t\t\t\tv,\n\t\t\t\t\t\tcast=cast and isinstance(k, int),\n\t\t\t\t\t),\n\t\t\t\t),\n\t\t\t\t**kwargs\n\t\t\t)\n\t\t\tfor k, t, v, cast in SQLParams(sql_query, in_style=cast_in_style)\n\t\t),\n\t)\n\ndef sql_replace_param(sql_query, param_name, value, cast=False):\n\t\"\"\"\n\t"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tin_style, params = to_params(sql, params, in_style)\n\tout_style = getattr(params, 'out_style', DEFAULT_OUT_STYLE)\n\t\n\treturn sql % params, params.map_out(params, out_style)\nif __name__ == '__main__':\n\timport sys\n\ttry:\n\t\timport configargparse\n\texcept ImportError:\n\t\tsys.stderr.write('Could not import configargparse.\\n')\n\t\tsys.exit(1)\n\tparser = config"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif sql.startswith(b'/') and len(sql) > 1:\n\t\tsql = sql[1:]\n\n\tparams = SQLParams(in_style)\n\n\tif not params.convert(sql, params, params.in_style):\n\t\traise ValueError(\"Unrecognized in-style parameter style\")\n\n\n\tout_style = params.out_style\n\n\tif out_style is None:\n\t\traise sqlite3.ProgrammingError(\"Invalid or unsupported in-style parameter style\")\n\n\tif out_style == PARAMSTYLE_TUP"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\n\tparams = params if params is not None else dict()\n\t\n\tfrom. import dbapi\n\t\n\t_tuple = ()\n\t\n\t# Prepare to insert parameters in formatted sql query.\n\tfor in_name, in_val in params.items():\n\t\tif type(in_val) is tuple or type(in_val) is dict:\n\t\t\tformatted = format_sql(in_val, with_params=False)\n\t\telif type(in_val) is int:\n\t\t\tformatted = \"{:d}\".format(in_val"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn query, params\n\ndef escape(sql, s, escape_string=ESCAPE_DICT):\n\t\"\"\"\n\tEscapes all common SQL wildcards in *sql* and replaces them with\n\tthe given *s*.\n\n\t*sql* (:class:`str` or :class:`bytes`) should be escaped before\n\tpassing to this function.\n\n\t*s* is the replacement string.\n\n\t*escape_string* is a :class:`dict` keyed by common wildcard\n\tsymbols. If :attr:`.SQLParams"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn tuple(iter_convert(*prepare_sql(sql, params)))\n\n\ndef iter_convert(*prepares):\n\t\"\"\"\n\tYield all SQL queries to convert a set of prepared\n\t:class:`sql` and :class:`SQLParams`.\n\t\"\"\"\n\tfor sql, params in prepares:\n\t\tyield sql, convert_params(sql, params)\n\n\ndef convert_params(sql, params):\n\t\"\"\"\n\tConvert the *params* according to :attr:`.SQLParams.in_style` and\n\t:attr"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t#: The parameter style, determines how named parameters are declared.\n\t#: There are two styles ``1`` (simple style - see Eq. 5.1) and ``2``\n\t#: (named style - See Eq. 5.2). The default is ``1`` and the only\n\t#: parameter style supported - ``1``.\n\tin_style = get_int(SQLParams.in_style or 1)\n\t\n\t#: Which value to use for the \"DEFAULT\" type. It's a fixed value\n\t#: and is expected as"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tdef wrap(sql, params):\n\t\ttry:\n\t\t\tif params is None:\n\t\t\t\tparams = ()\n\t\t\telif isinstance(params, OrderedDict):\n\t\t\t\tparams = tuple(params.items())\n\t\t\telif isinstance(params, set):\n\t\t\t\tparams = list(params)\n\t\t\t\n\t\t\tif isinstance(params, list):\n\t\t\t\tparams = tuple(params)\n\t\t\t\t\n\t\t\tstmt = SelectStatement(tuple(sql.strip().split()))\n\t\t\tstmt.setattr('parameters', params)\n\t\t\t\n\t\t\treturn sql, stmt"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\timport copy as _copy\n\tfrom.sql import format_sql, quote_str, unquote_str\n\t\n\tparamstyle = _copy.deepcopy(self)\n\tsql = format_sql(sql, paramstyle, formatmany_params)\n\tparams = []\n\tout_params = []\n\tfor many_param in many_params:\n\t\tif isinstance(many_param, dict):\n\t\t\tout_params.append(format_params_dict(paramstyle, many_param))\n\t\telse:\n\t\t\tout_params.extend(format_params"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tassert (not many_params or isinstance(many_params, (Mapping, Sequence, set, tuple, list, range)))\n\tassert (params is not None)\n\t\t\n\tres_sqlexpr = sql\n\tmany_params = listify(many_params)\n\t\t\n\t# out-style parameters\n\tfor m_i in many_params:\n\t\tassert isinstance(m_i, Mapping)\n\t\tres_sqlexpr = res_sqlexpr % convertdict(m_i)\n\t\t\n\tres_sqlexpr = res_sq"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tdef format():\n\t\t# For each set of in-style parameters, convert to out-style parameters.\n\t\tout = []\n\t\tin_names = [param.name for param in params]\n\t\tin_args = args[1:]\n\t\twhile len(in_args) > 0:\n\t\t\targ = args[0]\n\t\t\tvalue = arg\n\t\t\tdel args[0]\n\t\t\targ = str(arg)[1:-1]\n\t\t\tif len(in_args) == 1 and arg not in in_names:\n\t\t\t\traise SQLException(\n\t\t\t\t\t\""}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tif isinstance(many_params, Mapping):\n\t\traise ValueError(\"Can't use a Mapping or Sequence to create a list\")\n\telif (isinstance(many_params, Sequence) and\n\t\tnot isinstance(many_params, str) and\n\t\tlen(tuple(one for one in many_params if isinstance(one, (int, str)))) > 1):\n\t\traise ValueError(\"Can't use a Sequence to create a list\")\n\t\n\tmany_param_sets = tuple(many_params)\n\tif len(many_param"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tparams = []\n\tresult = []\n\tin_style = _in_style(sql)\n\tif in_style is None:\n\t\traise ArgumentError(\"no in-style parameter style\")\n\t\n\tif sql:\n\t\tif sql.startswith(b\"(\"):\n\t\t\tsql = sql[1:]\n\t\t\n\t\t# A named parameter style query.\n\t\tdef _add_named_param(param_name, sql):\n\t\t\tif isinstance(sql, bytearray):\n\t\t\t\treturn sql + b\"@\" + param_name.encode(\""}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t\n\tif len(params) > 0 and isinstance(many_params, dict):\n\t\tresult = formatmany(sql, sqltypes._QueryType, params, many_params['param']\n\t\t\t)\n\t\treturn result[:1] + [[dict(zip(many_params['param'], params))] + result[1:2]]\n\t\n\t# SQLParams -> Tuple (Str, List[Tuple])\n\t# InStyle -> Tuple (Str, List[dict]) (in_style_mapping\n\t#\tis in_style type, many"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tmany_params = []\n\tmany_params.append(parse(many_params, many_params))\n\tfor params in many_params:\n\t\tout_args, out_kwonly_args, out_defaults = many_params_to_out(\n\t\t\tmany_params[0],\n\t\t\tparams\n\t\t)\n\t\tout_sql = many_params_to_out_sql(\n\t\t\tmany_params[0],\n\t\t\tparams\n\t\t)\n\t\tyield out_sql, one(out_args), one(out_kwonly_args), out_default"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\treturn _formatmany(sql, _inmany)\n\t\ndef _inmany(sql, _many_params):\n\tparams = _many_params\n\tnewparams = {param: value for param, value in params.items() if value is not None}\n\n\tnew_sql = bytes(sql)\n\n\tif params.get(SQLParams.SQL_ORDINAL):\n\t\ttry:\n\t\t\tcurr_index = new_sql.index(b'(')\n\t\texcept ValueError:\n\t\t\traise ValueError(f\"Failed to locate parenthesis in query {"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tdef _format_str(stmt, params):\n\t\ttry:\n\t\t\tparm_iter = iter(params)\n\t\texcept TypeError:\n\t\t\treturn stmt, params\n\t\t\n\t\tdef process(stmt, params):\n\t\t\tparams, ret = stmt(**params)\n\t\t\tassert isinstance(params, (NoneType, dict, tuple))\n\t\t\treturn params\n\t\t\n\t\tdef unkeyify(params):\n\t\t\tif isinstance(params, (tuple, dict)):\n\t\t\t\tfor i, a in enumerate(params):\n\t\t\t\t\tif isinstance(params"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tdef _make_tuple(data):\n\t\tif isinstance(data, (tuple, list)):\n\t\t\ttup_iter = iter(data)\n\t\telse:\n\t\t\ttup_iter = iter([data])\n\t\treturn next(tup_iter)\n\n\tmany_params = _make_tuple(many_params)\n\tif many_params is not None:\n\t\targs = many_params if isinstance(many_params, list) else list(many_params)\n\tif type(args) == bytes:\n\t\targs = bytearray"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\t_validatePyFSObject(path)\n\t\treturn True\n\texcept:\n\t\timport traceback\n\t\t# This happens most often when files are being created, so it is better to\n\t\t# give warnings rather than die.\n\t\tprint >>sys.stderr, \"Warning: Validating %s resulted in an exception.\" % path\n\t\treturn False\n\"\"\"\n    ####################################################################################################\n    #\tC O N S T R U C T A - T R A N S T R Y\n    ####################################################################################################\n\"\"\"\ndef validateTransformation(root):"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\twith self.fs.open(path, \"rb\") as f:\n\t\ttry:\n\t\t\tcheck_object(fh=f, obj=OCFL(f.read(), path))\n\t\texcept (CheckError, ValidationError) as e:\n\t\t\tself.warnings += e.warning\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\ndef update(self, path, newobj, update_objs=True):\"\"\"\n\tUpdate object at path to replace newobj.\n\t\n\tNew obj can be a file-like object or an ODF node. If"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn _get_backing().validate(path, self)\n\ndef read_text(path):\n\"\"\"\n\tReturn string value of OCFL at path or pyfs root converted to UTF-8.\n\tIf errors or warnings, the text may still contain UTF-8 characters after\n\tthey have been transformed into XML.\n\t\n\tRaise ValueError if OCFL cannot be read.\n\t\n\tUse with reraise=True.\n\t\"\"\"\n\treturn _get_backing().read_text(path)\n\ndef write_text(path, text):\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t# This isn't very optimal, but it takes care of all the cases.\n\treturn ocflib.check_ocflib_valid(path)\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t\t# Validate OCFL directory tree's size.  Only the root.o ckroot directory contains the\n\t\t# actual OCFL object directory.\n\t\tif not self._validate_obj_dir_size(path):\n\t\t\treturn False\n\t\t\n\t\treturn True\n\t\n\tdef validate_ocfl_obj_tree(self, path):\n\t\"\"\"\n\tRun OCFL object and OCG object validation against specified OCFL OCG tree.\n\t\n\tReturns an OCFL OCG tree validity result structure.\n\t\"\"\"\n\t\n\t\t# Validator function signature is"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\t# path.isdir\n\t\n\tif path.isfile():\n\t\tdir_obj = path.parent / path.dirname()\n\t\t\n\t\tif not dir_obj.isdir():\n\t\t\tif dir_obj.exists():\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\twarn(\"WARNING: Non-directory parent object %s not valid or does not exist\" % dir_obj)\n\t\n\telif path.isdir():\n\t\twarn(\"WARNING: Root object %s not valid or does not exist\" % path)\n\t\t\n\telse:\n\t\twarn"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.warnings = []\n\t\tself._parse(path)\n\t\tif hasattr(self,'message'):\n\t\t\tself.failure = True\n\t\t\treturn False\n\n\t\treturn True\n\t\t\n\t# On failure, also store the error message (helpful if you're debugging)\n\t# if hasattr(self,'reason'):\n\t# \tself.failure = True\n\t# \tself.error = self.reason\n\ndef print_messages(out=None):\n\t\"\"\"\n\tIf an in-place repair is completed"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn self.fs.validate(path, warnings_allowed=True)\n\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tpass"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn True\n"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tpfx = u\"%s Validator status: \" % (prefix,)\n\tprint (pfx + (u\"validator disabled\" if self.state == VALIDATOR_STATE_DISABLED else u\"validator enabled\"), end = \",\")\n\tprint_str = (u\"%s%s%s\" % (pfx,self.state, u\" / %s\" % (self.description if self.description is not None else \"\")))\n\treturn (print_str)\n\t\nclass ValidatorProxy(object):\n\t\"\"\"\n\tProxy class for"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.__getattribute__(self._status_prefix).strip()\n\tif status_str:\n\t\treturn prefix +'' + status_str\n\telse:\n\t\treturn ''\n\ndef validate(instance, values=None, prefix='', ignore=None,\n\t\t\t  reject_on_errors=False):\n\t\"\"\"\n\tValidate value of model instance. Can be used to catch ValueErrors and\n\tTypeErrors during validation. To avoid having to import the\n\t'validators' package, use the following:\n\n\tif settings.DEBUG"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn(\"%s%s %s\" % (prefix, self.__class__.__name__.upper(), self.status_string()))\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tfrom. import constants\n\treturn prefix + ('active' if self.valid else 'inactive') +'' + constants.STATUS_STRING[self.valid]\n\n_CONSTANTS=\"\"\"\nconstants.HELP_STRING = '''\\\nThe help string for this application.''',\n\nconstants.OPTIONS = {\n\t'help': {\n\t\t'help': '''\\\nDisplay a list of options.''',\n\t},\n\t'validate': {\n\t\t'help': '''\\\nValidate the input data. Requires an"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn self.validator.status_str(prefix=prefix)\n\n\ndef validate_path(string):\n\t\"\"\"\n\tEnsure we get a valid path string back from get_path().\n\t\"\"\"\n\ttry:\n\t\tp = Path(string)\n\texcept:\n\t\traise argparse.ArgumentTypeError(f\"path '{string}' must be a path to an image.\")\n\telse:\n\t\treturn str(p)\n\n\ndef check_and_read_path(path):\n\t\"\"\"\n\tEnsure we get a valid path back from get_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.status == VALIDATOR_STATUS_PASS:\n\t\treturn '{}validated'.format(prefix)\n\telse:\n\t\treturn '{}failed'.format(prefix)\n# \n# \n# ValidatorStatus \n# \n# \nValidatorStatus = namedtuple('ValidatorStatus', ['error','status'])\nVALIDATOR_STATUS_PASS = 0\nVALIDATOR_STATUS_FAIL = 1\nVALIDATOR_STATUS_DOCKER_NO_PACKAGE = 2\nVALIDATOR_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn '%s' + prefix + self.__status_str()\n\n\nclass ValidatorSet(object):\n\t\"\"\"\n\tA container of all Validaators, for easy inspection.\n\t\"\"\"\n\n\tdef __init__(self, prefix):\n\t\tself.__set = {}\n\t\tself.__prefix = prefix\n\n\tdef get(self, name):\n\t\t\"\"\"\n\t\tIf the name is defined, return the validator, otherwise None.\n\t\t\"\"\"\n\t\treturn self.__set.get(name)\n\n\tdef get_or_add(self, name):\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tret = []\n\tret.append(f'{prefix}  - {type(self).__qualname__}')\n\tif hasattr(self, '_invalid'):\n\t\tret.append(f'{prefix}    - invalid')\n\tif hasattr(self, '_missing'):\n\t\tret.append(f'{prefix}    - missing')\n\tif True in getattr(self, '_optional', ()):\n\t\tret.append(f'{prefix}    - optional')\n\tif self._required and False in getattr(self"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn ('%s%s: %d/%d' % (prefix, self.__class__.__name__, self.status, self.max_status))\n\nclass BaseValidator(object):\n\t\"\"\"\n\tBase class for validators.\n\t\"\"\"\n\tmax_status = 400\n\tdef __init__(self, req):\n\t\tself.req = req\n\t\tself.schema_errors = []\n\n\tdef validate(self):\n\t\tself.validate_and_return()\n\n\tdef validate_and_return(self):\n\t\tself."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn self._validstr.strip() + (prefix and \" (%s)\" % prefix)\n\nclass ValidateError(Exception):\n\t\"\"\"\n\tException error class for validators.\n\t\"\"\"\n\tdef __init__(self, msg, value):\n\t\tself.value = val\n\t\tself.msg = msg\n\tdef __str__(self): return \"(Error %s: %s)\" % (self.msg, self.value)\n\nclass Validator(object):\n\t\"\"\"\n\tValidation class. This class is responsible of validating values\n\tagainst a target"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\ts = ''\n\twhile self._log:\n\t\tl, v = self._log.pop(0)\n\t\ts += '{0} {1}\\n'.format(\n\t\t\tprefix and l[len(prefix):] or '', v)\n\treturn s or 'validation was not completed'\n\nclass ConfigurationParser:\n\t\"\"\"\n\tA simple configuration file parser.\n\t\n\t>>> cfg = ConfigurationParser()\n\t>>> cfg.update(['section.setting=value', 'another.setting-3.4-5', 'other=6', 'no"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not self:\n\t\treturn 'Valid'\n\n\ts = []\n\tfor vl in self:\n\t\tif vl.rule == 'always':\n\t\t\tif vl.status or vl.severity:\n\t\t\t\ts.append(vl.status + ',' + vl.severity)\n\t\t\telse:\n\t\t\t\ts.append('OK')\n\n\t\telse:\n\t\t\ts.append(vl.status + ',' + vl.severity)\n\n\t\tfor sub in ('invalid', 'valid', 'used', 'optional'):\n\t\t\tcount = getattr("}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn _api.PluginStatus_status_str(self, prefix)\n\nclass _LogicalValidate(_object):\n\t\"\"\"Proxy of C++ Validate class.\"\"\"\n\n\t__swig_setmethods__ = {}\n\t__setattr__ = lambda self, name, value: _swig_setattr(self, _LogicalValidate, name, value)\n\t__swig_getmethods__ = {}\n\t__getattr__ = lambda self, name: _swig_getattr(self, _LogicalValidate, name)\n\t__re"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn self._check_log_str(prefix,self.failures,self.errors)\n\t\ndef check_results(\n    result_dict, fail_result_dict, outfile, logfile, stats=True,\n    no_check_cmds=False\n):\n\t\"\"\"\n\tRun checker on result dict, write output and update checks log\n\t\"\"\"\n\tcheck_log = Log(\"check_log\")\n\tfor key in result_dict.keys():\n\t\tresult = result_dict[key]\n\t\tcheck_log.count_items(result."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn self.error_map({\n\t\t1: 'Validation failed.',\n\t\t2: 'Faults raised but no errors generated.',\n\t\t3: 'One or more errors generated.',\n\t\t4: 'One or more errors raised.',\n\t}, prefix=prefix)\n\n\ndef log_status(status, value, message, level):\n\t\"\"\"\n\tIf status and level are specified; log a record\n\t\"\"\"\n\tif (status is not None and status >= level) or message is not None:\n\t\t_logger.log(logging.CRITICAL"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tres='Validation status code=%d\\n'%self.status\n\tres+='\\n'.join('%s %s'%t for t in self.val_log[0:10])\n\tif prefix:res='%s%s'%(prefix,res)\n\treturn res\n\t\ndef report_error(self,err_info,**kwargs):\n\t\"\"\" Report an error, and if present in input parameters, update the object. \n\t\"\"\"\n\tif not(err_info in self.err_log):\n\t\tself.err"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\ttxt = []\n\tfor (key,val) in self.items():\n\t\tmsg = ''\n\t\tif isinstance(val, list): val = ', '.join(val)\n\t\tif isinstance(val, str):\n\t\t\tmsg = ': {}'.format(val)\n\t\t\tval = val[:20]\n\t\ttxt.append('{}-{}{}'.format(prefix, key, msg))\n\treturn '\\n'.join(txt)\n\n\nclass ValidationError(ValidationError):\n\t\"\"\"\n\tGeneral class for validation errors, with optional prefix."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn ''\n\n# END"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn ('%s:<line # %d, file \"%s\"} %s%s'\n\t\t%(prefix, self.line, self.file,\n\t\t  self.message, self.note)\n\t)\n\nclass ValidationError(ValidationViolation):\n\t\"\"\"\n\tRepresentation of the first violation found when validating a string.\n\t\"\"\"\n\tdef __init__(self, rule, start, end, line,\n\t             file='',\n\t             precondition='',\n\t             message='',\n\t             note=''):\n\t\tline ="}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '%s%s: %s %s %s %s' \\\n\t\t   % (prefix, self.__class__.__name__, self._msg_header,\n\t\t   self._id, self._desc, self._val)\n\n"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass IServerCertificateIdentityResolver(Interface):\n\t\"\"\"\n\tMethods to resolve the identity of an N{IServerCertificate}.\"\"\"\n\n\tdef acceptServerCertificate(**kwargs):\n\t\t\"\"\"\n\t\tAccept an L{OpenSSL.crypto.X509} type containing information about a\n\t\troot-ca authority's certificate.\n\n\t\t@param kwargs: Keyword arguments that will be used in the verification\n\t\t\tprocess.  By default, should contain 'assurance'.\n\n\t\t@return: a L{twisted.internet.defer"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\nclass InvalidEntryID(Exception):\n\t\"\"\"\n\tRaised if an entry ID is invalid.\n\t\"\"\"\n\tpass\n\nclass NotValidEntryID(Exception):\n\t\"\"\"\n\tRaised if an entry ID is not valid.\n\t\"\"\"\n\tpass\n\nclass NotAnEntry(Exception):\n\t\"\"\"\n\tRaised if a non-entry is accessed, only to enable easy creation with a\n\t\"class factory\" using the _construct_entry and is_valid_entry methods.\n\t\"\"\"\n\tpass\n\nclass InvalidEntry(NotValidEntry"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef main():\n\tprint(list(input_validator_base()))\n\nif __name__ == '__main__':\n\tmain()\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass EnumeratedValues(ValType):  # pylint: disable=too-few-public-methods\n\t\"\"\"\n\tEnumeratedValue class represents a list of enumerated values.\n\n\tAttributes:\n\t\tsymbols (List[str] or None): Symbolic names / strings of enumerated values\n\t\tvalues (List[enum.Enum] or None): Enumerated values (not necessarily ints)\n\t\"\"\"\n\n\tATTRIBUTES = ['symbols', 'values']\n\n\tdef __init__(self, symbols=None"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef is_blacklisted(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is blacklisted, always False in this base implementation.\n\t\"\"\"\n\treturn False\n\n\ndef get_user_id(self, identifier=None):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn the user id associated with the given identifier, defaulting to the authenticated user id.\n\t\"\"\"\n\treturn self.get_authenticated_user().id\n\n\ndef get_user(self, identifier="}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\n@implementer(IResolverSimple)\nclass MemoryResolver(object):\n\t\"\"\"Implementor for IPatchResolverSimple\n\n\tProvides a simple and efficient way to register a new resolver instance.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreate a new MemoryResolver instance.\n\t\t\"\"\"\n\t\tself._resolvers = {}\n\t\tself._delegated_resolvers = {}\n\n\tdef query(self, identifier, timeout=None):  # pylint: disable=unused-argument\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\ndef encode():\n\t\"\"\"\n\tTo encode the file, we iterate over file with all the characters and in order \n\twe store the characters in the final encoding.\n\t\"\"\"\n\tencoding = \"\"\n\tdecoding = \"\"\n\n\tfor c in input_string:\n\t\tencoding += str(encoding_map.get(\"\".join(decoding[::-1]), -1)) + c\n\n\t\tif is_valid(decoding, c):\n\t\t\tdecoding += c\n\t\telse:\n\t\t\tif 2 > len(decoding):\n\t\t\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\n# ============================================================================\nclass ValidatedSchema(BaseSchema):  # pylint: disable=too-few-public-methods\n\t\"\"\"\"\n\tThis class extends the base schema by enforcing that a list of valid data\n\ttypes are used in each of the fields. You can specify your own constraint to a field with the following\n\tnotation:'my_field: type\n\twhere type is a valid data type.\n\tValid data types are a dict, list, string, int, bool, float and string.\n\t\"\"\"\n\tschema_prefix"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass BaseIdentifier(BaseIdent):\n\t\"\"\"\n\tAn identifier.\n\n\tThis is a common base class for all other identifier types (regexp, host, etc).\n\n\t:param regexp:       The regular expression.\n\t:param identifier:   The identifier string (or None if not set)\n\t:param groupdict:    The groupdict from the expression to provide additional information\n\t\"\"\"\n\t__slots__ = \"regexp\", \"identifier\", \"groupdict\", \"_regexp\", \"__parser\", \"__parser_ref\","}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\nclass Basic_InlineStyle(_InlineStyle_Base_):\n\t\"\"\"Basic_InlineStyle is an abstract base class used for defining objects which can be used for creating a new inline style.\"\"\"\n\n\t@staticmethod\n\tdef is_valid(identifier):\n\t\treturn isinstance(basic_inline_styles[identifier], Basic_InlineStyle_)\n\n\ndef GetStyle(identifier):\n\treturn basic_inline_styles[identifier]\n\ndef GetStyleString(identifier, attributes=True):\n\treturn basic_inline_styles["}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\treturn (True, None)\n\"\"\"\n\tWrite the data from the inventory to a file for later ingestion.\n\t\n\tNote that a dictionary containing a whole inventory, as returned by\n\tvalidate or parse_inventory, does *not* get written out here. Rather, any file\n\thandler (e.g. logging) must be used, to validate the data.\n\t\n\tKeyword arguments:\n\tin_file - The file object that the data will be written to.\n\tdata_list - A list of strings, each representing a piece of data to write.\n\tdata"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, Inventory):\n\t\treturn False\n\tif len(inventory._entries)!= 0:\n\t\treturn True\n\tif extract_spec_version:\n\t\t(type,spec_version) = Spec(inventory.spec).validate_type(0)\n\t\tif type!=None:\n\t\t\tinventory.type = type\n\t\telse:\n\t\t\treturn False\n\t\tif spec_version!=None and spec_version!= inventory.spec_version:\n\t\t\tinventory.spec_version = spec_version\n\t\tif Spec(inventory.spec).validate("}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tfrom pyatspi.interface import SYMBOLS\n\tfrom pyatspi.utils import InvalidInputError\n\n\tif self.log_errors:\n\t\t#log error messages\n\t\tself.log.setLevel('ERROR')\n\t\n\t#if there is no type value given, use the specification version\n\tif not self.getAttribute('type'):\n\t\tif self.log_errors:\n\t\t\tself.log.info(\"%s: no type value given - checking specification's version\" % self)\n\n\t\tif isinstance(self.spec_version, tuple):\n\t\t\tself"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tret = InventoryValidator().validate(inventory)\n\tif ret is None:\n\t\treturn (None, None)\n\n\t# If the inventory is the same as our spec, then nothing needs to be validated.\n\tif self.spec and inventory == self.spec:\n\t\treturn (ret, \"spec-only\")\n\t\n\t# Create an empty list of errors.\n\terrors = list()\n\t\n\t# This field is required unless the inventory is a child.\n\tif not self.minimarket_type == config.TYPES_INVENTORY and"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tself.validated = False\n\t\n\tif self.validation_enabled:\n\t\n\t\terrors = []\n\t\t\n\t\ttry:\n\t\t\tfor i in range(len(self.schemata)):\n\t\t\t\tschema = self.schemata[i]\n\t\t\t\t\n\t\t\t\t# First, make sure that the schema in question is valid. See if it's a simple object, or a record.\n\t\t\t\tif schema['type']!= 'basic' and schema['type']!='record':\n\t\t\t\t\traise InvalidSchemaValueError(\"Invalid type: 'type' must be"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\t# validate type of inventory\n\tvalid_type = inventory.spec_version is None or isinstance(inventory.spec_version, str)\n\tif not valid_type:\n\t\traise InvalidFormatError(\n\t\t\tf'Expected type <{self.spec_version}>' if self.spec_version else TypeError,\n\t\t\tinventory.spec_version, TypeError, self.spec_version,\n\t\t\t)\n\t\n\t# validate version, if not None\n\tif self.spec_version:\n\t\tversion = inventory.spec_version\n\telse:\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, (Inventory, type(None))) or \\\n\t\textract_spec_version:\n\t\treturn (True, 'Inventory')\n\n\tif extract_spec_version and (not isinstance(inventory.type, types.StringTypes) or \\\n\t\tnot inventory.spec_version or inventory.spec_version == ''):\n\t\treturn (True, 'Inventory')\n\n\tif (len(inventory.name) == 0) or (inventory.get('type') in [None, '','string']):\n\t\treturn (True, 'Inventory"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, Inventory):\n\t\traise ValueError(\"Invalid inventory object\")\n\t\t\n\tif extract_spec_version:\n\t\tif 'type' in inventory.content.spec:\n\t\t\tspec_version = inventory.content.spec.get('type')\n\telse:\n\t\tspec_version = self.spec_version\n\t\n\tif not isinstance(spec_version, str) or not SpecVersionValidator.evaluate_value(spec_version):\n\t\traise ValueError(\"Invalid spec version: %s\" % spec_version)\n\t\t\n\tif '"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\ttry:\n\t\ttype_class = inventory_types()[self.type]\n\texcept KeyError:\n\t\ttry:\n\t\t\ttype_class = inventory_types()[self.type_map[int(self.type)]]\n\t\texcept KeyError:\n\t\t\treturn False\n\n\tfor obj in inventory.contents:\n\t\ttry:\n\t\t\tobj_type_class = inventory_types()[inventory.lookup_type(obj.type)]\n\t\texcept KeyError:\n\t\t\tobj_type_class = inventory_types()[inventory.lookup_type(\"Object\")]"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor file in manifest_files:\n\t\tpath = os.path.join(self.out_dir,\"%s.manifest\"%file)\n\n\t\twith open(path) as manifest_fp:\n\t\t\tmanifest_text = manifest_fp.read()\n\t\t\tmanifest = Manifest.from_string(manifest_text,\n\t\t\t\ttype=manifest.Type.MANIFEST)\n\n\t\t\tdigest_prefix = manifest.digests.digest_prefix(self.default_digest_size)\n\t\t\tmissing_digests = digest_prefix - digests_used\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest not in manifest_files:\n\t\t\texit_with_error(\"%s does not represent a present manifest file\" % digest)\n\t\tif manifest_files[digest] not in manifest_files:\n\t\t\t# manifest_files[digest] contains the filepath, but this file\n\t\t\t# does not exist. Exit with error\n\t\t\texit_with_error(\"%s is not used in manifest\" % manifest_files[digest])\n\n\n# This function is an example.  Takes the list of manifest files and a\n#"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tif digests_used.keys() == []:\"\"\"\n\tCheck if the manifest itself specifies the required digests\"\"\"\n\t\tfor fi, ff in enumerate(manifest_files):\"\"\"\n\tIterate through all manifest files listed\"\"\"\n\t\tif (\n\"\"\"\n# If all necessary digests are not set, then do not check out.\n\t\t\tfor digest in digests_used.keys():\"\"\", \"\"\"\n\t\t\t\tif digest in digests_used.keys():\"\"\", \"\"\"\n\t\t\t\t\tlogger.info(\"%s - %s\",\n\t\t\t\t\t\tmanifest.filename, \"{} : digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\n\t# Find which digests were not downloaded and require downloading\n\trequired_digests = list(manifest_files.keys())\n\tfor digest_name in digests_used.keys():\n\t\tif digest_name not in manifest_files:\n\t\t\trequired_digests.remove(digest_name)\n\t\t\tlogger.info(\"Digest '%s' not in manifest (will therefore be downloaded)\", digest_name)\n\n\t# Check download status of each of the required digests\n\tfor digest_name in"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tLOG(\"Checking manifest file %s\" % manifest_file, info=True)\n\t\tif(self.digests[manifest_file] == None):\n\t\t\tself.digests[manifest_file] = digests_used\n\t\t\tif(manifest_file in self.compile_args):\n\t\t\t\tself.compile_args.remove(manifest_file)\n\t\telif(manifest_file in self.compile_args and self.digests[manifest_file] == digests_used):"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor file in manifest_files:\n\t\tif file not in digests_used:\n\t\t\tlogging.error(f\"Missing required file {file} digest\")\n\t\t\tsys.exit(3)\n\n\ndef merge_manifest_files(manifest_files, md5=True):\n\t\"\"\"\n\tRead the manifest files in manifest_files and merge them into one file\n\t\"\"\"\n\tif not manifest_files:\n\t\tlogging.error(f\"No manifest file found\")\n\t\tsys.exit(3)\n\tlogging.info(f\"Found {len("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tif digests_used:\n\t\t\treturn\n\tfor manifest_file in manifest_files:\n\t\twith open(manifest_file, 'r') as manifest:\n\t\t\tfor line in manifest:\n\t\t\t\titems = line.strip().split()\n\t\t\t\tif len(items) >= 3 and items[2]!= items[0]:\n\t\t\t\t\tdigest = items[2]\n\t\t\t\t\tself.compare_to_archive_digest(archive_file=manifest_file, digest=digest)\n\ndef get_local_files(manifest_files_dir):\n\tfiles"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest_name in digests_used:\n\t\tif digest_name in manifest_files.keys():\n\t\t\treturn False\n\n\treturn True\n\nclass build_file:\n\t\"\"\"\n\tThis class is used by the build script to handle each target,\n\teach of which was added to the build script's `targets` dict. This class\n\tis the top-level class that is used to build the whole build tree.\n\n\tFor the purposes of the class and functions within, this class doesn't\n\thave much to do with either the manifest or the CMake\n\tgenerated"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigest_list = [\"sha256\", \"sha384\", \"sha512\", \"blake2b\", \"blake2s\"]\n\n\tif(len(manifest_files) > 0):\n\t\tfor file in manifest_files:\n\t\t\tif(os.stat(file).st_size > 0):\n\t\t\t\tfor this_digest in digest_list:\n\t\t\t\t\t#print(\"\\n===============================\")\n\t\t\t\t\tdigest_found = False\n\t\t\t\t\tfile.seek(0)\n\t\t\t\t\tdata = file.read()\n\t\t\t\t\tfor i in range(0, len(data"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest_file in digests_used:\n\t\t\tif digest_file == manifest_file or str(digest_file) in manifest_file:\n\t\t\t\twith open(manifest_file) as f:\n\t\t\t\t\tcheck_content = f.read().strip()\n\t\t\t\tif check_content.startswith('*'):\n\t\t\t\t\tmessage = (\"manifest for '{}' has empty or wildcarded '*' \"\\\n\t\t\t\t\t            \"digest file present, expected no manifest in \"\\\n\t\t\t\t\t            \"tarball\".format("}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tassert(prior.version >= self.version)\n\ndef validate_version_in(self, item, prior):\n\"\"\"\n\tDo not use this function in either of the following classes:\t\n\t\n\t(i) an InventoryValidator class, \n\t(ii) the Inventory class, or other validator class. \n\t\n\tThis method may be used if it is known that the item's version must be\n\tprior to that defined by the given prior item. This is used for items which\n\tare found in an older collection. If the item is found"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not prior.is_valid():\n\t\traise ValueError(f'Prior version was not a valid value for either the prior or current version.')\n\t\ndef check_versions_match(self, prior):\n\"\"\"\tCheck that the versions match, allowing for a change in the\n\tprior version being used.\n\t\n\tRaise a ValueError if they do not match.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\ndef"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tif prior.number!= self.number:\n\t\traise ValueError(\n\t\t\t\"Prior version (%d) does not match the current version (%d)\"\n\t\t\t%(prior.number, self.number)\n\t\t)\n\t\n\treturn\n\n\nclass InventoryValidator(object):\n\t\"\"\"\n\tThe InventoryValidator class represents an InventoryValidationResult\n\tand contains methods for validating and reporting on a given\n\tInventory object.\n\t\n\tThe input Inventory object is assumed to have been checked for consistency\n\tbefore this class is used.\n\t\"\"\"\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\treturn prior.ver\n\"\"\"\n\nfrom rtconfig import *\nfrom copy_repository_inventories import *\nfrom copy_repoarchive_inventories import *\nfrom version import VERSION\n\n\n\nInventoryValidator.set_latest_inventory = InventoryValidator.set_prior_inventory\n\nclass IncompleteCopyRepoInventory(CopyRepoInventory):\n\t\"\"\"\n\tA version of the CopyRepoInventory class that will only create a temporary repo inventory file if a copy of the original inventory's archive is not available.  This allows for"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\t# Get the 'prior' version of the inventory\n\t\n\tprior_version = prior.version\n\t\n\t# If the prior version is greater than the current version, ignore\n\t\n\tif prior_version > self.version:\n\t\treturn\n\t\n\t# Check the minor version portion of the prior version against the current version\n\t\n\tprior_vers_minor = prior_version.split('.')[1]\n\t\n\tif int(prior_vers_minor) > self.version.minor:\n\t\treturn\n\t\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# Make sure prior isn't empty\n\tif len(prior.split('.')) < 2:\n\t\traise InvalidVersionException('Prior version'+ prior +'is invalid.')\n\n\"\"\"\n  End helper function declarations\n\"\"\"\n\n\n#########\n# Helpers\n\n\n\nclass InvalidVersionException(Exception):\n\t\"\"\"This is a generic exception class for when a version or version range is an invalid.\n\tIt just raises an exception to be caught in a try/except block.\n\t\"\"\"\n\tdef __init__(self, version_value_str):\n\t\tException."}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tmyVer = self\n\twhile not isinstance(myVer, PriorVersionOfVersion):\n\t\tmyVer = myVer.prior_version\n\treturn myVer\n# def __getattr__(self, attr):\n\n#     \"\"\"Override __getattr__ function, to raise an AttributeError if the instance\n#     does not have \"attr\"\"\"\"\n#     raise AttributeError(\"'%s' instance has no attribute '%s'\" % (self.__class__, attr))\n\n\nclass FileList(list):\n\n    \"\"\"Just a list with"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t \n\t\tif not(isinstance(prior, InventoryValidator) and self.valid):\n\t\t\tvalidity_err = \"Invalid prior version \" + str(prior.prior_version)\n\t\t\tself.log.error(validity_err)\n\t\treturn\n\nclass CCS():\n\t\"\"\"Class to contain component control script data\n\t\"\"\"\n\tdef __init__(self, name='None', version='Unknown', description=\"No Description\"):\n\t\t\"\"\"\n\t\tCreate a CCS script object with these four attributes;\n\t\tname, version, description, and checks"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif (prior is None)  or (type(prior) is not InventoryValidator):\n\t\treturn 1\n\t# check prior is prior to prior version of current object (if any, i.e. not already known)\n\tif not self.is_known(prior.prior_version):\n\t\treturn 1\n\treturn prior.is_current()\n\t\n\ndef get_next_version(current_version, prior):\n\t\"\"\"\n\tReturns the next prior version of the current inventory object,\n\ti.e. prior version prior to the current version,"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tcheck_is_a_valid_prior_version(prior, self)\n\t\ndef validate_as_possible_prior_version(self, prior):\n\"\"\"[Inventory.validate_as_possible_prior_version]\n\t\n\tValidate prior is a valid prior version of the current inventory in any of the supported\n\tversions of its major/minor components.\n\n\tInput:\n\tself\n\t\tRequired. The inventory that will be validated against the prior.\n\tprior\n\t\tRequired. The prior inventory\n\t\n\tOutput:\n"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlog.debug(\"get_logical_path_map(); inventory='%s'; version='%s'\" %(inventory, version))\n\n\t# Find the inventory entry for this version & load the inventory path cache.\n\tif version not in inventory:\n\t\tlog.error(\"Version not found in inventory; version=%s, inventory=%s\" %(version, inventory))\n\t\treturn None\n\n\tcache = inventory[version]['cache']\n\tif cache is None:\n\t\tcache = InventoryPath(version)\n\n\n\t# Make a map from the logical path to a set of"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tdef listdir_on_path(inventory, path):\n\t\t\"\"\"\n\t\tHelper function for get_logical_path_map. Returns the list of files\n\t\tfor a given path (given as an inventory), or on disk.\n\t\t\n\t\tIf return_names is True, the directories will be kept for later.\n\t\t\"\"\"\n\t\tif inventory[path].dir:\n\t\t\treturn inventory[path].dir\n\t\telse:\n\t\t\treturn listdir_on_path(inventory, inventory[path].parent)\n\t\t\t\n\tdef get_logical_path_files(inventory"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_data = {}\n\tlogical_files = inventory.logical_files(version)\n\n\tfor lp, lf in logical_files.items():\n\t\t# We have a logical path, get all versions\n\t\tlogical_data[lp] = set()\n\t\tfor lfi in lf:\n\t\t\t# We have a physical path for file, get its data\n\t\t\tfor version_data in inventory.read(lp, lfi):\n\t\t\t\tfile_path = version_data.path\n\t\t\t\tpath_parts = file_path.split(os.sep)"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tcwd = inventory.start_point\n\n\tversion_state_files = {}\n\t\n\tstate_root = os.path.join(inventory.root_location, version, \"root\", \"state\")\n\tdirs_to_visit = [state_root]\n\t\n\twhile dirs_to_visit:\n\t\tdirectory = dirs_to_visit.pop()\n\t\tif not os.path.isdir(directory):\n\t\t\tcontinue\n\t\tif os.path.basename(directory) in (\"root\", \"state\"):\n\t\t\t# this is"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tif version in inventory:\n\t\tfiles = inventory[version]\n\telif version == 0: \n\t\tfiles = inventory\n\telse: \n\t\tlog.fatal(\"No such version '%s' in inventory\", version)\n\t\tsys.exit(1)\n\t\n\tlogical_path_map = make_logical_path_map(files)\n\treturn logical_path_map\n\ndef make_logical_path_map(files):\n\t\"Makes a map of logical path name -> set(files whose path is a logical file path)\"\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor state in inventory.versions[version]: # iterate through state, sorted by 'lastModifiedAt', so that files with this timestamp are always in chronological order.\n\t\tfor key in state['contents'].keys():\n\t\t\tcontent = state['contents'][key]\n\t\t\tif isinstance(content, DirectoryEntry):\n\t\t\t\tif logical_path_map.get(content.logical_path) == None: # only create logiplith-map if not yet visited\n\t\t\t\t\tlogical_path_map[content.logical_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t# If no inventory was provided, return an empty dictionary as it will be\n\t# used by the 'path_info_by_state' method below for an empty inventory\n\tif not inventory:\n\t\treturn {k: set() for k in inventory.items()}\n\n\tlog_map = {}\n\tfor ikey in inventory:\n\t\tlog_path = os.path.abspath(inventory[ikey][1])\n\t\tcurrent_state = inventory[ikey][2]\n\t\tif version or current_state < current.STATE_VERSION_NO_FILES"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tlog.info('Parsing state for version %s', version)\n\t\n\tinventory_content_list = []\n\tfor (path, content) in inventory.state.iteritems():\n\t\tinventory_content_list.extend( content )\n\t\n\t# We need to know how to find the inventory and its files in this\n\t# version of the state file, in order to map logical paths in\n\t# inventory to files on disk\n\t(inventory_file_name, inventory_file) = inventory_content_list[0]\n\t(version_path"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\t# Initialize the map\n\tlogical_path_to_files = {}\n\t\n\t# First figure out the content_type of the revision\n\tif version == \"\":\n\t\traise ValueError(\"Cannot get logical path map for empty version\")\n\tcontent_type = inventory.get_content_type(version)\n\t\n\t# For each file, track which paths have been seen in this version too\n\tfor path in inventory.content_history(version):\n\t\tfor state_path in inventory.ancestors(version, path):\n\t\t\t# Only care about"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor version in inventory.keys():\n\t\tpath_to_version_files = version_files = None\n\t\tif version == version.base:\n\t\t\treturn {}\n\t\tbase_inventory = inventory.get(version.base)\n\t\tif not base_inventory:\n\t\t\tbase_inventory = inventory_get_by_directory_for_version(version)\n\t\tif base_inventory:\n\t\t\tpath_to_version_files = _get_paths_to_version_files(base_inventory)\n\t\tif path_to_version"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfrom types import IntType, LongType, DictType\n\timport dsz\n\timport os\n\tlocalPath = fixity['LocalPath']\n\tdel fixity['LocalPath']\n\tif (len(fixity)!= 0):\n\t\tdsz.ui.Echo(('InvData.inventory: Error, fixity has %d extra items:...' % len(fixity)), dsz.ERROR)\n\t\tfor curName in fixity:\n\t\t\tdsz.ui.Echo(('  curName = %s' % curName), dsz"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# Validate that manifest file has the fixed number\n\t# of records.\n\texpected = len(fixity)\n\tactual = self.get_n_file(manifest_files)\n\tcount = int(actual/expected)\n\tif  count!= expected:\n\t\treturn False, 'Fixity does not match the manifest file\\n'\n\t# Validate that all files referenced in the fixity block\n\t# are listed in the manifest.\n\tinvalid = []\n\twith open(fixity) as fix:\n\t\tlines = fix.readlines()\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif not type(self.inventory.fixity) is dict:\n\t\traise TypeError(util.format_message(\"fixity data is not a dict: %r\", self.inventory.fixity))\n\tif self.inventory.fixity['type']!= constants.FIXITY:\n\t\traise ValueError(util.format_message(\"Invalid type '%s' for fixity block\", self.get_type_string(constants.FIXITY)))\n\t\n\tif not self.inventory.inventory.get('files', None):\n\t\traise ValueError(\"Inventory data does not"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tpass\n\ndef valid_charset(charset):\n\t\"\"\"\n\tValidate character set.\n\t\n\tValidate given char set and returns true or false\n\t\"\"\"\n\tpass\n\ndef validate_chunk_hash(chunk_hash):\n\t\"\"\"\n\tValidate chunk hash.\n\t\n\tCheck that it's the right size for a MD5 hash and it's consistent within\n\tthe header.\n\t\"\"\"\n\tpass\n\ndef validate_check_value(check_value):\n\t\"\"\"\n\tValidate check value.\n\t\n\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tvalid, message = check_manifest(fixity, manifest_files)\n\t\n\tif not valid:\n\t\traise ValueError(message)\n\t\n\tref=fixity[\"ref\"]\n\t\n\t\n\t#Validates that ref is in the manifest - this is done because I would want\n\t#to create a list of files in the inventory to check against the manifest\n\tfiles=manifest_files.get_files(ref)\n\t\n\tif not files:\n\t\traise FileError(\"fixity reference: \" + json.dumps(fixity))"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif not fixity:\n\t\tself.error = \"fixity block must contain all manifest files\"\n\t\treturn False\n\tif not isinstance(fixity, dict):\n\t\tself.error = \"fixity block must be a dictionary\"\n\t\treturn False\n\n\t# First iterate through the manifest files to get a list of all files defined in\n\t# the file block. The content of those files may contains newlines (indicating \n\t# subdir).\n\tif \"file\" in fixity and fixity[\"file\"] is not None:\n\t\tfor filename in fixity[\""}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif fixity == None:\n\t\treturn\n\telse:\n\t\tassert type(fixity) == list, \"fixity must be a list\"\n\t\t# This is required to make list of list easy to compare. \n\t\tfiles = list(map(tuple, fixity))\n\t\tfor manifest_file in manifest_files:\n\t\t\tassert manifest_file in files, \"fixity entry for %s missing\" % manifest_file\n\t\nclass fixity_check:\n\t\"\"\"\"\"\"\n\t\n\tdef __init__(self, inventory):\n\t\t\"\"\"\"\"\"\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor filename, data in fixity.items():\n\t\t# Check the content of the fixity block. \n\t\t# If there is a '\\0' character in the body, it means that this file\n\t\t# is malformed. \n\t\tdata = bytes(data, encoding='utf-8')\n\t\tif b'\\0' in data:\n\t\t\traise InvalidInventoryError(\n\t\t\t\t_(\"Incorrect File Block in file %s: %s\") %\n\t\t\t\t(manifest_files[filename].name, filename))\n\treturn True\n\n\ndef validate_signatures("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tpass\ndef validate_manifest_fields(self, manifest_block):\n\"\"\"\n\tValidate that the manifest has keys that are supported by a\n\tdictionary version of an MFD.\n\t\n\tThis method should be applied after validate_file_list_and_sizes has been\n\tapplied.\n\t\"\"\"\npass\ndef validate_metadata(self):\n\"\"\"\n\tValidate that the MDMX manifest is fully compliant according to spec.\n\t\n\tAlso check that the manifest matches the MDMX schema.\n\t\"\"\"\npass\ndef validate_mfd_fields"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\titems_in_fixity = list(fixity.items())\n\tfixity_count = len(items_in_fixity)\n\n\tif fixity_count == 0: \n\t\traise MismatchedInventoryException('The fixity block in the inventory'+\n\t\t\t' should contain at least one item!')\n\n\tif fixity_count == 1:\n\t\tfixity_name, fixity_data = items_in_fixity[0]\n\t\tmanifest_name = fixity_name\n\n\t\trefs = fixity_data\n\t\tif refs =="}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn None\n\tif not os.path.isdir(path):\n\t\tpath = os.path.dirname(path)\n\tfor folder_name, folder_names, file_names in os.walk(path):\n\t\tfor i in file_names:\n\t\t\tabs_path = os.path.join(folder_name, i)\n\t\t\tfile_name, file_extension = os.path.splitext(abs_path)\n\t\t\tyield file_name, file_extension, abs_path"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn glob.glob(path + '**/*', recursive=True)\n\n\ndef get_files_and_dirs(path):\n\tfiles.extend(files_list(path))\n\treturn files\n\n\ndef get_file_size(files, file):\n    \"\"\"\n    Gets and prints each file size \n    file : the file name\n    \"\"\"\n    if (file in files):\n        print(\"Size of %s: %.0f KB\" %(file,round((stat(file).st_size /1000),3)))\n    else:"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn glob.glob(path+\"**/*\", recursive=True)\n\t\ndef make_dir(path):\n\t\"\"\"\n\tMake directory if it does not exist\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\tos.mkdir(path)\n\ndef get_timestamp(fname):\n    \"\"\"\n    Return the seconds since the epoch for file `fname`\n    \"\"\"\n    creation_time = os.path.getctime(fname)\n    return creation_time\n    \ndef get_seconds(fname):"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tdef visit_files(arg, dirname, names):\n\t\t\"\"\"\n\t\tArgument `arg` is only used for the purposes of calculating the\n\t\tlists of files to be visited.  It will be ignored internally.\n\t\t\"\"\"\n\t\tresult = []\n\t\tfor name in names:\n\t\t\tfullname = os.path.join(dirname, name)\n\t\t\tif os.path.isdir(fullname):\n\t\t\t\tresult.extend( files_list(fullname))\n\t\t\telse:\n\t\t\t\tresult.append( fullname)\n\t\treturn result\n\treturn visit_files"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn sorted(os.listdir(path))\n\ndef files_path(folder, filename):\n\"\"\"\n\tReturn the relative path of the file `filename` in `folder`\n\t\"\"\"\n\treturn os.path.join(folder, filename) if not os.path.isdir(folder) else os.path.join(folder, filename, filename)\n\ndef abs_path(*args):\n\"\"\"\n\tConvert `args` into absolute path using a relative current path.\n\t\"\"\"\n\treturn os.path.abspath(os.path.join(os."}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn[f for f in glob.glob(path)]\n\ndef get_path(path):\n\t\"\"\"Get the file/path if available\"\"\"\n\treturn os.path.abspath(path)\n\ndef check_dir(path):\n\t\"\"\"\n\tChecks if a dir 'path' exists. If it doesn't, \n\tit creates that dir\n\t\"\"\"\n\tif not os.path.isdir(path):\n\t\ttry:\n\t\t\tos.mkdir(path)\n\t\texcept OSError as e:\n\t\t\tprint(e)\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn os.listdir(os.path.expanduser(path))\nwith open(os.path.expanduser(\"~/Desktop/Bin/project2.json\"), \"w\") as file:\n\tkeywords = []\n\n\tfor key in dictionary.keys():\n\t\titem = str(sorted(dictionary[key][\"search\"].items()))\n\t\tkeywords.extend(item.strip(\"[]\").split(\",\"))\n\t\tkeywords = list(set(keywords))\n\t\tkeywords = sorted(keywords, key=str.lower)"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tlist = os.listdir(path)\n\tfull_list = []\n\tfor item in list:\n        \tif os.path.isfile(os.path.join(path, item)):\n                \tfull_list.append(item)\n\treturn full_list\n\t\ndef add_time(filename, time_step):\n    \"\"\"\n\tAdds time step before the end of filename\n\n\t\"\"\" \n    i = int(str('{:0'+str(time_step)+'d}').format(0))\n    with tempfile.Named"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfor f in os.listdir(path):\n\t\tyield os.path.join(path, f)\n\n\ndef files_search(path, pattern='*.png'):\n\t\"\"\"\n\tReturn a generator that match pattern in files in `path`\n\t\"\"\"\n\tpattern = os.path.normcase(pattern)\n\tyield from (p for p in files_list(path) if fnmatch(p, pattern))\n\n\ndef get_last_version(path):\n\t\"\"\"\n\tReturn the last file version\n\t\"\"\"\n\treturn max(version for version"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn glob.glob('%s/*' % path)\n\ndef loadimgs(input, resize=1.0):\n\t\"\"\"\n\tload raw images\n\t\"\"\"\n\ttry:\n\t\timages = [loadimg(x, resize=resize) for x in files_list(input)]\n\texcept:\n\t\timages = []\n\treturn images\n\ndef loadimg(path, resize=1.0):\n\t\"\"\"\n\tload img from the `path`\n\t\"\"\"\n\tim = Image.open(path).convert('RGB')\n\tif resize"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls = xmls or []\n\tgrouped_files = {}\n\tfor file in files:\n\t\txml_filename = os.path.basename(file)\n\t\t# If the filename starts with \"_\" - it is an empty xml file\n\t\tif not xml_filename or (not xml_filename.startswith('_') and '.' in xml_filename):\n\t\t\tcontinue\n\t\tfor xml in xmls:\n\t\t\tif xml in file:\n\t\t\t\tif not file in grouped_files:\n\t\t\t\t\t# if there is no file with the same xml name in the group\n\t\t\t\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tfile_dict = OrderedDict()\n\t\n\t# Group files by their basename\n\tfor xml in xmls:\n\t\tfilename = os.path.basename(xml)\n\t\tfile_dict[filename] = get_file_data(source, filename)\n\t\t\n\t# Group files by their XML basename and return data in dict\n\tfor key in file_dict.keys():\n\t\tdata = file_dict[key]\n\t\tdata['basename'] = key\n\t\t\n\t\tfor item in list(data): # make it a copy\n\t\t\tif isinstance"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t# find XML filename\n\txml_filenames = _group_files_by_files(source, files, '.xml')\n\t\n\t# prepare dict\n\txml_filenames = _group_files_by_xml_filename(source, xml_filenames, files)\n\t\n\t# return data\n\treturn xml_filenames\n\t\n\t\t\ndef _group_data_by_json_filename(source, xmls, jsonfiles):\n\"\"\"\n\tGroup data by their JSON filenames\n\n\tGroups data by their JSON filenames and"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\toutput = dict()\n\tfor xf in xmls:\n\t\toutput[xf] = list()\n\t\n\tfor f in files:\n\t\t# try:\n\t\t\tif not f.endswith('.xml'): raise Exception(\"Unexpected file\")\n\t\t\tif not source.endswith(file_separator):\n\t\t\t\traise Exception('Unexpected filename structure. It should end with a <{}>. The file <{}> is not supported'.format(file_separator,source))\n\t\t\t\n\t\t\tfilesize_prefix=None\n\t\t\tif source.endswith"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tif os.path.isdir(source):\n\t\tsrc = os.path.basename(source)\n\telse:\n\t\tsrc = os.path.splitext(src)[0]\n\t# Split the path as we need it to get xml filename for all files\n\tparts = src.split(XML_FILE_SEPARATOR)\n\t# If there was only one part, then we assume the source\n\t# is the XML filename\n\tif len(parts) == 1:\n\t\tparts = (src, )\n\t# In the next, we can"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\txml_by_file = {}\n\t\n\tfor file in files:\n\t    with open(source + file, 'r') as stream:\n\t        data = yaml.load(stream)\n\t        xml_by_file[data['xml_filename']] = Package(data)\n\t        \n\treturn xml_by_file\n\t\n\t\ndef _validate_file_type(value, allowed_file_types):\n\t\"\"\"\n\tValidate file extension.\n\t\n\tParameters\n\t----------\n\tvalue : str\n\t    value\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif isinstance(source, str):\n\t\tdata = dict()\n\t\tfor name in xmls.keys():\n\t\t\tif not name.endswith('.xml'):\n\t\t\t\tname += '.xml'\n\t\t\tdata[name] = Package(os.path.join(source, name), files)\n\t\td_to_sort = defaultdict(list)\n\t\tfor k in data.keys():\n\t\t\td_to_sort[os.path.splitext(k)[0]].append(\n\t\t\t\t(data[k], k))\n\t\tfor k, v in d"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tgrouped = {}\n\tfor xml_filename in xmls:\n\t\tsplit_xml_filename = xml_filename.split(\".\")[0]\n\t\tsplit_xml_filename = split_xml_filename.replace(\"-\", \"_\")\n\t\tgrouped[split_xml_filename] = \"Not_done\"\n\ti = 1\n\tfor xml_filename in files:\n\t\tsplit_xml_filename = xml_filename.split(\".\")[0]\n\t\tsplit_xml_filename = split_xml_filename.replace(\"-\", \"_\")\n\t\tif split_xml_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t# print(\"\\tXML files: %i\" % len(files))\n\tfiles_by_xml = {}\n\tfor i, file in enumerate(files):\n\t\tfor k, v in files_by_xml.items():\n\t\t\tif v['xml_filename'] == file.file_name:\n\t\t\t\tv['files'].append(file)\n\t\t\t\tbreak\n\t\telse:\n\t\t\tfiles_by_xml[file.file_name] = {'xml_filename': file.file_name, 'files': [file]}\n\treturn files_by_xml"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tfiltered_files = {k:v for k,v in [(os.path.basename(value),value) for value in files if value.endswith('.xml') or value.endswith('.xsd') or value.endswith('.xlsx') or value.endswith('.jpg') or value.endswith('.png') or value.endswith('.docx')]}\n\treturn {a:(f) for f,a in {v:k for k,v in filtered_files.items()}.iteritems()}"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif prefix in file_path:\n\t\treturn True\n\treturn False\n\n\ndef make_pkg_dir(pkg_name):\n\t\"\"\"\n\tMake a new local folder `directory` in the package cache named with `pkg_name`\n\t\t\n\t`directory` - str\n\t\tLocal directory name\n\t\"\"\"\n\t\n\tdirectory = '{}/{}'.format(pkg_root, pkg_name)\n\t\n\treturn directory\n\n\n@register_action('install/doc')\ndef install_doc(file_path):\n\t\"\"\"\n\tInstall"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tprefix = prefix + \"/\" + file_path.split(\"/\")[-1]\n\treturn prefix in docs_prefixes\n\ndef match_file_by_extension(extension):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given extension\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\textension : str\n\t    File extension\n\tReturns\n\t-------\n\tbool\n\t\"\"\"\n\treturn ext == \".py\"\n\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tfilename_prefix, filename_suffix = os.path.splitext(file_path)\n\tif filename_prefix.startswith(prefix):\n\t\treturn True # Path starts with the prefix of the package\n\telse:\n\t\treturn False\n\ndef validate_documents_paths(document_names, path_files):\n\t\"\"\"\n\tVerify if paths in the provided `document_names` of a list of `file_paths` actually exist.\n\tIt requires files in the `files_paths` with the exact same `document_names`\n\n\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif (file.endswith('.txt')!= True) :\n\t\tfor doc_path in glob.glob(\"./\"+package+\"/*.txt\"):\n\t\t\tif (file_path in doc_path):\n\t\t\t\treturn doc_path\n\telif (file.endswith('.html')!= True) :\n\t\tfor doc_path in glob.glob(\"./\"+package+\"/*.html\"):\n\t\t\tif (file_path in doc_path):\n\t\t\t\treturn doc_path\n\telif (file.endswith('.pp"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn match(prefix, file_path)\ndef find_all_packages_in_folder(folder):\n\"\"\"\n\tGenerate a list of packages inside a folder\n\"\"\"\n\tlst_packages,lst_subpackages = find_all_packages_rec(folder,prefix_pattern_documents_package)\n\treturn lst_packages+lst_subpackages\ndef find_all_subpackages_in_folder(folder):\n\"\"\"\n\tGenerate a list of subpackages inside a folder\n\"\"\"\n\treturn find_all_packages_rec(folder,prefix"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tif sys.platform == 'win32':\n\t\treturn file_path.endswith('.docx') or \\\n\t\t\tfile_path.endswith('.doc') or \\\n\t\t\tfile_path.endswith('.docm') or \\\n\t\t\tfile_path.endswith('.docb') or \\\n\t\t\tfile_path.endswith('.xlsx') or \\\n\t\t\tfile_path.endswith('.xls') or \\\n\t\t\tfile_path.endswith('.xlsm') or \\\n\t\t\tfile"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tprefix_len = len(prefix)\n\n\tlen_file = len(file_path)\n\tif ((len_file > prefix_len) and (file_path[:prefix_len] == prefix)):\n\t\treturn True\n\telse:\n\t\treturn False\n\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\"\"\"\n\t:type prefix: str\n\t:type file_path: str\n\t:rtype: bool\n\t\"\"\"\n\n \n    with tqdm(total=len(list_files_inside_dir(file_path)), desc=\"match_file_by_prefix\") as pbar:\n        for filename in get_files_path_from_dir(file_path):\n            if not(os.path.basename(filename).startswith(prefix)+\".json\"):\n                continue\n            # if this file isn't already in the dict, add it\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn file_path.startswith(prefix)\n\ndef create_package_db(db_path, file_set):\n\"\"\"\n\tAdd all `file_set` to a document package `db_path`.\n\tThe name of all files is in the `DB_PATH` subdirectory.\n\t\n\tParameters\n\t----------\n\tdb_path : str\n\t    Path to doc package database\n\tfile_set : set of str\n\t    Set of file names to add to the database (should correspond to `db_path`)\n\t    \"\"\"\n\tif not"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t#TODO: make this more efficient\n\tpattern = re.compile(r\"^\" + prefix + r\"(\\.(pdf|jpe?g|png|jpg|gif|bmp))$\")\n\tif pattern.match(file_path):\n\t\treturn True \n\telse:\n\t\treturn False"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tresults = []\n\t\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tresults.append(file)\n\treturn results\n\t\n\ndef select_files_by_hash(file_hash_list):\n\"\"\"\n\tGet files which are in a document package.\n\t\n\tRetorna a lista de arquivos cujas etiquetas de hash contem uma das strings iniciais contidas\n\tdentro de `file_hash_list`\n\t\n\tParameters\n\t----------\n\tfile_hash_"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfilenames_found = []\n\tfilenames_not_found = []\n\tfor filename in files:\n\t\tif os.path.basename(filename).startswith(prefix):\n\t\t\tif os.path.isdir(filename):\n\t\t\t\tfilenames_found.append(filename)\n\t\t\telse:\n\t\t\t\tfilenames_not_found.append(filename)\n\t\telse:\n\t\t\tif os.path.isdir(filename):\n\t\t\t\tfilenames_not_found.append(filename)\n\tfilenames_found = sorted(filenames"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tif not prefix:\n\t\treturn files\n\treturn [\n\t\tf for f in files\n\t\tif os.path.basename(f) == prefix\n\t]\n\ndef select_files_by_extension(ext, files):\n\"\"\"\t\n\tGet files which has the extension `[ext]`.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `[ext]`\n\t\n\tParameters\n\t----------\n\text : str\n\t    Filename extension\n\tfiles : str list\n\t    Files"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\"\"\"\n\tif not isinstance(prefix, str):\n\t\tmessage_box('Incorrect prefix', '`prefix` must be `str`.')\n\t\treturn\n\tif not isinstance(files, list):\n\t\tmessage_box('Incorrect files', '`files` must be `list`.')\n\t\treturn\n\tfor file in files:\n\t\tif not isinstance(file, str):\n\t\t\tmessage_box('Incorrect files', '%s must be `str`.' %file)\n\t\t\treturn\n\tif not is_text_file(prefix):\n\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn list(filter(lambda f: os.path.basename(f).startswith(prefix), files))\n\ndef get_files_relative_to_input(filenames):\n    \"\"\"\n        Get files in data/input folder or project folder, if you run this file from project folder.\n        Also create input folder if it doesn't exist.\n        Input folder should have all files relative to project folder structure.\n    \n        Parameters\n        ----------\n        filenames : str list\n            List of input documents' filename\n        \n        Returns\n        -------\n        list"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = get_documents_paths(files)\n\treturn list(\n\t\tfilter(\n\t\t\tlambda x: x.startswith(\"%s-\" % prefix),\n\t\t\tfiles\n\t\t)\n\t) \\\n\t       if files is not None else []\n\n\n\ndef set_filename_by_prefix(prefix, filename):\n\"\"\"\n\tSet filename by prefix.\n\t\n\tRetorna um novo nome\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfilename : str\n\t    Filename to be modified\n\tReturns"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [file for file in files if file.startswith(prefix)]\n\n\ndef find_file(file_path):\n\t\"\"\"\n\t\tGet a path of a file by a name from `/data_file`\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t\tThe file path\n\tReturns\n\t-------\n\tstr\n\t\tPath of the file\n\t\"\"\"\n\treturn _get_file_paths(file_path,\n\t                        prefix='/data_file/',\n\t\t\t\t\t\tprefix_regex='_.*',\n\t\t\t\t\t\tprefix_delimeter"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n\ndef select_files(data_dir, prefix, suffix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna as pastas caminhantes `data_dir` com nomes de arquivos em `prefix` e seus\n\tsufixos em `suffix`\n\t\n\tParameters\n\t----------\n\tdata_dir : str\n\t    Directory path\n\tprefix : str\n\t    Filename prefix\n\tsuffix : str\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tprefix = prefix.lower()\n\tfiles = sorted([os.path.basename(path) for path in files])\n\tif prefix in files:\n\t\tidx = files.index(prefix) # Invert order\n\t\tlogger.info('Return files from index {}'.format(idx))\n\t\tfiles = files[idx:]\n\treturn files\n\ndef get_filename_from_filepath(filepath:str):\n\"\"\"\n\tGiven a file path, this reads the filename\n\t\n\tParameters\n\t----------\n\tfilepath : str\n\t    File path"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfor fname in list(files):\n\t\tif fname.startswith(prefix+'\\n'):\n\t\t\tfiles.remove(fname)\n\treturn files\n\ndef fix_all_duplicate_keys_in_json(config):\n\"\"\"\n\tCorrect the duplication of keys in a json-config files.\n\t\n\tReplace the old filenames with new filenames\n\t\n\tUseful for the `get_prefixes_from_list_file` function\n\t\n\tParameters\n\t----------\n\tconfig : dict\n\t    Config dict"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata_by_basename = {}\n\tfor xmlfile in sorted(glob.glob('{}/**/*.xml'.format(folder), recursive=True)):\n\t\tbasename = xmlfile.split('/')[-1].split('.')[0]\n\t\tpkg_name = basename.split('-')[0]\n\t\tfilename = basename.split('-')[1]\n\t\t# print(pkg_name, filename)\n\t\tf = open(xmlfile, encoding='utf-8')\n\t\tfreader = c"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfrom xml.etree import ElementTree\n\t\n\tfiles = {}\n\tfor file in sorted(os.listdir(folder)):\n\t\tif file.endswith('.xml'):\n\t\t\tbasename = os.path.splitext(file)[0]\n\t\t\tparser = ElementTree.XMLParser(encoding=\"utf-8\")\n\t\t\ttree = ElementTree.parse(os.path.join(folder, file), parser)\n\t\t\tfiles[basename] = _convert_from_xml(basename, tree, parser)\n\treturn files\n\n\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles_dict = {}\n\tfnames = sorted(get_files_in_subdirectories(folder, \n                                                recurs=True, \n                                                exts=[\"*.dbc\", \"*.xml\"]))\n\tfor fname in fnames:\n\t\t# extract name\n\t\tif type(fname).is_string_type:\n\t\t\tbasename = os.path.splitext(os.path.basename(fname))[0]\n\t\t\tbase_dir = os.path.dirname(fname)\n        # Get only the"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfileList = listdir(folder)\n\t# Filter files\n\tdata = {}\n\n\tfor filename in fileList:\n\t\tdata[filename.upper()] = _get_data_from_package(filename, folder)\n\treturn data\n\n\ndef get_data_from_root_folder(root_folder):\n\t\"\"\"\n\tReturn data dict from root folder\n\t\n\tParameters\n\t----------\n\troot_folder : str\n\t    Root folder of the packages \n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = _explore_folder(root"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = os.scandir(folder)\n\tpack = {}\n\tfor f in files:\n\t\tb = os.path.splitext(f.name)\n\t\tif b[1] in ('xml'):\n\t\t\tpack.update({b[0] : _parse_xml(f.path)})\n\t#print(dict(pack))\n\t#print('folder = ', folder)\n\treturn pack\ndef _explore_fld(fld):\n\"\"\"\t\n\tHelper fn that takes input of only one folder\n\tand returns the contents of"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\ttry:\n\t\tbasename = [os.path.basename(pkgfile) for pkgfile in os.listdir(folder) if '.pkg' in pkgfile].pop()\n\t\tpkg = etree.parse(os.path.join(folder, basename))\n\texcept Exception:\n\t\tpkg = None\n\telse:\n\t\tpkg = {basename.split('#')[0].split('_')[0]:package_class.from_package(basename) for basename in os.listdir(folder) if '.pkg' == os.path."}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles, xmls = get_files_and_xmls(folder)\n\t\n\tpackage_dict = {}\n\tfor xml_path in xmls:\n\t\tfile_names = []\n\t\tbasename = _get_basename_from_xml(xml_path)\n\t\t\n\t\tfor file_name in files:\n\t\t\tif _check_file_name(basename, file_name):\n\t\t\t\tfile_path = os.path.join(folder, file_name)\n\t\t\t\tdata = _get_data_from_file_path(file_path)\n\t\t\t\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = {}\n\tfor file_name in files_in_folder(folder):\n\t\tprint(file_name)\n\t\tif file_name.endswith(\".xml\"):\n\t\t\tpackage = parse_package_xml(file_name, folder)\n\t\t\tpackage_name = package.get(\"@name\")\n\t\t\tif not result.has_key(package_name):\n\t\t\t\tresult[package_name] = package\n\t\telse:\n\t\t\tif folder == \"/pkg/pix4d_tools_c/app/app_helpers/scripts\":\n\t\t\t\tcontinue\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\ttry:\n\t\tlogger.info(folder)\n\t\tos.chdir(folder)\n\t\tdata = {}\n\t\tname = folder.split('/')[-2]\n\t\tdata[\"Name\"] = name\n\t\tdirs = glob.glob('*.xml')\n\t\tfor x in dirs:\n\t\t\tdata['files'][x] = _get_json_files_info(x)\n\t\tprint(json.dumps(data, indent = 4))\n\t\treturn data\n\texcept:\n\t\tlogger.error(traceback.format_exc())\n\t\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = dict()\n\tfiles = os.listdir(folder)\n\t\n\tfor filename in files:\n\t\tif filename.endswith(\".gz\"):\n\t\t\tbasename = filename[:-3]\n\t\t\ttry:\n\t\t\t\tdata[basename] = pd.read_parquet(folder+\"/\"+filename)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Could not load file:\", e, \". The file should be a.parquet file if the files were generated previously.\")\n\t\t\t\tprint(\"Uncompress it manually with pqtools and re-run this"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfile_path_split = file_path.split('/')\n\tfile_name_split = file_path_split[-1].split('.')\n\tfile_path_split[-1] = prefix + file_name_split[0] + file_path_split[-1]\n\t\n\tfile_path = '/'.join(file_path_split[:-1]) + '/'\n\t\n\tpackage = {\n\t\t'name': file_path_split[-2],\n\t\t'path': file_path,\n\t}\n\t\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tlogging.info('_eval_file: file: {0} - prefix: {1}'.format(file_path, prefix))\n\t\n\tfile_folder, filename = os.path.split(file_path)\n\t\n\t# Tipo de arquivo `asset`\n\tfile_type = get_file_type(filename)\n\t\n\tif file_type == 'asset':\n\t\tfilename, file_ext = os.path.splitext(filename)\n\t\tfilename = prefix + filename + file_ext\n\t\n\tpackages"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\troot = ElementTree.parse(file_path)\n\t\n\tfor i in root.getiterator():\n\t\tif str(i.tag).startswith('asset'):\n\t\t\tfile_extension = 'asset'\n\t\t\t\n\t\t\tbreak\n\t\t\n\t\telif str(i.tag).startswith('media'):\n\t\t\tfile_extension ='media'\n\t\t\t\n\t\t\tbreak\n\t\n\tpackage_extension = ''\n\t\n\tfile_folder = ''\n\t\n\tfor i in prefix.split('/'):\n\t\tfile"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport subprocess\n\timport xml.etree.ElementTree as xml\n\timport time\n\t\n\tdef __init__(self, base_pyscript, prefix, file_path, file_family):\n\t\tsuper().__init__(base_pyscript, prefix)\n\t\tself.base_pyscript = base_pyscript\n\t\tself.packages = {}\n\t\tself.file_family = file_family\n\t\tself.filename = file_path\n\t\tself.file_folder = file_family + file_path\n\n\tdef _eval_package(self"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif file_path.startswith(\"/\"):\n\t\tfile_path = ''.join(file_path.split()) \n\tif file_path.endswith('/'):\n\t\tfile_path = ''.join(file_path.split()) + prefix+'.asset'\n\telse:\n\t\tfile_path = file_path + '/' + prefix + '.asset'\n\t\n\tasset_data = AssetFile.objects.get(file_path=file_path)\n\tfile_object = ContentFile(asset_data.file)"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tend = file_path[len(prefix) + 1:]\n\tpackage = parse_package(file_path)\n\treturn {\n\t\t'url_name': file_path,\n\t\t'url_package': file_path.split(os.sep)[-1],\n\t\t'status': asset_type(filename=end),\n\t\t'file_name': None,\n\t\t'file_folder': None\n\t} if package else {\n\t\t'url_package': file_path.split(os.sep)[-1],\n\t\t'status': other_"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t##Identificando o nome do arquivo\n\tpackage_name = _get_package_info_name(file_path)\n\t##Identificando o tipo de arquivo\n\ttype_package = str(prefix).replace(\".xml\",\"\")\n\t# Caso o arquivo esteja no fonte destino\n\tif type_package == filespath_src:\n\t\treturn {\"src\": False}\n\t##Atualizando endere\u00e7o do arquivo\n\tpackage[\"packages\"][type_package][\"asset\"] = file_"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tarq = str(file_path).split('/', 1)[0] + '/'\n\tif re.match(prefix + '[a-z]' + '[A-Z]', file_path):#asset\n\t\tfile_name = arq + prefix + file_path#asset.jpg\n\t\treturn {\n\t\t\t\"type\": \"asset\",\n\t\t\t\"file\": file_name\n\t\t}\n\telif re.match(prefix + 'R' + '[a-z]' + '[0-9]{4}' + '[a-z]'#rend"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\"\"\"\n\tVerifica o nome do arquivo XML e se tem que ser importado ou exportado\n\texemplo: `asset.asset` ao inv\ufffdrio\n\t\n\tverifica se o arquivo tem a extens\u00c3\u00b5es `.xml`, `.xsd` ou `.xsd`. Caso exista\n\tdeve ser criado o arquivo de configura\ufffd\ufffdo\n\t\"\"\"\n\tpackage_type = str(file_path).split(\"/\")[3]\t\n\tpackage = asset"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\twith codecs.open(asset_file_path, 'r', 'utf-8') as filehandle:\n\t\tasset = xml.etree.ElementTree.parse(filehandle).getroot()\n\t\t\n\t\tif asset.tag == 'pacs':\n\t\t\tpackages[prefix] = [asset_file_path, []]\n\t\telif asset.tag == 'bundle':\n\t\t\tpackages[prefix] = [asset_file_path, asset.attrib['name']]\n\t\telse:\n\t\t\tprint(\"ERROR: '%s' is"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tlang = str(lang)\n\toriginal = file_path\n\tpdf_type = \"raw\"\n\n\tself.pdf_renditions += pdf_rendition.generate_pdf_rendition(file_path, lang, pdf_type, \n\t\t\toriginal, False)\n\n\n\t\nif __name__==\"__main__\":\n\t\"\"\"\n\tpdf_rendition = generatePDFRendition(\"test\", \"test\", \"pdf\", \"test\")\n#\tpdf_rendition = generatePDFRendition(\"artigo-02.pdf\", \"\", \"raw\","}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tresult = {\n\t    'original': '',\n\t    'en': '',\n\t}\n\tresult['original'] = os.path.basename(file_path)\n\tlangs = [lang] if not lang else langs_map.get(lang) or [lang]\n\tfor lang in langs:\n\t\tresult['en'] = os.path.join(lang, result['original'])\n\t\treturn result\n\n\ndef url_for(path):\n\t\"\"\"\n\treturn the url for the given path\n\t\"\"\"\n\tif not hasattr("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpass\n\n\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfiles = self.get_uploaded_files(file_path=file_path)\n\tif files == {}:\n\t    return None\n\n\toriginal_filename = files[\"original\"].split(\"/\")[-1]\n\tif original_filename.startswith(\".\") or files[\"original\"].lower().endswith(tuple(self.valid_formats)):\n\t    return None\n\n\tfilename = \"\"\n\ti = 0\n\twhile filename == \"\":\n\t    i += 1\n\t    filename = original_filename[:-4] + str("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpass\n\ndef get_pdf_by_name(self):\n\"\"\"\n\t{\"en\": \"artigo02-en.pdf\"}\n\t\"\"\"\n\tpass\n\ndef add_new_url(self):\n\"\"\"\n\t{\"pdf\": \"artigo02-en.pdf\"}\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpass\n\n    def list_renditions(self):\n\t\"\"\"\n\t{\n\t    \"en\": \"artigo02-en.pdf\",\n\t    \"original\": \"artigo02.pdf\"\n\t}\n\t\"\"\"\n\tpass\n\n    def get_rendition(self, lang):\n\t\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\"\n\t    }\n\t\"\"\"\n\tpass\n\n    def delete_renditions(self, lang, files: List[str]"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdata = {}\n\twith open('data/{}-{}.json'.format(lang, file_path), \"r\") as f:\n\t\tdata = json.load(f)\n\n\twith open(\"templates/rendition.html\", \"r\") as f:\n\t\tcontent = f.read()\n\tcontent = content.replace('$title$', data[\"title\"])\n\n\twith open('{}/{}-en.html'.format(BASE_DIR, file_path), \"w\") as f:\n\t\tf.write(content)\n\n\t\t#"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdef add_rendition(self, lang, file_path, data={}):\n\"\"\"\t{\n\t    \"original\": \"documento1.pdf\",\n\t    \"en\": \"documento-en.pdf\",\n\t    \"es\": \"documento-es.pdf\",\n\t}\n\t\"\"\"\n\t\tfile_name, file_ext = os.path.splitext(file_path)\n\t\tfile_ext = file_ext.split('/')[1]\n\t\twith open(file_path, 'rb') as file:\n\t\t\turl = f"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpass\n\ndef delete_rendition_by_id_language(self, id_rendition, lang):\n\"\"\"\n\t{\n\t    \"id\": id_rendition,\n\t    \"lang\": lang\n\t}\n\t\"\"\"\n\tpass\n\n\ndef delete_rendition_by_id(self, id_rendition):\n\"\"\"\n\t{\n\t    \"id\": id_rendition\n\t}\n\t\"\"\"\n\tpass"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself.list_renditions[\"rendition\"].append(\n\t\t{\"original\": file_path, \"lang\":lang}\n\t)\n\n\n\n\n\nif __name__ == \"__main__\":\n    print(\"Start to search\")\n    start_time = time.time()\n\n    root = \"../../../dataset2link_data/dataset/\"\n\n    lang_files = pd.read_csv(f\"{root}/metadata/lang.csv\")\n    lang_files_dic = {x:lang_files.loc[lang_files"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\titem = self.add_item(basename)\n\titem.path = file_path\n\nclass CatalogPage(Item):\n\t\"\"\"\n\t\"catalogPages\": {\n\t    1: {\n\t        \"pageName\": \"catalogPage_1\",\n\t        \"contents\": {\n\t            \"article01-gf01.json\": \"/path/article01-gf01.json\",\n\t        }\n\t    }\n\t}\n\t\"\"\"\n\tdef __init__(self, num, contents):\n\t\tself.num = num\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tdata = {\n\t\t\"artigo02-gf03.tiff\": file_path,\n\t\t\"artigo02-gf03.jpg\": file_path,\n\t\t\"artigo02-gf03.png\": file_path,\n\t}\n\tasset = self.get_asset(basename)\n\tfor k, v in data.items():\n\t\tasset[k] = v\n\t\tself.assess_asset(basename)\n\n\tself.asset_saver(basename, asset)\n\n\n\t\n\n\n\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tpass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.add_reference(self, basename, file_path)\n\ndef add_asset_type(self, basename, file_type):\n\"\"\"\n\"{\n\t\"artigo02-gf03\": \"image/tiff\"\n}\n\t\"\"\"\n\tself.add_reference(self, basename, file_type)\n\ndef add_metadata(self, basename, file_path):\n\"\"\"\n\"{\n\t\"xmp\": {\n\t            \"title\": \"PAPER\",\n\t            \"date\": \"2020-12-"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t#print(\"adicionando arquivo \\\"\" + str(basename) + \"\\\" na string base \\\"\" + str(base_asset_name) + \"\\\"\")\n\tself.asset_base += \"tiff::\" + basename + \"\\n\" #\"&\" + base_asset_name + \"::\" + basename + \"\\n\"    #\"&\\n\\t\\t\\t\" + base_asset_name + \"::\" + basename + \"\\n\"\n\treturn;\n\n\t\n\t\ndef remove_asset("}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tarticles = json.loads(configurations['app']['ASSETS'])\n\tif basename in articles:\n\t\treturn Response('{ \\\"error\\\": \\\"the file already exists\\\" }', 401, mimetype=\"application/json\")\n\tnew_article = Article(basename, file_path)\n\tarticles[basename] = new_article\n\treturn new_article.id\n\ndef del_asset(basename):\n\t\"\"\" Delete an already saved article \"\"\"\n\t# print(configurations)\n\tarticles = json.loads(configurations['app'"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif not file_path:\n\t\tlogger.error(f\"Invalid parameter file: {file_path}\")\n\t\n\tself.__store[basename] = file_path\n\tself.__configs[basename] = {\"path\": file_path, \"asset\": 0, \"is_preview\": 0}\n\n\tlogger.info(\"Added asset to configs.json\")\n\t\n\treturn self\n\t\n\t\n\t\n\"\"\"\n\tdef add_theme_asset(self, title, basename, file_path):\n\"\"\"\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself._db.query(\"\"\"insert into assets (filename, file_path)\n\t\t    values(%s,%s)\"\"\", (basename, file_path))\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif not self.__s3:\n\t\treturn\n\t\n\treturn self.__s3.upload_file_to_s3(file_path, basename)\n\n\t\n\t\n\tdef upload_file(self, bucket_name, filename, file_path):\n\t    \"\"\"\n\t    Faz o upload de arquivos para um determinado bucket no s3.\n\n\t    Args:\n\t        bucket_name: bucket a ser acessado\n\t        filename: nome do arquivo a ser acessado\n\t        file_path:"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t# if any(file_path in filename):  # checa a exist\u00eancia de arquivos n\u00e3o do mesmo tipo\n\t\t# os.rename(fileName, \"arquivos_processos/arquivos_processos.txt\") # renomeia o arquivo\n\t\t# print('Uma sequ\u00eancia j\u00e1 que este arquivo j\u00e1 existe foi retornada')\n\tcontents = self.assets.get(basename)\n\tif not contents: # se n\u00e3o n\u00e3o existe n\u00e3o ret"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tresult=dict()\n\tzf = ZipFile(zip_path, 'r')\n\tfor zipinfo in zf.infolist():\n\t\tfilename = zipinfo.filename.encode(sys.getfilesystemencoding())\n\t\tinfo = zf.getinfo(zipinfo.filename)\n\t\ttry:\n\t\t\tfile_data = info.extra[2] # zlib compressed files\n\t\texcept KeyError:\n\t\t\tfile_data = info.extra[3] # GNU zip files\n\n\t\tdata = zlib.decompress(file_data)\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tzip_archive = zipfile.ZipFile(zip_path)\n\tname_list = zip_archive.namelist()\n\tdict_data = defaultdict(dict)\n\tdata_keys = []\n\tfor name in name_list:\n\t\tif name.endswith('/'):\n\t\t\tcontinue\n\t\tif (name.endswith('.xml') and (name.startswith('data/compressed/') or name.startswith('data/xml/'))):\n\t\t\tdata_keys.append(name)\n\tfor data_key in data"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\td = {extrapolate_bins(x.replace('/', '.').split('.')[0].replace('_', '.')): x \n\t\t     for x in zip_file.namelist() \n\t\t      if x!= 'xml' and x.endswith('.xml')}\n\t# if xml_name in zip_path:\n\t\t# d[xml_name] = zip_file.read(xml_name).decode('utf-8"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tzf = zipfile.ZipFile(zip_path)\n\tpf = PackageFile()\n\t\n\t# Create a dicionary of package data\n\tzf_dict = {}\n\tfor package in pf.get_packages(pf.get_from_zip(zf, '')[0][-4:].upper()):\n\t\tprint(package)\n\t\tfilename = package.name\n\t\tif not filename:\n\t\t\tsys.stderr.write(\n\t\t\t\t'ERROR: File %s or package %s could not be found in zip_path: %"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith ZipFile(zip_path) as zf:\n\t\tfiles = [[name, info] for file_name, info in zf.infolist() if file_name.endswith('.xml')]\n\treturn files\n\ndef _get_package_data_zip_files(zip_d, \n                                pkg_d, \n                                pkg_id, \n                                all_files, \n                                data_dicts, \n                                ignore_data, \n                                data_dicts_by_data_key):\n\t\"\"\"\n\t\tGet"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tparsed_dir = etree.parse(zip_path + os.path.sep + \"packages.xml\")\n\tparsed_xml = parsed_dir.getroot()\n\tpackages = dict()\n\tfor _package in parsed_xml.iterdescendants():\n\t\tname = _package.get('name', None)\n\t\tparsed_data = {'id':name, 'files':{},'packages':{}}\n\t\tdirs = _package.get('dirs',{'default':''})\n\t\tparsed_data['"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfiles = defaultdict(dict)\n\t\t# groups in zipfile by folder name\n\t\tfolders = defaultdict(dict)\n\t\tfor nm in zf.namelist():\n\t\t\tif nm.endswith('/'):\n\t\t\t\tfolders[nm][nm.split('/')[-2]] = True\n\t\tfor nm in zf.namelist():\n\t\t\tif nm.endswith('/'):\n\t\t\t\tfor fnm, fld in folders.items():\n\t\t\t\t\tif fnm in"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport xmlrunner\n\n\trunner = xmlrunner.XmlTestRunner(output=os.path.join('reports', zip_path))\n\treturn runner.run(unittest.TestLoader().loadTestsFromTestCase())\n\n\n@cli.command()\n@click.option('--logs/--no-logs', default=False, help='write stdout in log files')\n@click.argument('zip_path', default='pyproject.toml')\t\ndef collect(logs, zip_path):\n\t\"\"\" \n\tCollects all packages' data"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not os.path.exists(zip_path):\n\t\treturn {}\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tlist_of_files   = zip_ref.namelist()\n\t\tlist_of_files   = [str(x.filename) for x in list_of_files ]\n\tpackages         = {}\n\tfor zipfile_file in list_of_files:\n\t\tif zipfile_file == 'package-info.xml':\n\t\t\treturn {}\n\t\tpackage_data    = get_package"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\t\n\t\tpackage_list = []\n\t\tfiles = zip_ref.namelist()\n\t\t\n\t\tfor file in files:\n\t\t\tif file.endswith('.xml'):\n\t\t\t\twith zip_ref.open(file,'r') as xml_file:\n\t\t\t\t\tpackage_list.append(xml_to_dict(xml_file.read()))\n\t\t\t\t\n\t\treturn package_list\n\n\n\ndef _check_data(package, package_keys):\n\t\"\"\""}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tzip_path = Path(zip_path)\n\treturn list(zip_path.rglob('*'))\n\ndef unzipfolder(a_path=None):\n\t''' unzip any file `a_path` '''    \n\tPath(a_path).rmtree(ignore_errors=True)\n\n\tdef _helper_helper(a, bp, ext):\n\t\tif a._path.endswith(\".zip\"):\n\t\t\tif a.is_dir() is False:\n\t\t\t\t\n\t\t\t\tz=ZipFile(a)"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tzip_names = []\n\twith zipfile.ZipFile(zip_path, 'r') as hjb: \n\t\tzip_names = hjb.namelist()\n\n\t# Extract zip file to an empty directory\n\tdir_name = os.path.dirname(zip_path)\n\tif not os.path.exists(dir_name):\n\t\tos.mkdir(dir_name)\n\t# z = zipfile.ZipFile(zipf, 'r')\n\t# z.extractall()\n\t# z.close()"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tarchive_path = zip_path.replace('/ZIPFILE/', '/ZIPFILE-E/')\n\tzip = zipfile.ZipFile(archive_path, 'r')\n\tfile_list = zip.namelist()\n\tzip.close()\n\n\ttry:\n\t\tassert os.path.exists(zip_path)\n\t\tassert zip_path.endswith('tar.gz')\n\t\tassert len(file_list) > 2\n\n\t\treturn file_list\n\n\texcept AssertionError:\n\t\tprint('Something wrong')\n\n\ndef files_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, \"r\") as myzip:\n\t\tfiles = list(myzip.namelist())\n\treturn files\n\ndef files_list_from_dir(*args):\n\"\"\"\n\tSame as `files_list_from_dirname` but returns path `args[0]` instead.\n\"\"\"\n\tzip_list = files_list_from_dirname(args[0])\n\tfiles = []\n\tfor file in zip_list:\n\t\troot, name = os.path.split(file)\n\t\tfiles.append"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tzip = zipfile.ZipFile(zip_path)\n\t\n\treturn zip.namelist()\n\t\t\n\ndef files_in_dir_from_zipfile(zip_path, *paths):\n\t\"\"\"\n\tReturn a list of files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-04"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tall_files=os.listdir(zip_path)\n\tfiles=[]\n\t\n\tfor file in all_files:\n\t\tfiles.append(os.path.join(os.path.join(zip_path,file)))\n\t\n\treturn(files)\n\ndef extract_files(files_path,archive):\n\"\"\"\nExtract the files with path `files_path` from `archive`.\n\t\n\tExample:\n\t\n\t```\n\tfiles_path=['2318-0889-tinf-33-0421/23"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith io.open(zip_path, 'rb') as zip_file:\n\t\tfiles_list = zip_file.read().decode('utf8').split()\n\t\tfiles_list.remove(os.path.basename(zip_path))\n\t\treturn files_list\n\n\ndef files_list_from_dir(dir_path):\n\t\"\"\"\n\t\tReturn the files in `dir_path`\n\t\t\n\t\tExample:\n\t\t\n\t\t```\n\t\t[\n\t\t\t'2318-0889-tinf-33-0421',\n\t\t\t'2318"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\"\"\"\n\tzip_name = zip_path\n\tarchive = zipfile.ZipFile(zip_name, \"r\")\n\n\t\n\tfiles = []\n\tfor file_name in archive.filelist:\n\t\tfull_name = os.path.abspath(archive.path_file_header[0])\n\t\tfiles.append(full_name)\n\t# print(files)\n\tarchive.close()\n\treturn files\t\n\t\"\"\"\n\ndef check_dpi(image):\n\t\"\"\"\n\tCheck DPI setting in an image\n\t\n\tArg"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\timport os\n\n\treturn ZipFile(zip_path).namelist()\n\ndef check_url(url):\n\timport requests\n\ttry:\n\t\tprint(requests.get(url))\n\t\treturn True\n\texcept:\n\t\tprint(\"URL not accessible. Can't download data\")\n\t\treturn False\n\ndef fetch_image(url=None, path=None, img_fmt=None, unzip=False):\n\timport numpy as np\n\timport urllib\n\tfrom os import makedirs\n\tfrom os"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith ZipFile(zip_path, 'r') as zip_file_obj:\n\t    return [\n\t        os.path.basename(zip_file_obj.extract(item)[0])\n\t            for item in zip_file_obj.namelist()\n\t    ]\n\t"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = re.sub(\n\t\tr'(?s)(ns:p=([\\d\\-+\\.x ]{1,})\\s?)',\n\t\tlambda x: x.group(1).replace(' ', '-'),\n\t\tcontent)\n\tcontent = re.sub(\n\t\tr'(?s)(ns:endnote)([\\d\\-+\\.x ]{1,})\\s?',\n\t\tlambda x: x.group(1).replace(' ', '-'),\n\t\tcontent)\n\treturn content\n\n\nclass OoxmlTidy:\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tprefix = 'w:'\n\tres = ''\n\tfor line in content.split('\\n'):\n\t\tif line.startswith('\"{}'.format(prefix)):\n\t\t\tline = line.replace(prefix, '')\n\t\t\tline = prefix + '\"' + line\n\t\tres += line + '\\n'\n\treturn res\n\n\n\ndef fix_namespace_prefix(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tprefix = 'w:'\n\tres = ''"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif len(content) < MIN_NAMESPACE:\n\t\traise ValueError(\"Invalid namespace prefix\")\n\tlast_slash_pos = content.rfind('/')\n\tif last_slash_pos > 0:\n\t\tcontent = content[last_slash_pos:]\n\tnew_xml = _fix_namespace_prefix_w(content, _NAMESPACE_PREFIX_W)\n\treturn new_xml\n\n\n\ndef fix_namespace_prefix_x(content):\n\"\"\"\n\tConvert text that defaults to 'x:"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tt1 = '<w:st>'\n\tt2 = '</w:st>'\n\tt3 = '<w:name>'\n\tt4 = '</w:name>'\n\tt5 = '<w:tstamp w:val=\"'\n\tt6 = '\"/>'\n\tt6_2 = '</w:tstamp>'\n\t#print(t1 + t2 + t3)\n\t\n\treturn t1 + content + t4 + t5 + content + t6 + t2 + t6_"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tstart = content.find('st=\"')\n\tend = content.find('\"', start+4)\n\tif start == -1 and end == -1:\n\t\treturn content\n\n\tif not start == -1:\n\t\tstart_tag = content[:start+1:]\n\t\tnew_content = start_tag.replace('st=\"','st=\"w-' ) + content[end:]\n\t\t# print 'fix_namespace_prefix_w', start, end, content,new_content\n\t\treturn new_content\n\n\tif not end == -1:\n\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content is None: return content\n\treturn ''.join([x if x!= 'w' else 'w-st' for x in content])\ndef fix_text_style_name(style_name):\n\t\"\"\"\n\tUse text-* to simplify font names\n\t\"\"\"\n\ttry:\n\t\tassert style_name[0].upper() == 'T' or style_name[0].upper() == 'M' or style_name[0].upper() == 'F'\n\t\treturn style_name.replace('-','')\n\texcept:\n\t\treturn"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif isinstance(content,unicode): content = content.encode('ascii','xmlcharrefreplace')\n\tif len(content) > 8 and content[:8] == unicode('st=\"', 'utf-8'):\n\t\tcontent = u'w-%s' % content.replace('st=\"', u'st=\"%s\"' % u'st-')\n\telif len(content) > 7 and content[:7] == unicode('w:st=\"', 'utf-8'):\n\t\tcontent = u'w-%s' %"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tassert content.startswith(\"w:\")\n\treturn \"w-%s\" % content[3:]\n\n##########################################################################\n##########################################################################\n\n\n##class _PropertyTags:\n##class _NameTags:\n##class _SchemaTags:\nclass _TextTags:\n\n\t\t\n\n\n\n##def fix_attributes(attr_l, o, n, old_names):\n##\t\"\"\"\n##\tFIX_ATTRIBUTES: convert attr[i][0] to attr[i][1].\n##\n##\tTODO:"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tdefaults = {'w': u'w:st=\\'', 'w-st': u'w-st=\\''}\n\t\n\tnses = ['w', 'w:']\n\tif u''in content:\n\t\t# Get the ns before and after the string\n\t\tns = spt._NS_map_inverted[content.split(u' ')[0].split(':')[0]]\n\t\tif ns == u'w':\n\t\t\tnses.append(u'w:')\n\t\telse:\n\t\t\tnses.append(u'"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\\\"\\\"\", \"w-st=\\\"\\\"\")\n\treturn content\n\ndef insert_xmlns_elem(content):\n\"\"\"\n\tInsert prefix:xmlns_\n\t\tUse this function to convert xmlns: to prefix:xmlns_\n\t\"\"\"\n\treturn re.sub(r\"xmlns=['\\\"]{w}\\:\", r\"prefix:xmlns_\", content)\n\ndef get_id_of_element(id_of_element):\n\"\"\"\n\tGet the id of the element by its id_ of element"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = node.getnext()\n\twhile match is not None:\n\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\tpubdate_pubdate = match.xpath(pubdate_xpath)\n\t\t\tif len(pubdate_pubdate) > 0:\n\t\t\t\treturn normalize_date(\n\t\t\t\t\tpubdate_pubdate[0].text_content()\n\t\t\t\t\t)\n\n\t\tmatch = match.getnext()\n\"\"\"\n\t\treturn node.getparent().getparent().text\n\n\"\"\"\t# print(pubdate.xpath('//link["}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_node = node.xpath(pubdate_xpath)\n\t\tif len(pubdate_node) == 1:\n\t\t\tpdate = parse_published(pubdate_node[0], datetime_pattern)\n\t\t\tif pdate:\n\t\t\t\treturn pdate\n\t\n\treturn None\n\ndef parse_title(node):\n\"\"\"\n\tReturns the title node as a string from a single node\n\t\"\"\"\n\t\n\ttitle_node = node.xpath('string(/"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif len(pubdate_xpaths) == 0:\n\t\treturn None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_nodes = node.xpath(pubdate_xpath)\n\t\tif len(pubdate_nodes) > 0:\n\t\t\treturn pubdate_nodes[0]\n\treturn None\n\n\ndef update_pubdate(node, pubdate_xpaths, pubdate):\n\t\"\"\"\n\tReturns the pubdate given the pubdate_xpaths list\n\t\"\"\"\n\treturn\n\n\ndef"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath, namespaces=namespaces)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n            \n\ndef match_text_between_tags(node, text_between_xpath):\n\t\n\ttext = node.text if node else ''\n\ttext_between = None\n\tstartIndex = None\n\tendIndex = -1\n\t\n\tfor match in re.finditer('\\s', text):\n\t\tstartIndex = match"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\treturn node.find_element_by_xpath(match_xpath(pubdate_xpaths))\n\ndef match_title(node, title_xpaths):\n\"\"\"\n\tReturns the first match in the title_xpaths list\n\t\"\"\"\n\treturn node.find_element_by_xpath(match_xpath(title_xpaths))\n\ndef match_description(node, description_xpaths):\n\"\"\"\n\tReturns the first match in the description_xpaths list\n\t\"\"\"\n\treturn node.find_element_by_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tpubdate_xpath = pubdate_xpaths[0]\n\tpubdate_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n\tpubdate_subpattern = re.compile(r'\\d{2}:\\d{2}')\n\tpubdate_match = pubdate_pattern.findall(node.get(pubdate_xpath))\n\tpubdate_match_sub = pubdate_subpattern.findall(node.get(pubdate_xpath))\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t\n\tif pubdate_xpaths: \n\t\t_soup = BeautifulSoup(node, 'html.parser') \n\t\tpubdates = _soup.findAll(['span', 'time']) \n\t\tfor pubdate in pubdates: \n\t\t\tif pubdate.has_attr('datetime'): \n\t\t\t\tpubdate = pubdate.attrs['datetime'] \n\t\t\t\tpubtime = datetime.datetime.strptime(pubdate[:15], \"%Y-%m-%dT%H:%M:%S\")"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath, namespaces=NSDICT)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn node.data\n\ndef extract_metadata(url, html):\n\t\"\"\"\n\tAn album page URL and the body of HTML from the page can be supplied, and this can extract the album title,\n\talbum artist, and publishing date from the HTML. The URL and the body of HTML are assumed to be already\n\tsatisfied by the URL, html_is_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_match = soup.select_one(pubdate_xpath)\n\t\tif pubdate_match:\n\t\t\tpubdate_string = pubdate_match.get_text()\n\t\t\tpubdate_list = pubdate_string.split()\n\t\t\tpubdate = tuple((pubdate_list))\n\t\t\treturn pubdate\n\n# match_url = soup.url.get_text()\n\ndef extract_id(url_xpath):\n\"\"\"\n\tReturns the string value of the first id value"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tpubdate = None\n\tfor pub in pubdate_xpaths:\n\t\tpubdate = node.find(pub)\n\t\tif pubdate is not None:\n\t\t\tbreak\n\treturn pubdate\n\ndef get_month(date_string):\n\t\"\"\"\n\tReturns the month from a string like this: 10-November-2011\n\t\"\"\"\n\tmonth_string = re.sub('[^a-zA-Z ]+', '', date_string)\n\tmonth = month_string.split()[0]\n\treturn month\n\ndef get_year("}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tissue = issue.text.replace('\\n', '').replace('\\t', '')\n\tnum_index = issue.find('#')\n\tissue = issue[num_index+1:]\n\tsuppl_index = issue.find('X') - 2\n\tif(suppl_index == -1):\n\t\tsuppl_index = len(issue)\n\tnum = issue[0:int(suppl_index)] if (suppl_index!= -1) else '0'\n\treturn (num)\n\n\n\n\n\"\"\" \n_issue_process\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t#issue_re = '\\d+'\n\tnumber = re.findall(r'\\d+', issue)\n\tnumber = re.findall(r'(\\d+).*', number[0])\n\tnumber = number[0]\t\n\tnumber = number[0:-1]\n\tnumber = re.findall(r'^[^\\w\\.']+([\\d\\.]+)', number)\n\treturn number\n\n\ndef _extract_suppl_from_issue_element(issue):\n\t\"\"\"\n\tExtract the possible values of"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tissue = issue.replace(\"$\",\"\")\n\tissue = issue.replace(\",\",\"\")\n\n\tissue = issue.replace(\"%\",\"\")\n\tissue = issue.replace(\".\",\"\")\n\tissue = issue.replace(\"-\",\"\")\n\n\tissue = issue.replace(\"(\",\"\")\n\tissue = issue.replace(\")\",\"\")\n\t\n\tissuespl= issue.split()\n\t#print(issuespl)\n\n\tissuecnt=0\n\twhile(issuecnt<len(issuespl)-1):\n\t\ttry:\n\t\t\tcount = float(issuespl["}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tdata = dict()\n\tissue_date = issue.xpath('.//*[@t=\"a\" or @t=\"c\"]/text()')\n\tdata.update( { 'issue_date' : issue_date[0] } )\n\tissue_date = _extract_string_element_data(issue_date)\n\tdata.update( {'issue_date' : issue_date } )\n\n\tdate_list = re.findall('(\\d{2}/\\d{2}/\\d{4})(\\s+)(\\d"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tpossible_number_list = []\n\tpossible_suppl_list = []\n\tif not issue:\n\t\traise Exception(self._error_msg)\n\tfor key, value in issue.items():\n\t\tif key == \"Number\" or key == \"Suppl\":\n\t\t\tpossible_number_list.extend(value)\n\t\telse:\n\t\t\tpossible_suppl_list.extend(value)\n\tnumber = possible_number_list.pop(0)\n\treturn number, possible_number_list\n\n\"\"\"_extract_number_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\ttry:\n \t\t#print(issue)\n\t\t#print(\"in issue num\")\n\t\tnumber=int(issue.split(':')[1].split(' ')[0])\n \t\tsuppl=int(issue.split(':')[1].split(' ')[1])\n\texcept:\n \t\t#print (\"Can't split %s into a number and suppl: will just assume it is a raw number\") \n \t\tnumber = int(issue.split(':')[1][0])\n \t\tsuppl = int(issue.split(':')[1"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif issue is None or issue.issuetype is None or issue.labels is None:\n\t\treturn None\n\n\tif len(issue.number) == 1:\n\t\tnumber = issue.number[0]\n\telif issue.labels.__len__ > 1:\n\t\tnumber = issue.number\n\telse:\n\t\treturn None\n\n\tif issue.issuetype.__len__!= 1:\n\t\tsuppl = 0\n\telse:\n\t\tsuppl = 1\n\n\treturn number, suppl\n\n\n\ndef _process_issue_data(\n\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tsuppl_value = 0\n\tnumber = 0\n\tfor s in issue.xpath('span[contains(@class,\"text\") and contains(@class, \"textNum\")]/span[contains(@class, \"numText\")]/text()').extract():\n\t\tif \",\" in s:\n\t\t\tnumber = number + float(s.replace(\",\", \"\"))\n\t\telse:\n\t\t\tnumber = number + float(s.replace(\".\", \"\").replace(\"\u0e3f\", \"\"))\n\t\tsuppl = 0\n\t\tsupp = issue.xpath('"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\ttry:\n\t\tissue['number'] = issue.xpath('.//*[@id=\"solutionBody\"]//p[contains(@class,\"mainsq\")]/text()').extract()\n\t\tissue['number'] = ''.join(issue['number'])\n\t\tissue['number'] = issue['number'].splitlines()\n\t\tissue['number'] = issue['number'][0].strip()\n\t\tissue['number'] = issue['number'].strip()\n\texcept:\n\t\treturn None\n\ttry:\n\t\tissue['suppl'] = issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif len(issue) >= 4 and str(issue[2]) == \"N\" and str(issue[3]) in \"01234567\":\n\t\treturn (int(issue[2]), int(issue[3]))\n\telif len(issue) == 5 and str(issue[2]) == \"N\" and str(issue[4]) in \"01234567\":\n\t\treturn (int(issue[2]), int(issue[4]))\n\telse:\n\t\treturn (None, None)\n\n\"\"\"\n\tThis function adds a supplment to the corresponding issue"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn self.__doc__\n\ndef getClassName(obj, short=False, fully=False, withPath=True):\n\"\"\"\n\tReturns the class name of an object.\n\t\"\"\"\n\tif not isinstance(obj, (tuple, list, set)):\n\t\tobj = (obj,)\n\tresult = []\n\tfor element in obj:\n\t\tprefix = \"\"\n\t\telementType = type(element)\n\t\tif fullTypes:\n\t\t\tif elementType == list or elementType == tuple or elementType == set or elementType == frozenset:\n\t\t\t\tprefix"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ty = indent\n\ty += \"-\" * indent\n\ty += f\"\"\"{self.name}{\"--\" if self.containtho else \"\"}\"\"\"\n\tif self.n:\n\t\ty = self.pretty_name(y, indent, debug)\n\tfor i in self.p:\n\t\ty = self.pretty_person(i, y, indent, debug)\n\ty = self.pretty_noun(y, indent, debug)\n\treturn y\n\n\tdef pretty_name(self, y, indent, debug):\n\t\"\"\"\n\tReturn"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tprint(\"{0}({1}){2}\".format(\" \" * indent, id(self), str(self)))\n\telse:\n\t\tprint(\"{0}<Node: {1}>\".format(\" \" * indent, hex(id(self))))\n\"\"\"\n\tself.left = self.left\n\tself.right = self.right\n\treturn self\n\t\"\"\"\n\ndef dups(lst):\n\t\"\"\"\n\tFind duplicates in a sorted list.\n\t\"\"\"\n\td = {}\n\tfor item in lst:\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn 'C'  #TODO\n\n\nif __name__ == '__main__':\n\n\tprint(\"Test...\")\n\tc = C()\n\tc.p = 1\n\tprint(repr(c))\n\tx = repr(c)\n\tprint(x)\n\n\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn self.value.pretty(indent, debug)\n\nclass _ValueListMixin(object):\n\n\t\"\"\"\n\tAn instance of this class can be used to represent a List containing any\n\ttype.\n\t\"\"\"\n\n\tdef _list_repr (self, iterable, indent=0, debug=False):\n\t\tresult = ''\n\t\tfirst = True\n\t\tfor item in iterable:\n\t\t\tresult += '\\n'\n\t\t\tif isinstance(item, type(self)):\n\t\t\t\trepr_item = item._repr(indent=indent,"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\traise NotImplementedError\n\"\"\"\n---------------------------------------------------------------------------------------------------------------------\n\"\"\"\n\tdef load(self, filename='solver.p'):\n\"\"\"\n\tLoad the content of filename into the solver.\n\t\tParameters\n\t\t----------\n\t\tfilename : string (optional)\n\t\t\tthe filename to load the solver from\n\"\"\"\n\t\traise NotImplementedError\n\"\"\"\n---------------------------------------------------------------------------------------------------------------------\n\"\"\"\n\tdef load_bfs(self, filename='bfs_input.txt'):\n\"\"\"\n\tLoad the content of filename into the solver in the form of a BFS\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn (\n\"\"\"%(indent)s%(type_str)s(\n'%(attrs)s',\n%(indent)s'%(text)s',\n%(indent)sself.type\n%(indent)s):\\n%(indent)s\"\"\" %\n({\"indent\" : indent*\"  \",\n\t\"type_str\" : type_str(self),\n\t\"attrs\" : str(self),\n\t\"text\" : str(self.text),\n\t\"indent\" : indent * \""}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif indent == 0:\n\t\t# Only print some of the header\n\t\tprint(self.__doc__)\n\tprint(' '*indent, self.__class__.__name__, end='\\n')\n\tfor k, v in self.__dict__.items():\n\t\tif type(v) == type({}):\n\t\t\tif debug:\n\t\t\t\tpprint(v, indent)\n\t\t\telse:\n\t\t\t\tpretty(v, indent+2)\n\t\telse:\n\t\t\tprint(' '*(indent+1), k, '=', v)\n\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn \"\"\n\n#*******************************************************************************************************\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif len(self.children) == 0:\n\t\treturn self.value\n\tfor i, v in enumerate(self.children):\n\t\treturn self.value + '{\\n' +' '*indent + v.pretty(indent+1, debug) + '\\n' +' '*indent + '}'\n\n# --------------------------------------------------------\n#  Helpers\n# --------------------------------------------------------\n\ndef parse_value(s):\n\t\"\"\"\n\tParse a string representing a value and return the value\n\tor None if parsing failed.\n\t\"\"\"\n\t"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif isinstance(args, str):\n\t\targs = [args]\n\n\tabs_list, neg_list = [], []\n\n\tfor arg in args:\n\t\tif is_const(arg, bool):\n\t\t\targ = boolneg(arg)\n\n\t\telif not(is_const(arg, int) or is_const(arg, float) or is_const(arg, str)):\n\t\t\traise ValueError(\"Expect constant or bool or str, not \" + repr(arg))\n\n\t\tres = self._absorb(arg)\n\t\targ = res[0]"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tdef __call__(self, *args, **kwargs):\n\t\tlst = sorted(args, key=id)\n\t\tif self.__config__.negative_a:\n\t\t\tlst = map(neg, lst)\n\t\tif self.__config__.absorption:\n\t\t\treturn simplify(lst[0]) & lst[1]\n\n\tdef __eq__(self, other):\n\t\treturn self.__config__ == other.__config__\n\t\n\tdef __hash__(self):\n\t\treturn object.__hash__(self)\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn [\n\t\tself if isinstance(arg, Expr_and_t_or) else Expr_and_t_or(*(x.absorb(args)\n\t\t\tfor x in reversed(arg)))\n\t\tfor arg in args if self.size > 0\n\t]\nif 0: from. import Expr_list_t\nclass Expr_negation(Expr_compute):\n\t\"\"\"\n\tRepresent `~...`\n\n\t@see: https://en.wikipedia.org/wiki/Negation_law\n\t\"\"\"\n\tdef __init__("}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tdef __mul__(self, expression):\n\t\t# Return new expression composed with itself and expression.\n\t\tif not isinstance(expression, Expression) and expression:\n\t\t\treturn Expression(self, expression)\n\t\treturn type(self)(self, *(self._expand(x) for x in expression))\n\t__rmul__ = __mul__\n\t__imul__ = __mul__\n\t\n\tdef __or__(self, expression):\n\t\treturn self(*(expression_absorb(x) for x in expression))\n\t\t\n\tdef _exp"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tassert isinstance(args, (tuple, list))\n\n\tif not args:\n\t\treturn args\n\n\texpr = args[0]\n\t\n\tif isinstance(expr, PInv):\n\t\treturn args\n\n\tif not expr.is_and():\n\t\targs = [expr] + args\n\t\texpr = PInv()\n\t\n\t# check first literal\n\tif isinstance(args[0], PConst):\n\t\tif expr.is_and():\n\t\t\treturn expr.get_args()\n\t\telif expr.is_or() and args[0].is"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn args\n\n@py3op\ndef concat(*args):\n\"\"\"\nConcatenate the zero or more arguments into a new list, appending them a.\n\t\nConcatenated lists::\n\t\n\t   a & b       = [a, b]\n\t   a | b       = (a, b)\n\t   a+b        = (a, b)\n\t   a+b+c      = (a, b, c)\n\t\"\"\"\n\treturn args\n\n@py3op\ndef concatenate(args):\n\t\"\"\""}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tfrom copy import deepcopy\n\tnew_args = args[:]\n\t\n\tif len(args) == 1:\n\t\targ = args[0]\n\t\t#print \"ABS\", args\n\t\tif isinstance(arg, BooleanAnd):\n\t\t\treturn arg.args\n\t\t\n\t\tif isinstance(arg, BooleanOr):\n\t\t\treturn BooleanAnd(neg_args(arg.args[0]))\n\t\t\n\t\tif isinstance(arg, BooleanNot):\n\t\t\tfor item in arg.args:\n\t\t\t\tif isinstance(item, BooleanNot):\n\t\t\t\t\treturn item."}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tfor i in range(len(args)):\n\t\targs[i] = self.absorb1(args[i])\n\t\targs[i] = self.absorb2(args[i])\n\treturn args\n\ndef invert(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying negation.\n\t\n\tSee https://en.wikipedia.org/wiki/Negation_law\n\t\n\tNegation::\n\t\n\t    X & A = A, X | ~A = A,"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn [_ for _expr in args for _ in self.absorb(_expr[0])]\n\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn [self.absorb_by(args, a) for a in args]\n\nclass Abs_BV(Abstract_Form, Abs_Const):\n\tATOMIC = \"x\"\n\n\tdef __init__(self, *args):\n\t\t\"\"\"\n\t\tCreate an Abs_BV.\n\t\n\t\tAbs_BV('1') :: Abs_BV\n\t\tAbs_BV('2') :: Abs_BV\n\t\tAbs_BV('~', '11') :: Abs_BV\n\t\t\"\"\"\n\t\tif len(args) == 0: raise"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\tif not isinstance(hook, Hook):\n\t\traise TypeError(\"Expected a HookType, got %s of type %s.\"\n\t\t\t% (hook, type(hook)))\n\tif not callable(hook):\n\t\traise TypeError(\"Expected a callable\")\n\tif str(hook) in self._registry:\n\t\traise ValueError(\"Listener with hook %s already registered\" %\n\t\t\tstr(hook))\n\tself._registry[str(hook)] = hook\n\t\n\treturn lambda f: f\n\t\n###############################################################################\n\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn registry.register(HookTypes.hook(hook), self)\n\ndef remove(fn):\n\t\"\"\"Remove all listeners from the handler object.\n\n\tArgs:\n\t    fn (callable): Callback function that is returned from listener\n\t    handlers.on() to be removed from the registry.\n\n\tReturns:\n\t    any: A boolean indicating that registration was successful or\n\t    ``None`` if there was no callback to remove.\n\t\"\"\"\n\treturn registry.remove(fn)\n\n# ---------------------------------------------------------------------- #"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\treturn lambda cls: self.add_class_listener(cls, hook, True)\n\nHookRegistrar.listener_registry.add_listener = listener_registry.add_listener\n\n\"\"\"\nHookRegistrar.subscriber_registry.add_listener\nDoc\nListeners for HookEvents\n\"\"\"\nclass listener:\n\t\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tCreate a new event listener to be added to the ListenerRegistrar.\n\t\t\n\t\tArgs:\n\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tself.registry.__setattr__(hook.__name__, None)\n\n\tdef actual_decorator(wrappee):\n\t\tself.registry.__setattr__(hook.__name__, wrappee)\n\t\treturn wrappee\n\n\treturn actual_decorator\n\t###\n\nif __name__ == \"__main__\":\n\tprint(\"Testing...\")\n\tl = Listener(lambda v, e: False)\n\ts = Sink(lambda x, e, h: x ** 2)\n\n\t@on(Event("}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn self._on(hook)\n\nHook = HookType\n\"\"\"HookType: Type returned by the :attr:`on` decorator.\"\"\"\n    \nclass HookRegistrar(object):\n\t\"\"\"Wrapper class to :class:`list` of handlers to functions that should be registered\n\ton a specific type of hook.\"\"\"\n\t#--------------------------------------------------------------------------\n\tdef __init__(self):\n\t\tself._registry = defaultdict(list)\n\t#--------------------------------------------------------------------------\n\tdef __contains__(self, key):\n\t\treturn key in self._registry"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef inner(handler):\n\t\ttry:\n\t\t\tsetattr(handler, \"__register_hook__\", hook)\n\t\t\tmethod = HookManager.__hooks__[hook]\n\t\texcept AttributeError:\n\t\t\tsetattr(method, \"__register__\", False)\n\t\telse:\n\t\t\tmethod.__register__ = True\n\t\t\ttry:\n\t\t\t\tHOOKS[hook] = method\n\t\t\texcept KeyError:\n\t\t\t\tHOOKS[hook] = []\n\t\t\tHookManager.__hooks__[hook].append(handler)\n\t\t\tif hook is Hook.CLE"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif not isinstance(hook, HookType):\n\t\traise TypeError(\"Only hooks can be registered.\")\n\tsetattr(HookRegistry, hook.__name__, type(hook.__name__, (HookRegistry,), {'__doc__': on}))\n\treturn on\n\ndef deregister(hook):\n\t\"\"\"\n\tDe-register a previously registered hook.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to unregister the handler.\n\t\"\"\"\n\tif not isinstance(hook, HookType):\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tself._hooks[hook].add(self)\n\treturn self.__call__\n\ndef remove():\n\t\"\"\"\n\tDecorator function to remove a handler.\n\t\"\"\"\n\tdef on(self, hook):\n\t\t\"\"\"\n\t\tRemoves the hook from the registry.\n\t\t\"\"\"\n\t\tself._hooks[hook].remove(self)\n\t\treturn self.__call__\n\treturn on\n\ndef handle(data):\n\t\"\"\"\n\tDecorator function to specify a dispatch callback for the specified\n\thook.\n\t\"\"\"\n\tdef on(self"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif hook not in self.HOOKS:\n\t\traise Exception(f\"Unsupported hook: '{hook}'\")\n\tif callable(hook):\n\t\treturn self.HOOKS.register(hook)\n\thook = getattr(hook, \"on\", None)\n\tif hook is None:\n\t\traise Exception(f\"Invalid hook: '{hook}'\")\n\treturn hook(self)\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tassert hook in HANDLER_REGISTRY, \\\n\t\"Hook {} not registered in HANDLER_REGISTRY.\".format(hook)\n\tf = HANDLER_REGISTRY[hook]\n\treturn f\n\ndef hook(*args, **kwargs):\n\t\"\"\"\n\tA decorator to get around the default-singleton implementation\n\tof HANDLER_REGISTRY for calling registered handlers of the\n\tassociated hook type.\n\t\n\tArgs:\n\t\thook (str): HOOK used by the hook to register\n\t\titself. See"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tstatic_auth = {\n\t    \"name\": \"__default__\",\n\t    \"tlsSecret\": \"\",\n\t    \"key\": \"\",\n\t    \"cert\": \"\"\n\t}\n\n\tetcd_config = {\n\t\t\"addr\": \"http://{host}:{port}\".format(\n\t\t\thost=etcd_host,\n\t\t\tport=etcd_port\n\t\t)\n\t}\n\n\tconfig = {\n\t\t\"database\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"params\": etcd_config\n\t\t},\n\t\t\"auth"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\t\"database\": {\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t},\n\t\t\t\"postgres\": {\n\t\t\t\t\"user\": user,\n\t\t\t\t\"pass\": \"demo\"\n\t\t\t}\n\t\t}\n\t}\n\treturn config\n\n\nclass DBConfig:\n\t\"\"\"\n\tA class encapsulating the connection to the database.\n\t\n\tAttributes:\n\t    connection (string): the database connection.\n\t\"\"\"\n\tdef __init__(self, config):"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {}\n\tconfig[\"users\"] = {}\n\tfor user in user:\n\t\tconfig[\"users\"][user] = {}\n\t\tconfig[\"users\"][user][\"userKey\"] += user\n\t\tconfig[\"users\"][user][\"userCipher\"] = \"none\"\n\t\tconfig[\"users\"][user][\"userDb\"] = etcd.Client(host=etcd_host, port=etcd_port).node(\"/users/\"+user[\"userKey\"]).key\n\treturn config\n\n\ndef base_config_from_file(file=\"../../config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\tdata = {\n\t\t\"master\": {\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port\n\t\t},\n\t\t\"users\":\n\t\t{\n\t\t\"user\": user,\n\t\t\"password\": user\n\t\t}\n\t}\n\t\n\treturn data\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\tconf = {\n\t    \"user\": \"root\",\n\t    \"host\": etcd_host,\n\t    \"port\": etcd_port,\n\t    \"etcd_host\": etcd_host,\n\t    \"etcd_port\": etcd_port\n\t}\n\n\tconf[\"etcd_auth\"] = base64.b64encode(bytes(conf[\"user\"], \"utf-8\")).decode('ascii')\n\n\treturn conf\n\n\ndef get_configure(etcd):\n#\t\"\"\"\n#\tFunction that is"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"etcd_user\": user,\n\t\t\"etcd_password\": config.get_db_password(user)\n\t}\n\n\ndef client_config(user):\n\t\"\"\"\n\tCreates a configuration with some default parameters.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn load_config({\n\t\t'etcd': {\n\t\t\t'host': etcd_host,\n\t\t\t'port': etcd_port\n\t\t},\n\t\t'debug': False,\n\t\t'user': 'root',\n\t\t'group': 'root',\n\t\t'static_auth': {\n\t\t\t'user': 'root',\n\t\t\t'password':'secret',\n\t\t\t'roles': ['superuser', 'user'],\n\t\t\t'public_url': '/api/v1',\n\t\t\t'admin_url': '/api/v1',\n\t\t\t'set_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t\"database\": {\n\t\t\t\"etcd_host\": etcd_host,\n\t\t\t\"etcd_port\": etcd_port,\n\t\t},\n\t\t\"auth\": {\n\t\t\t\"user\": user\n\t\t}\n\t}\n\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tPrint a string that can be used to get an example of a config.py file.\n\t\"\"\"\n\tprint(base_config())\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tif isinstance(etcd_host, str):\n\t\tetcd_host = f\"http://{etcd_host}\"\n\tif isinstance(etcd_port, int):\n\t\tetcd_port = f\"{etcd_port}\"\n\tdefaults = {\n\t\t'user': user,\n\t\t'cors_origin_regex': \"^/$\",\n\t\t'base_etcd_path': \"/config\",\n\t\t'base_etcd_prefix': \"\",\n\t\t'base_etcd_host': etcd_host,\n\t\t'base"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = dict()\n\n\tconfig['etcd_host'] = etcd_host\n\tconfig['etcd_port'] = etcd_port\n\tconfig['username'] = user\n\tconfig['password'] = read_auth_file()\n\n\treturn config\n\ndef default_config():\n\t\"\"\"\n\tCreates the default configuration.\n\t\n\tReturns:\n\t    dict: the default configuration.\n\t\"\"\"\n\treturn base_config(\n\t\tname=\"admin\",\n\t\tetcd_host=\"leader-etcd\",\n\t\tetcd_port=2379\n"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tglobal_min = MIN\n\tglobal_max = MAX\n\tstart = _get_start(dt.tzinfo)\n\tstop = _get_stop(dt.tzinfo)\n\tfold_start = start\n\tfold_stop = stop\n\tfirst = True\n\twhile (fold_start + fold_stop + timedelta(0, 1)) < dt < (fold_stop - timedelta(0, 1)):\n\t\tif fold_start <= dt < fold_stop:\n\t\t\traise _InvalidTimeError(dt)\n\t\tif first:\n\t\t\tglobal_"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\toldtz = dt.tzinfo\n\tlocal_time = dt.astimezone(tzlocal())\n\tdt = oldtz.fromutc(oldtz.utcfromtimestamp(\n\t\tlocal_time.timestamp()\n\t))\n\tassert dt.tzinfo == oldtz\n\treturn dt\n\ndef datetime_sort_key(dt):\n\t\"\"\"\n\tSort `datetime.datetime` objects naturally.\n\n\t:param dt:\n\t\tA timezone-aware `datetime.datetime` object.\n\t\"\"\"\n\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Get a datetime representation in UTC\n\tutc_dt = dt.astimezone(pytz.utc)\n\t# Determine if a fold has happened; if so, choose the offset which\n\t# started the fold, not the offset which started the preceding fold.\n\t_, fold_start, fold_stop = get_fold_start_and_stop(utc_dt, utc_dt.fold)\n\tif fold_start is not None:\n\t\tfold_duration = fold_stop - fold_start\n\t\tif fold_duration!= 1:"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = getattr(dt.tzinfo, '_fold', None)\n\tif fold is None:\n\t\treturn dt\n\t\n\t\n\t# Get the transition time in the new timezone\n\ttz = dt.astimezone(self)\n\ttrans_time = self._datetime_to_timestamp(tz.replace(fold=0))\n\t\n\t# Get the current time in new timezone\n\tnew_timezone = dt.astimezone(tz)\n\tcurrent_timestamp = self._datetime_to_timestamp(new"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\treturn dt.astimezone(timezone.utc).replace(tzinfo=None)\n\t\n\t\ndef naive_to_aware(naive_dt):\n\t\"\"\"\n\tGiven a naive timezone aware datetime object, converts it to\n\ta timezone aware datetime equivalent UTC.\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tutc = naive_to_aware(timezone.utc)\n\t\n\treturn utc."}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\timport tz\n\t\n\t# We're given a datetime with a tzinfo attached.  We want to\n\t# return a datetime in the users' timezone that is unambiguous.\n\t# What if we start in the users' timezone and have an ambiguous\n\t# datetime?  What if we start in the users' timezone and not have\n\t# an ambiguous datetime?  What if we end up in the users' timezone\n\t# and have an ambiguous datetime?  Then this is a no-no.  In\n\t# each case"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Calculate the fold status given the two datetimes.\n\t_fold_status = _fold_status_from_dt(dt, self.dt_tzinfo)\n\t\n\t# Decide based on the fold status of both datetimes.\n\tif not _fold_status:\n\t\t# This is a simple datetime instance\n\t\treturn dt.astimezone(*_best_effort_tzinfo_arg(dt, self.dt_tzinfo))\n\telse:\n\t\t# This is an ambiguous instance.\n\t\tseconds = (dt.replace(tzinfo="}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# the constructor of a timezone-aware object is private\n\ttzobj = dt.tzinfo\n\tif tzobj is None:\n\t\traise ValueError('Naive time instances cannot be used with'+\n\t\t\t\t       'astimezone().')\n\t\n\t# we must know the timezone we should return\n\treturn dt.astimezone(tzobj)\n\t\ndef picknthweekday(year, month, dayofweek, hour, minute, whichweek):\n\t\"\"\"\n\tThis is a hack to find the first day of the month\n\n\tThis"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# Find the first ambiguous datetime using a binary search\n\t# First, find the first datetime with an offset from UTC\n\tstart_datetimes = self.utc_transitions\n\t\n\t# If the offset is negative, the datetime must be after, so we return the\n\t# nearest previous datetime\n\tif dt.utcoffset() < ZERO:\n\t\t# The first ambiguous previous datetime cannot be after the initial\n\t\t# datetime\n\t\tend_datetime = min(start_datetimes, key=lambda dt: dt <"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# Get a local copy so we don't mess with the calling arguments\n\tin_tz = dt.astimezone()\n\t\n\t# Convert the datetime string into a POSIX timestamp\n\ttimestamp = time.mktime(in_tz.timetuple())\n\t\n\t# Convert POSIX timestamp into a datetime string\n\tdt_str = time.strftime(\"%Y%m%dT%H%M%S\", time.localtime(timestamp))\n\t\n\t# Use astimezone to determine if our datetime will be in a"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t#There are several duplicate pieces of code between here and the C version.\n\t#Factor time\n\tyears, months, weeks, days, hours, minutes, seconds, micros = self.__prepare(None)\n\tyear = years + months / 12 + weeks / 52 + days / 365\n\tif abs(micros) < 1e-6:\n\t\tmicros = 0\n\telse:\n\t\tmicros = 1000000 * (micros / 1e6)\n\treturn relativedelta(year=year, month=months, day=days, hour=hours, minute"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self\ndef to_timedelta(self):\n\"\"\"\n\tConvert the :func:`relativedelta.relativedelta` object back to a :class:`datetime.timedelta` object.\n\t\n\t>>> total = relativedelta(hours=1, days=6)\n\t>>> total.to_timedelta()\n\tdatetime.timedelta(1, 87600)\n\t\n\t:return:\n\t    Returns a :class:`datetime.timedelta` object.\n\t\"\"\"\n\treturn timedelta"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\thours, remainder = divmod(\n\t\tself.hours + 24 * self.days, 24\n\t)\n\thours, minutes = divmod(remainder, 60)\n\tdays, hours = divmod(hours, 24)\n\tyears, days = divmod(days, 365.242191)\n\tmonths, years = divmod(years, 12.5)\n\tweeks, months = divmod(months, 4.5)\n\tif self.leapdays and (\n\t\tself.year is not None and self.year % 4 == 0 and\n\t\tself"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self\n\n@total_ordering\nclass monthrange_year(object):\n\t\"\"\"\n\tmonthrange_year(year, month)\n\t\n\tThe number of days in the given month and year.\n\t\n\t>>> import datetime\n\t>>> x = monthrange_year(2016, 13)\n\t>>> int(x[0] + 1)\n\t32\n\t>>> int(x[1])\n\t29\n\t\n\t:param year:\n\t    A given year\n\t:param month:\n\t    A given month for that year\n\t:return"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdays = self.days\n\tif self.days < 0:\n\t\tdays = -self.days\n\thours = self.hours\n\tif self.hours < 0:\n\t\thours = -self.hours\n\tminutes = self.minutes\n\tif self.minutes < 0:\n\t\tminutes = -self.minutes\n\tseconds = self.seconds\n\tif self.seconds < 0:\n\t\tseconds = -self.seconds\n\tyear, month, day, hour, minute, second = divmod(\n\t\t365 * days + calendar.leapdays("}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdays = self.days() if self.days() is not None else 0\n\thours = self.hours() if self.hours() is not None else 0\n\tminutes = self.minutes() if self.minutes() is not None else 0\n\tseconds = self.seconds() if self.seconds() is not None else 0\n\tmicroseconds = self.microseconds() if self.microseconds() is not None else 0\n\t\n\tseconds = seconds + self.microsecond / 1000000.0\n\tdays += seconds / 86400.0\n\thours += days *"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t#: The set of attributes that are expected on the object\n\tNUMLIKE_CLASSES = (\n\t\trelativedelta,\n\t)\n\t\n\t#: The default values of the object. May be updated by\n\tDEFAULTS = {\n\t\t\"days\": 0,\n\t\t\"seconds\": 0,\n\t\t\"microseconds\": 0,\n\t\t\"minutes\": 0,\n\t\t\"hours\": 0,\n\t\t\"weeks\": 0,\n\t\t\"months\": 0,\n\t\t\"years\": 0,\n\t}\n\t\n\t#: The name"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self\n\t\n\t\n\t\n\t\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdays = int(self.days)\n\thours = int(self.hours)\n\tminutes = int(self.minutes)\n\tseconds = int(self.seconds)\n\tmicroseconds = int(self.microseconds)\n\tleapdays = int(self.leapdays)\n\tyear = int(self.years)\n\tmonth = int(self.months)\n\tweekday = int(self.weekday)\n\tif not self._has_time:\n\t\thours = minutes = seconds = microseconds = 0\n\t\tweekday = None\n\t"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t# Currently, normalized relaionships can only have positive between\n\tvalues = []\n\tfor key, value in list(self.__dict__.items()):\n\t\tif value is None:\n\t\t\tvalues.append(None)\n\t\telif isinstance(value, timedelta):\n\t\t\tseconds = abs(value.days * 86400 + value.seconds)\n\t\t\tvalues.append(seconds)\n\t\telse:\n\t\t\tvalues.append(value)\n\n\treturn relativedelta(days=values[0], hours=values[1],\n\t                     minutes=values["}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef __wrapper(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tReturn a bytestring representation of TZNAME, or None if it cannot be found.\n\t\t\"\"\"\n\t\treturn namefunc(self, self._name_object).decode('ascii') or None\n\treturn __wrapper\n\nis_python3 = sys.version_info[0] > 2\nif is_python2:\n\tlocaltz = get_localzone\n\ttzname = tzname_in_python2\n\tutc_time = getattr(time, '"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef f(self, *args, **kwargs):\n\t\tres = namefunc(self, *args, **kwargs)\n\t\tif sys.version_info >= (3, 0, 0):\n\t\t\tif isinstance(res, bytes):\n\t\t\t\treturn res.decode()\n\t\telif sys.version_info >= (2, 6):\n\t\t\tif isinstance(res, str):\n\t\t\t\treturn res.decode()\n\t\treturn res\n\treturn f\n\ndef replace_utf8(elem):\n\t\"\"\"\n\tHelper function. Replace non-ASCII chars"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tfrom sys import version_info\n\n\tdef _inner(*args, **kwargs):\n\t\tname = namefunc(*args, **kwargs)\n\t\tout = name if version_info < (3, 0) else name.encode('ascii')\n\t\treturn out\n\treturn _inner\n\n\nif __name__ == '__main__':\n\tprint (tzname_in_python2(gmtime))\n\ttz = 'America/Greece'\n\tprint (tz)\n\tt = localtime(mktime(strptime(tz, '%Z"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef utzname(self):\n\t\t# bytearray() is not available in Python 2\n\t\tn = namefunc(self)\n\t\tif type(n) == bytes:\n\t\t\treturn str(n)\n\t\telse:\n\t\t\treturn n\n\treturn utzname\n\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\n\tTZNAMES = '''ACDT\n           AFT\n           ACT\n           ACWST\n           ACDT\n           ADT\n           AWT\n           ACST\n           ADT\n           AEST\n           AFT\n          AUSST\n          BDT\n          BDST\n          BST\n          BTT\n          CAT\n          CCT\n          CEST\n          CET\n          CST\n          CBT\n          CBTT\n          IOT\n          CAT\n          CCT\n          CDT\n          CFT\n          CST\n           DET"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t@wraps(namefunc)\n\tdef wrapper():#get the original timezone name, so tzname() can be changed to a unicode string\n\t\ttz = timezone_object()\n\t\treturn namefunc().decode('utf-8')\n\treturn wrapper\n\n@tzname_in_python2\ndef change_tzname(tzstr, dststart=0):\n\t'''Change the timezone name for a Python 2\n\ttzstr -- Timezone name, e.g. Europe/London\n\tdststart -- offset for DST, needed if TZ"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef convert_bytes(value):\n\t\t\"\"\"\n\t\t\tDecompose Python 2 bytes into (name, gmtoff, isdst)\n\t\t\"\"\"\n\t\tif not isinstance(value, str):\n\t\t\treturn (value.decode(), 0, 0)\n\t\treturn (value, 0, 0)\n\tdef get_names():\n\t\t# get zone names from dateutil\n\t\tzone_to_names = {}\n\t\ttry:\n\t\t\tall_timezones, no_dst = _time.tzname\n\t\texcept AttributeError:\n\t\t\t# tzlocal"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef inner(self):\n\t\toffset = self._utcoffset\n\t\tif offset is None:\n\t\t\tabbr = self.tzname(None)\n\t\telse:\n\t\t\tabbr = self.tzname()\n\t\t\tif isinstance(abbr, bytes):\n\t\t\t\tabbr = abbr.decode('ascii')\n\t\t# ignore the DST flag\n\t\tx = tzwin32._StrictWindowsTimeSpec.strptime(abbr, '%z %Z')\n\t\tstart, end = x.utcoffset, x.dst\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn lambda a:b(namefunc(a).encode()) if PY2 else namefunc(a).encode(encoding)\n\ntzname_in_python2 = tzname_in_python2(tzname)\n\n#from _common import *\n\n#from dateutil.zoneinfo import *\nimport tz\nimport re\n#from dateutil.tz import *\n\n#try:\n#\ttz.gettz\n#except AttributeError:\n#\tpass\n\n#\t\tif tzname_in_python2.__name__ =="}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif python2(2):\n\t\tdef inner(self):\n\t\t\tres = namefunc(self)\n\t\t\tif res.encoding is not None:\n\t\t\t\ttry:\n\t\t\t\t\tres = res.encode(res.encoding)\n\t\t\t\texcept UnicodeEncodeError:\n\t\t\t\t\tres = res.encode(res.encoding.encode('utf8'))\n\t\t\treturn res\n\t\treturn inner\n\telse:\n\t\treturn namefunc\n\n\nif python3(3):\n    from dataclasses import dataclass, field\nelse:\n    from dataclasses import dat"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tversion_cache = get_cache(VERSION_CACHE_NAME)\n\tif version_cache is None:\n\t\treturn DEFAULT_VERSION\n\telse:\n\t\treturn version_cache\n\n#-------------------------------------------------------------------------------\n# Version Cache\n#-------------------------------------------------------------------------------\n@lru_cache()\ndef set_cache(cache, name):\n\t\"\"\"\n\tSet a cache and return the associated class definition, otherwise\n\tNone is returned and raise a CacheNotAvailable exception.\n\t\"\"\"\n\tklass = get_class(name)\n\tcache[name] = klass\n\treturn k"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\timport pip\n\texcept ImportError:\n\t\treturn {\"version\": 0.0, \"fullVersion\": None, \"error\": \"pip is not installed\"}\n\ttry:\n\t\treturn pip._vendor.pkg_resources.get_distribution(\"gitdb\").version\n\texcept:\n\t\treturn {\"version\": 0.0, \"fullVersion\": None, \"error\": \"gitdb is not installed\"}\n\ndef get_current_branch():\n\"\"\"Return current git branch if in a git repo\"\"\"\n\ttry:\n\t\tproc = subprocess.Popen"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tversion = requests.get(\"https://api.github.com/repos/jupyterhub/jupyterhub/tags\").json()[0]\n\t\tversion = version.get('name', 'unknown')\n\t\tif re.search(r'^[0-9]+\\.[0-9]+$', version):\n\t\t\trelease = 'rc' not in version\n\t\t\tversion = version.split('-')[0].replace('rc', '').strip()\n\t\telse:\n\t\t\tversion = ''\n\t\treturn version, int(version[0"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(_winreg,'version', lambda:None)()\n\texcept AttributeError:\n\t\treturn {\n\t\t\t\"exported\": False\n\t\t}\nversions = get_versions()\n\nif versions[\"exported\"]:\n  print(\"Imported Windows Registry version %d.%d.%d.%d from %s, build %d %d %d, %s UTC %d\" % (int(versions[\"Major\"]), int(versions[\"Minor\"]), int(versions[\"Build\"]), int(versions[\"Revision\"]), versions[\"file"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# Try to get versions from installed packages\n\tfor distribution in ['pip','setuptools', 'py2exe']:\n\t\ttry:\n\t\t\tversion = pkg_resources.get_distribution(distribution)\n\t\t\tif version:\n\t\t\t\treturn version.version\n\t\texcept:\n\t\t\tpass\n\n\t# Try to get version from Python\n\tif hasattr(sys,'version_info'):\n\t\treturn '.'.join([str(x) for x in sys.version_info])\n\n\treturn '0.0.0.0'\n\n\ndef get_"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\t# pkg_resources is available for dnspython==1.0.0\n\t\timport pkg_resources\n\t\tversion = pkg_resources.get_distribution('nspy').version\n\texcept ImportError:\n\t\t# pkg_resources (aka setuptools) isn't available for dnspython <1.0.0\n\t\tversion = '0.0.0'\n\n\treturn version,'.dev0'\n\n# Read in the contents of the current directory\nversion_list, version_rev, version_date = get_versions"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tfrom django.utils import translation\n\t\tfrom django.utils.translation import gettext_lazy as _\n\texcept ImportError: \n\t\ttranslation = None\n\t\t_ = lambda s: s\n\treturn {\n\t\t'name': getattr(mariadb_version, 'name','mariadb'),\n\t\t'major': getattr(mariadb_version,'major', 0),\n\t\t'release_level': '',\n\t\t'full': '%s.%s.%s' % (getattr(mariad"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tfancy_msg = \"{c}Checking if version strings exist...{fore}This appears to be a custom-built installation.\"\n\tversion_info = (\n\t\t{\n\t\t\t'name':'name',\n\t\t\t'description':'description',\n\t\t\t'current_version':'current_version',\n\t\t\t'parent_path':'__path__',\n\t\t\t'parent_name':'name',\"c\":fore.CYAN, 'w':fore.MAGENTA},\n\t\t{'version':'version_info', 'url':'github','ref' :'ref'"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tfrom.version import __version__\n\t\treturn (__version__,)\n\texcept ImportError:\n\t\treturn (\"unknown\",)\ndef _parse_args(args, *a, **kw):\n\t\"\"\"\n\tParse arguments provided to the exec module.\n\t\"\"\"\n\tif a and isinstance(a[0], str):\n\t\treturn [shlex.split(a[0])] + [_parse_args(args, *a[1:], **kw)]\n\treturn args, a\ndef run_setup(kw, is_pkg="}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif any(os.access(os.path.join(p,'version.py'), os.R_OK) for p in sys.path\n\t\t   if p and os.path.isdir(p) and os.path.isfile(os.path.join(p,'version.py'))):\n\t\tfrom..version_utility import get_version\n\telse:\n\t\tfrom..version_defaults import get_version\n\t\tget_version(True)\n\treturn\n\ndef main(version=None):\n\t\"\"\"\n\tDefault entry point.\n"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"]}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif style == \"pe"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"]}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n        # split '(' from 'pep440'\n\t\tif pieces[\"closest-tag\"]:\n\t\t\trendered = pieces[\"closest-tag\"]\n\t\t\tif pieces[\"distance\"]:\n\t\t\t\trendered"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\t\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"],\n\t\t}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\tel"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"]}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif style == \"pe"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\"date\": None}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"]}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif style == \"pe"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": \"unknown\",\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": pieces[\"error\"]}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif style == \"pe"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\t\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"],\n\t\t}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\tel"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\t\t\t\"version\": \"unknown\",\n\t\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\t\"dirty\": None,\n\t\t\t\t\t\"error\": pieces[\"error\"],\n\t\t\t\t\t\"date\": None\n\t\t\t\t}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not pieces or pieces[-1] == '.':\n\t\treturn '+'\n\treturn '.'\n\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged.DISTANCE"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces.get('rev') is not None:\n\t\treturn '.'\n\tif pieces.get('short') is not None:\n\t\treturn '+'\n\treturn '+'\n\n\ndef get_root(pieces):\n\t\"\"\"\n\tReturn the root (or top) directory, which we'll use\n\twritten files relative to.\n\t\"\"\"\n\tif pieces.get('root') is not None:\n\t\treturn pieces['root']\n\treturn get_paths(pieces)[0]\n\n\ndef get_version(pieces):\n\t\"\"\"\n\tReturn the short version"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\n\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged.D"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\n\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged.D"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged.DIST"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif '+' in pieces.get('closest-tag', ''):\n\t\treturn '.'\n\treturn '+'\n\n\ndef render_pep440(pieces):\n\t\"\"\"Build up version string, with post-release \"local version identifier\".\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\n\ndef render_pep440(pieces):\n\t\"\"\"\n\tBuild up version string, with post-release \"local version identifier\".\n\tOur goal: TAG[+DISTANCE.gHEX[.dirty]]. Note that if you\n\tget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\tExceptions:\n\t1: no tags. git_describe was just HEX. 0+untagged"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tchanged = False\n\tif all(pieces[\"closest-tag\"] == pieces[\"distance\"] for pieces in list(parse_index(pieces[\"index\"]))):\n\t\t# We have a one-to-one index at the far right side.\n\t\tif pieces[\"closest-tag\"]!= pieces[\"distance\"]:\n\t\t\tpieces[\"distance\"] += 1\n\t\t\tchanged = True\n\telse:\n\t\t# We have a one-to-many index.\n\t\tif pieces[\"closest-tag\"]!= None:\n\t\t\tpieces[\"closest-tag\"] += 1\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif len(pieces)!= 3:\n\t\treturn \".\"\n\treturn \"+\"\n\n\ndef render(pieces, style):\n\t\"\"\"Render the given version pieces into the requested style.\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\t\"version\": \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\"error\"].encode(\"utf-8\"),\n\t\t}\n\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces.get(\"closest-tag\", \"\"):\n\t\treturn \".\"\n\treturn \"+\"\n\n\t\ndef render_pep440(pieces):\n\"\"\"\n\tBuild up version string, with post-release \"local version identifier\".\n\"\"\"\n\tversion = []\n\tif pieces[\"closest-tag\"]:\n\t\tversion.append(pieces[\"closest-tag\"])\n\t\tif pieces[\"distance\"] or pieces[\"dirty\"]:\n\t\t\tversion.append(plus_or_dot(pieces))\n\t\t\trendered = render(pieces, render_pe"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif env is None: env = dict(os.environ)\n\n\t# -- NOTE: This comes off of a previous '_run_command_with_counts' function\n\t# for '--verbose' flag\n\tif not verbose and hide_stderr:\n\t\tenv['SYSTEMROOT'] = os.environ['SYSTEMROOT']\n\t\targs.extend('/i %s' % path for path in env['PATH'].split(os.pathsep))\n\treturn subprocess.Popen([command] + args, cwd="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t_log('run_command', commands, args)\n\n\tcmd = None\n\tfor command in commands:\n\t\t_log('command', command, args)\n\n\t\tif command.startswith('sudo'):\n\t\t\tif os.getuid()!= 0 and not hasattr(os, 'geteuid'):\n\t\t\t\traise RuntimeError(\n\t\t\t\t\t'Not running as root: {0}'.format(command))\n\t\tcmd = command\n\n\tif cmd is None:\n\t\traise RuntimeError('Unable to locate command: {0}'.format(command))\n\n\tif verbose"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tassert isinstance(commands, list)\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\tdisp_str = ''\n\t\t\tif p is None:\n\t\t\t\tdisp_str = '\\nCalling:'+ c\n\t\t\t\tp = subprocess.Popen(c, stdin=subprocess.PIPE, stdout=subprocess.PIPE, close_fds=True, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tdisp_str = '>>>'+ c\n\t\t\t\tp.stdin.write"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif env is None:\n\t\tenv = {}\n\n\t# -- NOTE: We need to keep a seperate list for stdout and stderr because\n\t#\tsome commands (e.g., powershell.exe, for example) will redirect both\n\t#\tto the same pipe, the stderr pipe.\n\n\tif isinstance(commands, string_types):\n\t\tcommands = [commands]\n\telse:\n\t\tcommands = list(commands)\n\n\tfor i, command in enumerate(commands):\n\t\tif'' in command:\n\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tall1 = COMMAND_LINE_ARGS\n\tall2 = COMMAND_LINE_ARGS\n\tall3 = COMMAND_LINE_ARGS\n\n\tif type(commands) == str:\n\t\tcommands1 = [ commands ]\n\telif type(commands) == list:\n\t\tcommands1 = commands\n\telse:\n\t\traise Exception('run_command accepts a string or a list.')\n\n\tcommand_args = list()\n\tcommand_args = all1 + commands1\n\tcommand_args.extend(args)\n\tcommand_string"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef envenv(env=None):\n\t\ttry:\n\t\t\tif env is None:\n\t\t\t\tenv = os.environ\n\t\t\tfor c in commands:\n\t\t\t\tfor k,v in six.iteritems(env):\n\t\t\t\t\tprint(env)\n\t\t\t\tyield c\n\t\tfinally:\n\t\t\tpass\n\t#\n\tif isinstance(cmd, BaseCommand):\n\t\tcmd_stack = [cmd]\n\telse:\n\t\tcmd_stack = argv = commands if not argv else argv\n\ttry:\n\t\t# _run_command() takes care of passing"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfor i, command in enumerate(to_list(commands)):\n\t\tsp_command = to_native_string(command)\n\t\tif args.verbose or conf.verbose or True:\n\t\t\tif not hasattr(shlex_quote, '_has_tab'):\n\t\t\t\tcmd_line = sp_command\n\t\t\telse:\n\t\t\t\tcmd_line = shlex_quote(' '.join(to_list(params(command))))\n\t\t\tif not sp_command.endswith('&'):\n\t\t\t\tif not isinstance(sp_command, collections.Iter"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t_logger(f\"run_command: {' '.join(commands)}\")\n\tenv = env or dict()\n\n\tif isinstance(commands, str):\n\t\tcmd = list([commands])\n\telse:\n\t\tcmd = commands[:]\n\tstarted = datetime.datetime.now()\n\tprocess = subprocess.Popen(\n\t\t['bash', '-c'],\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tcwd=cwd\n\t)\n\toutput"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif sys.version_info < (2, 4):\n\t\tfrom commands import Command\n\t\tcommand = Command(commands, args, cwd, env)\n\t\t\n\telse:\n\t\tfrom shlex import split\n\t\tcommand = ''\n\t\tfor x in commands:\n\t\t\tcommand += '(' + str(x).replace('sh', '$SHELL[-s].  ') + ')'\n\t\t\tcommand += \" \" + split(args)\n\tif verbose:\n\t\tprint(command)\n\tstdout, stderr = bm.stdout, bm."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand_str = \" \".join(commands)\n\tstart_time = time()\n\n\tif args.debug:\n\t\tprint(command_str, file=sys.stderr)\n\telif not args.quiet:\n\t\tsys.stdout.write(\"Executing command: %s%s\\n\" % (cwd, shell_quote(command_str)))\n\tenv = os.environ.copy()\n\tif env is not None:\n\t\tenv = env.copy()\n\n\t# Save and truncate stdout, here for convenience.\n\tstd"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tchar_prefixes = set(\"$+-.\").difference(config.versionfile_source.split(\"/\"))\n\tif len(char_prefixes) == 1:\n\t\treturn config\n\n\tground_matcher = compute_ground_matcher()\n\ttry:\n\t\tn = ground_matcher.match(\".gitignore$\").end()\n\texcept (AttributeError, ValueError):\n\t\tn = 0\n\tgfm = \"\".join(ground_matcher.get_lines(None, config.versionfile_source"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif versionfile_source is None:\n\t\tversionfile_source = pkg_resources.resource_filename(\n\t\t\t__name__,'versioneer.py')\n\twith open(versionfile_source) as f:\n\t\tlines = f.readlines()\n\tlines = filter(None, lines)\n\treturn VersioneerConfig(\n\t\tversionfile_source,\n\t\tcfg.VCS,\n\t\tstyle,\n\t\tfallback_version,\n\t\tparentdir_prefix,\n\t\tverbose)\n\n\nimport subprocess\nclass VersioneerConfig(object):\n\tdefault"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VersioneerConfig()\n\n\ndef get_cmdclass():\n\t\"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\"\"\"\n\tif \"versioneer\" in sys.modules:\n\t\tdel sys.modules[\"versioneer\"]\n\t\t# this fixes the \"python setup.py develop\" case (also 'install' and\n\t\t# 'easy_install.'), in which subdependencies of the main project are\n\t\t# built (using setup.py bdist_egg) in the same python process. Assume\n\t\t# a mainproject/setup.py"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VersioneerConfig()\n\n\nclass VersioneerConfig:\n\t\"\"\"\n\tContainer for Versioneer configuration parameters.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize VersioneerConfig() with defaults for all imported submodules.\n\t\t\"\"\"\n        # Global verbosity.\n\t\tself.verbose = False\n\n        # Discard every WARNING message in the build.\n\t\tself.ignore_warnings = False\n\n        # Options for various debugging situations.\n\t\tself.export_version = default(\"exported_version\")\n        # Tag name to export\n\t\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\ts = \"[config]\\n\" \\\n\t\t\"# Place all versions into s.versions, and all _finalize_<type>() dicts into s.finalized\\n\" \\\n\t\t\"# naming/description keys, or they will be deleted from the config.\"\n\n\tconf = VersioneerConfig()\n\n\tx = conf.get(\"cmdclass\")\n\tif x is None:\n\t\tx = {}\n\tconf[\"cmdclass\"] = x\n\n\ty = conf.get(\"versionfile_source\")\n\tif y is None:\n\t\ty = conf.get(\""}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig_class = VersioneerConfig\n\n\t# these variables are overridden later eval(in main())\n\t# see version='2.2.3' in prepare()\n\tconfig_vars = {\n\t\t# versionfile_source is the relative path from the top of the source\n\t\t# tree (where the.git directory might live) to this file. Invert\n\t\t# this to find the root from __file__.\n\t\t'versionfile_source': 'pykakasi/_version.py',\n\n\t\t# Make sure we don't warn a user if they import a port object"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# these strings are filled in when'setup.py versioneer' creates\n\t# _version.py\n\tcfg = VersioneerConfig()\n\tcfg.VCS = \"git\"\n\tcfg.style = \"pep440\"\n\tcfg.tag_prefix = \"\"\n\tcfg.parentdir_prefix = \"nonexistingdata-data-\"\n\tcfg.versionfile_source = \"nonexistingdata/__version__.py\"\n\tcfg.verbose = False\n\treturn cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\ttry:\n\t\ttoken = os.environ[\"TOKEN\"]\n\texcept KeyError:\n\t\traise EnvironmentError(\"No TOKEN environment variable\")\n\n\tconfig.VCS = \"git\"\n\tconfig.versionfile_source = \"jim_tools/_version.py\"\n\tconfig.versionfile_build = \"jim_tools/_version.py\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"jim_tools-\"\n\tconfig.verbose = False\n\tconfig.versionfile_build"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif vcs is None:\n\t\tif 'git' in sys.modules:\n\t\t\tadd_vcs_support()\n\t\telse:\n\t\t\traise ImportError\n\t\t\t\n\tconfig = VersioneerConfig()\n\t# These variables are only defined if we use the versioneer.py\n\t# wrapper.\n\tconfig.VCS = vcs\n\tconfig.style = style\n\tconfig.versionfile_source = versionfile_source\n\tconfig.versionfile_build = versionfile_build\n\tconfig.tag_prefix = tag_prefix\n\tconfig.parentdir_prefix ="}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\tconfig.style = \"\"\n\tconfig.versionfile_source = \"vendoredev/_version.py\"\n\tconfig.versionfile_build = \"./version.py\"\n\tconfig.tag_prefix = \"v\"\n\tconfig.parentdir_prefix = \"vendoredev-\".strip(\"_\")\n\tconfig.verbose = False\n\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"\n\tException raised if a method is not valid for the current scenario.\n"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef _register_vcs_handler(f):\n\t\tsetattr(f, 'vcs_handler', (vcs, method))\n\t\treturn f\n\treturn _register_vcs_handler\n\n\nVCS_HANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):\n\t\"\"\"\n\t\tCreate callback that implements each VCS operation.\n\t\tThe first argument to the callback will be the instance of the handler being initialized\n\t\tThe second argument will be the name of the operation being performed\n\t\t\"\"\"\n\tmethod = register_vcs_handler"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tprint(f'Registering {vcs} vcs handler')\n\treturn methods.register(method, vcs)(handler_method)\n\n\nclass VCS:\n\t\"\"\"\n\tCommon superclass.\n\t\"\"\"\n\n\t__handle__ = {}\n\t__register__ = {}\n\n\t# __register__ = {\n\t# \t'git': ('git', 'git'),\n\t# \t'hg': ('hg', 'hg'),\n\t# \t'bsd-git': ('git', 'bsd-git'),\n\t# }\n\n\t@"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdct = get_handler_config()\n\tvcses = set(vcs.lower().strip() for vcs in SUPPORTED_VCS)\n\n\tdef inner(func):\n\t\t_log.debug('Registration of %s in %s', func.__name__, vcses)\n\n\t\tdef set_config(self, config, key):\n\t\t\tif self.is_config(config, key):\n\t\t\t\t_log.debug('%s %s already set in %s', func.__name__, key, self.__class__.__name__)\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tsetattr(f, 'git_handler_%s' % vcs, True)\n\t\tsetattr(f, '_%s_handler' % vcs, f)\n\t\tif hasattr(f, '__doc__'):\n\t\t\tassert 'git repository :str:' in f.__doc__\n\n\t\t@functools.wraps(f)\n\t\tdef wrapped(git, *args, **kwargs):\n\t\t\treturn getattr(f, '_vcs_handler')(git, *args, **kwargs)\n\t\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorate(f):\n\t\t\"\"\"\n\t\tregister method for VCS.\n\n\t\tParameters\n\t\t----------\n\t\tvcs : class, \"vcs.VCS\"\n\t\t\tvalid VCS object (e.g. VCS.Repo).\n\t\tmethod : func\n\t\t\tfunction to register for VCS.\n\t\t\"\"\"\n\t\tif isinstance(vcs, type):\n\t\t\tvcs = vcs(repo_url=None)\n\t\tf._register(__dict__=getattr(vcs, '_register'))\n\t\tf._handler = vcs.add"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\tsetattr(f, \"_vcs\", (vcs, method))\n\t\treturn f\n\treturn decorator\n\n\n#########################################\n## Support classes for command line tools ##\n#########################################\n\n# Our list of \"extensions\" and their descriptions --\n# here for programmatic access by command lines.\nclass cli_list:\n\t__list = ()\n\tdef __init__(self, filename, extensions):\n\t\tself.name = filename\n\t\tself.extension = ''\n\t\tself.description = ''\n\t\tself.ext"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef _vcs_handler(func: Callable) -> Callable:\n\t\t_VCS_HANDLERS[vcs] = func\n\t\t@functools.wraps(func)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapped\n\treturn _vcs_handler\n\ndef get_vcs_handler(handler=_DEFAULT_SH) -> Callable:\n\t\"\"\"\n\tReturn (handler: Callable) if handler is vcs type.\n\tIf not, return handler.\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef _wrapper(func):\n\t\tif issubclass(vcs, VCSBase):\n\t\t\tvcs.set_handler(method)\n\t\telse:\n\t\t\traise NotImplementedError('not implemented yet')\n\t\treturn func\n\treturn _wrapper\n\n\nif __name__ == \"__main__\":\n\tclass Repo('git'):\n\t\tdef __init__(self, url):\n\t\t\tself.url = url\n\n\t\tdef print(self):\n\t\t\tpass\n\n\t\t@staticmethod\n\t\tdef init():\n\t\t\trepo = Repo('http://xxxg"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorate(func):\n\t\tmethod_dict = argmap(func) if ismethod(func) else None\n\t\tif (isinstance(method, str)):\n\t\t\tvcs.map(method)\n\t\telse:\n\t\t\tif (method not in vcs.methodMap):\n\t\t\t\tvcs.methodMap[method] = {}\n\t\t\tvcs.methodMap[method]['default'] = method_dict\n\t\treturn func\n\treturn decorate\n\n\ndef extend(handler, default=False):\n\t\"\"\" Extend argument handler for a VCS or just remove default handler with"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef class_decorator(cls):\n\t\tif vcs := getattr(cls, 'vcs_registered', False):\n\t\t\treturn cls  # Already registered -> do nothing\n\t\tcls.vcs_registered = True\n\t\t_METHOD_HANDLERS[vcs] = method\n\t\treturn cls\n\treturn class_decorator\n\n\n@register_vcs_handler('git', 'git')\n@register_vcs_handler('github', 'github')\ndef _handle_github_vcs(repo):\n\ttry:\n"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor t_f in ('', 't'):\n\t\tfor d_f in ('', 'd'):\n\t\t\tfor h_f in ('', 'h'):\n\t\t\t\t# Check tree.yaml\n\t\t\t\tif h_f!= '':\n\t\t\t\t\tcheck_tree = tree(h_f, validate_objects=0, check_digests=0)\n\t\t\t\t\tcheck_tree.read_file(self._yaml, True)\n\t\t\t\t\tfor root in (self._root(t_f), self._root("}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects, good_objects = self.check_storage(\"/\")\n#\tif validate_objects and good_objects == 0:\n#\t\tvalidation_failure(\"Storage Root\", num_objects, good_objects)\n#\tself.check_storage(\"/tmp/\")\n#\tnum_objects, good_objects = self.check_storage(self.tempDir)\n#\tif validate_objects and good_objects == 0:\n#\t\tvalidation_failure(\"Temp\", num_objects, good_objects)\n\n\tnum_objects, good_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tself.valid_objects = 0\n\tself.good_objects = 0\n\tself.failed_objects = 0\n\tdirs = self.storage.dirs(self.hierarchy, validate_objects=validate_objects, show_warnings=show_warnings)\n\tif dirs:\n\t\tif not check_digests:\n\t\t\traise StorageRootError('The following directories may be unsafe: %s' %(dirs,))\n\t\tfor cur_dir in dirs:\n\t\t\tnum_objects = self.storage.all_paths(dir=cur"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# Check if there is an archive in the root\n\tdef check_archive() -> (bool, str):\n\t\tif validate_objects and valid_dir(self.root_archive_path):\n\t\t\t# TODO: warn if the root is not an archive or if it is not the root to search\n\t\t\tarchive_hash = hash_dir(self.root_archive_path)\n\t\t\treturn True, f'root archive checksum: {archive_hash}'\n\t\treturn False, ''\n\t\t\n\tdef check_cache() -> (bool, str):\n\t\t# Check if there"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = good_objects = 0\n\tobj_class = storage.get_obj_class(storage.META_STORE_ROOT)\n\tfor o in self.iter_objects(validate_objects, check_digests):\n\t\tpath = o.get_full_path()\n\t\tif (o.isdir() and o.islink()):\n\t\t\t# TODO: Check path integrity\n\t\t\tobj_class.check_object_dir_attrib_mode(path)\n\t\telif (validate_objects and o.isdir()):\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif validate_objects:\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tfor obj in self._objects:\n\t\t\t# Check objects in the object dir\n\t\t\tok = obj.validate(self, check_digests=check_digests, show_warnings=show_warnings)\n\t\t\tnum_objects += 1\n\t\t\tif ok:\n\t\t\t\tgood_objects += 1\n\t\treturn num_objects, good_objects\n\telse:\n\t\treturn 0, 0\n\n\t\n\t\ndef remove_hierarchy(self, root_path=None):\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# Validate subdirectories/objects\n\th = self.root/self.hierarchy_path\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor o in h.iterdir():\n\t\ttry:\n\t\t\to.stat()\n\t\texcept Exception as e:\n\t\t\tif o.is_dir():\n\t\t\t\tprint(\"- Invalid subdirectory {}\".format(o))\n\t\t\telse:\n\t\t\t\tprint(\"- Invalid object {}\".format(o))\n\t\t\t\tif check_digests == True:\n\t\t\t\t\twith o.open() as bad_file:\n\t\t\t\t\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_bad = 0\n\tnum_obj_skipped = 0\n\t(num_obj, good_num) = self.hierarchy.validate(validate_objects, check_digests, show_warnings)\n\tif good_num > 0:\n\t\tprint(\"{} Good {} objects\".format(num_obj, good_num))\n\telse:\n\t\tprint(\"{} Bad objects\".format(num_obj - good_num))\n\t\tnum_bad += num_obj - good_num\n\treturn (num_obj, num_bad)"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tfrom binwen.utils import time_now\n\tfrom binwen.config import conf, LOG_FILE_PATH, VERBOSE\n\n\tstart_time = time_now()\n\tLOGFILE_HANDLE = open(LOG_FILE_PATH, \"ab\")\n\ttry:\n\t\tif check_digests==False: self.reread()\n\t\tobjects = self.get_objects()\n\t\tif objects is None:\n\t\t\tLOGFILE_HANDLE.write(\"%s: check_hierarchy(): Error while rereading root: no data found for objects\\n\""}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\ttot, good = 0, 0\n\t\n\troot_sha1 = gzip_file(self.root_file, self.root_digest, self.verbose, show_warnings)\n\thash_dict = {}\n\t\n\tfor key in self.root_keys:\n\t\ttot += 1\n\t\tif (key == ''):\n\t\t\thash_dict[key] = root_sha1\n\t\telse:\n\t\t\thash_dict[key] = self.read_key(key)\n\t\t\t\t\n\tself.hashes = hash_dict\n"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tdef remove(self):\n\t\t\"\"\"\n\t\tRemove the OCFL storage root.\n\t\t\"\"\"\n\tdef removeAllFiles(self):\n\t\t\"\"\"\n\t\tRemove all files from the storage root.\n\t\t\"\"\"\n\tdef update(self):\n\t\t\"\"\"\n\t\tUpdate this storage root.\n\t\t\"\"\"\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tassert not self._initialized, \"OCFLObject.initialize() should not be called twice!\"\n\n\tself._initialized = True\n\n#\tself.OCFLStorageManager = OCFLStorageManager(self)\n\tif self._defaultLogLevel is not None:\n\t\tconfig.registry().setDefaultLogLevel(self._defaultLogLevel)\n\n\tif self._debug:\n\t\tfrom AccessControl.security import getSecurityManager\n\t\tfrom App.class_init import InitializeClass\n\t\tfrom App.config import UserConfigurationAdapter\n\n\t\tdef setupLogging():\n\t\t\tloglevel ="}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\t\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tx.__init__(...) initializes x; see help(type(x)) for signature\n\t\t\"\"\"\n\t\tpass\n\tdef add(self, ocfl_obj):\n\t\t\"\"\"\n\t\tAdd a file or directory of files to the storage.\n\t\t\"\"\"\n\t\tpass\n\tdef copy_and_fetch(self, file_list):\n\t\t\"\"\"\n\t\tCopy a list of files from local to storage and fetch their contents by calling the OCFL Object Storage."}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# This is called by the default OSCore initialization code.\n\tself.initialized = True\n\tif self.getOSCoreType() == oscp.OscpType.NONE:\n\t\traise RuntimeError(\"Failed to create %s because it is an OSCore with no type\" % (self.getName(),))\n\tif self.getIsDefault() and (self.getOSCoreType() == oscp.OscpType.OSCAR):\n\t\traise RuntimeError(\"Unsupported default %s storage (which is an OS Car)\" % ("}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# We are being invoked directly, so make sure we have an OCFLStorage root.\n\tself.OCFLStorageRoot = OCFLStorageRoot(self.MdRoot)\n\t# Create the OCFL storage root\n\tself.create_new_storage_root()\n\tself._initialized = True\n\n\treturn self.OCFLStorageRoot.storage_root\n\ndef make_ocf(self, name, storage_root):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# We are being invoked directly, so make sure we have an OC"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif not self.initial_check():\n\t\ttry:\n\t\t\tos.mkdir(self.root)\n\t\t\t# make sure root is a non-directory\n\t\t\tif os.path.isdir(self.root):\n\t\t\t\tself.state = InitializeState.OK\n\t\t\t\tself.log.success(\"Initial root creation successful.\")\n\t\t\telse:\n\t\t\t\tself.state = InitializeState.ERR\n\t\texcept OSError:\n\t\t\tself.state = InitializeState.ERR\n\t\tfinally:\n\t\t\treturn\n\n\tself.create_log_file()\n\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif self.storage == None:\n\t\tself.store = Storage(self)\n\telse:\n\t\tif self.storage.store == None:\n\t\t\tself.store = Storage(self)\n\treturn self.store\n\n\nclass Storage:\n\t\"\"\"\n\n\tThis class is responsible for maintaining a container of OCFL objects\n\tstored and indexed by name.\n\n\tA container can be used to control the management of OCFL objects.\n\tThe container defines a persistent storage mechanism by encapsulating\n\ta directory on-disk in which all of its objects are stored. Methods\n\ttherefore"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\n\n\nclass OCFL_Error(Exception):\n\t\"\"\"OCFL exception class.\n\n\t\"\"\"\n\n\tdef __init__(self, message):\n\t\t\"\"\"\n\t\tOCFL exception class constructor.\n\n\t\t:param message: Exception message\n\t\t:type message: *\n\t\t:return: None\n\t\t:rtype: None\n\n\t\t\"\"\"\n\t\tsuper(Exception, self).__init__(message)\n\n\tdef InitializeErrorCode(self):\n\t\t\"\"\"\n\t\tOCFL exception code initializer\n\n\t\t\"\"\"\n\t\tpass\n\n\n\nclass OCFL_Exception_InvalidInput"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = tempfile.mkdtemp('-bclocf')\n\tself.initialize = staticmethod(self.initialize)\n\nif __name__ == \"__main__\":\n\tpass\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\troot_dir = '%(target_root)d%(sep)s' % self.path\n\tif not os.path.isdir(root_dir):\n\t\traise FSTemplateError('Unable to find storage root at \"%s\"' % root_dir)\n\tif not self.name in os.listdir(root_dir):\n\t\tos.makedirs(os.path.join(root_dir, self.name))\n\n\treturn os.path.join(root_dir, self.name)\n\n\nclass FSTemplateError(Exception"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tprecision = 0\n\twhile version.startswith(\"0\"): precision += 1\n\tversion = version[precision:]\n\tif version: version = version[0] + version[1:].rjust(precision,\"0\")\n\telse: version = precision * \"0\"\n\tversion = format(int(version), 'x')\t# Strip away decimal point when needed.\n\tversion = str((int(version) + 1) * 10 ** 9)\n\tversion = version[:precision] + (\"0\" if version[precision] == \"9\""}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tpass\n\n\nclass ParseError(Exception):\n \"\"\" Raised when an invalid sequence of characters was encountered. \"\"\"\npass\n\n\nclass SyntaxWarning(Warning): \"\"\" A warning caused by an invalid expression. \"\"\"\npass\n\n\nclass UnusedImport(SyntaxWarning): \"\"\"\n\tThe module/module function was never used.\n\n\tSee L{UsedImport}.\n\t\"\"\"\n\tdef __init__(self, real_modname, name):\n\t\tpass\n\tdef warn(self):\n\t\tpass\n\t\n\nclass InvalidPythonEnvironment(ImportError): \"\"\" An invalid Python environment was"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tv = make_version_tuple(version)\n\ttry:\n\t\t(major, minor, patch) = (max(2, int(v[0])), max(1, int(v[1])), max(0, int(v[2])))\n\texcept:\n\t\timport errno\n\t\traise errno.ENOENT, '', 0\n\tif 1 < v[1]:\n\t\tmajor -= 1\n\tif 0 < v[2]:\n\t\tminor -= 1\n\twhile True:\n\t\tnewversion = (major, minor, patch)\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not version:\n\t\traise ValueError('version must be non-empty string')\n\t\n\tpre = re.match('(?:0)', version)\n\tif pre:\n\t\traise ValueError('pre-fixed versions not acceptable')\n\t\n\tpre = re.match('0+', version)\n\tif not pre:\n\t\tpre = re.match('0+', version[0:len(version)-1])\n\t\t\n\tif not pre:\n\t\treturn None\n\t\n\t# we have at least one pre-zero\n\tpre = pre.group()"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\tversion, prefix, postfix = split_version(version)\n\t\n\tif prefix:\n\t\t# Zero-prefixed version: append non-zero-prefixed prefix for 1, 2... postfixes\n\t\tprefix = prefix + next_digit(prefix)\n\t\n\tif postfix:\n\t\t# Non-zero-prefixed version: prepend and append zero-prefixed postfix for 1, 2... non-0-prefixed postfixes\n\t\tpostfix = next_digit(postfix) + postfix\n\t\n\treturn '.'.join([\n\t\t'"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version.startswith('0'): return '1.0a'+version\n\telse: return ('0','0','1')[ord(version[0])-1]+version\n\ndef make_release_file(source, version, tags=[]):\n\"\"\"\n\tMake a release file using information from the source release.\n\t\n\tThe function will make a new source release file, copy the\n\tcontents of the release tag files from the source repository,\n\tupdate the source revision number and any tags specified,\n\tsave it into a release file, and then return"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\twhile version.endswith('0'):\n\t\n\t\tversion = version[:-1]\n\t\n\tif version[0] == '0':\n\t\n\t\treturn f'v{int(version[1:]) + 1}'\n\t\n\treturn f'v{int(version) + 1}'\n\n\ndef add_commit(app, version, commit_message):\n\"\"\"\n\tAdd a new commit to the application and create a release note entry for it\n\t\n\tAdds a new commit and releases the entry for it.\n\t\n\tArgs:"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t#print 'Next version called as requested', version\n\tif (version<1)or(version>99999999999):\n\t\traise ValueError('%x not as requested'%version)\n\telse:\n\t\ttemp=map(int,bin_form(version)[::-1])\n\t\tdel temp[0]\n\t\tnext_version=0\n\t\tfor x in temp:\n\t\t\tnext_version<<=1\n\t\t\tif (x==1): next_version|=1\n\t#print 'Next version calculated', next_version\n\treturn next_version\n\n\nversion="}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\tglobal full_version, version_pattern\n\t\n\tif len(version) < 2:\n\t\traise ValueError('Version %s too short' % repr(version))\n\t\n\tif not version_pattern.match(version):\n\t\traise ValueError('Invalid version %s' % repr(version))\n\t\n\tif full_version:\n\t\tnewversion = version_pattern.match(full_version)\n\t\tparts = newversion.groupdict()\n\t\tnext_nr = int(parts['next']) + 1\n\t\tnewnext = \"0.\" + \"%"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn version.rjust(3, '0')\n\ndef prev_version(version):\n\"\"\"\n\tPrevious version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn version.ljust(3, '0')\n\n\n\n\n\nif __name__ == \"__main__\":\n\tsys.setrecursionlimit(2500)\n\t\n\tprint(prev_version('00011111'))\t# '0')\n\tprint(next_version('00011111'))\t# '11110000"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tcurrent_index = 1\n\t\n\t# Version directories\n\tinventory_dirs = [\n\t\t# 1st\n\t\t('/inventory.jsonl', '{0}/{1}', ''),\n\t\t\n\t\t# 2nd onward\n\t\t(('{0}/{1}/{2}/{3}.jsonl', '{0}/{1}/{2}/{3}'), '/{4}'),\n\t\t(('{1}/{0}/{2}/{3}/{4}.jsonl', '{1}/"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_dirs = list(version_dirs)\n\tif not version_dirs:\n\t\tself._set_error(None, \"Version inventories missing in inventory\")\n\t\t\n\tif len(version_dirs) > 1 and self.options.single_inventory:\n\t\t\tself._set_error(\n\t\t\t\tNone,\n\t\t\t\t\"--single-inventory specified, but many (or \"\n\t\t\t\t\"all) inventories present. Please check --inventory-dirs\"\n\t\t\t)\n\t\t\t\n\tfor version_dir in version_dirs:\n\t\tinventory_path ="}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\ttry:\n\t\tvdir = version_dirs[0]\n\t\tindexer.do_validate = True\n\t\tindexer.inventories_by_version.refresh(dirname(fpath(vdir)))\n\t\tindexer.check_root_digest()\n\texcept ValueError:\n\t\traise\n\texcept Exception, err:\n\t\tlog.info(\"%s: validate %s\" % (sys.exc_info()[2].tb_frame.f_code.co_name, err))\n\t\traise\n\n\ttry:\n\t\tfor v"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\t\n\tfor ver in version_dirs:\n\t\ttry:\n\t\t\tver_path = path.join(self.location, ver)\n\t\t\tver_info = json.loads(open(path.join(ver_path, 'inventory.json')).read())\n\t\t\t\n\t\texcept Exception as e:\n\t\t\terrs += 1\n\t\t\tcontinue;\n\t\t\t\n\t\tif not ver_info or ver not in ver_info:\n\t\t\terrs += 1;\n\t\t\n\tif errs > 0:\n\t\treturn errs > 1\n\telse:\n\t\treturn False"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor dir_idx, v_dir in enumerate(version_dirs):\n\t\tcur_version = dir_idx\n\n\t\tv_content_dir = os.path.join(self.top_dir_name, v_dir)\n\n\t\tif not os.path.exists(v_content_dir):\n\t\t\tprint('Skipping content directory at {} as it does not exist.'.format(v_content_dir))\n\t\t\tcontinue\n\t\t\n\t\tv_content_dir_name = os.path.split(v_content_dir)[1]\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inventory = self.inventory_from_path(self.inventory_from_path(self.path + \"/\").root)\n\tversion_dirs = list(version_dirs)\n\tif not version_dirs:\n\t\tversion_dirs.append('1')\n\tif not version_dirs or len(version_dirs) > MAX_VERSION_VERSIONS:\n\t\tself.handle_error(3015, \"Invalid data supplied (%s) - maximum number of versions allowed is %d\" % (\n\t\t \tstr(version_dirs), MAX"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_dir = os.path.dirname(version_dirs[0])\n\tinventories = []\n\tdigests = {}\n\n\tfor version_dir in version_dirs:\n\t\tversion = os.path.basename(version_dir)\n\n\t\tinventories.extend(get_root_content_inventories(version_dir))\n\n\t\t# if there isn't an inventory for this version, don't bother with digests\n\t\tif len(inventories) < 1:\n\t\t\tcontinue\n\n\t\tfor storage, subdir_path in get_sub"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# Check that all the versions have an inventory\n\tfor version_dir in version_dirs:\n\t\tinventory_file = self.check_files([version_dir, \"./inventory.json\"])\n\t\tif inventory_file is None:\n\t\t\traise ValidationException(\"Version %s (directory name=%s) expected to have an inventory\" % (self._version, version_dir))\n\t\t\n\t\t# Check for existing content in the current version\n\t\texpected_content_digest = self.check_files([version_dir, \"./inventory.json\", \"./content\"]"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tfor version_dir in version_dirs:\n\t\tinventories_json_path = mf2_util.get_root_dir() + '/inventories/' + version_dir + '/inventory.json'\n\t\n\t\tif os.path.exists(inventories_json_path):\n\t\t\tinventories_file = io.open(inventories_json_path, 'r', encoding='utf8')\n\t\t\tjson_string = inventories_file.read()\n\t\t\tif json_string:\n\t\t\t\ttry:\n\t\t\t\t\tinventories"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor i in range(len(version_dirs) - 1):\n\t\t# Get file details for this version.\n\t\t(version, version_dir, file_details) = self.load_version_file(i)\n\t\tbase_filename = file_details['filename']\n\t\t\n\t\t# Find the previous inventory, if any:\n\t\tprevious = None\n\t\tfor (p_version, p_dir, p_file) in self.get_all_version_files(version_dir):\n\t\t\tif p_dir!= version_dir or p_filename!= base_"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tglobal files\n\n\tobj = get_path(path, True)\n\n\tif obj == None or type(obj)!= dict:\n\t\treturn ('file', obj if obj!= None else \"None object\")\n\n\tobj_type = _find_path_type(obj, ['0=*'])\n\n\tif obj_type == \"root\":\n\t\tobj_type = _find_path_type(find_path(path), '0=*')\n\n\treturn obj_type\n\ndef _get_tree_root(path=None):\n\t\"\"\"\n\tGet the root of"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t# If the path is a directory and a version is available but empty, return 'file'.\n\t# For now though, just assume it is a file.\n\tif _path.isdir(path) and _get_path_info(path, 'info')!= {}:\n\t\tresult = \"file\"\n\telif _path.isfile(path):\n\t\t# 'root' files are the top directory, and 'file' files\n\t\t# should be 'file' if they have'version/name' info, 'path/info'\n\t\t# otherwise.\n\t\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\treturn 'root'\n\ndef find_partner(root):\n\"\"\"\nReturn a path to any partner directory, possibly including one with a different file type.\n\nInput value: \n    root - a string describing the root; see return types\n\t\nReturn value:  \n    a string describing the root\n\"\"\"\nreturn 'partner'\n\n\n############################################\n\n\ndef write_bytes(f, bytes):\n\"\"\"\nWrite all 'bytes' into file 'f'\n\"\"\"\npass\n\ndef write_int16(f, x):\n\"\"\"\nf.write(struct"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tpath_type = '?'\n\tobj = object()\n\tm = re.search(r'^0=*\\s*(.+)$',path)\n\tif path_match(path, r'^0=*\\s*((?P<root>[^.]+)/)|(?P<object>[^.]+)/') == obj:\n\t\tpath_type = 'root'\n\telif m == None:\n\t\tpath_type = 'file' \n\telse:\n\t\tpath_type = '?Unknown' \n\treturn(path_type"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\ttry:\n\t\tobject = int(path[0])\n\t\treturn \"root\"\n\texcept:\n\t\ttry:\n\t\t\t(file_path, filename, size) = ocfl.find_inventory_entry(path)\n\t\texcept ValueError:\n\t\t\treturn \"file\"\n\t\telse:\n\t\t\treturn \"object\"\n\ndef get_object(path):\n\"\"\"\n\tReturn an OCFL Object at the paths given. \n\t\n\tThis will raise ValueError if the path is not a path in a valid OpenCFL Storage Root.\n\t\n\t:param path"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tpath = split(path)\n\tif path[0] == '': \n\t\tpath.pop(0)\n\tobj = OCFL.files.object(path.pop())\n\tif not isdir(path):\n\t\treturn 'file'\n\tif len(path) == 0:\n\t\tif obj.has_attr('_M_name'):\n\t\t\treturn 'object'\n\t\telse:\n\t\t\treturn 'root'\n\telse:\n\t\treturn 'file'\n\t\ndef dir_filter(pattern, path):\n\t\"\"\"\n\tReturns paths matching"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tif (os.path.exists(path) == False):\n\t\treturn 'error: The file or directory does not exist.'\n\t\n\tf = open(path,\"r\")\n\tdata = f.read()\n\tf.close()\n\t\n\tlines = data.split('\\n')\n\t\n\t# Check out the first line and see if it looks like an OCFL Storage Root\n\tline1 = lines[0]\n\tif (line1.startswith(const(\">\"))):\n\t\tif (line1.startswith(\">0"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tdir_path = os.path.dirname(path)\n\ttry:\n\t    if fs.access(dir_path, F_OK):\n\t        return 'root'\n\texcept:\n\t    return 'unknown error trying to access, assuming unknown directory.'\n\t\n\tif dir_path == '/': # directory to scan\n\t    if not fs.access('/', F_OK):\n\t\treturn 'unknown error, not root directory'\n\t    if fs.access('/', R_OK):\n\t\ttry:\n\t\t    size = fs.f"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif re.search(\"0=*\", path) is None:\n\t\treturn 'root'\t# Not a file - it should be an OCFL Storage Root\n\telse:\n\t\tif path.startswith(\"0=00\"):\n\t\t\treturn 'object'\n\t\telif path.startswith(\"0=1b\"):\n\t\t\treturn 'file'\n\t\telse:\n\t\t\treturn 'file?' + path\n\ndef path_type_str_type(path):\n\"\"\"\tReturns the textual type, such as \"Storage Root\" or\n\t\"Path not found\".\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tresult = \"root\"\n\tpaths = get_listing(path)\n\tif paths.startswith(\"0=\"):\n\t\ttype, identifier = paths.split(\"/\")[0:2]\n\t\tif type == \"0=\":\n\t\t\tresult = \"file\"\n\t\telif type == \"1=\":\n\t\t\tresult = \"object\"\n\t\telse:\n\t\t\tresult = \"other\"\n\telse:\n\t\t\tresult = \"other\"\n\t\n\treturn result\n\ndef object_identifier(path):\n\"\"\"\n\tReturn the identifier of an"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif not bug_ids:\n\t\tparams['status']='NEW'\t\n\t\tparams['type']='new'\t\n\t\treturn params\n\telse:\n\t\tparams['status']='ASSIGNED'\n\t\tparams['type']='assigned'\t\n\t\treturn params\n\n\t\nclass BugzillaManager(ResourceManager):\n\t\t\n\tdef __init__(self, bzhost, bzport, bzuser, bzpass):\n\t\tself.bzhost = bzhost\n\t\tself.bzport = bzport\n\t\tself.bzuser ="}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams.append(\"--\")\n\tfor x in bug_ids:\n\t\tparams.append(x)\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbz_params = {}\n\tfor attr in params.split(\"\\n\"):\n\t\t# only take the first line in each comment block\n\t\tattr = attr.strip()\n\t\tif \":\" in attr:\n\t\t\tbz_params[attr[0:attr.index(\":\")]] = attr[attr.index(\":\")+1:].strip()\n\t\telif bz_params:\n\t\t\t# This is a final params\n\t\t\tbreak\n\tif bug_ids and params.startswith(\"Fixed:\"):\n\t\t# Bugids set, but there"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbug_ids_list = bug_ids.split(',')\n\tparam_string = ','.join(['{}-{}'.format(key, params[key]) for key in self.bzrparam_keys])\n\tbug_id = self.branch.bzfix(self.ticket)\n\tbzparams_dict = {'component': 'BUG-{}'.format(bug_id),'short_description': 'bugzilla_amend_description {}'.format(datetime.datetime.now()),'summary':'Update Bugzilla"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t\n\tfor (bug_id, fields) in params.get_iter(bug_ids):\n\t\tbug = fields,\n\t\tnew_status = self._status_map[new_status]\n\t\tparams.update(bug[0], set__status = new_status)\n\t\tself.log_debug(\"AMEND BUG: %s:%s -> %s\", bug_id, fields.get(\"title\"), new_status)\n\n\treturn params\n\n\nclass BugzillaAmendment(Amendment):\n\tdef __init__(self, id, target_bug_id"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself._log.debug(\"amend_bzparams\")\n\n\tbug_ids_str = \",\".join(bug_ids)\n\tparams[\"reporter\"] = params[\"reporter\"] + \",\" + bug_ids_str\n\treturn bug_ids_str\n\ndef _extractbugids(url):\n\"\"\"\n\tTake a url and parse for bug ids.  This is a horrible hack to do get\n\tsimple enough to get any bug ids but it should work\n\treturns a tuple [(tname, bzid)...]\n\"\"\"\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif cfg.force_params:\n\t\tif bug_ids:\n\t\t\tself.log.info('Applying forced params to Bugzilla')\n\t\t\tfor line in params.split('\\n'):\n\t\t\t\tif line.startswith(config.BZOPEN_COMPOUND_BUG_PARAM) and '_ID' not in line and '_TICKET' not in line:\n\t\t\t\t\tbug_id = int(line.split('\"')[1])\n\t\t\t\t\tif bug_id in bug_ids:\n\t\t\t\t\t\tself.log.debug('Amend _ID"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bug_ids:\n\t\tif bug_id in params:\n\"\"\"\n\t\t\tprint params[bug_id]\n\t\t\tif 'title' in params[bug_id]:\n\t\t\t\tprint \"Bug id: \", bug_id\n\t\t\t\tprint \"Title: \", params[bug_id]['title']\n\t\t\t\tif 'priority' not in params[bug_id]:\n\t\t\t\t\tprint \"Priority set to None\"\n\t\t\t\t\tparams[bug_id]['priority'] = None\n\t\t\t\telse:\n\t\t\t\t\tprint \"Prioritiy is not None\"\n\t\t\t\tprint \"Description: \"+"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bug_ids:\n\n\t\t# We need to do the params in reverse\n\t\t# It's stupid, but I'm too lazy\n\t\torig_params = self.params[bug_id][-1][-1].copy()\n\t\torig_params['notify_email'] = ''\n\t\torig_params['attachments'] = ''\n\t\t\n\t\tself.params[bug_id][-1][-1] = orig_params[:]\n\t\tself.params[bug_id].append([self.name, self.type, self.priority])\n\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor idx in bug_ids:\n\t\tprint \"Amend ID %d\" % idx\n\t\targs = [self.get_bz_cmd(), \"changeset\", \"amend\", params[\"id\"], params[\"title\"]]\n\t\tprint \" \".join(args) + \"\\n\"\n\t\trc = easy_call(args)\n\n\t\t#print \"amend\", str(rc.status)\n\t\tif rc.status == 0:\n\t\t\treturn True\n\n\t\tif rc.status == 4:\n\t\t\tprint \"ID %d already exists\\n\" % idx\n\t\t\t"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 22.16 GiB total capacity; 20.53 GiB already allocated; 15.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(__doc__)\n\tparser.add_argument(\"target_d\", type = int, help = \"target dice\")\n\tparser.add_argument(\"start_d\", type = int, help = \"start dice\")\n\tparser.add_argument(\"-n\", \"--name\", help=\"target and start character\")\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", dest=\"quiet\", help=\"suppress log output.\")\n\tparsed_args = parser.parse_args()\n\treturn (parsed"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(__doc__)\n\n\tparser.add_argument('--start-end', nargs='+', type=int,\n\t\t\t\t\t\thelp='Start and end timestep from which to count and which to go until. Ex '\n\t\t\t\t\t\t\t '<start_timestep>-<end_timestep>.')\n\tparser.add_argument('--time-delta', type=int,\n\t\t\t\t\t\thelp='Time delta between each timestep in which to count.')\n\tparser.add_argument('--timescale', type=int,"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\targuments.insert(1, parser)\n\tfor arg, type in zip(arguments, Argument):\n\t\tif arg =='self' or not arg:\n\t\t\tcontinue\n\t\tif 'args' in type.kwargs:\n\t\t\traise TypeError(\"Unable to accept arguments for method 'parse_arguments'\")\n\t\tdefault = type.kwargs.pop('default', None)\n\t\tif default is None:\n\t\t\targ_help = None\n\t\telse:\n\t\t\targ_help = type.kwargs.pop('help', default.__"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\t# Default option\n\tdefault = arguments[0]\n\tdefault = options.parser.resolve_default(default)\n\n\tp = parser.add_argument_group('General Options')\n\t\"\"\"\n\tOptions specified in this section will be available through the options instance returned by this\n\tfunction, regardless of which parser was used to invoke this script.  For this reason, you must\n\timport this script first in every script that calls this function.\n\t\"\"\"\n\tp.add_argument('--version', action='version', version='"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tp = ArgumentParser(prog=basename(arguments[0]))\n\n\tp.add_argument(\n\t\t'--project',\n\t\t'-p',\n\t\ttype=str,\n\t\tdefault=\"projects\",\n\t\thelp=\"A location to find the project to be build in ('projects') or'site' (default; relative to '.local_modules')\",\n\t)\n\n\tp.add_argument(\n\t\t'--python',\n\t\t'-p',\n\t\ttype=choice,\n\t\tchoices=['python2', 'python3'],\n\t\thelp=\"The"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\"python3 -m cidr_to_netmask\")\n\n\tparser.add_argument(\n\t\t\"--ip-address\",\n\t\t\"-i\",\n\t\tdest=\"ip_address\",\n\t\tmetavar=\"IP ADDRESS\",\n\t\trequired=True,\n\t\thelp=\"IP address of CIDR address that needs to be converted.\"\n\n\t\t)\n\n\treturn parser.parse_args(*arguments)"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Train a CNN\")\n\tparser.add_argument(\n\t\t\"mode\", choices=(\"train\", \"test\"),\n\t\thelp=\"Specify mode of use\"\n\t)\n\tparser.add_argument(\"input_data\",\n\t\thelp=\"Path to a gzipped csv with the training data. \"\n\t)\n\tparser.add_argument(\"valid_data\",\n\t\thelp=\"Path to a csv with the training or validation data. \"\n\t)\n\tparser.add_argument(\"label_header\",\n\t\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--data\", help=\"which dataset to evaluate on\", default=\"valid\")\n\tparser.add_argument(\"--checkpoint\", help=\"which checkpoint in a trained model to use?\", default=None)\n\tparser.add_argument(\"--batch-size\", type=int, help=\"batch size during evaluation\")\n\tparser.add_argument(\"--dataset-path\", default=\"data/\", help=\"path to the dataset\")\n\tparser.add_argument(\"--use-cuda\", type=int,"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Process some integers.\")\n\tparser.add_argument(\"-u\", \"--user\", type=str, default=\"root\", help=\"The name of the user to run a command with.\")\n\tparser.add_argument(\"-e\", \"--environment\", type=str, help=\"The environment variables to pass to the command.\")\n\tparser.add_argument(\"command\", type=str, nargs=\"*\", help=\"The command to run (and options).\")\n\n\treturn parser.parse_args(*arguments)\n\n\ndef parse_"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom collections import namedtuple\n\tArgument = namedtuple(\"Argument\", (\n\t\t\"name\",\n\t\t\"nargs\",\n\t\t\"long\",\n\t\t\"short\",\n\t\t\"default\",\n\t\t\"help\"\n\t))\n\n\tparser = ArgumentParser()\n\tparser.usage = '%(prog)s --help\\n%(prog)s --version '\n\tparser.add_argument(\"--version\", action=\"store_true\")\n\tparser.add_argument(\n\t\t\"--dynamic\",\n\t\taction=\"store_true\",\n\t\t"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_flags = []\n\tfor arg in parser._optionals._option_string_actions:\n\t\t# ignore '--help all' and '-h all' short options too (as in parser.parse_args())\n\t\tparser_flags.append( arg[2:] )\n\treturn''.join(parser_flags)\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tassert isinstance(parser, argparse.ArgumentParser)\n\n\targs = parser.parse_args()\n\n\tflags = []\n\tfor name, value in inspect.getmembers(args):\n\t\tif isinstance(value, bool):\n\t\t\tif value:\n\t\t\t\tflags.append(name)\n\t\telif not hasattr(value, '__len__'):\n\t\t\tif value:\n\t\t\t\tflags.append('--%s' % name)\n\t\telif isinstance(value, (list, tuple)):\n\t\t\tif value:\n\t\t\t\tflags.append('--%s"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = parser.parse_args()\n\treturn''.join([a for a in args])\n\ndef parse_json(data):\n'''\n\tGiven a JSON dictionary, the data, return a dictionary of attributes (keys as strings,\n\tvalues are strings) where the key matches the JSON object key.\n\n\tArgs:\n\t\tdata [JSON object (string - as returned by json.dumps to a Python dict)]:\n\n\tReturns:\n\t\tdata_dict: dictionary with keys matching those of the JSON object, and\n\t\t\t\tvalues matching the string values of the keys in the JSON"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tdef format_flags(flags):\n\t\ts = \"\"\n\t\tif flags:\n\t\t\tif type(flags) == dict: flags = sorted(flags.items())\n\t\t\tif type(flags) == list: flags = sorted(flags)\n\t\t\tfor key, value in flags:\n\t\t\t\ts += f\"{key}={value}\"\n\t\treturn s\n\n\tdesc = parser.description if parser.description else parser.description\n\tdesc_lines = desc.split(\"\\n\")\n\tdesc = (desc_lines[0]\n\t\t+ \"\\n\"\n\t)\n\tif len"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.print_help()\nif __name__ == \"__main__\":\n\t\"\"\"\n\tExample:\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--x\", dest=\"x\", type=str)\n\tparser.add_argument(\"--y\", dest=\"y\", required = True, type=int)\n\n\targs = parser.parse_args()\n\tif args.x:\n\t\tparser_flags(parser)\n\t\tresult = parser.parse_args(args.x.split())\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = parser.parse_args()\n\treturn \" \".join(str(flag) for flag in vars(args) if isinstance(flag, int))\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\n\tflag_string = \"\"\n\tfor i in range(len(sys.argv[1:])):\n\t\tif parser.__getattribute__(sys.argv[1:][i],\"action\")=='store_true':\n\t\t\tflag_string += sys.argv[i]+\"\\n\"\n\treturn flag_string\n\n\"\"\"\n\tGiven an args object, run the experiment with the specified values of these args.\n\tArgs:\n\t\targs -- the namespace of arguments.\n\t\"\"\"\n\n#\tThis is a very simple version of argparse.Arg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\ttext = parser.format_usage()\n\treturn text.split()"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor argument, values in vars(parser.parse_args()).items():  # pylint: disable=no-member\n\t\tif values or values == 0:\n\t\t\tflags.append(\"--\" + argument.replace(\"_\", \"-\"))\n\t\t\tif values:\n\t\t\t\tflags.append(str(values))\n\n\treturn \"\\n\".join(flags)\n\ndef command_function(name):\n\t\"\"\"\n\tGiven a command name, return its function name.\n\n\tIf no function exists, raise an ImportError.\n\t\"\"\"\n\n\treturn _"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tf = \"\"\n\tfor arg in parser.__dir__():\n\t\tif arg.find(\"_\")!= -1:\n\t\t\tf+=  arg[0]   + \" \"\n\treturn f.strip()"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"A CLI tool to configure and update local database files\")\n\tsubparsers = parser.addsubparsers(help=\"commands\")\n\n\t#\n\t# parse the command line arguments for the user\n\t#\n\tconfigparser = subparsers.add_parser(\"config\", help=\"Read and save configuration information\")\n\tconfigparser.add_argument(\"--output-path\", type=str, default=\"{}/cron-config.json\".format(Path.home()))\n\tconfigparser.add_argument(\"--verbose\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = _get_parser()\n\tparsed_arguments = vars(parser.parse_args(unparsed_arguments))\n\n\t# make aliases explicit\n\tfor k, v in parsed_arguments.items():\n\t\tparsed_arguments[k] = v\n\n\tparsed_arguments.update(_get_plugin_args(vars(parser._actions)))\n\treturn parsed_arguments\n\ndef validate_arguments(*parsed_arguments):\n\t\"\"\"\n\tGiven dict mapping from subparser name to a Namespace"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog='parse_arguments')\n\tsubparsers = parser.add_subparsers(title='subcommands')\n\tfor cls in reversed(classes_to_add):\n\t\tparser_cmd = subparsers.add_parser(cls.name)\n\t\tparsers = cls()._parse_arguments(parser_cmd)\n\t\tfor k, v in parsers.items():\n\t\t\tsetattr(v, '_is_enabled', getattr(cls, '_is_enabled', k in parser._"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Create the necessary data for running the QA pipeline\")\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\t# Create parsers for each subparser.\n\tcreate_pipeline_parser(subparsers)\n\tcreate_test_parser(subparsers)\n\tcreate_compare_parser(subparsers)\n\tcreate_train_parser(subparsers)\n\tcreate_run_parser(subparsers)\n\n\t# Par"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A tool to create word vectors\")\n\tsubparsers = parser.add_subparsers(title=\"sub-commands\", description=\"valid sub-commands\",\n\t\t               help=\"sub-command description\")\n\n\tfor parser in create_subparser_parser(subparsers):\n\t\tparser.add_argument(\"cmd_arguments\", nargs=argparse.REMAINDER)\n\n\treturn parser.parse_args(*unparsed_arguments)\n#end def\n\ndef create_subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\targument_default = argparse.SUPPRESS,\n\t\tdescription = \"Utility for testing other modules\"\n\t)\n\tparser._optionals = parser\n\tsubparsers = parser.add_subparsers(title = \"subcommands\",\n\tdest = \"command\")\n\tfor module_name in (x for x in globals() if x.endswith(\"__main__\")):\n\t\tparser = subparsers.add_parser(module_name,\n\t# \ttitle = module_name"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tpossible_arguments = (\n\t\t(\"arguments\", parse_arguments_from_command_line),\n\t\t(\"--config\", parse_arguments_from_config_file),\n\t\t(\"--profile\", parse_arguments_from_profile_file),\n\t)\n\tnested = map(\n\t\tlambda x: x[1](x[0](unparsed_arguments)),\n\t\tenumerate(\n\t\t\t(parse_arguments_from_command_line,) + possible_arguments +\n\t\t\t(parse_arguments_from_config_file"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif len(unparsed_arguments) == 0:\n\t\tprint(\"ERROR: no args supplied.\", file=stderr)\n\t\tsys.exit(1)\n\n\tparser = argparse.ArgumentParser(allow_abbrevs=False, add_help=False)\n\tparser.add_argument('-v', '--verbose', action='count', help='Be verbose; repeat to increase log verbosity', default=0)\n\n\tdebug = False\n\tparsed_arguments = {}\n\n\tfor f in argv[1:]:\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif not unparsed_arguments:\n\t\treturn {}\n\n\tfrom argparse import ArgumentParser\n\targument_parser = ArgumentParser()\n\n\t# Make the unparsed arguments \"global\" so that --help can print them in a single line:\n\targument_parser.unparsed_arguments = unparsed_arguments\n\n\tglobal subparsers\n\n\tsubparsers = argument_parser.add_subparsers(help=argsparser.SUPPRESS)\n\n\tglobal args_parser, _parser_help\n\n\targs_parser = subparsers."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tdef convert_value(arg_value):\n\t\tif isinstance(arg_value, argparse.Namespace):\n\t\t\treturn arg_value\n\t\telif arg_type_hint is not None and isinstance(arg_type_hint, type):\n\t\t\treturn arg_type_hint(arg_value)\n\t\telse:\n\t\t\treturn arg_value\n\tdef convert_values(arg_values):\n\t\targ_values = tuple(\n\t\t\tmap(convert_value, arg_value)\n\t\t\tfor arg_value in arg_values\n\t\t)\n\t\t"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targs, remaining_arguments = None, []\n\tfor unparsed_arg in _unparsed_arguments:\n\t\tif subparser_matches_argument(unparsed_arg, unparsed_arguments):\n\t\t\targs, remaining_arguments = subparser_parser(unparsed_arg).parse_args(\n\t\t\t\tremaining_arguments,\n\t\t\t), []\n\n\treturn {args.name: args}, remaining_arguments\n\n\ndef add_subparser_options(subparsers, unparsed_arguments):\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfor name, subparser in subparsers.items():\n\t\targs, unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\tnamespace = {}\n\t\tunknowns = []\n\t\tfor key, value in vars(args).items():\n\t\t\tif getattr(value, \"action\", None) == \"store\":\n\t\t\t\tnamespace[key] = value.__get__(args)\n\t\t\telse:\n\t\t\t\tunknowns.append(key)\n\t\tyield namespace, unknowns\n\n\nclass SubparserAction(arg"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tremaining_arguments = get_remaining_arguments(unparsed_arguments)\n\tfor subparser_name, subparser_argparser in subparsers.items():\n\t\tavailable_arguments = arguments_for(\n\t\t\tsubparser_name,\n\t\t\tparse_arguments(subparser_name, unparsed_arguments, remaining_arguments)\n\t\t)\n\t\tnew_namespace, remaining_arguments = parse_subparser_arguments(\n\t\t\tremaining_arguments, available_arguments\n\t\t)\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\ttry:\n\t\tsubparser = subparsers[type(unparsed_arguments[0])](add=\"add\", remove=\"remove\")\n\t\tparsed_arguments = parser(subparser, unparsed_arguments, {})\n\t\tremaining_arguments = unparsed_arguments[len(parsed_arguments):]\n\texcept SystemExit:\n\t\tpass\n\t\n\treturn parsed_arguments, remaining_arguments\n\t\n\t\nclass RepositoryException(Exception):\n\tpass\n\t\nclass DuplicateRepo"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targs = {}\n\tremaining_args = []\n\tfor name, subparser in subparsers.items():\n\t\tif (subparser and len(unparsed_arguments)) and (len(unparsed_arguments) == 1):\n\t\t\targs[name] = subparser.parse_args(unparsed_arguments[0])\n\t\telse:\n\t\t\targs[name] = None\n\t\t# args[name] = subparser.parse_args(unparsed_arguments)\n\t\t# args[name] is populated at least one"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {subparser: subparsers[subparser].parse_args(unparsed_arguments)\n\t\tfor subparser in subparsers}\n\tremaining_arguments = unparsed_arguments \\\n\t\tif any(unparsed_arguments) else []\n\treturn result, remaining_arguments\n\ndef create_repository(subparser, unparsed_arguments, namespace):\n\t\"\"\"\n\tThe repository subparser can be configured with a --dry-run setting. The dry run\n\tflag is handled as such by ignoring"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_actions = {\n\t'show', 'info', 'edit', 'import','version', 'delete', 'pull'}\n\tparsed_arguments = {\n\tsubparser_name: subparser\n\tfor subparser_name, subparser in subparsers.items()\n\tif set(subparser.prog.split('-')).issuperset(all_actions)}\n\n\tremaining_arguments = [\n\targument for argument in unparsed_arguments if argument.startswith('-')]\n\treturn parsed_arguments"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers = list(subparsers)\n\t\n\t# Initialize the subcommand and subparsers\n\thelp_subparsers = {}\n\tfor subparser in subparsers:\n\t\tassert subparser.choices is not None\n\t\tsubcommand_name = subparser.prog\n\t\tsubparsers.remove(subparser)\n\t\tsubparser = subparsers.pop(0)\n\t\tsubparser.choices = {}\n\t\thelp_subparsers[subcommand_name] = subparser\n\n\tnamespace, remaining_arg"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif len(unparsed_arguments) == 0: return dict(), []\n\tif isinstance(unparsed_arguments[0], list):\n\t\targuments = unparsed_arguments[0]\n\t\tunparsed_arguments = unparsed_arguments[2:]\n\telse:\n\t\targuments = unparsed_arguments\n\tparsed_arguments = dict()\n\tsubparser_actions = subparsers.choices\n\tfor subparser_name, subparser_option_strings in sorted(subparser_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tavailable_arguments = {}\n\tunavailable_arguments = []\n\tfor subparser in sorted(list(subparsers.keys()), key=_get_key_for_subparser):\n\t\targuments, unused_arguments = subparsers[subparser].parse_known_args(unparsed_arguments)\n\t\tparsed_arguments = vars(arguments)\n\t\tif isinstance(subparsers[subparser], argparse._SubParsersAction):\n\t\t\tparsed_arguments = add_subparser_"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tversion = 'v1.0.0'\n\tdebug=None\n\tparser = argparse.ArgumentParser(prog='PyP2P', usage=\"%(prog)s [OPTION]...\",add_help=True)\n\t\n\tparser.add_argument('-c','--connection_ip',dest=\"connection_ip\",help='connection ip address of host where this peer is placed',default='localhost')\n\tparser.add_argument('-p','--port',dest=\"port\",help='connection port', type=int,default=None)\n\t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\tparser, subparsers = make_subparsers()\n\tadd_server_options(parser)\n\tadd_sender_options(parser)\n\tadd_file_options(parser)\n\tadd_message_options(parser)\n\tadd_auth_options(subparsers)\n\treturn parser,subparsers\n\ndef make_subparsers():\n\"\"\"\n\tBuild a top-level'send' command's arguments and return the parsed arguments.\n\t\"\"\"\n\tsubparsers = argparse.ArgumentParser(add_"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tcommon = ArgumentCommonParsers(name=\"common\")\n\tcommon.add_argument(name=\"common-arg\",type=argparse.FileType(\"r\"),help=\"A file with the common arguments\")\n\tcommon.add_argument(name=\"common-arg-str\",type=str,help=\"A string argument\")\n\n\tparser = ArgumentParser(name=\"parser\",fromfile_prefix_chars=\"@\")\n\tparser.add_argument(\"-v\",\"--verbose\",help=\"Say something, even if quiet mode\",action=\"store_true\")\n\tparser.add"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(add_help=False)\n\t\n\tsubparsers = parser.add_subparsers(title='subcommands', dest='sub_command')\n\t\n\treturn parser, subparsers\n\ndef get_parser_and_subparsers(parsers=None):\n\t\"\"\"\n\tParses the arguments of the CLI and returns a top-level parser, \n\ta list of command sub-parsers, and some flags to be passed to the sub-command.\n\t\n\tArgs"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport os  # TODO\n\timport configparser\n\tfrom. import arg_parse\n\t# TODO: should buildin the standard options as config.ini as well - but config.ini should have default values\n\t# to avoid user configs being changed (I guess there should be a config.ini in the project root but this would mean changing the default values of the other project's config.ini)\n\t# NOTE: this may introduce problems for the project's own config.ini, but if only the project's itself use the buildin config.ini that should work fine\n\tparser = arg"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = p.add_subparsers()\n\n\tsub1_parser = subparsers.add_parser(\"runTest\",description=\"Run a Test Function\")\n\tsub1_add_arguments(sub1_parser)\n\n\tsub2_parser = subparsers.add_parser(\"runAllTests\",description=\"Run Tests\")\n\tsub2_add_arguments(sub2_parser)\n\n\tsub3_parser = subparsers.add_parser(\"getOutput"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"Commands\", dest=\"command\")\n\n\t# Parsers for common commands.\n\tfor cmd in (\"add\", \"cleanup\", \"config\", \"dumpconfig\", \"help\", \"listdir\",\n\t\t\t\"remove\", \"set\", \"test\"):\n\t\tparser_cmd = subparsers.add_parser(cmd, help=command_map[cmd][0])\n\t\tcommand_map[cmd].append(parser_cmd)\n\t\tadd_options(parser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tsubparsers.dest = 'command'\n\n\t# Add commands here as they are implemented.\n\t# The following function is called if no subcommand is provided.\n\tdef add_command(parser, name, parser1):\n\t\tsubp = subparsers.add_parser(name, help=parser1.__doc__)\n\t\tparser1.parse_args(subp, namespace)\n\n\tadd_command("}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"h2p\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\n\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\tp = make_h2p_parser()\n\tsubparsers.add_parser(\"h2p\").set_defaults(func=lambda args: p.parse_args(args or [\"--help\"]))\n\n\treturn p, p.add_subparsers()"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Hole-filling and clustering of cell images')\n\tsubparsers = parser.add_subparsers(help='Sub-command help',\n\t\t\t\t\t\t\t\ttitle='Actions')\n\tparser.set_defaults(parser=parser)\n\tparser.set_defaults(subparsers=subparsers)\n\t\n\treturn parser, subparsers\n"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 22.16 GiB total capacity; 20.60 GiB already allocated; 15.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targs = argparse.ArgumentParser(description=__doc__)\n\n\t# Arguments from command-line, not passed here\n\targs.add_argument('command')\n\targs.add_argument('action')\n\targs.add_argument('--target')\n\targs.add_argument('--username')\n\targs.add_argument('--password')\n\targs.add_argument('--server')\n\n\t# Options that get passed from command-line, so we don't pass them.\n\targs.add_argument('--help',\n\t                   action='help"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tfor argument in arguments:\n\t\tif isinstance(argument, str):\n\t\t\tparser.add_argument(argument)\n\t\telif argument in vars(PathArgumentParser):\n\t\t\tparser.add_argument(\n\t\t\t\tstr(*argument.values()),\n\t\t\t\tmetavar=str(*argument.keys()),\n\t\t\t\tdefault=None,\n\t\t\t\tnargs=argument,\n\t\t\t\tdefault=frozenset()\n\t\t\t)\n\t\telif argument in vars(PathArgumentParserFlags):\n\t\t\tparser.add_argument("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\"%(prog)s \" + __doc__)\n\t# Add mutually exclusive arguments to the parser:\n\tgroup = parser.add_mutually_exclusive_group(required=True)\n\tgroup.add_argument(\"--in_csv\", type=str, help=\"The input CSV file\", action=\"store\")\n\tgroup.add_argument(\"--in_pdf\", type=str, help=\"The input PDF file\", action=\"store\")\n\tgroup.add_argument(\"--no_merging\", default=False, help=\"Do not merge PDFs"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Print a log in a readable format\")\n\tparser.add_argument(\"-V\", \"--verbose\", action=\"count\", dest=\"verbosity\", default=0, help=\"Print more information (use twice for more debug information)\")\n\treturn parser.parse_args(*arguments)\n\nparser = parse_arguments(sys.argv[1:])\n\n\n# import required libraries\nimport xml.etree.ElementTree as xmltree\nimport datetime\nfrom decimal import Decimal\n\ndate = datetime."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\",\n\t\tformatter_class=lambda prog: argparse.HelpFormatter(prog,max_help_position=30)\n\t)\n\tparser.add_argument(\"inputs\", type=str, nargs=\"*\", help=\"The input files\")\n\tparser.add_argument(\"outputs\", type=str,nargs=\"*\",help=\"The output file\")\n\tparser.add_argument(\"--inputs\",type=str,nargs=\"+\",default=None,help=\"The input files\")"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tif len(arguments) > 1:\n\t    raise ParseError(\"Command-line interface does not take more than one argument ({} given)\".format(len(arguments)))\n\n\tparser = ArgParser(prog=os.path.basename(sys.argv[0]))\n\tparser.add_argument(\"input\",\n\t\ttype=input_file_type,\n\t\thelp=\"Location of file with input data.\")\n\n\targs = parser.parse_args(arguments)\n\tif not args.input:\n\t\tparser.error(\"Input file not specified.\")"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Finds a single version of a package or distribution.\")\n\tparser.add_argument(\n\t\tdist_arg,\n\t\tnargs=\"?\",\n\t\ttype=DistributionType,\n\t\tdefault=\"all\",\n\t\thelp=\"The name of a distribution or a package, which version to use.\"\n\t)\n\tparser.add_argument(\n\t\t\"--path\",\n\t\ttype=Path,\n\t\thelp=\"The path to the source code, which may be used to find a distribution.\"\n\t)\n\tparser.add_argument("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(*arguments)\n\n#\tTODO: Add a help message to describe when this script was invoked directly on the command line. This\n#\tmessage will be displayed in the console.\n\tparser.add_argument(\"first\", help=\"The first number\", type=int)\n\tparser.add_argument(\"second\", help=\"The second number\", type=int)\n\treturn parser.parse_args()\n\ndef sum_them(*args):\n\"\"\"\n\tSum the given numeric values.\n\t\"\"\"\n\tresult = 0\n\tfor"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Perform a simple action on the database.')\n\tparser.add_argument('--database', default='app.db', help='Name of db file to use for database.')\n\tparser.add_argument('--create', action='store_true', help='Create table in given db file.')\n\tparser.add_argument('--clean', action='store_true', help='Delete any database.db.')\n\tparser.add_argument('--drop', action='store_true', help='Drop table in given database.'"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\treturn ap_.parse_args(*arguments)\n\ndef assert_greater_equal(lhs, rhs):\n\"\"\"\n\tAsserts that the first argument is greather or equal to the second\n\t\"\"\"\n\tassert lhs >= rhs\n\ndef assert_in(element, iterable):\n\"\"\"\n\tAsserts that element is in iterable\n\t\"\"\"\n\tassert element in iterable\n"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \"\".join(f\"{k} {args[k] if k in args else '-'} \" for k in parser._actions)\n\ndef main(flags):\n\t# Parse system arguments\n\tparser = argparse.ArgumentParser(description=\"\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"-b\", \"--build\",  metavar=\"build-id\", required=True, type=int,\n\t\t\t\thelp=\"The build id for the build to publish\")\n\tparser.add_argument(\""}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflag_string = str(sorted(parser.parse_args()))\n\treturn '-'.join(flag_string.split())\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.format_help()\n\treturn (\n\t\tflags.replace (\"\\n\", \" \").replace (\"  \", \" \")\n\t)\n\n##############################################################################################\n# Main script for generating the command line for the \"rnnlm.py\" and\n# \"rnnlm_apply_bpe.py\" scripts\n##############################################################################################\n\nif __name__ == \"__main__\":\n\n\tos.chdir(os.path.dirname(os.path.abspath(sys.argv[0])))\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targuments = parser.parse_args()\n\tparser_args = \"\"\n\tfor argument in arguments:\n\t\tvalue = arguments[argument].get()\n\n\t\tif isinstance(argument, str):\n\t\t\tparser_args = parser_args + \" \"+ argument\n\t\t\tparser_args = parser_args + \" {}\".format(value)\n\t\telse:\n\t\t\tparser_args += argument.__name__ + \" \" + value\n\n\treturn parser_args + \" \""}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tif parser.description:\n\t\tflags += \" \" + parser.description\n\tfor arg in parser._optionals._seq:\n\t\tif arg.required:\n\t\t\tflags += \" --\" + arg.dest\n\t\telse:\n\t\t\tflags += \" -\" + arg.dest\n\t\t\tif arg.choices:\n\t\t\t\tflags += \" -\" + \",\".join(arg.choices)\n\treturn flags\n\n\ndef parse_args(parser, argv):\n\t\"\"\"\tGiven an argparse.ArgumentParser instance, parse the args and return the\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tf = ArgumentFlags(parser)\n\treturn f.args_for_flags(all=True, args=True)\n\n\ndef get_parser():\n\"\"\"\n\tGet the default argparse.ArgumentParser instance for this module.\n\t\"\"\"\n\tfrom dol import dol\n\treturn dol.argp\n\n\ndef parse_args(*args, **kwargs):\n\"\"\"\n\tGiven a command-line argument string, return an argparse.Namespace instance.\n\t\"\"\"\n\tparser = get_parser()\n\treturn parser.parse_args(*args, **kw"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targuments = []\n\tif parser.description:\n\t\targuments.append(\"\\n\\n\".join(textwrap.wrap(parser.description, width=(70 - 10))))\n\n\tif len(parser.positionals) == 0 or not getattr(parser, 'nostrip', False):\n\t\targuments.append(\", \".join((_f for _f in parser.argument_strings if _f and _f!= \"-\")))\n\n\tif parser.parents:\n\t\tparsers = set(parser._name_parser_map.values())\n\t\tparsers"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\topt = parser.parse_args()\n\n\tflags = []\n\tfor flag in opt._actions:\n\t\topts = flag._long_opts[1:]\n\t\tnames = re.findall(\"--(.)+\", opt._arg_string)\n\t\tflags += opts if True else names\n\tflags.pop(len(flags) - 1)\n\treturn \" \".join(flags)\n\ndef parse_json(json_str):\n\treturn json.loads(json_str) if json_str else None\n\ndef get_json(json_url):\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = \" \".join(parser.format_help().split())\n\treturn args\n\n\ndef print_help():\n\tsys.stderr.write(parser.format_help())\n\texit()\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(parser.argv)\n"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t\n\ttext_comp = \"\"\"\n# bash completion support for the borgmatic CLI\n# by Jan Czenasinski\n\nCOMP_WORDS={COMP_WORDS}\nCOMP_CWORD={COMP_CWORD}\n_CONFIG_ARGS=${COMP_WORDS[1][_REPLACEMENT_WORDS]}\n_CONFIG_ARG=${COMP_WORDS[COMP_CWORD][_REPLACEMENT_WORDS]}\n\nif [ \\$# == 2 && command == COMP_WORDS[$COM"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\targs = read_bash_completion()\n\treturn borgmatic_complete.get_script(args.command, args.verbose)\n\nif __name__ == \"__main__\":\n\tprint(bash_completion())"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\timport optparse\n\n\tdef make_argparse():\n\t\tparser = optparse.OptionParser()\n\t\tfor k,v in list(args_map.items()):\n\t\t\toptname, argmode, type, help_ = v\n\t\t\tif argmode == 'optional':\n\t\t\t\topt = optparse.make_option(k, '--' + k, action='store')\n\t\t\telif argmode =='required':\n\t\t\t\topt = optparse.make_option(k, '--' + k, action='store', required=True)\n\t\t\telse:"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\toutput = \"\"\n\tfor (command, command_parser) in (command_parsers[command_module].items()\n\t                                 for command_module in command_parsers.keys()):\n\t\tcommand_name = \"export BORGMUTHIC_\" + command.upper()\n\t\tcomplete_args = command_name + \"_ARGS=\" + repr(command_parser.complete)\n\t\toutput += \"COMP_OPT=\\\"$COMP_OPT=\\\"\"\n\t\toutput += command_name + \" (\"\n\t\toutput += command + \")\"\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tcmdargs = [arg for arg, action in click.get_current_context().command.params.items() if action.nargs!= 0]\n\n\treturn (click.style(COMMAND_ARGS_BASH_COMPLETER_HEADER % \", \".join(sorted(cmdargs)), bold=True) +\n\t\tCOMMAND_ARGS_BASH_COMPLETER_DELIMITER.join(\n\t\t\"{}@{}\".format(arg, click.get_current_context().get_help_option(arg))\n\t\tfor arg in"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\toutput = [\"# Generated by borgmatic\",\n\tf\"_PARSER_HELP='\\\"{' '.join(COMPLETION_ARGUMENTS)}'\",\n\tf\"_COMPLETE='$(_PARSER_HELP {' '}--bash-completion -o _ARGCOMPLETE_FILENAME) $1'\",\n\t\"_ARGCOMPLETE_FILENAME='$(COMP_WORDS=( $( compgen -W \"$( _PARSER_HELP --help |cut -d ':' -f 2 ) )"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\targs = None # type: Arguments\n\tparser = argparse.ArgumentParser(description='Borgmatic CLI Tools')\n\tsubcommands = [] # type: List[argparse.Action]\n\t# subcommand for custom commands that do stuff\n\tsubcommands.append(parser.add_argument_group('subcommand group', description=\"subcommand group description\"))\n\tsubcommands.append(parser.add_argument('-h', '--help', help='show this help message and exit', action='help'))\n\tgroup = parser."}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\twith open(path.join(path.dirname(__file__), 'cli', 'completion.bash'), 'r') as f:\n\t\tscript = f.read()\n\tcompletion_parsers = {}\n\tfor k, parser in get_sub_commands().items():\n\t\tcompletion_parsers[k] = parser.completion_argument()\n\tscript, _ = inline_completion_parsers(parser, {}, script, completion_parsers)[1:]\n\treturn script\n\ndef main(argv=None):\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparsers = argparse.ArgumentParser.parse_args\n\timport inspect, re, attr # noqa\n\tfrom typing import Optional, Iterable, Union, List\n\tfrom enum import Enum\n\timport types\n\tif parsers.__func__ is argparse.ArgumentParser.parse_args or ins := ''.join(inspect.stack()):\n\t\tinspect.stack()[0][2]\n\tparsers, _, c_base = str(parsers[0].__class__).partition('.')\n\tif c"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport sys\n\tfrom textwrap import dedent\n\timport argparse\n\n\tdef get_choices(choices, args):\n\t\t\"\"\"\n\t\tArgs:\n\t\t-   choices is a list of strings\n\t\t-   args is a parsed arguments object, or None\n\t\t\"\"\"\n\t\tchoices = choices if args is None else args.dest\n\t\tchoices = \", \".join(choices) if isinstance(choices, (list,tuple)) else choices\n\t\tchoices = (\"--args=\" + choices, ) if choices and args is None else choices\n\t\treturn \","}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tresult = {}\n\tfor i in range(0, len(unparsed_arguments), 2):\n\t\tif len(unparsed_arguments) > i + 1:\n\t\t\tif not hasattr(unparsed_arguments[i + 1], \"__len__\"):\n\t\t\t\tparser = ArgumentParser()\n\t\t\t\tparser.add_argument(unparsed_arguments[i] or sys.argv[0], type=str)\n\t\t\t\tresult[unparsed_arguments[i]] = parser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\ttry:\n\t\timport argparse\n\texcept ImportError:\n\t\twarnings.warn(\"Cannot import argparse. Not able to import it to parse arguments as they were not created from calling script.\")\n\t\tsys.exit(1)\n\n\tparser = argparse.ArgumentParser(\n\t\tprog=sys.argv[0],\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\n\tsubparsers = parser.add_subparsers(title=\"command\", dest=\"command\")\n\n\tfor"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = get_argument_parser()\n\treturn {name: parser(args) for name, args in parse_pairs(unparsed_arguments)}\n\"\"\"\nThe parse_pairs function was pulled from\nhttps://github.com/clementheraud/clement-argparse/blob/master/scripts/argparse_completer.py.\n\"\"\""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = _parser(create_subparser)\n\treturn _parse_args_to_dict(parser, unparsed_arguments)\n\n\ndef main(arguments=None):\n\t\"\"\"\n\tMain function for calling this script directly.\n\t\n\tArgs:\n\t\targuments -- Optional argument list. May be used for command-line prompts and defaults.\n\t\n\tReturns:\n\t\t0 on success, nonzero on failure\n\t\"\"\"\n\tglobal G_ARGS, TTY_WIDTH\n\ttry:\n\t\targs = parse_arguments(arguments"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tscript_args = {\n\t\t'f': 'conda/envs/py26-python3.6/tests/python-script/setup.py',\n\t\t'h': '-h ',\n\t\t'l': 'conda/envs/py26-python3.6/tests/python-script/example.py',\n\t\t'p': '-p',\n\t\t'pr': 'conda/envs/py26-python3.6/tests/python-script/setup.py',\n\t\t'q': '--quiet',\n\t\t'reset': '"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\tunparsed_arguments.insert ( 0, \"--rootpath\" )\n\tusage = argparse.ArgumentParser()._get_action_summary(unparsed_arguments).rstrip()\n\tparser = argparse.ArgumentParser(description=textwrap.dedent(__doc__).lstrip('\\n'), usage=usage)\n\tversion_group = parser.add_mutually_exclusive_group()\n\tversion_group.add_argument( \"--version\", \"-v\",action=\"store_true\",help=\"Print version"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tclass GlobalNamespace:\n\t\tpass\n\n\tglobal_arguments = GlobalNamespace()\n\tglobal_arguments.prog ='subparser2argparse'\n\tglobal_arguments.description = 'This subcommand will call a subcommand\\'s own -h and parse its arguments.'\n\tglobal_arguments.epilog = 'This subcommand may be extended using another subcommand\\'s global arguments and positional arguments.'\n\tglobal_arguments.additional_subcommands = ()\n\tglobal_arguments.subcommands = {'help': parse_help_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tp = argparse.ArgumentParser(description=parser.__doc__, add_help=False)\n\tp.add_argument(\"--help\", \"-h\", action=\"store_true\", help=\"show this help message and exit\")\n\tp.add_argument(\"--yaml-config\", \"-y\", help=\"read configuration from YAML file\")\n\tp.add_argument(\"--yaml-spec\", help=\"read configuration from YAML file\")\n\tp.add_argument(\"--yaml-spec-url\", help=\"URL to YAML specification\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tall_parsers = dict(argparse=get_main_argument_parser(),\n\t                   model=get_model_argument_parser())\n\tif len(unparsed_arguments) < 2:\n\t\tunparsed_arguments = ([sys.argv[0]] + unparsed_arguments)\n\tknown_args, unparsed_args = parse_known_args(unparsed_arguments)\n\tfor parser_name, subparser in all_parsers.items():\n\t\tsubparser, consumed_args = parse"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tresult = {\n\t\tsubparser_name: subparser.parse_args(args)\n\t\tfor subparser_name, subparser in command_subparsers.items()\n\t\tfor args in subparser.form_arguments(*unparsed_arguments)\n\t}\n\treturn result\n\ndef command(subparser):\n\t\"\"\"\n\tDecorator for adding command handler functions to a subparser in\n\t:func:`load_commands`.\n\t\"\"\"\n\tdef deco(func):\n\t\tfor name, subparser in command_subpars"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_parsed_args, remaining_args = make_parsed_subparser_arguments(\n\t\t*unparsed_arguments,\n\t\tsubparser_collection=arguments.DEFAULT_PACKAGES_SUBPARSER_COLLECTION,\n\t)\n\n\tif len(all_parsed_args)!= len(arguments.DEFAULT_PACKAGES_SUBPARSER_COLLECTION):\n\t\traise ConfigurationError(\n\t\t\t\"Got %s results for %s, but expected exactly %d arguments\"\n\t\t\t% (\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targuments = {}\n\tfor sub in subparsers:\n\t\ts = subparsers[sub]\n\t\t# s.description.splitlines(keepends=True)[0] gives us the long name of the subparser.\n\t\ta = s.add_parser(s.description.splitlines(keepends=True)[0], help=s.description,\n\t\t\t\t\t\t**vars(argparse.ArgumentParser.parse_args(s, unparsed_arguments)))\n\t\targuments[sub] = vars(a.parse_args(unparsed_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tdef filter_unparsed_args(unparsed, nargs, nargs_min, nargs_max):\n\t\treturn subparser.parse_args(unparsed, namespace=namespace)\n\t\n\tnamespace = argparse.Namespace()\n\targs_to_add = []\n\t\n\tfor subparser in subparsers:\n\t\tsubparser.add_argument('-t', '--tree',\n\t\t                       help=SUPPRESS,\n\t\t                       action=lambda a: namespace.args,\n\t\t                       metavar='DIR',\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfor sub in subparsers:\n\t\tsub_parser = subparsers[sub]\n\t\ttry:\n\t\t\toptions, remaining_args = sub_parser.parse_known_args(\n\t\t\t\tlist(filter(\n\t\t\t\t\tNone,\n\t\t\t\t\tunparsed_arguments,\n\t\t\t\t)),\n\t\t\t\tnamespace=namespace,\n\t\t\t)\n\t\t\tbreak\n\t\texcept SystemExit:\n\t\t\traise\n\t\texcept UnrecognizedOptionError as e:\n\t\t\tsub_parser.error(e)\n\telse:\n\t\tassert False, \"should never happen\"\n\tif options is None"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Split the input by the first non-option-like arg. We want everything before it.\n\toptions, arguments = parse_arguments(unparsed_arguments)\n\t\n\t# We get back a list of options, so, keep only the first. It is unlikely to be \"subcommand\",\n\t# so this makes sense.\n\tif len(options) > 0:\n\t\toptions = {options[0]: options[1]}\n\t\t\n\t# We can't pass options via unparsed_arguments, so just set the subparser based options here.\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tparsed_arguments = {}\n\tleft_arguments = []\n\t\n\tfor subparser in subparsers:\n\t\tparser = subparsers[subparser]\n\t\tparser_args, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\tparsed_arguments[subparser] = parser_args\n\t\tleft_arguments = unparsed_arguments + left_arguments\n\n\treturn parsed_arguments, left_arguments\n\n\ndef parse_subparser_arguments_default"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresults = {}\n\trepository = repo()\n\textra_args = []\n\n\tfor key, subparser in subparsers.items():\n\t\tns = subparser.parse_args(unparsed_arguments)\n\t\tns_results = subparser.parse_known_args(unparsed_arguments, namespace=ns)\n\t\tresults[key] = ns\n\t\tunparsed_arguments = ns_results[1]\n\t\tif key == \"repository\" or key == \"subcommand\":\n\t\t\textra_args += ns_results"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_lookup[subparser.name].update(subparser.parse_args(unparsed_arguments))\n\n\tsubparser_args = {}\n\tremaining_args = []\n\tfor argument_name, argument in subparser.description.description_of_arguments().items():\n\t\tif any((x in argument_name for x in ('positional', 'global'))):\n\t\t\tcontinue\n\n\t\tsubparser_args.setdefault(subparser.name, {}).update({argument_name: argument})\n\n\t\tif getattr(argument, '_optional_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_args = {}\n\tsubparser_mapping = {}\n\tfor action in subparsers.choices:\n\t\tsubparser_mapping[action] = subparsers.add_parser(action)\n\t\tsubparsers = subparser_mapping[action].add_subparsers(title=action, dest=\"subparser_help\")\n\tif subparsers:\n\n\t\tfor arg in unparsed_arguments:\n\t\t\tsubparser = None\n\t\t\tmatched = False\n\t\t\tfor action in subparser_mapping:\n\t\t\t\tif not subp"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# use passed-in args rather than sys.argv if caller specifies this\n\tif len(unparsed_arguments)!= 0:\n\t    args = argparse.Namespace(**vars(unparsed_arguments[0]))\n\telse:\n\t    args = argparse.Namespace(**vars(argparse.ArgumentParser().parse_args(sys.argv[1:])))\n\"\"\"\n\t\tParse a list of arguments in argument order using an argument definition\n\t\tspecified by the argument_definitions function and any extra definitions\n"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t_parser = argparse.ArgumentParser(prog='pycoco',add_help=False)\n\t_subparsers = _parser.add_subparsers(dest='subcommand',\n\t\ttitle=\"Commands\",metavar='<command>')\n\n\t# make the parser for each dataset\n\tfor _data in _dataset.__all__:\n\t\t_dataset = _data[:-6]\n\t\t_parser = make_dataset_parser(_dataset,_parser)\n\n\treturn _parser,_subpars"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tversion = parser = subparsers.add_parser('version', help='show the version of this command')\n\tsubparsers = version.add_subparsers(help='subcommands', dest='sub_command')\n\n\tparser_config = subparsers.add_parser('config', help='show the config to this module')\n\n\tparser_show = subparsers.add_parser('show', help='show the module data')\n\tparser_show.add_argument('--name', '-n', help='The name of the module.')\n\tparser"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = argparse.ArgumentParser(prog=\"\")\n\tsubparsers = top_parser.add_subparsers(dest=\"command\")\n\n\tsubparsers.required = True\n\n\t# Subcommand options will be displayed in this order.\n\tparser_train = subparsers.add_parser(\"train\")\n\tparser_train.add_argument(\"experiment\", type=str, default=None)\n\tparser_train.add_argument(\"--seed\", \"-s\", type=int, default=0)\n\tparser_train.add"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom docopt import __doc__\n\timport docopt\n\timport functools\n\timport logging\n\timport sys\n\n\tLOG=False\n\n\tclass DocoptDispatcher(argparse.ArgumentParser):\n\t\t@log_parse_error\n\t\tdef __init__(self, name, description=None, **kwargs):\n\t\t\tif isinstance(description, str):\n\t\t\t\t# We don't use the argparse.ArgumentParser's.__init__ method because we don't want to replace the \n\t\t\t\t# description if any, it can be overwritten if the user"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\n\tsubparsers = parser.add_subparsers(title='Available commands',\n\t\t\tdest='command')\n\tsubparsers.required = True\n\n\t# List of commands that include a generator (i.e. return a generator)\n\tCOMMAND_WITH_GENERATED_LIST = ['list-bins']\n\n\tfor command in COMMAND_WITH_GENERATED_LIST:\n\t\tparse_command(command)\n\n\t# List command (i.e. not"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_p = subparsers.add_parser(\n\t\tname=\"get_p\",\n\t\tdescription = \"Get the path to the executable for a specific tool\",\n\t\tepilog = \"Example: m_make -g tool-1.3.12\",\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t)\n\tinputfile_group = parser_p.add_mutually_exclusive_group(required=True)\n\tinputfile_group.add_argument(\n\t\t\t\"-f\", \"--file\",\n\t\t\thelp = \"Path to"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest=\"cmd\")\n\treturn parser,subparsers\n\ndef main():\n\t# Create the top-level parser as the root of the CLI.\n\t# TODO: add arguments for command-line usage.\n\tparser,subparsers = make_parsers()\n\tsub_parsers = [parser]\n\twhile len(sub_parsers) > 0:\n\t\tparser,subparsers = make_parsers()"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser( description=__doc__ )\n\tsubparsers = parser.add_subparsers(help='command-specific description',\n\t\t\t\t\t\t\t\t\tdest='command')\n\tsubparsers.required = True\n\treturn parser, subparsers\n\ndef configure_and_load_dataset(args):\n\t\"\"\"\n\tGiven top-level options, this configures the dataset, and loads it.\n\t\"\"\"\n\tif args.random_seed > 0:\n\t\ttorch.manual_seed(args.random_seed)"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tcmdp = argparse.ArgumentParser(prog=__file__, add_help=False)\n\tcmdp.add_argument('filepath', help=\"The filepath of the JSON file you would like to edit.\")\n\tcmdp.add_argument('--overwrite', help=\"Override any existing values in existing.json\", action=\"store_true\")\n\tsubparser = cmdp.add_subparsers(help=\"What to do with your file.\")\n\tcmdp.set_defaults(func=default_action)\n\tparser_list = {}\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Create the top-level parser\n\ttop_parser = argparse.ArgumentParser(description='Controls the LAMMPS System.', allow_abbrev=False)\n\ttop_parser.add_argument('--version', action='version', version='%(prog)s 1.0')\n\t\n\t# Create the parser for the build-args argument\n\tsubparsers = top_parser.add_subparsers(help='Build Argument Parsers <build-args>')\n\n\t# Create the parser for the build parser, called --configuration"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tstart=0\n\tend=max_results\n\twhile True:\n\t\tpage = getresponse(response, start, end)\n\t\tstart+=max_results\n\t\tyield page\n\t\tif len(page) < max_results:\n\t\t\treturn\n\n\ndef getresult(item):\n\t\"\"\"\n\tGet dict result from API data type\n\n\tArgs:\n\t    item (dict): Object returned by API\n\tReturns:\n\t    dict: dict with object data type is returned in dictionary\n\tNote:\n\t    dict may vary with API data types\n\t\"\"\"\n\ttmp"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tassert max_results > 0\n\tassert type(max_results) == int\n\twhile True:\n\t\tfor i in response:\n\t\t\tyield i\n\t\tif len(response) < max_results:\n\t\t\tbreak\n\t\tresponse = list(response)\n\t\tdel response[0] # Trim last items\n\t\tresponse = list(chain.from_iterable(\n\t\t\t[[response[len(response) - k - 1], response[len(response) - k]] for k in range(1, max_results + 1)]))\n\t\tresponse = [item for item in"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturned_chunk = 0\n\tnumber_of_chunks = len(response)\n\twhile returned_chunk < number_of_chunks:\n\t\tchunk, returned_chunk = get_first_n_pages(response, max_results)\n\t\tyield chunk\n\t\ndef get_first_n_pages(response, max_results):\n\t\"\"\"\n\tReturns first N pages of the WAPI response\n\t\n\tArgs:\n\t    response (list): List with WAPI response.\n\t    max_results (int): Maximum number of objects"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfrom. import Wapi\n\t\n\tif not response:\n\t\treturn response\n\t\t\n\twhile(True):\n\t\ttry:\n\t\t\tfor single_result in Wapi.parse_response(response, key_name='ids', max_results=max_results):\n\t\t\t\tyield single_result\n\t\texcept WapiException as e:\n\t\t\tif str(e).startswith('WAPI '):\n\t\t\t\tif'max_page' in str(e) and 'passed' in str(e):\n\t\t\t\t\tif max_results < int(e['max_page']"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturn (response[x:x+max_results] for x in\n\t\trange(0, len(response), max_results))\n\n###################\n#\n# Get wapid\n#\n###################\nclass wapid:\n\t\"\"\"\n\tGet WAPI IDs\n\t\"\"\"\n\n\tdef __init__(self, apifile, apidict):\n\t\tself.apifile = apifile\n\t\tself.apidict = apidict\n\t\tself.update()\n\n\tdef update(self):\n\t\tglobal token\n\t\tglobal gw\n\t\t\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresult_total_pages = int(response['pageInfo']['totalElementsByPageIndex'])\n\tresult_element_per_page = int(response['pageInfo']['elementPerPage'])\n\tresponse_json = response['content']\n\tfor page_result in range(int(result_total_pages / result_element_per_page)):\n\t\tpage_result_url = f\"{response_json['requestUrl']}?page={page_result}{response_json['extensionRequest']}\"\n\t\tyield response[\"content\"][\""}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_size = 100\n\twhile True:\n\t\tif (len(response[\"data\"]) >= max_results):\n\t\t\tsize = len(response[\"data\"])\n\t\t\tresponse[\"data\"] = response[\"data\"][\n\t\t\t\t\t\t\t(page_size*response[\"page\"]):(size-page_size)]\n\t\telse:\n\t\t\tif (response[\"page\"] == 0):\n\t\t\t\tsize = len(response[\"data\"])\n\t\t\t\tresponse[\"data\"] = response[\"data\"][size-page_size:]\n\t\t\telse:\n\t\t\t\tresponse[\"data\"] = []\n\t\t\tbreak\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif max_results is None:\n\t\tmax_results = 100\n\n\tpage_counter, data = 1, []\n\twhile page_counter <= max_results:\n\t\tif (page_counter > 1):\n\t\t\tresponse = paging.paging(page_counter, page_counter-1, response, max_results)\n\t\tfor x in range(len(response)):\n\t\t\tif response[x] is not None and response[x]!= '':\n\t\t\t\tdata.append(response[x])\n\t\tpage_counter += 1   \t\n\treturn data\n\n\ndef"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresults = response\n\tpage_size = min(max_results, 1000)\n\twhile len(results) == page_size:\n\t\tyield results\n\t\toffset = page_size\n\t\twhile True:\n\t\t\tresponse = get_response(page=True, offset=offset)\n\t\t\tresults = [res[\"result\"] for res in response[\"result\"]]\n\t\t\tif len(results) < page_size:\n\t\t\t\tbreak\n\t\t\toffset += page_size\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\titem_count = len(response.data)\n\tif(item_count <= max_results - 1):\n\t\treturn (response)\n\treturn (response[0:-1])\ndef check_file_content_strings(wdfile):\n\"\"\"\n    Checks files for various file paths and strings.\n    File must be downloaded by the user, e.g. with unzip/tar/unpack.\n    \n    Args:\n        wdfile (str): path of the file to be checked.\n    Returns:\n        boolean: If strings are found in the"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn bytes(int(size.replace(\" \", \"\")[:-1]) * 1024)\n\n\ndef format_timedelta(total_seconds: int) -> str:\n\t\"\"\"\n\tFormat time as seconds.\n\t\n\tArgs:\n\t    total_seconds: time in seconds as integer\n\t\n\tReturns:\n\t    a string representing the number of seconds in 'H:MM:SS'\n\t    example: '1:13:02'\n\n\tExample::\n\t\n\t\t>>> format_timedelta(4321)\n\t\t'1:13:02'"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(size[:-2] + '1')"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ts = 0\n\tsumm = 0\n\tfor char in size[:-1]:\n\t\ts += 1 if char in 'kKMKtT' else (s - 1)\n\t\tsumm = (summ + 1) * (1 if char in 'kKMKtT' else (2 if s == 2 else 3))\n\treturn summ * 10 ** (s -1)"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ttry:\n\t\treturn int(humanize.naturalsize(size, binary=True))\n\texcept (ValueError, TypeError):\n\t\traise ArgumentTypeError(\"%s is not a valid file size\" % size)\n\n\ndef humanize_size(size_bytes: int) -> str:\n\t\"\"\"\n\tConvert a human-readable representation of file size to a machine-readable representation.\n\t\n\tArgs:\n\t    size_bytes: A decimal representation of the file size in bytes\n\t\n\tReturns:\n\t    A human-readable representation of the file"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tsize_bytes = None\n\n\ttry:\n\t\tsize_bytes = int(size)\n\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise RuntimeError(\"Error, invalid file size\")\n\n\treturn size_bytes \n\ndef check_is_directory(filepath: str) -> bool:\n\t\"\"\"\n\tCheck if filepath refers to a directory. Returns True if it does, \n\totherwise returns False.\n\t\n\tArgs:\n\t    filepath: A string representing path to a file or a directory\n\t\n\tReturns:\n\t    True"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tsize = re.sub('[^0-9KMGTP]', '', size)\n\treturn eval(f\"1{size}\")\n\ndef bytes_to_size(bytes: int, prefix: str=\"B\") -> str:\n\"\"\"\n\tConvert a total file size in bytes to human readable value.\n\t\n\tArgs:\n\t    bytes: A string representing the file size in bytes\n\t    prefix: The prefix of the file size indication (eg.: K for Kilobytes, MB for Megabytes, etc.)\n\t    \n\tReturns:\n\t    "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ttry:\n\t\treturn int(size.replace(\"K\", \"*1024\"))\n\texcept ValueError:\n\t\treturn int(size.replace(\"M\", \" * 1024 * 1024\"))\n\ndef bytes_to_size(size: int) -> str:\n\"\"\"\n\tConvert bytes to human readable file size.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A decimal representation of file size (eg: 500)\n\t\n\tReturns:\n\t    Human readable file size (eg: '500"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ttry:\n\t\tsize = int(size)\n\t\tif size < 0:\n\t\t\traise ValueError()\n\texcept ValueError:\n\t\traise ValueError(\"invalid size argument\")\n\tif size == 0:\n\t\tif _WINDOWS:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn 1\n\tfor unit in (\"B\", \"K\", \"M\", \"G\", \"T\"):\n\t\tif size < 1024.0:\n\t\t\treturn size\n\t\tsize /= 1024.0\n\treturn size\n\n\n# Decorator for unit conversion functions.\ndef unit_conversion"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t# TODO\n\"\"\"\n\nTaken from:\n\thttps://stackoverflow.com/questions/1743407/\n   from-decimal-to-binary-octal-string/\n\"\"\"\nfrom decimal import Decimal\n#from io import BytesIO\n\n\nclass Binary:\n\tdef __init__(self, string):\n\t\tself.length = len(string.encode('utf-8'))\n\t\tif isinstance(string, (str, bytes)):\n\t\t\tif isinstance(string, (str)):\n\t\t\t\tself.data = string."}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(size[:-1]) * 1000 if 'K' in size or 'M' in size or 'G' in size else int(size)\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {k: v + d[k] for k, d in dicts if k in d}\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\trv = {}\n\t\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k not in rv:\n\t\t\t\trv[k] = d[k]\n\t\t\telse:\n\t\t\t\trv[k] += d[k]\n\treturn rv\n\ndef _dictmerge_list(primary, secondary):\n\t\"\"\"\n\tMerge the lists of dictionaries within the list of dictionaries\n\tsupplied by iterable primary and secondary\n\t\n\t>>> _dictmerge(['key1': 1, 'key2': 2"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn sum([dict(x) for x in dicts])\n\nP2 = set(('A', 'B'))\nP1 = set(('A', 'C', 'E'))\nall_perms = set(map(P2, P2))\nall_perms2 = set(map(P1, P1))\nall_perms_intersect = all_perms.intersection(all_perms2)\nall_perms_union = all_perms.union(all_perms2)\nall_paths = sorted("}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\trslt = {}\n\tfor dct in dicts: rslt.update(dct)\n\treturn rslt\n\t\ndef _interpolate(seq, factor=100):\n\"\"\"\n\tGiven a list of tuples, this function takes them and computes their\n\taverage weight, multiplied by some factor (default ``100``).\n\t\n\t>>> _interpolate([(1, 10), (9, 1), (14, 1)], factor=1)\n\t[(9, 1), (7.9, 1), (11.9, 1)]\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dict(sum(d.items(), ()))\n\ndef _curry(func, *args, **kwargs):\n\t\"\"\"\n\tCurry fuction func into func(args,..., **kwargs) by returning a generator function.\n\t\n\t>>> tuple(map(list, curry(c, 'a', 'b')(1, 2, 3)))\n\t([1], [2], [3])\n\t\"\"\"\n\tdef g(*args, **kwargs):\n\t\treturn func(*args, **kwargs)\n\tdef f(**kwargs):\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn sum(itervalues(dicts[0]), {})\n\ndef _dictmerge(a, b):\n\t\"\"\"\n\tCombine the dictionaries a and b.\n\tThis performs a deep copy of a\n\tand a key merge b over a.\n\ta and b are assumed to be dict instances.\n\t\n\t>>> d = {}\n\t>>> _dictmerge(d, {'a': 1, 'b': 2})  #doctest: +ELLIPSIS\n\t{...}\n\t>>> d\n\t{...}\n\t\"\"\""}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t_a__initargs, _a__iteritems = None, None\n\t_d = {}\n\tfor _ in dicts:\n\t\t_k = _a__iteritems(_) if _a__iteritems(_) is not None else None\n\t\tif _k in _d: _d[_k] += _a__initargs(_)\n\t\telse: _d[_k] = _a__initargs(_)\n\treturn _d\n\ndef _dictstrrep(x, y=None):\n\"\"\"\n\tReplace all substrings of all matching keys in dictionary,"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tout = {}\n\tfor d in dicts:\n\t\tout.update(d)\n\treturn out\n\ndef _totranspose(M):\n\"\"\"\n\tConvert M to the transpose of M.\n\n\t>>> a = numpy.array([[1, 2, 3],\n\t\t\t\t\t\t[4, 5, 6]])\n\t>>> print numpy.array_str(_totranspose(a))\n\t[[1, 4],\n\t[2, 5],\n\t[3, 6]]\n\t\"\"\"\n\ttransp = numpy.empty(n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tret = {}\n\tfor d in dicts:\n\t\tret.update(d)\n\treturn ret\n\n\n###############################################################################\n###############################################################################\n# main \n###############################################################################\n###############################################################################\n\nif __name__ == \"__main__\":\n\timport doctest\n\t\n\t# test_doctest()\n\t# doctest.testfile('doctests.txt', verbose=True)\n\t# doctest.testfile('../doctests_py.txt', verbose=True)\n\t\n\tprint"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\td = {}\n\td.update({k: v for d in dicts for k, v in d.items()})\n\treturn d\n\n\t\t\n# =============================================================================\nif __name__ == '__main__':\n\timport doctest\n\tif doctest.testmod().failed == 0:\n\t\tprint(\"\\n*** ALL TESTS PASSED; GOOD WORK!\\n\")\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tresult = url\n\tfor k, v in url_args.items():\n\t\tresult = re.sub(k, str(v), result)\n\n\treturn result + url_args.get(\"/\", \"\")\n\nclass Url:\n\t\"\"\"\n\tSimple URL class that wraps the request method URLs in custom functions\n\t\"\"\"\n\n\tdef __init__(self, base_url, auth, useragent, headers, custom={}):\n\t\t\"\"\"\n\t\tInitialize URL class instance and load custom items\n\t\t\"\"\"\n\n\t\tself.base_url = base_url\n\n\t\t#"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor key, value in args.items():\n\t\turl = url % (key, value)\n\treturn url\n\n# Get the URL to log in to the DB\ndb_url = \"mysql://{}:{}@{}:{}/{}\".format(\n\targs['mysql_user'],\n\targs['mysql_pass'],\n\targs['mysql_host'],\n\tint(args['mysql_port']),\n\targs['mysql_db'])\n\n# Get the URL to the CSV source file\ncsv_url ="}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treturn_url = url\n\tfor key, value in url_args.items():\n\t\tif \"{\" in key and key.endswith(\"}\"):\n\t\t\ttry:\n\t\t\t\treturn_url = return_url.format(**value)\n\t\t\texcept (TypeError, ValueError):\n\t\t\t\treturn_url = return_url.replace('{%s}' % key, '{}')\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn_url = return_url.replace(\"{%s}\" % key, str(value))\n\t\t\texcept (TypeError, ValueError):"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor k, item in url_args.items():\n\t\turl = url.replace(\"{\"+k+\"}\", item)\n\treturn url\n\ndef _replace_url_query(url, url_args):\n\tfor k, item in url_args:\n\t\turl = url.replace(\"?\"+k+\"=\", str(item))\n\treturn url\n\ndef _make_url(url, name=None):\n\t\"\"\"\n\tBuild custom URI from a url and a name\n\t\"\"\"\n\tif name:\n\t\turl = _replace_url_query("}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_string_regex_format = r'{([^}]*)}'\n\t\n\turl_pattern = re.compile(url_string_regex_format)\n\t\n\turlparts = urllib.parse.urlparse(url)\n\t\n\tfor key,value in zip(kwargs['request'].POST.keys(), args):\n\t\turl = url_pattern.sub(value,url)\n\t\t\t\n\treturn url\n\n#@csrf_exempt   \ndef _redirect(request, url, url_args = None, **"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif(url is not None):\n\t\tfor i in range(0, len(url)):\n\t\t\turl[i] = url[i].replace(\"$\", str(url_args[i]))\n\"\"\"\n\tEnd\n\"\"\"\n\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tspt = url.split(\"|\"); sargs = url_args\n\ti = -1;\n\tfor arg in sargs:\n\t\ti = i + 1\n\t\tj = spt.index(\"$\"+str(i)); spt.pop(j)\n\n\tif len(sargs) > 0:\n\t\turl = \"\".join(spt)\nelse:\n\tpass\n\n# ----------------------------------------------------------------------\ndef _reconnect(url, url_args):\n\t\"\"\"\n\t\tReconnect to source after a reoccuring connection failure\n\t\t\"\"\"\n\tif"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif url_args is None or url_args == {} or url_args.values() is None or url_args.values() == []: return(url)\n\tresult = url\n\tfor key, val in dict(**url_args).items():\n\t\tresult = result.replace(\"{\" + key + \"}\", val)\n\treturn(result)\n\ndef xbmc_url_decider(*urls, **options):\n\t\"\"\"\n\tFunction used to determine the correct decoder to use with URL\n\n\t<url>, *args, **kwargs\n\t\"\"\""}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treturn url.format(\n\t\titem = url_args['item'],\n\t\tvalue = url_args['value'],\n\t\tsub_type = url_args['sub_type'],\n\t\t# group_type = url_args['group_type'],\n\t)\n\ndef _replace_json(json_string, **json_args):\n\tif json_args['format_in'] =='str':\n\t\tjson_string = json.dumps(json_string)\n\telif json_args['format_in'] == 'int':\n\t\tjson_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t\n\tresult = ''\n\targskeys = url_args.keys()\n\tif len(url_args.keys()) > 0:\n\t\tstart = url.index('%')+1\n\t\tstart += 1\n\t\tidx = 0\n\t\tfor argstr in argskeys:\n\t\t\tif argstr in url:\n\t\t\t\turl = url.replace(argstr, url_args[argstr])\n\t\t\t\tcontinue\n\t\t\tif idx < len(argskeys) - 1:\n\t\t\t\tend = start+url_args[argstr][1]-1\n\t\t\t\tresult += url[start"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tfrom vyperlogix.enum import Enum\n\t_is_none_string_values = (Enum._valToEnumData_(_e,val,None) for _e in (None,False,True))\n\ttheString = (str(_is_none_string_values) if isinstance(val,Enum._typeToClasses_ ) else val)\n\treturn theString == 'True' or theString == 'False' or theString == 'None' or theString == ''\n\ndef remove_all_entries_from_2d_dict"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn len(str(val).strip()) == 0"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\ttry:\n\t\tparse_string(val)\n\texcept ParseError:\n\t\treturn True\n\treturn False\n\n\ndef is_date(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a date.\n\t\"\"\"\n\n\tif type(val) == datetime:\n\t\treturn True\n\telif val == None:\n\t\treturn True\n\ttry:\n\t\treturn datetime.strptime(str_to_date(val),'%d/%m/%Y')\n\texcept Exception as e:\n\t\terr = str(e"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val == type(\n\t\t\tval)()\n\ndef typeof(value: any) -> str:\n\"\"\"\nReturn the Python type the `value` represents.\n\t\tThis can be `None` if `value` is `None`.\n\t\t\"\"\"\n\tif is_none_string(value):\n\t\treturn None\n\tif isinstance(value, float):\n\t\treturn \"{name}.{name}\".format(\n\t\t\t\tname=type(value).__name__)\n\treturn \"{name}({value!r})\".format(\n\t\t\t\tname=type(value).__name"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, str) and val.lower() == \"none\"\n\ndef is_missing_value(val) -> bool:\n\"\"\"\n\tCheck if a string represents a Missing value\n\t\"\"\"\n\treturn isinstance(val, str) and (val.lower() == \"missing\" or val.lower().startswith('m'))\n\n_is_numeric_value = lambda val: (not isinstance(val, str)) and (not isinstance(val, unicode))\n_is_boolean_value = lambda val: (not isinstance("}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val == None:\n\t\treturn True\n\telif issubclass(type(val), str):\n\t\tif val.strip() == '':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\"\"\"\nCheck if a string is an empty string or not. Use \"empty\" because an empty string would mean None.\n\"\"\"\ndef is_str_empty(val: any) -> bool:\n\tif val == \"\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\"\nCheck if the string is in between the"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val == None\n\n\ndef is_string(val: any) -> bool:\n\"\"\"\n\tString representation of the type of an object.\n\t\"\"\"\n\treturn inspect.isclass(type(val))\n\n\ndef is_none_key(val: str) -> bool:\n\t\"\"\"\n\tChecks if a python dictionary value matches a None string\n\t\"\"\"\n\treturn val.lower() == \"none\"\n\n\ndef is_string_key(val: any) -> bool:\n\t\"\"\"\n\tChecks if a python dictionary value is a non-empty string\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val:\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef is_nonstring_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string is not a string.\n\t\"\"\"\n\treturn type(val) is not str\n\ndef is_str_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string is a string.\n\t\"\"\"\n\treturn type(val) is str"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None or (isinstance(val, str) and val.lower() == \"none\")\n\nclass InvalidArgumentTypeError(InvalidTypeError):\n\t\"\"\"\n\tA type error raised when a function argument value doesn't conform to its\n\texpected type.\n\n\tAttributes:\n\t    func - the function call being made\n\t    arg_name - the argument name\n\t    arg_index - the argument index of the argument\n\t    correct_type - the expected type of the argument\n\t    actual_type - the actual type of the argument\n\t    value - the"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val == 'None' or val is None\n\n\ndef is_true(val: any) -> bool:\n\tval = str(val)\n\treturn val.lower() in (\"1\", \"true\", 't', \"yes\")\n\n\ndef get_datetime_str(timestamp, milli = False) -> str:\n\t\"\"\"\n\tGet a string representation of a timestamp\n\t\"\"\"\n\treturn datetime.datetime.fromtimestamp(int(timestamp)).strftime(\"%Y-%m-%d %H:%M:%S\") if mill"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif type(parser)!= ArgumentParser:\n\t\tprint(f\"Error: {parser} must be an instance of Python argparse.ArgumentParser.\")\n\t\tsys.exit(-1)\n\t\n\treturn \"\\n\".join([f\"--{x[1][0][0] if type(x[1]) == list else x[1]}\" \\\n\tfor x in vars(parser.parse_args()).items()])\n\ndef setup_logging(args):\n\t\"\"\"\n\tSetup logging.\n\n\tIf no logging level is passed through, set the default"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\ts = \"\"\n\tfor flag in vars(parser._actions[1:]):\n\t\tif flag.startswith(\"help_\"):\n\t\t\tcontinue # pragma: no cover\n\t\tif flag.startswith(\"_\"):\n\t\t\tcontinue # pragma: no cover\n\t\ttry:\n\t\t\tdefault = getattr(parser._actions[1], flag).default\n\t\texcept AttributeError:\n\t\t\tdefault = \"no-default\"\n\t\tif type(default) == str:\n\t\t\tdefault = \"\\033[93m%s\\033[0m\" % default\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join([ flag for flag in vars(parser.parse_args()).items() ])\n\nclass BaseFormatter(object):\n\"\"\"\n\tClass whose instances are used as mixin-classes by the formatter classes in this package.\n\"\"\"\n\tdef __init__(self):\n\t\tself.formatters = {}\n\nclass Autoformatter(BaseFormatter):\n\t\"\"\"\n\tClass implementing the automatic formatter functionality.\n\tThe formatter factory has to be derived from this class; see its module docstring.\n\t\"\"\"\n\tdef __init__(self"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join('--{}'.format(arg) for arg in parser._actions)\n\n@lru_cache\ndef get_recon_data(recondir):\n\t\"\"\"\n\tGiven the path to an Recon instance, return the reconstructed dataset dictionary in JSON format\n\tin the specified directory with a filename 'data.json' and a filename'metadata.json'.\n\t\"\"\"\n\tdf = pd.read_json(os.path.join(recondir, 'data.json'), lines=True)\n\trec = pd.read_json(os"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ''\n\tfor action in parser._actions:\n\t\t\t\tif action.default!= None:\n\t\t\t\t\tdefault_value ='' + str(action.default)\n\t\t\t\telif action.help == None or action.help == \"\":\n\t\t\t\t\tdefault_value = ''\n\t\t\t\telse:\n\t\t\t\t\tdefault_value ='' + str(action.help)\n\n\t\t\t\tflags = flags + str(action.dest) + default_value\n\n\treturn flags"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\ttry:\n\t\t\targs = list(action.default.__args__)\n\t\texcept BaseException:\n\t\t\targs = []\n\t\tflags = flags + [\"{}{}\".format(action.dest, arg)\n\t\t\t\tfor arg in args]\n\tfor default in parser._defaults:\n\t\ttry:\n\t\t\targs = default.__args__\n\t\texcept BaseException:\n\t\t\targs = []\n\t\tflags = flags + [\"{}={}\".format(action.dest, arg)\n\t\t\t\tfor arg in args]"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tactions = []\n\n\tclass Action:\n\t\tdef __init__(self, parser):\n\t\t\tself.parser = parser\n\n\t\tdef get_help(self):\n\t\t\treturn self.parser.format_help()\n\n\tfor action in parser._actions:\n\t\tactions.append([\"\", self.parser.format_help(), action.help])\n\t\tif not isinstance(action, Action):\n\t\t\tcontinue\n\n\t\tflags = []\n\t\tflags.append(action.get_help())\n\t\tflags.append(action.parser.format_help())\n\t\tif action.option_strings"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = [getattr(parser, arg) for arg in dir(parser) if arg.startswith('-')]\n\tfor typeflag, value in parser._option_string_actions.items():\n\t\targs.append('-{0}'.format(typeflag))\n\treturn''.join(args).strip()"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(map(str,parser.parse_args()))\n\ndef write_logging_config(config_file):\n\"\"\"\n\tGiven a path to a yaml config file, write the default logging configuration to that file.\n\t\"\"\"\n\twith open(config_file,'w') as f:\n\t\tyaml.dump(default_config,f)\n\ndef append_logging_config(config_file):\n\"\"\"\n\tGiven a path to a yaml config file, read the specified config file & return a merged\n\tyaml config.\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.formatter_class(argparse.ArgumentDefaultsHelpFormatter()).format_args(parser)\n\ndef print_progress_bar(n, step, bar_name=None, fill='#', empty=' '):\n\t\"\"\"\n\tGiven a start value, print a progress bar (https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console) to console.\n\n\tParameters\n\t----------\n\t\tn: int\n\t\t\tThe current value of the progress bar\n\n\t\tstep: int\n\t\t\tThe step size"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\ttry:\n\t\tif verbose:\n\t\t\tprint(\"Testing:\", f\"{path_name}...\")\n\t\n\t\tpath_abs = os.path.abspath(path_name)\n\t\tprocessed.add(path_abs)\n\t\n\texcept Exception as e:\n\t\tprint(e)\n\t\tprint(f\"{path_name} cannot be checked\")\n\t\treturn True\n\t\n\treturn False\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(f'Skipping file {path_name} as it was already processed.')\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\n\tif path_name not in os.listdir(os.path.expanduser(path_name)) or (path_name and verbose):\n\t\tprint(f'Processed {path_name}')\n\t\treturn False\n\t\n\tfor ri, (dir_name, dir_names, file_names) in"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = path_name.strip()\n\tprocessed.add(path_name)\n\tif path_name in processed:\n\t\tprint(\"Skipping \" + path_name)\n\t\treturn True\n\telif os.path.isdir(path_name):\n\t\tfor root, dirs, files in os.walk(path_name, topdown=False):\n\t\t\tfor file in files:\n\t\t\t\tprocess_file(processed, dirs, file, verbose)\n\telif os.path.isfile(path_name):\n\t\tprocess_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tprocessed.add(path_name)\n\treturn not (path_name in processed)\n\n\ndef process_directory(processor_func, path_name, verbose=False):\n\t\"\"\"\n\tRecursively search in a directory and process all files to be processed.\n\t\n\t:param processor_func: The function that processes each file or directory\n\t:type processor_func: function\n\t:param path_name: The path to start the search\n\t:type path_name: str\n\t\"\"\"\n\tif verbose:\n\t\tprint_time('Starting"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = abs_path(path_name)\n\tif verbose:\n\t\tprint(\"Checking {}\".format(path_name))\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"%s was already processed\" % path_name)\n\t\traise BaseException((\"%s was already processed\" % path_name))\n\tprocessed.add(path_name)\n\treturn False\n\nif __name__ == \"__main__\":\n\tmain()"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfor i in processed:\n\t    if i == path_name:\n\t        print(\"File or directory already processed, skipping:\", path_name)\n\t        return True\n\tprocessed.add(path_name)\n\treturn False\n\n# If a file has already been prcessd, the function returns True.\n# Else, False. Since this function is responsible for checking if\n# a file has been processed before, only the first file/folder from the\n# list can be analyzed. I.e., don't try to check if there are files already called\n# \".\""}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfor item in processed:\n\t\tif item == dirname(path_name):\n\t\t\ts=str(path_name)\n\t\t\ts=s.replace(\"\\\\\",\"/\")\n\t\t\ts=s.replace(\"//\",'/')\n\t\t\tif verbose:\n\t\t\t\tprint(\"was_processed failed: %s\"%dirname(path_name))\n\t\t\tprint(\"was_processed failed: %s\"%path_name)\n\t\t\treturn True\n\treturn False\n\n\ndef get_paths(directory):\n\t\"\"\"\n\tGets the directory root, all subdirect"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tprocessed.add(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(f\"ERROR: {path_name} has already been processed previously\")\n\t\treturn True\n\treturn False\n\ndef main():\n\t# Check command line arguments\n\tif len(sys.argv)!= 2:\n\t\tsys.exit(\"Usage:\\npython src/run.py your_directory\")\n\trun_path = sys.argv[1]\n\n\t# Make sure a directory was provided\n\tif not os.path.isdir("}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\tif verbose == 1:\n\t\t\tprint(('WARNING:'+ path_name +'already handled'))\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\t\ndef add_directory_recursion(pathname, processor):\n\t\"\"\"\n\tAdd the directories in the directory at pathname to the\n\tdict of data to process:\n\t\n\t* If ``pathname`` is a file, add its data to ``processor.data``.\n\t* For each directory in pathname:\n\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif already_in_procset(processed, path_name):\n\t\tif verbose:\n\t\t\tprint_error_and_exit(\"{}: Already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\ndef process_and_split(src_dir, output_dir, verbose, max_depth):\n\"\"\"\n\tIterate over src_dir for files, splitting files up into\n\tgroups of max_split files\n\t\n\tArgs:\n\t    src_dir: Root"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpoints = []\n\tfor i in range(len(vertices)):\n\t\ttemp = vertices[i]\n\t\ta, b, c, d = temp\n\t\tpoints.append((a, b, c))\n\t\tif i!= len(vertices) - 1:    #if this is not the last vertex in the vertex\n\t\t\tif type(a) == str:\n\t\t\t\tpoints.append((b, c, d))   #return a and b instead of d and c\n\t\t\telse:\n\t\t\t\tpoints.append((a, c, d))\n\t\t\n\t#print"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tx_value_list_0 = []\n\ty_value_list_0 = []\n\t\n\tx_value_list_1 = []\n\ty_value_list_1 = []\n\t\n\tx_value_list_2 = []\n\ty_value_list_2 = []\n\t\n\tx_value_list_3 = []\n\ty_value_list_3 = []\n\t\n\treturn vertices \ndef polygon_create():\n\t\"\"\"\n\tcreate the polygon from the vertices.\n\t\n\t\"\"\""}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tdab, ab, ac, cd = vertices\n\n\tvertices = [[dab,  [1,0]],[ab,  [1,1]],[ac,  [-1,1]],[cd,  [-1,0]]]\n\tverts = []\n\tfor i in range(6):\n\t\tsides = [[-1,1]]\n\t\tfor j in range(-2,-3,-1):\n\t\t\tsides.append([1,0])\n\t\t\tverts.append(verts[-2][-sides[-3]])\n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tA=list((vertices[0]/2.,vertices[1]/2.,vertices[2]/2.))\n\tB=list((vertices[1]/2.,vertices[2]/2.,vertices[3]/2.))\n\tC=list((vertices[2]/2.,vertices[3]/2.,vertices[0]/2.))\n\tD=list((vertices[0]/2.,vertices[2]/2.,vertices[3]/2.))\n\tB=[A["}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn [(polygon[i],polygon[i-1]) for i in range(1,len(polygon))], [(polygon[i],'start'),(polygon[i-1],'end') for i in range(1,len(polygon))]\n\n\n\ndef vertices3d(polygon):\n\"\"\"\n\treturns a list of tuples of the vertices of a 3d polygon in the following format::\n\t\n\t#A    B    C\n\t'Start':'2',  [start location of the vertex]\n\t'End"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tv1=vertices[0]\n\tp1=vertices[1]\n\tp2=vertices[2]\n\tv2=vertices[3]\n\tpoly=[v1,p1,p2,v2]\n\t\n\t#A\n\tpoly_A=[v1,v1]\n\t\n\t#B\n\t\n\tpoly_B=[p1,p2]\t\n\t\n\t#C\n\tpoly_C=[v2,v1]\n\t\n\t#D\n\tpoly_D=[p1,v2"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tn = len(vertices)\n\tv1, v2 = zip(*vertices) #* is used to iterate in tuple and then get first value and then get second one\n\treturn v1,(v1[1],v1[0]),v2,(v2[1],v2[0])\n\n\n\ndef create_mesh_from_npointstuple(points_tuple):\n\t\"\"\"\n\tcreates a mesh by dividing that single polygon into multiple segments. The input needs to be a tuples consisting of 3 coordinates as (x,y,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tpoints2return=[]\n\ttry:\n\t\t\n\t\tif type(vertices) not in (list,tuple):\n\t\t\traise TypeError\n\t\tif len(vertices) < 3:\n\t\t\traise TypeError\n\t\tif len(vertices) %2!= 0:\n\t\t\traise TypeError\n\texcept Exception as e:\n\t\traise TypeError(e)\n\telse:\n\t\tpoints2return=[]\n\t\tfor vertex,vertex2 in zip(vertices,vertices+vertices[::-1]):\n\t\t\tpointz=tuple(t"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t#vertices = list(vertices)\n\tvertices = list(vertices)\n\tvertices.append(vertices.pop(0))\n\tA = vertices.pop(0)\n\tB = vertices.pop(0)\n\tC = vertices.pop(0)\n\tD = vertices.pop(0)\n\tpairs = []\n\tpairs += sorted([A,B,C,D])\n\treturn tuple(pairs)\n\ndef centeroid(points):\n\"\"\"\n\treturn the centeroid (centre of mass)."}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tleft_right_points = []\n\tfor pt in vertices:\n\t\tpt = ( pt[0], pt[1] )\n\t\t#print(vertices.index(pt))\n\t\tleft_right_points.extend([pt,(pt[0] + pt[0], pt[1])])\n\t\n\treturn left_right_points\n\t\ndef calculate_diam_and_cir_center_of_shape(edges):\n\t\"\"\"  \n\tedges = edges of the shape. They have to be triangls or quadr"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tif __name__ == '__main__':\n\t\tnumber = int(input('Enter a number: '))\n\tif type(number)!= int:\n\t\tprint('ERROR: The input must be an integer.')\n\telse:\n\t\ttry:\n\t\t\treturn ''.join([alphabet[-1::-1] for _ in range(number)])\n\t\texcept KeyError:\n\t\t\tprint('ERROR: The given alphabet is wrong.')\n\t\t\texit(1)\n\n\nif __name__ == '__main__':\n\talphabet_size = 7\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# NOTE: Use a lookup table\n\tif not padding: padding = len(alphabet)\n\t\n\t# Sanity check that number is zero\n\tif number == 0: return alphabet[0]\n\t\n\tdef convert_one(number: int):\n\t\tif number < 0:\n\t\t\tnumber *= -1\n\t\t\treturn alphabet[number % padding]\n\t\telif number % 2 == 0:\n\t\t\treturn alphabet[(number // padding) % padding]\n\t\telse:\n\t\t\treturn alphabet[0]\n\t\n\t# Start with the largest value\n\ts ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\t# if no padding is desired\n\tif not padding:\n\t\treturn str(number)[0]+str(number)[1:]\n\t\n\t# create the padding of the required size\n\tremainder = int(len(str(number))/size(alphabet)) * size(alphabet)\n\tpadding_needed = int(remainder - len(str(number)))\n\tpadding_string = \"\"\n\tfor i in range(padding_needed):\n\t\tpadding_string = padding_string + alphabet[0]\n\tpadding_string = padding_string + str"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n    number = str(number)\n    return ''.join(x for i, x in enumerate(reversed(number)) if not i or not x.isdigit())"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# Check that the given alphabet contains only uppercase letters in some way:\n\tif any([True for char in alphabet if not char.isupper()]):\n\t\tlogger.warning(\"Possible error: int_to_string had non-alphabet characters in alphabet arguments\")\n\n\t# Pad the string if we needed to do so\n\tstring = ''\n\tif padding is not None:\n\t\t# Pad the string\n\t\twhile len(string) < padding:\n\t\t\tstring += alphabet[0]\n\telse:\n\t\t# Create an empty string\n\t\tstring = ''\n\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t...\n    \n    number = str(number).rjust(padding, '0')\n    \n    return ''.join([alphabet[int(i)] for i in number])\n        \ndef int_to_bigint(number: int, big_alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a big-endian big-endian encoded string: the most significant digit first.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n    big_number = int_to"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tpadded_digit = \"0\" * (padding - number % 10)\n\treturn alphabet[value -> number mod 10]\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tresult = '0' if (\n\t\t\tnumber  < 0  else  \n\t(\n\t\t\t(\n\t\t\t\t\"\"\tif (\n\t\t\t\t\t(number\t\t\t  %\t10 ==\t0\t  )\n\t\t\t\t)\tand\t(\n\t\t\t\t\t(number  /\t10\t\t  %\t10 == 0)\n\t\t\t\t)\telse\n\t\t\t\talphabet[number % 10]\n\t\t\t).replace\t(\"\", \"  \")\n\t\t))\n\tresult = result.replace(\n\n\t\t\t\"\", str(\tpadding\t  +\t1\t  ) if (\n\t\t\t\t(\tpadding \t  "}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\tif not alphabet:\n\t\traise ValueError(\"No alphabet\")\n\t# Convert number to binary and pad it with leading zeros if necessary.\n\tnumber_str = f'{number:016b}'\n\tif padding:\n\t\tif len(number_str) > padding:\n\t\t\tnumber_str = number_str[:padding]\n\t\telse:\n\t\t\tnumber_str = f\"{'0'*padding:0<{padding}}\"\n\t\tnumber_str = number_str[::-1]"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# Pad the number to use the length of the alphabet\n\tif padding:\n\t\tnumber = number[:padding]\n\toutput = ''\n\tif number < 0:\n\t\tnumber = number + 2**len(alphabet)*len(alphabet)\n\tfor letter in alphabet:\n\t\t# Count number of bits set in the number\n\t\tbit_count = 0\n\t\tfor bit in range(len(alphabet)*2):\n\t\t\t# Get bit of the number\n\t\t\tbit_value = (number // (2**bit)) % (2**len(alphabet))\n\t\t\t#"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\ttry:\n\t    for item in flow_params:\n\t    \tflow_params[item].update({register_number: register_value})\n\texcept Exception as e:\n\t    print(str(e))\n\t\n\ndef _find_flow(flow_params, filter_keys):\n\t\"\"\"Find flows matching certain keys.\n\t\n\tThis function first checks if input is string or not. If not,\n\tthen if input is list of arguments, the search will be of\n\tthat list elements.\n\t\n\tOtherwise, it will search the input of that key"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params['flows'][flow]['pkt_metadata'][_REGISTER]['value'] = {\n\t\t_REGISTER_KEY: register_number,\n\t\t_REGISTER_VALUE: register_value}\n\ndef _replace_length(flow_params, length_name, length_value):\n\"\"\"\n\tReplace `length_name` by given `length_value`\n\t\n\t'length_value' key in dictionary will be replaced by value of given 'length_name'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tkey = register_number\n\tflow_params[flow_params.keys()[0]][key] = register_value\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params[register_value] = int(flow_params[register_number])\n\ndef _print_table(table, row_label, column_label):\n\t\"\"\"\n\tPrint 'table' table\n\t\n\t'column_label' and 'row_label' parameter will be used\n\tin output\n\t\n\t:param table: Pretty table object to be printed\n\t:param row_label: The label of the row\n\t:param column_label: The label of the column\n\t\"\"\"\n\ttable.align[\"Column Name\"] = \"l\"\n\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params.pop('flow1')\n\tflow_params[f'value_{register_number}'] = {\n\t\tregister_value: \"register_number\"\n    }\n\treturn flow_params\n\ndef _create_register_value1(flow_params):\n\t\"\"\"\n\tCreate register value1 by adding (random_int1) to (register1)\n\tflow1\n\t\n\t\t:param flow_params: flow dictionary\n\t\t:returns: A copy of the flow dictionary updated\n\t\"\"\"\n\tglobal random_int1,random_value1,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params = _get_flow_params(flow_params)\n\tfor key, value in flow_params.items():\n\t\tif key == register_value:\n\t\t\tflow_params[key]['action'] = {'type': 'write_memory',\n\t\t\t                            'address': register_number}\n\treturn flow_params\n\n\n# def _create_register_replace(flow_params, register_number, register_value):\n\"\"\"\n\tCreate register replace action for each flow\n\t\"\"\"\n\t# flow_params = _get_flow_params(flow_params"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tprint(\"Replacing value from {:02x} to {:02x}\".format(register_number, register_value))\n\tflow_params[register_value].replace_value(register_number, register_value).put()\n\t\ndef _reset_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace current value from the given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\tfor flow_id in flow_params:\n\t\tflow_params[flow_id][register_value] = register_number\n\ndef _writeFlow(flow_params, flow_list):\n\t\"\"\"Write flows to flows\"\"\"\n\t\n\tfor flow_id in flow_params:\n\t\tflow = flow_params[flow_id]\n\t\taddress = _getFlowIndex(_getFlowLength(flow_list))\n\t\tflow_length = _getFlowLength(flow_list) + 1\n\t\tflow_list = _addFlowIndex(flow_list, address,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t# Search each flow to find the flow that has the register number\n\tfor (flow_name, flow) in flow_params.items():\n\t\t# Replace register value in the flow\n\t\tif flow.get('registers') is not None:\n\t\t\tfor i in range(len(flow['registers'])):\n\t\t\t\tif flow['registers'][i].get('number') == register_number:\n\t\t\t\t\tflow['registers'][i][register_value] = register_value\n\tpass\n\ndef _replace_data(flow_params, data_number, data):"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tcount = 0\n\tfor key in flow_params.keys():\n\t\tif (key == register_value):\n\t\t\tflow_params.update({register_number : flow_params[key]})\n\t\t\tdel flow_params[key]\n\t\tcount += 1\n\treturn int(count)\n\n\ndef remove_flow_params(flow_params, flow_name):\n\t\"\"\"\n\tRemove a key and value from dictonary\n\t\n\t:param flow_params: dictionary containing flows\n\t:returns: Return modified dictionary including deleted key and value\n\t\"\"\"\n\tif"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\n__all__ = [\n\t\"replace_dots\",\n]\n\nif __name__ == '__main__':\n\t\n\t\n\t'''\n\tTests\n\t'''\n\t\n\t# a string of some text\n\tx = 'the'*3 + 'quick'\n\tprint x\n\t# the quickthequickquickthequick thequickquickquickthequick thequickquickquickthequick thequickquickquickthequick\n\t\n\t# a function\n\tprint replace_dots('the', '')"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\twhile '..' in value:\n\t\tto_replace = re.escape('.')\n\t\tvalue = re.sub(to_replace, str(arg), value, count = 1)\n\treturn value\n\ndef add_extension(filepath,extension,arg):\n\tif '.' in filepath:\n\t\tfile = filepath.rsplit(\".\",1)\n\t\tname = replace_extension(file[0],\"\",arg) + \".\" + file[len(file) - 1]\n\t\t\n\t\tnew_filepath = filepath.replace(file"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif '.' in value:\n\t\ti = value.index('.')\n\t\tparsed = value[1:i] + arg + '.'\n\t\tif '.' not in value:\n\t\t\treturn (value[1:] if value else '') + arg\n\t\telse:\n\t\t\treturn replace_dots(parsed, arg)\n\telse:\n\t\treturn value\n\n\nclass DictAsDotLookup(ast.NodeVisitor):\n\t\"\"\"\n\tClass to parse AST and replace strings in dicts to new values\n\t\"\"\"\n\n\tdef __init__("}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace('.', str(arg))\n\n__all__ = (\n\t'replace_dots',\n\t)"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace('.', arg)\n\ndef run_experiment(experiment, args):\n\t\"\"\"\n\tRuns an experiment with args to produce a results.csv\n\t:param args: command line arguments\n\t\"\"\"\n\t# if \"args = None\": args=None\n\tif args == None: args=[]\n\tif len(args)>0: experiment['args'] = args[0]\n\texperiment_time = str(datetime.datetime.now())\n\texperiment['results_dir'] = os.path.join(experiment['"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace('.', ',' + arg)\n\n@app.route('/battery', methods=['GET','POST'])\ndef battery():\n\tglobal c,s,battStat,t,h,gpslocn,locn,batStat,btrStat,btr1Stat,btr2Stat,btr3Stat,btr4Stat,btr1,btr2,btr3,btr4\n\tbatStat=c.getBatteryState()\n\tif batStat == 1:\n\t\tbatStat=\"Charging...\"\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\targ = str(arg)\n\tvalue = str(value)\n\n\t# if we've been provided any argument at all\n\tif len(arg) <= 1:\n\t\tprint('You must provide an argument to substitute a dot!')\n\t\tsys.exit()\n\n\t# if this is the last argument\n\tif value.endswith('.'):\n\t\treturn value\n\n\tif value.find('.') >= 0 and value.find('.', value.rfind('.')) >= 0:\n\t\targ = str(arg).join(arg, '.')\n\n\telse:"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value == \"\":\n\t\treturn \"\"\n\treturn value.replace(\".\", arg)\n\ndef replace_reps(value, arg):\n\t\"\"\"\n\tReplaces occurrences of rep by arg from the given string value\n\t\"\"\"\n\tif value == \"\":\n\t\treturn \"\"\n\treturn value.replace(\"/\", arg)\n\ndef replace_slashes(value, arg):\n\t\"\"\"\n\tReplaces '\\\\' to arg from the given string\n\t\"\"\"\n\tif value == \"\":\n\t\treturn \"\"\n\treturn value.replace(\"\\\\\", arg)\n\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif not value: return arg\n\tnew_string = ''\n\tfor char in value:\n\t\tif char == '.':\n\t\t\tnew_string += arg\n\t\telse:\n\t\t\tnew_string += char\n\treturn new_string\n\ndef main():\n\turl = input('Enter URL: ')\n\turl_split = replace_dots(url, '@')\n\turl_split = replace_dots(url_split, '/')\n\turl_split = replace_dots(url_split, ':')\n\turl_split = replace_d"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = value\n\targ = arg\n\tindexes_value = value.find(\".\") \n\tindexes_arg = arg.find(\".\") \n\tcount_arg = 0\n\t\n\twhile indexes_value!= -1:\n\t\tif indexes_value < indexes_arg:\n\t\t\tfor p in range (indexes_arg):\n\t\t\t\tvalue = value.replace(\".\" + str(count_arg) + \".\", arg)\n\t\t\tcount_arg += 1\n\t\t\tif len(arg) > index_arg:\n\t\t\t\tindexes_arg = arg.find(\".\","}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses += subclass.subclasses()\n\treturn subclasses\n\ndef count_by(obj, key, count=1):\n\"\"\"\nGiven an object, count the number of times a property has a particular\nvalue.  For example:\ncount_by(car, 'color')\n\"\"\"\n\tif isinstance(obj, dict):\n\t\tif key in obj:\n\t\t\treturn count_by(obj[key], key)\n\n\t\telse:\n\t\t\treturn 0\n\n\telif is"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subclass in cls.__subclasses__():\n\t\tfor subsubclass in subclass.subclasses():\n\t\t\tyield subsubclass\n\t\tyield subclass\n\n\n\n\n__all__ = [\"Bundle\", \"Group\", \"User\", ]"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn cls.__subclasses__() + sum(map(subclasses,cls.__subclasses__()),[])\n\nimport csv\nimport os \nimport numpy as np\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks, peak_widths\n\nclass DataBase():\n\tdef __init__(self):\n\t\tself.data = np.array([])\n\t\tself.names = []\n\t\tself.path = './'\n\t\tself."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__(): \n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn cls_dict[cls]\\\n\t\t + sum(map(set, cls_dict.values()), [])\\\n\t\t - set(cls_dict[cls])\n\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t\n\tsubs = []\n\ttoret = False\n\tfor subclass in cls.__subclasses__():\n\t\tif subclass not in subs:\n\t\t\tsubs.append(subclass)\n\t\t\ttoret = True\n\t\t\t\n\t\tif toret == True:\n\t\t\tsubs.extend(subclasses(subclass))\n\treturn subs\n\ndef findSubclass(baseClass, targetClass):\n\t\n\tfor t in subclasses(cls):\n\t\tif t is baseClass and t == targetClass:\n\t\t\treturn True\n\t\tif t is not targetClass"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subclass in cls.__subclasses__():\n\t\tyield from subclass.subclasses()\n\tyield cls\nif (__name__ == \"__main__\"):\n\"\"\"\n\tMain function\n\t\"\"\"\n\tclass C: pass\n\tfor s in list(C.subclasses()):\n\t\tprint(s)\nimport random\nimport json\nimport sys\nimport time\nfrom copy import copy, deepcopy\nimport multiprocessing as mp\nfrom tqdm import tqdm\nimport networkx as nx\nsys.setrecursionlimit(100000"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn set(cls.__subclasses__()).union(sum((s._subclasses( ) for s in cls.__subclasses__()), ()))\t\nif __name__ == \"__main__\":\n\n\tprint(\"--- test 1 ---\")\n\tclass test1(object):\n\t\tdef __init__(self, name):\n\t\t\tself.name = name\n\t\tdef test1_1(self):\n\t\t\tprint(self.name + \"test1: test 1: test 1_1\")\n\t\t\treturn  {\"name\": self.name + \"test1:"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subclass in cls.__subclasses__():\n\t\tyield from subclass.subclasses()\n\t\tyield subclass\nreturn cls.__subclasses__()\n\n#################################################################\nclass ClassVisitor(ast.NodeVisitor):\n\tdef __init__(\n\t\tself, \n\t\tfilename: TextIO, \n\t\tfile_names: Optional[List[TextOS]] = None, \n\t\tnamespace: Optional[Dict[Text, object]] = None,\n\t\tbase_class: Optional[AbstractClass] = None,\n\t\tabstract: bool ="}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = cls.__subclasses__()\nif isinstance(subclasses,list):\n\t\treturn subclasses\n\telse:\n\t\treturn cls"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(''.join(alphabet[char] for char in string))\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# assert type(string) == str\n\tstring += '#' * (len(alphabet) - len(string))\n\treturn sum(map(lambda x: (alphabet.index(x)) * (3 ** (len(alphabet) - 2 - i)), string))\n\t\n\nif __name__ == '__main__':\n\t# Test: the expected values should be 1 + 2 * 3 * 5 +... + 10 * 17 * 23 - 1\n\tarr = []\n\tfor i in range(21, 0, -1):\n\t\tarr.append("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t# Initialization\n\tresult = 0\n\t\n\tindex = 0\n\tfor char in string:\n\t\tresult += alphabet.index(char) * (len(alphabet) ** index)\n\t\tindex += 1\n\t\t\n\treturn result\n\ndef int_to_string(integer: int, alphabet: List[str]) -> str:\n\t\"\"\"\n\tConvert an integer to a string, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# Initialization\n\tstring = \"\"\n\t\n\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif string[0] in alphabet:\n\t\treturn string_to_int(string[1:], alphabet) * pow(len(alphabet), -1.) + alphabet.index(string[0])\n\telse:\n\t\treturn int(''.join(alphabet), len(alphabet))-1\n\n\ndef number_to_string_alpha(number: int, alphabet: List[str]) -> str:\n\t\"\"\"Convert a number to a string, using the given alphabet.\"\"\"\n\tif number < 0:\n\t\traise (\"Sorry, negative numbers are not defined in this domain"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\t\n    # Handle empty strings and input strings with one character\n    if not string or string.isspace():\n        return 0\n    \n    if len(string) == 1:\n        return alphabet.index(string)\n        \n    num = ord(string[0]) - ord(string[1])\n    \n    for i in range(2, len(string)):\n        if not string[i].isalpha():\n            return string_to_int(string[i:], alphabet)\n        num += (ord(string[i]) - ord(string["}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tpass\n\n\ndef int_to_string(number: int, alphabet: List[str]) -> str:\n\"\"\"\tConvert an int to a string, using the given alphabet.\n\t\n\tThe output is assumed to have the most significant digit first.\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int.from_bytes(string.encode(), byteorder='big')\n\ndef int_to_string(int_: int) -> str:\n\"\"\"\tConvert a number to a string, using the standard alphabet.\n\"\"\"\n\treturn str(int_).encode().decode()\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn sum([alphabet.index(i) for i in string])\n\t\n\ndef encrypt_int(number: int, shift: int, alphabet: List[str]) -> str:\n\"\"\"\n\tEncrypt a number using the Caesar cipher, given the shift.\n\t\"\"\"\n\treturn alphabet[((number + shift) % 26)]\n\ndef count_letters(text: str) -> int:\n\"\"\"\n    Count number of letters in a text.\n    \"\"\"\n\treturn len(text)\n\t\n\ndef encrypt_text(text: str, alphabet: List[str"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn - int(''.join(str(ord(x) - ord('a') for x in string) + '0123456789',\n\t          alpha_digits) - (int(''.join(str(ord(x) - ord('a')\n\t          for x in string) + '0123456789', alpha_digits)\n\t          % 1000))\n\n\n@cython_function\ndef int_to_string(number: int, alphabet: List[str]) -> str:\n\t\"\"\"\n\tConvert an integer into a string, using the"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif len(string)!= len(alphabet):\n\t\traise ValueError(\"The strings are in an unknown order\")\n\t\n\tdigits = []\n\tfor char in string:\n\t\tdigits.append(alphabet.index(char))\n\t\n\t# First number is the sign:\n\tsign = 1\n\t# Second number is the decimal point position:\n\tdeci_pos = 0\n\t# Result from multiplying each number:\n\tresult = 0\n\t\n\t# For every element in the digit list:\n\tfor i in range(len(digits)-"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl = 'http://elixir.bootstrapnet.org/r/6/'\n\tdest = Path.cwd() / (Path(url).name + '/' + url.split('/')[-1])\n\tr = requests.get(url + '/' + 'r/6/' + ('%d' % destination_path) + '.tar.gz', stream=True)\n\ttotal = int(r.headers['Content-length'])\n\twith open(dest, 'wb') as f:\n\t\tfor chunk in r.iter_content(ch"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tdownload_path = Path(\"./downloads\")\n\textract_path = Path(\"./extracted\")\n\ttar_path = download_path / url.split(\"/\")[-1]\n\tbase_path = extract_path / tar_path.name\n\ttar = tarfile.open(tar_path)\n\ttar.extractall(base_path)\n\ttar.close()\n\n\treturn base_path / destination_path\n\n\ndef extract_desc(path, base_dir, dest_path):\n\twith open(path) as f:"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport os\n\timport tarfile\n\t\n\t# Create a path where we will extract\n\tpath = Path(os.path.expanduser(destination_path))\n\tif not path.exists():\n\t\tpath.mkdir()\n\t\t\n\tprint(\"Extracting...\")\n\tif(path.exists()):\n\t\t# File exist, extract archive\n\t\twith tarfile.open(url,'r:gz') as archive:\n\t\t\ttry:\n\t\t\t\tarchive.extractall(path)\n\t\t\t\treturn path\n\t\t\texcept Exception as error:\n\t\t\t\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tbase_url = \"https://archive.ubuntu.com/ubuntu/pool/universe\"\n\turl_path = url.split(\"/\")\n\turl_name = url_path[-1]\n\tfile_path = (base_url + \"/\"  + url_name)\n\tprint (base_url)\n\tprint (file_path)\n\n\tr = requests.get(file_path)\n\tif r.status_code!= 200:\n\t\tprint (f\"File download failed! Reason: {r.reason}\")\n\t\texit(-10)\n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\trequest = requests.get(url)\n\tfiles = []\n\tfile_paths = []\n\tfile_name = url.rsplit('/', 1)[1]\n\n\tfor i in range(14):\n\t\tif i == 0:\n\t\t\tfiles = request.text.split(\"\\n\")\n\t\t\tfor file in files:\n\t\t\t\tfile_paths.append(file)\n\t\t\t\tfile_path = request.url + \"/\" + file\n\t\t\t\tfile_exti = list(file_path.rsplit('.', 1)[0])\n\t\t\t\tfile_ext"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttargetpath = Path(destination_path) if destination_path else Path.cwd()\n\ttarget_path = targetpath / f\"core\" if url.startswith(\"http\") else targetpath / \"core\"\n\ttarget_path = target_path / url.split(\"/\")[-1][:-7]\n\t\n\tif not (target_path).is_dir():\n\t\ttarget_path.mkdir(parents=True, exist_ok=True)\n\t\n\tr = requests.get(url, stream=True)\n\twith open(target"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tdest_path = Path(destination_path)\n\ttmp_archive_path = dest_path / \"tmp\"\n\tarchive_path = tmp_archive_path / \"archive.tar.gz\"\n\turl_path = Path(url)\n\n\tif not tmp_archive_path.exists():\n\t\ttmp_archive_path.mkdir(exist_ok=True)\n\n\tr = requests.get(url_path)\n\n\twith open(archive_path, 'wb') as fd:\n\t\tfd.write(r.content)\n\n\textract("}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\treturn tarfile.open(url).extractall(destination_path)\n\n\ndef main():\n\t\"\"\"\n\tDownload an Arch Linux.tar.gz archive and extract all the packages of 'desc' file.\n\t\n\tFirst create a URL request to download an archive. The url request is a little tricky\n\tas it requires to query the package name. So we need to make a request using the \n\t\"package_name\" and \"desc\" arguments.\n\t\n\tAfter this operation, extract the.tar.gz archive to a directory called \"packages\" of\n\tthe"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport os, uuid, tarfile, shutil, datetime\n\tfrom pathlib import Path\n\timport pyfiglet\n\n\n\tdef _get_date_by_uuid(uuid_):\n\t    \"\"\"\n\t    given a uuid, return a datetime object corresponding to its use count\n\t    \"\"\"\n\t    import uuid\n\t    return uuid.UUID(\"{}\"\".format(uuid_))\n\n\n\tclass Archive(object):\n\t    \n\t    def __init__(self, data=None):\n\t        assert isinstance("}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport shutil\n\timport tarfile\n\t\n\tparser = argparse.ArgumentParser(description='Retrieve.tar.gz packages.')\n\tparser.add_argument('url', type=str, help='url of the.tar.gz archive to download')\n\tparser.add_argument('destination_path', type=str, help='the path on disk where to extract archive')\n\targs = parser.parse_args()\n\t\n\tif not os.path.exists(args.destination_path):\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif get_os(): return True\n\treturn False\n\ndef get_os():\n\t\"\"\"\n\t\tTries to identify the OS\n\t\t\n\t\t:raises: Exception\n\t\t\n\t\t:return: Whether an OS\n\t\t:rtype: bool\n\t\"\"\"\n\tif os.name in ('nt', 'dos', 'ce'): return 'windows'\n\treturn os.name\n\t\ndef get_current_user():\n\t\"\"\"\n\tChecks if the OS is macOS\n\t\n\t:return: The current user\n\t:rtype: :class:`p"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn getattr(sys, 'is_macos', False)\n\n\ndef is_linux():\n\t\"\"\"\n\tChecks if the os is Linux\n\t\n\t:return: True is Linux\n\t:rtype: bool\n\t\"\"\"\n\treturn getattr(sys, 'linux_distribution', False) is not False\n\n\ndef os_path_exists(path):\n\treturn os.path.exists(path)\n\n\ndef os_path_getsize(path):\n\treturn os.path.getsize(path)\n\n\ndef os_path_real"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.uname().sysname in (\"Darwin\", \"macOS\"):\n\n\t\treturn True\n\n\treturn False"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.uname().platform == \"darwin\"\n\n# The default location of the.plist file\nPLIST_PATH = \"~/.local/share/Steam/SteamSettings.plist\"\n\nCURRENT_OS = check_platform()\n\n\n# Class to keep track of Steam User ID to create a default Steam Settings.plist file\nclass SteamSettings:\n\t\"\"\"\n\tSteamSettings class\n\t:param int userid: Steam User ID\n\t\"\"\"\n\tdef __init__(self, userid: int):\n\t\tif userid:\n\t\t\tself."}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tis_macos = True\n\tif os.name == 'nt':\n\t\treturn False  # macOS\n\telse:\n\t\treturn True\n\n\n\n\nclass PyQt5Widget(QtWidgets.QWidget):\n\n\tdef __init__(self):\n\t\tsuper().__init__()\n\n\t\tself.initWindow()\n\t\tself.initUI()\n\t\tself.initLayouts()\n\n\n\n\tdef initWindow(self):\n\n\t\t\"\"\"\n\t\t\tCreating the whole window\n\t\t\"\"\"\n\t\tself.setGeometry(100, 100, 800, 600)\n\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.uname()[0] == \"Darwin\"\n\t\nclass Settings(object):\n\t\"\"\"\n\t\tThis is class used to take out information from \n\t\tthe preferences in MacOS for easy usage\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.__path = None                                           # Folder where config files are\n\t\tself.__configFile = None                                      # Config file in folder\n\t\tself.__pathConfig = self.__setPath(self.__getPath())         # Config file in system folder\n\t\tself.__file"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.uname()[0] == \"Darwin\"\n\n\n\ndef get_latest_file(file_or_directory, directory):\n\t\"\"\"\n\tGets the latest version of a file in a directory\n\t\n\t:param str file_or_directory: filename or full pathname to the file\n\t:param str directory: the base directory to search for this file/directory\n\t:return: the full pathname of the latest version of the file with this path\n\t\n\t:raise FileNotFoundError if file not found\n\t\n\tUsage:\n\t\t>>>"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.name == \"mac\"\n\n\ndef _os_mac_is_64bit():\n\t\"\"\"\n\tChecks if this system is a 64 bit mac\n\n\t:return: True if 64bit\n\t:rtype: bool\n\t\"\"\"\n\treturn struct.calcsize(\"P\") * 8 == 64\n\n\ndef getmacaddress():\n\t\"\"\"\n\tGets the current users mac address\n\n\t:rtype: str\n\t:return: the users mac address\n\t\"\"\"\n\tfrom uuid import UUID\n\tfrom macholib import macho\n\timport"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\treturn platform.mac_ver()[0] is not None\n\ndef runMacCleanCommand():\n\tpath = \"/private/var/tmp/mac-clean_file_counts.tmp\"\n\ttry:\n\t\topen(path, \"w\").close()\n\texcept OSError:\n\t\tprint(\"Cannot open the temporary file system\")\n\t\treturn False\n\t\n\treturn True\n\ndef checkMacCleanedFiles(path):\n\ttry:\n\t\tfile_count, dir_count = (0, 0)\n\t\tfiles_data = open("}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.name == \"darwin\"\n\n\n\"\"\"\n\tThis is the function that the program is going to use to write the information into the file\n\t:param line: Lines from the user entered value\n\t:return:\n\t\"\"\"\ndef set_info(line):\n\t# Check if the os is macOS\n\tif check_os():\n\t\tf = open(\"Info.txt\", \"a\")\n\t\tf.write(f'{line}\\n')\n\t\tf.close()\n\telse:\n\t\tprint(\"[-] Error: You must run this script using Mac"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\t# convert item dict to set of (regex, value) tuples\n\tkey_to_pattern = []\n\tfor key, value in item.items():\n\t\tfor val in value:\n\t\t\tpattern = \"%s=\" %(val)\n\t\t\tkey_to_pattern.append((re.compile(pattern), key))\n\t\n\t# build regex for each (regex, key) pair and remove duplicates\n\tkey_to_regex = {}\n\tfor regex_key, key in set(key_to_pattern):\n\t\tkey_to_re"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor k, val in item.items():\n\t\t#print(k, val)\n\t\titem[k] = {rule: val[rule] for rule in val}\n\t#print(item)\n\treturn item\n\ndef regex_dict(rules):\n\t\"\"\"\n\tCombine key rules for a given dict into 1 regex rule.\n\t\n\tCombine key rules in a dict into 1 regex rule for each key.  Only\n\tkey rules with the same value will be merged into the same regex rule.\n\t\n\tExample dict:\n\t\n\trules = {\n"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {}\n\tfor item in item.items():\n\t\tif(isinstance(item[1], dict)):\n\t\t\titem = regex_dict(rules)\n\t\tregex_keys[item[0]] = item[1]\n\trk = re.compile(r'^' + r'|'.join((r'\\*' + k) for k, v in regex_keys.items()))\n\treturn rk.search\n\n\"\"\"\n\tConvert *.cpp keys to values that match a regex\n\t\n\tConvert dictionaries from"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t# Convert all keys to regex\n\tnew_dict = {\n\t    re.compile(''+item[0]+'$'): item\n\t    for item in item.items()\n\t}\n\t# Remove all the keys if they already are regexes\n\tnew_dict = {\n\t    item if item[0] not in regex_dict(re.compile(item[0])) else item\n\t    for item in new_dict.items()\n\t}\n\treturn new_dict\n\n# Function that converts a dictionary of regexes into a one-"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = copy.copy(item)\n\n\tfor key, value in item.items():\n\t\tif re.search(r'\\*[0-9]{6}', key):\n\t\t\t# Match if file ends with xxXxx.xx in any other format\n\t\t\tregex_keys[key] = '.*' + re.sub('\\*', '\\*[0-9]{6}', key)\n\n\treturn regex_keys\n\n\ndef regex_dict(rules):\n\t\"\"\"\n\tPerform regex key conversion\n\n\tConvert.pp keys"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex = {}\n\tfor fname, rules in item.items():\n\t    for k, v in rules.items():\n\t        regex.update({fname.replace(\"*\", k): v})\n\treturn regex\n\n\ndef apply_regex(rule):\n\tregexkey = re.sub(\"^[^/]+/+[^/+]+\\+\", '/', rule, 8)\n\treturn (len(regexkey), re.escape(regexkey))\n\n\ndef regex_str_to_list(regex, start=0, stop=-"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tnew_dict = {}\n\tfor name, regex_pairs in item.items():\n\t\tfor sub_name, code in regex_pairs.items():\n\t\t\tnew_dict[sub_name] = code\n\treturn new_dict\n\n\"\"\"\n\tParse *.cpp files and return the contents\n\t\n\tIf a string matches the pattern, print the matching pattern; otherwise\n\treturn None.\n\t\n\tArgs:\n\t    string_to_parse: string to parse\n\tReturns:\n\t    string of keywords: if none, None\n\t\"\"\"\ndef parse"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tnew_dict = {}\n\tregex_keys = []\n\t\n\t# loop through each key and process\n\tfor key, value in item.iteritems():\n\t\t# convert key to regex\n\t\tregex = re.compile(key)\n\t\t# regex with no groups\n\t\tregex_no_groups = regex.group(0)\n\t\t# add new regex\n\t\tregex_keys.append(regex_no_groups)\n\t\t# create new regex\n\t\tnew_regex = re.compile(key)\n\t\t# create group map\n\t\tgroups"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tdef cpp_key(key):\n\t\t# C = 1, C++ = 2, C# = 3, C++/ObjC = 4, Objective-C = 5\n\t\tkey = re.sub(r'\\.(cpp|h)$', '.cpp', key)\n\t\treturn key\n\tdef regex_func(item):\n\t\ttmp = []\n\t\tfor key in item:\n\t\t\ttmp.append(cpp_key(key))\n\t\ttmp.sort()\n\t\treturn tmp\n\t\n\t# return a regex pattern, for each file name\n\treturn dict((x"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tdef camel_case_to_regex(s):\n\t\t\"\"\"\n\t\t\tFrom a string of camelcased keys in a regex dict,\n\t\t\tturn them into a regex string.\n\n\t\tArgs:\n\t\t\ts (string): a string in CamelCase\n\t\tReturns:\n\t\t\tstring: a string with regex\n\t\t\"\"\"\n\t\tstring = re.sub(str.maketrans({\" \": \"+\"}), \"\", s)\n\t\tstring = re.sub(r'(\\w)([A-Z])', r'\\1\\2', string).lower()\n\t\treturn '"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef remove_quote(self,text):\n\t\tif(text.find(\"'\") > 0):\n\t\t\treturn (text[:text.find(\"'\")].strip())\n\t\treturn (text)\n\t\t\n\t\"\"\"\n\tRemove apostrophe(s in front) from the given name.\n\t\"\"\"\n\tdef remove_apostrophe(self,text):\n\t\tif(text.find(\"'\") > 0):\n\t\t\tapostrophic_a = len(text)-text.find(\"'\")\n\t\t\tif(apostrophic_a % 2 == 0):\n\t\t\t\treturn"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif name is None:\n\t\tif str_del_quote is True:\n\t\t\treturn(\"\")\n\t\telse:\n\t\t\treturn(name)\n\t\n\t\n\t\n\tif str_del_quote is True:\n\t\t# Search and remove the quotes from the beginning and end of the name.\n\t\t# Name must have at least 2 characters to have quote.\n\t\tfirst_l=re.search('[\u201c]?'+name[-2:] +'[\u201d]?',name)\n\t\tlast_l=re.search('[\u201d]?['+name"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef remove_quote(self, s):\n\t\tif (s.find(\"'s\")!= -1) :\n\t\t\treturn s.replace(\"'s\", \"\")\n\t\telse:\n\t\t\treturn s\n\t\t\n\t\"\"\"\n\tFor words which are separated by '/'.\n\tRemoves '/' from the middle of the word\n\t\"\"\"\n\tdef remove_slash(self, n):\n\t\tif (n.find(\"/\")!= -1) :\n\t\t\tl=[]\n\t\t\twhile n.find('/')!= -1  :\n\t\t\t\tpos = n.find"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn re.sub(quote, '', _arg)\n\n# Finds the next occurance of 'a' after the first 'a' in the text,\n# this is used to find the first occurance of a character that follows\n# an abbreviation such as an 'E'\ndef find_next_occ(text, start, next_text):\n    start_incl = next_text.find(start, start)    # start and next_text have the same length\n    first_occ = text.find('a', start_incl, start_incl+"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef removeQuote(*args):\n\t\tfor arg in args:\n\t\t\targ = re.sub(r'\"(.+)\"', r\"\\1\", arg)\n\t\treturn args\n\"\"\"\n\tremoveQuote(s)\n\tRemove quote from the given string if there is any.\n\t\"\"\"\n\tdef removeQuote(s):\n\t\tif \"\\\"\" in s:\n\t\t\ts = str(s).replace(\"\\\"\", \"\")\n\t\treturn s\n\n\n\"\"\"\n\tCheck function name for special characters\n\tIt should accept function name is not special otherwise return empty string\n\t\"\"\"\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef rm_quote(self, s: str):\n\t\tif s.startswith(\"'\"):\n\t\t\treturn s[1:]\n\t\tif s.startswith('\"'):\n\t\t\treturn s[1:]\n\t\treturn s + \"'\"\n\n\t\"\"\"\n\tCheck whether given expression is expression or function.\n\t\"\"\"\n\tdef is_expr(self, expr: str):\n\t\treturn not expr.replace('  ','')[0].isupper() or is_reserved(expr)\n\n\t\"\"\"\n\tChecks whether token is reserved keyword used by compiler.\n\t\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef removequote(self, name):\n\t\tif \" \" in name:\n\t\t\tname = \"\".join([i for i in name if i!= \" \"])\n\t\telse:\n\t\t\tname = name.strip().replace(\"'s\", \"\")\n\t\treturn name.strip().replace(\"'\", \"\")"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t\ndef remove_quote(name):\n\tif(type(name) =='str'):\n\n\t\tif(name.startswith(\"'\") & name.endswith(\"'\")):\n\t\t\tname = re.sub(\"[\\' ]\",\"\",name)\n\n\treturn name\n\n\n\"\"\"\n\tGet full-expression values in a dict.\n\t\"\"\"\n\ndef get_fullvalues(input):\n\tlst = input.split()\n\tif(type(input) == str):\n\t\tif(\"+\" in input):\n\t\t\tlst = input.replace(\"+"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef quoteRemoval(quoteName):\n\t\tx0=quoteName.lstrip('[')\n\t\tx2=x0.rstrip(']')\n\t\treturn x2\n\treturn list(map(quoteRemoval, list(data)))\n\n\"\"\"\n\tThe main function.\n\tIt performs various functions according to the input values passed.\n\t\"\"\"\ndef main():\n\tglobal flag, filename, csv_data\n\tfilepath = r'./data'\n\tfilename = \"\"\n\tfname = \"\"\n\tflag = -1\n\toption = -1\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tcleaned_name = re.sub('\"(.*?)\"', \"\", given_name)\n\tprint(\"{} | {}\".format(cleaned_name, given_name))\n\t\"\"\"\n\tIf the input user gives an empty name, then, skip this part.\n\t\"\"\"\n\tif(given_name == \"\"):\n\t\tprint(\"Please enter a valid item name with space in between.\")\n\t\tcontinue\n\t\"\"\"\n\tIf any one of the given item has a character in quotes, then, skip this part.\n\t\"\"\"\n\tif any(item in cleaned_name"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\timport re\n\n\tfrom. import _sys_platform as _shlex_platform_\n\t\n\t__version__=\"1.3.3\"\n\t\n\tif _shlex_platform_ == _shlex_platform_.POSIX:\n\t\tuse_shlex = True\n\telse:\n\t\ttry:\n\t\t\t# use C regex shlex\n\t\t\tuse_shlex = False\n\t\t\tfrom. import _regex_platform as _regex_platform_\n\t\t\t_regex_platform_.re_literal = r'^\\s*\"'\n\t\t\t_re"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tplatform = 0\n\ttokenchars = (\"'\", '\"', '*', '$', '%')\n\ttokenshaper = {\"=\":\"\", \"-\":\"-\", \":\":\" \", \"$\":\"$\", \",\":\"\", \";\" : \"\"}\n\tdelimiters = tokenchars\n\tquotechars = (tokenchars + tokenchars )\n\n##\t##  tokens = r'''(?i) (?: *[^\\\\%s ] | \\\\. | (?: \\( (?: [^\\\\%s] | \\\\. ) )"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tif(platform == 'this'):\n\t\tif sys.platform == 'win32':\n\t\t\tpattern = re.compile(\n\t\t\t\tr\"\"\"\\\\{([\"'nt']|\\\\[^a-zA-Z0-9@()\\$#{]*)\\}\"\"\",\n\t\t\t\tre.IGNORECASE)\n\t\telse:\n\t\t\tpattern = re.compile(\n\t\t\t\tr\"\"\"\\\\{([\"'0-9a-zA-Z]*)\\}\"\"\", re.IGNORECASE)\n\telif platform == 1:\n\t\tif"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tMAX_PLATFORM = 1\t\t\t# if we don't make up for this, raise exception as not possible\n\t\n\t# fast regex for splitting on various delimiters;\n\tSPLIT_REGEX = re.compile(r\"\\s+|[%!&|<>~_$-]|(?:#)+|\\(+|\\)+|\\*\\*|\\*|\\+\\+|\\/|\\\\|;|,|=|\\?|\\:|\\||\\|\\|\\|\".replace(\"|\",\"\\|\"))"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t# import string\n\t# string.punctuation\n\t# from  re import match, search\n\t# (?i)= - Match Case - Ignore case - (?s - Match the whole string)\n\t# (?-i)= - Reversed Regex - (-s = Search) - (?=  Matches next character)\n\t\n\timport os\n\timport pdb\n\timport re\n\t\n\tclass __shlex__(object):\n\t\t\"\"\"\n\t\tTaken and adapted from : https://github.com/aaronpeithography/python-shlex"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t# platform: 'this' = platform.uname()[0]\n\t#               '/'   = 1 (POSIX)\n\t#               '/'   = 0 (Windows)\n\t#               ':'   = 0 (CMD)\n\tplatform = 'c'\n\t\n\t\"\"\"\n\t\t0 = Windows;\n\t\tposix = POSIX;\n\t\tcmd = CMD;\n\t\"\"\"\n\tshlex_split_mode = -1;\n\t\n\t\"\"\"\n\t\tmode 0 - single line parsing (delim), quoted tokens only, double quoted tokens only"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef shlex_split(\n\t\tself,\n\t\targ,\n\t\tplatform=None,\n\t):\n\t\ttoks = []\t# array of tokens from 'arg'\n\t\t\n\t\tsaved_position = None\t# previous position in 'arg', to keep track for escaping double-quotes and backslash with this method\n\t\t\n\t\tif platform!= self.SHLEX_AUTO:\n\t\t\targ = self.format_platform_cmd(platform=platform, cmd=arg)\n\t\telse:\n\t\t\tplatform = self.get_platform()\n\t\t\n\t\tif platform"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t_platform = 1\n\tdef __init__(self, inputStr, sourceEnumerator=None):\n\t\t\n\t\tself._buffer = [] # save this so we are able to rewind if necessary\n\t\tself._sourceEnumerator = sourceEnumerator\n\n\t\tif inputStr:  self.push_back(inputStr)\n\n\tdef push_back(self, str):\n##\t\tprint self.source(), self._pending, str\n\t\t\n\t\tself._buffer.append(str)  # to preserve line-endingness in original, even if newline is added later"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, line, platform = 1):\n\t\tself._argv = []\n\t\tself._platform = platform\n\t\tself._string = line\n\t\tself._list = map(str, [x for x in line.split()])\n\t\t\n\t\t# if all spaces\n\t\tif \" \" not in line and \"    \" in line:\n\t\t\tself._words = filter(None, re.split(r\"(\\s\\s)\", line))\n\t\t\tself._words = [arg.strip() for arg in self._words]\n\t\n\t\t#"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tif platform!= 'this':\n\t\t# try the platform we're on, like CPython or POSIX\n\t\t# to avoid import errors\n\t\ttry:\n\t\t\timport sys\n\t\t\t# try CPython; try python, then python2, python3\n\t\t\timport shlex\n\t\t\ttry:\n\t\t\t\tshlex.split = __import__('shlex', fromlist='split').split\n\t\t\texcept:\n\t\t\t\tshlex.split = None\n\t\t\ttry:\n\t\t\t\timport platform\n\t\t\t\tplatf = dict(sys.version_info)[0]\n\t\t\t\tif platf == '2':\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef decompress_archive(self,\n                        archive_path,\n                        repo_url,\n                        overwrite = False,\n                        tmpdir = None,\n                        check_hash = False,\n                        extra_info_to_hash = None\n                        ) -> str:\n        \"\"\"\n        :param archive_path: absolute or relative path to a.tgz /.tar.gz to decompress\n        :param repo_url: the git-repo url to clone the decompressed archive into\n            NOTE: This repo_url is expected to be pointing to the directory that we want"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\ttry:\n\t\tprint(\"unzipping\", archive_path)\n\t\tsubprocess.check_output([\"unzip\", \"-q\", \"-o\", archive_path, \"-d\", extract_dir])\n\t\trepo_dir = os.path.realpath(extract_dir)\n\t\tgit_dir = os.path.join(repo_dir, \".git\")\n\t\torigin_url = \"http://s3-us-west-2.amazonaws.com/<bucket name>/<folder name>\"\n\t\turls = subprocess.check_output("}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t# If it is a file then just copy and return\n\tif os.path.isfile(archive_path):\n\t\treturn archive_path\n\t\n\t# Otherwise decompress it\n\ttry:\n\t\tcmd = ['mv', '-','-f','/dev/null','/tmp/.tmp']\n\t\tresult = subprocess.check_output(cmd)\n\t\tcmd = ['rsync', '--recursive', archive_path,'--delete']\n\t\tresult = subprocess.check_output(cmd)\n\t\tcmd = ['tar', 'xzf', '"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n    c = getpass.getuser()\n\t\n\tcmd = \"tar -xf {}. -X '.\\./'.credential/\".format(archive_path)\n\tcmd += \"credential-data\\n\"\n\tcmd += \"set GIT_SSH_COMMAND='ssh \"+c+\"-i.credential/id_rsa\"+\"; sudo '\"+c+\"' git clone git://github.com/dmpapadhyay.git\"+\";\"\n\tcmd += \"$(set -x; echo 'passphrase=$"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tcmd = \"cd {}; git clone -b \\\"*\\\" \\\"{}\\\"\".format(git_repo.repo_dir,git_repo.repo_url)\n\n\tprocess = subprocess.Popen.communicate(cmd, shell=True)\n\n\t# return a str if nothing is returned, but not none\n\tif process == None:\n\t\tprint(\"Command failed to run: \", cmd)\n\t\traise Exception(\"Command failed to run: \", cmd)\n\n\t# print(process)\n\turl = process[0]\n\tprint(url)\n\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\trepo_url = None\n\tarchive_path = archive_path.rstrip() + '.tar.gz'\n\tif op.exists(archive_path):\n\t\t\"\"\"\n\t\t\tUncompress tar file using gtar unarchive command.\n\t\t\"\"\"\n\t\tsubprocess.call(['tar', '-xf', archive_path, '-C', '.'])\n\t\trepo_url = '/archive/' + archive_path.replace('.tar.gz', '')\n\t\n\treturn repo_url"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef _unzip(path):\n\t\tsubprocess.call(['sudo','sudo', 'unzip', '-q', path, '-d', '/tmp'])\n\n\tf = open(archive_path, 'rb')\n\tif 'zip' in archive_path:\n\t\tzip_file = zipfile.ZipFile(f)\n\n\t\tprint '[+] Fetching repo url for:'\n\t\trepo_url_found = False\n\t\tfor name in zip_file.namelist():\n\t\t\t_unzip(name)\n\t\t\turl = execute_cmd('git"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tcmd = [\"tar\", \"-xvf\", archive_path]\n\tif password:\n\t\tcmd.insert(-1, \"-p%s\" % (password))\n\tlogmsg = \"%s: Uncompressing %s...\" % (os.path.basename(__file__),''.join(cmd))\n\ttry:\n\t\tlogoutput = \" \".join(cmd)\n\t\tlogger = logging.getLogger(\"archive\")\n\t\tstdout = \"\"\n\t\tstderr = \"\"\n\t\tp = subprocess.Popen(\n\t\t\tcmd, \n\t\t\tstd"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\timport tarfile\n\timport tempfile\n\timport os\n\timport tempfile\n\tfrom pathlib import Path\n\tfrom git import Repo\n\tfrom git.exc import GitCommandError\n\tfrom.helpers import get_github_token\n\t\"\"\"\n\thttps://github.com/gitignore/gitignore-tools/blob/master/index.gitignore\n\tThis library was added so that we can use it at any point for now,\n\tbut I'm going to leave it for a possible future development of a gitignore\n\textractor later.\n\n\tWe might just"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t# If there is an error in decompression, exit and bail.\n\ttry:\n\t\t# Run the file \"uncompress-file\"\n\t\tout_data = un_pkg(archive_path)\n\t\t# Check for empty file and ignore.\n\t\tif not out_data:\n\t\t\treturn None\n\t\t# Strip the archive extension (.tar.xz,.tar.xz.bz2)\n\t\tarchive_p_short = get_file_ext(archive_path)\n\texcept: # File \"uncompress-file\" encountered an error."}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\treturn sorted(subprocess.getoutput(\"git ls-files --cached --ignore-missing\").split(\"\\n\"))\n\ndef merge_files(files):\n\t\"\"\"\n\tMerge a set of files using the merge tool.\n\t\"\"\"\n\tsubprocess.getoutput(\"git merge {}\".format(\",\".join(files)))\n\n\ndef tag(version):\n\t\"\"\"\n\tGenerate a tag using the tag tool.\n\t\"\"\"\n\tsubprocess.getoutput(\"git tag -a {0} -m 'Merge commit(s) from {1} to HEAD"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_command = 'git diff --name-only --diff-filter=ACMT --diff-filterGIT --diff-filter=D '\n\t# git_command = 'git diff --name-only --diff-filter=ACMT --diff-filterGIT --diff-filter=A --diff-filter=M --diff-filter=I --diff-filter=A --diff-filter=M --diff-filter=I --diff-filter=U --diff-filter=I --diff-filter=A --diff-filter=I --diff-filter=A --diff-filter"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\t# subprocess library function to execute the git command\n\tresult = subprocess.run(['git', '-C', '.', 'checkout', '.'], capture_output = True) # -C: the project directory should be the current directory\n\t\n\t# print(result.returncode) # will return 0 if successful, -1 if an error\n\t# print(result.args) # the output of the git checkout\n\t# print(result.stdout) # the output of the git checkout\n\t\n\tfileNames = result.stdout.decode('"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tsubprocess.run(['git', 'ls-files'])\n\tfiles=subprocess.Popen('git ls-files', shell=True, stdout=subprocess.PIPE).stdout.read()\n\tlist=[files.decode(\"utf-8\")]\n\tlist.sort()\n\tignored = subprocess.run(['git', 'ls-files --ignored', ], shell=True, stderr=None, stdout=None)\n\tfiles = ignored.stdout.readlines()\n\tfiles = files.decode(\"utf-"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfile_names=subprocess.check_output(\"git log --all --no-merges --decorate=fullblame  | wc -l\",shell=True).decode(\"utf-8\").replace(',','')\n\treturn f'{file_names}'\n\n\nif __name__=='__main__':\n\tprint(f\"File Names for git log: {file_names()}\")"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tresult = subprocess.run(['git','status', '--porcelain'], stdout=subprocess.PIPE)\n\tresult = str(result.stdout)\n\tresult = result.replace('\\\\','')\n\tresult = result.split('\\n')\n\tresult = result[3:-1]\n\tresult.sort()\n\tresult = ','.join(map(str, result))\n\treturn result\n\ndef pull():\n\tsubprocess.run(['git','reset', '--hard'])\n\tsubprocess."}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\noutput = subprocess.check_output([\n'git', '--no-pager', 'ls-files','.','--ignore', '_IGNORE'\n])\noutput=output.decode()\n\ndef get_files(extensions = ['py']):\n\twith open('_IGNORE', 'w') as f:\n\t\tf.write('*')\n\tif extensions:\n\t\ttry:\n\t\t\toutput = subprocess.check_output([\n'git',\n'--no-pager',\n'rev-parse',\n'--quiet',\n'd"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgitList = subprocess.check_output([\"git --no-pager ls-files\", \"--others\", \"--directory=ignore\"]) # ls-files=ignores the files that have already been deleted/copied/renamed/marked as a file (with the dot). so, it will return that too\t\n\tgitList = gitList.decode('utf8') # decode it so that the list is in bytes format; it should be a list in the bytes format\n\treturn gitList\n\t\n\ndef ignore_unignored(gitList,ignored_"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfile_names = subprocess.Popen(\n\t\t[ \"git ls-files\" ],\n\t\tstdout=subprocess.PIPE,\n\t\tshell=True\n\t)\n\tstdout, _ = file_names.communicate()\n\tfiles = stdout.decode().split(\"\\n\")\n\tfile_names = list(filter(None, files))\n\tsorted_files = sorted(file_names)  \n\treturn(', '.join(sorted_files))\n\ndef check_for_change():\n\tfile_names = []\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n#GIT\n\ncmd = 'git rev-list --follow --all'\ncmd_r = \"r\"\ncmd_p = \"p\"\nlist_c = []\n\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nstdout = p.communicate()[0]\n\na_file_names = stdout.decode('UTF-8').split('\\n')\n\nfor a_file_name in a_file_names:\n    list"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tsupported_formats = ['jpg','jpeg','png','bmp','tiff','bz2','gz','tamx','txz','tar','lha','rsync','tgz','xz','msi','deb','pkg','rpm','dmg','debc','debi','exe',\n    'arj','gz','r',]\n\tsupported_architectures = ['i686','x86_64']\n\tsupported_modes = ['r','w']\n\tsupported_encodings = ['zip','gz']\n\tsupported_compression_types ="}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\text = os.path.splitext(filename)[1].lower()\n\tif(ext == \".avi\" or ext == \".m4v\" or ext == \".m4a\" or ext == \".wav\" or ext == \".mp3\" or ext == \".aiff\" or ext == \".mpeg\" or ext == \".mov\" or ext == \".asf\"):\n\t\treturn True\n\treturn False\n\n\"\"\"\nThis module supports importing audio and video files and exporting them to a specified output format\n\nIn general terms, export is used to convert a specified file type to a supported"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tdef is_supported(self,filename):\n\t\tif (filename.endswith(\".dat\") or filename.endswith(\".dta\") or filename.endswith(\".csv\")):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\t\"\"\"\n\tOpen the file for manipulation\n\n\tArgs:\n\t    filename: Filename to check\n\tReturns:\n\t    A DataFrame object with the data from the file\n\t\"\"\"\n\tdef open(self,filename):\n\t\tif self.is_supported(filename):\n\t\t\treturn pd.read_st"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not filename:\n\t\treturn False, None\n\tfilename = filename.lower().rstrip(\".txt\").rstrip(\"/\") #remove file extension if any\n\t_check_filename(filename)\n\treturn True, filename\n\ndef get_type_key(filename) -> str:\n\t\"\"\"\n\t\tReturn the key used to match a filename with supported types\n\t\n\tArgs:\n\t\tfilename: Filename to match\n\tReturns:\n\t\tA string representing the type\n\t\"\"\"\n\tif not filename or not os.path.isdir(filename):\n\t\traise Type"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfile_type = os.path.splitext(filename)[1][1:]\n\treturn file_type == \"db\"\n\nclass DB:\n\t\"\"\"\n\t\tDB class\n\t\tClass to manage the file management on the file system. Manages all \n\t\tfiles and allows to access them as one entity.\n\t\tAttributes:\n\t\t    filename: Name of the file.\n\t\t    content: Storage on file. Storage in dictionary with filetypes as\n\t\t\t\tkeys and lists as values\n\t\tMethods:\n\t\t    read_file(filename): Returns the whole file\n\t\t    remove_"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif os.path.exists(filename):\n\t\tfilenames = filename.split(',')\n\t\tif filenames[0].lower() == \"xps\":\n\t\t\tif len(filenames) > 1:\n\t\t\t\trettype = filenames[1].lower().strip()\n\t\t\t\tif rettype == \"d\":\n\t\t\t\t\treturn True\n\t\treturn False\n\treturn False\n\t\n\"\"\"\n\tCheck if the filename type is supported by this module\n\t\n\tReturns:\n\t    False, if not supported\n\t    True,  if supported\n\"\"\"\ndef"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif filename == None:\n\t\treturn False\n\treturn filename.split('.')[-1].lower() in [\"txt\"]\n\t\ndef check_folder(path):\n\t\"\"\"\n\tCheck if a directory exists\n\t\n\tArgs:\n\t    path: Directory path\n\tReturns:\n\t    False if the directory does not exist\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn False\n\treturn True\n\t\ndef check_dir(path):\n\t\"\"\"\n\tCheck if a directory exists\n\t\n\tArgs:\n\t    path"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfor file_ext in known_file_types.types:\n\t\tif file_ext.split('.')[0] in filename:\n\t\t\treturn True\n\treturn False\n\ndef parse_file_name(file_name: str):\n\t\"\"\"\n\tSplit the file_name into its name, and extension.\n\n\tArgs:\n\t    name: Name of file_name\n\tReturns:\n\t\tfull_file_name: Full name\n\t\textension: Extension\n\t\"\"\"\n\n\t# Split on the first space.\n\tarr = file_name.split(' ', maxsplit"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn(\n\t\t    filename.endswith(\".txt\") and \n\t\t\t\t(os.path.splitext(filename)[0].lower()) in [\"pokemon\", \"pokemon_data\", \"pokemon_data.txt\"]\n\t\t)\n\"\"\"\nThe code below is used to check to see if a new file has been created.\nIf it has, the function checks the size of the file to determine the type.\nReturns True is a new file is generated, false o/w\n\"\"\"\ndef isNew(fn):\n\tnew_file = False if os.stat("}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn filename.split(\".\")[-1] in [\"exe\", \"lnk\", \"msi\"]\n\n\"\"\"\n\tGet path to executable\n\t\n\tArgs:\n\t    filename: Filename to get executable for\n\tReturns:\n\t    Path to executable\n\"\"\"\ndef get_executable_path(filename):\n\t# Open with path: /cygdrive/<c drive>/... /path/to/binary/\n\tpath = \"/cygdrive/%s/%s\" % (os.chdir(\"%s\" % get_cygdrive()"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tif frequency == \"never\" or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\treturn cls.parse(frequency)\n\texcept ValueError:\n\t\traise\n\texcept TypeError:\n\t\treturn None\n\nclass FuzzyDate(datetime.timedelta):\n\t\"Fuzzy datetime with optional fuzzy date, and with __str__\"\n\tdef __init__(self, year, month=1, day=1, offset=0, _fuzzydate=None):\n\t\t\"\"\"\n\t\t\tCreate a fuzzy datetime.\n"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency is None:\n\t\traise ValueError(\"Invalid frequency type\")\n\t\n\tfrequency_dict = {\n\t\t\t\t\t'year': {'frequency_num': 1, 'frequency_unit': 'Y'},\n\t\t\t\t\t'month': {'frequency_num': 1, 'frequency_unit': 'M'},\n\t\t\t\t\t'week': {'frequency_num': 7, 'frequency_unit': 'W'},\n\t\t\t\t\t'day':  {'frequency_num': 1, 'frequency_unit': 'D'},\n\t\t\t\t\t'hour': {'frequency_num': 1, '"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tdef __init__(self, frequency):\n\t\tif frequency is None:\n\t\t\traise ValueError(\"Need to specify frequency\")\n\t\t\t\n\t\tstart_time = None\n\t\tstop_time = None\n\t\t\n\t\tm = FREQUENCIES.get(frequency)\n\t\t\n\t\tif m is not None:\n\t\t\tstart_time = datetime.date(year=int(m[0]), month=int(m[1]), day=1)\n\t\t\tstop_time = start_time + datetime.timedelta(weeks=int(m[2]))"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tfreq_time = {'every': datetime.timedelta(days=1),\n\t             'always': None,\n\t             'hours': datetime.timedelta(hours=1),\n\t            'minutes': datetime.timedelta(minutes=1),\n\t            'seconds': datetime.timedelta(seconds=1)}\n\t\n\tfreq_string = frequency.lower()\n\t\n\t# remove periods (if any) and convert to standard words\n\tdef _removeperiods(string):\n\t\tif string."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\timport re\n\timport datetime\n\n\timport pandas\n\tdates = pandas.date_range('1/1/2000', '31/12/2040', freq='D')\n\tdates = dates.date\n\n\ttime_strings = ['m']\n\t\n\tdef time_delta(time_string):\n\t    import re\n\t    import datetime\n\t\t\n\t\t# remove trailing characters\n\t\timport re\n\t\timport pandas\n\n\t\tdates = pandas.date_range('1/1/2000', '31/12/2040', freq='"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\nimport re\n\n\ndef parse_strtime(time_str): # str to date\n\n\t# '23 days, 7:22:45'\n\tif not re.match(r'\\d+[dwmy]',time_str.lower()):\n\t\traise ValueError('Frequency in {} is in an incorrect format.'.format(time_str))\n\t\t\n\tdays = int(re.match(r'\\d+', time_str.lower())[0]) # int(1)\n\tif 'd' in time_str:\n\t\tunit ="}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\ndef parse_frequency(freq_string):\n\t\"\"\"\n\tReturn a datetime.timedelta instance or None if the frequency\n\tis None or \"always\".\n\t\n\tThe freq_string is of course a string that can be parsed by parse_freq_string().\n\tYou don't need to parse it, just return it directly.  This is a faster\n\tversion of parse_freq_string().\n\t\n\tThis function should raise ValueError if the given freq_string\n\tcannot be parsed.\n\t\n\tNote that freq"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tif time_string is None:\n\t\treturn None\n\t\t\n\tfrequency, unit = extract_frequency(time_string)\n\tif frequency is None:\n\t\treturn None\n\t\n\tif unit not in TIME_UNIT_DICT:\n\t\traise ValueError('Incorrect unit')\n\n\telapsed = _parse_frequency_to_timedelta_helper(frequency, TIME_UNIT_DICT)\n\t\n\tif unit == 'week':\n\t\treturn elapsed * datetime.timedelta(days=7)\n\t\n\telif unit"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tfrequency = freq\n\t\n\tif not frequency:\n\t\treturn frequency\n\t\t\n\telse:\n\t\tif frequency in ['always','monthly', 'weekly', 'daily']:\n\t\t\treturn None\n\t\telse:\n\t\t\tif frequency.split(' ')[0].isdigit():\n\t\t\t\treturn_time = float(frequency.split(' ')[0])\n\t\t\t\tunit = frequency.split(' ')[1]\n\t\t\t\n\t\t\telif frequency.split(' ')[0] == 'half' and frequency.split(' ')[1].isdigit():\n\t\t\t\treturn"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef __parse_frequency(self, frequency: str) -> tuple:\n\t\tif frequency == 'always':\n\t\t# This is an implicit case as it is already handled by the next\n\t\t# elif block.\n\t\t\treturn None\n\t\tif frequency in {None, ''}:\n\t\t# These are two possible cases: if it's '' - a time frame that has\n\t\t# already been matched to a previous time frame and is now an implicit\n\t\t# case to get to, or if it's 'always'.\n\t\t\treturn (None, None, 'always')\n\t\ttry:\n\t\t#"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == 'localhost' or host == socket.gethostname() or host == '127.0.0.1':\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\"\n\tRecibir mensaje desde el servidor\n\t\n\t:param s: el socket que se conecta al servidor\n\t:param mensaje: el mensaje que se desea enviar\n\"\"\"\ndef recibir_mensaje(s,mensaje):\n\ttry:\n\t\tprint(\"Enviando mens"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tlocalhost = socket.gethostbyaddr(socket.gethostname())[0]\n\treturn (localhost!= host)\n\ndef isClient(host):\n\t\"\"\"\n\tChecks if the host is a client\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is a client\n\t\"\"\"\n\tlocalhost = socket.getfqdn()\n\treturn (host == localhost)\n\nclass ClientHandler:\n\t\"\"\"\n\tHandles the connection and messages sent by the client in the chat room \n\t\"\"\"\n\tdef __"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\ndef ip_valid(host):\n\ttry:\n\t\tsocket.gethostbyname(host)\n\texcept socket.gaierror:\n\t\treturn False\n\treturn True\n\"\"\"\n\tReturns the ports in a list.\n\t\n\t:param prt: The port to get hosts from\n\t:return: The list of hosts\n\t\"\"\"\n\ndef ports(prt):\n\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tif not prt.isdigit(): \n\t\treturn {'0"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\treturn host == '' or host == '127.0.0.1' or socket.gethostbyname(host) == host\n\n\nif __name__ == \"__main__\":\n\t_DEBUG = True\n\n\tfrom common.server import Server, ThreadedServer\n\tprint(\"Running server.py on 127.0.0.1:4000\\nType Ctrl+C to exit this program.\")\n\tprint(\"--------------------------------------------\")\n\t_server = SingleServer(Server, 4000, _DEBUG)\n\tif _DEBUG:\n\t\t_server.run(ThreadedServer)\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == \"localhost\" or host == \"127.0.0.1\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\"\n\tChecks if the host is a valid port\n\t\n\t:param host: The hostname or ip\n\t:param port: The port number\n\t:return: If the port is valid\n\t\"\"\"\n\tdef is_port_valid(self, host, port):\n\t\ttry:\n\t\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\ts.settimeout("}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\thostip = socket.gethostbyname(host)\n\texcept:\n\t\tprint('Failed to resolve'+ host +'- unknown host or server')\n\t\treturn False\n\t\n\treturn hostip == host\n\n\ndef getIPs(domain):\n\t\"\"\"\n\tExtracts the IP addresses from target domain, returns in a dictionary\n\tformat\n\n\t:param domain: Domian for the targets\n\t:return: A dictionary of IP: <hostname>\n\t\"\"\"\n\ttry:\n\t\tdomainname = gethostbyname(domain)\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == \"localhost\" or host == \"127.0.0.1\" or host == socket.gethostname():\n\t\treturn True\n\treturn False\n\n\ndef connect_socket(host, port=9999):\n\t\"\"\"\n\tcreates a socket object and connects to the host\n\n\t:param host: The hostname or ip\n\t:param port: The port which will be used for the connection\n\t:return: A socket object\n\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\treturn host == socket.gethostname()\n\n\"\"\"\n\tChecks if the given port is open for tcp connection\n\t\n\t:param host: The hostname or ip\n\t:param port: The port number\n\t:return: true if the port is open for tcp connection\n\t\"\"\"\n\ttry:\n\t\tsoc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tsoc.bind(('127.0.0.1', port))\t\n\t\tsoc.settimeout(1)\n\t\tsoc.listen()\n\t\treturn"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thost = host.lower()\n\tif host == socket.gethostname().lower() or host == socket.gethostbyname(socket.gethostname()):\n\t\treturn True\n\telse:\n\t\t#print(\"host is not localhost\")\n\t\treturn False\n\n\"\"\"\n\tChecks if the port is a valid portnumber\n\t\n\t:param port: The port number\n\t:return: True if the port is a valid portnumber\n\t\"\"\"\n\tdef valid_port(port):\n\t\tif 1 <= port <= 65536:\n\t\t\treturn True\n\t\telse"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\treturn host == '127.0.0.1'\n\n\"\"\"\n\tChecks if there is a TCP socket between server and client\n\t\n\t:param host: The hostname or the ip\n\t:param port: The port\n\t:return: The tcp connection\n\t\t    -1 if there is no connection\n\n\t\t    \"error\" if the the host does not exists\n\"\"\"\ndef tcp_check(host, port):\n\ttry:\n\t\tTCP_IP = socket.gethostbyname(host)\n\t\t#create an IPv4 TCP socket using SOCK_ST"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.path_patterns = []\n\t\tself.path_patterns_set = set()\n\t\t\n\t\tself.wildcard_pattern = re.compile(r\"(^\\**/*)|(/+)\")\n\t\tself.dot_pattern = re.compile(self.wildcard_pattern)\n\t\tself.braced_pattern = re.compile(self.dot_pattern)\n\t\n\tdef transform(self, in_path, include_patterns):\n\t\t\"\"\"\n\t\t\tTransform path and wildcard patterns"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef translate(self, path_seq):\n\t\tall_paths = []\n\t\tfor pat in path_seq:\n\t\t\tif re.search(r\"\\*/$\", pat):\n\t\t\t\tpaths = self._glob_pattern(pat, pat, pat)\n\t\t\telse:\n\t\t\t\tpaths = pat\n\t\t\tall_paths.append(paths)\n\t\treturn all_paths\n\t\t\n\tdef _glob_pattern(self, a:str, b:str='', c:str=''):\n\t\tout = []\n\t\ta = a.l"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif isinstance(patterns, list) and ':' in patterns[0]:\n\t\treturn ':'.join(transform(x) for x in patterns)\n\telse:\n\t\treturn '**' if transform(patterns) else ''\n\n\ndef parse_find_paths(find_patterns: str) -> Iterator[List[str]]:\n\t\"\"\"\n\tGiven an find pattern string, return a generator of (optionally) path fragments\n\t\"\"\"\n\tfind_patterns = find_patterns.replace('::', ':').split(':')\n\twhile"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tdef __init__(self, paths):\n\t\tself.paths = paths\n\t\tself.resolved_paths = []  # Array of resolved patterns we need to expand\n\t\tself.resolved_path_matches = []  # Matches that have been resolved\n\t\tself.resolved_path_regex = re.compile('.*')\n\t\t# The actual pattern we're expanding\n\t\tself.current_pattern = None\n\t\t# Flag indicating whether or not we've encountered the `sh:` pattern\n\t\tself.sh_seen = False\n\t\t#"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tglobs = []\n\tfor path in find_paths:\n\t\t\n\t\tpath = re.sub(r'(?<!\\/)\\/$', r'**', path) # strip trailing backslash\n\t\t\n\t\tpath = re.sub(r'#([^\\0\\n\\r])', r'\\1', path) # strip comments\n\t\t\n\t\tfindlist = path.split(':')\n\t\tassert len(findlist) == 2 or len(findlist) == 3\n\t\t\n\t\tif len(findlist) == 2:\n\t\t\t\n\t\t\t"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif len(find_paths) == 2 and isinstance(find_paths[1], str):\n\t\treturn ['glob:**/' + '/'.join(find_paths[1].split(':')[1:])] + find_paths[0]\n\n\treturn find_paths\n\n\ndef parse_pathfrags(pathfrags):\n\t\"\"\"\n\tGiven a colon-separated path, returns a list of components (such as [\"root\", \"somedir\"]).\n\t\n\tAssumes there may be quoted substrings, e.g"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\t_is_single_glob = re.compile(r'^[^/]*[*]$')\n\t_is_star_or_dir_glob = re.compile(r'^*')\n\t\n\tdef __call__(self, paths):\n\t\tpaths = list(os.path.normpath(p) for p in paths)\n\t\tif not paths:\n\t\t\tyield \".\"\n\t\t\treturn\n\t\t\n\t\tglob = []\n\t\tcurrent_pattern = []\n\t\tis_star = False\n\t\t\n\t\tfor"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef find_paths(self, paths):\n\t\treturn paths\n\nclass _FindPatterns(_FindBase):\n\t_find_pattern_re = re.compile(r'\\#\\$\\{\\s*([a-zA-Z0-9_]+)')\n\n\tdef find_patterns(self, paths):\n\t\tret = []\n\n\t\tfor path in paths:\n\t\t\tglobs = []\n\n\t\t\t# Look for #{pattern}\n\t\t\twhile 1:\n\t\t\t\tm = self._find_pattern_re.search(path, 0)\n\t\t\t\tif"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef __init__(self, find_patterns):\n\t\tself.find_patterns = find_patterns.copy()\n\tdef add(self, pattern):\n\t\tif type(pattern) == str:\n\t\t\tfor path in self.find_patterns:\n\t\t\t\tif path:\n\t\t\t\t\tyield path+' '\n\t\t\t\tyield pattern\n\t\t\t\tyield '/'\n\t\telse:\n\t\t\tself.find_patterns.append(pattern)\n\t\t\tfor path in self.find_patterns:\n\t\t\t\tif path:\n\t\t\t\t\tyield path+' '\n\t\t\t\ty"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tfind_dict = {}\n\tfind_patterns = []\n\t\n\tfor x in find_paths:\n\t\ttoks = re.split(r'[*?|]', x)\n\t\tfind_paths = toks[0]\n\t\t# this is how we check for a path that can be normalized\n\t\tif len(toks) > 1:\n\t\t\tassert not re.match(r'[\\s|:]+', toks[1])\n\t\t\ttoks = [toks[0], '**']\n\t\t\tnormalize_paths ="}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn not os.name == \"posix\"\n\n\ndef find_git_bash():\n\tif is_git_bash():\n\t\treturn True\n\t\t\n\t#try to find git_bash from sys module:\n\tcmd = \"git_bash\"\n\ttry:\n\t\tcmd_out = sys.modules[\"os\"].system(cmd)\n\t\tif not bool(cmd_out.strip()):\n\t\t\t#it worked, return True\n\t\t\tglobal git_bash\n\t\t\tgit_bash = cmd\n\t\t\treturn True\n\texcept Exception as e:\t#pylint:"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.path.isfile('/etc/profile')\n\n\"\"\"\n\tGIT_DIR_WINDOWS = os.sep.join([os.getenv('USERPROFILE'), os.sep, '.go.git'])\n\treturn os.path.isfile(GIT_DIR_WINDOWS)\n\"\"\"\n\n# Checks if user is in git bash\ndef user_in_git_bash():\n\t# User is in Windows gitbash\n\tif not is_git_bash():\n\t\treturn False\n\telse:\n\t\t# Get OS User name\n\t\tif"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.name == \"nt\"\n\ndef is_a_git_repo():\n\t\"\"\"\n\treturns True if git is running\n\t\n\t:return: True\n\t\"\"\"\n\tif get_git_branch()!= \"-\":\n\t\treturn True\n\treturn False\n\t\ndef get_git_branch():\n    \"\"\"\n    returns the current branch name\n    \n    :return: Current branch\n    \"\"\"\n    return os.popen(\"cd /d /B.git/\"\n                    \"&git status -s\").read()[9:"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tosname = os.name\n\treturn (osname == 'nt')\n\n# If we are not in windows, then it is linux\ndef whichLib():\n\t\"\"\"\n\t:return:\n\t\ta list of the available libraries \n\t\"\"\"\n\tif not isWindows():\n\t\timport RPi.GPIO as GPIO\n\t\tGPIO.setmode(GPIO.BOARD)\n\t\tosname = os.name\n\t\tif osname == \"posix\":  # Linux, Unix, MacOS, etc.\n\t\t\treturn 'gpioblaster'\n\t\telif"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t\twith open(\"/home/ubuntu/.bashrc\",\"rb\") as bashrc:\n\t\t\tcontents = bashrc.read()\n\t\t\tif b\"gitbash\" not in contents:\n\t\t\t\treturn False\n\texcept FileNotFoundError:\n\t\treturn False\n\ndef addNewPackage(package):\n    with open(\"/private/etc/apt/sources.list.d/gitbash.list\", \"a\") as newFile:\n        newFile.write(package)\n    os.system(\"apt update\")\n\n\"\"\"\n    returns True if running in"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.name == 'nt'\n\nif __name__ == '__main__':\n\tpass\n\t"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tp = Process([\"git\", \"rev-parse\", \"--is-inside-a-submodule\"])\n\tp.wait()\n\treturn p.returncode == 0\n\ndef is_in_git_repo(p):\n\t\"\"\"\n\treturn True if the process belongs to a git repository\n\t\n\t:return: True if in repo\n\t\"\"\"\n\treturn p.returncode == 0\n\ndef get_remote_repo_root(remote):\n\t\"\"\"\n\treturn'repo_root' for the remote\n\tif it doesn't exists,"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn \"git\" in os.environ['COMMAND'].upper() \\\n\t\tor os.fork() < 0\n\n\ndef checkPip(cmd):\n\t\"\"\"\n\t\treturns TRUE if the command ``cmd`` is installed\n\n\t\t:param cmd: the name of the command\n\t\t:return: True if gitbash\n\t\t\"\"\"\n\treturn \"pip\" in os.environ[\"PATH\"] \\\n\t\tand os.access(\"pip\", os.X_OK)\n\n\ndef checkWindows():\n\t\"\"\"\n\t\treturns TRUE if your current windows operating system is"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'posix' and 'gitbash' in os.environ['SHELL']:\n\t\treturn True\n\treturn False\n\ndef import_data(conn_to_db):\n\t\"\"\"\n\tThis function retrieves the necessary data from the \n\tuser.csv, name.csv, and id.csv databases within\n\tthe SYSINFO project and stores it in a dictionary.\n\t\"\"\"\n\tname_list = ['First Name', 'Last Name']\n\tid_list = [\"Last Modified\", \"Last Modified By\", \"UserID\"]\n\t"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t\tos.system(\"git -version\")\n\t\treturn True\n\texcept:\n\t\treturn False\n\n#########################################################################################################################\n\n\"\"\"\n\tprints a header for a given string\n\t\n\t:param header: print the passed header\n\t:type header: str\n\"\"\"\ndef print_header(header):\n\tprint('*'*100+'\\n')\n\tprint('='*100+'\\n')\n\theader = header+'\\n'\n\tprint(header)\n\tprint('='*100+'\\n')\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.isdir(root):\n\t\tos.makedirs(root)\n\tif overwrite:\n\t\tos.makedirs(f\"{root}/configs\", exist_ok=True)\n\t\tif fileExists and not overwrite:\n\t\t\treturn ValueError(\"Destination is already populated!\")\n\t\tif os.stat(f\"{root}/configs/{file}\").st_size == 0:\n\t\t\tdestExists = False\n\t\t\tif not createDirs:\n\t\t\t\treturn ValueError(\"Destionation contains an empty file!\")"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tpath = None\n\tnew_filename = target_filename\n\tnew_filepath = os.path.realpath(new_filename)\n\tnew_filepath = os.path.dirname(new_filepath)\n\tfilepath = os.path.realpath(target_filename)\n\tfilepath = os.path.dirname(filepath)\n\n\tif len(filepath) and len(os.path.basename(filepath)) < 255:\n\t\tpath = filepath\n\telse:\n\t\tnew_filepath = os.path.join(file"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tpath, tail = os.path.split(config)\n\tif os.path.isdir(path):\n\t\tif not overwrite:\n\t\t\tassert not os.path.isfile(config)\n\treturn writeYAML(config, path, tail)\n\n\"\"\"\n\tGiven a path to a file and the tail, return a path to the file or create it. Make any\n\tcontaining directories as needed.\n\"\"\"\ndef writeFile(f, path, tail, overwrite, verbosity=0):\n\tpath, tail = os.path.split(path)\n\n\tif"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tdirname = os.path.dirname\n\tif os.path.exists(filename):\n\t\tif not overwrite:\n\t\t\tprint(\"File '%s' exists, aborting\" % filename)\n\t\t\tsys.exit(-1)\n\t\telse:\n\t\t\tprint(\"Overwriting file '%s'\" % filename)\n\n\t# If the parent directories don't exist, bail now because our dirname function won't\n\t# create them:\n\tos.makedirs(dirname(filename), exist_ok=True)\n\n\twith open(filename, 'w') as f:"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\timport os\n\timport pathlib\n\timport yaml\n\toutpath = pathlib.Path(outpath)\n\tif os.path.exists(outpath):\n\t\tif not overwrite:\n\t\t    raise FileExistsError('Target.json already exists and overwrite set to False.')\n\t\telse:\n\t\t    print('Overwrite target file:...', end='')\n\t\t    os.remove(outpath)\n\twith open(outpath, 'w') as f:\n\t\tyaml.dump(config, f)\n\t\tprint('Ok.')\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Given a target config filename and rendered config YAML, write it out to file.\")\n\tparser.add_argument(\"configfile\", help=\"The filename of the target config file\")\n\tparser.add_argument(\"outputfile\", help=\"The filename of the output (yaml) config file\")\n\tparser.add_argument(\"-d\", \"--dryrun\", action=\"store_true\", help=\"Do not actually write anything to file\", default=False)\n\t\n\tparser.add_argument(\"-o"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t#print_debug(2,'start write_yaml()')\n\tconfigfiles = list(paths_with_suffix(config.targetconfig_dir,'.yaml'))\n\tif len(configfiles) == 0:\n\t\tconfig.log_post(\"INFO\",'No target configs to write')\n\telse:\n\t\tconf = load_yaml(config.targetconfig_file)\n\t\tfor conf_file in configfiles:\n\t\t\tif overwrite and os.path.isfile(conf_file):\n\t\t\t\tconfig.log_post(\"ERROR\", \""}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t# Assume there's no config directory by default unless one is named specifically\n\tconfigDir = os.getcwd()\n\t# If we don't have a config, make one. Otherwise we'll exit with an error.\n\tif not os.path.isdir(configDir): os.mkdir(configDir)\n\tconfigP = os.path.join(configDir, target)\n\tif os.path.exists(configP):\n\t\tif not overwrite:\n\t\t\texit('Unable to save because '+target+' already exists and overwrite=False')\n\t"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tfilename = target_filename\n\trendered_content = ''\n\twith open(target_filename) as target_file:\n\t\trendered_content = target_file.read()\n\n\twith open(filename, 'w') as target_file:\n\t\tif(overwrite == True):\n\t\t\ttarget_file.write(rendered_content)\n\t\telse:\n\t\t\tif(os.path.exists(filename)):\n\t\t\t\traise Exception('Attempted to write non-overwrite file: '+filename)\n\t\t\ttarget_file.write(rendered_content)\n\ndef write_"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.isfile(target_filename):\n\t\tif not overwrite:\n\t\t\traise RuntimeError(\"Target file already exists and we do not let overwrites happen. To run this script from the command line, replace 'write_target_configs' with one of 'write_target_configs_once', 'write_target_configs_forever', or 'write_target_configs_custom'\")\n\twith open(target_filename, \"w\") as f:\n\t\tconfig_str = yaml.load(content)\n\twith open(target_filename,"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tlines = textwrap.wrap(script, 2*80)\n\tcmd = \"ssh \"\n\tfor l in lines:\n\t\tcmd += l\n\t\t#if not l.strip() == '':\n\t\t# cmd +=''\n\treturn cmd\n\ndef line_command_execute(command, verbose=False):\n\t\"\"\"\n\tRuns a script by calling a line.command and parsing the output for warnings (stderr) and errors (output)\n\n\t:param command:\n\t:return:\n\t\"\"\"\n\treturn textwrap.dedent(command).replace"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\tlines = (n for n in (s.strip() for s in script.split(\"\\n\")) if n)\n\tcmd = textwrap.wrap(\" \".join(lines), 80)\n\tcmd = '\\n'.join(cmd)\n\treturn cmd\n\n\ndef _set_param_list(cmd, param_list:list):\n\t\"\"\"\n\tset param in a param list\n\n\tex: _set_param_list(\"echo 1|tr -d '\\n'\", [\"-v\"])\n\t\t\n\t\t\"\"\"\n\treturn '{} {} {}"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tresult=''\n\tcount=0\n\tfor line in script.split('\\n'):\n\t\tif len(line):\n\t\t\tif not line.startswith('#'):\n\t\t\t\tif count > 0:\n\t\t\t\t\tresult=result+'|'\n\t\t\t\tresult=result+line\n\t\t\t\tcount=1\n\t\t\telse:\n\t\t\t\t#print line\n\t\t\t\tresult=result+line.lstrip()\n\treturn result\n\n\ndef convertToShell(script):\n\t\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# no command\n\tif script == None:\n\t\treturn\n\t# single line ssh command\n\telif script == \"\":\n\t\treturn \"ssh\"\n\t# multi line ssh commend line script\n\telif isinstance(script, str):\n\t\tlines = str(script).strip().split('\\n')\n\t\tif lines[0] == \"\":\n\t\t\tlines = lines[1:]\n\t\tprint(lines)\n\t\t\n\t\tlines = \" \".join(lines)\t\t\n\t\treturn \"ssh \" + lines\n\t# multi line command line script\n\telif isinstance"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\tlines = textwrap.wrap(textwrap.dedent(script), width=80)\n\tprint(\"\"\"/bin/bash\n\n{}\n\"\"\".format(\"\\n\\n\".join(lines)))\n\n\nif __name__ == \"__main__\":\n\t\n\texample = \"\"\"# example command script\n#\n# This script converts one line to one command by wrapping it in bash code and\n# copying it to stdout.\n#\n# It also handles the line count.\n#\n\necho \"Hello World!\"\necho \"This one line is longer than 80"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif type(script) == str:\n\t\tlines = script.split(\"\\n\")\n\t\treturn lines\n\telif type(script) == list:\n\t\treturn script\n\telse:\n\t\tprint(\"unknown script type.\")\n\t\treturn \"\"\n\ndef check_output(cmd):\n\tr_bytes = subprocess.check_output(cmd, shell=True)\n\tr_string = r_bytes.decode()\n\tprint_output(r_string)\n\treturn r_string\n\ndef check_err(cmd,print_output=False):\n\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif not script:\n\t\treturn ''\n\tcode = textwrap.dedent(script)\n\t#return '\\n\\n'.join(code.splitlines())\n\treturn code + ';\\n'\n\ndef run_command(cmd, *args, timeout=3):\n\t\timport subprocess as sp\n\t\tcmd_process = sp.Popen(cmd, stdout=sp.PIPE)\n\t\ttry:\n\t\t\toutput = ''\n\t\t\tout = ''\n\t\t\twhile output.lower()!= b'.\\n' or out.lower()!= b'.\\"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif type(script) is str:\n\t\tif script[-3:] == '\\r\\n':\n\t\t\tscript_line = script.split('\\r\\n')[0]\n\t\t\treturn script_line\n\t\telse:\n\t\t\treturn textwrap.dedent(script)\n\telse:\n\t\traise TypeError('Please provide a string input argument')\n\n\ndef ssh_script(server,\n\t\t\t  client,\n\t\t\t  server_key_path,\n\t\t\t  remote_folder,\n\t\t\t  cmd,\n\t\t\t  script_file,\n\t\t\t  cwd=None"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tscript = \"\"\"\n\t\t\tpython\n\t\t\t\t%s\n\t\t\t\n\t\t\tpython\n\t\t\t\t%s\n\t\t\t\n\t\t\tpython\n\t\t\t\t%s\n\t\t\t\"\"\" % (script_path, import_path, python_import)\n\t\treturn textwrap.dedent(script)\n\texcept:\n\t\tprint\n\t\t'Error'\n\n\ndef make_line_command_with_shell(command, shell=\"pyscript\", indent=True):\n\t\"\"\"\n\t\tmakes a single line command\n\n\t\t:param command:\n\t\t:param shell:\n\t\t:return:\n\t\t\"\"\"\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tsplit_script = textwrap.dedent(script)\n\tsplit_script = split_script.strip()\n\n\tlines = split_scripts(split_script)\n\treturn ''.join(lines)\n\n\"\"\"\n\truns multiple commands on the device, and writes the output to a file as well as stdout and stderr.\n\tYou can then pass in an iterable and it will run commands on each device in order\n\"\"\"\ndef run_script(commands, file=None, stdio=True, interactive=False):\n\n\t#print the initial commands\n\tif"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif \"__env\" in dir(func):\n\t\tnew_env = vars(func.__env)\n\telse:\n\t\tnew_env = {}\n\tnew_env[\"OUTPUT\"] = (\"{:d}\" + subprocess.STYLES['COURSE_OUTPUT']\n\t\t+ \" {:s}\").format(sys.version_info[0], *sys.argv[1:])\n\treturn subprocess.run(sys.argv[1:], *args, env=new_env, **extra_env)\n\n#----------------------------------------------------------------"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\treturn subprocess.run(args=(func,) + args, **extra_env)\n\ndef list_instrumented_class_methods(cls):\n\t\"\"\"\n\tGet the functions currently instrumented as per\n\t:obj:`InstrumentingContext` for a particular class ``cls``.\n\n\tThis will return a dictionary mapping a class name to a :obj:`list` of\n\t:obj:`str` class names.  The class names in the list are those with any\n\tmethod of the instrumented class ``cls`` included in the list, including"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif func == None:\n\t\tprint(\"You need to pass in a function.\")\n\t\treturn\n\n\ttry:\n\t\tsubprocess.run([\"python3\",\n\t\t\t\t\t   extra_env[\"_CLI_ROOT_\"],\"-m\",\"_utils\",\"_utils\",\"__main__\",\"--log','debug','--function_name',quote_plus(func),\"--args\",*args])\n\texcept KeyError:\n\t\tsubprocess.run([sys.executable, \"-m\", \"_utils\",\"_utils\",\"__main__\",\"--log','debug','--function_name',quote_"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\ttry:\n\t\tp = subprocess.run(\n\t\t\t[sys.executable, '-B', __name__] + list(args),\n\t\t\tnew_line=True, **extra_env)\n\t\treturn p\n\texcept subprocess.CalledProcessError:\n\t\tp = subprocess.run([sys.executable, '-B', '-m', f'{__name__}.{func}'] + list(args),\n\t\t\t\t\t\t   new_line=True, **extra_env)\n\t\treturn p\n\ndef get_files(path): return [f"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef subprocess_runner(func, *args, extra_env={}, **kwargs):\n\t\tenv = os.environ.copy()\n\t\tenv.update(extra_env)\n\t\toutput = run_in_separate_process_context(func, *args, env=extra_env, **kwargs).output.decode('utf-8')\n\t\ttry:\n\t\t\tjson.loads(output)\n\t\texcept Exception as e:\n\t\t\traise InvalidInputsFromRunner(f\"Invalid output of runner: {func}\")\n\t\t\n\t\treturn output\n\treturn"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tproc = subprocess.run(\n\t\t[sys.executable, __file__, '--'],\n\t\tinput=pickle.dumps(func),\n\t\t*args,\n\t\tenv=extra_env,\n\t\tcheck=False,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t)\n\tassert proc.returncode == 0, proc.stderr.decode()\n\treturn pickle.loads(pickle.loads(proc.stdout))\n\n\n@click.command()\n@"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tprint('[ RUNNING ]', f'{func.__name__}', '(' + ', '.join(i for i in args) + ')')\n\tsubprocess.run(\n\t\t[sys.executable, join('.', 'run-process.py'), str(func.__code__).replace('\\n','').replace(',', ', ') ],\n\t\t*args,\n\t\t**extra_env,\n\t\tencoding='utf8')\n\tprint('[ DONE ]')\n\ndef get():\n\t\"\"\"\n\n\tReturns\n\t-------\n\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tout = subprocess.run([sys.executable, \"-c\", \"import %s; %s\" % (func.__module__, func.__name__)],\n\t\t *args,\n\t\t env=merge_dicts(os.environ.copy(), extra_env, func.__module__ + \".\" + func.__name__ + \"_ENV\"),\n\t\t **kwargs)\n\tif out.returncode!= 0:\n\t\traise RuntimeError(out)\n\n\ndef merge_dicts(d1, d2, path=None):\n    \"\"\""}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport functools\n\tfunc_keyword = {}\n\tfor keyword in vars(func).keys():\n\t\tif keyword.startswith(\"_\"):\n\t\t\tcontinue\n\t\tfunc_keyword[keyword] = vars(func)[keyword]\n\tfunc_object = functools.partial(func, **func_keyword)\n\tfunc_object.__module__ = func.__module__\n\tcmd = subprocess.run(\n\t\tmap(str.split, args),\n\t\tcapture_output=True,    # This just captures the std"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif extra_env is None or (extra_env == {}): extra_env = os.environ\n\tfull_env = func.__globals__[\"__builtins__\"].copy() # copy environment before modifying\n\tfull_env.update(extra_env)\n\treturn_code = subprocess.run([get_executable(func_name=func.__name__)],\n\t\tinput = \"\\n\".join(args),\n\t\tstdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tcheck=True,"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvar = os.getenv(name)\n\tif var == None:\n\t\tif default == None:\n\t\t\traise EnvironmentError(\"Variable {} was not defined in environment; it should have been set\"\n\t\t\t\t\t\t\t\t\t\" to its value!\".format(name))\n\t\telse:\n\t\t\treturn default\n\telse:\n\t\treturn var\n\n\"\"\"\n\tA string, which contains the path, to which to write the result of the evaluation of the condition.\n\"\"\"\ndef check_condition(condition_str, target_value):\n\tif condition_str == target_value:\n\t\treturn True\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef getenv(matcher, default=None):\n\t\tname = matcher[0]\n\t\tif name in os.environ:\n\t\t\treturn os.environ[name]\n\n\t\tif default==None:\n\t\t\traise Exception(f'Variable'+ name +'is not defined.')\n\n\t\treturn default\n\n\t\"\"\"\n\tSet the given variable in the environment to the given value.\n\t\"\"\"\n\tdef setenv(name, value):\n\t\tos.environ[name] = str(value)\n\n\t\"\"\"\n\tSet the given variables to the given values in the"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif (name not in os.environ):\n\t\tif default!= None:\n\t\t\treturn default\n\t\telse:\n\t\t\traise RuntimeError(\"Error: variable '\" + name + \"' not declared in environment\")\n\t\n\treturn os.environ[name]\n\ndef makeList(arg):\n\t\"\"\"\n\t\tTry to make a variable into a list.\n\t\"\"\"\n\tif (isinstance(arg, list)):\n\t\treturn arg\n\n\tsplit = arg.split(',')\n\treturn split\n\t\ndef cleanCaps(name):\n\t\"\"\"\n\t\tSt"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tfor key in os.environ:\n\t\tif match in key:\n\t\t\treturn os.environ[key]\n\t\telif match in key + \" \" + default:\n\t\t\treturn os.environ[key + \" \" + default]\n\n\traise Exception(\"$(\" + match + \") or \"$(\" + match + \" \" + default + \") value not found in environment\")\n\ndef GetVariableFromFileContent(inputStringContent, varName, searchTerm='=', ignoreLinesStartWith='#'):\n\tregex = \".+\" + searchTerm + \"(\""}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tif os.environ['_MLC_ROOT'] not in value:\n\t\t\traise ValueError(f\"{value} is an invalid path\")\n\t\treturn value\n\texcept:\n\t\treturn None\ndef __set_env_var_name(name):\n\treturn f\"_MLC_\" + name\ndef __set_env_var_value(value):\n\treturn os.environ['_MLC_' + name]\ndef set_env_var(name, value, no_error = False):\n\tos.environ["}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tresult = getVar(env, matcher, default=None)\n\n\tif result is None and default is not None:\n\t\tresult = default\n\treturn result\n\ndef getEnvMatcher(env):\n\treturn lambda name: getVar(env, name)"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\treturn os.environ[matcher.split(\"=\")[0]]\n\texcept KeyError:\n\t\tif default_value is None:\n\t\t\traise Error(f\"Variable <{matcher.split('=')[0]}> not defined.\")\n\t\treturn default_value\n\ndef set_variable(matcher=None, value=None):\n\t\"\"\"Create a variable and set its value. Will add the created variable to the OS environment of the executing process\"\"\"\n\tif matcher is None: return\n\tos.environ[matcher.split(\""}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef getEnv(strVarName, strDefault):\n\t\ttry:\n\t\t\tretValue = os.environ.get(strVarName, strDefault)\n\t\t\tif retValue is None: raise ValueError(\"Env var \" + strVarName + \" is not defined\")\n\t\t\treturn retValue\n\t\texcept Exception as e:\n\t\t\traise \"Error getting value: \" + str(e)\n\t\"\"\"\n\t\tCheck the logon screen and return True if it has logged on.\n\t\tlogon = (IsWindowVisible(0x9017) && IsWindowVisible("}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry: return os.getenv(name, default)\n\texcept KeyError: raise Error(f\"The required environment variable {name} is not defined.\")\n\ndef expandVariables(cmd, envVariables):\n\t# Add environment variables\n\tenvVariables = {var: env(var) for var in envVariables}\n\tenvVariables = {key: value if value is not None else var for key, value in envVariables.iteritems()}\n\texpandedCmd = []\n\tfor part in cmd:\n\t\tif envVariables.get(part):"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not name in os.environ and default in os.environ:\n\t\tprint(default)\n\t\treturn default # os.environ[name] = default\n\telif not name in os.environ:\n\t\traise ConnectionError(f'''\n\t\t{name} is not defined in environment.\n\t\tDefine it in your local environment as defined by\n\t\t$ export  ${name}=\"${default}\";\n\t\t''')\n\telse:\n\t\treturn os.environ[name]\n\n\ndef get_version(name, default=''):\n    try:"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# split first on ( so we don't end up with something like\n\t# https://foo.com/some/url:https:1.png\n\tpieces = urllib.parse.urlparse(image_href, allow_fragments=False)\n\tif pieces.netloc == '':\n\t\traise ValueError(\"'image_href' must be a full image href including protocol, domain name, and file name\")\n\n\ttry:\n\t\tpieces = urllib.parse.urlparse(image_href, allow_fragments=False)\n\t\tif pieces."}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif '/' not in image_href:\n\t\traise ValueError('Invalid image href - no protocol or network location prefix')\n\t(image, _, image_id) = image_href.partition('/' + image_href.split('/').pop()) # strip off /img\n\t(netloc, _, use_ssl) = image.partition('?' + image.split('?').pop())\t\t\t# strip off image ID and GET request\n\tif not netloc:\n\t\traise ValueError('Invalid image href - no network location prefix')\n\treturn image_id"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_id = None\n\tfound_ssl = False\n\tif '/' in image_href:\n\t\timage_id, image_href = image_href.split('/', 1)\n\tif '?' in image_href:\n\t\timage_id = image_href.split('?', 1)[0]\n\tif not image_id:\n\t\traise ValueError(\"empty image_id given in href\")\n\tif '://' in image_href:\n\t\tfound_ssl = use_ssl = True\n\t\tnetloc, path = image_href.split('://"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttokens = image_href.split('+')\n\ttokens = list(map(str.strip,(tokens)))\n\tif len(tokens)!= 3:\n\t\tprint \"Cannot parse image href\"\n\t\treturn None, None, None\n\ttry:\n\t\timage_id = int(tokens[0])\n\t\timage_uri_net = tokens[1]\n\t\tuse_ssl = tokens[2]\n\t\treturn (image_id, image_uri_net, use_ssl)\n\texcept:\n\t\tsys.st"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# if an image_href is not of type href:\n\t# \t- raises ValueError\n\t# - else:\n\t# \t- raises ValueError\n\t#\n\t# example - href = https://www.cvs.co.nz/content/images/1-7.png\n\t#\n\t# example - href = https://www.cvs.co.nz/pages/images/1-7.png\n\t\n\tif 'href' not in image_href.keys():\n\t\traise ValueError('Not a valid hypertext link!')\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_id, _, netloc, _ = urllib.parse.urlparse(image_href)[1:5]\n\tif netloc and not netloc.startswith(r'www.'):\n\t\traise ValueError(\"invalid image href: %s\" % image_href)\n\tuse_ssl, = urllib.parse.urlparse(image_href)[1:2]\n\tif not netloc or not use_ssl or not netloc.endswith(r'image'):\n\t\traise ValueError(\"invalid image href: %s"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tparts = image_href.split('/',3)\n\tif not parts[0]:\n\t\traise ValueError(\"invalid image href\")\n\n\timage_id = parts[0]\n\tnetloc = parts[2] + ('/' if '.' in parts[2] else '')  # netloc is url parts\n\tuse_ssl = 'https' == netloc[:5]\n\tif not parts[1]:\n\t\traise ValueError(\"invalid image href\")\n\n\treturn image_id, netloc, use_ssl\n\n\nif __name__ == '"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_type = image_href[:image_href.rfind('/')]\n\tif image_href.endswith(image_type):\n\t\timage_id = image_href[image_href.rfind('/')+1:]\n\telse:\n\t\timage_id = image_href\n\n\tnetloc = image_href[:image_href.rfind('/')+1]\n\tuse_ssl = 0\n\t# The second bit here may look different between image types\n\t# Google (image_type == 'image') and Amazon's (image"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tparts = urlparse.urlparse(image_href)\n\tif parts.hostname:\n\t\treturn parts.hostname, parts.netloc, False\n\treturn None, '/', True\n\ndef _parse_href(href, fallback_protocol, fallback_hostname=None):\n\t\"\"\"\n\tParse an image href into the image id and protocol and fallback details.\n\n\t:param href: href of an image\n\t:param fallback_protocol: protocol to fall back on if it is not specified\n\t:param fallback_hostname: the"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tm = re.match('(^.*?\\:?\\//?)([\\.a-zA-Z0-9\\-]+)(\\?[a-z0-9=&_%\\[\\]]+)?(\\?)?([A-Za-z0-9]+[.])*([a-z0-9]+)((\\?[A-Za-z0-9=&_%\\[\\]]+)?)([%])?$'\n\t\t\t\t , image_href)\n\tif m is None:\n\t\t#print(f\"{"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tdef path_name_remover(input_list):\n\t\tfor index, string in enumerate(input_list):\n\t\t\tif len(string) > 1 and string[len(string) - 1] == \"/\" and \\\n\t\t\t\t\t\t\t\t\t\t\tstring[len(string) - 2] == \"/\":\n\t\t\t\tinput_list[index] = input_list[index][:len(input_list) - 2]\n\t\t\t\tbreak\n\t\n\t\treturn input_list\n\t\n\treturn path_name_remover(input_list)\n\t\ndef get_subdirectories("}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t_list = []\n\tfor string in input_list:\n\t\tif not any(string) or string[-1] is '/':\n\t\t\traise TypeError\n\t\tif len(string) is not 0:\n\t\t\t_list.append(string[:-1])\n\treturn _list\n\n\ndef is_dir(dir_path, isabsolute=True, join_files=False):\n\t\"\"\"\n\t\tReturn true if the given path is a directory, recursive.\n\t\t\n\t\tArgs:\n\t\t    dir_path: the path to check\n\t\t    isabsolute: boolean\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tprocessed_list = []\n\t\n\tif not isinstance(input_list, list):\n\t\traise TypeError('input_list must be of type list')\n\t\n\tfor item in input_list:\n\t\tif (isinstance(item, str) and (len(item) > 1) and (item[-1] == os.sep)):\n\t\t\tprocessed_list.append(item[0:-1])\n\n\treturn processed_list\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tresult = []\n\tfor item in input_list:\n\t\titem = item.strip()\n\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\titem = item[:-1]\n\t\tresult += item,\n\treturn result\n\ndef sort_filelist(filelist):\n\t\"\"\"\tSort a list of file and directory objects in natural order and return both combined\n\tList of file and directory objects in order.\"\"\"\n\t\n\tnew_filelist = []\n\tnew_filelist = os.listdir(os.path."}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not isinstance(input_list, list):\n\t\traise TypeError()\n\n\ttemp_list = []\n\tfor item in input_list:\n\t\tif len(item) > 1:\n\t\t\tif item[len(item) - 1] in os.sep:\n\t\t\t\titem = item[:-1]\n\t\ttemp_list.append(item)\n\treturn temp_list\n\ndef list_to_array(input_list):\n\t\"\"\"\n\tIterate over passed list converting it to array.\n\t\n\tArgs:\n\t    input_list: list"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif type(input_list) == str:\n\t\toutput_list = []\n\t\tfor item in input_list:\n\t\t\tif not isinstance(item, str) \\\n\t\t\t\tor len(item) < 2 \\\n\t\t\t\tor item.endswith(os.path.sep):\n\t\t\t\toutput_list.append(item)\n\t\t\t\tcontinue\n\t\t\toutput_list.append(item[:-1])\n\t\treturn output_list\n\telse:\n\t\traise TypeError(\"Input must be a string list\")\n\ndef is_number(input_string):\n\t\"\"\"\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif (not isinstance(input_list, list)):\n\t\traise TypeError(\"argument 'input_list' must be a list\")\n\tfor i in range(len(input_list)):\n\t\tif (len(input_list[i]) > 1):\n\t\t\tif (input_list[i][len(input_list[i])-1] == os.sep):\n\t\t\t\tinput_list[i] = (input_list[i][:-1])\n\treturn input_list\n\n\"\"\"\n\tIterate over a list of lists and remove trailing os se"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not isinstance(input_list, list) or isinstance(input_list, tuple): \n\t\traise TypeError(\"Invalid input_list specified\")\n\tmodified_list = []\n\tfor item in input_list:\n\t\tif (isinstance(item, str) or isinstance(item, unicode)) \\\n\t\t\tand len(item) >= 2:\n\t\t\tif item[-1] == os.sep:\n\t\t\t\tmodified_list.append(item[:-1])\n\t\t\telse:\n\t\t\t\tmodified_list.append(item)\n\treturn modified_list"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tnew_list = []\n\n\tfor i in input_list:\n\t\tif i == '' or len(i) == 1:\n\t\t\tcontinue\n\t\tif i[-1] == os.path.sep:\n\t\t\ttry:\n\t\t\t\tnew_list.append(i[:-1])\n\t\t\texcept TypeError:\n\t\t\t\traise\n\t\telse:\n\t\t\ttry:\n\t\t\t\tnew_list.append(i)\n\t\t\texcept TypeError:\n\t\t\t\traise\n\treturn new_list\n\n\n\"\"\"\n\tDetermine if two lists have the same elements in the same order.\n\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif(type(input_list)!= list):\n\t\traise TypeError(\"input_list must be a list\")\n\tfor string in input_list:\n\t\tif(string is not None and len(string) > 1 and string[-1] == os.sep):\n\t\t\tstring = string[:len(string)-1]\n\treturn input_list \n\ndef removeFilesPrefix(file_list,prefix):\n\t\"\"\"\n\tRemoves all the files from a file list prefixes with a string\n\t\n\tArgs:\n\t    file_list: list"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef make_regex_pattern(self, s):\n\t\tp = r''\n\t\ta = []\n\t\tcannot = ['(', '/', ')', '*', '{', '[', ']', '|', '.', '+', ')', '$', '^','&']\n\t\t#splits the string on comma if exists otherwise splits the string on space\n\t\tfor i in s.split(\",\"):\n\t\t\tif \"{\" in i or \"]\" in i or \"{%\" in i:\n\t\t\t\tp += re.escape(i)\n\t\t\t\tp += '"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef to_regex_pattern(self, input_string):\n\n\t\treturn re.compile(input_string)\n\n\t\"\"\"\n\t\tThis method finds a set of word tokens using Stanford CoreNLP's word tokenizer.\n\t\"\"\"\n\tdef find_words(self, input_string):\n\t\t# Use the Stanford CoreNLP tagger to make a sequence of words, a pos tag, and a chunk tag.\n\t\tout = self.sentence_tagger.tag(\n\t\t    input_string.split())\n\n\t\twords_and_pos_tokens = self"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef getRaw(self, inp):\n\t\tif len(re.findall('\\[\\[', inp)) > 0:\n\t\t\traw_str = inp.replace('[', '').replace(']', '').replace(\"'\", '')\n\t\telse:\n\t\t\traw_str = inp.replace('{', '').replace('}', '').replace(\"'\", '')[1:-1]\n\t\t\n\t\treturn raw_str\n\t\t\t\t\t\n\"\"\"\n\tThis method returns the value of the key using the dictionary\n\"\"\"\nclass GetValue(object):\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef toRegex(self, value: str = None) -> bool:\n\n\t\tpattern = None\n\n\t\ttry: \n\t\t\tif(self.__value.isupper()):\n\t\t\t\tpattern = r'^' + re.escape(self.__value.lower()) + '$'\n\t\t\telse:\n\t\t\t\tpattern = '^' + re.escape(self.__value) + '$'\n\t\texcept:\n\t\t\treturn False\n\n\t\treturn True\n\n\n\t\"\"\"\n\tThis method prints the given string in the console\n\t\"\"\"\n\tdef printValue(self):\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef getRegex(self, str_in):\n\t\texp = r\"[a-zA-Z]+|[0-9]\\d*|[-|()/+/]|-|\\s\"\n\t\tstr_in = str_in.split()\n\t\tout_str = \"\"\n\t\tpattern = self.getFullRegex(exp)\n\t\tfor x in str_in:\n\t\t\texp = exp.rstrip()\n\t\t\tx = x.strip()\n\t\t\tout_str = re.split(exp, x)\n\t\t\tout_str = ''.join(out"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef to_regex(string):\n\t\"\"\"\n\t\tThis function takes a string and convert it into regular expression\n\t\tit can be used for creating regex patterns to search the input string\n\t\"\"\"\n\n\tif type(string) == str:\n\t\tstring = string.rstrip(' ')\n\t\tpattern_for_search = re.compile(\"((\\\\d|[a-z][A-Z]|_|\\\\&|\\\\ |\\\\[|\\\\s\\\\.])*\\d)\", re.DOTALL)\n\t\tpattern_for_substitution = re.comp"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef convertToDict(self, s):\n\t\tans = dict()\n\t\tspl = s.split(\",\")\n\t\tfor each in spl:\n\t\t\tk,v = each.split(\":\")\n\t\t\tif k.startswith(\"{\"):\n\t\t\t\tif k.endswith(\"}\"):\n\t\t\t\t\tc = k[1:len(k)-1]\n\t\t\t\t\tk = c\n\t\t\t\t\tif not k in ans.keys():\n\t\t\t\t\t\tv = [int(each) for each in v.split(\",\")]\n\t\t\t\t\t\tans[k] = v\n\t\t\t\t\telse:\n\t\t\t\t\t\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef getPattern(self,inp_str):       \n        \t\n\t\tif (self.flag == 1):\n\t\t\t\n\t\t\tself.pattern = self.strReplace(inp_str)\n\t\t\n\n\t\telse:\n\t\t\t\n\t\t\tif self.pattern == None or re.search(\"error\", self.pattern):\n\n\t\t\t\tself.pattern = self.strReplace(inp_str)\n\n\t\"\"\"\n\t\tThis method is used for strReplace method of regular expression\n\t\t\"\"\"\n\t\t\n\t\"\"\"\n\tThis method contains all regex operators in regular expression\n\tOperators"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef pattern_creation(s): \n\t\n\t# initializing list of alphabets\n\talpha_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j\n        , 'k', 'l','m', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\t\n\t# Creating an empty pattern consisting only of single blank characters\n\tfinal_pattern"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef to_regex_pattern(self, str1):\n\t\tpattern = re.compile('[a-z][0-9]{4}[A-Z]{4}[a-z]{4}[A-Z]{1}[a-z]{1}[0-9]{1}[0-9a-zA-Z]*[1-9]{1}[0-9]{1}[0-9A-Z]*[A-Z]*[\\s0-9a-zA-"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommand = \" \".join(command)\n\treturn subprocess.run(command, shell=True)\n\t#return 1\ntry:\n\tfrom collections import deque\nexcept ImportError:\n\tfrom future.utils import iteritems\n\n\tdeque = lambda: collections.deque()\n\nclass Worker:\n\n\t\"\"\"\n\t\tRun commands in worker instance concurrently\n\t\"\"\"\n\n\tdef __init__(self, args):\n\t\tself.args = args\n\n\tdef __call__(self, commandQueue):\n\t\t\"\"\"\n\t\t\tCall this function if commandQueue is empty.\n\n\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t#p = subprocess.run(cmd, shell=True, capture_output=True)\n\t#print(p.stdout.decode('utf-8'))\n\t#print(p.stderr.decode('utf-8'))\n\t#print(p.returncode)\n\"\"\"\n\tCall the command(s) and collect the output (the result).\n\t\"\"\"\n\tp = subprocess.run(cmd, shell=True, capture_output=True)\n\tout = p.stdout.decode('utf-8').strip()\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\toutput = subprocess.check_output(command)"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\tresult = subprocess.check_output(command, stderr=subprocess.STDOUT, timeout=timeout).strip().decode('utf-8')\n\texcept Exception:\n\t\terror(command)\n\t\terror(result)\n\t\tinfo('\\n')\n\t\traise SystemExit(errno.ENOEXEC)\n\treturn result\n\n\"\"\"\n\tExecutes and returns the return value of the commands that are ran.\n\t\"\"\"\ndef run(commands, *, timeout=None):\n\tif isinstance(commands, list):\n\t\tresult"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\toutput = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\tstdout, _ =  output.communicate()\n\texitcode = output.returncode\n\tif (exitcode):\n\t\traise Exception(' '.join(cmd) +'failed with exit code'+ str(exitcode))\n\treturn stdout\n\ndef run_shell_command_file(infile, cmdline, outfile):\n\tcmdline +='>'+ outfile\n\t(exitcode, _) = shell_"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\ndef checkOutput(command):\n\t\"\"\"\n\t\tCheck the exit status and return it.\n\t\t\"\"\"\n\tcommand.wait()\n\tif command.returncode!= 0:\n\t\tlog = open(\"log.txt\", \"a\")\n\t\tlog.writelines(command.stdout)\n\t\tlog.write(\"\\n\\n[*] Error in command execution.\\n\")\n\t\texit(\"[!] Error - Please check Log file.\")\n\telse:\n\t\treturn None\n\ndef showOutput(command):\n\treturn str(command.stdout)\n\n#Check"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tsubprocess.run(*args, **kwargs)\n\n\ndef _isJVMInstalled():\n\t\"\"\"\n\tCheck that Java was installed.\n\t\"\"\"\n\tcmd = [\"java\", \"-version\"]\n\tcmdline = Popen(cmd, stdout = PIPE)\n\ttxt = cmdline.stdout.read().decode()\n\tif'sun.misc.boot' in txt:\n\t\tif 'openjdk' in txt:\n\t\t\treturn True\n\treturn False\n\n\ndef _getJVMPath():\n\t\"\"\"\n\tReturn the Java"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef run(self, cmd):\n\t\tprint(self.name + \": \" + cmd + \"...\")\n\t\tp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\t\t(output, errors) = p.communicate()\n\t\tstatus = 0\n\t\tif p.wait()!= 0:\n\t\t\tstatus = p.wait()\n\t\t\tprint(errors)\n\t\treturn (output, status)\n\n\nclass InstallerWrapper(Wrapper):\n\n\t\"\"\"\n\tAn"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcaller = caller if caller else 'Unknown'\n\n\tprint(f'Calling {caller}')\n\n\tsubprocess.call(command, shell=True)\n\n\n# --------------------------------------------------------------------------\n\nif __name__ == '__main__':\n\t\n\tparser = argparse.ArgumentParser(description=\"Commandline tool for converting BLE beacon data (JSON format) on a macOS device to MQTT messages.\")\n\tparser.add_argument(\"-B\", \"--bluepy\", action=\"store_true\", help=\"Use bluepy instead of ftdi\")\n\tparser.add"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\toutput = ''\n\tfor command in commands:\n\t\tprocess = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n\t\toutput += process.communicate()[0].decode()\n\n\treturn output\n\n\ndef call_file(file):\n\t\"\"\"\n\tCall a binary file.\n\t\"\"\"\n\tcommands = [' '.join([\n\t\t'sudo', 'chmod', '+x', file\n\t]),'sudo', './'+file]\n\n\treturn call(commands)\n\n\ndef install():\n\t\"\"\""}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\na=ipaddress.ip_address('192.168.1.6')\n\n\nif a.is_global:\n\tprint(\"global\")\n\ttry:\n\t\ta.is_private\n\texcept:\n\t\tprint(\"not exists\")\nelse:\t\n\tprint(\"not global\")\n"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ttry:\n\t\tipaddress.ip_address(ip)\n\t\tisIP = True\n\texcept:\n\t\tisIP = False\n\treturn isIP\n\ndef isIpAddress(ip):\n    valid_ipv4 = '^\\d+\\.\\d+\\.\\d+\\.\\d+$'\n    octet_ipv4 = re.compile( valid_ipv4 )\n    octets = ip.split( '.' )\n    if len(octets) == 4:\n        if octet_ipv4.fullmatch( ip"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef __init__(self, ip, verbose=2):\n\t\tif self.isIP4(ip):\n\t\t\tself.ip = ip\n\t\telse:\n\t\t\traise Exception('IP address is not valid')\n\n\tdef isIP4(self, ipaddr):\n\t\t\"\"\"\n\t\tChecks if the address is given in the 4th octets.\n\t\t:param ipaddr: IP address\n\t\t:type ipaddr: string\n\t\t:return: True if the address is in 4th octets else false\n\t\t:rtype: bool\n\t\t\"\"\"\n\t\toctets = ip"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t#print(ip_addr)\n\ttry:\n\t\tnetwork, mask = ip_addr.split(\"/\")\n\t\treturn (network, mask[2])\n\texcept:\n\t\t# print(\"Error! Invalid input! (Error! Invalid input! )\")\n\t\tnetwork, mask = \",\", \",\"\n\t\treturn (network, mask)\n\n\t#print(network, mask)\n\n\"\"\"\n\tTest if IPv4 address/mask is valid\n\t\n\tInput:    (network, mask)=parse_ip_addr(ip_addr)\n\t\t\t\tip_addr in standard"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\treturn ipaddress.ip_address(value).is_loopback()\n\ndef isIPNetwork(value):\n\t\"\"\"\n\tReturn true if value is a range \n\t    \n\t\"\"\"\n\tvalue = value.replace('/', '')\n\tip = ipaddress.ip_network(value, strict=False)\n\treturn ip.subnets()[0].prefixlen > 0\n\t\ndef isIPNetworkSubnet(value):\n\t\"\"\"\n\tReturn true if value is a subnet \n\t    \n\t\"\"\"\n\tvalue = value.replace('/"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef test_IPv4Addr(self):\n\t\t# print(\"\\n\\nIPv4 Addr Test Case\")\n\t\tres = ipaddress.IPv4Address('10.0.0.1')\n\t\tself.assertTrue(isinstance(res, ipaddress.IPv4Address))\n\t\tself.assertEqual('10.0.0.1', res)\n\n\t\tres = ipaddress.IPv4Address('256.256.256.256')\n\t\tself.assertTrue(isinstance(res, ipaddress.IPv4Address))\n\t\t"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef validate_ipv4_address(self, address):\n        self.assertRegex(address, r'^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')\n        \n\t\n\t\"\"\"\n    Check whether the IP is valid or not\n    \"\"\"\n\tdef checkInternet(self, ip"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ta = ipaddress.ip_address (\"30.0.0.2\")\n\tb = ipaddress.ip_address (\"30.0.0.1\")\n\tprint(\"IP address: %s\" % a)\n\tprint(\"IP address: %s\" % b)\n\tprint(\"\\nIf IP address ends with 0, and the IP address is not a hostname, the IP address is hostname\" %a, b)\n\nip_addr_test()\n\n\"\"\"\n\tTest if IPv6 address or not\n\t\n\t\"\"\"\n\tc = ipaddress"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\",ipaddress):\n\t\treturn('Valid')\n\telse:\n\t\treturn('Invalid')\n\t\t\n#Check valid IPv4 address\t\t\t\t\nipaddress1 = '123.23.53.34'\nprint(checkIP(ipaddress1))\n\nipaddress2 = '123.23.53.f1253'\nprint(checkIP(ipaddress2))\n\nipaddress3 = '"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\ndef input():\n\t\"\"\"\n\t\"\"\" \n\ttry :\n\t\thost = str(input(\"Input a IPv4 address(or Host address): \\n\"))\n\texcept Exception:\n\t\tprint(\"please input valid IP address\")\n\t\tinput()\n\t\tsys.exit(0)\n\t\n\treturn host\n\ndef isIPv4(host):\n    \"\"\"\n\t\"\"\"\n    try :\n        if host.split(\".\")[0] == \"127\":\n            is_ipv4 = False \n        else:\n            is_ipv4 = True\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t\"\"\"\n\tgraph = rdfs_subj_graph\n\t\n\tchilds = graph.subjects(RDFS.subClassOf, None) \n\tparents = set()\n\t\n\tfor child in childs:\n\t\tchild_parents = rdflib.resource_triples(graph, child, None)\n\t\tfor child_parent in child_parents:\n\t\t\tparent = child_parent[2]\n\t\t\tparents.update(child_parent)\n\treturn parents\n\t\"\"\"\n\t\n\ttry:\n\t\treturn rdflib."}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef find_roots(graph, root_class):\n\t\trdfclass_parents = graph.subject_objects(\n\t\t\tpredicate=root_class\n\t\t\t)\n\t\trdfclass_roots = []\n\t\tfor rdfclass in rdfclass_parents:\n\t\t\tparents = list(graph.predicate_objects(rdfclass))\n\t\t\tfor parent in parents:\n\t\t\t\tif (parent, root_class, rdfclass) not in graph: continue\n\t\t\t\tif rdfclass_roots.count(parent) == 0:\n\t\t\t\t\t# parent does not already exist in"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots = set()\n\n\tfor c, p, o in graph.triples((None, rdflib.RDFS.subClassOf, None)):\n\t\troots.add(c)\n\n\tfor c, p, o in graph.triples((None, rdflib.RDFS.subPropertyOf, None)):\n\t\troots.add(c)\n\n\tfor c, p, o in graph.triples((None, rdflib.RDFS.equivalentClass, None)):\n\t\troots.add(c)\n\n\tfor c"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(self,graph, className):\n\t\treturn self.find_roots_aux(graph.subjects(predicate=RDFS.domain, object=URIRef(className)), graph, className)\n\t\n\tdef find_roots_aux(self,subj, graph, parentName):\n\t\tans = set()\n\t\t\n\t\tif(subj in graph.subjects(predicate=RDFS.domain)):\n\t\t\tans.add(subj)\n\t\tfor pred, obj in graph.predicate_objects(subj):"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef find_roots(graph, property_URI):\n\tr\"\"\"\n\tfind_roots(graph, property_URI)\n\t\n\tReturns all roots for the property specified\n\t\"\"\"\n\troots = set()\n\troots_temp = graph.predicates(subject = None, predicate = property_URI)\n\troots = roots.union(set(roots_temp))\n\t\n#\twhile roots_temp:\n#\t\troots = roots.union(set(roots_temp))\n#\t\tnew_roots = graph.predicates(subject = None, predicate = roots_"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(graph, sub):\n\t\tres = graph.objects(sub, RDFS.subClassOf)\n\t\troots = set()\n\t\tfor p in res:\n\t\t\tif not graph.has_path(sub, p):\n\t\t\t\troots.add(p)\n\t\t\troots |= find_roots(graph, p)\n\t\treturn roots\n\t\t\n\t\n\tgraph = get_simple_graph(fname)\n\treturn find_roots(graph,graph.value(subject, RDFS.label))\n\t\t\t\n\t\ndef get_simple_graph"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef compute_path_len(p, r):\n\t''' compute length of rooted path from p to r\n\t\treturn None indicates there's no such path\n\n\t\tThe root of a path is considered a leaf\n\t'''\n\tif p!= r:\n\t\tdist = 0\n\t\tqueue = [p]\n\t\twhile len(queue) > 0:\n\t\t\tc = queue.pop()\n\t\t\tdist = dist + 1\n\t\t\tfor n in c.get_children():\n\t\t\t\tif n == r:\n\t\t\t\t\treturn dist\n\t\t\t\telse:\n\t\t\t\t\tqueue.append(n)"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tdef depth_first_search(self, graph, obj, visited, depth, limit):\n\t\t\"\"\"\n\t\tReturns a list of all of the descendants of obj, as defined by RDFS.subClassOf and its\n\t\tchilds, as well as the depth passed.\n\t\tIt will also visit the depth, plus 1, times, until the limit is reached\n\t\t\"\"\"\n\t\tout = []\n\t\tif depth <= limit:\n\t\t\tfor o in graph.subjects(predicate=obj, object=None):\n\t\t\t\tif not o in visited:\n\t\t\t\t\tvisited.add"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t# get root node\n\tnode = graph.value(URIRef(root_name), rdflib.RDFS.subClassOf, None)\n\treturn _get_roots(graph, node, rdflib.RDFS.subClassOf, rdflib.RDFS.subClassOf)\n\ndef _get_roots(graph, node, prop, parent):\n\tif type(node) is not list:\n\t\treturn set()\n\t\t\n\troots = []\n\tfor child in node:\n\t\troots.append(_get"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots=set()\n\n\tfor child, prop, parent in graph.triples((None, rdflib.RDFS.subClassOf, None)):\n\t\tif parent in roots:\n\t\t\troots.remove(parent)\n\t\troots.add(child)\n\n\treturn roots\n\ndef add_roots(graph, roots):\n\tgraph.add((None, rdflib.RDFS.subClassOf, None))\t\n\tfor root in roots:\n\t\tgraph.add((root, rdflib.RDFS.subClassOf, None))"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tif dumpfile:\n\t\tdumpfile = unicodeFilePath(dumpfile)\n\telse:\n\t\tdumpfile = u\"\"\n\td = yaml.dumper.SafeDumper(stream, v2)\n\t# d = yaml.dumper.SafeDumper(file, default_flow_style=False)\n\tif (default or load_yaml_str):\n\t\ts = yaml.dump(d)\n\telse:\n\t\timport cStringIO\n\t\timport StringIO\n\t\ts = StringIO.StringIO(yaml.dump(d))"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(self, *args):\n\t\tself.dumper = yaml.SafeDumper\n\t\tyaml.SafeDumper.__init__(self, *args)\n\n\tdef _dict_representer(self, tag, mapping, flow_style=None):\n\t\treturn self.dumper.represent_dict(mapping)\n\n\nYAML = _YAMLDumper()\nyaml = YAML\n__all__ = ['yaml']\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef __init__(self, file_handle):\n\t\tself.file_handle   = file_handle\n\t\tself.dicts_out \t  = []\n\n\tdef dump(self, data):\n\t\tif sys.hexversion>0x3000000:\n\t\t\tself._write_py3_unicode_to_file(data)\n\t\telse:\n\t\t\tself._write_py2_unicode_to_file(data)\n\n\tdef _write_py3_unicode_to_file(self, data, force_ascii = False):\n\t\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\treturn dict2str(yaml.dump(data), indent_str)\n\ndef dict2csv(d, separator = \";\") -> str:\n\t\"\"\"\n\t\tTODO, replace dict2csv\n\t\"\"\"\n\tpass\n\t# import\n\t# import pandas\n\t#\n\t# tmp=pandas.DataFrame(pd.DataFrame(d).values)\n\t# tmp.to_csv(pandas.ExcelWriter(), sheet_name='test')\n\t#\n\t# return tmp\n\n\nclass yaml2str():pass\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tif PY3:\n\t\treturn str(s, 'utf-8')\n\telse:\n\t\treturn s\n\n\ndef unicode_load(s):\n\t\"\"\"\n\tLoad from a py2-string\n\t\"\"\"\n\tif PY3:\n\t\treturn s.encode('utf-8')\n\telse:\n\t\treturn s\n\n\ndef safe_load_all(s):\n\t\"\"\"\n\tLike yaml.safe_load(), but supports utf-8 encoded bytes\n\t\"\"\"\n\tload_all = list(yaml.SafeLoader.load_"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dict2class(self, obj, name):\t\t\n\t\t\"\"\" Recursive to dict to class \"\"\"\n\t\tkeys = list(obj.keys())\n\t\tif (len(keys) == 0): return\n\t\t\n\t\tobj = obj[keys[0]]\n\t\tif   (isinstance(obj, str) or isinstance(obj, bytes) or isinstance(obj, int)): val = obj\n\t\telif (isinstance (obj, bool)): val = obj\n\t\telif (isinstance (obj, float)): val = obj\n\t\telif (isinstance ("}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tprint(\"\\n>>>>   \\033[33m>>>   \\033[94m\"+\"\\033[0m\")\n\tprint(\">>>> \"+\"\\033[0m\\033[35m>>>     \"+\"\\033[94m\"+\"\\033[0m\")\n\tprint(\">>>> \\n\\033[33m>>>\"+\"\\033[94m\"+\"\\033[1m\"+u\"\\u25a3\".join(i for i in input(text)))\n\tprint(\">>>> \\033[0m\\033[35m<<"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef to_unicode(self, data):\n\t\tif isinstance(data, memoryview):\n\t\t\ttry:\n\t\t\t\tif 'cstr' in data.format == 'cstr':\n\t\t\t\t\tdata = data.tobytes().decode('utf-8')\n\t\t\texcept:\n\t\t\t\tif 'cstr' in data.format == 'cstr':\n\t\t\t\t\tdata = data.tobytes()\n\t\t\t\tif 'cstr' in data.format == 'cstr':\n\t\t\t\t\tdata = str(data)\n\t\treturn data\n\t\n\tdef dump(self, data):\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\ndef yaml_tag_to_unicode(py2_or_3_u, tag):\n\t\"\"\"\n    Converts a tag from yaml (python 2.x) to py2-unicode or py3-string.\n\n  \t\"\"\"\n\tif python_2:\n\t\treturn py2_u.u(u\"<%s>\\n\" % tag)\n\telif python_3:\n\t\treturn py3_u(u\"<%s>\\n\" % tag)\n\n\"\"\"\n\tDump all items from input to output\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(self, value):\n\t\tif type(value) == str:\n\t\t\treturn value.decode()\n\t\treturn super(Dumper, self).dump(value)\n\n\t\"\"\"\n\tTakes a string (or any unicode-encodeable string)\n\tand converts it to a string we can write to a safe-to-print\n\trepresentation\n\t\"\"\"\n\tdef represent_object(self, value):\n\t\tif isinstance(value, unicode):\n\t\t\treturn self.represent_scalar(u'tag:yaml.org,2002:"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass Logger:\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\n\tdef __init__(self, name='GeneralApp.Logger', loglevel=logging.INFO):\n\t\t\"\"\"\n\t\t* name: str, name of the logfile. (default = GeneralApp.Logger)\n\t\t* maxlevel: int, maximum level at which to capture the messages. (default = None)\n\t\t\"\"\"\n\t\tself._logger = logging.getLogger()\n\t\tif loglevel:\n\t\t\tself._logger.setLevel("}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\ndef log_debug(msg):\n\t\t\"\"\"\n\t\tThis function takes the message as string and logs everything with logging.debug format\n\t\t\"\"\"\n\t\tfor module in logging.Logger.manager.moduleDict.values():\n\t\t\tif 'debug' in dir(module):\n\t\t\t\tmodule.debug(f'{module.name():20} : <{module.__name__}>: {msg}')\n\t\n\ndef log_warn(msg):\n\t\t\"\"\"\n\t\tThis function takes the message as string and logs everything with logging.warning format\n\t\t\"\"\"\n\t\tfor module in"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\nclass DefaultFormatter(logging.Formatter):\n\t__delimiter = \"=\"*60\n\t__template_log = (\n\t\t\"%(levelname)-8s%(name)-20s%(message)s\"\n\t)\n\n\tdef __init__(\n\t\tself\n\t\t,formatter=None\n\t\t,date_format=\"[%x %X]\"\n\t):\n\t\t\"\"\"\n\t\tInitializes a new logging.Formatter instance.\n\n\t\tKeyword arguments:\n\t\tdate_format -- The date format to convert to for messages\n\t\t\"\"\""}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t# TODO: replace with logging\n\t# try:\n\t# \timport cStringIO\n\t# \tdef _str_or_io(string):\n\t# \t\tif isinstance(string,str):\n\t# \t\t\treturn string\n\t# \t\tif StringIO in dir(string):\n\t# \t\t\treturn StringIO(string.getvalue())\n\t# \t\treturn string\n\t# except ValueError:\n\t# \tlogging.warning(\"logging not working with your python version\")\n\t# \tprint \"logging failed, try"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\t\n#######################################################################################\n# class Logger\n#######################################################################################\nclass Logger:\n    logger = None # type: Logger\n    \"\"\"\n    Logger class.\n\n    :param logFname: Name of the log file.\n    :param verbose: If 1, will include all debug output including\n                    print statements.\n    :returns: None.\n    \"\"\"\n\n    def __init__(self, logFname='', verbose=0):\n        self.logger = logging.getLogger('logger')\n        log"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n__author__='darcyhex'                     \t\n                    \nif logfile:\n    \"\"\"\n    Logging via python logger object that can be used with a simple \n    if __name__ == '__main__' block\n    \"\"\"\n    import logging                                \n    \n    logger = logging.getLogger()                            \n    logger.setLevel(logging.DEBUG)   \n    \n    # Create the logging file handler                                \n    fh = logging.FileHandler(logfile)\n    \n    fh.setLevel(logging.DEBUG)\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nlogger=logging.getLogger('app')\nlogger.setLevel(logging.DEBUG)#logging level: 10:DEBUG 100:INFO 200:WARNING 300:ERROR 400:CRITICAL.  default:30\ndef log_to_file(mode,log):\n\t\"\"\"  mode can be 'log','data' and 'both'.  data and log files must have their paths specified at runtime before the first call to log_to_file()\"\"\" \n\tlog=\" \".join(log)#  removes special characters like newline(s),tab("}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t \nclass AppLoggerHndlr(logging.Handler): \n\t\"\"\"\n    An application level logger\n    \"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(AppLoggerHndlr, self).__init__(*args, **kwargs)\n\t\tself.setLevel(logging.DEBUG)\n\t\tself.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s (%(levelname)s) - %(message)s\"))\n\t\tself._logger = None"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tclass __DefaultRotator(object):\n\t\tmode = \"a\"\n\t\tdef __init__(self,fileName,mode=mode):\n\t\t\tself.fileName = fileName\n\t\t\tself.mode = mode\n\t\tdef GetRotator(self,noRotations,*a):\n\t\t\tself.fileName = a[0]\n\t\t\treturn self\n\t\tdef SetRotations(self,noRotations,*a):\n\t\t\tif noRotations < self.noRotations:\n\t\t\t\tself.noRotations = noRotations\n\t\t\t\treturn self\n\t\t\telse: return self"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\n\ndef init_logger(self, name):\n\t\t'''\n\t\tInitiates the logger with the name passed in\n\t\t\n\t\t@param str name\n\t\t'''\n\t\t\n\t\t# if a logger wasnt requested for this module name\n\t\tif(name == None) :\n\t\t\t#create a logger for this module of an empty name\n\t\t\tself.logger = logging.getLogger(\"\")\n\t\telse:\n\t\t\t# create a logger for the specified name\n\t\t\tself.logger = logging.getLogger(name)\n\t\t\tlogging.addLevelName"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn np.empty(shape, dtype)\n\n\"\"\"\n\tfunction to add two arrays together in numpy.\n\t\n\tParameters\n\t----------\n\ta : `numpy.array`\n\t    array to add to\n\tb : `numpy.array`\n\t    array to add\n\t\"\"\"\n\t\n\ta = np.array([1, 2.5]).astype(np.complex128)\n\tb = np.array([ -np.sqrt(2.5), np.sqrt(2.5) ]).astype(np"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarray_ = np.empty(shape,dtype=dtype) \n\treturn array_ \n\n\"\"\"\n\tFunction to fill an array with\n\tthe given data.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tvalue : int or float\n\t    value to be assigned to the array\n\t\"\"\"\n\tarray_ = np.empty(shape,dtype=dtype)\n\tarray_[:] = value\n\treturn array_ \n\n\"\"\"\n\tFunction to insert the given\n\tdata into an array"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarray = np.zeros(shape, dtype)\n\treturn array\n\ndef random_array(shape, dtype=np.float32):\n\t\"\"\"\n\t\tFunction to random initialize an array.\n\n\t\tParameters\n\t\t----------\n\t\tshape : tuple\n\t\t    shape of the randomly initialized array\n\t\tdtype : `numpy.dtype`\n\t\t    data-type of the randomly initialized array\n\t\t\"\"\"\n\treturn np.random.randn(*shape)\n\t\ndef full_ones_like(array, broadcast_dimensions=None):\n\t\"\"\"\n\t\tCon"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn np.empty(shape=shape, dtype=dtype)\n\t\ndef copy_array(array, shape=None):\n\t\"\"\"\n\tFunction to copy an array.\n\t\n\tParameters\n\t----------\n\tarray : `numpy.ndarray`\n\t    array to copy\n\tshape : tuple, optional\n\t    shape attribute of the array to copy. If None (default) it is infered by the function.\n\t\"\"\"\n\tif shape is None:\n\t\tshape = array.shape\n\treturn array.view()[:].reshape("}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tx = np.empty(shape, dtype=dtype)\n\t\"\"\"\n\tFunction to calculate the product of a list.\n\t\n\tParameters\n\t----------\n\tvalues : list\n\t    list of numbers\n\n\tReturns\n\t----------\n\tout : list, product of the first value in the list.\n\t\"\"\"\n\treturn x\n\t\ndef create_list(shape, dtype):\n\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\tIt returns a list\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tnew_array = np.empty(shape, dtype=dtype)\n\t\n\treturn new_array\n\n\"\"\"\nFunction that creates a matrix of shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\n\"\"\"\ndef create_matrix(shape, dtype):\n\tmatrix = np.zeros(shape, dtype=dtype)\n\treturn matrix\n\n\"\"\"\nFunction to create the diagonals of matrix.\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.zeros(shape,dtype=dtype)\n\t\ndef set_random_seed(seed:int) -> None:\n\t\"\"\"\n\tSet the random seed.\n\n\tParameters\n\t----------\n\tseed : int\n\t    integer seed\n\t\"\"\"\n\tnp.random.seed(seed=seed)\n\ndef get_random_state() -> int:\t\n\t\"\"\"\n\tReturn the random seed used in NMT\n\t\"\"\"\n\treturn np.random.get_state()[2]\n\t\ndef set_random_state(seed:int"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.zeros(shape, dtype)\n\n\"\"\"\n   Function to write a numpy (2D) array to a file with\n      two ways:\n\n      a) using numpy.write:\n      b) using Numpy.save:\n\"\"\"\n\ndef numpy_write(f_name, data, header_size, dtype=\"<u4\"):\n\t\"\"\"a) numpy.write\n\t    Parameters\n\t    ----------\n\t    f_name : str\n\t\tfile name to be opened or written to\n\t    data : 2D numpy"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\tdef array(shape, dtype):\n\t\t\n\t\tmyarray = np.empty(shape, dtype)\n\t\t\n\n\t\treturn myarray\n\"\"\"\n\tFunction to reshape an ndarray based on the new shape. For example, you might want \n    to reshape a 50x50 array to a 20x20 image and you'll prefer to reshape based on \n\tthe smallest dimension.\n\t\n\tParameters\n\t----------\n\tmyarray : `numpy.ndarray`\n\t    an ndarray of shape (N,M) to reshape\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.empty(shape,dtype=dtype)\n#\n\ndef create_array_of_1xsize(size,dtype):\n\t\"\"\"\n\t\tFunction to create an array of size size and dtype. It's an array of \n\t\tfloat numbers with 1.0 on all positions.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tsize : int\n\t\t    size of the array to create\n\t\tdtype : `numpy.dtype`\n\t\t    data-type of the array to create\n\t\t\"\"\"\n\treturn np.ones(size)*1.0"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tcenter, sigma = 0.2, 0.1\n\n\tif (x_0 - np.floor(center)) + sigma <= 0:\n\t\tprint(\"\\n(x_0 - np.floor(center)) + sigma <= 0!\")\n\t\tprint(\"It is necessary to change center and sigma.\")\n\t\treturn \"\"# This should never happen\n\tif (x_0 - np.floor(center))*0.95 + np.ceil(center) + sigma >= 1: \n\t\tprint(\"\\n(x_0 - np.floor(center"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\t\ndef gaussian_centered(center=0.2, sigma=0.1, size=5):\n\t\"\"\"Generate points from a gaussian centered around center with sigma.\"\"\"\n\n\treturn center + sigma*np.random.randn(size)\n\ndef gaussian_centered_data(data):\n\t\"\"\"Generate new points from a gaussian centered around center with sigma.\"\"\"\n\n\treturn np.full(data.shape[0], 0.2)\n\ndef linear(x, y, x_0=np.array([0,"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nclass LickSensor(EnvironmentComponent):\n\t\n\tsensor_name=\"lick\"\n\t\n\tdef __init__(self, dt, parameters, verbose=True):\n\t\tself.current_t = 0\n\t\tsuper(LickSensor, self).__init__(dt,verbose)\n\t\tself.current_value = parameters\n\t\t\n\tdef compute_tendencies(self):\n\t\t\"\"\"\n\t\tThis is the same as in any other component\n\t\t\"\"\"\n\t\tself.current_t += self.internal_dt\n\t\tself.current_"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndata_filename = 'data/data_gauss_centered.txt'\nN,T = np.loadtxt(data_filename,delimiter=',',unpack=True)                    \n# N - number of time series,  \n# T - length of time series.  \nT = int(T)#number of measurements\n\n# Read the covariance matrix of the random effects\n# The covariance should have the form \n\"\"\"\nSigma = np.zeros((S,S))\nnp.fill_diagonal(Sigma,np.diag(c"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef get_params_from_fits(filename, name):\n\tfrom PyAstronomy import pyasl\n\tfits,fitsx,fitsfile=pyasl.imreadfits(filename)\n\txind=fits[0,0].shape[0]-1\n\tyind=fits[0,0].shape[1]-1\n\tparams=fits[0,0,1:][:,xind-3:yind+3]\n\tres=fits[xind-3:yind+3,1:][:,xind-3:yind"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nx = 0.2\nmu = 0.2\nsigma = 0.1\nx = np.linspace(0,4,1000)\ny = 1/(2*np.pi*(sigma**2)) * np.exp(-((x - mu)**2)/(2*(sigma**2)))\n\nplt.figure()\nplt.plot(x,y)\nplt.show()\n\n\"\"\"\n#Gaussian centered around 2.3 with a sigma of 0.2.\ny_data = np.random.normal(2"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\ndef gaussian(x, y, x0, y0, xsigma, ysigma):\n\treturn np.exp(-((x-x0)**2/(xsigma**2)+(y-y0)**2/(ysigma**2)))\n\ndef log_gaussian(x, y, x0, y0, xsigma, ysigma):\n\treturn -1*((x-x0)**2/(2*(xsigma**2))) -((y-y0)**2/(2*(ysigma**2)))\nclass Line"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\td2dx2 = 1.0/(1.0/0.9**2 + 1.0/0.1**2)\n\td2dy2 = 1.0/(1.0/0.9**2 + 1.0/0.1**2)\ngaussian = lambda x,y: 1.0/(0.1*x**2 + 0.1*y**2)\n\t\n\n\"\"\"\n\tThis function is used in gaussian1d. Used ndimage to quickly\n\tcreate the gaussian kernel.  \n\"\"\"\ndef makeGaussian"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\ndef computeGaussianDistribution(min,max,res):\n\n\tresult = []\n\tn = ((max-min)/(res))+1\n\tsigma = (max-min)/n\n\tfor j in range(0,n):\n\t\tx = min+j*res\n\t\ty = (math.exp((-1.0)*(2.0)*(x**2))/math.sqrt(2.0*math.pi*sigma**2)) * 10**8 \n\t\tresult.append(y * 2**20)\n\n\treturn"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tb = [0.2]\n\ts = np.arange(-3.,3.8,0.1)\n\tfor a in s:\n\t\tg = stats.norm.pdf(a,b[0],0.1)\n\t\tplt.plot(s,g,'k',label = str(a))\n\tplt.xlabel(r'P')\n\tplt.ylabel(r'p(P)')\n\tplt.show()\n\n\nif __name__ == '__main__':\n    #a\n    p = 0."}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfrom copy import copy\n\tfrom datetime import datetime\n\tfrom hashlib import sha256\n\tfrom os.path import basename\n\tfrom os.path import dirname\n\tfrom os.path import exists\n\tfrom os.path import join\n\tfrom os import environ\n\tfrom re import I\n\tfrom re import search\n\tfrom re import sub\n\tfrom requests import post\n\tfrom requests.exceptions import ProxyError\n\tfrom structlog import configure\n\tfrom sys import stderr\n\tfrom typing import (\n\t\tAny,\n\t\tCallable,\n\t\t"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(lineno)d: %(message)s')\n\n\tlog_errors = []\n\tparse_errors = []\n\tfor f in filenames:\n\t\ttry:\n\t\t\tparsed_config, log_errors = parse_file(f)\n\t\texcept ParseError as parse_err:\n\t\t\tparse_errors.append((f, parse_err))\n\tlogging.info('Parsing completed. {} files with valid configurations returned ({} parse errors)"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig = {}\n\terrors = []\n\tfor conf in conf_files:\n\t\tkey = get_key()\n\t\tconfig[key] = None\n\t\tif not validate_config(get_root_path() + '/configuration/' + conf, False):\n\t\t\terrors.append(LogRecord(get_key(), get_filename(), 'unparsable config'+ conf))\n\t\t\tcontinue\n\t\tparsed_config = get_config(get_root_path() + '/configuration/' + conf)\n\t\t\n\t\tif not parsed_config:\n\t\t\terrors"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = []\n\tresults = []\n\tfor filename in filenames:\n\t\ttry:\n\t\t\tconfiguration = load(open(filename, 'rb'))\n\t\t\tconfigurations.append(configuration)\n\t\texcept:\n\t\t\tresults.append(traceback.format_exc())\n\treturn (configurations,results)\n\ndef check_validity(configuration):\n\t\"\"\"\n\tGiven a configuration dict, validate it using some basic data checks, logging any\n\tinvalid key/value pairs, including their associated log warning message.\n\t\"\"\"\n\tif 'url"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\treturn parse(sorted(glob(cfg_glob)), **kwargs)\n"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\nimport configparser, logging, os, sys\n\ntry:\n\timport json\nexcept ImportError:\n\timport simplejson as json\n\nimport io\n\n\n\n\ndef loadConfigs(cfPaths):\n\t\"\"\"\n\tGiven a list of configuration file paths, construct a dict of name to configuration values,\n\tand an empty list of parse exceptions. For each configuration file, read it. If the file\n\tcan't be read, add an exception to the exception list, otherwise extract the loaded\n\tconfiguration values, which are a dict of name to value pairs.\n\n\tThe \""}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\twith logger.LogHandler() as handler:\n\t\tlogger_name = handler.name\n\t\tresult = parse(filename=filename)\n\t\tlogger.info(\"%s has parsed configuration: %s\", logger_name, result)\n\t\treturn result, handler.records\n\t\ndef parse(filename):\n\t\"\"\"Parse a single Configurator configuration file. Return a dictionary mapping configurator names to\n\ttheir corresponding dictionaries of settings. Return an empty dictionary in the\n\tevent of an invalid file.\n\t\"\"\"\n\tlogger = logging.getLogger()\n"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfiguration_filenames = get_valid_configuration_filenames()\n\tparsed_configurations = []\n\tfailed_files_log_collection = []\n\tfor configuration_filename in configuration_filenames:\n\t\tif len(failed_files_log_collection) > log_retention_count_per_log_file:\n\t\t\tlogging.info('Parsed configuration logs are kept at {} for a total of {} logs.'\n\t\t\t             .format(log_filename,\n\t\t\t                      len(failed_files_log_collection) + logs_"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tresults_dict = {}\n\tparsed_filenames = []\n\t# parse each file in config_filenames, store in results_dict\n\t\n\t# to be used with an iterable\n\tfor config_file_name in config_filenames:\n\t\ttry:\n\t\t\twith open(config_file_name, \"r\") as config_file:\n\t\t\t\t# Parse each line\n\t\t\t\t_config_data = load_config_file(config_file, filename=config_file_name)\n\t\t\t\tparsed_filenames.append("}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\ttry:\n\t\tconfigs_parsed = {}\n\t\tparse_errors = []\n\t\tfor filename in filenames:\n\t\t\ttry:\n\t\t\t\tconfiguration = load_configuration(\n\t\t\t\t\tfilename,\n\t\t\t\t\tvalidate_filenames=True,\n\t\t\t\t\tvalidate_configuration=True,\n\t\t\t\t\tdebug_loggers=debug_loggers,\n\t\t\t\t)\n\t\t\t\tif configuration is not None and configuration[1] is not None:\n\t\t\t\t\tconfigs_parsed[filename] = configuration[0]\n\t\t\t\telse:\n\t\t\t\t\tparse_errors.append(\n\t\t\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\treturn b(obj) if type(obj) is unicode else obj\n\ndef b(obj):\n\t\"\"\"\n\tThis function forces the bytes object ``obj`` to a binary representation.\n\t\"\"\"\n\tif type(obj) is not bytes:\n\t\treturn obj.encode(\"utf-8\")\n\treturn obj\n\ndef dct_size(n, **kwargs):\n\t\"\"\"\n\tThis function calculates a number of bytes needed for saving an array\n\t\"\"\"\n\treturn sizeof(n*{(n//2)*4 : 1, 1 : n} if k"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\telif isinstance(obj, str):\n\t\treturn obj.encode(\"UTF-8\")\n\telse:\n\t\traise TypeError(\"Can only encode strings or bytes\")\n\n\"\"\"\n\tThis class is a way to deal with binary strings using NumPy arrays.\n\tIt is also used in our implementation of NumPy's np.array.\n\t\"\"\"\nclass np_buffer:\n\t\"\"\"\n\tIn Python, np.array is an array\n\t\"\"\"\n\tdef __init__(self, data):\n\t\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\ndef get_encoded_data(data):\n\t\"\"\"\n\t\tThis function returns the object to be used in a request, \n\t\tfrom bytes, str, int, or numpy types, as required.\n\t\t\"\"\"\n\tif data is bytes:\n\t\treturn data\n\tif bytes == str or bytes == int or bytes == numpy or not isinstance(data, bytes):\n\t\treturn encode_obj(data)\n\t# TODO:"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str) == True :\n\t\treturn (obj.encode())\n\telif isinstance(obj, bytes) == True :\n\t\tif type(obj).__name__ == \"bytes\" :\n\t\t\treturn obj\t\n\telse :\n\t\traise ValueError\n\nclass Encoder(object) :\n\n\t\"\"\"  \n\tThis is the encoder object. It is an instance of a specific subclass of Encoding that is designed for the specific problem statement. \n\tThe specific subclass of :class:`Encoding` should be declared within this class\n\n\tArgs"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj,bytes):\n\t\treturn obj\n\telif isinstance(obj,str):\n\t\treturn str.encode(obj)\n\n\n\"\"\"\n\tThis function returns the string object corresponding to ``obj``\n\tin which there are two bytes replaced with the character '\\' and\n\tthe character '$'. \n\t\"\"\"\n\t\ndef string2bytes_win(obj):\n\tglobal PY2,UNICODE_SIZE\n\tif PY2:\n\t\treturn str2bytes_py2(obj).replace(b'\\x00',b'"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode()\n\telif isinstance(obj, object) and (not isinstance(obj, int)):\n\t\traise TypeError('can only convert bytes to bytes')\n\telse:\n\t\treturn obj\n\n\ndef hx(data):\n    def make_unicode(data):\n        try:\n            return data.decode('utf-8')\n        except UnicodeError as e:\n            logger.warning(\"Unicode decoding error: \", e)\n            return data.decode('ISO-88"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\treturn obj.encode('utf-8')\n\n_intTypes = ('int16', 'int32', 'int64', 'ubyte', 'uint16', 'uint32', 'uint64')\n_floatTypes = ('float32', 'float64')\n\nclass array_utils:\n\t@staticmethod\n\tdef ensure2D(x, dim_1 = None, dim_2 = None):\n\t\td1 = x.ndim\n\t\td2 = len(x.shape)\n\t\tif d1 == 1 and d2 == 2 : return x\t\t\t\t\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tobj=str(obj)\n\tif isinstance(obj,unicode):\n\t\treturn obj.encode('utf-8') #bytes object\n\telse:\n\t\treturn obj\n\t\t\n\t#if isinstance(obj, (str,unicode)):\n\t\t#if isinstance(obj,unicode):\n\t\t\t#return obj.encode('utf-8') #bytes object\n\t\t#elif isinstance(obj,str):\n\t\t\t #return bytes(obj, 'utf-8')\n\t#else:\n\t\t#print \"obj must be either a string or"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\treturn obj.encode('utf-8')\n\texcept AttributeError:\n\t\treturn obj\n\ndef array2string(arr, separator=',',\n\t\t\t\t\t\t\t\tfloat_format=None,\n\t\t\t\t\t\t\t\tmax_line_width=80):\n\t\"\"\"\n\tReturn a string of the elements in `arr`.\n\n\tThe output of this function is suitable to print in an area delimited table.\n\tAdditionally function will automatically determine a header width from the first\n\tline.\n\n\tParameters\n\t----------\n\tarr : 1D array-like\n\t\tInput array.\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\t\t\t# otherwise; nothing to do\n\n\n\"\"\"\n\tThis function takes the bytes object ``bytes_obj`` \n\tand the size ``size``  \n\tand returns the ``bytearray`` equivalent of ``bytes_obj``\n\twith the size ``size``. \n\t\"\"\"\ndef bytearray_from_bytes(bytes_obj, size):\n\treturn bytes_obj.decode(\"utf-8\").encode('utf-8', errors='"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(seconds=(ticks / 1000.0), tzinfo=tz)\n\ndef to_ms(cls, secs):\n\t\"\"\"\n\tCreate a time from seconds.\n\t\n\t:param secs: seconds\n\t:type secs: int\n\t\n\t:rtype: Time\n\t\"\"\"\n\treturn cls(secs)\n\ndef to_seconds(cls, ms):\n\t\"\"\"\n\tGet number of seconds from time.\n\t\n\t:param ms: delta\n\t:type ms: int\n\t\n\t:r"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(ticks, 1, tz=tz)\n\n\"\"\"\n@classmethod\ndef to_ticks(cls, obj, unit, tz=None):\n\t\n\t:param obj: a time (e.g. a datetime, Time, ISO time, or ISO timestamp)\n\t:type obj: Time|datetime|date|time|string\n\t:param unit: the length of the object in nanoseconds\n\t:type unit: int|float\n\t:param tz: timezone to interpret tzinfo with\n\t:type"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError\n\telif (ticks >= 86400000000000) or (ticks <= 0):\n\t\traise ValueError\n\telif ticks >= 4320000000000000:\n\t\treturn cls(ticks=ticks // 2678400000, tz=tz)\n\telif ticks >= 86400000000000:\n\t\treturn cls(ticks=ticks // 8640000000000, tz=tz)\n\telif ticks >= 64800000000000:\n\t\treturn cls(ticks=ticks // 2678400000"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls.from_ticks_int(int(ticks), tz=tz)\n\nclass Time(object):\n\t\"\"\"Simple time wrapper -- only microseconds are supported, and\n\talways UTC\"\"\"\n\t__slots__ = ('_seconds', '_tzinfo')\n\t\n\t_DATEPART_FORMAT = '%Y%m%d'\n\t_TIMEPART_FORMAT = '%H%M%S'\n\t\n\tdef __init__(self, year=None, month=None, day=None,\n\t             seconds="}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(tz or utc) + timedelta(-ticks//1000000)\n\n\ndef from_seconds(s, tz=None):\n\t\"\"\"\n\tCreate a time from seconds (nanoseconds since midnight).\n\n\t:param s: seconds\n\t:type s: float\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if seconds is out of bounds\n\t    (0 <= s < 6048*60*24)\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tnano_day = 24 * 60 * 60 * 1000000000\n\tseconds = ticks // nano_day\n\t\n\t# Find what timezone we're in, and add it to the microsecond\n\tif not tz:\n\t\ttz = Time.TZ_UTC\n\ttzinfo = tz\n\t\n\t_time = datetime.datetime.utcfromtimestamp(0).replace(tzinfo=tzinfo)\n\tsecs = (_time.second * 60 * 60) + (_time.minute * 60) + (_time.hour * 24)\n\t#"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tseconds = int(ticks) * 1.0\n\tnanoseconds = ((ticks % seconds) * 10) % 1000000000.0\n\tsecSinceMidnight = (\n\t\t(int(nanoseconds) + (int(nanoseconds) % 10000000)) / 1000000000)\n\treturn cls.from_seconds(secSinceMidnight, tz=tz)\n\n\ndef compute_time_diff(ts_A, ts_B=None, ns_A=None):\n\t\"\"\"\n\tCalculate the difference between two timestamps,"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not 0 <= ticks < 86400000000000:\n\t\traise ValueError(\"Tick %d(%d) is out of bounds\" % (ticks, ticks))\n\tseconds = ticks / float(1000000000)\n\treturn cls.from_float(seconds)\n\nif __name__ == '__main__':\n\timport sys\n\ttime = Time.from_ticks(123 * 1000000)\n\tprint(time, type(time))\n\tprint(repr(time))\n\tprint(repr(Time.from_nanoseconds("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttimestamp = cls.from_timestamp(0, tz=tz)\n\tticks = timestamp(ticks)\n\treturn cls.from_timestamp(ticks, tz=tz)\n\ndef from_ordinal(days_from_epoch):\n\t\"\"\"\n\tUse to return a timezone-aware, ordinal-only time.\n\t\n\t:param days_from_epoch: number of days from the specified calendar date\n\t:type days_from_epoch: int\n\n\t:rtype: Time\n\t:returns:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"Ticks must be a positive integer\")\n\telif ticks >= 86400000000000:\n\t\traise ValueError(\"Ticks must be less than 876400000000000\")\n\t\n\tmin_ticks = 86400000000\n\ttz_delta = tz.utcoffset(datetime.now(tz)).total_seconds()\n\t\n\tif abs(tz_delta) > timedelta(seconds=3).total_seconds():\n\t\traise ValueError(\"Ticks must be within 3 seconds of UTC\")\n\t\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn SUPPORTED_PROTOCOL_VERSIONS\n\n\t# Return the dictionary from the cache, if there is one.\n\tprotocol_map = PROTOCOL_MAP.get(protocol_version)\n\tif protocol_map is not None:\n\t\treturn protocol_map\n\n\tfor protocol_version in [None] + list(protocol_version):\n\t\ttry:\n\t\t\tprotocol_class = import_class_or_name(\n\t\t\t\tcls.protocol_class_prefix +\n\t\t\t\tstr(protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls.protocol_handlers_base.copy()\n\n\nclass ProtocolHandlerRegistry(Registry):\n\t\"\"\"\n\tExtension of the standard Python ``Registry`` type, which\n\tprovides facilities for managing protocol handler instances.\n\t\n\tThis class exists to simplify protocol handler management,\n\tallowing the registration of protocol handlers as well as the \n\tretrieval and calling of protocol handlers by their version\n\ttuple. \n\t\"\"\"\n\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize an empty protocol handler registry,\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = defaultdict(lambda: [])\n\tfrom.. import protocol\n\tfor version in protocol.versions():\n\t\tif protocol_version and (version!= protocol_version):\n\t\t\tcontinue\n\t\thandler = protocol.ProtocolHandler('%d.%d' % version)\n\t\tif handler.supports_protocol_v4:\n\t\t\thandlers['v4'].append(version)\n\t\tif handler.supports_protocol_v5:\n\t\t\thandlers['v5'].append(version)\n\t\tif handler.supports_protocol_v6"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version = normalize_version_tuple(protocol_version)\n\tif protocol_version is None:\n\t\tsupported_versions = cls._available_versions()\n\telif protocol_version in cls._peer_protocol_handler_map:\n\t\tsupported_versions = (protocol_version,)\n\telse:\n\t\tsupported_versions = []\n\t\tfor protocol_version in cls._available_versions():\n\t\t\tif protocol_version < protocol_version:\n\t\t\t\tsupported_versions.append(protocol_version)\n\t\t\t\tbreak\n\t\telse:"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif isinstance(protocol_version, tuple):\n\t\t# Single protocol version selected\n\t\tif protocol_version in cls._protocol_handlers:\n\t\t\treturn {protocol_version: cls._protocol_handlers[protocol_version]}\n\t\telif not protocol_version:\n\t\t\tif not cls._protocol_handlers:\n\t\t\t\tcls._protocol_handlers = {}\n\t\t\t\tfor version, handler in cls._SUPPORTED_VERSIONS.items():\n\t\t\t\t\tcls._protocol_handlers[version] = handler\n\t\telse"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tbase = cls.protocol_handlers.copy()\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version must be tuple\")\n\n\t\tmatching_protocol_handlers = dict()\n\t\tfor version, handler_class in cls.protocol_handlers.items():\n\t\t\tif protocol_version[0] == version[0] and protocol_version[1] == version[1]:\n\t\t\t\tmatching_protocol_handlers[version] = handler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\ttry:\n\t\tresult = {}\n\t\tfor hcls in cls._protocol_handlers:\n\t\t\thcls_def = (hcls._protocol_version,) if protocol_version is None else protocol_version\n\t\t\tif hcls._protocol_version in result:\n\t\t\t\tresult[hcls._protocol_version].append(hcls)\n\t\t\telse:\n\t\t\t\tresult[hcls._protocol_version] = [hcls]\n\t\treturn result\n\texcept (TypeError):\n\t\traise TypeError(\"'protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tvalid_versions = cls.VALID_PROTOCOL_VERSIONS\n\tif protocol_version:\n\t\tif type(protocol_version) is not tuple:\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\n\t\tif protocol_version not in valid_versions:\n\t\t\traise ValueError(\"invalid protocol version\")\n\n\t\thandlers_for_version = dict()\n\t\tfor protocol_version in valid_versions:\n\t\t\thandlers_for_version[protocol_version] = None\n\n\t\tif protocol_version in handlers_for_version:\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tsupported_versions = set(cls.protocol_versions(use_list=True))\n\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version must be a tuple\")\n\t\tif protocol_version not in supported_versions:\n\t\t\traise ValueError(\"protocol_version must be one of: %s\" %\n\t\t\t\tlist(sorted(supported_versions)))\n\n\t\tsupported_versions.remove(protocol_version)\n\n\t\tif not protocol_version:\n\t\t\t# We don't consider explicit version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tsupported_protocol_version_tuple = cls.protocol_version_tuple(\n\t\tknown_versions = cls._versions() if protocol_version is None else {protocol_version},\n\t\tdefault = protocol_version,  # Fallback to default protocol version\n\t)\n\t# Create handlers for all known protocol versions\n\tprotocol_handlers = {\n\t\tversion: cls(version=version)\n\t\tfor version in supported_protocol_version_tuple\n\t}\n\treturn protocol_handlers\n\n\n\nclass ProtocolHandler(metaclass"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tbookmarks = Bookmarks()\n\tfor value in values:\n\t\t_, bookmark = value.partition(\"*\")\n\t\tif bookmark:\n\t\t\tbookmarks.append_position(bookmark)\n\t\telse:\n\t\t\tbookmarks.append_item(0, item)\n\n\treturn bookmarks\n\n\n_INVALID_BM_FORMAT = \"Invalid Bookmarks format!\"\n_EXCLUDED_FORMAT = [\"b:*\"]\n_VALID_FORMAT = (\n\t\"'v'\",\n\t\"'V'\",\n\t\"'c'\",\n\t\"'"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tvalues = map(lambda x: x.strip(), values)\n\tfor key, value in zip(Bookmarks.KEY_TO_COL, Bookmarks.COL_TO_KEY):\n\t\tif key not in values:\n\t\t\tsetattr(cls, key, None)\n\t\telse:\n\t\t\tsetattr(cls, key, key_to_int_or_float(value))\n\t\t\n\tfor key, value in zip(Bookmarks.KEYS_REVERSED, Bookmarks.NAMES_REVERSED):\n\t\tif key not in values:\n\t\t\tcontinue"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t# Create Bookmarks object\n\treturn cls()\n\ndef from_json(json):\n\"\"\"\n\tCreate a Bookmarks object from a JSON object\n\t\n\tYou should not need to use this method unless you want a JSON-decoded\n\tbookmarks object and you want to deserialize a bookmarks JSON payload.\n\t\n\t:param json: JSON decodes a bookmarks JSON payload (raw bookmarks)\n\t:type json: str\n\t\"\"\"\n\t# Create Bookmarks object\n\treturn cls()\n\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tvalues = listify(values)\n\t\n\trettype = cls()\n\t\n\tif not values:\n\t\treturn rettype\n\t\n\tindex = 0\n\t\n\t# We need to parse values as if they came\n\t# straight from an svg or an xml object and not\n\t# an iterable string.\n\t\n\t# (For svg, that seems like an issue with pysvg)\n\tdata = BytesIO(str.encode(\"\\n\"*200))\n\t\n\tfor val in values:\n\t\tdata.seek(0"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tbookmarks = cls.empty(raw_ids=True)\n\tb = RawBookmark.from_raw_value(values[0])\n\tbookmarks.ids.append(RawBookmarkRef([0]))\n\tfor pair in values:\n\t\tb = RawBookmark.from_raw_value(pair)\n\t\tbookmarks.ids.append(RawBookmarkRef([b.id]))\n\t\tbookmarks.refs.update(b.parent_refs)\n\treturn bookmarks\n\n\nclass RawBookmark(namedtuple('RawBookmarkRef"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\t# create a dict to hold the raw values\n\traw = ''\n\t\n\t# add the entries\n\tfor val in values:\n\t\traw += val\n\t\n\t# return the object\n\t# NOTE: this will be a shallow copy\n\treturn cls(**{k:v for k, v in cls.deserialize(raw).items()})\n\n\n@classmethod\ndef from_filesystem(cls, path):\n\t\"\"\"\n\tCreate a Bookmarks object from its filesystem representation. \n\t\n\tIf you don't know where the book"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\ttry:\n\t\tmapping = parse_bookmarks_string(values)\n\texcept Exception as exc:\n\t\traise DeserializationError(\n\t\t\tf'Attempted to deserialize bookmarks string {values.decode()}'\n\t\t) from exc\n\t\n\tobj = cls()\n\tobj._from_dict(mapping)\n\treturn obj\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values)\n\n\nBookmarks.from_list = classmethod(from_list)\nBookmarks.from_raw_values = classmethod(from_raw_values)\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif len(values) == 0:\n\t\treturn Bookmarks()\n\n\tbms = []\n\tfor value in values:\n\t\tbookmarks = Bookmarks.parse(value)\n\t\tif bookmarks is None:\n\t\t\traise ValueError(\n\t'invalid bookmarks value:\\n' + value)\n\n\t\tbms.append(bookmarks)\n\n\tif len(bms) == 1:\n\t\treturn bms[0]\n\treturn bms\n\n\n\nclass Bookmarks(object):\n\t\"\"\"\n\tRepresents the bookmarks data.\n\t\n\tA ``"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif _PY3:\n\t\treturn cls(*(i.decode() if isinstance(i, bytes) else i for i in values))\n\t\t\n\treturn cls(*(i.encode() if isinstance(i, unicode) else i for i in values))\n\n\ndef serialize_prefs(prefs):\n\t\"\"\" Serialize preferences to a UTF-8 string\n\t\n\t:param prefs: A dictionary with preference names as keys\n\t:type prefs: dict\n\t    \n\t:return: A UTF-8 encoded string representing all preferences"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not seq:\n\t\treturn '', type(seq)\n\tseq = str(seq)  # in case the user gives a list or a set\n\n\t# See if it has a meaningful seq type\n\tif isinstance(seq, (list, set, tuple)):\n\t\t_seq_type = type(seq)\n\telse:\n\t\t_seq_type = StringTypes\n\t\ttry:\n\t\t\t_seq_type = type(seq)\n\t\texcept NameError:\n\t\t\tpass\n\t# if not, assume string\n\tif type(_seq_type)!= StringTypes"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tret = _get_seq(seq, bufsize)\n\treturn ret, type(ret)\n\ndef seq_t(*args):\n\"\"\"\n\tAccepts an argument or sequence of arguments,\n\ta type argument, and returns a variable\n\tgiving a generator or sequence in which\n\tthe elements where that type has been used.\n\tThe given argument is first converted\n\tto a sequence.\n\t\"\"\"\n\ttry:\n\t\tt = args[-1]\n\texcept IndexError:\n\t\tt = None\n\telse:\n\t\targs = args[:-1]\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, SEQUENCE_TYPES):\n\t\treturn (seq,) + (\"set\",)\n\treturn (seq, \"set\")\n\ndef _get_set_attr(seq, attr):\n\treturn {\n\t\t\"setattr\": _setattr,\n\t\t\"getattr\": _getattr,\n\t\t\"hasattr\": _hasattr,\n\t}.get(attr, lambda x, y: _check_type(x, y, \"getattr\"))(seq, \"seq\")\n\ndef _check_type(obj, type, method_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif seq is None:\n\t\treturn seq, tuple\n\tseq = as_bytes(seq)\n\tif isinstance(seq, text_type):\n\t\tseq = seq.encode(ENCODING)\n\treturn seq[:bufsize if bufsize is not None else 0], type(seq)\n\n\ndef _get_char_with_type(seq, bufsize=None):\n\tif seq is None:\n\t\treturn b'', (str, bytes, bytearray, array.array, list)\n\tseq = as_bytes(seq)\n\tif"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tis_str = not isinstance(seq, str)\n\n\tif isinstance(seq, dict):\n\t\t# for the values we just want to get the first,\n\t\t# which may come back as a generator\n\t\ttry:\n\t\t\tfirst = seq.values()[0]\n\t\texcept TypeError:\n\t\t\tfirst = seq.values()[next(iter(seq.values()))]\n\t\tif isinstance(first, dict):\n\t\t\trtype = 'dict'\n\t\t\tseq = first.values()\n\t\telif isinstance(first, set):\n\t\t\trtype ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = BUFSIZ\n\n\ttry:\n\t\t# buffer interface\n\t\tbuf = []\n\t\tappend = buf.append\n\texcept:\n\t\tbuf = array.array('B', b'')\n\t\tappend = buf.append\n\t\t\n\ttry:\n\t\tit = iter(seq)\n\texcept TypeError:\n\t\tappend(seq)\n\t\t\n\telse:\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\thead = next(it)\n\t\t\texcept StopIteration:\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\ttry:\n\t\t\t\ts ="}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\ttype = type(seq)\n\t\tfor t in type.__bases__:\n\t\t\tif str(t) == \"tuple\":\n\t\t\t\traise\n\texcept:\n\t\ttype = None\n\n\ttry:\n\t\tif type == tuple or str(type) == \"<type 'tuple'>\" \\\n\t\t\t\tor isinstance(seq, buffer):\n\t\t\treturn seq, tuple\n\texcept:\n\t\tpass\n\n\ttype = None\n\ttry:\n\t\tif type == None:\n\t\t\ttype = type(seq)\n\texcept:\n\t\tpass\n\n\t# If"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not isinstance(seq, str):\n\t\tseq = str(seq)\n\tseq = seq.strip()\n\tif \" \" in seq and seq[0] == \" \":\n\t\traise ValueError\n\tif not bufsize:\n\t\timport sys\n\t\tbufsize = sys.maxsize\n\ts, t = seq, None\n\tif seq[:3] in _seq3:\n\t\tt = _seq3[seq[:3]]\n\telif len(seq) > 2 and seq[:2] == \"re\":\n\t\tt = re\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize:\n\t\tbuf = buffer(seq, 0, bufsize)  # binary\n\t\tif ord(buf[:1]) == 10:  # newline    -> end of string\n\t\t\tbufsize = len(buf)\n\t\t\tbuf = buffer(seq, 1, bufsize)\n\telse:\n\t\tbufsize = len(seq)\n\n\t# no newline means that seq is a binary sequence (str)\n\n\tif is_text_string(type(seq)):  # if *seq* is an instanceof str\n\t\treturn seq, S_STR\n\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttype = \"s\"\n\tif not bufsize:\n\t\tbufsize = 10\n\tif isinstance(seq, tuple):\n\t\tseq = list(seq)\n\tif any(isinstance(_, string_types) for _ in seq):\n\t\t_, n, type = seq\n\t\ttype = type.lower()\n\t\tseq = islice(chain(iter(n), repeat(1)), bufsize)\n\t\tseq = flatten(islice(seq, bufsize//2), islice(seq, bufsize - (bufsize//2), None), isl"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\ts = None \t# for return\n\tif other is None:\n\t\tself._s = self._bins_scaled()\n\telse:\n\t\tself._s = self._bins_scaled(other)\n\t\tif self._s == 0:\n\t\t\traise LenaValueError(\"non-positive unscaled histogram\")\n\tif recompute:\n\t\tself._s = self._bins_scaled()\n\ts = self._s\n\t\n\treturn(s)\n\t\n\ndef shift(self, by=0, recompute=True):\n\"\"\""}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is not None:\n\t\tself.set_scale_or_recompute(other, recompute=recompute)\n\treturn self.scale_\n#@-node:compute_scale\n#@+node:fit_scale\ndef fit_scale(h, x):\n\t\"\"\"\n\tGiven the input histogram *h*, fit the scale\n\tto a linear function a + b*x, where a and b are the fitted parameters.\n\t\n\tThe function returns a 2-tuple ``(a, b), r_squared``.\n\t\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif self._data:\n\t\t\treturn self.data.scale()\n\t\ttry:\n\t\t\treturn self.__scale_private\n\t\texcept AttributeError:\n\t\t\treturn None\n\n\tif not recompute:\n\t\ttry:\n\t\t\tself.__scale_private\n\t\texcept AttributeError:\n\t\t\tpass # histogram was not yet computed\n\telse:\n\t\t#if self.__scale_private is None:\n\t\tself.__compute_scale()\n\n\tself.__scale_private = other\n\treturn other\n\n\t\ndef set_"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\traise NotImplementedError\n\t\n\t\n\t\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is not None and isinstance(other, float):\n\t\tassert not isfinite(self._scale)\n\t\tself._scale = other\n\t\tself._finite = self._finite and bool(isfinite(other))\n\treturn self._scale\n\ndef divide_scale(self, other, recompute=False):\n\"\"\"\n\tDivide scale of this histogram by *other*.\n\t\n\tIf *other* is ``None``, return division of scale *self* by.\n\tIf its scale was not computed before,\n\tit is computed and stored for"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\treturn ht[0][0]\n\n\tif recompute:\n\t\tself.scale = float(other) * ht[0][0]\n\t\treturn self.scale\n\telif self.scale == 0:\n\t\traise ValueError('cannot rescale to a zero scale')\n\n\tif other == self.scale:\n\t\treturn self.scale\n\t\n\tht2 = self.histogram()\n\ts = ht2[0] - ht2[1]\n\tht2[0] = float(other) * float"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tret = _scaled_args(self, other,'scale')\n\tself[self.mask] = 0.0\n\tnx = self._sum()\n\n\tif not nx:\n\t\tself._scale = 0.0\n\t\treturn\n\n\tif other is not None:\n\t\t# update self and mask\n\t\tself.scale = other\n\t\t# recompute self.mask and mask.counts\n\t\tself._cache_statistics()\n\t\t# recompute self._counts\n\t\tself._update_mask()\n\t# else, compute only _scale\n\n\tn"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is not None:\n\t\t# set and recompute (if not already computed)\n\t\tif len(self._counts) == 2:\n\t\t\tself._counts[0], self._counts[1] = other, other\n\t\t\tself._counts[3] = other * other\n\t\telse:\n\t\t\tself._counts = tuple(other if i!= 3 else j for i,j in enumerate(self._counts))\n\ttry:\n\t\t# scale and return\n\t\treturn self._counts[1]\n\texcept:\n\t\traise LauraRuntimeError"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t_check_zero_scale(self)\n\tif other is None: return self.scale\n\tif not_finite(other): raise err.LenaValueError(\"non-finite scale value\")\n\tif recompute or self._scale is None:\n\t\tself._scale = sum(h.data for h in self.flatten() if h.count > 0)\n\t\treturn self._scale\n\tself._scale = other\n\treturn self\n_cvar_scale = ('rescale_scale','scale')\n\n@prop_doc(scale)\n@mem"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself._check_sanity()\n\tif self._scale is None:\n\t\tif other is None:\n\t\t\treturn float(self.n) / self.width\n\t\telse:\n\t\t\tself.scale = float(other) / self.width\n\telse:\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\telse:\n\t\t\traise LenaValueError(\"cannot scale histogram\")\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tdef setScale(self, *args):\n\t\tif args:\n\t\t\tif type(args[-1]) in int_types:\n\t\t\t\targs += (args[-1] / 1000.0,)\n\t\t\tself._setField(\"scale\", tuple(args))\n\n\tscale = property(getScale, setScale)\n\n\t@property\n\tdef xscale(self):\n\t\t\"\"\"\\\n\t\tGet the xscale of the graph.\\\n\t\t\"\"\"\n\t\treturn float(self._getFieldAndErr(\"xscale\", 0.0)[0])\n\t\n\t@xscale"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn float(self.scale_x) / float(self.scale_y), float(self.scale_x) / float(self.scale_z)\n\tscale_x, scale_y = other\n\tif scale_x!= self.scale_x:\n\t\tscale_x = self.scale_x\n\t\tscale_y = float(self.scale_y) * float(scale_x) / float(self.scale_x)\n\t\tself.scale_x, self.scale_y = scale_x, scale_y"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t# default value\n\tdefault_kwargs = {'nofill':True, 'bg':'white'}\n\n\t# set the scale, if provided\n\tif other is not None:\n\t\tkw = self.kwargs.copy()\n\t\tkw.update(other)\n\t\tself._scale(**kw)\n\n\t\t# default value\n\t\tfor name in default_kwargs:\n\t\t\tif name not in kw:\n\t\t\t\tkw[name] = default_kwargs[name]\n\t\tself.kwargs = kw\n\n\t\t# check that the scale is meaningful\n\t\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tdef setScale(self, scale):\n\t\tpass\n\tscale = property(getScale, scale)\n\"\"\"\n    Get or set the horizontal/vertical shift of the graph.\n    \n    If *other* is ``None``, return the shift of this graph.\n    \n    If a numeric *other* is provided, shift to that value.\n    If the graph has unknown or zero shift,\n    shifting one will raise :exc:`~.LenaValueError`.\n    \"\"\"\n\tdef setShift(self, shift):\n\t\tpass\n\tshift = property("}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif not get_config('lena.scaled_graphs', False):\n\t\traise LenaValueError(_(\"Scaling is not supported on this graph type.\"))\n\treturn self.scale_fields(None, other) #pylint: disable=no-value-for-parameter,no-member\n\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tdef center(self):\n\t\"\"\"\n\tGet or set the center position of the graph.\n\t\n\tWhen *other* is None,\n\treturn the center position of this graph\n\t\"\"\"\n\"\"\"\n\tWhen *other* is ``None``,\n\treturn the center position of this graph\n\t\"\"\"\n\tdef size(self):\n\t\"\"\"\n\tGet or set the overall size of the graph.\n\t\n\tThis is used to apply transformations to the\n\tgraph's bounding box.\n\t\n\tWhen *other* is None,\n\treturn the overall size of"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass\ndef translate(self, other=None):\n\"\"\"\n\tGet or set the translation of the graph.\n\t\n\tIf *other* is ``None``, return the translation of this graph.\n\t\n\tIf *other* is a point or 3-tuple or 4-tuple\n\twith the coordinates in the range ``[0, 1]``,\n\tthen translate *other* to the graph.\n\tIf the graph has unknown or zero translation,\n\ttranslation that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self._scale\n\tself._set_scale(other)\n\t\n\t\n\t\n\t\n\t\ndef get_point(self, coords, **kwargs):\n\"\"\"\n\tGet a point in this graph.\n\t\n\t*coords* is a triple of floats.\n\tThe returned structure has the same type as *coords*.\n\t\n\tKeyword arguments *kwargs* may be given.\n\tSee :class:`.LenaPoint` for additional information.\n\t\"\"\"\n\timport warnings\n\twarnings.warn"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tdef setScale(self, xscale=1, yscale=1, zscale=1):\n\"\"\"\n\tSet the parameters of a scale graph.\n\n\tBoth *xscale* and *zscale* are passed in as\n\t:class:`numbers.Real` values.\n\t\n\tIf either *xscale* or *scale* is ``None``,\n\tall scales are assumed to be the same.\n\tIf either *xscale* or *zscale* is ``-1``,\n\tthey will be dropped,\n\tif other is ``-1"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn _cv.scale(self)\n\treturn _scale(self._graph, other)\ndef scale_xy(self):\n\"\"\"\n\t\tGet a copy of the current scaled XY scale.\n\t\t\n\t\tNote that by default :meth:`scale` uses first, while this one uses last.\n\t\t\n\t\tReturns a tuple (x_start, x_end, y, y_start, y_end).\n\t\tThe returned range corresponds to the current graph's coordinate system.\n\t\t\"\"\"\n\treturn _scale_xy(self._graph)"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\timport numpy as np\n\tfrom collections import Iterable\n\t# TODO: check that hist has no context,\n\t# because it is the only input parameter\n\t# (and we don't want to set it to defaults or anything)\n\tif check_list(get_coordinate):\n\t\tassert(len(get_coordinate)==2)\n\telse:\n\t\tget_coordinate=\"left\"\n\tif isinstance(hist, Iterable):\n\t\thist = list(hist)\n\tif isinstance(hist, histogram):\n\t\tif \"numpy\""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn hist_to_graph_impl(hist, make_value, get_coordinate, False, False, False, True)\n\ndef histogram_graph_to_graph(graph, make_value=None, get_coordinate=\"right\",\n\"\"\"\n\tConvert a :class:`.graph` to a :class:`.graph`.\n\tThis is only possible if *graph* is created from histogram_graph_to_graph.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is the bin"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tdef make_value_context(make_value, value, context):\n\t\tdef make_value_context1(make_value, value, context):\n\t\t\tvalue, context = value; context.set_for_hist(True)\n\t\t\treturn make_value(value)\n\t\tmake_value_context1.__doc__ = doc = \"%s(%s)\" % (make_value, value)\n\t\tdoc_1 = doc.replace(\"\\n\", \" \").replace('    ','') ; def\n\t\tassert \"\\n\\n%s\\n\" % doc"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tret = graph(x_coordinate=get_x_coordinate(hist), \n\t\t\t     y_coordinate=get_y_coordinate(scale,\n\t       \t              get_coordinate,\n\t        \t          get_value(hist, get_value),\n\t\t\t            data=get_value(hist, make_value),\n\t\t\t\t      make_value))\n\t#ret.field_names = [ \"x\", \"y\" ]\n\t#ret.y.title = \"y\"#\"Mean of the distribution\"\n\t#add_title(ret"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# hist should be a list of bin objects\n\t# create a graph from their information\n\t# and adjust its scale (maybe)\n\tif get_coordinate == \"middle\":\n\t\tfor coord in \"x\", \"y_min\", \"y_max\":\n\t\t\tsetattr(make_value, coord, getattr(make_value, \"coordinate\").middle)\n\tif get_coordinate == \"right\":\n\t\tsetattr(make_value, \"coordinate\", center)\n\tif get_coordinate == \"left\":\n\t\tsetattr(make_value, \""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\timport collections\n\t\n\t# check that the first element is not a context\n\tif isinstance(hist[0].context, type(None)):\n\t\tif hist[0].data is not None:\n\t\t\treturn _hist_to_graph_no_context(hist, make_value, get_coordinate, field_names, scale)\n\t\telse:\n\t\t\treturn _hist_to_graph_no_data(hist, make_value, get_coordinate, field_names, scale)\n\t\n\t# check that the context is a field names tuple\n\tassert"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\thist = hist.to_array()\n\tassert hist\n\tgraph = histogram(\n\t\tlen(hist),\n\t\thist\n\t)\n\tif get_coordinate:\n\t\tcenter = len(graph.vertexes()) // 2\n\t\tgraph.vertexes()[center][get_coordinate] = 1.0\n\tif make_value is None:\n\t\tmake_value = lambda bin_: (bin_.content, bin_.n)\n\treturn graph.to_graph(\n\t\tmake_value=make_value,\n\t\tfield_names=graph."}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif get_coordinate not in \"left right middle\":\n\t\traise ValueError(\"get_coordinate must be 'left', 'right' or'middle'\")\n\tscale = (1, 0, 0) if not get_coordinate else None\n\tvalue_coord = \"right\" if get_coordinate == \"right\" else None\n\tfield_names = (\"x\", ) if not make_value else (\"x\", \"y_mean\", \"y_mean_error\", value_coord)\n\tscale_y_mean = scale_y_mean = (True, False)"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tassert_type(field_names, str, allow_none=True, arg_name=\"field_names\")\n\tfor name in field_names: assert_type(name, str, arg_name=field_names)\n\t\t\n\tif type(scale) is not bool: assert_type(hist, histogram.Histogram, arg_name=\"hist\")\n\t\n\tmake_value = lambda _, **kwargs: kwargs\n\tcontext = {\"x\": 0}\n\tif make_value is not None:\n\t\tif get_coordinate!=\"left\":"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tdef __init__(self, hist, make_value=None, get_coordinate=\"left\",\n\t\t\"\"\"\n\t\t\tClass constructor. Do not call outside.\n\t\t\"\"\"\n\t\tdef get_bin(bin_):\n\t\t\tbin_.add_context(hist)\n\t\t\treturn bin_\n\t\t\n\t\tfield_names = (\"x\",) + zip(*_flatten(x*[None, get_field_name] for x\n\t\t\tin hist.bin_array))\n\t\tdata = {'data': [[*it] for it in zip(*[x.data for x in"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not tentative and not iface.implementedBy(candidate.__class__):\n\t\traise pywintypes.com_error(\n\t\t\t-2147467261,\n\t\t\t'Interface \"%s.%s\" not implemented by %r' % (\n\t\t\t  candidate.__class__.__module__, candidate.__class__.__name__,\n\t\t\t  iface\n\t\t\t  ),\n\t\t\t)\n\ttry:\n\t\tmissing = interface.get(\"__provides__\", ()) + list(iface)\n\texcept AttributeError as e:\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif iface is None:\n\t\tiface = providedBy(candidate)\n\t\tif candidate is None:\n\t\t\traise Invalid(\"Could not find class implementing interface %r (None)\"\n\t\t\t\t% iface)\n\tif iface is Interface:\n\t\tif isinstance(candidate, type):\n\t\t\tif not candidate.__implemented__:\n\t\t\t\traise Invalid(\"Class %s is not implemented\" %\n\t\t\t\t\t(getattr(candidate, '__module__', None) or \"<unknown>\",))\n\t\t\treturn candidate\n\n\tif inspect.isclass(candidate):\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfor error in checker(iface, candidate, tentative, vtype):\n\t\traise error\n\treturn True\n\n\ndef verify(iface, candidates, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that the *candidates* might correctly provide *iface*.\n\t\n\tThis is similar to :func:`_verify`, except we check all the candidates\n\tto find one that provides *iface*. This checks if:\n\t\n\t- The interface is implemented by the candidates (see :func:`_verify`)\n\t\n\t-"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t_verify_#untilcodev6\n\t_verify_aftercodev6\n\t\n\t# TODO: we could make something more intelligent here now that methods,\n\t# attributes and __implemented__ are actually lists\n\n\tif tentative:\n\t\t# skip checking for other kinds of declarations, since we are trying to\n\t\t# verify some declarations were provided.  see below\n\n\t\tif not iface.providedBy(candidate):\n\t\t\treturn True\n\t\tfor method in iface.optional:\n\t\t\tif method not in candidate:\n\t\t\t\treturn False\n\t\treturn True\n\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Verify interface class declaration\n\tdeclared = getattr(candidate, iface, None)\n\tif declared is None:\n\t\tif tentative or (request is None):\n\t\t\treturn # Not Implemented\n\t\t# If tentative, and no request, report what we would do\n\t\ttry:\n\t\t\tdeclared = provider_validate(\n\t\t\t\tqualifiediface(iface, interface.__module__),\n\t\t\t\tcandidate, tentative)\n\t\texcept Exception:\n\t\t\tprovider_raise_from(_providedBy_miss_message, provider_name(candidate) or iface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t# Check providedBy, if tentative, skip this check:\n\tif tentative or iface.providedBy(candidate):\n\t\t# We don't have enough information\n\t\treturn True\n\t\n\t# Does it define at least the minimal interface?\n\tif (not iface.implementedBy(candidate)) and vtype is not Interface:\n\t\t# Nope, skip this check\n\t\treturn True\n\t\n\tdef _checkMethod(iface, name):\n\t\tmethod = iface.get(name)\n\t\tif not method:\n\t\t\treturn\n\t\ttry:"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif issubclass(candidate, providedBy(iface)):\n\t\treturn True\n\tif isinstance(candidate, interfaces.CannotAdapt):\n\t\t# self-type\n\t\treturn False\n\t# Does it provide the interface?\n\tif (iface.providedBy(candidate)\n\t\t\t\tand (iface not in candidate.__iro__)):\n\t\treturn True\n\tif tentative and providedBy(candidate):\n\t\treturn True\n\t# Does it provide the interface, with the required methods?\n\tif not tentative:\n\t\trequired = implementedBy(if"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t_v = 1  # turn off for performance\n\t#_iface_name = iface.__name__\n\t#if isinstance(candidate,  (ClassProvidesBase,  implementedBy, providedBy)):\n\t#\tcandidate = candidate.__implemented__\n\tattr_resolution = 0\n\treq_resolution = 0\n\tfor c in candidate:\n\t\tif c == '':\n\t\t\tbreak\n\t\tif isinstance(c, basestring):\n\t\t\tiface, method_name = c.strip().split('.', 1)"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tiface.verify(candidate)\n\n\ttry:\n\t\tfor iface in iface.flattened():\n\t\t\tif not (iface.providedBy(candidate) or \\\n\t\t\t\t\tiface.providedBy(candidate) and tentative):\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn True\n\texcept TypeError as te:\n\t\tmsg = 'TypeError while considering {!r} for {!r}'.format(\n\t\t\tcandidate, iface)\n\t\traise Invalid(msg) from te\n\n\targs = tuple(iter_attr_names(iface))"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\tresult = True\n\tif not tentative and not iface.providedBy(candidate):\n\t\tyield (\n\t\t\t\"Candidate doesn't claim to provide `%s`; it only claims \"\n\t\t\t\"`%s`\" % (iface.getName(), candidate.__class__.__name__)\n\t\t)\n\t\tresult = False\n\n\t# Check method signatures\n\tfor method, desc in iface.getMethods():\n\t\tif not tentative and not method.getSignatureInfo().matches(candidate):\n\t\t\tyield \"Methods do not match ("}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef _defaultVerifyObjectMethod(iface, candidate, tentative):\n\t\tif not tentative and hasattr(candidate, 'provides'):\n\t\t\ttry:\n\t\t\t\tiface.providedBy(candidate)\n\t\t\texcept TypeError:\n\t\t\t\treturn  # ProvidedBy uses __provides__\n\t\t\tiface = iface.declarations()[0]\n\t\t\treturn (iface, candidate)\n\t\t\n\t\tif(candidate is None or not hasattr(candidate, '__provides__')\n\t\t\t\tor not iface.implementedBy(candidate)):"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif tentative and IPyramidRequest.providedBy(candidate):\n\t\t# Allow for a tentative object coming from a thread or something\n\t\treturn True\n\n\tif not verifyObject(iface, candidate):\n\t\treturn False\n\n\treturn True\n\n\ndef providedBy(iface_or_candidate):\n\t\"\"\" Check whether the interface exists in the candidate.\n\n\tAn interface has been provided by the candidate. This is\n\tintended for use with Zope interfaces and declarative style,\n\twhere the caller knows the interface is declared at declaration\n\tlevel, but cannot yet be convinced"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tverifier = Verifying(cached_callables.pop(iface, []))\n\trequired = required_attr_names(iface)\n\tkw = dict(tentative=tentative)\n\terrors = set()\n\ttry:\n\t\tprovided = candidate if tentative else providedBy(candidate)\n\texcept TypeError:\n\t\treturn False\n\tverifier.checkAttrs(required, provided, errors)\n\tmethods = list(provided)\n\tkw.update(\n\t\tmethod_name=method_name, method_or_constructor=ver"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tresult = True\n\tfor attr, name, info in iface.getTaggedValue('__provides__'): info['name'] = name\n\tif tentative: results = list(_verifyProvides(candidate))\n\telse: results = [(iface, candidate, info)]\n\tfor iface, candidate, info in results:\n\t\tfor attr, name, info in iface.getTaggedValue('__provides__'):\n\t\t\tif not hasattr(candidate, attr): continue\n\t\t\tgetattr(iface, attr).__get__"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tassert iface is not None\n\tif tentative and iface.implementedBy(candidate.__class__):\n\t\t# See comment above; this is the same example\n\t\treturn True\n\n\tif not iface.implementedBy(candidate.__class__):\n\t\t# The candidate does not claim to provide the interface\n\t\tassert iface not in candidate.__provides__, (candidate, candidate.__provides__, iface)\n\t\treturn False\n\telse:\n\t\t# For interface claiming to provide an interface which the candidate does\n\t\t# not claim"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tglobal _verify_failure\n\t\n\tif not tentative:\n\t\tiface = provideUtility(iface, provided)\n\t\tif not iface:\n\t\t\treturn False\n\n\t# This is the \"fast\" check -- we are about to instantiate the class as\n\t# much as possible\n\tc = candidate\n\tbases = provided if tentative else provided.__bases__\n\tfor checkFunc in verifyObject_checkers:\n\t\ttry:\n\t\t\tcheckFunc(provided, bases, c)\n\t\texcept SkipError:\n\t\t\treturn False\n\t\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tchecker = iface.extends(Checker)\n\t\n\t# Check if interface contains providedBy interface\n\tif checker.providedBy(candidate):\n\t\tif checker.providedBy(iface): # pylint:disable=too-many-boolean-expressions\n\t\t\treturn True # noqa; pylint:disable=too-many-boolean-expressions\n\t\n\tif checker.providedBy(iface):\n\t\tif not tentative:\n\t\t\traise BrokenImplementation(candidate, iface)\n\t\n\t# if we got"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tvalue = candidate\n\t\tfor check in check_implementer(iface):\n\t\t\tvalue = check(value)\n\t\t\tif value:\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn tentative\n\t\t\t# No non-implementation checks on tentative values\n\t\tfor check in check_provides(iface):\n\t\t\tvalue = check(value)\n\t\telse:\n\t\t\treturn tentative\n\t\tfor check in check_methods(iface, value):\n\t\t\tcheck(value)\n\t\telse:\n\t\t\treturn tentative\n\t\tfor check in check_attrs(iface,"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef _failingClass(reason):\n\t\tif reason is None: # nocover\n\t\t\treturn \"<failing class>\"\n\t\telse:\n\t\t\treturn \"<failing class, because '%s'>\"%reason\n\n\tif hasattr(object, \"__name__\"):\n\t\tname = object.__name__\n\telse:\n\t\tname = repr(object)\n\n\tif tentative:\n\t\tfailures = _collectInterfaceTentatives\n\telse:\n\t\tfailures = _collectInterfaceFailures\n\n\trequired = list(iface)\n\ttried = set()"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Verify that the class is providing the interface\n\tif not iface.implementedBy(candidate.__class__):\n\t\tif tentative:\n\t\t\treturn None\n\t\traise Invalid(\n\t\t\t\"Class %s doesn't provide the interface %s\" % (\n\t\t\t\tcandidate.__class__, iface)\n\t\t)\n\t# Check that the candidate object implements the interface\n\tif not iface.implementedBy(candidate):\n\t\tif tentative:\n\t\t\treturn None\n\t\traise Invalid(\n\t\t\t\"%s doesn't implement the interface %s\" % ("}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif tentative:\n\t\twarnings.warn(\n\t\t\t\"In API compatibility mode we use only interface declarations\")\n\n\td = tentative and pyattrs._declarations.values() or pyattrs._declarations\n\n\tfor interface in iface:\n\t\t# Check if it has been explicitly provided or not\n\t\tif not interface.provided and interface.name not in d:\n\t\t\treturn 0\n\n\t\tif interface not in d:\n\t\t\t# If it is not in the definition it could be that it is provided\n\t\t\t# by one of the bases.\n\t\t\tfor b in iface:\n\t\t\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif tentative and iface!= candidate.__class__.__name__:\n\t\traise TypeError(\n\t\t\t(\"%r not in %r's MRO (tentative=True)\"\n\t\t\t\"[possible Python bug]\")\n\t\t\t% (candidate, iface))\n\tfor cls in candidate.__mro__:\n\t\tif cls.__name__ == iface:\n\t\t\treturn cls\n\t# invert based on expected missingness\n\tmissing = ((tentative and iface) or (not tentative and \"I don't know how to make %s"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tinterface, cls = candidate.split('.')\n\tc = __import__(interface)\n\t__import__(interface)\n\ti = getattr(c, interface)\n\tci = getattr(c, cls)\n\tprint(iface, ci) #DEBUG\n\tif isinstance(ci, interface) or isinstance(i, interface):\n\t\tif tentative:\n\t\t\traise ValueError(interface + \" not expected to be a proxy\")\n\t\treturn True\n\traise ValueError(\"cannot verify %s in %s\" % (cls, interface"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\treturn iface.providedBy(candidate)\n\ndef addMethod(obj, name, function, tentative=False, docs=''):\n\t\"\"\"\n\taddMeth(obj, string, class) -> None\n\t\n\tAdd a method *function* to class *obj*, setting the attribute *name* on\n\tthe object. *tentative* sets the attribute *tentative* on the\n\tattribute. *docs* sets the docs to *function*.\n\t\"\"\"\n\ttype = type(obj)\n\tsetattr(obj, name, function)\n\tobj"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tverifier = unittest.skipUnless(\n\t\ttentative or candidate is not None, \"No alternate implementation for %s\" % (iface, ))\n\tverifier(candidate)() if (candidate is not None) else verifier\n\t\n\nverifyClass(regex, 'compoundRegex', False)\nverifyClass(int, 'compoundIntegerConstant')\nverifyClass(str, 'compoundStringConstant')\nverifyClass(iface, 'compoundClassDeclaration')\nverifyClass(iface, 'compound"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# correct interfaces are better than wrong ones\n\tif not tentative and not candidate.providedBy(iface):\n\t\treturn\n\treturn _verifyClass(iface, candidate, tentative, )\n\ndef _findObject(name, ob, tentative=False):\n\t\"\"\"\n\tReturn an object.  Look in *ob* for an attribute named *name*\n\tIf not present, or it's in *tentative*\n\t\"\"\"\n\t# first try without the tentative param\n\tob = getattr(ob, name.replace(\"_\", \"\"), getattr(ob,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\treturn candidate in _implements(iface) or tentative and iface in getBases(candidate)\n\nfrom.interfaces import IObjectWillBeAddedEvent\nfrom.interfaces import IIndicateDeletionEvent\nfrom.interfaces import IObjectAddedEvent\nfrom.interfaces import IObjectRemovedEvent\nfrom.interfaces import IWillRemoveEvent\n\nimport traceback\nfrom zope.event import subscribers, notify\nfrom zope.dottedname import resolve\nfrom zope.component.event import ObjectAddedEvent\nfrom zope.component.event import ObjectWas"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# some simple checks\n\tif not isClassOrInterface(candidate):\n\t\traise Exception()\n\tif not isClassOrInterface(iface):\n\t\traise Exception()\n\t# try to import it first\n\ttry:\n\t\tmodulename, attribute = candidate.rsplit(\".\", 1)\n\t\t__import__(modulename, {}, {}, fromlist=[attribute])\n\texcept:\n\t\tpass\n\t# second, try to introspect it\n\tinst = getattr(candidate, \"__init__\", None)\n\tif inst:\n\t\targs"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif (IInterface.providedBy(candidate)\n\t    and IInterface.providedBy(iface)\n\t    and candidate!= iface):\n\t\traise WrongAnswer\n\tif tentative and IAdapter.providedBy(candidate):\n\t\treturn\n\tif not IInterface.providedBy(candidate) and not IAdapter.providedBy(candidate):\n\t\traise Error\n\tverifyClass(IInterface, iface)\n\"\"\"\n###########################################################################\n## WMI Code\n###########################################################################\n# The following is the code fragment from this answer(s):\n\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not iface.providedBy(candidate):\n\t    return 0\n\n\tif iface is type(candidate):\n\t    if type(candidate)!= type(interface):\n\t        return 0\n\tif not tentative and \\\n        issubclass(candidate,  InterfaceClass):\n\t    c_interface = candidate\n\t    candidate = None\n\tinterface = resolve.maybeByName(c_interface, candidate)\n\tif interface is None:\n\t    return 0\n\treturn 1\n\n\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tclass Meta(type):\n\t\tdef __new__(meta, name, bases, dct):\n\t\t\tif '__metaclass__' in dct and dct['__metaclass__'] is not meta:\n\t\t\t\traise TypeError(\"__metaclass__ already set to: {0}.{1}\".format(dct['__module__'], dct['__metaclass__'].__name__))\n\t\t\tif explicit_mc is not None:\n\t\t\t\tdct['__metaclass__'] = explicit_mc\n\t\t\tif not bases:\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = None\n\n\tfor base in bases:\n\t\tif hasattr(base, '__qualname__'):\n\t\t\tclsname = base.__qualname__\n\t\telif hasattr(base, '__name__'):\n\t\t\tclsname = base.__name__\n\t\t\t# for namedtuple\n\t\t\tif clsname == 'nt':\n\t\t\t\tclsname = 'py.collections.namedtuple'\n\t\t\telse:\n\t\t\t\tclsname = 'py.'+clsname\n\n\t\tif meta is None:\n\t\t\t\t#### try to determine first\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdefining = type(explicit_mc or object) if explicit_mc is not None else None\n\t# Use type explicitly provided in __metaclass__\n\tif defining and not inherit_from(bases, defining):\n\t\traise TypeError(f\"__metaclass__ {define=} must be a subclass of class {bases}\")\n\tif not defining and not bases:\n\t\tprint('ERROR: define __metaclass__')\n\t\tprint('      bases: {'+','.join([str(_x) for _x in bases if _x is not"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = class_or_tuple_types.determineMetaclass(bases)\n\t#__metaclass__ may be redefined in metaclass, unless explicitly set\n\tif explicit_mc is not None:\n\t\tif explicit_mc is False or explicit_mc is meta:\n\t\t\tmeta = explicit_mc\n\t# check for valid metaclass\n\tif meta is any or issubclass(meta, class_or_tuple_types):\n\t\treturn meta\n\telif meta is _class_type: meta = type  # find a class\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmcs = None\n\texplicit_mc = explicit_mc or []\n\tfor base in bases:\n\t\tif isinstance(base, (NodeType, tuple)):\n\t\t\tbase = base.__class__\n\t\tif (mcs is None) and ((explicit_mc and base in explicit_mc) or not explicit_mc):\n\t\t\tmcs = base\n\treturn mcs\n\ndef mergeBases(bases, mcs, attrs):\n\t\"\"\"\n\tMerge base classes into a new base\n\t\"\"\"\n\tbases = (mcs,) +"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t# Allow for two different forms of __metaclass__\n\t# Default: type()\n\t# from six.moves import builtins\n\t# builtin_default = builtins.__class__\n\t# class metaclass:\n\t#\tdef __call__(cls):\n\t#\t\treturn super(metaclass, cls).__call__.cls\n\t\n\t# Use the 1st base metaclass of the class definition, if it exists.\n\t#   Otherwise, use builtin_default\n\tif explicit_mc is not None"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = implicit_mc = decl = None\n\tif explicit_mc is not None:\n\t\tif not explicit_mc:\n\t\t\t# TODO: determine class.__class cell from MRO\n\t\t\tif not explicit_mc_for_mcs(bases, None):\n\t\t\t\tbases = mro_for_mt(bases)\n\t\tmeta = explicit_mc\n\telif bases:\n\t\tdecl = bases[0]\n\t\tfor base in bases[1:]:\n\t\t\tif meta is None:\n\t\t\t\tif type(base)!= ClassCell:\n\t\t\t\t\tbase,"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = explicit_mc or IMetaclass\n\tif isinstance(meta, (type, tuple)):\n\t\treturn meta\n\t# Find last declared metaclass\n\tmetas = reversed(bases)\n\tfor meta in metas:\n\t\tif isinstance(meta, MetaClass) and meta is not IMetaclass:\n\t\t\treturn meta\n\treturn meta\n\n#==============================================================================\n\nclass MetaClass(type):\n\t\"\"\"\n\tNew-Style class for declaring abstract (i.e. not-yet-defined) methods\n\tthat can be replaced for"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = class_type\n\tif explicit_mc:\n\t\tfor base in bases:\n\t\t\tif __metaclass__ is not None:\n\t\t\t\terr = (\n\t\t\t\t\t'error computing metaclass for builtin '\n\t\t\t\t\t+ repr(base) +\n\t\t\t\t\t':\\n'\n\t\t\t\t\t+ 'explicit __metaclass__ found: '\n\t\t\t\t\t+ repr(explicit_mc))\n\t\t\t\traise TypingError(err)\n\t\treturn explicit_mc\n\n\t# need to consult __metaclass__ of bases\n\tfor base in bases:\n\t\tif isinstance(base"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\t# Explicit metaclass in __metaclass__ or explicit __metaclass__ declaration\n\t\tif hasattr(explicit_mc, '__class__') and explicit_mc.__class__ is not type:\n\t\t\t# explicit __metaclass__ declared via class statement is already a class\n\t\t\tbases = (explicit_mc,) + bases\n\t\t\texplicit_mc = None\n\n\tif PY36_PLUS:\n\t\treturn _determineMetaclass_superclass_hack(bases, explicit_mc)"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\ndef clear(self):\n\tD = {}\n\tfor k in iter(self):\n\t\tself[k] = D[k]\n\tself.store = {}\n\tself.removed = set()\n\tself.cache = {}\n_init_collections()\nclass lazyattr(_collections_abc.Mapping):\n\tdef __init__(self, attr):\n\t\tself.attr = attr\n\t\tself.__doc__ = getattr(attr, '__doc__')\n\tdef __getitem__(self, key):\n\t\tvalue"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\texcept:\n\t\tif default is self.__marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\treturn value\n\t\ndef setdefault(self, key, default=None):\n\t\"\"\"S.setdefault(k[,d]) -> S.get(k,d), also set S[k]=d if k not in S\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n\ndef range(start, stop=None, step=1):\n\"\"\"\nReturn an object used to iterate a number of steps starting from index start.\n\nArgs:\n\tstart (int): start position/index\n\tstop (int): stop position/index\n\tstep (int): step value\n\t\nReturns:\n\titerator: range object\n\t\nNote:\n\tThe stop value is noninclusive, i.e start and stop value are in the range, not inclusive.\n\tSo range(stop) is equal to range(start, stop).\n\"\"\"\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n\ndef rpop(self):\n\t\"\"\"\n\tRemove and return the last item in the queue.\n\t\"\"\"\n\tpass\n\ndef popleft(self):\n\t\"\"\"\n\tRemove and return the first item in the queue.\n\t\"\"\"\n\tpass\n\ndef clear(self):\n\t\"\"\"\n\tRemove all items from the queue.\n\t\"\"\"\n\tpass\n\ndef count(self, item):\n\t\"\"\"\n\tReturn the number of times a key is repeated in the queue.\n\t\"\"\"\n\tpass\n\ndef setdefault(self, key,"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\ndef setdefault(key, defval=None):\n\t\"\"\"\n\tIf key is in the dictionary, return its value.\n\tOtherwise insert key with a value of defval and return defval.\n\t\n\t\n\t\n\t\n\tsetdefault(key[, default]) -> value\n\tIf the key is in the dictionary, return its value.\n\tIf not, insert the key with a value of default and return default.\n\t>>> d = {'key1': 1, 'key2': 2, 'key3': 3}\n\t>>> d.setdefault('"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\ndef items(self, /__key=_symbol):\n\tpass\ndef isidentifier(self, /__raise=False, /__strict=True):\n\tpass\ndef iteritems(self, /__key=_symbol):\n\tpass\ndef iterkeys(self, /__key=_symbol):\n\tpass\ndef itervalues(self, /__key=_symbol):\n\tpass\ndef keys(self, /__key=_symbol):\n\tpass\ndef rename(self, *args, **kwds):"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\timport dbi as db\n\tif key in self._d:\n\t\tdel self._d[key]\n\treturn db.dbi_delete(self._db, None, None, self._d)\n\n\ndef __delitem__(self, key):\n\tself.pop(key)\n\n\ndef close(self):\n\t\"\"\"\n\tC.close() -> None\n\t\n\tClose the mailbox.\n\t\"\"\"\n\tpass\n\n\ndef dump(self):\n\t\"\"\"\n\tD.dump()\n\t\n\tEmulate the Unix 'cat' command.  This"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self.__tokdict:\n\t\treturn self.__tokdict.pop(key)\n\tif default is self.__marker:\n\t\traise KeyError(key)\n\treturn default\n\ndef clear(self):\n\t\"\"\"\n\tClear all elements and results names.\n\t\"\"\"\n\tfor elem in self.__toklist:\n\t\telem[2:] = self.__tokdict.values()\n\tself.__toklist = []\n\tself.__tokdict.clear()\n\treturn self\n\ndef full"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n\n\ndef has_key(self, key):\n\t\"\"\"\n\tD.has_key(k) -> true if D has the key k, false otherwise.\n\t\"\"\"\n\tpass\n\n\ndef update(self, E=None, **F):\n\t\"\"\"Update D from E and F: for k in E: D[k] = E[k] (merge)\n\t    or\tfor k in E: D[k] = E[k]\n\t\t\t or\n\t   for k in F: D[k] = F[k]\n\t\"\"\"\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tpass\n\treturn None\ndef clear(self):\"\"\"\n\tD.clear() -> None.  Remove all items from D.\n\t\"\"\"\n\tpass\n\ndef copy(self):\"\"\"\n\tReturn a shallow copy of D.\"\"\"\n\tpass\n\treturn set()\ndef difference(self, other):\"\"\"\n\tReturn a new set with elements in the original set but not in the other.\n\t\n\t**NOTE**: Mutates the original.\n\t\"\"\"\n\tpass\n\ndef difference_update(self, other):\"\"\"\n\tUpdate the set with the difference between itself"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn self.pop()\n\n\tdef close(self):\n\t\"\"\"\n\tFree up allocating memory.\n\t\"\"\"\n\tself.free_count = None\n\t\n\tsuper().close()\nfrom multiprocessing import Pipe, Process \nfrom multiprocessing.connection import Client\n\ndef producer(item, conn):\n\t\"\"\"\n\tGenerate data and send to `conn`\n\t\"\"\"\n\tfor value in item:\n\t\tconn.send(value)\n\t\t\n\tconn.close()\n\ndef consumer(conn):\n\t\"\"\"\n\tWait and receive"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# \u627e\u5230\u6700\u5c11\u7684\u64cd\u4f5c\u6570\u540e\uff0c\u5220\u6389\u8fd9\u90e8\u5206\u5df2\u7ecf\u4f7f\u7528\u7684\u6570\u636e\u5e93\u952e\u548c\u5bf9\u5e94\u7684\u5bf9\u8c61\n\twith self.lock:\n\t\tkeys = list(self.keys())\n\t\t(key, _) = keys[0]\n\t\tself.pop(key)\n\t\treturn (key, self.get(key))\n\n\nclass LFUCache:\n\n\n\tdef __init__"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.elemCount <= 16:\n\t\twhile True:\n\t\t\tminKeyIndex = self.minKeyValIndex();\n\t\t\tif minKeyIndex is None: break\n\t\t\tself.removeEntry(minKeyIndex);\n\n\t\t\tif self.elemCount <= 16: # to be implemented\n\t\t\t\tpass\n\n\n\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\t(key, value) = self.cache.popitem()\n\t\treturn (\n\t\t\tkey, value)\n\texcept Exception:\n\t\traise ValueError('cache is on 0 length.')\n\t\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\traise NotImplemented\n\tif len(self._store) == 0: raise KeyError('Cache is empty!')\n\treturn (next(iter(self._store.keys())))\n\n\"\"\"\n\tA second LRU cache, that keeps track of the number of cache hits.\n\tThis cache will store the values of `size` elements only.\n\t\"\"\"\nclass LRUCacheSize(Cache):\n\tdef __init__(self, size=128):\n\t\tsuper(LRUCacheSize, self).__init__()\n\t\tself._store = {}\n\t\tself._"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t(key, _,tts) = self.items.popitem(last=False)\n\treturn key\nprint(a)\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, _ = self.min_freq()\n\tval = self.storage.pop(key)\n\tself.size -= 1\n\tassert val is not None\n\treturn (key, val)\n\ndef popsmallest(storage):\n\t\"\"\"Remove and return the `(key, value)` pair having the smallest key.\"\"\"\n\tk, v = storage.min_freq()\n\tstorage.remove(k, v)\n\treturn (k, v)\n\ndef clear(storage):\n\tstorage.reset()\n\nfrom collections import deque\nclass"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# Return top pair only\n\t(key, value) = self._table[0]\n\tself._remove(key)\n\treturn (key, value)\n\n\n# End of SimpleLRU class\n\n\"\"\"\n\tMain loop to iterate over the items and maintain a cache of the LRU.\n\tWhenever a new item is inserted into the LRU, it must be handled as the top item.\n\t\"\"\"\nfor (key, value) in data:\n\t# if not found or if it has been cached for long enough\n\tif (cache_size == 0) or ("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# self.remove_least_frequent()\n\t# del self;\n\t# self = None;\n\t# self = self.defaultdict(None)\n\twhile True:\n\t\tx = (self.min_freq,self.min_freq_key)\n\t\tyield x\n\t\tself[x[1]] -= 1;\n\t\t#self.remove(x[1])\n\t\t#if not(self):\n\t\t#\treturn None\n\"\"\"\n\tInserts the given key (or none if we need to make room) and value.\n\t\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tdef popleft(self):\n\"\"\"\n\tRemove and return the `key` most frequently used.\n\t\"\"\"\n\tdef update(self,iterable):\n\"\"\"\n\tUpdate the dict into `iterable` items.\n\t\"\"\"\n\tdef values(self):\n\"\"\"\n\tReturn the value sequence of the dictionary.\n\t\"\"\"\n\t__hash__ = None\nclass deque:\n\tdef __init__(self, iterable=()):\n\"\"\"\n\tInitialize a new ordered collection with a copy of an\n\titerable.\n\t\n\tThe `maxlen"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.h.popitem()\n\ndef items(self):\n\"\"\"\n\tReturn a copy of a HashTable\u2019s items.\n\t\"\"\"\n\treturn self.h.items()\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tassert self._count!= 0\n\titem = self._map.popitem(last=False)\n\tself._count -= 1\n\treturn item\ndef items(self):\n\"\"\"\nReturn a copy of `(key, value)` pairs, preserving order.\n\n# >>> l1 = CounterList([3,1,2,1,5])\n# >>> l2 = l1.items() ### Doesn't modify generator ###\n# >>> list(l2)                 ### Reveals original generator!\n# [(3, 1), (1, 2), (2, 1), (1"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.__len__():\n\t\tself._list.pop( -1 )\n\t\tself._set.pop( -1 )\n\n\t\treturn (False, False)\n\traise KeyError (\"'lru_cache' is empty\")\n\ndef clear ():\n\"\"\"\tDelete all keys from the cache dict.\n\t\"\"\"\n\tself._list = []\n\tself._set = {}\n\"\"\"\n\tdef get (self, *args, **kwds):\n\"\"\"\t  def get (self, *args, **kwds):\n\"\"\"\t\ttry:\n\t\t\treturn self._"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself._assert_not_closed()\n\n\ttry:\n\t\tpopped = (self._dict.popitem(last=False))[1]\n\n\t\tif self._onetime:\n\t\t\tdel self._dict, self._onetime\n\t\t\tself._onetime = None\n\t\t\tdel self\n\t\treturn popped\n\texcept KeyError:\n\t\treturn\n\texcept Exception as e:\n\t\traise e\n\n\treturn\n\ndef iteritems(self):\n\"\"\"\tReturn an iterator over the items.\n\t\"\"\"\n\tself._assert_not_closed()\n\n\ttry:\n\t\treturn"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t(k, v) = self.h.popitem()\n\"\"\"\n\tThe `OrderedDict.popitem` function\n\tremoves and returns the last `(key, value)` pair inserted,\n\talso when no pairs are present.\n\tThe order of access is preserved, not the least recently used.\n\t\"\"\"\n\ndel popitem\n\n__all__ = ['HashedSet', 'OrderedDict', 'popitem']\n\n################################################################\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself = self.__popitem()\n\tself.cursor = self.cursor - 1\n\tif self.cursor < 0:\n\t\tself.cursor = self.maxKey\n\tif self.cursor == self.minKey:\n\t\tself.cursor = self.maxKey\n\treturn self\n\n\nclass LRUCache:\n\tdef __init__(self, maxsize=128, expiration=10):\n\t\t\"\"\"\n\tInitialize an instance.  maxsize is the maximum number of \n\telements to be cached.  If maxsize is not"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn (key(), value()) \n\nclass ExpiringDict(dict):\n\t\"\"\"\n\tA subclass that implements a dictionary that will delete\n\titems after a certain amount of time.\n\n\tItems can be set on init or after setting with the\n\tset_key(key, value, timeout) method. If timeout is not set,\n\tthe value is always left untouched.\n\n\tThe remaining timeout can be retrieved with the timed_out() method.  Keys\n\twhich were not touched before this call will remain untouched.\n\n\tKeys which have not been touched will automatically expire after they were"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\traise NotImplementedError(\"%r.popitem is unsupported\" % self.__class__)\n\nclass TimeMap(OrderedDict):\n\t\"\"\"\n\tTimeMap is an ordered dictionary that can't insert duplicate items.\n\n\tIt can be solved however by replacing `OrderedDict.setdefault` with\n\t`TimeMap.get_insertion_index()`. But it can be faster/cheap in some cases.\n\t\"\"\"\n\tdef _get_index_for_key(self, key, create_if_missing=True):\n\t\tindex = Ord"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n\tself.current_hits -= 1\n\treturn key_value\n\n\nif __name__ == '__main__':\n\td = LRU()\n\tfor x in range(0, 10000):\n\t\td.add(x, \"element\")\n\t\tif d.size() > 1:\n\t\t\tassert d.get('first')!= 'element'\n\t\t\tassert d.get('last')!= 'element'\n\t\tassert d['first']!= 'element'\n\t\tassert d['last']!= 'element'\n\td['first'] = None\n\td['"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.lru -= 1\n\tkey, value = self.h[self.lru]\n\tself.h.remove(self.h[self.lru])\n\n\tdel self.h[self.lru]\n\tself.d[key] = self.l\n\tself.l += 1\n\treturn (key, value)\n\nif __name__ == \"__main__\":\n\tLRUCache(3)\n\n\n"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tdef popleft(self, key):\n\t\"\"\"\n\tReturn and remove the `(key, value)` pair most recently used.\n\t\"\"\"\n    pass\ndef pop(self, key, *default):\n\"\"\"\n        If there are no `(key, value)` pairs in the cache, it should return\n        `default` if given, otherwise ``KeyError``. Otherwise, it should\n        remove the item matching `key` from the cache and return the\n        corresponding value.\n\"\"\"\n    def popitem(self, key, *default):\n        \"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self+self.keys,self+self.vals\n\n#=======================================================================\ndef lru_cache(size):\n\t\"\"\"\n\tLRU cache decorator, caching the function's return value the least\n#\tthe more time it has already been called\n\t\"\"\"\n\tdef decorator(klass):\n\t\t# We're caching using these attribute names defined in the cachefile\n\t\t# We can't just use the instance methods, as a subclass overrides them\n\t\t# We have to read the file and know the instance name which was defined\n\t\tklass.size = size\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\traise NotImplementedError\n\n\t\n\n\n\nclass LRUList():\n\t\n\t\t\ndef getitem_at(self, index):\n\t\t\"\"\"\nReturn the item at index, or a default value if no such item exists.\n\nFor a list, this implementation will iterate from start to end, looking for the\nindex-th item (where counting starts at 0). Once it has found the nth item, it will\nreturn this item and return (where zero-based) the number of items it has\ncounted, which will be one less than the requested index.\n\nThe"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\traise NotImplementedError\n\n### Cached Multi DB\n\nclass LRUCache:\n\tdef __init__(self, cnt):\n\t\tself.cnt = cnt\n\t\tself.mc = {}\n\n\tdef size(self):\n\t\treturn len(self.mc.keys())\n\n\tdef lookup(self, key):\n\t\t\"\"\"\n\tReturn `key` if present in the cache. If not, add `key` to the cache for FIFO access.\n\t\"\"\"\n\t\tif key in self.mc:\n\t\t\tself.mc[key] = ut"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.h.popitem(last=False)[0]"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tpass\ndef pushpopitem(self):\n\t\"\"\"\n\tAdd the key,value pair to the end of the ordered dictionary.\n\tRemove and return the (key, value) pair having the higher priority by\n\tthe removal.\n\t\"\"\"\n\tpass\ndef clear(self):\n\t\"\"\"\n\tClear the contents of the dictionary object.\n\t\"\"\"\n\tpass\ndef copy(self):\n\t\"\"\"\n\tReturn a shallow copy of the dictionary\u2019s items (key, value pairs).\n\t\"\"\"\n\tpass\ndef fromkeys(self, seq, value=None):"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self._dict.popitem()\nclass Queue(object):\n\t\"\"\"\n\tImplements an ordered collection which supports `pushleft`, `popleft`, `peekleft`, `iter`, `clear`,\n\tand `index`.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInitializes a new instance of the class.\n\t\t\"\"\"\n\t\tself.items = []\n\t\tself.dict = {}\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturns an expression which can be used to represent"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tdef clear(self):\n\"\"\"\n\tRemove all items from.\n\t\"\"\"\n\tdef close(self):\n\"\"\"\n\tClose the WSAEvictBucket.\n\t\"\"\"\n\tdef destroy(self):\n\"\"\"\n\tReturn the internal data structures associated to this WSAEvictBucket into\n\tthe freeable state.\n\t\"\"\"\n\tdef evict(self, item):\n\"\"\"\n\tEvict object `x` from the queue, calling it's finalizer.\n\t\"\"\"\n\tdef get(self, block=True, timeout=None):\n\"\"\"\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t(key, value) = self.lru[0]\n\tself.lru.remove(key)\n\tif self.len > 0:\n\t\tf = min(len(self.lru)-1, bisect_left(self.lru_time, self.lru_time[-1])) or 0\n\t\tself.lru.insert(0, self.lru.pop(f))\n\n\treturn key,value\n\n\t\"\"\"\n\tPeeks at the right item such that when popped, queue most\n\trecent will be least.\n\t\"\"\"\n"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self:\n\t\tkey, value = random.choice(list(self.items()))\n\t\tdel self[key]\n\t\treturn key, value\n\telse:\n\t\traise Exception('Dictionary is empty')\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn next(iter(self))\n\"\"\"\n\tRemove and return a random element.\n\t\"\"\"\n\tdef sample(self, k=1):\n\t\"\"\"\n\tReturn a list of k unique elements chosen from this container. Unlike the regular `list` built-in, the returned element is guaranteed to be unique. Will throw an error if `k` not in range [1, len(self)].\n\t\"\"\"\n\treturn random.sample(self, k)\n\"\"\"\n\tReturn True if this container contains a value. False otherwise.\n\n\"\"\"\n\tdef __contains__("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.s.pop().key.value, self.s.pop().value\n\n\t\nclass MapReduce :\n\tdef __init__(self, mapper, reducer):\n\t\t\"\"\"\n\t\tA map reduce task.\n\t\t\n\t\t- `mapper`: The mapper function\n\t\t\n\t\tThe mapper takes two argument:\n\t\t- input : The input data\n\t\t- `input`: A pair (key, value)\n\t\t- It should return:\n\t\t- key: The key of the pair\n\t\t- value: The value of the input\n\t\t-"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tkey, value = self.data.popitem()\n\treturn key, value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\n\t\n\t\n\t\n\t\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t(key, value) = self.random_element()\n\tself.move_to_end(key)\n\treturn (key, value)\n\"\"\"\n\tReturns a random element from the deque. Creates a different element each time and returns it in a tuple\n\t\"\"\"\n\tdef random_element(self):\n\tx = self.index + random.randint(1, self.capacity) if self.index!= None else None\n\treturn self.data[x]\n\nimport random\nclass TestRandomMock (unittest.TestCase):\n\tdef test"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.irff()\ndef pop(self, key=r):\n\t\"\"\"\n\tRemove the `key` from this dict and return the corresponding `value`.\n    If `key` is not in the dictionary, `key` will not be in the return value.\n\n\tRaises:\n\t\tKeyError -- if `key` is in the dictionary.\n\t\"\"\"\n\treturn self.irff()[0]\n\t\n\t\n    \n  \nclass LRU_Cache(object):\n    def __init__(self, capacity):\n        # Initialize class variables\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self.dict_size == 0:\n\t\treturn (None, None)\n\telse:\n\t\t(k, v) = self.random_pair()\n\t\t(self.dict_size, self.hash_func_used) = (self.dict_size - 1, self.hash_func_used[1])\n\t\treturn (k, v)\ndef is_empty(self):\n\"\"\"\nCheck if a hash table is empty.\n@param: void or None\n@returns: True if hash table empty, False otherwise.\n\"\"\"\n\tif self.dict"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn dict.popitem(self) if self else (None, None)\n\n\nimport unittest\n\nclass DictTest(unittest.TestCase):\n\n\tdef test_init(self):\n\t\t\"Test initializing a dict\"\n\t\td = dict(name=\"Bob\", age=41)\n\t\tself.assertEqual(d, {\"name\": \"Bob\", \"age\": 41})\n\n\tdef test_keys(self):\n\t\t\"\"\"\n\t\td.keys() -> return d's keys\n\t\t\"\"\"\n\t\td = dict(name='Bob', age=41"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.choice.popitem()\ndef move(self, word, index, other):\n\t\"\"\"\n\tmove(word, index, other) -> None\n\t\n\tMove the specified \"word\" to the specified index.\n\t\"\"\"\n\tself.dict[word][index] = self.dict[word].pop(index)\n\tself.dict[word].insert(index, other)\n\nclass FreqDist(dict):\n \n\t\"\"\"\n\tA frequency distribution is a mapping from **random**\n\tobjects to integer counts.\n\t\"\"\"\n\t\n\tdef"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn(self.popitem())\n\n\tdef clear(self):\n\"\"\"\n\tDelete all items from :class:`OrderedDict`.\n\t\"\"\"\n\tself.initems = True\n\tself.items.clear()\n\n\tdef copy(self):\n\"\"\"\n\tReturn a shallow copy of a :class:`OrderedDict`.\n\t\"\"\"\n\tc = OrderedDict(self)\n\tf = {}\n\tfor k, v in c.items():\n\t\tf[k] = v\n\treturn(f)\n\n\tdef keys(self):\n"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(f'[^\\{\\){\",;=}]'\n\t\tf'{\"\".join([char for char in self.in_parameter] if len(self.in_parameter) else \"\")}')\n\n\n\nif __name__ == \"__main__\":\n\tpass"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_pattern = {}\n\tin_pattern[\"key_regex\"] = re.compile(r\"\\{(.*})\\}\")\n\tin_pattern[\"case_regex\"] = re.compile(r\"\\{([A-Z])\\}\")\n\tin_pattern[\"match_id_pattern\"] = re.compile(r\"\\[[0-9]+\\]\")\n\tin_pattern[\"no_match_pattern\"] = re.compile(r\"\\[$\\]\")\n\tno_match_pattern_regex = re"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(\"\\$(IN)\\s+(.+?)\\s*\\(\\s*([^\\)]+)\\)\\s+\\(?([^\\)]+)\\)?\")\n\t\n\treturn in_regex\n\t\n\t\ndef _create_optional_regex(self) -> Pattern:\n\"\"\"\n\tCreate the optional-style parameter regular expression.\n\t\n\tReturns the optional-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\toptional_regex = re.compile(\"\\$"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.PARAM_PATTERN)\n\t\n\t\n\t\n###########################################################################\n\nclass TextPattern(BasePattern):\n\t\"\"\"\n\tA text value. A text pattern represents a value with a character separator,\n\te.g. \":\", (\"::\"), \":space\", or a mixture of both.\n\t\n\tA text value can represent a variable, or it\n\tcan represent a literal string, e.g. \"Hello\" or \":space\".\n\t\"\"\"\n\t\n\t\n\t\n\tdef _create_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tif self.params is None:\n\t\treturn None\n\t\t\n\tif self.in_:\n\t\tin_parts = self.in_.split(\", \")\n\t\tregex_string = \"\".join(re.escape(p) for p in in_parts)\n\t\tif not regex_string:\n\t\t\traise ValueError(\"in_ must not be empty.\")\n\n\t\treturn re.compile(f\"[{regex_string}]\")\n\n\ndef _create_regex_list(\n\t\t\t\t self                # type: _MatcherT\n\t\t\t\t )                 # type:"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tret = re.compile(r\"(?:\\^|\\\\)\" + self.PATTERN_SEPARATOR + r\"[A-Z0-9_\\.]{1,%d}\" +\n\tr\"+(?:\\\\(?:(?:[^\\\\\\\\][A-Z0-9_\\.]*)|(?:\\\\\\\\[A-Z0-9_\\.]*))[a-zA-Z0-9_\\.]+(?:\\\\.)?)\" +\n\tr\"(?:(?:\\\\((?:[^\\\\\\\\]\\\\\\\\)*\\\\){0,2}("}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\texpression = re.compile(r\"(\\s*({0.value})\\s*).*\".format(self), re.DOTALL)\n\treturn expression if expression else None\n\n\nclass ParameterDescriptorMixin:\n\t\"\"\"\n\tA base class for parameter descriptors.\n\t\n\tProvides the :func:`ParameterDescriptor.get` and :func:`ParameterDescriptor.exists` methods.\n\t\n\tUsage::\n\t\tclass User(View):\n\t\t\t@form_view_url_for(\"view_name\")\n\t\t\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(\"(.*<)?(&.*?=?)?(?P<inparam>{})(.*)\".format(self._string_to_re_pattern()), re.I)\n\"\"\"\ndef _create_in_param_matches_for_url_searches(self, url: str) -> List[Tuple[str, Optional[str], Optional[str], Optional[str]]]:\n\"\"\"\ndef _get_parameter(self, p: str, required_type: Type[T], default_value: Optional[T] ="}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self._in_regex\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tThe expression will return ``None`` (indicating no match) on\n\t:attr:`_create_regex` and :meth:`_in_regex` methods.\n\t\"\"\"\n\twith self._lock:\n\t\tregex = self._create_regex()\n\t\tif regex is not None:\n\t\t\tself._in_regex = regex\n\treturn regex\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tThe expression will"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile('^%s(?:%s|%s)*(?:%s|%s)*$' % (self._name,\n\t\t\t_get_param_list_pattern(self._args),\n\t\t\t'(?:%s)' % self._optional,\n\t\t\t'(?:{})+'.format(_get_regex_param_list()),\n\t\t\t'(?:{})+'.format(_get_regex_params())))\n\n\ndef _get_type(type_: Type, *, strict: bool = False) -> Type:\n\tif"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self)\n\n# Default converter.\n_default_converter = _create_converter()\n\n# Default converter factory.\n_default_converter_factory = None\n\ndef DefaultConverterFactory(module_name):\n\t\"\"\"\n\tReturn a new converter factory created from the given module name.\n\tThe converter factory uses import and getattr to resolve a subclass of\n\t:class:`._converting.Converter`.\n\t\n\t:return: The converter factory.\n\t:raises ValueError"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converter\n\t\n\t\n\t\n@deprecation.deprecated(\n\tdeprecated_in=\"1.4.5\",\n\tremoved_in=\"2.0.0\",\n\tcurrent_version=__version__,\n\tdetails=\"Use `style` argument only\",\n)\ndef create_converter(style: int,\n\t*args: Any,\n\t**kwargs: Any) -> _converting._Converter:\n\t\"\"\"\n\tDecorate and wrap the decorated function to make its return value be passed to a parameter"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converter\n\n\ndef _set_converter(\n\t\tself: typing.Any,\n\t\tconverter: _converting._Converter,\n) -> None:\n\t\"\"\"\n\tSet the parameter style converter.\n\t\n\tThis is an internal, non-public method.\n\t\n\t:param converter: the parameter style converter.\n\t:type converter: :class:`._converting._Converter`.\n\t\"\"\"\n\tself._set_component(c.SETUPLIST_PARAM_STYLE_CONVERTER,"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self)\n\ndef create_parameter(self: _parameter._Parameter, *expr: list) -> _parameter._Parameter:\n\"\"\"\nCreates the new parameter from the provided list of expressions.\n\"\"\"\n\t_parameter.check_type(expr)\n\tname = _text.f(expr, self.get_style().name, 'name','str')\n\tstyle = _text.f(expr, self.get_style(),'style', _converting.Styles.text)\n\tvalue = _text"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t# get the first parameter converter which has '_Converter' type\n\tdef _get_converter(type: type) -> None:\n\t\t\"\"\"\t\n\t\t\tGet the converter if a converter is known or create and register a new converter.\n\t\t\t\n\t\t\t:param type: a type of parameter converter\n\t\t\t:raises Exception if no :class:`._converting.Converter` is found.\n\t\t\t:returns a converter.\n\t\t\t\"\"\"\n\t\ttry:\n\t\t\treturn {'bool': lambda x: (x == 'True') if isinstance(x, ("}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn create_converter(self)\n\n\ndef _create_parameter(\n\tname                         : str,\n\tdefault                      : None,\n\tdescription                 : str | int | float,\n\tdoc_format                 : str,\n\tcategory                   : str,\n\tmin_value                   : number | None,\n\tmax_value                   : number | None,\n\tunit                      : str | None,\n\ttype_group                 : str,\n\ttype_group_min             : number | None,\n\ttype_group_max             : number | None,\n\tdisplay"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter()\n\n\ndef _create_style_converter(\n\tstyle: str = CURRENT_STYLE,\n\tconverter: _converting._Converter = _create_converter()\n\t) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\t:param style: Default value from which styles will be derived.\n\t:param converter: Default value from which style converter will be derived.\n\t\n\t:return: A style converter.\n\t:rtype: :"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(\n\t\ttype_to_converter=self._type_to_converter,\n\t\tdefault_type_converter=self._default_type_converter,\n\t\ttypes_to_converters_mapping=self._types_to_converters_mapping\n\t)"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(parent=self)\n\n#noinspection PyArgumentList\nclass Parameter(ABC):\n\"\"\"\n\tParameter interface base class.\n\t\n\tUse :meth:`__new__` to create a new instance.\n\t\n\t.. seealso::\n\t\n\t   :meth:`__new__`\n\t\n\t   :meth:`ParameterStyle.style`\n\t\n\t   :attr:`.ParameterStyle.has_multiple`\n\"\"\"\n\ndef _create_parameter(self: 'Parameter')"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\treturn ParametricParameterStyle(self)\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 22.16 GiB total capacity; 20.55 GiB already allocated; 19.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 22.16 GiB total capacity; 20.60 GiB already allocated; 19.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = getattr(dt, 'fold', 0)\n\tdt = dt.astimezone(self)\n\tdt = dt.replace(fold=fold)\n\treturn dt\n\nclass DateTimeTZ(DateTime):\n\t__slots__ = ()\n\t\n\t#: An :class:`IntervalSet`, allowing comparisons of datetimes\n\t#: with timespans against both absolute and relative datetimes. This\n\t#: sets `tz` to UTC if no tzinfo is defined.\n\t@classmethod\n\tdef interval(cl"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tfold_secs = self.get_current_timestamp() - \\\n\t\tself.get_first_occurance(dt)\n\t\n\tif fold_secs < 0:\n\t\t# If this is a DST-aware datetime to convert to summer\n\t\t# time (i.e.: the end-of-day DST-aware \"fold\" datetime), apply\n\t\t# the conversion from standard time to summer-time.\n\t\tconversion_from_dst_start = datetime(\n\t\t\tdt.year, dt.month, dt."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif hasattr(dt, 'fold'):\n\t\treturn dt\n\telse:\n\t\t# This technique has the downside that zero-duration times,\n\t\t# while valid according to the Python datetime specification,\n\t\t# have no mathematical meaning. For example, 00:00 on the\n\t\t# first day of the month is ambiguous in a naive fashion.\n\t\t_z = FixedOffset(0, \"UTC\", False, None)\n\t\t# We are crossing into the fold, which means that the\n\t\t# datetime is effectively deprecated\n\t\tx = FixedOffset(0, \"UTC\","}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t_fold_cache_logger.debug(\"Converting %s to UTC\", dt)\n\t\n\tif hasattr(dt, 'fold') and dt.fold in (0, 1):\n\t\treturn dt\n\t\n\t# We have the option of preserving the original timezone and just\n\t# computing the offset, or computing the offset and then some\n\t# adjustment to revert back the datetime. I'm going for the first\n\t# option since it seems to be the most widely understood and has\n\t# the most reasonable chance of being correct. It is only"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Get a datetime-independent representation of the given\n\t# time.\n\ttup = dt.utctimetuple()\n\tdttz = datetime.datetime(tup.tm_year, tup.tm_mon, tup.tm_mday,\n\t\t\t\t\t\t\t tup.tm_hour, tup.tm_min, tup.tm_sec,\n\t\t\t\t\t\t\t tup.tm_wday, tup.tm_yday, tup.tm_isdst)\n\ttup = dttz.utctimetuple()\n\t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# fold_second=11):\n\ttd = dt.dst() or _zero\n\tfold = dt == td\n\n\t# This implementation is lifted from the python 2.3 codebase\n\t# First, normalize vto to the fold\n\tvto = dt.replace(tzinfo=None)\n\ttti = vto.utctimetuple()\n\ttti = tti[:3] + (clamp(tti[-1], 0, 3 if fold else 12),) + tti[3:]\n\tvto = time.struct_time("}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt_wall = dt.astimezone(self)\n\ttry:\n\t\treturn self._fold(dt_wall)\n\texcept AmbiguousTimeError:\n\t\treturn self.localize(dt_wall - dt_wall.utcoffset())\n\n\t\nclass tzrange(tzinfo):\n\t\"\"\"\n\tA :class:`tzinfo` subclass for timezones specified using multiple timezone\n\tranges.\n\n\t:param stdabbr:\n\t\tAn ISO 3166-1 two-letter code for the timezone's standard\n\t\tabbreviation"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttzinfo = getattr(dt, 'tzinfo', None)\n\n\tif tzinfo is None:\n\t\traise TypeError(\"fromutc() requires a datetime with a tzinfo attached\")\n\n\tEPOCH_naive = datetime.datetime(1970, 1, 1)\n\tEPOCH = EPOCH_naive.replace(tzinfo=tzinfo)\n\n\tif dt.tzinfo is None:\n\t\tdt = EPOCH_naive.replace(tzinfo=tzinfo) + dt - EPOCH_naive.replace"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# determine new tzinfo\n\tif self._tzinfo is None:\n\t\traise TypeError(\"fromutc() is timezone-naive, use timezone.utc_localize()\")\n\t\n\t# get constructor arguments\n\tme._fold = 0\n\targs = dt.timetuple()[:6] + (0,) + tuple(dt.microsecond) + (dt.tzinfo,)\n\n\t# Convert negative timestamps to positive and add tzinfo\n\tif args[-1] == -1:\n\t\targs = args["}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tnew_tz = tz.tzutc()\n\t\n\tdst_start = new_tz.fromutc(dt)\n\tdst_stop = dst_start + HOUR\n\t\n\tdst_start_fold = dst_start.replace(fold=1, tzinfo=tz.tzlocal())\n\toffset_after_start = dst_start_fold - dst_start + HOUR\n\tdt_after_start = dst_start_fold.replace(tzinfo=new_tz)\n\n\tdst_stop_fold = dst"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n# vim:ts=4:sw=4:et\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\ttry:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\texcept AttributeError as ex:\n\t\traise ex\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\t\n\t\ndef resolve_imaginary(dt):\n\t\"\"\"\n\tGiven a datetime that may be aware, attempt to resolve it to an actual\n\tdatetime. This means that it may have been created from a timezone aware\n\tdate, in which case it will be forced to a timezone-naive date.\n\t\n\tExample::\n\t\n\t    >>> from dateutil.parser import parse\n\t    >>>"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef normalize(dt):\n\t\tif dt.tzinfo is not None:\n\t\t\tdt = dt.astimezone(UTC)\n\t\treturn dt.replace(tzinfo=None)\n\n\treturn normalize(dt) if dt.tzinfo is None else dt\n\n# vim:ts=4:sw=4:et\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# If dt is UTC or naive, there is nothing to do.\n\tif dt.tzinfo is not None:\n\t\tif dt.tzinfo is not tzinfo:\n\t\t\traise ValueError(\"dt.tzinfo is already set to {0}\".format(dt.tzinfo))\n\t\treturn dt\n\t\n\t# dt is naive and no time zone is specified.  Set it to the default time\n\t# zone according to the Django convention of \"UTC\" (-03:00).\n\tdtoff = tzinfo.utcoffset(dt"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\ndef resolve_imaginary(dt):\n\t\"\"\"\n\tGiven a naive datetime, ``resolve_imaginary`` returns an\n\tappropriate naive datetime representing the same moment in\n\ttime. If the given datetime is an aware datetime, it is returned\n\tunchanged. If it's a naive datetime, then its timezone is\n\tassumed to be ``pytz.utc``.\n\t\n\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n# vim:ts=4:sw=4:et\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n# vim:ts=4:sw=4:et\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\t# convert non-naive datetime to naive\n\t\treturn dt.astimezone(tzinfo)\n\telse:\n\t\t# C parser doesn't support time zone offset\n\t\t# assert not dt.tzinfo, \"Naive datetime is ambiguous\"\n\t\treturn dt.replace(tzinfo=tzinfo)\n\t\nclass UTCTimeZone(tzinfo):\n\t\"\"\"\n\tFixed offset in Coordinated Universal Time.\n\t\"\"\"\n\tdef utcoffset(self, dt):\n\t\treturn self"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# For backwards compatibility, we can't use 'from __future__ import\n\t# unicode_literals' here because of python 2.6\n\ttzinfo = _check_tzinfo_arg(tzinfo)\n\treturn dt.replace(tzinfo=None)\n\n# vim:ts=4:sw=4:et\n"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tpass\n\ndef set_default_extension(suffix: int) -> None:\n\"\"\"\n\tSet the default extension used to automatically file new files during save.\n\t\n\tArgs:\n\t    suffix: The extension used if no extension is set on a new file.\n\t\"\"\"\n\tpass\n\ndef set_file_format(shortcut: int, full_path: str, file_format: str) -> None:\n\"\"\"\n\tChange the format of the file saved to.\n\t\n\tfile_format needs to match the format of the exported data.\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.chars_before = before\n\tself.chars_after  = after\n\n################################################################################################################################################################################################\n\n\n\nclass SlicePoint(object):\n\t__slots__ = [\"bytes_offset\"]\n\tdef __init__(self, bs: Union[bytes, None]) -> None:\n\t\tself.bytes_offset = 0\n\t\tif bs is not None:\n\t\t\tself.bytes_offset = len(bs)\n\nclass FileData(object):\n\t\"\"\"\n\tThis class represents one file and all of its associated data. If one of"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\treturn _gdcmswig.Control_set_cut_chars(self, before, after)\nControl_swigregister = _gdcmswig.Control_swigregister\nControl_swigregister(Control)\n\nclass Application(Wrap):\n    \"\"\"\"\n\tAn application with its file name.\n\t\n\tAttributes:\n\t    * `name`  - application name;\n\t        type: std::string;\n\t    * `file`  - file name;\n\t        type: std::string;\n\t    * `private`  - is"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tpass\n\ndef split(self, path: str, file_offset: bytes, file_name: str) -> None:\n\"\"\"\n\tSplit the file.\n\t\n\tArgs:\n\t    path: Path to file to be split.\n\t    file_offset: The file offset of the first file.\n\t    file_name: Name of the first file.\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\treturn False\n\n@PublicAPI\ndef splitext(s: str, sep: str) -> Tuple[str, str]:\n\"\"\"\n\tSplit an extension out of the supplied full file name.\n\t\n\tArgs:\n\t    s: Full file name on which to split the extension.\n\t    sep: Separator character; can be used as a delimiter if the file name does not contain any spaces.\n\tReturn: A tuple containing the split file name and the file extension, in that order.\n\t\"\"\"\n\treturn (False, False)\n\n@Public"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tpass\n\"\"\"\nDump()\n\tDump this object to a JSON formatter\n\t\n\tReturns:\n\t    The formatted strings\n\t\"\"\"\n\tpass\ndef _make_json(obj, jsonfn, default_nest):\n\t\"\"\"\n\tInternal Method: Serialises obj into JSON and returns a JSON string\n\t\n\tArgs:\n\t    obj: The object to serialise\n\t    jsonfn: JSON function\n\t    default_nest: Default nest level for serialisation\n\t\n\tReturns:\n\t    The serialised JSON as a string\n\t\"\"\""}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\treturn \n\n\n\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tpass\n\n\"\"\"\n    Function: reset_cut_chars\n\tResets the cut letter delimiters if any.\n\"\"\"\ndef reset_cut_chars() -> None:\n\tpass\n\n\"\"\"\n    Function: get_cut_chars\n\tGets the cut letter delimiters if any.\n\t\n\tReturns:\n\t    [before, after]: A pair of bytes that are used to delimit slice points.\n\t    [slicing_on]: If true, this file is being sliced.\n\t\"\"\"\ndef get_cut_chars()"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.file_extension = file_extension(self.file_path)\n\ttry:\n\t\tbefore = bytes(before, 'utf-8')\n\t\tafter = bytes(after, 'utf-8')\n\t\tbefore_idx = self.file_path.index(before)\n\t\tafter_idx = self.file_path.index(after) + len(after)\n\t\tself.file_path = self.file_path[0:before_idx] + self.file_path[after_idx:]\n\t\tself.file_length ="}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tdef set_cut_lines(self, lines: int) -> None:\n\t\t\"\"\"\n\t\t    Set number of lines to split file into. If 0, don't split.\n\t\t    \"\"\"\n\"\"\"\n\tdef set_split(self, split: bool) -> None:\n\t\t\"\"\"\n\t\t    Set whether to split the file.\n\t\t    \"\"\"\n\t\tdef set_num_lines_per_chunk(self, num_lines: int) -> None:\n\t\t\"\"\"\n\t\t    Set number of lines per chunk of split file\n\t\t    \"\"\"\n\t\tdef get_num_"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\treturn'request', dict(raw_data=request[0]['raw_data'])\n\n@router.register(\"response\")\ndef handle_response(response: ResponseType):\n\"\"\"\n\tGet the user's message and reply if needed.\n\t\"\"\"\n\tif response.type == \"response\":\n\t\treply = None\n\t\tdata = response.conversation.data\n\t\tif data and \"type\" in data:\n\t\t\tdata_type = data[\"type\"]\n\t\t\tuser_msg = data.get(\"user_msg\", data.get(\"message\"))\n\t\t\tif"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\t# try public message\n\t# check if message is a JSON object from Diaspora/3.1 and not a simple string\n\ttry:\n\t\tjson.loads(request.body)\n\t\tmessage = request.body\n\t\tjson_data = json.loads(message, object_hook=dict_decoder)\n\t\tif \"id\" not in json_data or \"data\" not in json_data or \"type\" not in json_data or not json_data[\"data\"] or \"userid\" not in json_data[\"data\"] or json_data[\"type\"]"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif isinstance(request[\"payload\"], PublicMessage):\n\t\treturn True\n\telif isinstance(request[\"payload\"], PrivateMessage):\n\t\treturn True\n\telif isinstance(request[\"payload\"], DiasporaIncomingPayload):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n# ==============\n# Payload handlers\n# ==============\n\nasync def handle_payloads(app, payload):\n\t\"\"\"\n\tDispatch Diaspora payloads\n\t\"\"\"\n\tif payload.status_code!= 200:\n\t\tawait"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry: \n\t\trequest.client_id, request.client_platform, request.client_version,_ = parse_diaspora(request)\n\t\treturn \"0\"\n\texcept: \n\t\ttry: \n\t\t\trequest.client_id, request.client_platform, request.client_version,_ = parse_diaspora(request, private_only = True)\n\t\t\treturn \"1\"\n\t\texcept: \n\t\t\tpass\n\t\n\treturn 0\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# this can be in either a private message, so use the 'chat' channel for it\n\t# or the 'channel' (the normal chat) for it\n\tidentify_request = {\n\t\t\"from\": _identify_from(request.message.author),\n\t\t\"chat\": _identify_channels(request.message.message_id),\n\t}\n\treturn success_http_response(identify_request)\n\nasync def _identify_from(author: Optional[SlackUser] = None) -> dict:\n\t\"\"\" Check whether this is"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\treturn Message.objects.filter(sender=request.from_user.id, chat_id=request.chat.id).order_by('date').first()\n\texcept Message.DoesNotExist:\n\t\tpass\n\ttry:\n\t\treturn Message.objects.filter(to = request.from_user.id, chat_id=request.chat.id).order_by('date').first()\n\texcept Message.DoesNotExist:\n\t\tpass\n\ttry:\n\t\treturn Message.objects.filter(sender=request.from_user"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tglobal bot\n\tfrom datetime import datetime\n\tbot = None\n\trequest._id = None\n\treturn dict(status = \"OK\")\n\n@public_request\ndef process_message(request: RequestType, _: MessageType):\n\t\"\"\"\n\tProcess the request\n\t\"\"\"\n\tfrom typing import Dict, Any\n\tglobal bot, d, request\n\tif bot is None:\n\t\tcredentials = Credentials.load()\n\t\tbot = Bot(credentials.oauth_token, credentials.oauth_refresh_token, credentials."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tresend = False\n\ttry:\n\t\tres = request.get_body()[:20]\n\t\tif re.match('^:!\\[.+\\] \"?\\w:?', res):\n\t\t\traise PayloadNotStrictlyStub\n\t\telif re.match('^:!\\[.+\\] \"?/', res):\n\t\t\traise PayloadNotStrictlyStub\n\t\telif re.match('^:!\\[] \"?\\w:?', res):\n\t\t\traise PayloadNotStrictlyStub\n\t\tel"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\tlegacy = False\n\t\tif request.is_legacy:\n\t\t\tlegacy = True\t\t\n\t\t# Try to detect if this is a Diaspora request (not a Diaspora legacy payload).\n\t\tif legacy:\n\t\t\tif request.path_parts[1].startswith(\"d/messages/%u/\"):\n\t\t\t\trequest.legacy = True\n\t\telse:\n\t\t\tif request.path_parts[1].startswith(\"d/messages/%u/\"):\n\t\t\t\trequest.is_legacy = True\n\texcept KeyError"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t_ = request.locale.translate\n\tif request.is_legacy_payload:\n\t\tsender = User(id=int(request.legacy_payload['sender_id']), is_bot=False, locale=request.locale).validate()\n\t\tif sender is None:\n\t\t\ttry:\n\t\t\t\tsender = User(id=int(request.legacy_payload['sender']['id']), is_bot=False, locale=request.locale).validate()\n\t\t\texcept KeyError:\n\t\t\t\treturn False\n"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\theaders = request.headers \\\n\t\t\tif isinstance(request.headers, dict) \\\n\t\t\telse request.headers.items()\n\t\tfor name, value in headers:\n\t\t\tif value.startswith(get_mux_prefix()):\n\t\t\t\treturn True\n\texcept:\n\t\tpass\n\ttry:\n\t\tbody = getattr(request, request.method)\n\t\tif body:\n\t\t\tif type(body) is not list:\n\t\t\t\tbody = [body]\n\t\t\tfor element in body:\n\t\t\t\tif element.startswith(get_m"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\treturn request.json[\"request\"][\"command\"] == \"m.call.room\"\n\texcept Exception:\n\t\treturn False\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request == None:\n\t\traise Exception('You must provide a matrix request')\n\treturn'requests' in request\n\t\ndef matrix_post_payload(request: RequestType, eventID: int, roomID: str, roomState: str, responseType: str, roomAliases: Dict[str, str], content: str, contentType: str) -> Dict[Union[str, List[Dict[str, str]]], Dict[str, str]]:\n\t\"\"\"\n\t\"\"\"\n\tif request == None:\n\t\traise Exception(\"You must"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# we probably should add a config option to not use this\n\t# unless you specifically mention that in your config\n\n\tif \"client\" in request.mx_client_options: \n\t\treturn False\n\tident = request.mx_matrix_version.identifier\n\treturn request.id == ident\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request.type is RequestType.MSG and request.sender == matrix_user_id\n\n\ndef send_reply(user_id: str, room: str, reply: dict) -> None:\n\t\"\"\"\n\tLog a reply\n\t\"\"\"\n\tif not settings.is_log_enabled(settings.LOGGER_NAME):\n\t\tlog('Logging disabled')\n\t\treturn\n\n\tif not settings.is_debug_log_enabled(settings.LOGGER_NAME):\n\t\tlog('Debug logging disabled')\n\t\treturn\n\n\tfor message in reply:\n\t\tlog"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# https://matrix.org/docs/spec/r0.1.x/client-server-protocol.html#mixed-commands\n\tif request is not None:\n\t\tif request['content'] == {}:\n\t\t\treturn False\n\n\t\tcontent = request['content']\n\t\tif CONTENT_ID in content:\n\t\t\treturn True\n\t\tif content['cmd'] == 'list':\n\t\t\tif CONTENT_ROOMS in content:\n\t\t\t\tif isinstance(content['rooms'], list):\n\t\t\t\t\treturn True\n\t\t\t\treturn False\n\t\t\treturn False\n\t\tif"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn (\n\t\t(request.command, request.arguments)\n\t\t== (Command.IDENTIFY, {})\n\t)\n\n\ndef sync():\n\t\"\"\"\n\t\"\"\"\n\treturn (\n\t\tMessageType.SYNC,\n\t\t{\n\t\t\t\"session\": dict(identifier=\"\", token=\"\"),\n\t\t\t\"matrix_service\": {\n\t\t\t\t\"id\": \"fake-id\",\n\t\t\t\t\"name\": \"Matrix test\",\n\t\t\t\t\"verifier_key\": \"\",\n\t\t\t},\n\t\t\t\"headers\": [\"test\"],\n\t\t},\n\t)\n\n\ndef echo"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request['type'] == 'identify':\n\t\tidentity_info = request['identity']['user'].lower()\n\t\tif identity_info.startswith('h.'):\n\t\t\treturn True\n\t\telif identity_info.startswith('@'):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n####\n# Commands:\n####\n\n\n@command()\ndef list(_: Self) -> str:\n\t\"\"\"\n\tReturns a table of supported Matrix commands\n\n\t\t%list\n\t\"\"\"\n\treturn"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tlogger.debug(\"identify_request()\")\n\n\tif (request.method == \"PUT\" and\n\t\t\trequest.requestVersion == \"1.0\" and\n\t\t\trequest.path == \"/put/\"):\n\t\t# This is a Matrix PUT request, we would like to reply to the sender with their credentials\n\t\treturn False\n\treturn True\n\nmatrix_config = dict(\n    endpoint_url=\"http://localhost:8448\",\n    endpoint_token=\"\",\n    homeserver_url=None,\n    homeserver_verify=True,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# We can match a Matrix request by looking for the `event` header\n\tif 'x-matrix-requestid' in request.headers:\n\t\treturn True\n\treturn False\n\n\ndef identify_response(request: RequestType) -> bool:\n\t\"\"\"\n\tidentify_response\n\n\tTry to identify whether this is a Matrix response\n\t\"\"\"\n\n\tif 'x-matrix-responseid' in request.data:\n\t\treturn True\n\treturn False\n\n\ndef create_headers(matrix_sig_key: bytes) -> Dict[str, str]:"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(xsd_dateformat)\n\ndef cast_datetime(val, _type=datetime.datetime):\n\t\"\"\"\n\tCast a string datetime value into the specified datetime type.\n\n\tRaises an xml.xsd.xmlschema.exceptions.ValidationError if the value is\n\tnot valid. Raises ValueError if the type is not understood.\n\t\"\"\"\n\tif isinstance(val, _type): return val\n\ttry: return datetime.datetime.strptime(val, _type.strpt"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn time.strftime(\"%M:%S\", time.gmtime(dt))\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(DATETIME_FMT)\n\n\ndef parse_dt(dt):\n\t\"\"\"\n\tConvert a ISO formatted date to a Python datetime.\n\t\"\"\"\n\treturn datetime.datetime.strptime(dt, DATETIME_FMT)\n\n\ndef parse_datetimestr(dt):\n\t\"\"\"\n\tLike parse_dt, but with dt being in the format the tree's dt_fmt field\n\tis stored in.\n\t\"\"\"\n\treturn parse_dt(dt)\n\n\ndef"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime.datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\").strftime(\"%a %b %d %Y %I:%M %p\")\ndef is_dt_diff(olddt, newdt):\"\"\"\n\tReturn whether two datetimes are different\n\t\"\"\"\n\tdiff = delta(newdt, olddt)\n\treturn diff.total_seconds() > 200\ndef convert_dt(dt):\n\"\"\"\n\tConvert a string datetime to a datetime object.\n\t\"\"\"\n\t"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(DTFORMAT)\n\nclass DictObj(object):\n\t\"\"\"\n\t\tD* node/node-group specific data (aka 'D' node)\n\t\t\"\"\"\n\tdef __init__(self,**kw):\n\t\tself.__dict__.update(kw)\n\ndef parse_date(date,format):\n\t\"\"\"\n\t\tParse date (format string) into a datetime object.\n\t\tIn some cases like time, the timezone information is not available but\n\t\tit can still be inferred in some cases. In those cases"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdt = datetime.strptime(dt, DtFormatString)\n\treturn dt\n\n\"\"\"---------------------------------------------------------------------\"\"\"\n\ndef read_file(path):\n\"\"\"\n\tA function which accepts a path to a file as a parameter and returns\n\ta Python dictionary which contains data gleaned from the file.\n\t\"\"\"\n\t#The file should be named according to the template given in the D2L document\n\tdoc = {}\n\tdoc[\"datetime\"] = format_dt(get_datetime_from_file(path))\n\tdoc[\"timezone\"] = get_"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S%z')\n\nclass Node:\n\t# Abstract class to represent nodes inside a tree.\n\t#\n\t\n\tdef _get_text(self):\n\t\tif not hasattr(self, '_text'):\n\t\t\tself._text = 'No name'\n\t\treturn self._text\n\tdef _set_text(self, val):\n\t\tself._text = val\n\ttext = property(_get_text, _set_text)\n\t\n\tdef _"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn \"[%d/%s/%s %d:%d:%d]\" % (\n\t\tdt.year, months[dt.month], dt.strftime(\"%d\"),\n\t\tdt.hour, dt.minute, dt.second)\n\n# @deprecated\ndef format_numbers(nums):\n\treturn \" \".join([fmt_num(num) for num in list(nums)])\n\n# @deprecated\ndef format_numbers2(nums):\n\treturn \" \".join(fmt_num2(num"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(\"%Y/%m/%d\")\n\ndef format_d(d):\n\t\"\"\"\n\tFormat a Python datetime to the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.datetime.strptime(\n\t\t\td.strftime(\"%Y/%m/%d\"),\"%Y/%m/%d\").strftime(\"%Y%m%d\")\n'''\n\nimport re\n\ndatepat = re.compile(r'''^(\n\t[^\\s\\()\\w"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(\"%Y/%m/%d %H:%M\")\n\ndef calc_tot(res, f_per_min, d_per_min):\n\treturn float(format((f_per_min*RESOLUTION_PER_MIN + d_per_min), '.2f'))/f_per_min\n\ndef find_res_idx(time, res=RESOLUTION_MIN, res_start=RESOLUTION_MIN_START):\n\t\"\"\"\n\tDetermine the index of the datetime"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# First, find tags that are not inside code blocks.\n\t_, text = find_tags(\n\t\ttext, replacer=replacer, inside_block=lambda pos: False\n\t)\n\tif not text:\n\t\treturn set(), text\n\t\n\t# Next, find text inside code blocks.\n\twords = replace(text, BLOCK, \"```\\n\").split(\"```\")\n\ttags, _ = find_tags(words[0], replacer)\n\t# If we're replacing, apply the replacer to all tags\n\ttexts_ ="}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t# Find all text in text marked with <>\n\t# Replace them with the tag word + \"</>\" + newtext + \"<\" + tagword + \">\"\n\t# This won't change the original text\n\t# Replacer is either None or a function that takes a word and returns the new word (string)\n\t\n\tif replacer == None:\n\t\treturn set(), text\n\t\n\t# Split up tagged text\n\tsplit_text = text.split(\"<\")\n\ttags = set(split_text[0::2])\n\toriginal_text = \""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tresults = set([])\n\t\n\tpos = 0\n\t\n\twhile (pos < len(text)):\n\t\t# try finding an entire start tag\n\t\tendpos = text.index(\">\", pos)\n\t\t\n\t\tif (endpos == -1):\n\t\t\tbreak\n\t\t\t\n\t\ttag = text[pos : endpos + 1]\n\t\t\n\t\tif (len(tag) > 0):\n\t\t\tresults.add(tag)\n\t\t\tif (replacer is not None):\n\t\t\t\ttext = text.replace(tag, str(replacer(tag)))\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = re.findall('({})'.format(TEXT_PREFIX), text)\n\tif replacer is not None:\n\t\tfor i, tag in enumerate(tags):\n\t\t\tparts = re.split('\\s\\s+', tag, maxsplit=1)\n\t\t\toriginal_tag = original = parts[0]\n\t\t\ttags[i] = replacer(original_tag)\n\t\ttext = re.sub('({})'.format(TEXT_PREFIX), r'\\1', text)\n\telse:\n\t\toriginal = text\n\t\ttags = set"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tpass\n\ndef find_words(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind words in text.\n\t\n\tOptionally, if passed a \"replacer function\",\n\twill replace matches in words matched by each\n\tone of the patterns in patterns with the\n\tresult of calling the replacer function.\n\t\n\tReturns a set of words and the original or replaced text.\n\t\"\"\"\n\tpass\n\n\ndef find_digits(text: str, replacer: callable = None) -> Tuple"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif text.startswith('{') and text.endswith('}'):  # ignore if no tag\n\t\ttext = text.strip('{\"}')\n\t\treturn {}, text\n\tstart, end = '{', '}'\n\ttags = 0\n\tresult = []\n\tif replacer:\n\t\ttext = text.replace('{', '(replacer(start))')\n\t\ttext = text.replace('}', '(replacer(end))')\n\telse:\n\t\ttext = text.replace('{', '(' + start)\n\t\ttext = text"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\t# Get tags\n\tpattern = re.compile(\"(?<!\\[)(\\[.*?\\])(?!\\])\")\n\ttags: Set[str] = set(pattern.findall(text))\n\t\n\t# Get text inside tags\n\tclean_text = \"\"\n\t\n\tif replacer:\n\t\t# Use replacer to insert the results wherever we had tags\n\t\tfor tag in tags:\n\t\t\tclean_text = clean_text + replacer(tag) + \"\\n\"\n\telse:\n\t\t# Keep clean_text intact\n\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\ttext = strip_html_tags(text)\n\t\n\t# Ignore inlined code blocks\n\ttext = strip_inlined_code(text)\n\t\n\ttags = set()\n\t\n\ti = 0\n\twhile i < len(text):\n\t\tif text[i].isalpha() or text[i].isdigit():\n\t\t\tif text[i] == '@':\n\t\t\t\ttags.add(text[i+1])\n\t\t\t\ti += 1\n\t\t\ti += 1\n\t\telse:\n\t\t\t# Start a new tag\n\t\t\twords = text["}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treturn set(), text\n\nfind_tags = deprecated_function(find_tags,\n\"\"\"\nThis module is deprecated. Please use\n>>> from pattern.web import find_tags\ninstead.\"\"\")\n\nTEST = \"\"\"\n1\n2\n3\nfoo4bar\n\nclass A:\n\tdef __init__(self):\n\t\tself.a = 1\n\t\tself.b = 2\n\t\tself.c = 3\n\t\t\n>>> class A:\n...     def __init__(self):\n...         self.a = 1\n...         self.b = 2\n..."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tif replacer:\n\t\t\n\t\tdef replace(old, new):\n\t\t\treturn re.sub(\n\t\t\t\tpattern = \"\\{[\\\"'][ ]*[\" + re.escape(\"\\(\").join(TAG_PATTERNS) + \"][ ]*\\}\", \n\t\t\t\trepl = new, \n\t\t\t\tstring = text, \n\t\t\t\tflags = re.VERBOSE)\n\t\n\t\ttext = replacer(old = text)\n\t\n\tlines = re.split(pattern = \"[]|\\\\n\", string = text)\n\t\n\ttext_"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = BeautifulSoup(text, HTML_PARSER).get_text()\n\treturn text\n\ndef process_markup_tweet(tweet, tweet_id, tweet_date):\n\"\"\"\n\tProcess the tweet by processing its content, and linkifying its text,\n\tas well as replacing URLs with <a> tags.\n\t\n\tThis function only processes text, not entities.\n\t\"\"\"\n\tparsed_entities = tweet[\"parsed_entities\"]\n\tuser_mentions = tweet[\"entities\"][\"user_mentions\"]\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlink_re = '''(?i)^(http[s]?://|[a-z0-9.]+\\.[a-z]{2,}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\ufffd"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Process links in text\n\tfor link in re.findall(r\"<\\w+[ ]?href='(.+?)'[ ]?>\", text+' '):\n\t\tyield requests.get(link).text\n\t\tyield html.escape(link)\n\ndef run(url, num_results=10, verbose=True):\n\tpage = requests.get(url)\n\tsoup = BeautifulSoup(page.text, \"html.parser\")\n\t\n\tdivs = soup.find_all(\"div\", attrs={\"class"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tglobal link_counter\n\tlink_counter += 1\n\ttext = re.sub(\"([\\w\\/\\-#])*\\@[\\w\\/\\-#]*\\.[\\w\\/\\-#]*\", \"<a name=\\\"\\\\g<1></a>\", text)\n\ttext = re.sub(\"http[s]?:\\/\\/{2}\\w.+\", \"<a href=\\\"\\\\g<1>\\\" rel=\\\"nofollow\\\">\\\\g<1></a></a>\", text)\n\ttext = re.sub(\"http[s]?:\\/\\/"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Make a BS4 element to be the parent (our'super' element)\n\tsoup = BeautifulSoup(text, \"html.parser\")\n\t# Find any anchor elements within the parent and get their links\n\tfor href in soup.find_all(\"a\"):\n\t\ttext = text.replace(href.attrs['href'], \"%s\\t%s\" % (href.attrs['href'], href))\n\t# Find any self href links within the parent and add attributes for the linkified text\n\tfor href in soup.select(\"a[href]"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\turl = re.compile(r\"(.*?)\\|\\|.*\")\n\treturn url.sub(\"<a href='\\2'>\\1</a>\", text)\n\n\ndef process_html_links(html):\n\t\"\"\"\n\tProcess html links, adding some attributes and linkifying html links.\n\t\"\"\"\n\tfrom w3lib.url import safe_url_string, canonicalize_url\n\thtml_url_re = (\n\t\t# Google Document URL\n\t\tre.compile(\n\t\t\tr\"(https?://[-\\.0-9a-z"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(\n\t\t\t# http\n\t\t\t\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n\t\t\t'<a href=\"__url__\">__url__</a>', \n\t\t\ttext,\n\t\t\t0)\n\ttext = re.sub(\n\t\t\t# https\n\t\t\t\"http[s]?"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r\"([a-zA-Z][^'\\\"\\#]+|['\\\"].+?')\", lambda x:x.group(1).lower(), text)\n\ttext = re.sub(r'<a.+?>', lambda x:x.group(0) + \" target=\\\"_blank\\\" href=\\\"\", text)\n\ttext = text.replace(\"<img\", \"<img src=\\\"\" + os.path.join(MEDIA_ROOT, \"thumbs/\") + \"\\\"\")\n\n\treturn text\n\ndef make_url"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\treturn linkify(text)"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks_processed = soup.new_string(text)\n\tif links_processed is None:\n\t\tprint(\"\\nError.\")\n\t\treturn\n\n\t# remove html\n\tlinks_processed = linkify(links_processed)\n\tdoc = links_processed.doc\n\n\t# process HTML entities\n\tresolved = True\n\tfor e in doc:\n\t\tif isinstance(e, Entity):\n\t\t\tif e.resolved is False:\n\t\t\t\tresolved = False\n\t\t\t\te[\"original_text\"] = e.text\n\t\t\t\te[\"text\"] ="}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif not url:\n\t\treturn None\n\n\tc_url = requests.head(url, allow_redirects=True, verify=False)\n\n\tif \"Content-Type\" in c_url.headers:\n\t\treturn c_url.headers[\"Content-Type\"]\n\telse:\n\t\treturn None\n\n\ndef fetch_link(url: str) -> Optional[str]:\n\t\"\"\"\n\tIf the remote URL is HTML,\n\tparse it for an HTML link.\n\tElse, return None.\n\t\"\"\"\n\tif not url:\n\t\treturn None\n\n\thtml"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t_debug(\"fetch_content_type\", url)\n\n\theaders = {}\n\tsession = requests.Session()\n\tmethod = \"HEAD\"\n\tdata = {}\n\n\tresponse = session.request(method, url, headers=headers, data=data)\n\treturn (response.headers[\"content-type\"]\n\t\t\tif response.status_code == HTTPStatus.OK\n\t\t\telse None)\n\n\ndef content_type_exists(url: str, content_type: str) -> bool:\n\t\"\"\"\n\tReturn ``True`` if the given content type corresponds to the response"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\thttp_req = http.request(\"HEAD\", url)\n\treturn http_req.info()[\"Content-Type\"] if http_req.status < 400 else None\n\n\nasync def fetch_url(url: str, timeout: Optional[float] = None) -> str:\n    \"\"\" Fetch the url with the default headers. \"\"\"\n    url_resp = (await http.request(method=\"GET\", url=url,\n                                   headers={}, timeout=timeout))\n    return url_resp.data\n\n\ndef fetch_mime_suffix(url: str"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tresponse = requests.head(url, allow_redirects=False)\n\tif response.status_code == 200:\n\t\tcontent_type: Optional[str] = response.headers.get('content_type', None)\n\t\tif content_type is None:\n\t\t\tcontent_type = response.headers.get('Content-Type', None)\n\telse:\n\t\tcontent_type = 'application/octet-stream'\n\treturn content_type\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tr = requests.head(url)\n\t\tif r.status_code == 200:\n\t\t\tif r.headers['Content-Type'] in supported_types:\n\t\t\t\treturn  r.headers['Content-Type']\n\texcept requests.exceptions.HTTPError as e:\n\t\traise Exception(f\"GET HEAD failed: {url}, error - {e}\")\n\telse:\n\t\traise Exception(f\"Invalid url format, unable to fetch {url}\")\n\n\ndef fetch_text(url: str) -> Optional[str]:\n\t\"\"\"\n\tF"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\n\theaders = {\"Accept\": \"*/*\"}\n\ttry:\n\t\tresp = get(url, headers=headers)\n\t\theaders = resp.headers\n\texcept:\n\t\treturn None\n\treturn headers.get(\"content-type\", None)\n\ndef fetch_file_size(url: str) -> Optional[int]:\n\"\"\"\n\tFetch the size of a remote file using GET\n\"\"\"\n\t\n\ttry:\n\t\tresponse = requests.head(url)\n\t\tif response.status_code == 200:\n\t\t\treturn int(response.headers.get(\"content"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn urlgrab(url, f'--header=Content-Type/{url[0].scheme}')\n\n\ndef download(url: str, path: str, check: bool = False) -> bool:\n\t\"\"\"\n\tDownload url into path.\n\t\"\"\"\n\t# Make directory if it does not exist\n\tdirectory = os.path.dirname(path)\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory, exist_ok=True)\n\ttry:\n\t\ttarget = urlgrab(url)\n\texcept HTTP"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tpath = '/'.join(url.split('/')[3:])\n\tr = requests.head('https://' + path, verify = False)\n\theaders = r.headers\n\tif r.status_code == 200:\n\t\tif headers.__eq__([]): return None\n\t\treturn headers['Content-Type']\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tresult = requests.head(url, allow_redirects=True)\n\t\treturn result.headers.get('Content-Type', None)\n\texcept OSError as exc:\n\t\tprint(f\"There was an OSError while fetching the content type from {url}\")\n\t\tprint(exc)\n\t\tif 'HTTPError' in str(type(exc)):\n\t\t\tprint(\n\t\t\t\tf\"It looks like the error, {exc}, is from the package.json. \"\n\t\t\t\t\"We don't really know what to do about this"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ts = requests.head(url, allow_redirects=True)\n\tif s.status_code == 404:\n\t\t#raise OSError(f\"URL {s.url} does not exist/can't be reach\")\n\t\treturn \"none\"\n\t# else\n\tcontent_type, _ = mimetypes.guess_type(s.url)\n\treturn content_type\n\ndef build_file_hash(target: Path) -> str:\n\t\"\"\"\n\tGenerate the SHA256 of the file.\n\t\"\"\"\n\tif target is"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == 'eof':\n\t\treturn tag\n\telif tag == 'u':\n\t\treturn tag\n\telif tag.isupper():\n\t\treturn tag\n\telse:\n\t\traise IllegalTag(tag)\n\n# Test cases\n\ndef test_accept_tag():\n\tinput = 'eof'\n\tactual = tag(input)\n\texpected = 'eof'\n\tassert actual == expected\n\ndef test_ignore_tag():\n\tinput = 'u'\n\tactual = tag(input)\n\texpected = 'u'\n\tassert actual"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == \" \":\n\t\tmsg.warning(\"illegal tag, skipping!\")\n\t\treturn False\n\ttry:\n\t\ttag = tag.rjust(max(tag, 1), \"_\")\n\t\t_test_tag(tag[:-1], 2, 3, 4, 5)\n\texcept Exception:\n\t\tmsg.warning(\"word \\\"{}\\\" cannot be tagged as \\\"{}\\\", skipping!\".format(word, tag))\n\t\treturn False\n\telse:\n\t\treturn True\n\n\ndef _test(filepath: str, _type: str = \"raw\") -> Union"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag in TASK2_TAG_SET:\n\t\treturn True, \"\"\n\telse:\n\t\treturn False, \"Invalid Tag: %s\" % tag\n\ndef tag_to_idx(tag: str) -> int:\n\treturn TASK2_IDX[tag]\n\ndef idx_to_tag(idx: int) -> str:\n\treturn TASK2_TAG_LIST[idx]\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tdef test_validation(tag: str):\n\t\ttry:\n\t\t\tvalidation_tag(tag)\n\t\texcept ValueError:\n\t\t\tpass\n\t\telse:\n\t\t\traise ValueError\n\tfor _tag in constants.ALL_TAGS:\n\t\ttest_validation(_tag)\n\ndef test_syllable(syllable: Syllable) -> bool:\n\t\"\"\" \n\tTest a word whether it could be a syllable.\n\t\"\"\"\n\ttest_word(syllable.word)\n\treturn syllable.is_valid\n\ndef test"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif not isinstance(tag,str):\n\t\treturn False\n\tif tag is None:\n\t \treturn False\n\tif len(tag)==0:\n\t \treturn False\n\tif not re.match(r'^[\\#a-zA-Z ]+$',tag):\n\t\treturn False\n\tif len(tag)<=0:\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n@helper\ndef test_word(word: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as an English"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag in tag_to_idx\n\n\ndef _to_word_tag(text_label: str) -> Tuple[str, str]:\n\t\"\"\"\n\tConvert a character text label to (word, tag)\n\t\"\"\"\n\tch_word, ch_tag = text_label.split()\n\treturn ch_word, ch_tag\n\n\ndef prepare_sequence_tensor(\n\t\tsequence: List[Tuple[str, str]],\n\t\tword_to_idx: Dict[str, int],\n\t\tdevice: torch.device,\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tassert re.match(r'^\\[.*\\]$', tag)\n\treturn True\n\n\ndef test_tag_is_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag or a tag with a\n\t\tspace before or after it.\n\t\"\"\"\n\tassert re.match(r'^\\[.* \\]$', tag)\n\treturn True\n\n\ndef test_word(word: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be used as a word. If a word is"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tres = False\n\n\tif tag in [\"JJ\", \"JJR\", \"JJS\"]:\n\t\tres = True\n\n\treturn res\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag in ['<start>', '<end>', '<pad>']:\n\t\treturn False\n\treturn True\n\n\ndef test_segment_word(segment_word:'segmented_word'):\n\t\"\"\"\n\t\tTest whether a segmented word could be used for the training purpose\n\t\t\"\"\"\n\ttag_type = 'B-seg' if segment_word.is_seg else 'I-seg'\n\ttag = ['O', 'B-seg', 'I-seg', 'B-sp', 'I-sp',"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn tag in word_tags\n\n\ndef test_word(word: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a word.\n\t\"\"\"\n\treturn word in words\n\n\ndef get_tag(word_tag: str) -> str:\n\"\"\"\n\tGet the tag of word passed in.\n\t\"\"\"\n\tpass\n\n\ndef change_word_count(word_count: int) -> None:\n\"\"\"\n\tChange the value of word count.\n\t\"\"\"\n\tpass\n\n\ndef change_tag_count(tag_count: int"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\td = node.__dict__.copy()\n\tfor k,v in d.items():\n\t\tif v is None:\n\t\t\tdel d[k]\n\t\telif k == 'tag':\n\t\t\tdel d[k]\n\t\telif k == 'xml':\n\t\t\td[k] = {'xml': d['xml'].__dict__.copy()}\n\t\t\tfor k1,v1 in d[k].items():\n\t\t\t\td[k][k1] = {'attrib':{},'text': '<unknown>'}\n\t\t\t\tif type"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif node.tag == 'xml':\n\t\t# print node.text\n\t\treturn xml_nodes_as_dict(node)\n\telif len(node) == 0:\n\t\tnode_dict = {}\n\t\tfor child in node:\n\t\t\tnode_attr = node.attrib.copy()\n\t\t\tnode_key = node_attr['id'].split(':')\n\t\t\tdel node_attr['id']\n\t\t\tnode_key = ':'.join(node_key)\n\t\t\tnode_dict[node_key] = xml_nodes_as_dict("}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren_as_dict = {}\n\tfor child_node in xml_children_as_list(node):\n\t\tchildren_as_dict[child_node.tag] = xml_attribute_value_for_child_node(child_node)\n\treturn children_as_dict\n\ndef xml_get_attribute_value(node, attribute_name):\n\t\"\"\"\n\tGet the text content\n\tof the named attribute.\n\t\"\"\"\n\tif hasattr(node, \"get\"):\n\t\tattribute_value = node.get(attribute_name)\n\telse"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif len(node) > 0:\n\t\td = {}\n\t\tchildren = getattr(node, node._name)\n\t\tfor child in children:\n\t\t\td[child.tag] = child\n\t\treturn d\n\telse:\n\t\treturn {node.tag: node}\n\n\ndef dict_as_xml(root, xml_format=\"utf-8\"):\n\t\"\"\"\n\tCreate an XML element from the given root dict, recursively.\n\t\n\txml_format may be set to 'iso-8859-1' to generate an ISO-8859-1"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn {tag.tag: node.getchildren() for tag, node in Etree.iterparse_depth_limited(node)}\n\n\ndef get_attributes(tag_or_node, attrib_string):\n\t\"\"\" Returns element attributes as a dict. \"\"\"\n\tret = {}\n\tfor k, v in six.iteritems(tag_or_node.attrib.items()):\n\t\tpatt = attrib_string.replace(\".\", \"/\").replace(\"[\", \"\").replace(\"]\", \"\")\n\t\tx = k + \"::\" + patt\n\t\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\td = {}\n\tfor n in xml_children(node):\n\t\tif n.tag in d:\n\t\t\td[n.tag].append(n)\n\t\telse:\n\t\t\td[n.tag] = [n]\n\treturn d\n\ndef xml_children_as_text(node):\n\"\"\"\n\tTurn the children of node <xml> into a text (merged as-is).\n\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn \"\".join(map(str, xml_children(node)))"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\telement_dict = {}\t\t# start empty\n\telement_dict[node.tag] = {}\t# start with this element\n\tsub_elements = node.getchildren()\t# get all child subelements\n\t\n\t# Process all subelements\n\t\n\tif sub_elements:\n\t\tfor sub_element in sub_elements:\n\t\t\t\n\t\t\telement_dict[node.tag][sub_element.tag] = sub_element.text\n\t\t\n\t\t# Now process the remaining subelements too,\n\t\t# to avoid having to find them"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\t\n\t# Initialize the results dict\n\tresult_dict = {}\n\t\n\t# Process all children nodes\n\tfor child_node in node.iterchildren():\n\t\t\n\t\t# Add the node's tag as an entry to the results dict\n\t\tresult_dict[child_node.tag] = child_node.text\n\t\t\n\t\t# Process child nodes recursively and add to results dict\n\t\tresult_dict.update(xml_children_as_dict(child_node))\n\t\t\n\treturn result_dict\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = dict()\n\tfor child in node.iterchildren():\n\t\tchildren[child.tag] = child\n\treturn children\n\ndef check_tags(children, tags):\n\t\"\"\"\n\tLook for the presence of the tags in a dict of children\n\t\"\"\"\n\tfor tag in tags:\n\t\tif children.get(tag) is None:\n\t\t\treturn False\n\treturn True\n\n\ndef parse_xml(xml):\n\t\"\"\"\n\tConvert xml from a single page into a data structure.\n\t\"\"\"\n\troot = ET.fromstring(xml)\n\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.findall(\"XML\"):\n\t\tchildren[child.tag] = child.attrib\n\treturn children\n\ndef xml_children_as_list_or_dict_recursively(node):\n\"\"\"\n\tTurn the children of node <xml>, including nested nodes, into a list of dicts or dicts\n\twith the same structure as an xml_children_as_dict() that returns a dict, for use with\n\ta dictionary comprehension.\n\n\t\"\"\"\n\tchildren = []\n\tfor child in node.findall(\""}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle and sender_handle!= entity_handle:\n\t\traise ValueError(\"Sender ({}, {}) in request doesn't match entity ({}, {}). Cannot continue.\".format(sender_handle, sender_handle_details, entity_handle, entity_handle))\n\t\t\ndef check_version_is_supported(ver):\n\"\"\"\n\tSince we are adding stuff now, we don't need to ensure that the sender is a Diaspora user.\n\t\n\tSince this is going to be the most common case to happen, we need not require that version.\n\tIn"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\tif sender_handle == None or sender_handle == \"\" :\n\t\tsender_handle = \"MESSAGER:\"+entity_handle\n\t\t\n\tassert sender_handle, \"Sender '{}' from payload '{}' doesn't exist\".format(entity_handle, data)\n\tassert sender_handle[:6] == 'MESSAGER:', \"Sender '{}' from payload '{}' not from an author\".format(entity_handle, data)\n\treturn sender_handle\n\n\n@app.route(\"/api/get-"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tassert sender_handle == get_object_owner(entity_handle)\n\treturn sender_handle == entity_handle\n\t\ndef process_group_message(msg_obj):\n\tprocess_message(msg_obj, \"group\")\n\ndef process_message(msg_obj, type):\n\t\"\"\"\n\t\tFunction to process incoming messages from Diaspora and call the appropriate callback functions\n\t\n\t\t\t\t\"msg_obj\"\t    \t--\t    \t\tDict to be processed\n\t\n\t\t\t\t\"type\"\t\t    \t--\t    \t\t\"group\", \"direct\", \"chat"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle is None or entity_handle is None:\n\t\treturn\n\treturn sender_handle == entity_handle or bool(Entity.all_keys.ALLOCATE_ENTITY.match(sender_handle) and Entity.all_keys.ALLOCATE_ENTITY.match(entity_handle))\n\n\n# TODO: Refactor\nasync def _validate_not_reacting_to_someone_self(user, event):\n\tif (Event.has_member_event and user.id == event.sender.id) or user.state."}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle == sender_handle:\n\t\tif get_entity_object(entity_handle, sender_handle) == None:\n\t\t\treturn True\t\n\t\n\treturn sender_handle == get_entity_object(entity_handle, sender_handle).author_email\n\ndef parse_from_timestamp(timestamp):\n\t\"\"\"\n\tThis function will return a datetime object. We're checking the timestamp format by testing whether\n\tthe date/time portion of the timestamp is valid.\n\t\"\"\"\n\tfmt = re.compile(fr'''^\\d"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle:\n\t\treturn entity_handle == sender_handle\n\telse:\n\t\treturn entity_handle == ObjectType(\n\t\t\tObjectTypeId.SELF,\n\t\t\tObjectHandle(\n\t\t\t\tObjectType.SELF,\n\t\t\t\tHandleType(HandleType.PRIMARY)\n\t\t\t),\n\t\t\tObjectHandle(\n\t\t\t\tObjectType.SELF,\n\t\t\t\tHandleType(HandleType.PRIMARY)\n\t\t\t),\n\t\t)\n\n\ndef find_outbound_links(client, recipient_handle, sender_handle, entity_handle, message_id, message_body"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\treturn sender_handle == entity_handle\n\ndef process_content(event, message):\n\t\"\"\"\n\t\tThis is a simple function to process the content (in this case, the event) before we continue.\n\t\t\n\t\tI have a couple of scenarios using this function:\n\t\t\t- If the message is a reply to an event (or some other event we don't parse), we don't get an 'entity_handle'\n\t\t\tto get the message object to process -- the function doesn't care about where it comes from.\n\t\t\t\n\t\t\t- If the message does come from the same entity_handle"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\ttry:\n\t\tentity_author = get_document(handle, owner_handle, entity_handle, False, False)\n\t\tsender_author = get_document(handle, entity_author, owner_handle, False, False)\n\n\t\tassert sender_author[\"handle\"] == sender_handle\n\t\tassert entity_author[\"handle\"] == entity_handle\n\texcept exceptions.NotFoundInOwners as err:\n\t\tprint(f\"ERROR: cannot find entity author; sender not authorized?\")\n\t\t# If there is no entity author in the object, then the auth is not required."}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\treturn sender_handle == entity_handle\t\n\t\ndef check_timestamp(timestamp):\n\t\n\t\"\"\"\n\tCheck the timestamp from the sender, ensuring its format is UTC.\n\t\n\tIf it's not UTC, then we log it, and return false, and we'll try again.\n\t\"\"\"\n\t\n\t## Convert timestamp as an int to a string to ensure we can parse that\n\t## time with the correct python code, if we haven't already.\n\tif not isinstance(timestamp, str):\n\t\ttimestamp = str(int(timestamp))"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t\n\t# Get these from the sent record\n\tsender = payload[\"sender_name\"]\n\t_from = payload.get(\"from\")\n\n\t# Sometimes we'll have an entity_handle in both in the sent record and elsewhere. We'd like to compare\n\t# sender_handle and entity handle, so go by the entity handle.\n\tentity_handle = payload.get(\"entity_handle\") or payload[\"sender_handle\"]\n\t\n\tif sender not in [sender_handle, entity_handle]:\n\t\traise Exception(f'When sending, sender_handle and"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tfrom. import NodeInfo\n\tdoc = NodeInfo(url=url)\n\turl = urlparse(url)\n\tif document_path:\n\t\tif not document_path.startswith('/'):\n\t\t\tdocument_path = '/'+document_path\n\t\tassert len(url.path) > 0, \"Can not get NodeInfo document for empty URL\"\n\t\turl = url._replace(path=document_path)\n\t\tnodeinfo_doc = NodeInfo(url=url).get_nodeinfo_document()\n\t\treturn nodeinfo_doc[document_path"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = 'nodeinfo.well-known'\n\treturn make_request(url, document_path)\n\ndef get_json(url):\n\"\"\"\n\tConvert the specified URL into JSON.\n\t\n\t:arg url: The full URL to convert, this MUST include protocol (ie https://example.com)\n\t:returns: dict or None\n\t\"\"\"\n\t\n\t# Parse with urlparse, check for missing 'path'\n\turl_parts = list(urlparse(url))\n\turl_parse"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is None:\n\t\tif nodeinfo_wsdoc == '':\n\t\t\tfrom.web.auth.web_authenticator import WebAuthenticator\n\t\t\twebauth = WebAuthenticator(url)\n\t\t\tnodeinfo_wsdoc = webauth.get_userinfo_nodeinfo_document()\n\t\tdocument_path = nodeinfo_wsdoc.split(';base=')[-1].split('&')[0].strip()\n\treturn make_nodeinfo_wellknown_document(url, document_path)\n\ndef make_nodeinfo_wellknown_document"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t\n\treturn_obj = make_nodeinfo_well_known_document()\n\treturn_obj.DocumentDirectoryPath = document_path\n\n\treturn_obj.SetDefaultLocation(\"%s/location\"%url)\n\treturn_obj.SetDefaultNodeID(\"%s/nodeid\"%url)\n\n\treturn return_obj\n\n\n\n\n\ndef get_nodeinfo_document(url, document_path=None):\n\"\"\"\n\tGet a NodeInfo document\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif not document_path:\n\t\tbase_url = parse_uri(url)\n\t\tif base_url is not None:\n\t\t\tdocument_path = urljoin(base_url, NODE_INFO_URL)\n\treturn {'documentPath': document_path}\n\t\ndef get_nodeinfo_xml_from_well_known_document(url, xml=None, custom_dumps=None):\n\t\"\"\"\n\tGenerate an XMP from a NodeInfo.well-known document URL.\n\t\n\tSee spec: http://nodeinfo.diaspora"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdoc, error = get_object(url, \"v0/well-known\", None)\n\tif error: return None, error\n\t\n\tif document_path:\n\t\tif not 'well-known' in doc:\n\t\t\tdoc['well-known'] = {}\n\t\tif not document_path in doc['well-known']:\n\t\t\tdoc['well-known'][document_path] = './documents/'\n\treturn doc, None\n\ndef build_nodeinfo(url, document_path=\"/v0/nodeinfo.json\"):\n\t\"\"\"\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tassert 'https' in url\n\t\n\troot = urlparse.urlparse(url).path\n\tnodeinfo_url = urlparse.urljoin(\n\t\turl, \n\t\t'/'.join([root, 'nodeinfo']))\n\tdocument_path = document_path or root + '.nodeinfo'\n\t\n\tdef _write(lines):\n\t\twith open(document_path, 'wb') as fd:\n\t\t\tfd.writelines(\n\t\t\t\t'\\n'.join([l for l in lines])\n\t\t\t\t.split('\\n'))\n\t\t\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Set custom document path if provided\n\tif document_path:\n\t\t_document_path = '/' + document_path\n\telse:\n\t\t_document_path = '/'\n\t\n\t# Build well-known document base params\n\twell_known_url = url + _document_path + 'nodeinfo.well-known'\n\twell_known_nodeinfo_doc = {\n\t\t'scheme': {\n\t\t\t'nodeinfo.well-known': 'https://example.com'\n\t\t},\n\t\t'nodes': {}\n\t}\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif not document_path:\n\t\tdocument_path = 'well-known'\n\treturn get_nodeinfo_document(\n\t\tnodeinfo_document_well_known, url, document_path)\n\t\ndef get_nodeinfo_document(document, url, document_path=None):\n\t\"\"\"\n\tGet a document by ID and return a dictionary\n\t\n\t:arg document: The dictionary from nodeinfo_document()\n\t:arg url: The full base URL with protocol (eg https://example.com) (optional)\n\t:arg document_path: Specify"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Get the url with no parameters\n\turl_parts = urlparse(url)\n\turl_without_params = url_parts.geturl().split(path_sep)\n\tnodeinfo_url = url_without_params[0] + path_sep + '?' + url_without_params[-1]\n\t\n\t# Construct the full url now that we have the NodeInfo base url\n\tif document_path is None:\n\t\tfull_url =  urlparse(url).geturl()\n\telse:\n\t\tfull_url = '"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tpub = base64_to_key(public_key)\n\tverifier = pysodium.signing.SodiumSigningKey(pub)\n\txml_key_signature = sign(verifier, doc)\n\treturn verify(verifier,xml_key_signature, signature)\n    \n\"\"\"\nVerify that a document was signed by the correct account\n\"\"\"\ndef verify_authorized_signature(authorized_keys, doc, signature):\n    for authorized_key in authorized_keys:\n        if verify_relayable_signature(authorized_key,"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t\n\t# Retrieve the root element\n\troot = doc.getroot()  # type: Element\n\n\t# Ensure this is verified.\n\tassert (root.tag!= 'claim')\n\n\t\t# Retrieve the claim sub-element, that is an encrypted element with\n\t\t# the claim signature.\n\tclaim_element = find_child_sub_tag_of_type(root, \n\t\t'claim').getchildren()[0]  # type: Element\n\n\t\t# Retrieve the X509 certificate, the certificate is stored in \n\t\t# the x509= attribute, we"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# parse the message\n\tdoc = minidom.parseString(doc.decode(\"utf8\"))\n\theader = doc.find(\"header\")\n\tbody = doc.find(\"body\")\n\tsender = doc.find(\"sender\")\n\tsignature = doc.find(\"signature\")\n\t# look for the signed data and verify signature\n\tif sender is None or body is None or signature is None or \\\n\t   doc.getElementsByTagName(\"from\").parent.get_attribute(\"uri\")!= signature.parent.get_attribute(\"uri\"):"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tmessage = _relay_signature_message(doc, signature)\n\tverifier = _relay_get_verifier(public_key)\n\ttry:\n\t\tverifier.verify(message, signature)\n\t\treturn True\n\texcept nacl.exceptions.VerifyError as e:\n\t\traise Exception(\"signature is invalid: %s\" % (str(e),))\n\n# Signature method using NIST 800-53\n# signature = {\n# \t\"body\": <the content of the signature element>,\n# \t\"signature\": <"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# Parse input variables\n\tpk = base64.b64encode(str(public_key).encode('utf-8')).decode('utf-8')\n\tdoc = base64.b64encode(doc.encode('utf-8')).decode('utf-8')\n\tsig = base64.b64encode(str(signature).encode('utf-8')).decode('utf-8')\n\tdoc_data = {'Message-ID': '<MESSAGE-ID>',\n\t\t\t\t'From-"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tparsed = lxml.etree.parse(open(doc))\n\troot = parsed.getroot()\n\tsigned = root.findall(\"sec:SignedInfo/sec:Signature[@Id='x']\", root.nsmap)\n\tassert len(signed) == 1\n\tsecurity_attributes = signed[0].getchildren()[0].getchildren()[0]\n\tsava_id = security_attributes.find(\"d:ReferenceList/d:Reference/d:Id\", root.nsmap)\n\tassert sax_id"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif _is_text_node(doc):\n\t\tcontent = doc.text\n\telse:\n\t\tcontent = doc.getElementsByTagName(\"content\")[0].firstChild.nodeValue.encode(\"utf8\", \"replace\")\n\n\tcontent_type = public_key.verify(bytes(content), bytes(signature), SignatureError)\n\n\tif content_type.verifyTime + 30000 < _now():\n\t\traise SignatureError(\"Verification of signature against time failed\")\n\telse:\n\t\treturn True\n\n\ndef generate_signature(k:"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tdoc.setAttributeNS(NS_XML, 'xmlns:xs', NS_XML_SCHEMA_1_0)\n\tdoc.setAttributeNS(NS_XML, 'xmlns:xsi', NS_XSI)\n\txsi = doc.createElementNS(NS_XSI, 'xsi:schemaLocation')\n\txsi.setAttributeNS(NS_XSI, 'xsi:noNamespaceSchemaLocation', NS_XSI_BRIX_SCHEMA)\n\tdoc.appendChild(xsi)\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tdoc_e = minidom.parseString(doc).firstChild.firstChild\n\tsignlist = doc_e.getElementsByTagName(\"ds:SignatureType\")\n\tif len(signlist) > 0:\n\t\tsigtype = signlist[0].value\n\t\tif sigtype!= \"SignatureValue\" and sigtype!= \"SignedInfo\":\n\t\t\t# it should only be SignatureValue.\n\t\t\t# The rest of the data is invalid.\n\t\t\treturn False\n\n\t\t# if it was a signed message, try to parse the signature, verify\n\t\t#"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tkey_pair = RSA.importKey(public_key)\n\ttry:\n\t\trelay_pub_key = PKCS1_v1_5.new(key_pair)\n\texcept BaseException as e:\n\t\tprint(\"Error generating public key: \" + str(e))\n\t\traise SystemExit(1)\n\n\ttry:\n\t\treturn relay_pub_key.verify(signature, doc)\n\texcept BaseException as e:\n\t\tprint(\"Error verifying signature: \" + str(e))\n\t\traise SystemExit(1)\n\n\nclass Encrypt"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\timport jsonschema\n\timport json\n\tfrom jsonschema import validate\n\t\n\t# old version\n\twebfinger = json.loads(document)\n\tdocument_version = webfinger[\"@version\"]\n\tdel webfinger[\"@version\"]\n\tvalidator = jsonschema.Draft4Validator(json.loads(\"\"\"\n\t\t{\n\t\t\t\"$id\": \"http://example.com\",\n\t\t\t\"test\": \"must be a string\",\n\t\t\t\"required\": [],\n\t\t\t\"properties\": {\n\t\t\t\t\"@document_version\": {\n\t\t\t\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tif document_type_is_json(document):\n\t\tresult = json.loads(document)\n\t\tlog_text = json_to_dict_log_text(result)\n\telse:\n\t\tresult = xrd_to_dict(document)\n\t\tlog_text = xrd_to_dict_log_text(result)\n\n\tresult = {\n\t  \"uri\": result[\"uri\"],\n\t  \"type\": result.get(\"type\"),\n\t  \"type_label\": result.get(\"type_label\"),\n\t  \"email\": result.get(\""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\ttry:\n\t\treturn webfinger_json.loads(document)  # Returns the document\n\texcept JSONDecodeError:\n\t\treturn webfinger_xrd.loads(document)  # Returns the document\n\ndef parse_diaspora_webfinger_json(document: str) -> Dict:\n\t# if document == \"\":\n\t\t# LOGGER.info(\"No web server found, webfinger data\")\n\treturn webfinger_json.loads(document)\n\n\t# LOGGER.debug(document)\n\n\t# LOGGER.debug(document.read())\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\treturn _parse_webfinger(_parse_json(_parse_xrd(document)))\ndef parse_diaspora_xrd(_document: str) -> Dict:\n\"\"\"\n\tParse Diaspora XRD.\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/xrd.html\n\t\"\"\"\n\treturn _flatten_list(json.dumps(_parse_xrd(_document), ensure_ascii=True))\ndef _parse_xrd(_document: str) -> Dict:\n\t\"\"\""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\tdef read_fields(self, lines: List[str], idx: int) -> Dict:\n\t\tfields = {}\n\t\t\n\t\tfor field in lines[idx].split():\n\t\t\ttry:\n\t\t\t\tkey, val = field.split('=')\n\t\t\texcept:\n\t\t\t\t# skip to next line\n\t\t\t\tidx = idx + 1\n\t\t\t\tcontinue\n\n\t\t\tkey = key.strip('\"')\n\t\t\tval = val.strip('\"')\n\n\t\t\tfields[key] = val\n\n\t\treturn fields\n\n\tdef parse_fields(self, doc: str) ->"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdocument = str(document)\n\tdiaspora_webfinger = re.search( r'(?ims)diaspora\\s+webfinger\\s?=\\s?(\\{.+\\}|([A-Za-z0-9_$%-]+))\\s?;?$', document )\n\tif diaspora_webfinger is None:\n\t\traise Exception('Diaspora webfinger is undefined!')\n\t\n\tdata = json.loads(diaspora_webfinger.group(1))\n\tdata_asdict = {}\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tlines = document.split('\\n')\n\tjson_url = False\n\tdict_url = False\n\twebfinger_dict = {}\n\tfor line in lines:\n\t\tif u\"json:\" in line:\n\t\t\tjson_url = True\n\t\telif json_url:\n\t\t\tline = line.replace(\"\\t\", \" \")\n\t\t\tm = re.match(\"'''(.*)'''\", line)\n\t\t\tif m:\n\t\t\t\t# line_json = literal_eval(m.group(1))\n\t\t\t\twebfinger_dict = json.loads("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tresult = {}\n\ttry:\n\t    webfinger = _parse_webfinger(document)\n\texcept DiasporaBadXRD:\n\t    return result\n\texcept DiasporaXRDFormatError as e:\n\t    _LOGGER.debug(e.args)\n\telse:\n\t    result[\"service\"] = service_url = webfinger[\"service_url\"]\n\t    result[\"identifier\"] = identifier = webfinger[\"identifier\"]\n\t    result[\"realm\"] = realm = webfinger[\"realm\"]\n\t    result[\"claimed_identifier"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\timport re\n\t\n\timport json\n\timport requests \n\n\t# Old is deprecated, not used anymore in Diaspora Federation\n\told = False\n\t\n\ttry:\n\t\t\n\t\tif old:\n\t\t\tdata = json.loads(document)\n\t\telse:\n\t\t\tdata = json.loads(document)\n\n\t\tif old:\n\t\t\t\n\t\t\t# Only used for old Diaspora\n\t\t\t\n\t\t\t# Find the type\n\t\t\ttype = data[\"type\"]\n\t\t\tif type == \"domain\":\n\t\t\t\ttype = \"https://\"+data[\"url\"]\n\t\t\telse:"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdocument_split = document.split(\"?\")\n\tif len(document_split)!= 2:\n\t\traise ValueError(\"Document should be either a JSON Web Token or a X-Road FED URI.\")\n\t\n\tif \"application/json\" in document_split[1]:\n\t\treturn __parse_diaspora_webfinger_json_webtoken(document_split[1])\n\telse:\n\t\treturn __parse_diaspora_webfinger_xroad(document_split[1])\n\n\n\ndef __parse_diaspora_json_webtoken("}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# The URI to send\n\turi = \"https://rcc.openbsd.org/webfinger/docs\"\n\n\ttry:\n\t\t# Exec the function\n\t\treturn _try_retrieve_webfinger_document(uri, handle)\n\texcept Exception as e:\n\t\treturn None\n\t\t\ndef _try_retrieve_webfinger_document(url: str, handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document (aka URI) using the\n\thandle on an RFC7033 page. Does not raise an"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn __http_open(handle).read()\n\texcept (IOError, requests.RequestException):\n\t\tlogging.exception('Failed during webfinger retrieval')\n\t\treturn None\n\n\ndef retrieve_webfinger(handle: str) -> str:\n\t\"\"\"\n\tRetrieve a webfinger, try the cached value, and return either the cached value or the retrieved\n\tvalue. Returns '', if no webfinger could be accessed and loaded - most likely due to 404 or 405.\n\t\"\"\"\n\twebfinger = cached_get_webfinger(handle)\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\th = Handle.from_string(handle)\n\t\th.__open()\n\texcept (InvalidHandle, ValueError, OverflowError):\n\t\treturn None\n\texcept LookupFailed:\n\t\tpass\n\n\ttry:\n\t\traw = urlsplit(h.__webfinger_path())\n\texcept AttributeError:\n\t\treturn None\n\n\tscheme = raw.scheme or 'http'\n\tnetloc = raw.netloc\n\tusername = h.__user\n\n\treturn urlunsplit((scheme, netloc, '/',"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\twith get_docdb() as docdb:\n\t\tfor d in docdb.search_webfinger(handle):\n\t\t\treturn d.document\n\treturn None\n\ndef get_webfinger(handle, web_server: str = DEFAULT_WEB_FINGER_WES, max_fails: int = 5) -> Optional[str]:\n\t\"\"\"\n\tIf `handle` is a URL, retrieve it from the specified WebServer. If it is a Document,\n\tretrieve the webfinger document from the Webfinger server if available.\n\t\"\"\"\n\t# Ensure"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tresult = None\n\ttries = 0\n\twhile not result and tries <= 3:\n\t\ttry:\n\t\t\tlog.debug(\"Loading a webfinger document for {0}\".format(handle))\n\t\t\tresult = doc.Document('uri/%s.html' % handle).get_text()\n\t\texcept:\n\t\t\tpass\n\t\ttries += 1\n\treturn result\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tfrom.document import Document\n\t# Get the base URL, for what we need\n\turl = urljoin(f'{__baseurl__}/', handle)\n\tres = send_request_get_str(url)\n\n\tif not res:\n\t\treturn None\n\n\ttry:\n\t\t# Document URL looks like this:\n\t\t#\thttp://<domain>.org/<document>-#<handle>.xml\n\t\tdocument = Document(res)\n\t\tdocument.document_id = document.document_id.replace('-#', '-')\n\n\texcept Exception:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# get a list of webfiered documents matching the search\n\twebfiledirs = get_webfiledir_from_domain_handle(handle)\n\treturn get_most_frequently_used_webfiledir(webfiledirs)\n\ndef get_webfiledir_from_domain_handle(handle: str) -> Optional[List[str]]:\n\t# get the list of webfile dirs matching\n\tif handle == \"_\":\n\t\t# we want to search all webfiledirs\n\t\twebfiledirs = [\n\t\t\tpath for"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# See if it already works\n\traw_url = get_webfinger_document_url(handle)\n\tif raw_url:\n\t\treturn urllib.request.urlopen(raw_url).read()\n\n\t# It does not!\n\tpass\n\n\ndef is_unsubscribed(name: str, handle: str) -> bool:\n\t# TODO: Maybe should be more specific? Maybe we can check for\n\t# any of the following:\n\t#\n\t#  {handle:...,subscription_endpoint:...,subscription:...}"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_document(handle)\n\texcept (urllib.error.HTTPError, requests.exceptions.InvalidSchema):\n\t\treturn None\n\n\ndef retrieve_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tAttempt to retrieve the XML document hosted by a webfinger URI. Does not raise\n\tif the document is not known or if it fails.\n\n\tReturns the document if able to find it, otherwise None.\n\t\"\"\"\n\theaders = {\"Accept\": \"application/rdap+json,application/ld+json;q=1"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresp = requests.get(f\"{WK_HANDLE_BASE_URL}{handle}/rdata_webfingerevent\", {\n\t\t\t\"resource\": str(handle)\n\t\t})\n\t\tresp.raise_for_status()\n\texcept Exception:\n\t\tlog.info(f\"Failed to retrieve {handle}, webfinger document\")\n\t\treturn None\n\n\tlog.info(f\"Retrieved {handle} webfinger document\")\n\treturn resp.text\n"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresponse = diospora.fetch(handle)\n\tdocument = diospora.parse(response)\n\treturn document\n\ndef retrieve_and_parse_camelist(handle):\n\t\"\"\"\n\tSimple utility to return a document from a Diaspora handle in CAMELList format.\n\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresponse = diospora.fetch(handle)\n\tdocument = dio.parse(response)\n\treturn document\n\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tfile_data = ''\n\ttry:\n\t\tfile_data = retrieve_diaspora_webfinger_file(handle)\n\texcept Exception as e:\n\t\terror_traceback = traceback.format_exc()\n\t\tdebug('diaspora_retrieve_and_parse_diaspora_webfinger failed due to: %s' % error_traceback)\n\t\treturn {'error': error_traceback[1:]}\n\t\n\treturn {'data': get_file_data(file_data), 'is_binary': 0}\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\twebfinger = get_webserver(\"diaspora\")\n\tpath = os.path.join(webfinger, handle, \"?format=\",\"html\")\n\t(status, out, _) = module.run_command(\"/usr/bin/\", \"wg\", \"webarchive\", path)\n# \twebfinger_file_name = \"index.html\"\n\t(status, out, _) = module.run_command(\"/usr/bin/\", \"wg\", \"webarchive\", path)\n\thtml = \"\".join(out)\n\t\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = requests.get(DIA_WEBFINGER_URL+handle,\n        headers={\"X-Moodle-API-Key\": DIA_WEBFINGER_SCKEY})\n\treturn process_webfinger(r)\n\n\ndef retrieve_and_parse_diaspora(handle):\n\t\"\"\"\n\tRetrieve and parse a remote Diaspora document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tj = requests.get(DIA_URL+handle).json()\n\td = {}"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tsio = urllib.urlopen(\"%s/document/%s/source\" % (\n\t\tdios_db, urllib.quote(handle)))\n\treturn webfinger_parser.parse(sio)\n\ndef retrieve_all_documents(doc_ids, mode=\"raw\"):\n\t\"\"\"\n\tFetch or parse a document and store it.\n\t\n\tIf mode = \"raw\", the raw webfinger document is returned;\n\totherwise, the parsed version will be returned.\n\t\n\t:arg doc_ids: Ids of documents"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresult = {}\n\txml = retrieve_webfinger(handle)\n\tif xml:\n\t\t_parse_diaspora_webfinger_doc(xml, result)\n\treturn result\n\t\t\ndef _parse_diaspora_webfinger_doc(xml, result):\n\tfor key, value in xml.items():\n\t\ttry: value = float(value)\n\t\texcept: value = unicode(value)\n\t\tresult[key] = value\n\t\t#value = unicode(value)\n\n\ndef retrieve_media_urls(doc_id, media_type"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t# TODO: Use json\n\treturn\n\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tretval = {}\n\ttry:\n\t\tdoc = fetch_webfinger(handle)\n\t\tif not doc:\n\t\t\treturn False\n\t\tdoc = doc.rstrip(\"\\n\")\n\t\tdoc = re.sub(' +',' ',doc)\n\t\tdoc = doc.replace(\"\\r\",\"\")\n\t\tmatches = re.findall('([^-]+)-([A-Z]+)-([\\w\\d-]+)',doc)\n\t\tretval = {\"handle\":matches[0][0], \"name\":matches[0][1], \"profile\":\"s"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\td = {}\n\tdata = retrieve_webfinger_as_string(handle)\n\n\t# Iterate over every line, looking for a line starting with a \"#\"\n\t# and looking in all the \"#\"s in the lines for a comment (with an\n\t# equals sign around it), or looking in the first \"#\" for a line\n\t# with no comment around (and therefore not a comment).\n\tfor line in data.split(\"\\n\"):\n\t\t#print(line)\n\t\tif line.startswith(\"#\"):\n\t\t\tmatch = re.search(\""}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t# Create a URL that allows HTTP compression.\n\turl = '{0}data/{1}?format=webfinger&format=xml'.format(\n\t\tdiaspora_webfinger_endpoint,\n\t\thandle)\n\t\n\t# We now have to handle some of the common response code that\n\t# would make this a very common operation.\n\tresp = urlopen(url)\n\n\t# The result should also contain a well-formed XML document.\n\tres = tostring(resp)[2:-2].encode('utf-8')\n\n\treturn fromstring("}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdoc_host = \"{}:Diaspora/Host?id={}&mode=view\".format(host, host)\n\tdoc_id = requests.get(doc_host).headers[\"location\"]\n\t# Diaspora responds with HTTP 301 for \"no content\" - see #52\n\tdoc_meta_url = \"{}:Diaspora/{}?mode=view\".format(host, doc_id)\n\tdoc_meta = requests.get(doc_meta_url).json()\n\treturn doc_meta\n\ndef get_diaspora_host_meta"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\turl = 'http://{}/rd.jsong'\n\turl = url.format(host)\n\tconn = http.client.HTTPSConnection(host, timeout=10)\n\ttry:\n\t\tconn.request('GET', url)\n\t\tres = conn.getresponse()\n\t\tdata = res.readall()\n\t\tif not data:\n\t\t\tlog.error('No data returned')\n\t\telse:\n\t\t\txrddata = json.loads(data.decode())\n\t\t\thostXrddata = XRD(**xrddata)\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\trequest = Request(\"https://{}/host_meta.json\".format(host))\n\t\t\t\n\t# This is needed for older Diaspora.\n\trequest.add_header(\"Accept\", \"application/json\")\n\t\n\t# Note: If a new Diaspora server gets added, \n\t# `Content-Type` should now be `application/json`.\n\t\n\tresponse = urllib_request.urlopen(request)\n\tdata = jsonlib.load(response)\n\treturn XRD(data)\n\ndef new_user(email, f"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn _xrd(host, \"/api/document/document.json?id={host}\".format(host=host))\n\n\n###\n# Document functions\n###\t\ndef add_document(content, to=None):\n\t\"\"\"\n\tUpload a document to the Diaspora Document Store (DOCS).\n\t\n\t:arg content: ``str`` containing file contents of the document\n\t:arg to: Host to upload the document to. Default is current host.\n\t\"\"\"\n\tdocument = _xrd(to or _current_host(), \"/store/document/add\")"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\ttry:\n\t\tdoc=host.xml_doc\n                d = xrdict()\n\t\td['host_id']=doc.attrib['host_id']\n\t\td['host_type']=doc.attrib['type']\n\t\td['server_version']=doc.attrib['server_version']\n\t\td['version']=xrdict()\n\t\td['version']['client']=doc.attrib['client_version']\n\t\td['version']['server']=doc.attrib['server_version']\n\t\treturn d\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\tdoc = XRD(host)\n\tdoc[\"type\"] = \"XRD\"\n\t\n\treturn doc\n\ndef retrieve_xrd(host):\n\t\"\"\"\n\tRetrieve remote node metadata (if any) as well as the local document\n\tmetadata.\n\t\n\t.. note::\n\t\tWhen :func:`retrieve_meta_document` retrieves the local Diaspora\n\t\tmetadata, then it also retrieves ``XRD`` documents, but we aren't\n\t\tstrictly doing this yet. It may be better to use both meta documents\n\t\tand"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tpath = _path('/host-meta',host)\n\td = _method('GET',path)\n\td = d.json()\n\tassert d['meta']['version']!=0\n\tassert d['id']\n\treturn XRD(d,path)\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\thost_path = d.conf.storage_base_path + \"/%s/meta/\" % host\n\tmeta = d.client.meta.get(host_path)\n\treturn parse_diaspora_raw_xrd(meta)\n\ndef get_old_xrd(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdoc = get_retrieved_document('diaspora/%s/meta"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn RemoteXRDSubmission(host, 'XRD document')\n\n\ndef retrieve_diaspora_meta(host, meta_key):\n\t\"\"\"\n\tRetrieve Diaspora host-meta documents for remote Diaspora host\n\n\t:arg host: Host-meta document to retrieve\n\t:returns: list of ``XRD`` instances\n\t\"\"\"\n\treturn RemoteXRDSubmission(host, 'XRD documents', meta_key)\n\n\ndef retrieve_diaspora_meta_as_json(host, meta_key):\n\t\"\"\"\n\tRet"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdoc = get_diaspora_service().host_meta(host)\n\t# TODO: Remove '#meta-title', '#meta-description', '#meta-summary' and '#meta-tags'.\n\t# The Diaspora meta file has all of the metadata currently in it. They're not actually in the\n\t# Diaspora host-meta record, but since Diaspora doesn't update the meta information after every\n\t# change, they seem to be preserved. The problem is, if a record is merged into the database\n\t# without re-downloading the"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif isinstance(data, (BytesIO, StringIO)):\n\t\tkwargs['files'] = {args[0]: data}\n\t\tdata = None\n\telse:\n\t\tkwargs['data'] = '&'.join(\n\t\t\t(\n\t\t\t\t'{}={}'.format(k, v)\n\t\t\t\tfor k, v in data.items()\n\t\t\t)\n\t\t)\n\ttry:\n\t\tmethod = getattr(requests, method)\n\texcept AttributeError:\n\t\traise AttributeError(\n\t\t\t'Not a supported method ({}); possible options: {}"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif data:\n\t\tmime = kwargs.get(\"mime\", getattr(data, \"mime\", None))\n\t\tfiles = {\"file\": (mime, data),} if mime else {\"file\": data}\n\t\treturn _request(url, \"files\" if mime else \"multipart\", method, *args, files=files, timeout=timeout, **kwargs)\n\telse:\n\t\twith tempfile.NamedTemporaryFile(suffix=\".docx\") as temp_file_handle:\n\t\t\ttemp_file_handle.write("}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\tr = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\t\tstatus = r.status_code\n\texcept Exception:\n\t\timport sys, traceback\n\t\tprint(sys.exc_info())\n\t\ttraceback.print_exc()\n\t\tstatus = None\n\treturn (status, None)\n\ndef get_response(url, params=\"\", timeout=10, method=\"get\", *args, **kwargs):\n\t\"\"\"\n\tHelper method to extract an html page via GET.\n\t\n\tAdditional ``*args"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tmime_type = None\n\n\tif not mime_type:\n\t\tmime_type, _encoding = mimetypes.guess_type(url)\n\n\tif not mime_type:\n\t\tmime_type = \"application/octet-stream\"\n\n\t# Handle files\n\tif hasattr(type(data), \"read\") and hasattr(data, \"name\"):\n\t\tfiles = {'file': data}\n\telse:\n\t\t# Handle bytes\n\t\tfiles = {'file': data} if mime_type == \"application/oct"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\theaders = {'Host':get_host_ip(), 'User-Agent':user_agent_dict['os'], 'Accept':'*/*'}\n\t\n\t# Check if we are sending a file\n\tfiles = None\n\tif hasattr(data, \"read\") and hasattr(data, \"seek\"):\n\t\tfiles = {'file':data}\n\t\t\n\tr = requests.post(url, data=data, headers=headers, files=files, timeout=10, *args, **kwargs)\n\t\n\treturnr = (r.status"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif data is None: \n\t\tdata = {}\n\theaders = kwargs.pop(\"headers\", {})\n\theaders[\"User-Agent\"] = random_agent()\n\ttry:\n\t\tif method == \"post\":\n\t\t\twith kwargs.pop('session', None) or requests.Session() as session:\n\t\t\t\trequest = requests.Request(method, url, data=data, headers=headers)\n\t\t\t\tprepped = session.prepare_request(request)\n\t\t\t\ttry:\n\t\t\t\t\tkwargs[\"proxies\"] = kwargs.pop(\"proxies"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\tif method not in [\"get\", \"post\"]:\n\t\t\traise ValueError(\"Method must be 'get' or 'post' \")\n\n\t\tif isinstance(data, basestring):\n\t\t\tif method == \"post\":\n\t\t\t\tmethod = \"post\"\n\t\t\t\tdata = data.encode('utf-8') if not isinstance(data, bytes) else data\n\t\t\telse:\n\t\t\t\tmethod = \"get\"\n\t\t\t\tdata = (data.decode('utf-8') if isinstance(data, bytes)\n\t\t\t\t\t\t\t\telse data).encode('utf-8"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\twith Session() as session:\n\t\tresponse = session.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\tresponse.raise_for_status()\n\t\treturn response.status_code, None\n\t\ndef send_form(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a form via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif data and type(data) == dict:\n\t\t# Dictionary to form-encoded parameters\n\t\tdata = urlencode(data)\n\t\tmethod = \"post\"\n\telif not data and type(data) in (str, bytes):\n\t\t# Data to send in as a form\n\t\tdata = (data, )\n\tbody, files = encode_to_multipart_formdata(data, enforce_types=False)\n\treturn urlopen_using_session(url,\n\t\tbody=body, files=files, method=method, timeout=timeout, *"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tmethod = method.lower()\n\tassert method in (\"get\", \"post\", \"put\", \"delete\")\n\tif method == \"get\":\n\t\tmethod = requests.get\n\telif method == \"post\":\n\t\tmethod = requests.post\n\telif method == \"put\":\n\t\tmethod = requests.put\n\telif method == \"delete\":\n\t\tmethod = requests.delete\n\telse:\n\t\tassert False, \"No such method\"\n\ttry:\n\t\treq = requests.Request(method, url, data=data)\n\t\treq = req.prepare"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key_pair in keys:\n\t\tdic[key_pair] = val\n\n#-------------------------------------------------#\n# dict_pop\n#-------------------------------------------------#\ndef dict_pop(dic, *keys):\n\tr\"\"\"\n\tpop a value of a nested key from a dictionary\n\t\n\texample:\n\t  dict_pop({}, 'key1.key2', 'key1.key3')\n\t\n\t:param dic: a dictionary to remove the value from\n\t:param key: the first key in a keys chain that should not be removed\n\t:param"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tkeys = keys if keys else ['key_1']\n\n\tkeys = key.split('.')\n\t\n\tfor k in keys[:-1]:\n\t\tif k not in dic:\n\t\t\tdic[k] = {}\n\t\tdic = dic[k]\n\n\tdic[keys[-1]] = val\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys: keys = ('',)\n\tdef _dict_insert(dic, val, key, *keys):\n\t\tif key in dic and key not in keys: return\n\n\t\tdic = dic.setdefault(key, {})\n\t\tif not isinstance(dic, collections.MutableMapping): raise Exception(\"dic at {!r}:{} is not a MutableMaping type\".format(key, val))\n\n\t\tif not keys or key in keys: keys = keys + (key,)\n\t\tfor i in range(len(keys)"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys:\n\t\tif type(dic)==dict:\n  \t\t\tv = dic.get(k, {})\n\t\t\tdic[k]=dict_combine(v, val)\n\t\telse:\n\t\t\tdic.insert(k, val)\n\t\t\treturn None\n\treturn None\n\ndef dict_append(dic1, dic2, *keys):\n\"\"\"\n\tcreate an iterative combination of values of two iterables\n\tcombined elements from dic2 should be inserted into dic1\n\t\n\texample:\n\t  dict"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif key not in keys:\n\t\tdic[key] = dict()\n\tdic[key][vals(vals(vals(values(keys))))] = val\n\treturn dic\n\ndef dict_insert_(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tparent = None\n\t\n\t# recursively fill a nested dictionary\n\tfor key in keys:     # for every key try to insert the value\n\t\tparent = parent.setdefault(key, {})\n\t\t# update the parent\n\t# when we are done, a copy will be returned\n\tret = {k: parent[k] if k in parent else None for k in dic.keys()}\n\t# replace the dictionary to the copy\n\tif key == keys[-1]:\n\t\ttry:\n\t\t\tret.update(dic[key], val)"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tif keys:\n\t\tk = key + '.' + keys[0]\n\t\tif k in dic:\n\t\t\tif (len(keys) == 1):\n\t\t\t\tdic[k] = val\n\t\t\telse:\n\t\t\t\tdict_insert(dic[k], val, k, *keys[1:])\n\t\telse:\n\t\t\tif (len(keys) == 1):\n\t\t\t\tdic[k] = val\n\t\t\telse:\n\t\t\t\tdict_insert(dic, val, k, *keys[1:])\n\telse:\n\t\tif key not in"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn dic\n\telse:\n\t\td1 = dic.setdefault(key, {})\n\t\tif keys[0] not in d1:\n\t\t\td2 = dict_insert(d1, val, keys[0])\n\t\t\td1.update(d2)\n\t\t\treturn d1\n\t\telse:\n\t\t\treturn dict_insert(d1[keys[0]], val, keys[0], *keys[1:])\n\ndef dict_insert_dict(dic, dict):\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tfor i in range(len(key)):\n\t\tkeys_ = keys[:i+1]\n\t\tkey_ = stringify(*keys_)\n\t\tdic.setdefault(key_, {})\n\t\t# print(key_)\n\t\t\n\t# print(key, keys)\n\t# print(key_)\t\n\tparent_key = key if not keys else key[:-2]\n\tparent_key = '_'.join(parent_key)\n\t# print(parent_key)\n\tdic.setdefault(parent_key,"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key_chain in keys:\n\t\tdic = dic.setdefault(key_chain, {})\n\tdic[key] = val\n\t\n\t\t\n\t\ndef add_to_dic(dic, val, key='value ', count='_count', sep=':'):\n\t\"\"\"\n\t\tto add an element to a dictionary where key is key:value\n\t\tcount is added to the dictionary under key\n\t\t\n\t\tExample: if the dictionary is {a:5, b:1} \n\t\tadd_to_dic(dic, 'c"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfile_names = settings_dirs.get_file_names(spec_option)\n\treturn ICONCfg_FileNames(file_names=file_names)\n\n\ndef list_of_settings(pathdir, spec_option):\n\"\"\"\n\tCreate new ICONCfg_Settings.Txt complex type from the file path\n\t\"\"\"\n\tsettings = []\n\tfile_names = list_of_file_names(pathdir, spec_option)\n\tfile_names.append(spec_option[\"default\"])\n\tfor file_name in file_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\t# Create new complex type from string\n\ts = '<IniType>\\n' +\\\n\t'<name>' + spec_option + '</name>\\n' +\\\n\t'<search>\\n' +\\\n\tspec_option.upper() + '\\n' +\\\n\t'</search>\\n' +\\\n\t'<sort>\\n' +\\\n\t'<field>\\n' +\\\n\t'<id>%s</id>\\n' % (spec_option.upper()) +\\"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tsettings_file_names = []\n\tfor settings_dir in settings_dirs :\n\t\tfor root, dirs, files in os.walk(settings_dir, topdown=True):\n\t\t    if '.' not in root:\n\t\t        for f in files:\n\t\t            if f == \"settings.ini\":\n\t\t            \tthis_file_name = os.path.join(root, f)\n\t\t                if spec_option == 'all':\n\t\t                    settings_file_names.append(this_file_name)\n\t\t                else:\n\t\t                    if"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn settings_dirs.create_complex_type(spec_option,\n\t'list_of_file_names',\n\ttype_class=ListOfFileNamesType,\n\textends_base_type=SpecBaseType['Settings'],\n\toption_groups=\n\t[\n\t\tSpecOptionGroup('Base',\n\t\ttype_name='list_of_file_names_type_element',\n\t\tkey='type_name',\n\t\tdefault_value='list_of_file_names_type_element',\n\t\tkeyed=False,\n\t\telements_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tif settings_dirs!= None:\n\t\tsettings1_key = {'baseSettingsDir': settings_dirs,  'SpecPath':spec_option}         # Define the'settings1' item of the IniType complex type\n\t\tif spec_option == 'Default.settings':                                          # For 'Default.settings' mode\n\t\t\tsettings1 = IniType(settings1_key)                                        # Create an Ini type object with 'baseSettingsDir' and 'SpecPath' items\n\t\telif spec_option == 'Configure.settings': \n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tdef_s = { \"name\": \"\", \"value\": \"\" }\n\tdefaults = []\n\tout = []\n\tfor d in settings_dirs:\n\t\tsettings = get_settings(d)\n\t\tif spec_option not in settings:\n\t\t\tcontinue\n\t\tname = settings[spec_option][\"name\"]\n\t\tif not name:\n\t\t\traise Error(\"No name specified for {0:s}\".format(name))\n\t\tsettings = settings[spec_option][\"value\"]\n\t\tif type(settings) == bool:\n\t\t\tdefault_val = \"True"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t\n\t# Get the default settings\n\tsettings = os.path.join(settings_dirs, spec_option)\n\t\n\t# Set-up the IniType\n\tIniType = \"IniType\"\n\t\n\tinilist = []\n\tinilist.append('[IniType]')\n\n\tfor line in get_config_lines(settings):\n\n\t\tinilist.append(line)\n\t\t\n\treturn inilist\n\n\ndef ini_to_list(ini):\n\t\"\"\"\n\tRead the ini and return a list"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tINI_SCHEMA = IniType(\n\t\t[\n\t\t\t('# Name', 'name'),\n\t\t\t('# Path', 'path'),\n\t\t\t('# Options', 'file_options'),\n\t\t\t('# Content', 'content'),\n\t\t]\n\t)\n\n\tini_file_names = {}\n\n\tfor spec_setting in spec_option:\n\t\tif spec_setting.values == 'FILE' or spec_setting.values == 'F':\n\t\t\tspec_file_name = os.path.join(settings_dirs, spec_setting.name)\n\t\t\tini"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn [\n\t\tIniType(\n\t\t\tsettings_dir=settings_directory,\n\t\t\tspec_setting=specs_dir_option,\n\t\t\t)\n\t\tfor settings_directory, specs_dir_option\n\t\tin zip(settings_dirs, spec_option)\n\t]\n\ndef create_list_of_settings_dirs(list_of_spec_options):\n\t\"\"\"\n\tMake a dict of dirs dict-of-lists\n\twith each 'folder' key referring to a list of settings_dirs with 'directory' keys\n\t\"\"\"\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\ttry:\n\t\tsettings = []\n\t\tfor d in settings_dirs:\n\t\t\tfn=os.path.join(d, spec_option)\n\t\t\twith open(fn, \"r\") as f:\n\t\t\t\tsettings+=f.readlines()\n\n\t\treturn IniType(section=spec_option, lines=settings)\n\texcept Exception as error:\n\t\tprint(error)\n\t\traise\n\n"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn AnsibleConfigManager(cls.config_file_dir)\n\n  "}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tlogger = logging.getLogger(\"AnsibleConfigManager\")\n\n\tif cls.current_config_name == cls.config_name_default:\n\t\tfrom ansible.cli.playbook import Playbook\n\t\tpp = Playbook([],inventory=\"localhost, group=hosts\")\n\telse:\n\t\tfrom ansible.cli.playbook import Playbook\n\t\tpp = Playbook(\n\t\t\t{\n\t\t\t\t\"_meta\": {\n\t\t\t\t\t\"hostvars\": {},\n\t\t\t\t\t\"hosts\": []\n\t\t\t\t},\n\t\t\t\t\"hosts\": []\n\t\t\t"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn Cli(hostname=config.HOST, username='root',\n              password=config.PASS, port=22, look_for_keys=False).send_command(\n    'cd /opt/{}'.format(config.PROJECT)\n    )\n\n\n@task\ndef copy_scripts()\n\"\"\"\n    Creates the copy of the scripts needed for the Ansible playbook.\n    \"\"\"\n\tmanager = ansible_config_manager()\n\tmanager.send_command(\n\t    \"wget -r --progress http://{}:{}/scripts"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.get('ansible_config_manager')\n\n\n@classmethod\ndef ansible_callback_api(cls, target_id):\n\t\"\"\"\n\tGets the ansible callback api.\n\t\"\"\"\n\treturn cls.get('ansible_callback_api', target_id)\n\n\n@classmethod\ndef ansible_callback_handler(cls):\n\t\"\"\"\n\tGets the ansible callback handler name.\n\t\"\"\"\n\treturn cls.get('ansible_callback_handler')\n\n\n@classmethod\ndef ansible_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\twith open(cls.CONFIG_MANAGER) as f:\n\t\tconfig_value = yaml.safe_load(f)\n\treturn config_value\n\n\n\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn AnsibleConfigManager(cls.ansible_config_backend, cls.ansible_config_profile)\n\n\t\ndef ansible_inventory_manager(cls):\n\"\"\"\n\tGets the ansible inventory manager.\n\t\"\"\"\n\treturn AnsibleInventoryManager(cls.ansible_inventory_backend, cls.ansible_inventory_profile)\n\n\t\ndef ansible_playbook_manager(cls):\"\"\"\n\tGets the ansible playbook manager.\n\t\"\"\"\n\treturn AnsiblePlaybookManager(cls.ans"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\timport jinja2.nodes, jinja2.Template, os, os.path, re, sys, traceback, yaml, json\n\timport jinja2.meta\n\tfrom subprocess import Popen, DEVNULL\n\ttry:\n\t\tfrom StringIO import StringIO\n\texcept ImportError:\n\t\tfrom io import StringIO\n\ttry:\n\t\timport collections\n\texcept ImportError:\n\t\timport sys\n\t\tdict = sys.modules[__name__.rsplit('.', 2)[0]]\n\ttry:\n\t\timport shlex"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tlogging.info(\"Gathering Ansible Configuration Manager...\")\n\tansible_conf_manager = cls.get_config_files_by_group(\"ansible\")\n\tansible_conf_manager = cls.resolve_variables(ansible_conf_manager)\n\tconfig_manager = cls.set_ansible_inventory(ansible_conf_manager)\n\tlogging.info(\"Configuration Manager: {}\".format(config_manager))\n\treturn config_manager\n\n \n\n  def check_ansible_connection_method(self):\n "}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn 'config_manager'\n\n\nclass Host(models.Model):\n\t\"\"\"\n\tRepresents a known host.\n\t\"\"\"\n\tcreated_at = models.DateTimeField(auto_now_add=True, blank=True, null=True)\n\tupdated_at = models.DateTimeField(auto_now=True, blank=True, null=True)\n\t# We should have one host and many configurations.\n\t# Each configuration represents a set of config names.\n\n\tname = models.CharField(max_length=16, blank=False)\n\t"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\twith open(str(pathlib.Path(__file__).parent.absolute()) + \"/../ansible-data/ansibleconfig.yml\") as f:\n\t\t\tdata = f.read()\n\t\treturn yaml.safe_load(data)\n"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._get_workspace(workspace=globals.workplace)\n\n\nclass JobManagerSingleton:\n\t\"\"\"\n\t\"\"\"\n\t_JOBMANAGER = None\n\n\t@classmethod\n\tdef get_job_manager(cls):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif cls._JOBMANAGER is None:\n\t\t\tcls._JOBMANAGER = JobManager()\n\t\treturn cls._JOBMANAGER\n\n\nclass RunManagerSingleton:\n\t\"\"\"\n\t\"\"\"\n\t_RUNMANAGER ="}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tfrom pycherwell.workspace import workspace\n\treturn workspace(cls)\nworkspace_manager.__doc__ = workspace.__doc__\n\nworkspace = workspace_manager()\n\n\n\n\n\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t#\n\t# TODO: Replace with workspace_managers.\n\tws_mgr = getcurrent(cls)\n\treturn ws_mgr\n\ndef wspace_manager(): return workspace_manager()\ndef get_workspace(): return get_workspace_mgr()\n\ndef set_working_directory(wd):\n\tif not os.path.isdir(wd):\n\t\traise ValueError(\"Working directory {} is not a working directory\".format(wd))\n\tos.chdir(wd)\ndef getcwd(): return os.getcwd"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn Managers.get_workspace_manager()\nworkspace_manager = classmethod(workspace_manager) \n\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__sw_mgr\n\nclass FileExplorer(QObject):\n\t\"\"\"\n\tCette classe regrouper toutes les actions qui concerne la navigation des fichiers sur l'interface de la fen\u00eatre.\n\tUne instance de cette classe se retrouvera dans le dictionnaire global fenetre.\n\t\"\"\"\n\t# SIGNAL de connection quand un nouveau document est cree\n\tcreateNewDocument = Signal(str)\n\t# SIGNAL de connection quand le fichier est"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\timport maya\n\n\tif cls._instance is not None:\n\t\treturn cls._instance\n\n\ttry:\n\t\tcmds.workspaceManager(globals())\n\t\tcls._instance = cls._get_base_class()()\n\n\texcept RuntimeError:\n\t\tcmds.workspaceManager(newWorkspace=os.getcwd(),openWorkspace=True)\n\t\tcls._instance = cls._get_base_class()()\n\n\treturn cls._instance\n\n\nclass get_model_handler(BaseClass):\n\"\"\"\nget_model"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn g_workspace_manager\n\n\n\n\ndef delete_file(file_path):\n\t\"\"\"\n\tDeletes the given file.\n\n\tParameters\n\t----------\n\tfile_path : str.\n\t\tThe path to the file.\n\t\"\"\"\n\tif(os.path.exists(file_path)):\n\t\tos.remove(file_path)\n\n\t\ndef get_file_system():\n\t\"\"\"\n\tGets the platform specific file system object.\n\n\tReturns\n\t-------\n\tobj\n\t\tAn object that implements the file system interface.\n\t\"\"\""}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.env.get_entity(cls.__ws_manager_cls, cls)\n\n\n@class_dispatch_\ndef get_workarea_manager():\n\"\"\"\n\tGets the workarea manager.\n\t\"\"\"\n\tpass\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tglobal _global_workspace_manager\n\tif not _global_workspace_manager:\n\t\treturn WorkspaceManager(cls())\n\treturn _global_workspace_manager\nWORKSPACE_MANAGER = workspace_manager()\n\ndef get_workspace(path):\n\t\"\"\"\n\tget_workspace(path) -> Workspace\n\tReturns the workspace with the specified path.\n\n\tIf no such workspace is found (or if a workspace with that path does\n\tnot exist), creates a new one.\n\t\"\"\"\n\treturn WORKSPACE_MANAGER"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn _get_api()[\"workspaces\"]\n\n\nclass WorkspaceManager:\n\t\"\"\"\n\tClass representing the workspace manager.\n\n\tParameters\n\t----------\n\tname: str\n\t\tName of the workspace manager.\n\tuser: str\n\t\tName of the user of the workspace manager.\n\t\"\"\"\n\n\tdef __init__(self, name: str, user: str, user_key: str = None, token: str = None):\n\t\tself.__api = _get_api().workspaces.get(\n\t\t\tname=name,\n\t\t\tuser=user,\n\t\t\t"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._plugins_manager\nplugins_manager = classmethod(plugins_manager)\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tplugins_manager = cls.manager.get_plugins_manager()\n\treturn plugins_manager\n\n\n\tdef __init__(self, server):\n\t\t\"\"\"\n\t\tClass constructor.\n\t\t:param int ID:\n\t\t:param int ServerID:\n\t\t:param bool PreInit:\n\t\t:param ServerManager server:\n\t\t:param client_manager.ClientManager client_manager:\n\t\t:param plugins_manager.plugins_manager plugins_manager:\n\t\t:param str prefix:\n\t\t\"\"\"\n\t\tself.ID = ID\n\t\tself.serverID = ServerID\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._plugins\n\t\n\nclass _PluginManager(object):\n\t\"\"\"\n\t\n\tClass which provides simple manager to plugin class objects.\n\t\n\t:members:\n\t\n\t:public members:\n\t\t\n\t.. warning::\n\t\n\t\tPlugin Manager requires that the plugin manager be initialized with the ``_plugins`` attribute equal to the PluginManager instance \n\t\"\"\"\n\t_plugins = {}\n\t\n\t_plugin_classes = {}\n\t\"\"\"\n\tDefines a mapping table between plugin type and plugin class\n\t\"\"\"\n\n\tdef __init__(self"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.plugin_manager\n\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._plugins_manager\n\nimport lib.pluginlib\nimport lib.plugin\n\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.__get_plugins_manager__()\n\n\n\n\n\ndef plugin_manager(cls):\n\"\"\"\n\tGet the plugin manager by given the plugin manager name, this is used for\n\tre-using the same manager if the plugin manager does not exists.\n\t\"\"\"\n\treturn cls.__get_plugin_manager__(name = cls.name(),\n\tclass_name = cls.class_name)\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.pluginManager\n\n\nclass Plugin(object, metaclass=ABCMeta):\n\t\"\"\"\n\tAbstract class that implement plugin.\n\t\"\"\"\n\tpluginId = None\n\tpluginName = None\n\n\t@abstractproperty\n\tdef name(self) -> str:\n\t\tpass\n\n\t@abstractproperty\n\tdef help(self) -> str:\n\t\tpass\n\n\t@abstractmethod\n\tdef configInfo(self) -> str:\n\t\tpass\n\n\t@abstractmethod\n\tdef init(self, gui: GUILib): pass\n\n\t@ab"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif cls._plugins_manager is not None:\n\t\treturn cls._plugins_manager\n\telse:\n\t\treturn g_pool.plugin_controller if g_pool else None\n\n\n@lru_cache\ndef plugins_loaded():\n\tglobal plugins_mgr\n\tfor plugin in plugins_mgr.plugins.values():\n\t\tif not plugin.loaded:\n\t\t\treturn False\n\treturn True\n\n\ndef plugin_path(plugin):\n\treturn plugin._settings.path\n\n\nasync def close_all():\n\tawait plugin_controller.close_all()"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.instance\n\n\n@lru_cache(None)\ndef instance(cls=__class__):\n\"\"\"\n\tInstance of the PluginManager.\n\t(singleton)\n\t\"\"\"\n\treturn cls()\n\n\ndef log(message, error=False, linebreak=False):\n\t# type: (object, object, object) -> Type__AnkisVerboseLogger\n\t# noqa: F821 # due to the above comment\n\t# noqa: F821\n\t\"\"\"\n\n\tLog message to the screen.\n\t\"\"\""}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn PluginManager(cls.plugins_dirpath, cls.plugins_names, cls.debug, cls.stderr)\n\n\t\n\n\n\n\n\n"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom.json_schema_validator import IRSchemaValidator\n\n\tif not spec_content:\n\t\tcls._logger.info('No content provided. Hence, validation is skipped.')\n\t\treturn {}\n\n\tif not isinstance(spec_content, YAML):\n\t\tcls._logger.error('Bad content provided as spec file. {}'.format(spec_content))\n\t\traise IRValidatorException('From spec content validation failed.')\n\n\tschema = {\n\t  \"$schema\": \"https://raw.githubusercontent.com/intel"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tcls._init()\n\t\tif not isinstance(spec_content, dict):\n\t\t\tspec_content = cls._open_yaml_file(cls.SPEC_FILE_NAME)\n\t\tfor field in cls._fields:\n\t\t\tif not cls.MANDATORY_FIELDS.__contains__(field):\n\t\t\t\tcontinue\n\t\t\tdata_field = \"data.\" + field\n\t\t\tif field in spec_content:\n\t\t\t\tif not isinstance(spec_content[field], list) and field not in cls.IGNORED_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t# Validate mandatory fields\n\tdata_dict = YamlLoader().loadYaml(spec_content)\n\tcls.validate_required_fields(data_dict)\n\n\t#  Validate all other fields as well\n\tcls.validate_remainder_data(data_dict)\n\n\treturn data_dict\n\ndef validation_report_details(filename):\n\t\"\"\"validation report details of filename\"\"\"\n\t\n\t# Get full path\n\tfile_path = os.path.abspath(file_location)\n\tfile_path, file"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\tdef _validate_from_content(spec_content, expected_type):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif spec_content:\n\t\t\t\tdata = getattr(yaml, expected_type)(\n\t\t\t\t\t\tspec_content,\n\t\t\t\t\t\tLoader=construct_yaml_loader(expected_type)\n\t\t\t\t\t)\n\t\t\t\treturn data\n\t\texcept yaml.YAMLError as yaml_err:\n\t\t\traise IRValidatorException(\"Invalid YAML content - {}\".format(\n\t\t\t\tyaml_err.__str__()\n\t\t\t))\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\treturn ValidateFromContent(spec_content).validate_from_content()\n\n\nif __name__ == \"__main__\":\n\tprint(__doc__)\n\t\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif(spec_content is None):\n\t\tspec_content=base_config.SCHEMAS.CONFIG_CATALOG_DATA['schema']\n\treq_items = _common.validate_from_content(spec_content=spec_content)\n\treturn req_items\n# #######################################################################\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise Exception(\"Error :: Spec file content cannot be empty\")\n\n\treturn cls.validate(fetch_schemas(), spec_content)\n\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\treturn {}\n\t\t\t\n\ttry:\n\t\tspec = yaml.safe_load(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException('Error parsing YAML file {}: {}'.format(spec_file_name, e))\n\n\treturn spec\n\n\ndef update_dict_with_required_fields(d, spec):\n\t\"\"\"Populate dict dictionary using dictionary specification\n\t\n\t:param d: Dictionary for updated\n\t:param spec: Dictionary to be used for update\n\t:return: Dictionary with"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\traise Exception(\"Validation error: file content not given\")\n\tif not cls._is_valid(spec_content):\n\t\traise Exception(\"Validation error: No schema found\")\n\n\tdata = {}\n\tif not cls._is_valid(spec_content, schema=cls._get_schema_from_validator(cls)):\n\t\traise Exception(\"Validator error: no schema provided for validation\")\n\t\t\n\t# Load YAML file\n\tdata = yaml.safe_load(open(cls._"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tvalidated_content = {}\n\ttry:\n\t\tif spec_content is None:\n\t\t\treturn validated_content\n\t\t_content = {}\n\t\tif not is_valid_yaml(spec_content, raise_error=True):\n\t\t\treturn validated_content\n\t\t_content = spec_content['spec']\n\t\tfor entry in _content:\n\t\t\tfor key in entry:\n\t\t\t\tif 'value' in entry:\n\t\t\t\t\tvalue = entry['value']\n\t\t\t\t\tif isinstance(value, str):\n\t\t\t\t\t\tvalue = value.upper()\n\t\t\t\t\tif not type(entry"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdef _validate_from_file(_cls):\n\t\tif yaml_file is None:\n\t\t\traise IRValidatorException(ValidationError.MISSING_FILE, f\"Missing file={_cls.FILE}\")\n\t\ttry:\n\t\t\twith open(yaml_file, 'r') as yaml_file:\n\t\t\t\tdata = yaml.safe_load(yaml_file.read())\n\t\texcept Exception as e:\n\t\t\traise IRValidatorException(ValidationError.READ_FILE_IO, f\"Reading file={_cls.FILE"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tif not yaml_file:\n\t\tfile_name = sys._getframe(0).f_code.co_name\n\t\traise IRValidationException('\"file\" data is required for \"%s\"' % file_name)\n\telif not yaml_file.is_file():\n\t\traise IRValidatorException('\"file\" data is required for \"%s\"' % get_function_name())\n\t\n\tvalid_data = None\n\twith open(yaml_file, 'r') as f:\n\t\tvalid_data = cls.yaml2"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = super().validate(yaml_file)\n\t\n\tif data:\n\t\tif data.get('from') is None:\n\t\t\traise IRValidatorException('Invalid YAML file - \"from\" is mandatory')\n\t\n\treturn data\n\n\n@classmethod\ndef validate_to_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t# validate that the YAML file is created correctly\n\tif not bool(yaml_file):\n\t\traise ValueError('YAML file is empty')\n\t\n\tif yaml_file.is_file():\n\t\tif not yaml_file.suffix == '.yml':\n\t\t\traise ValueError('Invalid Yaml file')\n\t\t\n\t\tLOGGER.info(f'Found a valid yaml file for the input path \"{yaml_file.absolute()}\"')\n\t\twith open(yaml_file, 'r') as f:\n\t\t\t#"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\n\t\tif yaml_file is None:\n\t\t\traise ValueError('YAML file not specified.')\n\n\t\tdoc_dict = cls.read_yaml_doc_as_dict(yaml_file)\n\t\tcls.validate_doc_dict(doc_dict)\n\t\treturn doc_dict\n\n\texcept (ValueError, KeyError) as e:\n\t\traise IRValidatorException(f'File \"{yaml_file.name}\" has formatting issues. {e}')\n\n\texcept IOError:\n\t\traise IRValidatorException(f"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n    with open(yaml_file) as f:\n    \tdata = yaml.safe_load(f)\n\t\t\n\treturn data\n\t\n# def write_repository_yaml(yaml_file):\n# \"\"\"\n# Write a given dictionary of data to YAML Repository file\n#\n# :param yaml_file: Path to YAML file\n# :raise IRValidatorException: if repo YAML file is empty\n# :return: None\n# \"\"\"\n#\n# def get_data_from_repo_file"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tif not yaml_file:\n\t\traise IRValidatorException(f'Invalid file path provided: {yaml_file}')\n\t\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.load(f, Loader=yaml.FullLoader)\n\texcept TypeError as err:\n\t\ttry:\n\t\t\tdata = yaml.load(yaml_file.read(), Loader=yaml.FullLoader)\n\t\texcept TypeError:\n\t\t\traise IRValidatorException(f'Invalid file"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise ValueError(_('YAML file path is required.'))\n\t\n\twith open(yaml_file) as f:\n\n\t\ttry:\n\t\t\tdata_file = yaml.safe_load(f)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise e\n\n\t# Check mandatory fields\n\tfor field, field_data in data_file.items():\n\t\tif not field_data:\n\t\t\traise Exception(_('Mandatory field %s is missing' % (field)))\n\n\t# Validate against"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\t\n\"\"\"\nThe below is the class method version to validate a python class\n\"\"\"\n\nclass PyClassValidate(yaml.loader.SafeLoader):\n\tdef __init__(self, stream, dict_constructor=dict):\n\t\tsuper(PyClassValidate, self).__init__(stream, dict_constructor)\n\t\t\t\n\t@classmethod\n\tdef validate_from_file(cls, yaml_file=None):\n\t\t\"\"\"\n\t\tGet a dictionary of a PyClass object with the validator defined\n\t\t\n\t\t:param yaml_"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tif not yaml_file:\n\t\tyaml_file = cls._base_yaml_file\n\t\n\ttry:\n\t\twith open(yaml_file) as f:\n\t\t\tfile_dict = yaml.safe_load(f)\n\texcept Exception as e:\n\t\tfile_dict = {}\n\t\tLogError.error(e)\n\t\t\t\n\tif not file_dict:\n\t\traise IRMappedException(\"No data/config file found\")\n\t\n\tmapping = cls.CONFIG_SECTION_MAPPING\n\t"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# A dictionary to hold out includes\n\t# {include_filepath: included_file_path}\n\n        # Dict to hold up include groups:\n        # {include_group_name: [include_files_path] }\n        include_groups = {}\n\n\tcurr_dict = self.data\n        for dict_path in parser_dict['include_groups']:\n                curr_dict = curr_dict[dict_path]\n\tif not isinstance(curr_dict, dict):\n\t\tparser_dict['include_groups'] = {}\n\t\t"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\t\n\tfrom.core import _parseDict\n\t\n\tinclude_groups = copy.deepcopy(_parseDict(parser_dict,[\"include_groups\", \"--include-groups\"]))\n\tif (include_groups and isinstance(include_groups,dict)): return include_groups\n\treturn None\n\t\ndef _add_include_groups(parser_dict):\n\t\"\"\"\n\tAdds include_groups directive to the specification file.\n\t\"\"\"\n\t\n\tfrom.core import _add_include_groups\n\t\n\t_add_include_groups"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tspec_parser =  parser.FileParser(parser_dict['SPEC_PATH'], \\\n\t\t\t\t    parser_dict['DEBUG'], \\\n\t\t\t\t    parser_dict['LOG_LEVEL'], \\\n\t\t\t\t    parser_dict['LOG_TYPES'])\n\n\tif parser_dict['IS_SUBSPEC'] is True:\n\t\tsubspec = subprocessor.Subprocessor(parser_dict)\n\n\t# Go through each group.\n\tfor group in parser_dict['GROUP_ITER']:\n\t\tgroup_path = f'{parser_dict[\"SPEC"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_group = parser_dict[\"include_group\"]\n\t\n\tif \"dependencies\" not in parser_dict:\n\t\tparser_dict[\"dependencies\"] = {}\n\t\n\tif not include_group in parser_dict[\"dependencies\"]:\n\t\tparser_dict[\"dependencies\"][include_group] = {}\n\t\t\n\tfor file in parser_dict[\"files\"]:\n\t\twith open(file, \"r\") as file:\n\t\t\tfile_contents = file.read()\n\t\t\tfor include in parser_dict[\"includes\"]:\n\t\t\t\tinclude_path ="}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif 'include' in parser_dict and parser_dict['include'] is not None:\n\t\tinclude = Path(parser_dict['include'])\n\t\tif not include.exists():\n\t\t\traise ValueError('include file does not exist!')\n\t\tinclude_dict = _load_json(include)\n\t\tfor key, value in include_dict.items():\n\t\t\tsetattr(self, key, value)\n\t\treturn\n\n\treturn\n\ndef _resolve_directives(input_dict, include):\n\t\"\"\"\n\tResolve the directives in the spec files. Returns"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif 'include' in parser_dict:\n\t\tincludes = parser_dict['include']\n\t\tincludes_list = []\n\t\tfor n, group in enumerate(self.file_groups):\n\t\t\tfor i, fg in enumerate(group):\n\t\t\t\tif 'include' in fg:\n\t\t\t\t\tincludes_list.append(fg['include'])\n\n\t\tfor i, include in enumerate(includes):\n\t\t\t# Only use the included file if it exists\n\t\t\tif include not in includes_list:\n\t\t\t\traise errors.SpecError('Include directive with name'+ include +"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfile_groups_to_include = []\n\tfor dir_name in self._get_matching_directories(parser_dict):\n\t\tfile_groups_to_include.append( os.path.join(dir_name, 'groups.ini') )\n\n\treturn Utils.process_file_groups(file_groups_to_include, parser_dict, parser_dict.groups) + \\\n\t\t\tUtils.process_dirs(parser_dict.includes, parser_dict, parser_dict.groups)\n\ndef _include_rules(self, parser_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t_path = os.path.join(self.directory, self.specs_from)\n\t_dir = os.path.dirname(_path)\n\t_files = os.listdir(_dir)\n\tfiles = []\n\tif any(f.endswith('.spec') for f in _files):\n\t\tfor _fname in _files:\n\t\t\tif _fname.endswith('.spec'):\n\t\t\t\tfiles.append(_fname)\n\t\t\t\tfiles.sort()\n\tif os.path.exists(os.path.join(_dir"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\timport json\n\t# Remove the \"include\" directive and it's arguments from\n\t# the specification\n\tfor key in list(parser_dict[\"spec\"]):\n\t\tif key == \"include\":\n\t\t\tdel parser_dict[\"spec\"][key]\n\t# Open the file and parse the include dict directive\n\twith open(os.path.join(self.spec_dir,parser_dict[\"spec\"],\n\t\tparser_dict[\"spec\"]+\".json\")) as ifile:\n\t\ttemp_dict = json.load(ifile)\n\t# Merge the include dict into"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif not \"include\" in list(parser_dict.keys()):\n\t\treturn None\n\t\n\tinclude_groups = [] #groups that need processing\n\tfor i in parser_dict[\"include\"]:\n\t\tinclude_groups += self._include_groups_of_directive(i)\n\t\n\tgroup_list = [] #The groups that need processing\n\tfor i in include_groups:\n\t\tif i in parser_dict.keys():\n\t\t\tgroup_list.append(parser_dict[i])\n\t\n\tif (len(group_list) == 0):"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tself.defaults = utils.spec_defaults(\n\t\tself.spec_dict, default_data=self.options.get('default-data'))\n\n\tif default_data:\n\t\tself.defaults.update({'default_data': default_data})\n\n\tself.defaults.update(utils.argspec_kw_to_dict(self.argspec))\n\n\treturn self.defaults\n\n\n# -----------------------------------------------\n#\n# -----------------------------------------------\ndef run(raw_options, config_file):\n\t\"\"\"\n\tRun a new task in the new"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn resolve_args(self.spec, self._definitions, **self.arguments)\n\nimport argparse\nimport copy\n\n\nclass CMake:\n\t\"\"\"Class describing a generic cmake build script.\"\"\"\n\tdef __init__(self, options=None, spec=None, args=None, **kwargs):\n\t\tself.options = options or {}\n\t\tself.spec = spec or {}\n\t\tself.args = args or {}\n\n\t\tfor k, v in kwargs.items():\n\t\t\tself.options[k] = v\n\n\tdef as_command"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tdefaults = super(Spec, self).get_spec_defaults()\n\tdefaults.update(self.get_spec_defaults_from_module(u_env_settings))\n\tdefaults.update(self.get_spec_defaults_from_spec(u_env_settings))\n\tdefaults[u'CMAKE_GENERATOR'] = u'Ninja'\n\treturn defaults\n\n\ndef cmake_module_options_generator(moduleName):\n\t\"\"\"\n\tGenerates a list of options for cmake modules that might be\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tself.resolve_arg_special()\n\tself.resolve_arg_spec()\n\tself.resolve_arg_env()\n\tself.resolve_arg_file()\n\tself.resolve_arg_dir()\n\"\"\"\n\tAdd to all arguments values from spec and other sources.\n\t\"\"\"\n\tself.parse_arg()\n# End of get_spec_defaults()\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn self.spec\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn {}\n\n\nclass Command(object):\n\t'''\n\tManage a call to a command.\n\n\tParameters\n\t----------\n\tcommand: the command to be executed\n\trun_env: The environment in which to set the options\n\targ_mapping_table: Dictionary specifying how to map arguments from\n\t   argv to environment variables\n\tdefaults: Dictionary specifying default options\n\toptions: Options for this command.\n\t'''\n\tdef __init__(self, command, run_env=None, arg_mapping_table=None,\n\t             default"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tres = {}\n\tif self.is_value_set(self.SCHEMA_DEFAULTS):\n\t\ttry:\n\t\t\tres.update(ast.literal_eval(self.get_value(self.SCHEMA_DEFAULTS)))\n\t\texcept:\n\t\t\tpass\n\n\tif self.is_value_set(self.SCHEMA):\n\t\ttry:\n\t\t\tres.update(ast.literal_eval(self.get_value(self.SCHEMA)))\n\t\texcept:\n\t\t\tpass\n\treturn res\n\ndef set_default_"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tfor spec in self._spec_defaults:\n\t\tif callable(spec):\n\t\t\t# Spec can contain function with parameters.\n\t\t\t# However, functions has access to any other\n\t\t\t# objects through the call stack, so we can't\n\t\t\t# modify it. \n\t\t\tdefault = spec()\n\t\telse:\n\t\t\tdefault = spec\n\t\tsetattr(self, spec.attr_name, default)\n\n\tif not hasattr(self, '_config_keys'):\n\t\t# Keys aren't set, so let's try to load from env variable\n\t\tself._config_"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tif _is_string(self.defaults):\n\t\tself.defaults = _parse_dict(self.defaults)\n\treturn self.defaults\n\n\ndef _parse_dict(s):\n\t\"\"\"\n\tResolve a dictionary from a strig and parse it.\n\t\"\"\"\n\tret = {}\n\tfor e in s.split():\n\t\tparts = e.split(\"=\", 1)\n\t\tif len(parts) == 1:\n\t\t\tif \" \" not in parts[0]:\n\t\t\t\tret[parts[0].lower()] = True\n\t\t\telse"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn self.args_spec_toplevel.get_spec_defaults()\n\n#------------------------------------\n#_Global_Configure_\ndef _Global_Configure(self):\n\t\"\"\"\n\n\t\"\"\"\n\t#TODO: Add default values from the spec.\n\t#Note that the function will be called only if no args have been passed to it\n\t\n\tif self.args_spec_toplevel.get_args()!= {}:\n\t\t# We have arguments, we need to get more information from them\t\n\t\tfor spec, default_value in self"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}\n\n\ndef options_flow(self, *info):\n\t\"\"\"\n\tCalled by options_parser for the flow widget.\n\t\"\"\"\n\tself.option_add(\n\t\t*info,\n\t\t**{\n\t\t\t'deep_nested': True,\n\t\t\t'nested': True,\n\t\t\t'nested_only': True\n\t\t}\n\t).update(\n\t\t{\n\t\t\t'default_deprecate': {\n\t\t\t\t'_from': '.options.defaults.',\n\t\t\t\t'deprecated': {\n\t\t\t\t\t'a.options."}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targs_to_remove = {}\n\tfor option in self.deprecated_options:\n\t\tcurr = self.get(option)\n\t\tprev = self.__getattribute__(option)\n\n\t\tif type(curr) == type(None):\n\t\t\tcontinue\n\n\t\tif isinstance(curr, tuple):\n\t\t\tcurr = list(curr)\n\n\t\tif curr == prev:\n\t\t\tcontinue\n\n\t\targs_to_remove[option.replace('_', '-')] = (prev, curr)\n\n\treturn args_to_remove\n\n\ndef check"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\td = {}\n\tfor i in self.__deprecated_options.keys():\n\t\td[i] = self.get_option(i)\n\tfor i in self.__other_options.keys():\n\t\tif i!= 'auto':\n\t\t\tfor e in self.__other_options[i].keys():\n\t\t\t\td[i] = e\n\t\telse:\n\t\t\tfor k in self.__other_options[i].keys():\n\t\t\t\tprint(k.replace(\"'\", \"\"))\n\t\t\t\tif k!= 'default':\n\t\t\t\t\td[i] = k\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targs_to_remove = {}\n\t# deprecated arguments\n\tfor action in self._actions:\n\t\targs_to_remove[action] = []\n\t\tfor arg in action.option_strings:\n\t\t\tif arg in self.deprecated_args:\n\t\t\t\targs_to_remove[action].append(arg)\n\tfor arg, keys in self.deprecated_args.items():\n\t\targs_to_remove[self._get_action(arg)].extend(keys)\n\treturn args_to_remove\n\n\ndef check_deprecated_args(self, args"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}\n\n\nclass IFileFilter(Interface):\n\n\ndef apply_filters(filenames, filters):\n\t\"\"\"Apply filters to the given filename, the arguments are pathnames\n\n\tfilters is a list object, each element may conains a 2-tuple, a filefilter\n\tand file parameter.  Each element may also contain a dictionary with its\n\town filefilter and/or file keyword.  When any dictionary is used, other\n\targuments take precedence. For example using a dictionary on\n\t\"/var/log/apt/history\" would use apt more than"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}\n\n\nclass Nested:\n\t_attrs = []\n\n\tdef __init__(self, cls, name):\n\t\tself.cls = cls\n\t\tself._name = name\n\n\tdef __setattr__(self, name, value):\n\t\tif name not in self._attrs:\n\t\t\traise TypeError(f\"Can't set attribute '{name}' on {self.cls.__name__}, it's an outer one\")\n\t\tobject.__setattr__(self, name, value)\n\n\nclass NestedMeta(type):"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdef convert_deprecated_args(parser, args=None, inputs_path=None, outputs_path=None):\n]\n\nargs = None\ninputs_path = \"inputs/path/to/inputs\"\noutputs_path = \"inputs/path/to/outputs\"\n\n# create args object and set other arguments\nargs = arg.Arguments()\nargs.add_args(train_args)\nargs.add_paths(train_paths)\nargs.add_paths(evaluate_paths)\nargs.add_path"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn self.__deprecated__\n\n\n# The below class is an example of MultiOption class.\nclass My_class:\n\t\"\"\"\n\t\tDescrition of My_class class.\n\t\t\"\"\"\n\tdef __init__(self, opt1, opt2, *args):\n\t\tself.opt1 = opt1\n\t\tself.opt2 = opt2\n\n\t# This function is used inside another method.\n\tdef multi_option(self, *args):\n\t\tmulti_opt = MultiOption(*args)\n\n\t\t## To create a list\n\t\tnew_list = multi_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {'verbose': ['e','s']}\n\n\n"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t# Print the missing answer file arguments\n\tif len(cli_args)!= len(answer_file_args):\n\t\tprint(\"\\n- Deprecated argument(s) provided for:\")\n\t\tfor value in answer_file_args.values():\n\t\t\tprint(\"\\t\", value)\n\t\t\tprint(\"- For support, please send all the arguments provided in a separate file: \\n\")\n\t\t\tprint(f\"{os.path.basename(__file__)} -f <{value}>\")\n\t\t\treturn\n\t\t\n\telse:\n\t\tprint(\"\\"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tis_valid = None\n\tdeprecated_args = {}\n\tfor key, value in cli_args.items():\n\t\tif key in answer_file_args.keys() and type(value) is int and value > 0:\n\t\t\tis_valid = False\n\t\t\tif key in deprecated_args.keys():\n\t\t\t\tdeprecated_args[key].append(value)\n\t\t\telse:\n\t\t\t\tdeprecated_args[key] = [value]\n\tif is_valid == None:\n\t\t# It should be False if there is a invalid argument, it shouldn't be True"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tfor arg in sys.argv:\n\t\tif arg in cli_args:\n\t\t\tprint('{} - This argument is no longer needed, please use {} instead. \\n '\n\t\t\t\t.format(arg, arg))\n\t\t\tsys.exit(-1)\n\ndef main():\n\n\tglobal args\n\n\tparser = argparse.ArgumentParser(description='QuestionAnswering')\n\tparser.add_argument('-v', '--verbose', action='store_true',\n\t\t\t\t\tdefault=False, help=\"Prints information messages or adds some new information messages\")\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tanswer_file_args = answer_file_args['args'] # dict\n\tfor deprecated_key in cli_args.keys():\n\t\tif deprecated_key in answer_file_args:\n\t\t\tprint(deprecated_key, 'has been deprecated by',\n\t\t\t'second argument. You can change it there.')\n\nclass validate_arg_file_name(ArgValidate):\n\t\"\"\"\n\tValidates file name.\n\t\n\t:param question: the question to ask.\n\t\"\"\"\n\n\tdef validate_file_name(self, question):\n\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tdeprecated_args = {\n        'username-arg': 'login-arg',\n        'password-arg': 'login-arg',\n        'tags-arg': 'tags-arg',\n\t\t'username-value': 'username',\n\t\t'password-value': 'password',\n\t\t'api-endpoint-value': 'host_discovery_service_endpoint',\n\t\t'tags-value': 'tags',\n\t\t}\n\t\n\tfor arg_key in deprecated_args:\n\t\tif cli_args[arg_key] and args[arg_"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\t\n\tif 'yes' in cli_args:\n\t\tprint('\\nThe following arguments are deprecated. Consider using command line arg instead. Use --help to see the '\n\t\t      'arguments and their alternatives.\\n')\n\t\t\n\t\tlist_args = []\n\t\t\n\t\tfor arg in cli_args:\n\t\t\tif arg in answer_file_args and cli_args[arg]!= answer_file_args[arg]:\n\t\t\t\tprint('--{0}={1}'.format(arg, answer_file_args[arg]))\n\t\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"\\n\")\n\tprint(\"The arguments used that are not used anymore:\\n\")\n\t\n\tdeprecated_args = []\n\t\n\tif cli_args.use_default_value!= None and cli_args.use_default_value == answer_file_args:\n\t\tdeprecated_args.append(\"use_default_value\")\n\tif cli_args.time_elapse!= None and cli_args.time_elapse == answer_file_args:\n\t\tdeprecated_args.append(\"time_elapse\")\n\tif"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef deprecation_check(cli_arg_name, answer_file_arg_name, question=\"Argument:\", answer_file_question=\"\"):\n\t\tif question in cli_args and cli_args[question] and not answer_file_args.get(answer_file_arg_name):\n\t\t\tprint('Using deprecated \"{}\" value. {}'.format(answer_file_arg_name, cli_args[question]))\n\"\"\"\n\tChecks if the answer_file has the required fields.\n\"\"\"\n\tdeprecation_check('databags"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\timport utils\n\timport pprint\n\tpp = pprint.PrettyPrinter(indent=4)\n\tmsg = 'Please make sure the arguments in {} do not change behavior outside or change their meaning.\\n',\n\targs =  ['--config-file']\n\n\n\targs_str = msg.format(utils.path_from_root(__file__) + '/../dependencies/arg_deprecation.py')\n\targs_args = utils.parse_args(args,cli_args)\n\tparser = utils.init_argparse"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated = False\n\t\n\tif len(cli_args) > 0:\n\t\tif len(cli_args) > 1:\n\t\t\tsys.exit(\"\\nInvalid --{0} parameter(s).\\nParameters must be given in one or the other.\".format(', '.join(cli_args)))\n\t\n\t\tparams = cli_args[0]\n\t\tparam = next(iter(params))\n\t\n\t\tif param in deprecated_arg_map:\n\t\t\tdeprecated = True\n\t\n\tif deprecated == True and len(cli_args) =="}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toptions,_ = self.parser.parse_known_args(add_help=False)\n\treturn list(dict(options._get_kwargs()).values())\n\n@overrides(Parser)\ndef convert_options_to_args(self, options,\n\t\t\targlist):\n\"\"\"\nConverts all the options to the value of the args (arguments)\nWill set up the default values for this parser (if it doesn't exist)\n\n:param options: the original args\n:param arglist: the list to which all the options will be added\n\n:"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\ttry:\n\t\tparser = self.command_to_parser[command_name]\n\texcept KeyError:\n\t\tparser = self.parser_d[command_name]\n\treturn _pjsua2.get_parser_option_specs(parser)\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toptions = list()\n\tcommand = '%s %s' % (command_name, self.command_template)\n\tcommand_template = '%s' % (self.command_template)\n\toption = \"%s%s.%s\" %(\"-\", \"--command-template\", command_template)\n\toptions.append(option)\n\toption = \"%s%s.%s\" %(\"-\", \"--command-name\", command_name)\n\toptions.append(option)\n\toption = \"%s%s.%s\" %(\"-\","}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tpass\n\treturn OptionList()\n\n\ndef get_command_option_specific_help(command_names, command_hint):\n\"\"\"\n\tGets the help information regarding the specific arguments given in the command_hint\n\t\n\t:param command_hint: the string that was used to enter the option - if possible\n\t\twe try to match the given command to a particular command\n\t\t(virsh, ln, create, etc) and return the help messages for that command\n\t:return: the help information for the specific command\n\t\"\"\"\n\tpass\n\treturn Help"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn super(Virsh, self).execute_command(\"virsh get-parser-options '%s'\" % command_name)\n\ndef get_parseroptions_from_spec(self, command_name, option_spec):\n\"\"\"\n\tExtracts the options from the option_spec from the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:param option_spec: the option spec\n\t:return: the list of all specified options\n\t\"\"\"\n\treturn super(Virsh,"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\t# command_name = \"virsh\"\n\tif command_name not in self.__options:\n\t\treturn None\n\tparser_spec = {\t\"main\": \"Virsh option to execute \", \n\t\t\t\t\t\t\"virt\": \"Virtual machine option to execute \",\n\t\t\t\t\t\t\"add\": \"Add option to add \", \n\t\t\t\t\t\t#\"delete\": \"Add option to delete \", \n\t\t\t\t\t\t\"delete\": \"Option can be deleted \",\n\t\t\t\t\t\t\"ls\": \"Option can be listed \", \n\t\t\t\t\t\t\"list\": \"Option can be listed \",\n\t\t\t\t\t\t\"remove\": \"Option deleted \","}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_command_opt_parser(command_name)\n\treturn parser.option_specs\n\n\ndef get_sub_command(parser, cmd_name, sub_command_name):\n\"\"\"\n\tGets the specified sub-command\n\t\n\t:param parser: the command's parser\n\t:param cmd_name: name of the command (eg. get, set, add etc...)\n\t:param sub_command_name: name of the sub-command (eg. create, start, stop)\n\t:return: the specified sub-command\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcommand_obj = self.parser.get_command_obj(command_name, argparse.ArgumentParser)\n\toption_list = []\n\tfor option in self.all_options[command_obj.object_hash][\"options\"]:\n\t\tif option[\"arg\"] not in option_list:\n\t\t\toption_list.append(\"--\"[1:]+option[\"arg\"])\n\treturn option_list\n\t\n\t\n\t\n\t\n\t\n\n\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tmethod = getattr(self, 'get_%s_option_specs' % (command_name, ))\n\toptions = method()\n\treturn options\n\ndef get_base_parser():\n\t\"\"\"\n\tGet parser for this action\n\n\t:return: the parser object with the command arguments or aliases and their help strings\n\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False)\n\tparser.add_argument('--help',\n\t\t\t\t\t\t action='store_true',\n\t\t\t\t\t\t help='Show this help message and exit')\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparsed_ret = {}\n\tall_options = self.parse(command_name)\n\toption_list = self.get_all_options(command_name)\n\t\n\tfor option in all_options:\n\t\tif option['name']!= 'help:':\n\t\t\tif option['arg'] == '-' or option['arg'] == '--':\n\t\t\t\targ_spec = self.argument_specification(command_name)\n\t\t\t\tif option['spec']['type'] == 'dict' and option['spec']['required'] == 'true':\n\t\t\t\t\t"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tcommand_name = self._hg_prefix + command_name + ':'\n\ttry:\n\t\treturn self.options[command_name][argument_name][1]\n\texcept KeyError:\n\t\traise error.UnknownIdentifier(self._repo, argument_name, command_name)\n\ndef _make_cmdoptions_parser(repo_ui, cmd):\n\t\"\"\"Build and return a parser for the given command.\n\n\tThis function exists to handle the case of adding and removing options. All\n\tother commands have the same.options variable. We have this here"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif (command_name, argument_name) in self.options_map_by_name:\n\t\treturn self.options_map_by_name[(command_name, argument_name)]\n\treturn None\n\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t#print command_name\n\tfor optionspec in self.optionspecs:\n\t\tif optionspec._long_opts[0][2] == argument_name:\n\t\t\treturn  optionspec\n\n\treturn (\"\", [\"--%s\" % argument_name])\n\n\ndef is_command_supported(self, command_name):\n\t\"\"\"\n\tReturns :\n\t\"\"\"\n\tfor cmdclass in self.commands:\n\t\tif cmdclass.command_name == command_name:\n\t\t\treturn True\n\treturn False\n\ndef get_module_completions(self"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tglobal OPTIONS\n\ttry:\n\t\treturn OPTIONS[command_name][argument_name]\n\texcept:\n\t\treturn None\n\tglobal NO_COLOR\n\tif NO_COLOR: print\nif __name__ == \"__main__\":\n\timport sys\n\timport getopt, time\n\n\tARGDICT = {}\n \ttry:\n\t\toptlist, list = getopt.getopt(sys.argv[1:], \"b:c:d:i:o:p:vhqVHqgk:s:l:u:r:"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tgetopt_spec = None\n\tget_option = getattr(self.subcommand, 'get_%s' % (command_name,))\n\tfor _opt in get_option():\n\t\tif _opt.dest == argument_name:\n\t\t\tgetopt_spec = _opt.getopt\n\treturn getopt_spec\n\n\ndef complete_subcommand(parser, all_args, subcommand_args):\n\t\"\"\"\n\tDetermines if there are valid subcommand arguments in the remainder.\n\t\"\"\"\n\tsubcommand = parser.prog\n\tfor arg"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tcommand = self.command_dict[command_name]\n\tif argument_name in command.arg_spec:\n\t\treturn command.get_argument_opt_spec(argument_name)\n\telse:\n\t\treturn None\n\n#-----------------------------------------------------------------------------\n\nclass CommandBase(_OptionMixin,\n                  _OptionGroupMixin,\n                  _HelpColoringMixin,\n                  cmdoptions.ParseArgs):\n\t\"\"\"\n\tBase class for commands.\n\t\n\tAn instance may only have one direct child.  Also each instance may only have\n\targuments as children,"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\ttry:\n\t\ttry:\n\t\t\treturn self.opt_mapping[command_name][argument_name]\n\t\texcept:\n\t\t\treturn self.opt_mapping[command_name].get(argument_name)\n\texcept:\n\t\tfor group_name, options in self.opt_mapping[command_name].items():\n\t\t\tif isinstance(options, dict):\n\t\t\t\tif argument_name in options:\n\t\t\t\t\treturn options[argument_name]\n\t\treturn None\n\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.get_command('get_option_spec', '0', command_name, argument_name)\n\n\ndef get_option_type(self, command_name, argument_name):\n\"\"\"\n\tGets the type of the specified option.\n\t\"\"\"\n\treturn self.get_command('get_option_type', '0', command_name, argument_name)\n\n\ndef get_option_description(self, command_name, argument_name):\n\"\"\"\n\tGets the description of the specified option.\n\t\"\"\"\n\treturn self.get_command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\ttry:\n\t\tprint(locals(), globals())\n\t\tprint(self.specification)\n\texcept:\n\t\treturn None\n\n\ttry:\n\t\treturn self.specification[command_name][argument_name].option_spec\n\texcept:\n\t\treturn None\n\n\ttry:\n\t\treturn self.specification[command_name][argument_name].type\n\texcept:\n\t\treturn None\n\n\treturn self.specification[name].type\n\n\nclass Test:\n\t\"\"\"\n\tA test script.\n\n\t\"\"\"\n\n\tdef __init__(self, input):"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tcommand_def = self.__get_defaults()\n\tif not self.has_command(command_name) or \\\n\t\tnot self._options.has(command_name):\n\t\treturn command_def\n\telse:\n\t\treturn self._options[command_name][argument_name][1]\n\n\noptions = Options()\n"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t\n\treturn ['--silent']\n\n\ndef get_p2(args):\n\"\"\"\n\tp2 parameter.\n\t\n\t:param args: The received arguments.\n\t:return: List, p2 parameter.\n\t\"\"\"\n\treturn args.p2\n\ndef get_p3(args):\n\"\"\"\n\tp3 parameter.\n\t\n\t:param args: The received arguments.\n\t:return: List, p3 parameter.\n\t\"\"\"\n\treturn args.p3\n\ndef get_p4(args):\n\"\"\"\n\tp4 parameter."}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif \"--yes\" in args or \"--no\" in args or \"--disable\" in args:\n\t\tif \"--yes\" in args:\n\t\t\tslienced_args = list(arg for arg in args if arg!= \"--yes\" and arg!= \"--no\" and arg!= \"--disable\")\n\t\t\treturn slienced_args\n\t\telif \"--no\" in args:\n\t\t\tslienced_args = list(arg for arg in args if arg!= \"--no\" and arg!= \"--disable\")\n\t\t\treturn slienced_args\n\t\telif \"--disable\""}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif 'loglevel' in args:\n\t\targs.remove('loglevel') \n\tsilent = args.pop(args.index('silent'))\n\treturn silent\n\ndef check_args_length(self,args):\n\tif len(args) < 6:\n\t\treturn False\n\targs = list(args)\n\treturn True\n\nif (__name__ == '__main__'):\n    print(sys.executable)\n    print(sys.version)\n    print(sys.maxsize)\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif args and isinstance(args, tuple):\n\t\tsilenced = silence(args)\n\t\treturn silenced\n\traise ValueError('Argument needs to be a tuple')\n\t\t\ndef silence(parameter):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced = []\n\tfor i in parameter:\n\t\tif isinstance(i,tuple) and any(map(lambda y:isinstance(y, str) and y, i)):\n\t\t\tsil"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn []\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_argumnets = []\n\treturn silenced_argumnets\n\t\ndef get_slience_kwarg(args):\n\"\"\"\n\tkeyword slience argument\n\t\n\t:param args: The received arguments.\n\t:return: The keyword argument that is scilenced.\n\t\"\"\"\n\treturn\n\t\n# def get_args(args):\n\"\"\"\n\tget all argument\n\t\n\tget all argument\n\t\n\t:param args: The received arguments.\n\t:return: all the argument.\n\t\"\"\"\n\targs.add_argument(\""}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tlist_of_args_silenced = []\n\n\tsilencic_args = self.silent_args['no']\n\n\tfor a in silencic_args:\n\t\tif args[a] is not None:\n\t\t\tlist_of_args_silenced.append(a)\n\n\treturn list_of_args_silenced\n\n\ndef get_kwargs(self, args):\n\"\"\"\n\nkwargs\n\n:param args: The received arguments.\n:return: dict, processed args.\n\"\"\"\n\tkwargs = {}\n\n\tfor k in args:\n\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn self.config.get(self.config_section).get('silent', {}).items()\n\t\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [arg for arg in args if '--silent' in arg]\n\n\ndef read_args_from_str(string):\n\t\"\"\"\n\tconvert str to dict\n\t\"\"\"\n\tstring_list = shlex.split(string) \n\treturn dict(zip(string_list[::2], string_list[1::2]))\n\n\ndef create_silent_args(args, arguments):\n\tnew_args = []\n\tif arguments.silent:\n\t\tfor val in arguments.silent:\n\t\t\tnew_args += list(filter("}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [ name for name in dir(args) if name in self._attrd_dict ]\n\"\"\"\n# END CLASS"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tcheck_non_missing_required_args = lambda *a, **z: \\\n\t    self.required_args is None or \\\n\t    (set(a[2::]) & set(self.required_args) == set(self.required_args))\n\treturn check_non_missing_required_args(*args)\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args.args)!= len(self.required_args):\n\t\tparser.error(\"\\033[91mRequired arguments not mentioned in command line.\\n\\033[0m\")\n\t\treturn False\n\treturn self.is_valid_type(args.args)\n\n\ndef validate_required_files(args):\n\"\"\"\n\tThe main function to validate required arguments if mentioned in\n\tcommand line. The files are searched by filename and directory name.\n\t\"\"\"\n\tif args.filelist and not get_files_matching_pattern(self, args.directory"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.check_required_arguments(args)\n\tif self.needs_args_without_flags:\n\t\tself.missing_args = True\n\tself.check_unknown_options(args)\n\tif self.needs_extra_args:\n\t\tself.has_invalid_args = True\n\t\tself.no_arg_string = '<unnamed>'\n\t\tself.extra_args = True\n\t\tfor flag in self.extra_args_allowed:\n\t\t\tif not getattr(args, flag, False):\n\t\t\t\tself.extra_args = False"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args: # if some args provided\n\t\tfor arg in args:\n\t\t\tif utils.is_vararg(arg): # if vararg\n\t\t\t\treturn True\n\t\treturn False\n\telse:\n\t\treturn True\n\ndef validate_args(args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif args: # if some args provided\n\t\tfor arg in args:\n\t\t\tif utils.is_vararg(arg) or utils.is_kwarg(arg): # if vararg or kwarg\n\t\t\t\treturn True\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\treturn args\n\"\"\"\n\tValidate if the number of arguments match.\n\t\"\"\"\ndef validate_size(self, args):\n\"\"\"\n\tValidate if the number of arguments match.\n\t\"\"\"\n\treturn args\n\"\"\"\n\tValidate if the number of arguments match.\n\t\"\"\"\ndef validate_add(self, args):\n\"\"\"\n\tValidate if the number of arguments match.\n\t\"\"\"\n\treturn args\n\"\"\"\n\tValidate if the number of arguments match.\n\t\"\"\"\ndef validate_add_all(self, args):\n\"\"\"\n\tIf the"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tp = self.parser\n\targ = args.requirement\n\n\tfor item in p.option_list:\n\t\tif hasattr(item, 'get_opt_string'):\n\t\t\tif item.dest == arg:\n\t\t\t\tif not getattr(args, arg):\n\t\t\t\t\tself.error('At least one'+ item.get_opt_string() +'must be provided')\n\t\t\telif getattr(args, item.dest):\n\t\t\t\tself.error('Only one of'+ item.get_opt_string() +'may be provided.')\n\nclass Val"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.required_args:\n\t\tif args.__contains__(arg)!= True:\n\t\t\tprint self.__name__,\" requires \",arg\n\t\t\traise Exception(self.__name__ + \" requires \" + arg)\n        \n\t\"\"\"\n\t# Ensure that the required arguments are present\n\tif (args.get('domain') == None ) or (args.get('endpoint') == None): \n\t\tprint \"At least, a domain and an endpoint must be specified\" \n\t\treturn False\n\t\n\t# Ensure that the required arguments are present\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args)!= 3:\n\t\traise UserError(\"Too few arguments\")\n\n\ndef get_vars_from_args(args, fields=[]):\n\"\"\"\n\tGet the values (or sub-values) of the arguments, after evaluating the attributes provided in the fields argument.\n\tReturn the list of values.\n\tfields: a list of fields to be evaluated\n\tIf fields is empty, all fields should be ignored.\n\t\"\"\"\n\tvalues = []\n\tif args is not None:\n\t\tfor field in fields:\n\t\t\tif field in args and args[field"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.arguments:\n\t\tif not getattr(args, arg, None):\n\t\t\tprint(f\"argument '{arg}' is required.\")\n\t\t\tsys.exit()\n\n\tif not all(x in args.dataset for x in self.arguments):\n\t\tprint(f\"invalid arguments: one or more arguments are missing:\\n {', '.join(self.arguments)}\")\n\t\tsys.exit()\n\n\ndef validate_format_args(self, args):\n\"\"\"\n\tCheck if a format has been provided. This should never happen,"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tprint(arg, 'is a required argument.')\n\t\t\tself.missing_args = True\n\t\t\treturn\n\t\n\ndef main(args):\n\"\"\"\n\tMain function.\n\t\"\"\"\n\ttry:\n\t\tparser = argparse.ArgumentParser(description='Arguments for building a graph from a file.')\n\t\tparser.add_argument('-v', '--verbose', type=int, nargs='?', const=True, default=False, help='turn verbosity on"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treq_args = []\n\tfor opt, opt_spec in options_spec.items():\n\t\tif opt in req_args:\n\t\t\tcontinue\n\t\tif opt_spec.get(\"required_when\") and \\\n\t\t\t\tnot self._is_opt_true(command_name, opt_spec, opt):\n\t\t\tcontinue\n\n\t\treq_args.append(opt)\n\n\treturn req_args\n\n\ndef _split_list_option(opt_str):\n\t\"\"\"\n\tSplit list options like --files from '--files file1 file2 fileA fileB... file1."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\treturn []\n\n\toptions = []\n\targuments = {\n\t\t\"<command_name>\": {\n\t\t\t\"arguments\": arguments,\n\t\t\t\"condition\": options_spec.get(\"required_when\"),\n\t\t},\n\t}\n\n\treturn options\n\n\ndef _get_user_inputs_from_spec(inputs_spec):\n\t\"\"\"\n\tGet user inputs from command spec.\n\n\t:param inputs_spec: the command spec.\n\t:return: dictionary, the key is a command name; val is list of arguments.\n\t\"\"\"\n\t\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs = self._get_input_args(command_name, args)\n\trequired_when = map(parse_required_when, options_spec[\"required_when\"])\n\targs_required_when = map(lambda v: v[\"arg_name\"], required_when)\n\treturn args, filter(lambda v: is_matched(v, args), args_required_when)\n\n\nclass CommandsMgr(commands.Commands):\n\n\tname = \"cli\"\n\n\tdef _process_command_line_options(self, command_line_options, args):"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tdef _get_unconditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``always_required`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``always_required``\n\t    condition\n\t\"\"\"\n\tfor arg in options_spec:\n\t\tif'required_when' in options_spec[arg]:\n\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\trequired = []\n\tfor op_name, _, condition in options_spec:\n\t\tif op_name.endswith(\"_or_\") and \"or\" in condition:\n\t\t\tfor _, _, required_and_invert in condition[\"or\"]:\n\t\t\t\tif required_and_invert:\n\t\t\t\t\trequired += [arg.name for arg in self._args if arg.name.endswith(op_name[:-4]) and not op_name[:-4] in required]\n\t\t\t\telse:\n\t\t\t\t\trequired += [arg.name for arg in self._"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\trequired_args = []\n\n\tif command_name in self.cli.all_commands:\n\t\tcommand = getattr(self.cli, command_name)\n\n\t\tif command.parser_options.required_when:\n\t\t\tfor required_option in command.parser_options.required_when:\n\t\t\t\tif required_option in options_spec:\n\t\t\t\t\trequired_args = required_args + options_spec.pop(required_option)\n\n\treturn required_args, options_spec\n\n\t\"\"\"\n\tReturns an iterator over `options spec` arguments.\n\t\n\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targ_names = []\n\targs_options = get_args_options(options_spec)\n\targs_flags = get_args_flags(options_spec)\n\tfor input_args in sys.argv[1:]:\n\t\ttokens = input_args.split()\n\t\tflag_position = None\n\t\tfor pos in range(0, len(tokens)):\n\t\t\tif tokens[pos] in args_flags:\n\t\t\t\tflag_position = pos;\n\t\t\t\tbreak\n\t\tposition_after_command = flag_position + 1\n\t\tfor flag_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs = options_spec\n\tfor required_option in self.required_when:\n\t\tif required_option.match(command_name, args.get('name')):\n\t\t\tif required_option.optional:\n\t\t\t\tif not args.get('optional_when') or required_option.match(command_name, args.get('optional_when')):\n\t\t\t\t\treturn copy.copy(required_option.list)\n\t\t\telse:\n\t\t\t\treturn copy.copy(required_option.list)\n\treturn []\n\n\t\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn get_conditionally_required_args(command_name, options_spec, args)\n\n\ndef get_conditionally_required_args(command_name, options_spec, args):\n\t\"\"\" Get all the required args in the command with `required_when` condition \n\t\tmatched.\n\t\n\t.. code-block:: python\n\t\n\t\toptions_spec = [\n\t\t\t{'name':'required_when', 'type': list},\n\t\t\t{'name': 'arguments', 'nargs': \"+\"}\n\t\t]\n\t\t\n\t\targs = '--foo"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif options_spec:\n\t\tcommand_options = [option_spec['opts'] + option_spec['args']\n\t\t\tfor option_spec in options_spec]\n\t\targs = options_spec\n\t\targ_names = get_arg_names(command_name, command_options)\n\t\t\n\t\tfor args_option, opt_arg in zip(command_options, arg_names):\n\t\t\tspec_opts = args_option[:opt_arg]\n\t\t\topts = {}\n\t\t\tfor item in args_option[opt_arg:]:\n\t\t\t\tkey, value"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) >= self.number_of_arguments:\n\t\ttry:\n\t\t\tlength = int(args[1])\n\t\t\tif length > len(args[0]):\n\t\t\t\traise ValueError(\"Tried to use a too long argument\")\n\t\texcept:\n\t\t\traise ValueError(\"Unexpected number of arguments\")\n\tfor arg in args:\n\t\tself.validate_argument_value(arg)\n\n@register_validation_scheme(\"array\")\nclass ArraysValidator(ArrayValidatorSchemeImpl):\n\tdef validate_length_args("}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) > self.length:\n\t\tself.fail(\n\t\t\targs = args,\n\t\t\tlength = self.length)\n\n\treturn args\n\n\ndef run_checks(self, args):\n\"\"\"\n\tRun all checks for this method.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args is None:\n\t\tself.fail(\n\t\t\targs = args,\n\t\t\tlength = self.length)\n\n\tif len(args) > self.length:\n\t\tself.fail(\n\t\t\targs = args,\n\t\t\tlength = self."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tlength = self.get_length()\n\tif len(args[\"args\"][\"value\"]) > length:\n\t\traise InvalidArgumentValueException(args[\"args\"][\"name\"], \\\n\t\t\"The value is longer than specified length (%d)\" % length)\ndef validate_length_args_default(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than default length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif \"value\" in args[\"args\"] and \"default\" in args[\"args\"]:\n\t\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself.args = args\n\tif len(args) > 1:\n\t\tself.message = 'ERROR: Too many arguments'\n\t\traise ValidationError(self.message)\n\n\nclass ValidationError(Exception):\n\t\"\"\"Raise error if validation failed\"\"\"\n\tdef __init__(self, message):\n\t\tself.message = message\n\ndef validate_string_args(self, args):\n\"\"\"\n\tCheck if command args is not string or is blank.\n\t\n\t:param args: The arguments that were sent to the command.\n\t\"\"\"\n\tself."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t# Check if value of argument is longer than permitted (length specified in argpase)\n\tif len(args) > self.length:\n\t\tprint(\"Error: value of arguments is longer than permitted (length specified in argument).\")\n\t\texit(1)\n\"\"\"\n\tCheck if value of arguments is not shorter than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tif len(args) < self.length:\n\t\tprint(\"Error: value of arguments is shorter than permitted (length specified in argument).\")\n\t\texit(1"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif (args!= None and len(args) > 0):\n\t\tif (args[0] > (self.length - 1) or args[0] < 0):\n\t\t\traise ValueError(\"Argument is longer than expected\")\n\"\"\"\n\tFunction that uses the 'generate_number' function to validate the\n\treferred list of arguments\n\t:param args:\n\"\"\"\ndef validate_length_generate_number_args(self, args):\n\"\"\"\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor (ind, val) in enumerate(args):\n\t\twhile (args[ind] > (args[ind + 1] - 20)):\n\t\t\targs[ind] = args[ind + 1]\n\n\n\n\t\n\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n\ndef do_action(self, args):\n\"\"\"\n\tExecutes a request according to its kind.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treserved_length=args.get('_length', 100000)\n\tif args['_value'] > reserved_length:\n\t\t\targs['_error']=_err_value_length_default\n\t\t\treturn False\n\treturn True\n\n\ndef create(connection, module):\n\"\"\"\n\tControl function of integration.\n\n\t:param connection: Connection to remote host.\n\t:param module: Ansible module.\n\t\"\"\"\n\twarnings = list()\n\treserved_length=module.params.get('_length', 100000)\n\tif(module.params."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor attr in [\"len_value\", \"len_offset\", \"value_offset\", \"len_total\"]:\n\t\tattr_value = getattr(args, attr)\n\t\tif attr_value > getattr(self.info.frozen_section, attr):\n\t\t\tself.fail()\n\nclass validate_length_offset:\n\"\"\"\"\n\tCheck if offset of value of arguments is not longer than specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.info = Info()\n\n"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn not not(\n\t\targs[\"choice_fields\"] or\n\t\targs[\"choice_values\"] or\n\t\targs[\"text_choices\"] or\n\t\targs[\"boolean_choices\"] or\n\t\targs[\"date_choices\"]\n\t)\n\"\"\"\n\tCheck if value of radio arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\ndef validate_radio_args(self, args):\n\"\"\"\n\tCheck if value of radio arguments is one of the available choices.\n\t\n\t:param args: The received arguments."}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif (\n\t\targs is not None and\n\t\t\"choice\" in args\n\t):\n\t\tif (\n\t\t\tisinstance(args[\"choice\"], list)\n\t\t\tor\n\t\t\tisinstance(args[\"choice\"], tuple)\n\t\t):\n\t\t\tchoices = args[\"choice\"]\n\t\telif (\n\t\t\tisinstance(args[\"choice\"], str)\n\t\t):\n\t\t\tchoices = [\n\t\t\t\targs[\"choice\"],\n\t\t\t]\n\t\telse:\n\t\t\traise ValueError(\"type of 'args[choice]' must be a list or a string\")\n\t\targs[\"choice\"] = None\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\t#print('Arguemto ', arg)\n\t\tif arg == 'choices':\n\t\t\tcontinue\n\t\tif hasattr(args[arg], '__iter__') and args.get(arg) not in choices:\n\t\t\traise ValueError('Value of {} must be one of {}'.format(arg, choices))\n\ndef validate_choices_kwargs(kwargs):\n\"\"\"\n\tCheck if argument of choice are one of the available choices.\n\t\n\t:param kwargs: The received keyword arguments.\n\t\"\"\"\n\tfor k"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = ''\n\tfor arg in args:\n\t\tif arg[1] == 'None': choices = arg[0]\n\t\t\t\t\n\tif not choices == '':\n\t\tif not options.choices == choices:\n\t\t\tmessage = ['Invalid choice:',\n\t\t\t'  Valid Choices: %s (%s)' % (options.arg2str(choices), options.arg2str(options.choices)),\n\t\t\t'  Given Arguments (%s)' % (options.arg2str(args))]\n\t\t\treturn message\n\treturn args\n\t\n\t\n#"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif(not self.get_validator(self.CONFIG_CHOICE, args={\n\t\tself.CONFIG_KEY: True,\n\t\tself.CONFIG_VALUE_KEY: self.CONFIG_CHOICES,\n\t\tself.CONFIG_ARG_KEY: True,\n\t\tself.CONFIG_ARG_NAME_KEY: True,\n\t\t})):\n\t\t\n\t\traise ValueError(u'{name} value must be in {choices}'.format(\n\t\t\tname=self.get_display_name().lower(),\n\t\t\t\tchoices=self"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn True\n\ndef validate_choices_kwargs(self, name, value, error_message='invalid choice', choices=()):\n\"\"\"\n\tCheck if value is one of the valid choice.\n\t\n\t:param name: The name of the argument.\n\t:param value: The recieved value.\n\t:param error_message: The error message to be displayed for errors.\n\t:param choices: The argument choices.\n\t\n\t:return: The validated value.\n\"\"\"\n\treturn value\n\ndef validate_columns(self, args):\n\"\"\""}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself.required = args.get(ARGUMENT_CHOICE_ARG_REQUIRED, False)\n\tif self.required and 1 <= len(args[ARGUMENT_CHOICE_ARG_CANDIDATE]):\n\t\traise ArgumentException('Not enough arguments')\n\n\n\tif not self.required and len(args[ARGUMENT_CHOICE_ARG_CANDIDATE])!= self.choices_count():\n\t\traise ArgumentException('Not enough options')\n\n\n\targs_set = set(args.get(ARGUM"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoice_args = self.get_args([\"choice\"])\n\tif(choice_args is None or choice_args == []):\n\t\treturn\n\tchoice_args = choice_args[0]\n\t# args are strings in the form: {choice}={index}\n\t# we need to split them by '=' and strip whitespace\n\tchoice_args = choice_args.split(\"=\")\n\tchoice = choice_args[0]\n\tchoice = choice.strip()\n\tif(choice not in self.choices):\n\t\traise InputError(\"Invalid choice"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise argparse.ArgumentTypeError(\"%s is not a valid choice\" % (arg))\n\nclass List(ListType, object):\n\"\"\"\nList is a special type of argument that accepts a list of items (option arguments)\nbut lets you use it in a command as if it were a single argument (e.g.\n[\"file1.yaml\", \"file2.yaml\"]).\n\nExample:\n\t@parameters(\n\t\tList(\n\t\t\thelp=\"List of files to"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif hasattr(self, '_choices_args'):\n\t\tif None in args.values():\n\t\t\traise Error(\"Choices are required.\")\n\tchoices_list = self.validate_args()\n\tif not choices_list:\n\t\t# If no choices, do not require them.\n\t\treturn\n\t\t\n\tself.choices_validate_args(choices_list)\n\n"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n\"\"\"\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_args = self._min_args_type\n\tmax_args = self._max_args_type\n\tif isinstance(min_args, range): # Range arguments\n\t\tfor argvalue in args:\n\t\t\tif argvalue < min_args.start or argvalue > min_args.stop:\n\t\t\t\tself._message_func(\"Not in range: {}\".format(argvalue))\n\t\t\t\treturn False\n\t\treturn True\n\telif isinstance(max_args, range): # Range arguments\n\t\tfor argvalue in args:\n\t\t\tif argvalue < max_args"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif (\"min\" in args and not is_number(args['min'])) or (\"max\" in args and not is_number(args['max'])):\n\t\traise InvalidArgumentsException('Argument max or min not number!')\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_val = self.__options.min\n\tdef max_val(val):\n\t\treturn val == None or val >= min_val\n\ttry:\n\t\tif not all([self.__check(item, max_val) for item in args.values()]):\n\t\t\tfor key in args.keys():\n\t\t\t\tinvalid_argument = True\n\t\t\t\tvalue = args[key]\n\t\t\t\tif value == None:\n\t\t\t\t\targs[key] = None\n\t\t\t\telse:\n\t\t\t\t\tinvalid_argument = False\n\t\t\t\tinvalid_range = False\n\t\t\t\ttry:\n\t\t\t\t\tassert"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tminValue = args.get('minValue', -99999.0)\n\tmaxValue = args.get('maxValue', 99999.0)\n\n\tif maxValue < minValue:\n\t\treturn 'Maximum value (%f) must be greater than or equal to minimum' %maxValue\n\n############################################################\ndef validate_georaster_raster(self, args):\n\"\"\"\n\tValidate the minimum required values for georaster raster type.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif 'Raster' in args:\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor i in range(len(args)):\n\t\tif ((args[i] < self.args_min[i]) | (args[i] > self.args_max[i])):\n\t\t\tif args[i] > self.args_max[i]:\n\t\t\t\tprint('Error! Argument {} must between {} and {}'.\n\t\t\t\tformat(args[i], args[i] - self.args_max[i],\n\t\t\t\t       self.args_max[i]))\n\t\t\telif args[i] < self.args_min[i]:\n\t\t\t\tprint('Error"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_ = self.get_min_value()\n\tmax_ = self.get_max_value()\n\t\n\tfor i in range(args.__len__()):\n\t\targs[i] = max(args[i], min_[i])\n\t\targs[i] = min(args[i], max_[i])\n\t\t\n\t\t\n\t\nclass CheckNumericalArguments(ValidateArgumentsBase):\n\t\"\"\"\n\tCheckNumericalArguments class represents check\n\targuments for numerical functions.\n\t\n\tThis class uses :class"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.min is not None and args.max is not None and args.max >= args.min:\n\t\treturn args\n\tif args.only_min is not None and args.max is None:\n\t\tif args.min is None and args.only_min < 0:\n\t\t\targs.min = args.only_min\n\t\treturn args\n\tif args.only_max is not None and args.min is None:\n\t\tif args.max is None and args.only_max > 0:\n\t\t\targs.max = args.only_max\n\t\treturn args\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn\n\n# <Autogenerated>\nautogen_functions = set()\n# </Autogenerated>\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_value = args.get(\"min_value\")\n\tmax_value = args.get(\"max_value\")\n\tif min_value is not None:\n\t\tif (min_value > max_value):\n\t\t\treturn JsonResponse(\n\t\t\t\t{\"success\": False, \"message\": \"The maximum value is less than the minimum value.\"},\n\t\t\t\tstatus=400\n\t\t\t)\n\t\tvalue = args.get('value')\n\t\tif (value < min_value):\n\t\t\treturn JsonResponse(\n\t\t\t\t{\"success\": False, \"message\": \"The minimum value is greater"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tfrom ctypes import c_long\n\tfrom ctypes import c_double\n\tfrom ctypes import c_char_p\n\tfrom ctypes import c_short\n\tfrom ctypes import c_longlong\n\tfrom ctypes import Structure\n\tfrom ctypes import CFUNCTYPE\n\tfrom ctypes import c_int\n\tfrom ctypes import c_uint \n\tfrom ctypes import CDLL\n\tfrom ctypes import c_long\n\tfrom ctypes import byref\n\tfrom ctypes import pointer\n\tfrom ctypes import POINTER\n\tfrom ctypes"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tComplexArgument = namedtuple('ComplexArgument', option_name)\n\tspec = get_type(type_name)\n\targuments = get_argument(option_name, spec)\n\tcomplex = self.build_complex_argumet(subcommand, option_name)\n\targument = ComplexArgument(**comand_argument_dict[subcommand][option_name])\n\tcomplex.__setattr__(option_name, argument)\n\tcomplex.__setattr__(type_name, options[option_name](spec=arguments))"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t# generate a complex type\n\tcomplex_type = create_complex_type_instance()\n\t# build the option argument\n\tsimple_argument = self.create_simple_argument_type(option_name, spec_option)\n\tcomplex_argument = complex_type.add_argument(simple_argument)\n\n\treturn complex_argument\n\n\n\tdef create_single_argumet_type(self, subcommand, type_name, option_name, **kwargs):\n\t\"\"\"\n\tBuild the simple argument type\n\t\n\t:param subcommand: the command name\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\treturn ComplexArgumentType(subcommand, type_name, option_name, spec_option)\n\t\t\n\t\t\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n   type_builder = getattr(self,subcommand)\n   return type_builder(**spec_option)\n\n\n  def new_arg_type(self):\n   \"\"\"\n\t\treturn a new argument type instance\n   \"\"\"\n   name = self.arg_formats['name'].format(self.formatter_name)\n   type_name=self.arg_formats['type'].format(self.formatter_name)\n   default_str='\"{}\"'.format(self.formatter_default)\n   option_spec={ 'namespace"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\tif option_name is None:\n\t\treturn\n\t\n\targuments = self.create_simple_option_type(type_name, option_name, spec_option)\n\tif arguments is None:\n\t\treturn\n\t\n\tfor i in xrange(len(arguments)):\n\t\tif arguments[i] is None:\n\t\t\tcontinue\n\t\t\n\t\tfor j in xrange(len(arguments[i])):\n\t\t\tself.parse_option(subcommand, arguments[i][j], j, i, True)\n\ndef create_enum_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\toption_type = type_name\n\n\t# if the type_name is a complex type, create an instance of it\n\t# call super method to add the option to the list of the type\n\n\t# check if the argument type exists.\n\t# if type exists, do not create it.\n\t# otherwise, create one.\n\tif option_type not in self.args_complex_types.keys():\n\n\t\t# create option with type name\n\t\tself.args_complex_types[type_name] = type(type_name, (ComplexOptionArgument,),"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n\tc_type = CCommandArgumentType({ \n\t\tsubcommand:ComplexType(type_name,option_name,**spec_option)\n\t})\n\treturn (subcommand,c_type)\n\n\ndef create_parameter_object(command_spec, cmd_name, arg_idx, arg_name, default_value=False):\n\t\"\"\"\n\tcreate an argument object\n\t\n\t:param command_spec: the command specification\n\t:param c_command: the command name\n\t:param arg_idx:\n\t:param"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\treturn ComplexType({\n\t\t\"$ref\": f\"#/definitions/{type_name}\"\n\t}),\n\t\n\t\n\t\n\n\t\t\t\n\t\t\t\n\t\n\t\n\n\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tsubcommand = subcommand + \":\" + option_name\n\treturn ComplexTypeParser(subcommand, type_name, self.__complex_args(), spec_option)\n\t\n\n"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcustom_vars = _get_cmd_line_variables(args.cmd_line_file)\n\tcustom_vars.update(args.vars) if args.vars else {}\n\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args.control:\n\t\ttry:\n\t\t\t# split into control arguments\n\t\t\tcontrol_args[arg.name] = arg.default\n\t\texcept AttributeError:\n\t\t\tif arg.name in custom_vars:\n\t\t\t\tnested_args[arg.name] = custom_vars"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args, control_args = [], []\n\tcontrol_vars = {}\n\tfor arg in args:\n\t\tif is_var(arg):\n\t\t\targ = get_var(arg) or arg\n\t\tif is_control_var(arg):\n\t\t\tcontrol_vars[arg] = True\n\t\telif is_nested(arg):\n\t\t\tnested_args.append(arg)\n\t\telse:\n\t\t\tcontrol_args.append(arg)\n\tcontrol_args = dict((arg, True) for arg in control_args)\n\tcontrol"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\n\t\tif arg.startswith('control:'):\n\t\t\t_, custom_variables = arg.split(':', maxsplit=1)\n\t\t\tcustom_args.update(custom_variables)\n\t\telif arg in DEFAULT_NESTED:\n\t\t\tnested_args[arg] = True\n\t\telse:\n\t\t\tcontrol_args[arg] = True\n\treturn control_args, nested_args, custom_args\n\n\ndef get"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif not arg:\n\t\t\tcontinue\n\t\tif '=' not in arg:\n\t\t\traise AnsibleError('control/command value \"%s\" doesn\\'t contain \"=\"' % arg)\n\t\t(key, val) = arg.split('=', 1)\n\t\tif key in ('no_log', 'become', 'become_method', 'become_user', 'become_flags') or key.startswith('no_log_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs = ansible_args_to_dict(args)\n\n\treturn default_control_args(args), custom_nested_args(args)\n\n\ndef get_flat_args(args):\n\t\n\targs = get_nested_custom_and_control_args(args)\n\targs = flatten(args)\n\t\n\tflat_args = default_flat_args(args)\n\tflat_args.update(custom_flat_args(args))\n\t\n\treturn flat_args\n\n\n\n@main.command()\ndef version():\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\t# Loop through and split the args in two dicts\n\t# Each item is a key value pair\n\tfor item in args:\n\t\tif '__ANSIBLE_UNDER_CONDUCT__' in item:\n\t\t\tnested_args[item.split('__ANSIBLE_UNDER_CONDUCT__')[-1]] = item.split('__ANSIBLE_UNDER_CONDUCT__')[0]\n\t\telse:\n\t\t\tcontrol_args[item] = {}\n\treturn control_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested = {}\n\tflattend = {}\n\targs = map(lambda x: str(x), args)\n\t# Filter control_args and flatten dict\n\tcontrol_args, *flatten = next(\n\t\tfilter(lambda x: x in self.CONTROL_ARGS, args), None)\n\tflatten = list(flatten)\n\tcontrol_args = sorted(control_args.split(\",\"))\n\tcontrol_args = list(map(lambda x: x.split(\":\"), control_args))\n\tfor k, *v in"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\targs = dict(args)\n\tfor arg in args.keys():\n\t\tif arg not in self.arguments:\n\t\t\tif arg!= self.spec_id and arg!= \"user\":\n\t\t\t\traise AnsibleOptionsError(\"Unknown argument defined: '%s'\" % arg)\n\t\t\tcontinue\n\t\t# Get the nested/custom argument\n\t\tif args[arg] is None:\n\t\t\tcontinue\n\t\tvalue = args[arg]\n\t\tif type(value) is dict or (type(value) is list and all"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tresult = {\"control_args\": {}, \"nested_args\": {}}\n\n\t# flat controls args dictionary\n\tcontrol_arg = {}\n\tcontrol_arg_namespace = \"CONTROL_ARGS\"\n\tfor arg in args.I(config.get_arg_namespace(config.CONTROL_NAMESPACE)):\n\t\tcontrol_arg[arg] = args.I(arg)\n\tresult[\"control_args\"][control_arg_namespace] = control_arg\n\n\t# parse nested args\n\tflat_namespace = \"N"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcustom_args = {}\n\tcontrol_args = {}\n\tfor item in args:\n\t\tif \"=\" in item:\n\t\t\t(nested_item, arg) = item.split(\"=\", 1)\n\t\t\tif arg.startswith(\"@\"):\n\t\t\t\tcustom_args[nested_item] = os.path.basename(arg[1:])\n\t\t\telse:\n\t\t\t\tcontrol_args[nested_item] = arg\n\t\telse:\n\t\t\tcustom_args[nested_item] = item\n\t\t\n\t# make sure control_args is a"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn\n\n\tfor extra_var_set in extra_vars:\n\t\tfor key, value in extra_var_set.items():\n\t\t\tif key in vars_dict.keys():\n\t\t\t\tif not isinstance(vars_dict[key], list):\n\t\t\t\t\tvars_dict[key] = [vars_dict[key]]\n\n\t\t\t\tvars_dict[key].extend(value)\n\t\t\telse:\n\t\t\t\tvars_dict[key] = value\n\ndef filter_values(options, valid_values):\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor extra_var in extra_vars:\n\t\tif extra_var in vars_dict:\n\t\t\tif isinstance(vars_dict[extra_var], list) and isinstance(extra_var, list):\n\t\t\t\t# merge multiple lists into one\n\t\t\t\tvars_dict[extra_var].extend(extra_var)\n\t\t\telse:\n\t\t\t\tvars_dict.update([extra_var,])\n\n\nclass EnvironmentDefinition(object):\n\t\"\"\"\n\tClass that defines an environment with variables:\n\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars == None:\n\t\textra_vars = {}\n\treturn_vars = {}\n\tfor extra in extra_vars:\n\t\tif extra not in return_vars:\n\t\t\treturn_vars[extra] = extra_vars[extra]\n\treturn_vars.update(vars_dict)\n\tif return_vars!= {}:\n\t\tvars_dict = return_vars\n\treturn(vars_dict)\n\n## merge_extra_vars\n\n\t\t\ndef merge_overrides(in_over"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif vars_dict is None:\n\t\treturn\n\n\tif extra_vars is None:\n\t\treturn\n\n\tfor var_name in set(extra_vars):\n\t\tvar_value = extra_vars[var_name]\n\t\tif 'vars' not in vars_dict:\n\t\t\tvars_dict['vars'] = {}\n\t\tif var_name not in vars_dict['vars']:\n\t\t\tvars_dict['vars'][var_name] = var_value\n\t\telse:\n\t\t\tvars_dict['vars"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars == None: extra_vars = ['vars', 'cmd', 'args']\n\t\n\tfor i in extra_vars:\n\t\t#if i == vars_dict.has_key('vars'):\n\t\tif i in vars_dict.keys(): # TODO: Shouldn't vars be a default? (v2.0)\n\t\t\tvars_dict[i].append(' '.join([x.strip() for x in extra_vars]))\n\t\t\n\t\telse:\n\t\t\tvars_dict[i"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\n\n\ndef merge_ymls(yml_list, yml_dict, merge_list):\n\t\"\"\"\n\t:param yml_list/dict: YAML file(s) to merge\n\t:param yml_dict: YAML file that we wish to\n\t\treplace or update into the list of files\n\t:param merge_list: YAML files to merge into\n\t\"\"\"\n\tfor yml in yml_list + [yml_dict]:"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars == None: extra_vars = {}\n\tfor var_name, var in extra_vars.items():\n\t\tif var_name not in vars_dict: vars_dict[var_name] = var\n\t\telse: pass\n\ndef list_to_dict_str(vars_list):\n\"\"\"\n\tMerge extra-vars and vars (merge_extra_vars, merge_vars)\n\t\n\t:param vars_list: List of variables\n\t\"\"\"\n\t\n\tvars_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfrom ansible.module_utils.ec2 import get_aws_connection_info\n\tif vars_dict['ec2_instance_id'] is None:\n\t\tinstance_id = os.environ.get('EC2_INSTANCE_ID')\n\t\tif instance_id is None and extra_vars is not None:\n\t\t\tfor ev in extra_vars:\n\t\t\t\tif ev.get('tags')!= None and 'aws_ec2_instance_id' in ev['tags'] :\n\t\t\t\t\tinstance_id = ev['tags']['aws_ec2"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra = extra_vars or []\n\tfor item in extra:\n\t\tif item in vars_dict:\n\t\t\tpass\n\t\telse:\n\t\t\tvars_dict.update({item:get_dict_val(item)})\n\t\n\treturn vars_dict\n\ndef replace_value(vars_value_dict, value_to_replace=\"${name}\"):\n\"\"\"\n\tReplaces ${name} in ``vars_file`` key with ``replace_value``\n\t\n\t:param vars_value_dict: Value for replacing\n\t:"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict['extra-vars'] = merge_strings(vars_dict.get('extra-vars', ''), extra_vars or [])\n\ndef merge_env_vars(vars_dict, path='', extra_vars=None):\n\t\"\"\"\n\tMerge extra-vars into environment variables\n\n\t:param vars_dict: Dictionary to merge environment-vars into\n\t:param path: Path to look for environment variables\n\t:param extra_vars: List of environment variables to look for\n\t\"\"\"\n\textra_v"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t# create the environment to execute the playbook\n\tprint_out_str(\"Calling 'ansible-playbook' to execute the playbook\")\n\n\tansible_parser = MyAnsibleParser(\n\t\t{\n\t\t   'verbosity': 2,\n\t\t})\n\tansible_parser.set_workspace(ir_workspace)\n\tansible_parser.set_plugin(ir_plugin)\n\n\t# set some default args\n\tif ansible_args is None:\n\t\tansible_args = {\n\t\t    'inventory': ['/etc/ansible"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tself.ir_workspace = kwargs.get('ir_workspace', None)\n\t\tself.ir_plugin = kwargs.get('ir_plugin', None)\n\t\tself.extra_vars = kwargs.get('extra_vars', {})\n\t\tself.ansible_args = kwargs.get('ansible_args',{})\n\t\tself.verbose = kwargs.get('verbose', None)\n\t\tself.ansible = AnscrCommand(\"ans"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\tif verbose:\n\t\tverbosity = \"--quiet\"\n\telse:\n\t\tverbosity = \"--no-pty --no-banner\"\n\tcmd = ir_plugin.ansible_bin + \":playbooks\" + \" \" + \\\n\t\t\"%s %s\" % (verbosity,playbook_path)\n\tif not extra_vars:\n\t    extra_vars = {}\n\tif not ansible_args:\n\t    ansible_args = {}\n\tfor key, value in extra_vars.items():\n\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = {}\n\t\t\n\tif verbose is None:\n\t\tverbose = -10\n\textra_vars.update({\n\t\t'ANSIBLE_REMOTE_TEMP_DIR': ir_plugin.remote_tmp_path,\n\t\t'ANSIBLE_KEEP_REMOTE_FILES': 'True'\n\t})\n\tprint('PLUGIN')\n\tif verbose >= 20:\n\t\textra_vars.update({\n\t\t\t'ANSIBLE_VERBOSITY': verbose\n\t\t})"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tfrom ansible.errors import AnsibleError\n\tfrom ansible.utils.display import Display\n\n\tinfra = Infra(\n\t\tir_workspace, \n\t\tir_plugin\n\t)\n\tif verbose is None:\n\t\tverbose = infra.get_variable(\"-vvv\")\n\tp = PlayBook(\n\tir_workspace,\n\tinfra,\n\tplaybook_path=playbook_path,\n\tplaybook_module_args=\"%s -vvv -o 'vars_files ='-vvv\"' % playbook"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\tfor v in (('extra-vars' not in ansible_args), \n\t\t\t('ansible-args', dict() if isinstance(ansible_args, str) else ansible_args)):\n\t\tif not v[0]:\n\t\t\tlog.error(\"The provided Ansible arguments cannot \"\n\t\t\t\t\"be used unless extra-vars are supplied.\")\n\t\t\treturn False\n\t\tansible_args = v[1]\n\tif verbose is not None: \n\t\tansible_args['verbosity']\t= (verbose or 1)\n\telse"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tif os.path.isdir(playbook_path):\n\t\tplaybook_path = os.path.join(playbook_path,\n\t\t\t\t\tos.path.basename(playbook_path))\n\n\tir_plugin.execute_playbook(ir_workspace, playbook_path, verbose=verbose,\n\t\textra_vars=extra_vars, ansible_args=ansible_args)\n\n@cli.command()\n@clidriver.plugin_option\n@clivar.require_active_workspace\n@clivar.require"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\toptions = dict()\n\tansible_args = dict()\n\n\tfor var in ['extra_vars', 'ansible_args','verbose']:\n\t\tif ir_plugin.plugin_cfg.has_option('playbook', var):\n\t\t\tif var == 'ansible_args':\n\t\t\t\tansible_args = yaml.safe_load(ir_plugin.plugin_cfg.get('playbook', var))\n\t\t\tif var =='verbose':\n\t\t\t\toptions['--verbose'] = ir_plugin.plugin_cfg.get('playbook', var)."}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tif os.path.dirname(ansible_plugin_defaults.ANSIBLE_PLAYBOOK_PATH)!= \"\":\n\t\tansible_plugin_defaults.ANSIBLE_PLAYBOOK_PATH = \"%s/\" % \\\n\t\t\tos.path.dirname(ansible_plugin_defaults.ANSIBLE_PLAYBOOK_PATH)\n\n\tansible_path = os.path.join(ir_workspace.get_ir_config_root_path(),\n\tansible_plugin_defaults.ANSIBLE_WORKSPACE_DIR,\n\tansible_plugin"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\treturn run_ansible(ir_workspace=ir_workspace,\n\tir_plugin=ir_plugin,\n\tplaybook_path=playbook_path,\n\tverbose=1 if verbose is None else verbose,\n\textra_vars=None,\n\tansible_args=\"--unattended --ssh -c'sysctl kernel.yama.ptrace_scope=1'\")\n\n# ansible connection plugins/tasks\ndef run_ansible(ir_workspace,\n\t\t\t\tir_plugin,\n\t\t\t\tplaybook_path,"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tif not vars_dict:\n\t\treturn {}\n\n\tresult = ExecutorHelper().run_playbook(\n\t\tcl=cli_args,\n\t\tvars_dict=vars_dict,\n\t\tir_workspace=ir_workspace,\n\t\tir_plugin=ir_plugin\n\t)\n\n\treturn result.json_result\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tlogger.debug(2, \"Executing with command: %s\", cli_args)\n\tcmd_results = None\n\ttry:\n\t\twith hide('running','msg'):\n\t\t\tcmd_results = run_cmd(cli_args, shell=True, timeout=300)\n\texcept CommandError as ce:\n\t\t# In case of an error, display the result file and save it to\n\t\t# the temporary file if it exists.\n\t\tif cmd_results is None:\n\t\t\tlogger.debug(2, \"No results yet\")\n\t\telse:\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# Generating vars dict for Ansible\n\tvars_str = ','.join('%s=%s' % x for x in vars_dict.items())\n\n\treturn cli_args, ir_plugin.run(workspace=ir_workspace,\n\t\t\t  command = ir_plugin.name,\n\t\t\t  extra_vars = \"--extra-vars '%s'\" % vars_str,\n\t\t\t  extra_args = \"--extra-vars\")\n  \ndef _run_npm_command(cli_args):\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\tglobal results_paths\n\tresults_paths = vars_dict.get('results_path')\n\tvars_dict['results_path'] = True\n\t\n\t\n\t# create an Ansible Playbook\n\tplay_book = []\n\ttry:\n\t\tif vars_dict.get('verbose_flag', True):\n\t\t\tlogger = logger_m.logger\n\texcept:\n\t\t\n\t\n\t\tlogger = logger_m.logger\n\t\n\tplay_book.append(AnsiblePlayBook(\n\t\t\t\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# If we are setting ansible vars on a hostgroup, we need to set them\n\t# with the --become and --become-user flags\n\tbecome_arg = cli_args[0]\n\tbecome_arg_val = os.environ.get(become_arg, None)\n\tif become_arg_val is not None:\n\t\tbecome_arg_val = getattr(ir_plugin,\n\t\t\t\t\t\t\t\t ir_plugin.env_settings.get(become_arg, None))\n\t\tif getattr(ir_plugin,"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn run_ansible_cli_with_plugin(\n\t\tcli_args,\n\t\tvars_dict,\n\t\tir_workspace,\n\t\tir_plugin,\n\t)\n\ndef execute_command_cli(\n\targs,  # this is the argv without the first argument\n\tparser,  # this is the argv without the first argument\n\tvars_dict,\n\tdebug=None,\n\tworkspace=\"\",\n\tplugin=\"\",\n\tcommand_string=None,\n):\n\t\"\"\"\n\t:param commands: a list of dict,"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\ttry:\n\t\t# Setup the local temp directory (if specified)\n\t\tif cli_args.temp_dir_path:\n\t\t\tif os.path.isdir('{0}/tmp'.format(cli_args.temp_dir_path)): \n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tshutil.rmtree('{0}/tmp'.format(cli_args.temp_dir_path),True) # Remove any existing temp folder\n\t\t\t\tos.mkdir('{0}/tmp'.format(cli_args.temp_dir_path))\n\t\t\ttemp"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn IrActionsUtil().run_ansible_playbook(\n\t\tcli_args=cli_args, \n\t\tvars_dict=vars_dict, \n\t\tworkspace_id=ir_workspace.get_id(),\n\t\tplugin_type=ir_plugin.get_plugin_type(),\n\t\treboot=False\n\t)\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\thosts = {}\n\tif not cli_args.inventory:\n\t\thosts[\"all\"] = {\n\t\t\t\"ansible_connection\": plugin_connection_type(plugin_object_or_plugin_url(ir_plugin)),\n\t\t\t\"ansible_inventory\": str(ir_workspace._session.inventory.relative_dst_to(ir_plugin)),\n\t\t\t\"ansible_user\": str(cli_args.api_user),\n\t\t}\n\telse:\n\t\tfor inv_file in cli_args.inventory:\n\t\t\thosts."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn ir_plugin.run(cli_args, ir_workspace, vars_dict)\n\ndef _get_config(vars_dict):\n\t\"\"\"\n\tGathers the values of the config variables from the Ansible vars dict\n\t\n\t:param vars_dict: dict, represents the Ansible vars dict\n\t:return: An OrderedDct, consists of config variables and their values from\n\tthe Ansible vars dict\n\t\"\"\"\n\t# Define default config variables\n\tdef_configs = OrderedDict()\n\t\n"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\ttype = {}\n\t\n\t# convert ints to strings\n\ttype['id'] = str\n\tfor k in ['memory',\n\t\t\t'cpus',\n\t\t\t'disk_size',\n\t\t\t'extra-specs',\n\t\t\t'interfaces',\n\t\t\t'interface_type',\n\t\t\t'boot_order',\n\t\t\t'console',\n\t\t\t'mgmt_addr',\n\t\t\t'mac_addr',\n\t\t\t'domain-name',\n\t\t\t'hostname',\n\t\t\t'vpc-id',\n\t\t\t'host-name',\n\t\t\t'password',\n\t\t\t'key"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor values in values_dict.itervalues():\n\t\tif not isinstance(values, basestring):\n\t\t\tvalues_dict[parser_name] = str(values)\n\n\ndef register(cmd_name, description='', run_as_root=False, root_helper='sudo'):\n\t\"\"\"\n\tRegister new cmd for a cli.\n\t\n\t:param cmd_name: The name of the subcommand under which this subcommand is registered\n\t:param description: A short description of the command\n\t:param run_as_root: run command"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalues_dict.update({\n\t'remove': True,\n\t'timeout': int,\n\t'host': str,\n\t'password': str,\n\t'username': str,\n\t})\n\t\n\n\nclass RunScriptCommand(BaseCommand):\n\t\n\tdef get_parser(self):\n\t\treturn get_parser(parser_name=self.name)\n\t\n\tdef run(self, args):\n\t\t(args.remove, args.timeout, args.host, args.password, args.username) = self._detect_cli_args(**"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor key, val in values_dict.iteritems():\n\t\t\tvalues_dict[key] = (type(val)!= type(None))\n\ndef str2bool(v):\n\tif not v or v.lower() in ('false', '0'):\n\t\treturn False\n\treturn True\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor k, v in six.iteritems(values_dict):\n\t\tcast_to = self._CONST_TYPE_ARGS_MAPPING.get(k, \"str\")\n\t\tif cast_to == \"bool_str\" and v not in (\"True\", \"False\"):\n\t\t\ttry:\n\t\t\t\tvalues_dict[k] = bool(v)\n\t\t\texcept Exception:\n\t\t\t\tif not v:\n\t\t\t\t\tvalues_dict[k] = False\n\t\t\t\telse:\n\t\t\t\t\tvalues_dict[k] = True\n\t\t\t\tif parser_name in self.STR_AR"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor flag, value in values_dict.items():\n\t\t\n\t\t# Handle the flag.\n\t\targ_type = self._get_arg_type(flag)\n\t\tvalue = self._convert_value(arg_type, value)\n\t\tvalues_dict[flag] = value\n\t\t\n\t\t# Handle the value.\n\t\tif arg_type in self._cli_to_ro_kwargs_converter:\n\t\t\tvalues_dict.pop(flag)\n\t\t\tnewflag = self._get_cli_kwarg_flag(arg_type, flag)\n\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor argument in values_dict.keys():\n\t\tif vars(values_dict)[argument].strip():  # only convert non empty strings\n\t\t\tvars(values_dict)[argument] = int(vars(values_dict)[argument])\n\t\t\ttry:\n\t\t\t\tvars(values_dict)[argument] = float(vars(values_dict)[argument])\n\t\t\texcept ValueError:\n\t\t\t\traise ValueError(\"Error - only convert ints, floats, and booleans!\")\n\"\"\"\n\tReturns:\n\t\ttuple of (name_of_parser,values_dict"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalues_dict['image_dir'] = str(values_dict['image_dir'])\n\tvalues_dict['output_dir'] = str(values_dict['output_dir'])\n\tvalues_dict['config_dir'] = str(values_dict['config_dir'])\n\tvalues_dict['image_type'] = str(values_dict['image_type'])\n\tvalues_dict['template_dir'] = str(values_dict['template_dir'])\n\tpass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##########################################################################\n#\n#"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tfor k in values_dict:\n\t\tvalues_dict[k] = convert(parser_name, values_dict[k])\n\n\ndef _build_parser(parser_name):\n\t\n\tparser = configparser.ConfigParser(strict=False)\n\t\n\twith open(os_vars.get(parser_name, parser_name, \".ini\")) as config_file:\n\t\tparser.read_file(config_file)\n\t\n\tparser_section = None\n\n\t_convert_non_cli_args(parser_name, copy("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\t\t\n\t\tdefault_to_str = False\n\t\t\n\t\tfor key, val in list(values_dict.items()):\n\t\t\tvalues_dict[key] = val\n\t\t\t\n\t\t\tif isinstance(val, (int, float)):\n\t\t\t\tvalues_dict[key] = str(val)\n\t\t\t\t\n\t\t\t\tif val == int:\n\t\t\t\t\tdefault_to_str = True\n\t\n\t\t\t\n\t\t\tif isinstance(val, list):\n\t\t\t\tvalues_dict[key] = \",\".join(val)\n\t\n\t\t\tif key == \"--all\":\n"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport os, json\nfrom common.util import validate_file_and_directory, get_all_files_from_dir, validate_param_type, validate_param\n\n\ndef get_plugin_spec_flatten_dict(plugin_yaml_file=\"plugin_spec.yaml\", validate=True):\n\t\"\"\" \n\t\tflatten is a simple function used to flatten dictionary\n\n\t\tIt should be fed into param_dict_flatten dict created above and it should return flattened param_dict\n\n\t\"\"\"\n\timport yaml\n\n\t# The plugin_dir is for plugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\twith open(\"%s/plugin_spec.json\" % plugin_dir) as plugin_file: json_data = json.load(plugin_file)\n\treturn flatten_dict(json_data)\n\t"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.yml')\n\tplugin_spec_file = get_file_if_exists(plugin_spec_file)\n\ttry:\n\t\treturn yaml.full_load(open(plugin_spec_file, 'r').read())\n\texcept Exception as e:\n\t\traise exceptions.InvalidYamlFile(\"The plugin spec file [{}] is not a valid YAML file\".format(plugin_spec_file))\n\ndef get_plugin_spec_raw(plugin_dir"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport json\n\t\n\tif 'plugins' not in get_plugin_spec(plugin_dir):\n\t\treturn None\n\t\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tplugin_spec_json = plugin_spec['plugins']\n\t\n\tplugin_flatten_spec_dict = {}\n\tif plugin_spec_json:\n\t\tfor name, spec in plugin_spec_json:\n\t\t\t# flatten spec and add to plugin_flatten_spec_dict\n\t\t\tplugin_flatten_spec_dict[name] = json.d"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\tplugin_file = os.path.join(plugin_dir, 'Plugin.proto')\n\tplugin_content = \"\"\n\twith open(plugin_file, 'rb') as plugin_f:\n\t\tplugin_content = base64.b64encode(plugin_f.read())\n\t\n\tflatten_dict = {\n\t\t'name': '',\n\t\t'plugin_id': '',\n\t\t'plugin_version': '',\n\t\t'module_name': '',\n\t\t'executable_type': '',\n\t\t'timeline_name"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\tplugin_dict = {}\n\n\t# Search for and load plugins\n\tfor class_name in os.listdir(plugin_dir):\n\t\tif class_name.endswith(\"Plugin\"):\n\t\t\tplugins = load_module_class(plugin_dir, class_name)\n\n\t\t\t# get all the members of the plugin if exist\n\t\t\tfor member_name, member in inspect.getmembers(plugins):\n\t\t\t\tif \"plugin\" in member_name.lower():\n\t\t\t\t\tsetattr(plugins, member_name, member)\n\t\t\t\t\tplugin_dict[member_name] ="}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t\n\t# The name of the module the class is in\n\tmodule_name = plugin_dir.replace(plugin_dir.rfind('.') + 1)\n\tplugin_spec = {}\n\t\n\ttry:\n\t\t_, plugin_cls = get_plugin_module(module_name)\n\texcept AttributeError as e:\n\t\traise Exception(u\"The Plugin: \" + plugin_dir + u\" could not be loaded at: \" + str(e))\n\t\t\n\treturn plugin_cls.schema()\n\n\ndef get_plugin_spe"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tspec = get_plugin_spec(plugin_dir)\n\treturn flatten_dict(spec)\n\ndef get_plugin_spec_str_dict(plugin_dir):\n\"\"\"\n\tCreates a str based dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A str based dictionary contains the plugin's properties\n\t\"\"\"\n\tspec_dict = get_plugin_spec_flatten_dict(plugin_dir)\n\treturn convert_dict_to_str(spec_dict)\n\n\ndef get_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_specs_dict = {}\n\tfor plugin_file in (f for f in os.listdir(plugin_dir) if os.path.isfile(\n\t\t\t\t\t\t\t\t'/'.join([plugin_dir, f])) and ('.spec.yaml' in f)):\n\t\twith open('/'.join([plugin_dir, plugin_file]), 'r') as fp:\n\t\t\tplugin_specs_dict = {**plugin_specs_dict, **YAML.safe_load(fp)[plugin_file]}\n\treturn plugin_specs_dict"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tconfig = get_plugin_spec(plugin_dir)\n        return flatten_dict(config)\n\n\n\ndef flatten_dict(d):\n\t\"\"\"Flattens a recursive dict into a single list of key/value pairs\"\"\"\n\tout = []\n\tfor k, v in d.items():\n\t\tif isinstance(v, collections.MutableMapping):\n\t\t\tout.extend(flatten_dict(v).items())\n\t\telse:\n\t\t\tout.append((k, v))\n\treturn out\n\ndef get_plugin_spec_flat_"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv_key = 'S2HIVE_FACADE_CONFIG_PATH'\n\tif not env_key in os.environ:\n\t\tos.environ[env_key] = self.config_path\n\n\nclass Config:\n\t\"\"\"A read-only configuration object.\n\n\tSubclasses should provide their own implementation. An instance\n\tof this object is required to provide read-only functionality.\n\n\tAttributes:\n\t\tconfig (dict): Environment variables read from the config file\n\t\"\"\"\n\n\tdef __init__(self, config_path=None):\n\t\t\"\"\""}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.env_var is not None:\n\t\tos.environ[self.env_var] = self.config_path\n\ndef set_config_value(self, var, val):\n\"\"\"\n\tSet a configuration value in environment variable, e.g. \"HUBFILE_CONFIG\".\n\t\"\"\"\n\tos.environ[var] = val\n\nclass EnvironmentError(Exception):\n\tpass\n\n###########\n# Main #\n###########\n\nclass SdkProject:\n\tdef __init__(self, name, hub_file=None):"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = \"\"\n\t\n\t# Set config path from the environment variable\n\tconfig.load_config('.', os.environ['CONFIG_PATH'])\n\t\n\t# Try to read the config with default path\n\tconfig.load_config('.', \"~/.config\"+os.sep+\"lutris.conf\")\n\t# Load the custom config if user-enabled\n\tcustom = config.load_custom_config()\n\tif"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tself.write_to_os_environ(\"INJECTECONFIG\", \"%s/injection_config\" % self.getcwd())\n\n  def get_configdir(self):\n\"\"\"\n\tGet the injection config directory.\n\t\"\"\"\n\treturn \"%s/injection_config\" % self.getcwd()\n\n  def get_config(self, filename, module_name):\n\"\"\"\n\tObtain a given config file.\n\t\"\"\"\n\tresult = \"\"\n\tif not os.path.exists(self.get_configdir()):\n\t\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv_var = 'AWS_CONFIG_FILE'\n\tif self.config:\n\t\tif not os.path.exists(env_var):\n\t\t\tos.environ[env_var] = self.config\n\n\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tpath = os.environ.pop('CONFIG_PATH', None)\n\tif path is not None:\n\t\tos.environ['CONFIG_PATH'] = path"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\ttry:\n\t\timport site\n\t\tpath = site.getsitepackages()[0]\nexcept:\n\tpass\n#sys.path.append('./.venv/Lib/site-packages')\nsys.path.append(path)\n\n# Inject python libraries if needed.\nif not sys.path.count('./dmlc_v2.5/python'):\n  sys.path.insert(0, './dmlc_v2.5/python')\n\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfig_path = os.getenv('QSICHAT_CONFIG', self.default_config_path)\n\"\"\"\n\tSet the environment variable for config file extension, if it is undefined.\n\t\"\"\"\n\tconfig_extension = os.getenv('QSICHAT_FILE_EXTENSION', self.default_config_extension)\n\n\tif config_path:\n\t\tself.cwd = config_path\n\t\tLOGGER.debug('QSICHAT_PATH:'+ self.cwd)\n\t\tconfig_extension = self.clean_ext"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv_key = 'FUGUI_CONFIG'\n\tif env_key not in os.environ:\n\t\tos.environ[env_key] = os.path.join(self.project_name, 'gui')\n\n\ndef _main(root_p):\n\tdef __main():\n\t\tif __name__ == '__main__':\n\t\t\tos.chdir(os.path.dirname(root_p))\n\t\t\tfrom gui3d.app import App\n\t\t\tApp(root)\n\n\tdef main():\n\t\tif __name__ == '__"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = os.path.join(os.getcwd())\n\ndef main(start=['bin/qemu-system-x86_64'],\n\t\tport=1224,\n\t\tip_pool=[],\n\t\tinterval=0.3,\n\t\ttimeout=10,\n\t\tout_file=\"qemu.txt\",\n\t\tlogger=None,\n\t\tprint_cmd=False):\n\t\"\"\"\n\tStart qemu-system"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tif not hasattr(self, 'args'):\n\t\t\tself.args = []\n\n\t# the parser arguments\n\tself.parser.set_defaults(spec_mode=False)\n\t# group options\n\t_ = root_subparsers.add_parser()\n\t_.set_defaults(command=None)\n\t_.set_defaults(spec=None)\n\t_._actions[0].add_argument('-s', '--spec_mode', action='store_true', required=False, help='enables specification mode.')\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\t# Register the options\n\tparser = root_subparsers.add_parser('specs', help='spec related cli options ')\n\t#\n\tparser.set_defaults(dispatch = dispatch_specs)\n\tsubparsers = parser.add_subparsers(dest='dispatch')\n\t#\n\tcluster_parser = subparsers.add_parser('cluster', help='dispatch option')\n\tcluster_parser.set_defaults(func = dispatch_cluster)\n\tcluster_subpars"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tself._parser = parser\n\n\n\n#---------------------------------------------------------------------------------------------------\n#\n#\t\t\t\t\tEXTERNAL SUPPORT FUNCTIONS\n#\n#---------------------------------------------------------------------------------------------------\n\ndef _set_verbose(args):\n\t\n\t\"\"\"\n\tSet the output verbosity level based on command line arguments.\n\tIf none are passed, use a sensible default.\n\t\"\"\"\n\t\n\t# set up verbose or quiet\n\tif args.verbose == None:\n\t\tverbose = False\n\tif args.verbose == True:\n\t\tverbose = True\n\telse:\n\t\tverb"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser('specs', help ='spec related commands', description ='spec related commands')\n\tsubparsers.set_defaults(which ='specs')\n\tsubparser = subparsers.add_subparsers(title = 'commands', dest = 'which', description = 'the spec related commands')\n\tsubparser.required = True\n\tspec_parser = subparser.add_parser('create', help = 'create a new spec.', description = 'create a new spec.')\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\treturn root_subparsers.add_parser(\"spec\", help=\"Enables spec generation, with a few flags\")\n\n@spec.command()\ndef default(args):\n\t\"\"\"\n\t\"\"\"\n\tpass\n\n@spec.command()\n@argument(\"-g\", \"--generate-tests\", help=\"generate tests for the spec\", action=\"store_true\")\n@argument(\"-t\", \"--testing\", help=\"spec should be tested\", action=\"store_true\")\ndef run(args):\n\tif args.generate_tests:\n\t\ttests."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser('extend',help='extend specified model file',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t formatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t\t\t\t\t\t\t\t\t\t )\n\t\n\t# required arguments\n\tparser = subparsers.add_argument('--model','-m',required=True,default=None,type=str,\n\t\t\t\t\t\t\t\t\t\t\t\t\t  help='the file or folder containing the model you want to extend'\n\t\t\t\t\t\t\t\t\t\t\t\t\t   )\n\tparser = subparsers.add_argument('--add',"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot = root_subparsers.add_parser(\"extend\", help=\"Specify an extension to extend a template\")\n\troot.add_argument('-m', '--module', dest=\"module\", help=\"Specify the name of the module (ie: 'core'). Default is 'core'\")\n\troot.add_argument('-i', '--include-dirs', help=\"Specify an include directories, either a single directory or a comma separated list (ie: 'foo, bar')\")\n\troot.add_argument('-s', '--strip-dir',"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_subparsers.add_parser(\n\t\t'spec',\n\t\thelp=\"Extract information that would be useful in creating a testcase specification.\")\n\n\nclass ExtractCli(BasePySpecCLI):\n\t\"\"\"\n\tCommand line interface to be used for extracting configuration options that would be useful in \n\tpopulating a testcase specification.\n\t\"\"\"\n\tdef create_parser(self):\n\t\t\"\"\"\n\t\tAdds the spec cli options to to the main entry point.\n\t\t\n\t\t:returns: argument parser for the command line interface.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\textend_common_cli(root_subparsers, [\"spec\", \"format\", \"show-spec\"], \"Show spec\")\n\textend_common_cli(root_subparsers,\n\t\t[\"spec\", \"extract\", \"export\"],\n\t\t\"Export spec\"\n\t)\n\ndef main(argv=None):\n\targv = argparse.ArgumentParser().parse_args(argv[1:]) if argv else None\n\n\tparser = argparse.ArgumentParser(prog=\"specsplit\")\n\tsubparsers"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tif len(self.plugins):\n\t\textender = self.plugins[0]\n\t\textender.extend_cli(root_subparsers)\n\nclass Plugin(metaclass=ABCMeta):\n\t\"\"\"\n\tAbstract method used to implement a plugin.\n\t\n\tThe interface of a plugin is described as follows:\n\t\n\t* create a new cli subcommand and create a new CLI object (i.e. cli command)\n\t  or modify its command group\n\t* implement some command line interface elements\n\t* create or extend your existing commands\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\thead, tail = os.path.split(path)\n\tif head == root:\n\t\traise RuntimeError(\"Cannot strip root path\")\n\treturn os.path.join(head, tail)\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError(\"path '{0}' doesn't start with root '{1}'\".format(path, root))\n\treturn path[len(root):]"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not path:\n\t\treturn\n\tl = path.split('\\\\')\n\tif l[0] == root:\n\t\tl = l[1:]\n\treturn '\\\\'.join(l)\n\ndef remove_backslash(path):\n\"\"\"\n\tRemove backslash at end of path.\n\t\"\"\"\n\tif not path:\n\t\treturn\n\tl = path.split('\\\\')\n\tif l[-1] == '':\n\t\tl = l[:-2]\n\tpath = '\\\\'.join(l)\n\treturn path\n\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not (path.startswith(root) and len(root)>0):\n\t\traise Exception(\"Invalid path.\")\n\treturn path[len(root):]\n\ndef strip_ext(path, ext='.zip'):\n\t\"\"\"\n\tRemove.zip from the end of the path, throw exception on failure.\n\t\"\"\"\n\tif not (path.endswith(ext) and len(ext)>0):\n\t\traise Exception(\"Invalid path\")\n\treturn path[:len(path)-len(ext),\"]\")\n\ndef to_base(path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t# Strip off the leading `/` if any.\n\tpath = path.lstrip('/')\n\t# Remove the root if present.\n\tif path.startswith(root):\n\t\tpath = path.lstrip(root)\n\t\tif path:\n\t\t\traise FilePathException(\"No files at {}\".format(path))\n\t\t# All done\n\t\treturn None\n\t# Otherwise, return the full root removed.\n\treturn root.rtrim(path)\n\n\n# noinspection PyPep8Naming,PyMethodMayBeStatic\nclass FilePathException"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif os.path.isabs(path) and  len(os.path.commonprefix([path, root])) > 0:\n\t\treturn os.path.relpath(path, os.path.join(root, os.pardir))\n\telse :\n\t\treturn path.encode(\"utf-8\")\n\t\ndef strip_file_prefix(path, prefix):\n\t\"\"\"\n\tRemove prefix from a path, throw exceptions.\n\t\"\"\"\n\tif os.path.sep in prefix:\n\t\traise Exception(prefix + \" is not a file path.\")\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif len(path) > 0 and path[0] == root:\n\t\traise Exception('Attempting to strip root of empty path.')\n\n\treturn path[len(root):]\n\ndef strip_extension(path, ext):\n\t\"\"\"\n\tFor file-system paths, strip the `ext` prefix off\n\tany path component.\n\n\tIgnore path components if `ext` is empty, as in case\n\tof `/foo.ext`.\n\t\"\"\"\n\tif not ext or path.endswith(ext):\n\t\treturn path\n\n\troot, ext ="}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not path.startswith(root):\n\t\traise ValueError(\"'{}' is not part of '{}'\".format(path, root))\n\tif not path[len(root)].isalnum():\n\t\traise ValueError(\"'{}' doesn't have expected format, should be \\\".+\\'\\\"\".format(path))\n\treturn path[len(root):]\n\n\ndef to_timestamp(t):\n\"\"\"\n\tConvert UNIX timestamps to human-readable format.\n\t\"\"\"\n\treturn time.strftime(\"%c\", time"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t\n\tif not path:\n\t\tif isinstance(root, list) and 0 in root:\n\t\t\treturn path, \"\"\n\t\t\n\t\traise Exception(\"unable to strip root from : |%s|\" % path)\n\t\n\tpath, path_root = os.path.split(path)\n\t\n\tif not path:\n\t\tif isinstance(root, list) and 0 in root:\n\t\t\treturn path, \"\"\n\t\t\n\t\traise Exception(\"unable to strip root from : |%s|\" % path)\n\t\n\tsplit_index = path."}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tpath = os.path.abspath(path)\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise ValueError(\"%s already outside root %s\"%(path, root))\n\n\ndef strip_prefix(path, prefix):\n\t\"\"\"\n\tRemove prefix from path, throw exception.\n\t\"\"\"\n\tpath = os.path.abspath(path)\n\tif path.startswith(prefix):\n\t\treturn path[len(prefix):]\n\telse:\n\t\traise ValueError"}]}
